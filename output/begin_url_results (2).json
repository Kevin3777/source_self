{
  "model_path": "models/begin_url/final_model",
  "evaluation_params": {
    "max_new_tokens": 200,
    "temperature": 0.7,
    "top_p": 0.9,
    "batch_size": 32,
    "num_samples": 1604
  },
  "average_scores": {
    "rouge-1": 0.17477637665102388,
    "rouge-2": 0.024183252651398656,
    "rouge-l": 0.16048199030608845
  },
  "evaluation_time": 234.59040713310242,
  "samples_per_second": 6.837449235892749,
  "individual_results": [
    {
      "paper_id": "econ.EM.econ/EM/2411.16978v2",
      "true_abstract": "We establish normal approximation in the Wasserstein metric and central limit\ntheorems for both non-degenerate and degenerate U-statistics with\ncross-sectionally dependent samples using Stein's method. For the\nnon-degenerate case, our results extend recent studies on the asymptotic\nproperties of sums of cross-sectionally dependent random variables. The\ndegenerate case is more challenging due to the additional dependence induced by\nthe nonlinearity of the U-statistic kernel. Through a specific implementation\nof Stein's method, we derive convergence rates under conditions on the mixing\nrate, the sparsity of the cross-sectional dependence structure, and the moments\nof the U-statistic kernel. Finally, we demonstrate the application of our\ntheoretical results with a nonparametric specification test for data with\ncross-sectional dependence.",
      "generated_abstract": "We consider a model of stochastic volatility in which the volatility process\nis driven by the mean of an Ornstein-Uhlenbeck process. We derive a central\nlimit theorem for the mean of the stochastic volatility process and prove that\nthe central limit theorem is valid even when the mean of the Ornstein-Uhlenbeck\nprocess is not identically zero. We further derive a Cram\\'er-Rao lower bound\nfor the mean of the stochastic volatility process, which is shown to be\nasymptotically normal. Finally, we derive an expression for the Cram\\'er-Rao\nlower bound for the mean of the Ornstein-Uhlenbeck process, and show that this\nlower bound is asymptotically normal. We illustrate the validity of the\ncentral limit theorem and the Cram\\'er-Rao lower bound with numerical\nsimulations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22077922077922077,
          "p": 0.3333333333333333,
          "f": 0.26562499520629884
        },
        "rouge-2": {
          "r": 0.05660377358490566,
          "p": 0.07142857142857142,
          "f": 0.0631578898038785
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.27450980392156865,
          "f": 0.21874999520629895
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.17215v1",
      "true_abstract": "This paper presents an integrated modelling assessment that estimated the\nsensitivities of five endogenous factors in commercial rangelands, i.e. number\nof active farmers, profits, stocking rate, standing herbage biomass, and soil\nerosion, to the same percentage variation in 70 factors, including economic and\nclimate drivers. The assessment utilised a system dynamics model (107\nequations) which represents an area of extensive private farms, its farmers,\nthe main local markets on which they trade, and key ecosystem services\ninvolved. The assessment procedure consisted in analysing the behaviours of\n288,000 variants of this system during 300 years, each under a different\neconomic and climate scenario. Our key findings were as follows: 1) It is\nlikely that at least annual grasslands will suffer environmental degradation in\nthe future, and that such degradation will be primarily caused by climate\nchange, not by the increasing demand for livestock products; 2) Private farming\nsystems provide social and economic security to farmers against the effects of\nclimate change, especially in a scenario of rising prices of animal products.\nHowever, this research will remain incomplete until its methods and results can\nbe contrasted with other similar assessments.",
      "generated_abstract": "r develops a novel theoretical framework for explaining how\nestablished firms' profit-maximizing behavior can be altered by the introduction\nof new technologies. We argue that firms' behavior can be driven by three\nfundamental forces: (1) the firm's incentives, (2) the technological\ninnovation, and (3) the market size. Understanding how these forces interact\nwill help us to understand the dynamics of market evolution and the role of\nfirm size in economic growth. Our theoretical model suggests that, under\ncertain conditions, firms will adopt new technologies if the market size is\nlarge enough to capture the value of the innovation. When the market size is\nsmall, firms may not adopt the new technology even if it has a large value,\nwhich is the opposite of the case in the case of market size being small.\nFurthermore, our theoretical model shows that,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14388489208633093,
          "p": 0.22727272727272727,
          "f": 0.17621144899687566
        },
        "rouge-2": {
          "r": 0.016304347826086956,
          "p": 0.024,
          "f": 0.01941747091044411
        },
        "rouge-l": {
          "r": 0.1223021582733813,
          "p": 0.19318181818181818,
          "f": 0.14977973093520167
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.15264v1",
      "true_abstract": "Speech recognition systems often face challenges due to domain mismatch,\nparticularly in real-world applications where domain-specific data is\nunavailable because of data accessibility and confidentiality constraints.\nInspired by Retrieval-Augmented Generation (RAG) techniques for large language\nmodels (LLMs), this paper introduces a LLM-based retrieval-augmented speech\nrecognition method that incorporates domain-specific textual data at the\ninference stage to enhance recognition performance. Rather than relying on\ndomain-specific textual data during the training phase, our model is trained to\nlearn how to utilize textual information provided in prompts for LLM decoder to\nimprove speech recognition performance. Benefiting from the advantages of the\nRAG retrieval mechanism, our approach efficiently accesses locally available\ndomain-specific documents, ensuring a convenient and effective process for\nsolving domain mismatch problems. Experiments conducted on the CSJ database\ndemonstrate that the proposed method significantly improves speech recognition\naccuracy and achieves state-of-the-art results on the CSJ dataset, even without\nrelying on the full training data.",
      "generated_abstract": "This paper presents a novel method for generating synthetic audio\naudio samples from a given text. We introduce a novel neural network architecture\nto generate audio samples from text descriptions. We also introduce a new\nlearning framework that incorporates a latent code-to-text generation model,\nwhich is trained on the synthesized audio samples. This framework allows for\nfine-tuning and fine-tuning on unseen text, enabling the generation of\nhigh-quality audio samples. We demonstrate the efficacy of our approach by\ngenerating high-quality audio samples for several text-to-speech (TTS) models,\nincluding OpenTTS, Tacotron2, and TTS2. Our results show that our approach can\ngenerate high-quality audio samples that are indistinguishable from those\ngenerated by the real TTS models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18691588785046728,
          "p": 0.2857142857142857,
          "f": 0.22598869578345948
        },
        "rouge-2": {
          "r": 0.028169014084507043,
          "p": 0.039603960396039604,
          "f": 0.032921805841928616
        },
        "rouge-l": {
          "r": 0.16822429906542055,
          "p": 0.2571428571428571,
          "f": 0.20338982572696232
        }
      }
    },
    {
      "paper_id": "math.GM.math/GM/2502.17466v1",
      "true_abstract": "For a hypergroup $(H,\\circ)$ we consider $\\gamma^{\\ast}$, as the smallest\nequivalence relation on $H$ such that the quotion\n$(H/\\gamma^{\\ast},\\tiny{\\otimes})$ is an abelian group. We study some more\nproperties of $\\gamma^{\\ast}$. Initially, it is investigated which\nsubhypergroup the congruence relation modulo is strongly regular on, and its\nquotient results in an abelian group? This is directly related to the\nfundamental relation $\\gamma^{\\ast}$, since such subhypergroups must contain\n$S_{\\gamma}$. Then, we examine the functor $\\gamma^{\\ast}$ from a categorical\nperspective and investigate properties such as continuity and cocontinuity\nconcerning it using the decomposition $\\gamma=\\delta\\tiny{\\ast}\\beta$. For this\npurpose, we define the reduced words on strongly regular hypergroups. This has\na direct application in studying how the functor $\\gamma^{\\ast}$ affects on the\nstalks of the sheaves of hypergroups.",
      "generated_abstract": "We study the $k$-th roots of unity for $k\\in \\{3,5,7,11\\}$ and prove the\n$3$-root conjecture. We also prove that the $3$-root conjecture implies that\n$p$ is a $p$-th power in $\\mathbb{Z}_3$, for $p$ prime, and that $p$ is a\n$p$-th power in $\\mathbb{Z}_7$, for $p$ prime. Finally, we show that the\n$7$-root conjecture implies that $p$ is a $p$-th power in $\\mathbb{Z}_5$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12195121951219512,
          "p": 0.3125,
          "f": 0.17543859245306256
        },
        "rouge-2": {
          "r": 0.01694915254237288,
          "p": 0.046511627906976744,
          "f": 0.024844716581922614
        },
        "rouge-l": {
          "r": 0.12195121951219512,
          "p": 0.3125,
          "f": 0.17543859245306256
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2411.05951v1",
      "true_abstract": "Multifractality is a concept that helps compactly grasping the most essential\nfeatures of the financial dynamics. In its fully developed form, this concept\napplies to essentially all mature financial markets and even to more liquid\ncryptocurrencies traded on the centralized exchanges. A new element that adds\ncomplexity to cryptocurrency markets is the possibility of decentralized\ntrading. Based on the extracted tick-by-tick transaction data from the\nUniversal Router contract of the Uniswap decentralized exchange, from June 6,\n2023, to June 30, 2024, the present study using Multifractal Detrended\nFluctuation Analysis (MFDFA) shows that even though liquidity on these new\nexchanges is still much lower compared to centralized exchanges convincing\ntraces of multifractality are already emerging on this new trading as well. The\nresulting multifractal spectra are however strongly left-side asymmetric which\nindicates that this multifractality comes primarily from large fluctuations and\nsmall ones are more of the uncorrelated noise type. What is particularly\ninteresting here is the fact that multifractality is more developed for time\nseries representing transaction volumes than rates of return. On the level of\nthese larger events a trace of multifractal cross-correlations between the two\ncharacteristics is also observed.",
      "generated_abstract": "er the problem of model predictive control (MPC) of a continuous-time\nmodel of a stochastic, nonlinear financial market. The model is a stochastic\ndifferential equation whose solution is a function of a state variable and the\nprevious $T$ time steps of the state. The model is described by a time-invariant\nmatrix $A$, a time-varying parameter $\\theta$, and a state-space model of the\nMarket. The state of the market is described by the state variable $z$ and\nincludes a noisy observation of the market $z = x + w$ where $x$ is the\nrelevant information and $w$ is a white noise. The control input $u$ is\ndescribed by a $T$-dimensional function of the state variable $z$, i.e.\n$u(z) = \\varphi(z,t)$ where $\\varphi$ is a given function",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0703125,
          "p": 0.13846153846153847,
          "f": 0.0932642442374294
        },
        "rouge-2": {
          "r": 0.016216216216216217,
          "p": 0.029411764705882353,
          "f": 0.020905918763127907
        },
        "rouge-l": {
          "r": 0.0703125,
          "p": 0.13846153846153847,
          "f": 0.0932642442374294
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2502.08613v4",
      "true_abstract": "As operators acting on the undetermined final settlement of a derivative\nsecurity, expectation is linear but price is non-linear. When the market of\nunderlying securities is incomplete, non-linearity emerges from the bid-offer\naround the mid price that accounts for the residual risks of the optimal\nfunding and hedging strategy. At the extremes, non-linearity also arises from\nthe embedded options on capital that are exercised upon default. In this essay,\nthese convexities are quantified in an entropic risk metric that evaluates the\nstrategic risks, which is realised as a cost with the introduction of bilateral\nmargin. Price is then adjusted for market incompleteness and the risk of\ndefault caused by the exhaustion of capital.\n  In the complete market theory, price is derived from a martingale condition.\nIn the incomplete market theory presented here, price is instead derived from a\nlog-martingale condition: \\begin{equation}\np=-\\frac{1}{\\alpha}\\log\\mathbb{E}\\exp[-\\alpha P] \\notag \\end{equation} for the\nprice $p$ and payoff $P$ of a funded and hedged derivative security, where the\nprice measure $\\mathbb{E}$ has minimum entropy relative to economic\nexpectations, and the parameter $\\alpha$ matches the risk aversion of the\ninvestor. This price principle is easily applied to standard models for market\nevolution, with applications considered here including model risk analysis,\ndeep hedging and decentralised finance.",
      "generated_abstract": "This paper studies the problem of pricing European options with a\ncontract-free model. We introduce a novel approach to the problem of pricing\nEuropean options with a contract-free model and present the first results\nshowing that our approach is superior to existing methods for pricing European\noptions. We also present a numerical study of our approach to illustrate its\neffectiveness. The results are compared with existing methods, including the\nMarkowitz model, the Black-Scholes model, and the Monte-Carlo method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11363636363636363,
          "p": 0.32608695652173914,
          "f": 0.16853932200984734
        },
        "rouge-2": {
          "r": 0.005208333333333333,
          "p": 0.015625,
          "f": 0.007812496250001799
        },
        "rouge-l": {
          "r": 0.11363636363636363,
          "p": 0.32608695652173914,
          "f": 0.16853932200984734
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2411.02531v3",
      "true_abstract": "The techniques suggested in Fr\\\"uhwirth-Schnatter et al. (2024) concern\nsparsity and factor selection and have enormous potential beyond standard\nfactor analysis applications. We show how these techniques can be applied to\nLatent Space (LS) models for network data. These models suffer from well-known\nidentification issues of the latent factors due to likelihood invariance to\nfactor translation, reflection, and rotation (see Hoff et al., 2002). A set of\nobservables can be instrumental in identifying the latent factors via auxiliary\nequations (see Liu et al., 2021). These, in turn, share many analogies with the\nequations used in factor modeling, and we argue that the factor loading\nrestrictions may be beneficial for achieving identification.",
      "generated_abstract": "p a novel framework for analyzing the estimation of functional\ndata models using functional Bayesian regression (FBReg). This approach\nintegrates the functional data approach with Bayesian functional modeling,\nenabling simultaneous estimation of the functional data model and its functional\nBayesian prior. This framework enables us to leverage the functional Bayesian\nmodeling framework in a more flexible way, allowing for the incorporation of\nfunctional data in the estimation of the functional Bayesian model, as well as\nthe incorporation of functional priors in the estimation of the functional\nBayesian model. We propose a novel method for modeling functional data by\nassuming that the observed data are generated from a functional process\ngenerated by a functional model. We further propose a functional Bayesian\nprior for the functional model that incorporates functional priors for the\nparameters of the functional process. We also propose a novel method for\nincorporating functional data in the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16049382716049382,
          "p": 0.22413793103448276,
          "f": 0.18705035484912805
        },
        "rouge-2": {
          "r": 0.018867924528301886,
          "p": 0.019801980198019802,
          "f": 0.019323666500503052
        },
        "rouge-l": {
          "r": 0.12345679012345678,
          "p": 0.1724137931034483,
          "f": 0.14388488722322879
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.13228v1",
      "true_abstract": "We investigate whether and why people might reduce compensation for workers\nwho use AI tools. Across 10 studies (N = 3,346), participants consistently\nlowered compensation for workers who used AI tools. This \"AI Penalization\"\neffect was robust across (1) different types of work and worker statuses and\nworker statuses (e.g., full-time, part-time, or freelance), (2) different forms\nof compensation (e.g., required payments or optional bonuses) and their timing,\n(3) various methods of eliciting compensation (e.g., slider scale, multiple\nchoice, and numeric entry), and (4) conditions where workers' output quality\nwas held constant, subject to varying inferences, or controlled for. Moreover,\nthe effect emerged not only in hypothetical compensation scenarios (Studies\n1-5) but also with real gig workers and real monetary compensation (Study 6).\nPeople reduced compensation for workers using AI tools because they believed\nthese workers deserved less credit than those who did not use AI (Studies 3 and\n4). This effect weakened when it is less permissible to reduce worker\ncompensation, such as when employment contracts provide stricter constraints\n(Study 4). Our findings suggest that adoption of AI tools in the workplace may\nexacerbate inequality among workers, as those protected by structured contracts\nface less vulnerability to compensation reductions, while those without such\nprotections risk greater financial penalties for using AI.",
      "generated_abstract": "r examines the relationship between the welfare effects of climate\nmeasures and their distributional impacts. We provide a general framework for\nanalyzing the welfare effects of climate measures in an endogenous setup. We\ndemonstrate that the welfare effects of climate measures can be decomposed\ninto two components: a direct welfare effect and a redistributive welfare\neffect. We show that the welfare effects of climate measures can be\nsignificantly different from the redistributive welfare effects, even when the\nmeasures have the same welfare effects in a given redistributive regime. We\nemploy a Bayesian model to quantify the uncertainty associated with the welfare\neffects of climate measures. We show that the welfare effects of climate\nmeasures can be more accurately estimated when the uncertainty is quantified.\nWe also investigate the potential",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11564625850340136,
          "p": 0.2698412698412698,
          "f": 0.161904757704762
        },
        "rouge-2": {
          "r": 0.005,
          "p": 0.011111111111111112,
          "f": 0.0068965474435222765
        },
        "rouge-l": {
          "r": 0.10884353741496598,
          "p": 0.25396825396825395,
          "f": 0.1523809481809525
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2405.10453v1",
      "true_abstract": "Team and player evaluation in professional sport is extremely important given\nthe financial implications of success/failure. It is especially critical to\nidentify and retain elite shooters in the National Basketball Association\n(NBA), one of the premier basketball leagues worldwide because the ultimate\ngoal of the game is to score more points than one's opponent. To this end we\npropose two novel basketball metrics: \"expected points\" for team-based\ncomparisons and \"expected points above average (EPAA)\" as a player-evaluation\ntool. Both metrics leverage posterior samples from Bayesian hierarchical\nmodeling framework to cluster teams and players based on their shooting\npropensities and abilities. We illustrate the concepts for the top 100 shot\ntakers over the last decade and offer our metric as an additional metric for\nevaluating players.",
      "generated_abstract": "llenge in the study of economic data is the identification of\nrelevant predictors of observed outcomes. Traditional approaches for the\nidentification of predictors often rely on multivariate statistical models,\noften with high-dimensional explanatory variables. In this paper, we propose a\nnovel method for identifying relevant predictors in the presence of high-\ndimensionality using a novel method of constructing a latent space of\npredictors. Our method consists of a regression-based method for constructing\na latent space of predictors, and a method for identifying the relevant\npredictors in the latent space. The proposed method is applied to a dataset\nfrom the Federal Reserve Board's Survey of Professional Forecasters. The\nproposed method is compared to a method of constructing a latent space of\npredictors using principal component analysis, and to a method of constructing\na latent space of predictors using",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.2222222222222222,
          "f": 0.1739130387145559
        },
        "rouge-2": {
          "r": 0.024390243902439025,
          "p": 0.03125,
          "f": 0.027397255349972737
        },
        "rouge-l": {
          "r": 0.1326530612244898,
          "p": 0.20634920634920634,
          "f": 0.1614906784661087
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.13850v1",
      "true_abstract": "We provide a formal framework accounting for a widespread idea in the theory\nof economic design: analytically established incompatibilities between given\naxioms should be qualified by the likelihood of their violation. We define the\ndegree to which rules satisfy an axiom, as well as several axioms, on the basis\nof a probability measure over the inputs of the rules. Armed with this notion\nof degree, we propose and characterize i) a criterion to evaluate and compare\nrules given a set of axioms, allowing the importance of each combination of\naxioms to differ, and ii) a criterion to measure the compatibility between\ngiven axioms, building on a analogy with cooperative game theory.",
      "generated_abstract": "We study the dynamic allocation of labor to a pool of workers, where workers\ninteract through a preferences-based matching market. We focus on the case of\none-shot workers, where workers cannot pre-plan their interactions with\npotential employers. We show that the optimal pool size depends on the degree of\ncoordination in the matching market. The optimal pool size is smaller than the\nmaximum pool size, and increases in the degree of coordination. We further\nshow that the optimal pool size is asymptotically optimal in the limit of a\nhigher-dimensional market with infinite pool size.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15492957746478872,
          "p": 0.22,
          "f": 0.1818181769687864
        },
        "rouge-2": {
          "r": 0.037383177570093455,
          "p": 0.05333333333333334,
          "f": 0.04395603911061519
        },
        "rouge-l": {
          "r": 0.1267605633802817,
          "p": 0.18,
          "f": 0.14876032572911704
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2409.18816v1",
      "true_abstract": "This paper presents a novel approach to evaluating blue-chip art as a viable\nasset class for portfolio diversification. We present the Arte-Blue Chip Index,\nan index that tracks 100 top-performing artists based on 81,891 public\ntransactions from 157 artists across 584 auction houses over the period 1990 to\n2024. By comparing blue-chip art price trends with stock market fluctuations,\nour index provides insights into the risk and return profile of blue-chip art\ninvestments. Our analysis demonstrates that a 20% allocation of blue-chip art\nin a diversified portfolio enhances risk-adjusted returns by around 20%, while\nmaintaining volatility levels similar to the S&P 500.",
      "generated_abstract": "This study examines the potential of using artificial intelligence (AI) to\nincrease the efficiency of trading systems, focusing on the role of\nsupervised machine learning in optimizing trading processes. The research\nutilized a case study involving the trading of Bitcoin and Ethereum using\nsupervised machine learning to evaluate the effectiveness of various trading\nstrategies. The study found that the use of supervised machine learning\nsignificantly improved trading efficiency by optimizing trading strategies,\nenhancing trading accuracy, and reducing trade costs. The study concluded that\nthe use of supervised machine learning in trading systems can improve\ntrading efficiency by optimizing trading strategies, reducing trade costs, and\nenhancing trading accuracy.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12048192771084337,
          "p": 0.18181818181818182,
          "f": 0.14492753143772333
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12048192771084337,
          "p": 0.18181818181818182,
          "f": 0.14492753143772333
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.02011v1",
      "true_abstract": "Regression models are essential for a wide range of real-world applications.\nHowever, in practice, target values are not always precisely known; instead,\nthey may be represented as intervals of acceptable values. This challenge has\nled to the development of Interval Regression models. In this study, we provide\na comprehensive review of existing Interval Regression models and introduce\nalternative models for comparative analysis. Experiments are conducted on both\nreal-world and synthetic datasets to offer a broad perspective on model\nperformance. The results demonstrate that no single model is universally\noptimal, highlighting the importance of selecting the most suitable model for\neach specific scenario.",
      "generated_abstract": "We consider the problem of learning the mean and covariance of a Gaussian\ndistribution given a finite number of observations. We study the case where\neach observation is a set of data points in the unit ball of a Hilbert space\n$\\mathcal{H}$. We propose a novel algorithm that requires only a polynomial\namount of data to learn the mean and covariance of the Gaussian. Our approach\ninvolves a novel method of constructing a low-rank approximation to the\ncovariance matrix of the observations. We prove that the approximation error\nconverges to zero as the number of observations goes to infinity, and we\ndemonstrate theoretically that the algorithm converges to the true mean and\ncovariance. We also provide an implementation of our algorithm in C++.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16883116883116883,
          "p": 0.19696969696969696,
          "f": 0.18181817684776777
        },
        "rouge-2": {
          "r": 0.01020408163265306,
          "p": 0.009433962264150943,
          "f": 0.009803916576319342
        },
        "rouge-l": {
          "r": 0.11688311688311688,
          "p": 0.13636363636363635,
          "f": 0.1258741209037119
        }
      }
    },
    {
      "paper_id": "physics.acc-ph.physics/acc-ph/2503.09665v1",
      "true_abstract": "Optimizing accelerator control is a critical challenge in experimental\nparticle physics, requiring significant manual effort and resource expenditure.\nTraditional tuning methods are often time-consuming and reliant on expert\ninput, highlighting the need for more efficient approaches. This study aims to\ncreate a simulation-based framework integrated with Reinforcement Learning (RL)\nto address these challenges. Using \\texttt{Elegant} as the simulation backend,\nwe developed a Python wrapper that simplifies the interaction between RL\nalgorithms and accelerator simulations, enabling seamless input management,\nsimulation execution, and output analysis.\n  The proposed RL framework acts as a co-pilot for physicists, offering\nintelligent suggestions to enhance beamline performance, reduce tuning time,\nand improve operational efficiency. As a proof of concept, we demonstrate the\napplication of our RL approach to an accelerator control problem and highlight\nthe improvements in efficiency and performance achieved through our\nmethodology. We discuss how the integration of simulation tools with a\nPython-based RL framework provides a powerful resource for the accelerator\nphysics community, showcasing the potential of machine learning in optimizing\ncomplex physical systems.",
      "generated_abstract": "erturbative effect of the QCD vacuum polarization (VP) on the\nnonlinear electrodynamics (NLE) of charged particles has been recently\ninvestigated. In this work, we extend our previous analysis by investigating the\nVP effect on the electrodynamics of charged particles in the framework of the\nKotera-Takahashi (KT) model. We focus on the longitudinal polarization\ndistribution of charged particles, which has not been investigated in the\nliterature so far. We find that the longitudinal polarization distribution\nexhibits a non-monotonic behavior with respect to the electric field and\ntemperature. This result implies that the QCD VP affects the longitudinal\npolarization distribution of charged particles. We further discuss the\ninterpretation of our results in the context of the KT model and the\nnon-perturbative QCD",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13008130081300814,
          "p": 0.23529411764705882,
          "f": 0.16753926243030634
        },
        "rouge-2": {
          "r": 0.005988023952095809,
          "p": 0.010101010101010102,
          "f": 0.007518792319240849
        },
        "rouge-l": {
          "r": 0.13008130081300814,
          "p": 0.23529411764705882,
          "f": 0.16753926243030634
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2408.00942v2",
      "true_abstract": "Composition is a powerful principle for systems biology, focused on the\ninterfaces, interconnections, and orchestration of distributed processes to\nenable integrative multiscale simulations. Whereas traditional models focus on\nthe structure or dynamics of specific subsystems in controlled conditions,\ncompositional systems biology aims to connect these models, asking critical\nquestions about the space between models: What variables should a submodel\nexpose through its interface? How do coupled models connect and translate\nacross scales? How do domain-specific models connect across biological and\nphysical disciplines to drive the synthesis of new knowledge? This approach\nrequires robust software to integrate diverse datasets and submodels, providing\nresearchers with tools to flexibly recombine, iteratively refine, and\ncollaboratively expand their models. This article offers a comprehensive\nframework to support this vision, including: a conceptual and graphical\nframework to define interfaces and composition patterns; standardized schemas\nthat facilitate modular data and model assembly; biological templates that\nintegrate detailed submodels that connect molecular processes to the emergence\nof the cellular interface; and user-friendly software interfaces that empower\nresearch communities to construct and improve multiscale models of cellular\nsystems. By addressing these needs, compositional systems biology will foster a\nunified and scalable approach to understanding complex cellular systems.",
      "generated_abstract": "The field of evolutionary biology has evolved to incorporate the concept of\ngene flow, which allows for the transfer of genetic information from one\nindividual to another. In this article, we discuss the role of gene flow in\nevolution and how it has changed over the years. We provide a historical\nperspective on the development of the concept of gene flow and how it has been\nused to study the evolution of genes and populations. We also review the\ncurrent state of the art in the use of gene flow in evolutionary biology,\nhighlighting the challenges and opportunities for future research. By\nunderstanding the history and development of the concept of gene flow, we\nexplore its role in shaping the evolution of genes and populations and how it\ncan be used to understand the evolution of organisms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11194029850746269,
          "p": 0.20833333333333334,
          "f": 0.14563106341408252
        },
        "rouge-2": {
          "r": 0.010582010582010581,
          "p": 0.018518518518518517,
          "f": 0.013468008839915883
        },
        "rouge-l": {
          "r": 0.1044776119402985,
          "p": 0.19444444444444445,
          "f": 0.13592232555000489
        }
      }
    },
    {
      "paper_id": "math.NA.cs/NA/2503.10196v1",
      "true_abstract": "In this paper, we present an error estimate for the filtered Lie splitting\nscheme applied to the Zakharov system, characterized by solutions exhibiting\nvery low regularity across all dimensions. Our findings are derived from the\napplication of multilinear estimates established within the framework of\ndiscrete Bourgain spaces. Specifically, we demonstrate that when the solution\n$(E,z,z_t) \\in H^{s+r+1/2}\\times H^{s+r}\\times H^{s+r-1}$, the error in\n$H^{r+1/2}\\times H^{r}\\times H^{r-1}$ is $\\mathcal{O}(\\tau^{s/2})$ for\n$s\\in(0,2]$, where $r=\\max(0,\\frac d2-1)$. To the best of our knowledge, this\nrepresents the first explicit error estimate for the splitting method based on\nthe original Zakharov system, as well as the first instance where low\nregularity error estimates for coupled equations have been considered within\nthe Bourgain framework. Furthermore, numerical experiments confirm the validity\nof our theoretical results.",
      "generated_abstract": "We consider the problem of estimating the mean and covariance of a joint\ndistribution of $n$ random variables from $n$ observations. For this we propose\na novel method based on the eigen-decomposition of the joint distribution. We\nestablish the consistency of the proposed estimator, and show that it is also\nasymptotically normal. We apply the method to study the joint distribution of\nthe number of successes and failures of an $n$-symbol binary linear channel,\nwhich is a standard benchmark for evaluating digital signal processing\nalgorithms. The results demonstrate that the proposed method yields a more\nefficient estimator than existing methods, and can provide a better estimate\nof the covariance matrix, which can be used in other applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19101123595505617,
          "p": 0.22077922077922077,
          "f": 0.20481927213456247
        },
        "rouge-2": {
          "r": 0.03418803418803419,
          "p": 0.037037037037037035,
          "f": 0.035555550563556255
        },
        "rouge-l": {
          "r": 0.14606741573033707,
          "p": 0.16883116883116883,
          "f": 0.15662650105022516
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.09435v1",
      "true_abstract": "In this paper, we consider the dynamic oscillation in the Cournot oligopoly\nmodel, which involves multiple firms producing homogeneous products. To explore\nthe oscillation under the updates of best response strategies, we focus on the\nlinear price functions. In this setting, we establish the existence of\noscillations. In particular, we show that for the scenario of different costs\namong firms, the best response converges to either a unique equilibrium or a\ntwo-period oscillation. We further characterize the oscillations and propose\nlinear-time algorithms for finding all types of two-period oscillations. To the\nbest of our knowledge, our work is the first step toward fully analyzing the\nperiodic oscillation in the Cournot oligopoly model.",
      "generated_abstract": "uce a general framework for analyzing the effects of interventions\nthat induce learning. The framework is based on the notion of a\nnonparametric test statistic, and allows for a wide range of learning\nmechanisms. We show that the expected value of this test statistic can be\nderived using only conditional moment conditions, and we demonstrate its\napplicability to a wide range of learning mechanisms. We apply our framework to\nstudy the effect of various forms of interventions on the welfare of a\npopulation, including randomized trials, fixed effects treatment effects,\nperformance evaluation, and the treatment of a large number of people. We\nanalyze the impact of various interventions on the expected outcome, the\nexpected outcome minus a surrogate outcome, and the average treatment effect on\nthe outcome. We show that the expected value of the test statistic can be\nderived using only conditional",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20270270270270271,
          "p": 0.2112676056338028,
          "f": 0.20689654672627839
        },
        "rouge-2": {
          "r": 0.028846153846153848,
          "p": 0.026785714285714284,
          "f": 0.027777772784637386
        },
        "rouge-l": {
          "r": 0.20270270270270271,
          "p": 0.2112676056338028,
          "f": 0.20689654672627839
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.08732v1",
      "true_abstract": "Background: Circadian desynchrony characterized by the misalignment between\nan individual's internal biological rhythms and external environmental cues,\nsignificantly affects various physiological processes and health outcomes.\nQuantifying circadian desynchrony often requires prolonged and frequent\nmonitoring, and currently, an easy tool for this purpose is missing.\nAdditionally, its association with the incidence of delirium has not been\nclearly explored. Methods: A prospective observational study was carried out in\nintensive care units (ICU) of a tertiary hospital. Circadian transcriptomics of\nblood monocytes from 86 individuals were collected on two consecutive days,\nalthough a second sample could not be obtained from all participants. Using two\npublic datasets comprised of healthy volunteers, we replicated a model for\ndetermining internal circadian time. We developed an approach to quantify\ncircadian desynchrony by comparing internal circadian time and external blood\ncollection time. We applied the model and quantified circadian desynchrony\nindex among ICU patients, and investigated its association with the incidence\nof delirium. Results: The replicated model for determining internal circadian\ntime achieved comparable high accuracy. The quantified circadian desynchrony\nindex was significantly higher among critically ill ICU patients compared to\nhealthy subjects, with values of 10.03 hours vs 2.50-2.95 hours (p < 0.001).\nMost ICU patients had a circadian desynchrony index greater than 9 hours.\nAdditionally, the index was lower in patients whose blood samples were drawn\nafter 3pm, with values of 5.00 hours compared to 10.01-10.90 hours in other\ngroups (p < 0.001)...",
      "generated_abstract": "sis of the dynamics of a single-cell population of cells can be\ninterpreted as a system of linear equations with the dynamics of the population\ndepending on the values of the variables of the system. For instance, the\ndynamics of the number of cells in a population can be interpreted as a\nsystem of linear equations with the dynamics of the population depending on the\nvalues of the variables of the system. It is a well-known fact that the\ndynamics of the number of cells in a population depends on the number of cells\nat the beginning of the simulation. In this work, we show that the same\nrelationship is observed in a system of coupled linear equations that describe\nthe dynamics of a population of cells. We show that the dynamics of the number\nof cells in a population depends on the values of the variables of the system\nand on the initial conditions of the population. We show that the dynamics of\nthe number",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08496732026143791,
          "p": 0.2765957446808511,
          "f": 0.1299999964045001
        },
        "rouge-2": {
          "r": 0.014018691588785047,
          "p": 0.039473684210526314,
          "f": 0.020689651304638058
        },
        "rouge-l": {
          "r": 0.0784313725490196,
          "p": 0.2553191489361702,
          "f": 0.1199999964045001
        }
      }
    },
    {
      "paper_id": "hep-ex.hep-ex/2503.09392v1",
      "true_abstract": "The muon anomalous magnetic moment, $a_\\mu=\\frac{g-2}{2}$, is a low-energy\nobservable which can be both measured and computed to high precision, making it\na sensitive test of the Standard Model and a probe for new physics. This\nanomaly was measured with a precision of $0.20$~parts per million (ppm) by the\nFermilab's Muon g-2 (E989) experiment. The final goal of the E989 experiment is\nto reach a precision of $0.14$~ppm. The experiment is based on the measurement\nof the muon spin anomalous precession frequency, $\\omega_a$, based on the\narrival time distribution of high-energy decay positrons observed by 24\nelectromagnetic calorimeters, placed around the inner circumference of a $14$~m\ndiameter storage ring, and on the precise knowledge of the storage ring\nmagnetic field and of the beam time and space distribution. Achieving this\nlevel of precision requires strict control over systematics, which is ensured\nthrough several diagnostic devices. At the accelerator level, these devices\nmonitor the quality of the injected beam (e.g., verifying that it has the\ncorrect momentum), while at the detector level, they track both the magnetic\nfield and the gain of the calorimeters. In this work the devices and techniques\nused by the E989 experiment will be presented.",
      "generated_abstract": "experiment has published a new measurement of the ratio of the\ndecay branching fractions of the $B_c\\to J/\\psi K_S$ and $B_c\\to J/\\psi\n\\phi$ decays, $R_{J/\\psi K_S}=0.216\\pm0.002$ and $R_{J/\\psi \\phi}=0.213\\pm0.002$.\nThe $R_{J/\\psi K_S}$ value is in agreement with previous measurements and\nconsistent with the SM prediction. The $R_{J/\\psi \\phi}$ value is lower than\nexpected, indicating the presence of an anomalous $B_c\\to J/\\psi \\phi$ decay.\nTheoretical predictions of the $R_{J/\\psi \\phi}$ value are presented and\ncompared with the experimental value. The new LHCb measurement",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.112,
          "p": 0.27450980392156865,
          "f": 0.15909090497481934
        },
        "rouge-2": {
          "r": 0.01092896174863388,
          "p": 0.02702702702702703,
          "f": 0.015564198234039445
        },
        "rouge-l": {
          "r": 0.104,
          "p": 0.2549019607843137,
          "f": 0.14772726861118296
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SP/2503.08726v1",
      "true_abstract": "Traditional single-modality sensing faces limitations in accuracy and\ncapability, and its decoupled implementation with communication systems\nincreases latency in bandwidth-constrained environments. Additionally,\nsingle-task-oriented sensing systems fail to address users' diverse demands. To\novercome these challenges, we propose a semantic-driven integrated multimodal\nsensing and communication (SIMAC) framework. This framework leverages a joint\nsource-channel coding architecture to achieve simultaneous sensing decoding and\ntransmission of sensing results. Specifically, SIMAC first introduces a\nmultimodal semantic fusion (MSF) network, which employs two extractors to\nextract semantic information from radar signals and images, respectively. MSF\nthen applies cross-attention mechanisms to fuse these unimodal features and\ngenerate multimodal semantic representations. Secondly, we present a large\nlanguage model (LLM)-based semantic encoder (LSE), where relevant communication\nparameters and multimodal semantics are mapped into a unified latent space and\ninput to the LLM, enabling channel-adaptive semantic encoding. Thirdly, a\ntask-oriented sensing semantic decoder (SSD) is proposed, in which different\ndecoded heads are designed according to the specific needs of tasks.\nSimultaneously, a multi-task learning strategy is introduced to train the SIMAC\nframework, achieving diverse sensing services. Finally, experimental\nsimulations demonstrate that the proposed framework achieves diverse sensing\nservices and higher accuracy.",
      "generated_abstract": "The 2D optical flow field is fundamental to the understanding and manipulation\nof images, but its computation remains computationally expensive due to the\nintensity of its mathematical formulation. In this work, we introduce a novel\napproach for the 2D optical flow estimation with a sparsity-based framework,\nwhich combines the traditional optimization approach with a sparse grid\napproach. The proposed method is based on a sparsity-based approach, which\nachieves significant computational efficiency. Moreover, the proposed approach\nis based on a sparse grid approach, which facilitates a more efficient\ncomputational process. In addition, the proposed method provides a more\neffective and efficient way to solve the 2D optical flow estimation problem.\nExperiments show that the proposed method has superior performance compared to\nthe state-of-the-art methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.20833333333333334,
          "f": 0.1449275316950222
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.0297029702970297,
          "f": 0.020905918783523922
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.20833333333333334,
          "f": 0.1449275316950222
        }
      }
    },
    {
      "paper_id": "cs.GR.cs/GR/2503.08929v1",
      "true_abstract": "Accurate and efficient 3D mapping of large-scale outdoor environments from\nLiDAR measurements is a fundamental challenge in robotics, particularly towards\nensuring smooth and artifact-free surface reconstructions. Although the\nstate-of-the-art methods focus on memory-efficient neural representations for\nhigh-fidelity surface generation, they often fail to produce artifact-free\nmanifolds, with artifacts arising due to noisy and sparse inputs. To address\nthis issue, we frame surface mapping as a physics-informed energy optimization\nproblem, enforcing surface smoothness by optimizing an energy functional that\npenalizes sharp surface ridges. Specifically, we propose a deep learning based\napproach that learns the signed distance field (SDF) of the surface manifold\nfrom raw LiDAR point clouds using a physics-informed loss function that\noptimizes the $L_2$-Hessian energy of the surface. Our learning framework\nincludes a hierarchical octree based input feature encoding and a multi-scale\nneural network to iteratively refine the signed distance field at different\nscales of resolution. Lastly, we introduce a test-time refinement strategy to\ncorrect topological inconsistencies and edge distortions that can arise in the\ngenerated mesh. We propose a \\texttt{CUDA}-accelerated least-squares\noptimization that locally adjusts vertex positions to enforce\nfeature-preserving smoothing. We evaluate our approach on large-scale outdoor\ndatasets and demonstrate that our approach outperforms current state-of-the-art\nmethods in terms of improved accuracy and smoothness. Our code is available at\n\\href{https://github.com/HrishikeshVish/HessianForge/}{https://github.com/HrishikeshVish/HessianForge/}",
      "generated_abstract": "e a novel approach to 3D human action recognition, leveraging\nself-supervised pretraining on large video datasets to learn action representations\nthat are invariant to both spatial and temporal context. Our approach\nconsists of two components: a self-supervised pretraining framework that\ncomprises a self-supervised video model and a pretraining-based video\nfeature-extraction method, and a contrastive learning-based action recognition\nmodel. The self-supervised pretraining framework is inspired by the\nself-supervised learning paradigm of video self-attention, which has been\nsuccessfully applied to video representation learning. To capture the\nspatial-temporal interactions in action videos, we design a pretraining-based\nvideo feature-extraction method that leverages self-supervised pretraining to\nlearn temporal and spatial features that are robust to temporal context. To\nfinally achieve action recognition, we design a contrast",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1310344827586207,
          "p": 0.2753623188405797,
          "f": 0.1775700890885668
        },
        "rouge-2": {
          "r": 0.004901960784313725,
          "p": 0.009523809523809525,
          "f": 0.006472487422631696
        },
        "rouge-l": {
          "r": 0.1103448275862069,
          "p": 0.2318840579710145,
          "f": 0.1495327059109967
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.08062v1",
      "true_abstract": "Orthogonal frequency division multiplexing (OFDM), which has been the\ndominating waveform for contemporary wireless communications, is also regarded\nas a competitive candidate for future integrated sensing and communication\n(ISAC) systems. Existing works on OFDM-ISAC usually assume that the maximum\nsensing range should be limited by the cyclic prefix (CP) length since\ninter-symbol interference (ISI) and inter-carrier interference (ICI) should be\navoided. However, in this paper, we provide rigorous analysis to reveal that\nthe random data embedded in OFDM-ISAC signal can actually act as a free ``mask\"\nfor ISI, which makes ISI/ICI random and hence greatly attenuated after radar\nsignal processing. The derived signal-to-interference-plus-noise ratio (SINR)\nin the range profile demonstrates that the maximum sensing range of OFDM-ISAC\ncan greatly exceed the ISI-free distance that is limited by the CP length,\nwhich is validated by simulation results. To further mitigate power degradation\nfor long-range targets, a novel sliding window sensing method is proposed,\nwhich iteratively detects and cancels short-range targets before shifting the\ndetection window. The shifted detection window can effectively compensate the\npower degradation due to insufficient CP length for long-range targets. Such\nresults provide valuable guidance for the CP length design in OFDM-ISAC\nsystems.",
      "generated_abstract": "ng demand for high-resolution sensing and communication has\nmotivated the development of ultra-wideband (UWB) systems, which offer\nsignificantly higher resolution in the frequency domain. However, the\nintensity-based sensing of UWB signals presents significant challenges,\nparticularly for non-uniformly distributed sources, which hinders the accurate\ndetection of the target signal. To address this issue, we propose a novel\nnonlinear detection method based on the nonlinear Schrodinger equation.\nNumerical simulations are conducted to evaluate the performance of the proposed\nmethod and compare it with the existing methods, demonstrating its superiority\nover them in terms of detection performance, accuracy, and computational\ncomplexity. The effectiveness of the proposed method is also verified by\nsimulating the detection of two-dimensional (2D) UWB signals. Finally, the\nresults show that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21428571428571427,
          "p": 0.30337078651685395,
          "f": 0.2511627858457546
        },
        "rouge-2": {
          "r": 0.0446927374301676,
          "p": 0.06956521739130435,
          "f": 0.054421763944421726
        },
        "rouge-l": {
          "r": 0.1984126984126984,
          "p": 0.2808988764044944,
          "f": 0.23255813468296388
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2501.13135v1",
      "true_abstract": "The complexity of human biology and its intricate systems holds immense\npotential for advancing human health, disease treatment, and scientific\ndiscovery. However, traditional manual methods for studying biological\ninteractions are often constrained by the sheer volume and complexity of\nbiological data. Artificial Intelligence (AI), with its proven ability to\nanalyze vast datasets, offers a transformative approach to addressing these\nchallenges. This paper explores the intersection of AI and microscopy in life\nsciences, emphasizing their potential applications and associated challenges.\nWe provide a detailed review of how various biological systems can benefit from\nAI, highlighting the types of data and labeling requirements unique to this\ndomain. Particular attention is given to microscopy data, exploring the\nspecific AI techniques required to process and interpret this information. By\naddressing challenges such as data heterogeneity and annotation scarcity, we\noutline potential solutions and emerging trends in the field. Written primarily\nfrom an AI perspective, this paper aims to serve as a valuable resource for\nresearchers working at the intersection of AI, microscopy, and biology. It\nsummarizes current advancements, key insights, and open problems, fostering an\nunderstanding that encourages interdisciplinary collaborations. By offering a\ncomprehensive yet concise synthesis of the field, this paper aspires to\ncatalyze innovation, promote cross-disciplinary engagement, and accelerate the\nadoption of AI in life science research.",
      "generated_abstract": "r is devoted to the analysis of the evolutionary dynamics of the\nsystem of two populations, each of which consists of a single individual, with\ntheir interaction taking place only between individuals of the same population.\nThe population with the smaller size is initially more stable than the\npopulation with the larger size, as long as the two populations are initially\nclose in size. As the two populations become more distant in size, the\npopulation with the smaller size becomes more stable, while the population with\nthe larger size becomes more stable than the population with the smaller size\nunder a similar initial condition. This behavior is qualitatively different\nfrom the case of a single population. The analysis shows that the stability\nof the population with the smaller size depends on the initial condition, while\nthe stability of the population with the larger size depends on the initial\npopulation size. The stability of the population with the smaller size depends\non the initial population size and the initial",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10204081632653061,
          "p": 0.22727272727272727,
          "f": 0.14084506614560613
        },
        "rouge-2": {
          "r": 0.004807692307692308,
          "p": 0.009433962264150943,
          "f": 0.006369422279203108
        },
        "rouge-l": {
          "r": 0.09523809523809523,
          "p": 0.21212121212121213,
          "f": 0.1314553947841038
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.18182v1",
      "true_abstract": "Blind source separation (BSS) refers to the process of recovering multiple\nsource signals from observations recorded by an array of sensors. Common\napproaches to BSS, including independent vector analysis (IVA), and independent\nlow-rank matrix analysis (ILRMA), typically rely on second-order models to\ncapture the statistical independence of source signals for separation. However,\nthese methods generally do not account for the implicit structural information\nacross frequency bands, which may lead to model mismatches between the assumed\nsource distributions and the distributions of the separated source signals\nestimated from the observed mixtures. To tackle these limitations, this paper\nshows that conventional approaches such as IVA and ILRMA can easily be\nleveraged by the Sinkhorn divergence, incorporating an optimal transport (OT)\nframework to adaptively correct source variance estimates. This allows for the\nrecovery of the source distribution while modeling the inter-band signal\ndependence and reallocating source power across bands. As a result, enhanced\nversions of these algorithms are developed, integrating a Sinkhorn iterative\nscheme into their standard implementations. Extensive simulations demonstrate\nthat the proposed methods consistently enhance BSS performance.",
      "generated_abstract": "cale music generation task requires generating a large amount of\nmusic from a limited set of audio and text prompts. While existing music\ngeneration models exhibit strong performance on the standard benchmarks, they\nlack the ability to generate diverse and high-quality music, particularly when\nproviding only audio prompts. In this paper, we introduce MusicMixer, a music\ngeneration model that learns to generate diverse and high-quality music from\naudio and text prompts. The core of MusicMixer is a novel mixer module that\ncombines music-specific and text-prompt-specific features to provide a more\nrepresentative audio-text interaction. Specifically, the mixer module selects\naudio features that are most relevant to the text prompt, and then leverages\nthem to enhance the generated music. We also propose a new dataset,\nMUSIC-GPT, that integrates text prompts with audio",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11811023622047244,
          "p": 0.18072289156626506,
          "f": 0.14285713807664416
        },
        "rouge-2": {
          "r": 0.005847953216374269,
          "p": 0.008547008547008548,
          "f": 0.006944439620229045
        },
        "rouge-l": {
          "r": 0.11023622047244094,
          "p": 0.1686746987951807,
          "f": 0.1333333285528346
        }
      }
    },
    {
      "paper_id": "physics.ins-det.physics/ins-det/2503.10098v1",
      "true_abstract": "The linear response of CsI(Tl) crystals to $\\gamma$-rays plays a crucial role\nin their calibration, as any deviation from linearity can introduce systematic\nerrors not negligible in the measurement of $\\gamma$ energy spectra,\nparticularly at high energies. In this study, the responses of CsI(Tl) crystals\nto high-energy photons up to 20 MeV are investigated using quasi monochromatic\n$\\gamma$ beam provided by the Shanghai Laser Electron Gamma Source. The spectra\nare folded using a detector filter implemented by Geant4. Both quadratic and\nlinear fits to six energy points are used to assess the linearity of the\nCsI(Tl) detector. The results demonstrate that the difference between the\nlinear and non-linear fits is at the level of 4\\%. Applying these findings to\nthe $\\gamma$ hodoscope of the Compact Spectrometer for Heavy Ion Experiment\n(CSHINE), the potential systematic uncertainties caused by CsI(Tl)\nnon-linearity are evaluated. This work provides a comprehensive calibration\nmethodology for employing CsI(Tl) crystal to detect high energy $\\gamma$-rays.",
      "generated_abstract": "n of the Future Circular Collider (FCC-ee) requires a robust, stable\nand fully integrated electronics infrastructure, with the ability to handle\nextreme energy conditions, large beam currents, and extreme sensitivity to\nelectronics noise. The FCC-ee electronics system consists of three major\ncomponents: the trigger, the electronics control unit (ECU), and the\nelectronics boards. The trigger and ECU are designed to operate at extreme\nconditions, while the electronics boards are designed to operate at low\nelectronics noise. This paper describes the design and integration of the\ntrigger electronics board, which is the heart of the FCC-ee trigger system.\nThe design of the FCC-ee electronics system is based on the current state of\nthe art of electronics development, with particular emphasis on the\nintegration of the electronics board. The paper describes the key challeng",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11428571428571428,
          "p": 0.17647058823529413,
          "f": 0.13872831892813006
        },
        "rouge-2": {
          "r": 0.006578947368421052,
          "p": 0.00909090909090909,
          "f": 0.007633582914751666
        },
        "rouge-l": {
          "r": 0.11428571428571428,
          "p": 0.17647058823529413,
          "f": 0.13872831892813006
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2411.10726v2",
      "true_abstract": "We study an optimal execution problem in the infinite horizon setup. Our\nfinancial market is given by the Black-Scholes model with a linear price\nimpact. The main novelty of the current note is that we study the constrained\ncase where the number of shares and the selling rate are non-negative\nprocesses. For this case we give a complete characterization of the value and\nthe optimal control via a solution of a non-linear ordinary differential\nequation (ODE). Furthermore, we provide an example where the non-linear ODE can\nbe solved explicitly. Our approach is purely probabilistic.",
      "generated_abstract": "aper, we introduce a novel framework for constructing robust\ntraditional mean-variance portfolios (MVPFs) that incorporate multi-asset\nrisk-aware factors. We first define a new concept of risk-aware factor\nportfolios (RAPs) as a special case of traditional MVPFs, which are then\nfurther extended to incorporate a broader range of risk-aware factors, such as\nmarket efficiency, volatility, and systemic risk, in addition to traditional\nrisk factors such as price and momentum. We then introduce a portfolio\nconstruction framework based on a new RAP that exploits the synergy between\nmarket efficiency and systemic risk to enhance the robustness of traditional\nMVPFs. To validate the proposed framework, we apply it to the U.S. equity and\nforeign equity markets and compare it with traditional MVPF",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19117647058823528,
          "p": 0.16666666666666666,
          "f": 0.17808218680427862
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.19117647058823528,
          "p": 0.16666666666666666,
          "f": 0.17808218680427862
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.16298v1",
      "true_abstract": "Speech foundation models have demonstrated exceptional capabilities in\nspeech-related tasks. Nevertheless, these models often struggle with non-verbal\naudio data, such as vocalizations, baby crying, etc., which are critical for\nvarious real-world applications. Audio foundation models well handle non-speech\ndata but also fail to capture the nuanced features of non-verbal human sounds.\nIn this work, we aim to overcome the above shortcoming and propose a novel\nfoundation model, termed voc2vec, specifically designed for non-verbal human\ndata leveraging exclusively open-source non-verbal audio datasets. We employ a\ncollection of 10 datasets covering around 125 hours of non-verbal audio.\nExperimental results prove that voc2vec is effective in non-verbal vocalization\nclassification, and it outperforms conventional speech and audio foundation\nmodels. Moreover, voc2vec consistently outperforms strong baselines, namely\nOpenSmile and emotion2vec, on six different benchmark datasets. To the best of\nthe authors' knowledge, voc2vec is the first universal representation model for\nvocalization tasks.",
      "generated_abstract": "vancements in multi-modal deep learning have shown promise in\nreconstructing and analyzing complex speech signals, particularly when\ncombined with ground-truth labels. However, most existing methods rely on\nexisting audio-visual pairs, limiting their generalizability and scalability. In\nthis paper, we propose SAVE (Speech Audio Visual Embedding), a novel framework\nthat leverages unlabeled audio-visual pairs to enhance the robustness and\ngeneralizability of multi-modal speech analysis. SAVE first converts audio and\nvisual data into vectors using a pre-trained transformer model, which enables\nthe model to understand the relationships among audio and visual signals. Then,\nit introduces a novel audio-visual loss that promotes alignment between\nembedded audio and visual signals, effectively enhancing the model's\nunderstanding of audio-visual data. Furthermore, SAVE incorporates",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22018348623853212,
          "p": 0.27586206896551724,
          "f": 0.24489795424666816
        },
        "rouge-2": {
          "r": 0.014285714285714285,
          "p": 0.018018018018018018,
          "f": 0.015936250046826177
        },
        "rouge-l": {
          "r": 0.22018348623853212,
          "p": 0.27586206896551724,
          "f": 0.24489795424666816
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.10116v1",
      "true_abstract": "The knowledge transfer from 3D printing technology paved the way for\nunlocking the innovative potential of 3D Food Printing (3DFP) technology.\nHowever, this technology-oriented approach neglects userderived issues that\ncould be addressed with advancements in 3DFP technology. To explore potential\nnew features and application areas for 3DFP technology, we created the Mobile\nFood Printer (MFP) prototype. We collected insights from novice chefs for MFP\nin the restaurant context through four online focus group sessions (N=12). Our\nresults revealed how MFP can be applied in the current kitchen routines\n(preparation, serving, and eating) and introduce novel dining experiences. We\ndiscuss our learnings under two themes: 1) dealing with the kitchen rush and 2)\nstreamlining workflows in the kitchen. The opportunities we present in this\nstudy act as a starting point for HCI and HFI researchers and encourage them to\nimplement mobility in 3DFP with a useroriented lens. We further provide a\nground for future research to uncover potentials for advancing 3DFP technology.",
      "generated_abstract": "t a new method for training generative models in the time domain.\nGenerative models are designed to synthesize new data in response to a\nspecified set of input-output pairs. The current state of the art in\ngenerative model training relies on data augmentation, where the model is\ntrained with synthetic data generated through transformations of the input\ndata. However, the use of data augmentation in the time domain can be\nsignificantly more costly than in the spatial domain, where transformations\nare performed on the original data. In this work, we present a method for\ntraining generative models in the time domain, where transformations are\nperformed on the time domain, rather than the spatial domain. Our method\nuses a variant of the transformer architecture as a generative model, and we\ndemonstrate that it can be trained in a more efficient manner in the time\ndomain than in the spatial domain. Additionally,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1810344827586207,
          "p": 0.28378378378378377,
          "f": 0.22105262682326882
        },
        "rouge-2": {
          "r": 0.025806451612903226,
          "p": 0.03361344537815126,
          "f": 0.029197075378284164
        },
        "rouge-l": {
          "r": 0.1724137931034483,
          "p": 0.2702702702702703,
          "f": 0.21052631103379513
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/TH/2503.04876v1",
      "true_abstract": "Given two populations from which independent binary observations are taken\nwith parameters $p_1$ and $p_2$ respectively, estimators are proposed for the\nrelative risk $p_1/p_2$, the odds ratio $p_1(1-p_2)/(p_2(1-p_1))$ and their\nlogarithms. The estimators guarantee that the relative mean-square error, or\nthe mean-square error for the logarithmic versions, is less than a target value\nfor any $p_1, p_2 \\in (0,1)$, and the ratio of average sample sizes from the\ntwo populations is close to a prescribed value. The estimators can also be used\nwith group sampling, whereby samples are taken in batches of fixed size from\nthe two populations. The efficiency of the estimators with respect to the\nCram\\'er-Rao bound is good, and in particular it is close to $1$ for small\nvalues of the target error.",
      "generated_abstract": "In the context of Bayesian statistical inference, we consider the problem of\nestimating a functional parameter of a linear functional equation. We assume\nthat the functional equation is known, but the functional parameter is unknown.\nThis problem arises in the context of functional data analysis and functional\ninverse problems. We propose a novel method for estimating the functional\nparameter based on a generalized linear model and a functional likelihood\nfunction. We show that the proposed method is consistent and asymptotically\nnormal. Moreover, we derive the asymptotic covariance matrix of the proposed\nestimator, and we prove that the estimator is unbiased and asymptotically\nnormal. We apply the proposed method to a functional data analysis problem,\nwhere the functional data are related to the functional parameter. The method\nprovides a robust method for the estimation of the functional parameter in\nsuch a situation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15584415584415584,
          "p": 0.1791044776119403,
          "f": 0.16666666169077946
        },
        "rouge-2": {
          "r": 0.043859649122807015,
          "p": 0.043859649122807015,
          "f": 0.04385964412280759
        },
        "rouge-l": {
          "r": 0.15584415584415584,
          "p": 0.1791044776119403,
          "f": 0.16666666169077946
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.15376v1",
      "true_abstract": "Land use is a critical factor in the siting of renewable energy facilities\nand is often scrutinized due to perceived conflicts with other land demands.\nMeanwhile, substantial areas are devoted to activities such as golf, which are\naccessible to only a select few and have a significant land and environmental\nfootprint. Our study shows that in countries such as the United States and the\nUnited Kingdom, far more land is allocated to golf courses than to renewable\nenergy facilities. Areas equivalent to those currently used for golf could\nsupport the installation of up to 842 GW of solar and 659 GW of wind capacity\nin the top ten countries with the most golf courses. In many of these\ncountries, this potential exceeds both current installed capacity and\nmedium-term projections. These findings underscore the untapped potential of\nrethinking land use priorities to accelerate the transition to renewable\nenergy.",
      "generated_abstract": "r examines the role of government in shaping economic outcomes in\nEthiopia, using a new dataset of administrative data on government expenditures\nand economic indicators from 1974 to 2023. We find that government spending\nis the primary driver of economic growth, accounting for 80% of the\neconomic expansion between 1974 and 2023. The spillover effects of government\nexpenditures are strongest in rural areas, where government spending is\nassociated with a 7% increase in per capita gross domestic product (GDP) in\nthe years following a government-financed infrastructure project. We find that\ngovernment spending drives growth by raising wages and consumption, reducing\nunemployment, and stimulating private sector investment. These findings\nhighlight the importance of government spending in supporting economic\ndevelopment",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1368421052631579,
          "p": 0.16455696202531644,
          "f": 0.14942528239859973
        },
        "rouge-2": {
          "r": 0.014598540145985401,
          "p": 0.018691588785046728,
          "f": 0.016393437698536815
        },
        "rouge-l": {
          "r": 0.12631578947368421,
          "p": 0.1518987341772152,
          "f": 0.13793102952503652
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08361v1",
      "true_abstract": "This technical report analyzes non-contrast CT image segmentation in computer\nvision. It revisits a proposed method, examines the background of non-contrast\nCT imaging, and highlights the significance of segmentation. The study reviews\nrepresentative methods, including convolutional-based and CNN-Transformer\nhybrid approaches, discussing their contributions, advantages, and limitations.\nThe nnUNet stands out as the state-of-the-art method across various\nsegmentation tasks. The report explores the relationship between the proposed\nmethod and existing approaches, emphasizing the role of global context modeling\nin semantic labeling and mask generation. Future directions include addressing\nthe long-tail problem, utilizing pre-trained models for medical imaging, and\nexploring self-supervised or contrastive pre-training techniques. This report\noffers insights into non-contrast CT image segmentation and potential\nadvancements in the field.",
      "generated_abstract": "aper, we introduce a novel method for non-invasive monitoring of\nnon-infected lung tissue by leveraging the non-intrusive character of magnetic\nresonance imaging (MRI). Our approach integrates MRI and a novel deep learning\nmodel for segmentation of non-infected lung tissue. This approach is\nsignificantly different from current state-of-the-art methods that leverage\nMRI alone, as it integrates a segmentation model into the learning process.\nThis integration allows us to improve segmentation accuracy while reducing\ntraining time, allowing for a more efficient process of non-invasive lung\nmonitoring. The results demonstrate the effectiveness of our approach, with\nmean Average Precision (MAP) of 0.9938, precision of 0.9959, and recall of\n0.9983. We",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.15584415584415584,
          "f": 0.14906831799081843
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11904761904761904,
          "p": 0.12987012987012986,
          "f": 0.12422359749392403
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.07692v1",
      "true_abstract": "This paper narrowly replicates Chen and Kung's 2019 paper ($The$ $Quarterly$\n$Journal$ $of$ $Economics$ 134(1): 185-226). Inspecting the data reveals that\nnearly one-third of the transactions (388,903 out of 1,208,621) are perfect\nduplicates of other rows, excluding the transaction number. Replicating the\nanalysis on the data sans-duplicates yields a slightly smaller but still\nstatistically significant princeling effect, robust across the regression\nresults. Further analysis also reveals that coefficients interpreted as the\neffect of logarithm of area actually reflect the effect of scaled values of\narea; this paper also reinterprets and contextualizes these results in light of\nthe true scaled values.",
      "generated_abstract": "r examines the effects of the COVID-19 pandemic on the U.S.\nHousing Market. Our analysis focuses on the impact of the pandemic on\nmortgage and rental rates, the supply of housing, and the demand for housing.\nWe utilize a panel data set that includes 5,624 U.S. counties and covers the\nperiod from January 2020 to December 2021. Our results suggest that the\npandemic significantly impacted mortgage and rental rates, as well as the\ndemand for housing. We find that mortgage rates increased by 25 basis points\n(bps) during the pandemic compared to pre-pandemic levels, while rental rates\nincreased by 18 bps. Furthermore, the increase in mortgage rates was significantly\nassociated with the pandemic. Our findings highlight the impact",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1506849315068493,
          "p": 0.1527777777777778,
          "f": 0.1517241329312725
        },
        "rouge-2": {
          "r": 0.03225806451612903,
          "p": 0.0297029702970297,
          "f": 0.030927830060049694
        },
        "rouge-l": {
          "r": 0.1232876712328767,
          "p": 0.125,
          "f": 0.12413792603472078
        }
      }
    },
    {
      "paper_id": "gr-qc.physics/ins-det/2503.10332v1",
      "true_abstract": "The double-pass interferometer scheme was proposed in Ref.\\,[Light Sci. Appl.\n{\\bf 7}, 11 (2018)] as the method of implementation of the quantum speed meter\nconcept in future laser gravitational-wave (GW) detectors. Later it was shown\nin Ref.\\,[Phys. Rev. D {\\bf 110}, 062006 (2024)] that it allows to implement\nthe new type of the optical spring that does not require detuning of the\ninterferometer. Here we show that both these regimes can coexist, combining the\nspeed meter type broadband sensitivity gain with the additional lows-frequency\nminimum in the quantum noise originated from the optical spring. We show that\nthe location of this minimum can be varied without affecting the core optics of\nthe interferometer, allowing to tune the quantum noise shape in real time to\nfollow the ``chirp'' GW signals.",
      "generated_abstract": "very of the B-meson, which is composed of a bottom quark and an\n$u$-$d$ quark, is a milestone in modern particle physics. It provides\nsignificant insights into the strong interaction and may be used to explore\nnew physics. In this paper, we propose a novel method to measure the\nB-meson mass, based on the angular distribution of the muon g-2. We first\nanalyze the muon g-2 data from the LHCb experiment, which includes a large\nstatistics of $B\\bar{B}$ decays. We then design and construct a novel\ncalorimeter to measure the angular distribution of the muon g-2, and show\nthat our method can measure the B-meson mass with a precision of 0.32$\\pm$0.12\nMeV. We expect that our method will be a powerful tool for measuring",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17045454545454544,
          "p": 0.18292682926829268,
          "f": 0.17647058324152262
        },
        "rouge-2": {
          "r": 0.025210084033613446,
          "p": 0.02727272727272727,
          "f": 0.02620086837016933
        },
        "rouge-l": {
          "r": 0.17045454545454544,
          "p": 0.18292682926829268,
          "f": 0.17647058324152262
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.06611v1",
      "true_abstract": "Performance and reliability analyses of autonomous vehicles (AVs) can benefit\nfrom tools that ``amplify'' small datasets to synthesize larger volumes of\nplausible samples of the AV's behavior. We consider a specific instance of this\ndata synthesis problem that addresses minimizing the AV's exposure to adverse\nenvironmental conditions during travel to a fixed goal location. The\nenvironment is characterized by a threat field, which is a strictly positive\nscalar field with higher intensities corresponding to hazardous and unfavorable\nconditions for the AV. We address the problem of synthesizing datasets of\nminimum exposure paths that resemble a training dataset of such paths. The main\ncontribution of this paper is an inverse reinforcement learning (IRL) model to\nsolve this problem. We consider time-invariant (static) as well as time-varying\n(dynamic) threat fields. We find that the proposed IRL model provides excellent\nperformance in synthesizing paths from initial conditions not seen in the\ntraining dataset, when the threat field is the same as that used for training.\nFurthermore, we evaluate model performance on unseen threat fields and find low\nerror in that case as well. Finally, we demonstrate the model's ability to\nsynthesize distinct datasets when trained on different datasets with distinct\ncharacteristics.",
      "generated_abstract": "This paper studies the problem of multi-agent system (MAS) synchronization\nunder the assumption of distributed energy resources (DERs). First, we present a\nrecent formulation of the MAS synchronization problem with distributed energy\nresources (DERs). Then, we introduce a novel MAS synchronization model with\nmulti-energy storage (MES) nodes, which can effectively model the energy\nstorage systems in the real world. The MES model is further extended to\ninclude a new type of energy storage, namely, the energy storage systems\n(ESS) node. In this paper, we establish the existence and uniqueness of the\nsolution to the extended MES model with ESS nodes. Moreover, we propose an\noptimization strategy to solve the extended MES model with ESS nodes. Finally,\nwe prove that the proposed MES model with ESS nodes is equivalent to the\noriginal MAS synchronization problem with distributed energy resources.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.175,
          "p": 0.28378378378378377,
          "f": 0.21649484064193866
        },
        "rouge-2": {
          "r": 0.041884816753926704,
          "p": 0.07272727272727272,
          "f": 0.0531561415414845
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.2702702702702703,
          "f": 0.2061855622914232
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/SY/2503.09892v1",
      "true_abstract": "As inverter-based resources (IBRs) penetrate power systems, the dynamics\nbecome more complex, exhibiting multiple timescales, including electromagnetic\ntransient (EMT) dynamics of power electronic controllers and electromechanical\ndynamics of synchronous generators. Consequently, the power system model\nbecomes highly stiff, posing a challenge for efficient simulation using\nexisting methods that focus on dynamics within a single timescale. This paper\nproposes a Heterogeneous Multiscale Method for highly efficient multi-timescale\nsimulation of a power system represented by its EMT model. The new method\nalternates between the microscopic EMT model of the system and an automatically\nreduced macroscopic model, varying the step size accordingly to achieve\nsignificant acceleration while maintaining accuracy in both fast and slow\ndynamics of interests. It also incorporates a semi-analytical solution method\nto enable a more adaptive variable-step mechanism. The new simulation method is\nillustrated using a two-area system and is then tested on a detailed EMT model\nof the IEEE 39-bus system.",
      "generated_abstract": "This paper proposes a novel control strategy for a multibody robotic\nsystem with a nonlinear mass matrix. The control scheme is based on a\ndiscrete-time Lyapunov function approach, which is used to monitor the\nsystem's dynamic performance and to design control actions. The developed\ncontroller provides a robust and stable solution for the multibody robotic\nsystem. The proposed control strategy is validated through simulation\ninvestigations. The results demonstrate that the proposed approach effectively\nhandles the nonlinear mass matrix and improves the stability of the system. The\nproposed control strategy can be applied to various robotic applications,\nparticularly in nonlinear mass-spring-damper systems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.265625,
          "f": 0.20481927237044575
        },
        "rouge-2": {
          "r": 0.04225352112676056,
          "p": 0.06593406593406594,
          "f": 0.051502141162298516
        },
        "rouge-l": {
          "r": 0.1568627450980392,
          "p": 0.25,
          "f": 0.1927710795993614
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.20467v1",
      "true_abstract": "A one-shot device is a unit that operates only once, after which it is either\ndestroyed or needs to be rebuilt. For this type of device, the operational\nstatus can only be assessed at a specific inspection time, determining whether\nfailure occurred before or after it. Consequently, lifetimes are subject to\nleft- or right-censoring. One-shot devices are usually highly reliables. To\nanalyze the reliability of such products, an accelerated life test (ALT) plan\nis typically employed by subjecting the devices to increased levels of stress\nfactors, thus allowing life characteristics observed under high-stress\nconditions to be extrapolated to normal operating conditions. By accelerating\nthe degradation process, ALT significantly reduces both the time required for\ntesting and the associated experimental costs.\n  Recently, robust inferential methods have gained considerable interest in\nstatistical analysis. Among them, weighted minimum density power divergence\nestimators (WMDPDEs) are widely recognized for their robust statistical\nproperties with small loss of efficiency. In this work, robust WMDPDE and\nassociated statistical tests are developed under a log-logistic lifetime\ndistribution with multiple stresses. Explicit expressions for the estimating\nequations and asymptotic distribution of the estimators are obtained. Further,\na Monte Carlo simulation study is presented to evaluate the performance of the\nWMDPDE in practical applications.",
      "generated_abstract": "We present a novel approach to estimating the spectral covariance matrix of a\nspatially correlated random field. We develop an asymptotic expansion for the\nspectral covariance matrix of the field under consideration, and apply it to\nestimate the spectral covariance matrix of the corresponding random field of\nthe population. We also develop a Bayesian approach to estimate the spectral\ncovariance matrix of the field under consideration by modeling the field with\na Gaussian process. We demonstrate the effectiveness of our methods through\nsimulations, and we apply them to the estimation of the spectral covariance\nmatrix of a spatially correlated random field.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09395973154362416,
          "p": 0.30434782608695654,
          "f": 0.143589739984747
        },
        "rouge-2": {
          "r": 0.009950248756218905,
          "p": 0.029411764705882353,
          "f": 0.014869884698111818
        },
        "rouge-l": {
          "r": 0.08053691275167785,
          "p": 0.2608695652173913,
          "f": 0.12307691947192648
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.gr-qc/2503.10346v1",
      "true_abstract": "The Hubble tension has emerged as a critical crisis in cosmology, with the\ncause remaining unclear. Determining the Hubble constant ($H_0$) independently\nof cosmological models and distance ladders will help resolve this crisis. In\nthis letter, we for the first time use 47 gravitational-wave (GW) standard\nsirens from the third Gravitational-Wave Transient Catalog to calibrate\ndistances in the strong lensing system, RXJ1131-1231, and constrain $H_0$\nthrough the distance-sum rule, with minimal cosmological assumptions. We assume\nthat light propagation over long distances is described by the\nFriedmann-Lemaitre-Robertson-Walker metric and that geometrical optics holds,\nbut we do not need to assume the universe's contents or the theory of gravity\non cosmological scales. Fixing $\\Omega_K=0$, we obtain\n$H_0=73.22^{+5.95}_{-5.43}$ ${\\rm km}~{\\rm s}^{-1}~{\\rm Mpc}^{-1}$ and\n$H_0=70.40^{+8.03}_{-5.60}$ ${\\rm km}~{\\rm s}^{-1}~{\\rm Mpc}^{-1}$ by using the\ndeflector galaxy's mass model and kinematic measurements to break mass-sheet\ntransform, respectively. When $\\Omega_K$ is not fixed, the central value of\n$H_0$ increases further. We find that our results are still dominated by\nstatistical errors, and at the same time, we notice the great potential of\nusing GW dark sirens to provide calibration, owing to their higher redshifts.\nWhen using 42 binary black holes and RXJ1131-1231, we obtain a $8.46 \\%$ $H_0$\nconstraint precision, which is better than that from the bright siren GW170817\nusing the Hubble law by about $40\\%$. In the future, as the redshift range of\nGW dark sirens increases, more and more SGLTDs can be included, and we can\nachieve high-precision, model-independent measurements of $H_0$ without the\nneed for GW bright sirens.",
      "generated_abstract": "t the first results from a three-year campaign of X-ray\nobservations of the young open cluster NGC 2273. We obtained 554\\,ks of\nobservations using the {\\sl Chandra} X-ray telescope, including 132\\,ks of\nhigh-resolution X-ray spectroscopy. The spectra were obtained in the soft\nexposure mode, covering the energy range 0.5-10\\,keV. We obtained a new\nspectral energy distribution for the cluster, which includes an additional\ncomponent that we call a non-thermal component. This component is well fitted\nby a power-law with a photon index of $\\Gamma = 2.35 \\pm 0.03$, and it is\nsignificantly higher than the photon index of the thermal component, $\\Gamma =\n2.16 \\pm 0.02",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12138728323699421,
          "p": 0.2727272727272727,
          "f": 0.1679999957372801
        },
        "rouge-2": {
          "r": 0.016,
          "p": 0.038834951456310676,
          "f": 0.022662885385486553
        },
        "rouge-l": {
          "r": 0.10982658959537572,
          "p": 0.24675324675324675,
          "f": 0.15199999573728015
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/TR/2411.05951v1",
      "true_abstract": "Multifractality is a concept that helps compactly grasping the most essential\nfeatures of the financial dynamics. In its fully developed form, this concept\napplies to essentially all mature financial markets and even to more liquid\ncryptocurrencies traded on the centralized exchanges. A new element that adds\ncomplexity to cryptocurrency markets is the possibility of decentralized\ntrading. Based on the extracted tick-by-tick transaction data from the\nUniversal Router contract of the Uniswap decentralized exchange, from June 6,\n2023, to June 30, 2024, the present study using Multifractal Detrended\nFluctuation Analysis (MFDFA) shows that even though liquidity on these new\nexchanges is still much lower compared to centralized exchanges convincing\ntraces of multifractality are already emerging on this new trading as well. The\nresulting multifractal spectra are however strongly left-side asymmetric which\nindicates that this multifractality comes primarily from large fluctuations and\nsmall ones are more of the uncorrelated noise type. What is particularly\ninteresting here is the fact that multifractality is more developed for time\nseries representing transaction volumes than rates of return. On the level of\nthese larger events a trace of multifractal cross-correlations between the two\ncharacteristics is also observed.",
      "generated_abstract": "e a novel Bayesian inference framework for pricing American\ndividend options in the presence of market impact. This approach is based on a\nmultivariate GARCH model, which is commonly used to model option price\nvolatility. The model is formulated as a multivariate GARCH-type model, and the\nGARCH parameters are estimated using the Bayesian inference method. The\nframework incorporates the risk premium, which is a stylized fact that returns\nexhibit a positive correlation with the dividend yield. The risk premium is\nmodelled as a multivariate GARCH-type model. The risk premium is assumed to be\na stochastic factor that affects both the dividend yield and the option price.\nWe derive the joint posterior distribution of the risk premium parameters and\nestimate the risk premium. The joint posterior distribution of the risk\npremium parameters is obtained via",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1171875,
          "p": 0.2112676056338028,
          "f": 0.15075376425443815
        },
        "rouge-2": {
          "r": 0.016216216216216217,
          "p": 0.027522935779816515,
          "f": 0.020408158599427233
        },
        "rouge-l": {
          "r": 0.109375,
          "p": 0.19718309859154928,
          "f": 0.14070351299815678
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2412.11113v1",
      "true_abstract": "We consider an economic environment with one buyer and one seller. For a\nbundle $(t,q)\\in [0,\\infty[\\times [0,1]=\\mathbb{Z}$, $q$ refers to the winning\nprobability of an object, and $t$ denotes the payment that the buyer makes. We\nconsider continuous and monotone preferences on $\\mathbb{Z}$ as the primitives\nof the buyer. These preferences can incorporate both quasilinear and\nnon-quasilinear preferences, and multidimensional pay-off relevant parameters.\nWe define rich single-crossing subsets of this class and characterize\nstrategy-proof mechanisms by using monotonicity of the mechanisms and\ncontinuity of the indirect preference correspondences. We also provide a\ncomputationally tractable optimization program to compute the optimal mechanism\nfor mechanisms with finite range. We do not use revenue equivalence and virtual\nvaluations as tools in our proofs. Our proof techniques bring out the geometric\ninteraction between the single-crossing property and the positions of bundles\n$(t,q)$s in the space $\\mathbb{Z}$. We also provide an extension of our\nanalysis to an $n-$buyer environment, and to the situation where $q$ is a\nqualitative variable.",
      "generated_abstract": "uce a framework for the analysis of fair allocation problems\nwith a single object. The proposed approach extends the classic\nframework of social welfare maximization to settings where agents' valuations\nare unknown and the objective is non-monotonic. The framework is based on\nrepresentative agents, i.e., the agents are grouped into classes, each\nrepresenting a distinct valuation. The allocation problem is then viewed as a\nminimization of a weighted sum of social welfare losses, where the weights are\ninversely proportional to the class size. We show that the problem admits a\nclosed-form solution, and provide a computational approach for computing\nsolutions in polynomial time. We further extend the framework to the context of\nmultiple objects, where agents' valuations are also unknown. Our analysis\nreveals that the optimal allocation is a weighted sum of the social welfare of\nall allocations that maxim",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19444444444444445,
          "p": 0.24705882352941178,
          "f": 0.21761657538188958
        },
        "rouge-2": {
          "r": 0.04487179487179487,
          "p": 0.05384615384615385,
          "f": 0.04895104399237177
        },
        "rouge-l": {
          "r": 0.17592592592592593,
          "p": 0.2235294117647059,
          "f": 0.19689118678085332
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2502.07868v1",
      "true_abstract": "This paper studies the ubiquitous problem of liquidating large quantities of\nhighly correlated stocks, a task frequently encountered by institutional\ninvestors and proprietary trading firms. Traditional methods in this setting\nsuffer from the curse of dimensionality, making them impractical for\nhigh-dimensional problems. In this work, we propose a novel method based on\nstochastic optimal control to optimally tackle this complex multidimensional\nproblem. The proposed method minimizes the overall execution shortfall of\nhighly correlated stocks using a reinforcement learning approach. We rigorously\nestablish the convergence of our optimal trading strategy and present an\nimplementation of our algorithm using intra-day market data.",
      "generated_abstract": "aper, we propose a novel, robust, and efficient method to generate\nlarge-scale synthetic equity data, which is particularly useful for testing\nfinancial models and benchmarking investment strategies. Our approach uses\nhistorical market data to generate synthetic equity prices and returns using\na two-stage process. First, we use a machine learning model to generate\nsynthetic prices for the entire historical data set. Second, we apply\napproximate Bayesian computation to generate synthetic returns. The\napproximate Bayesian computation process provides a way to account for\nuncertainty in the historical data. The proposed method has three key\nadvantages: (1) it is computationally efficient, (2) it produces high-quality\ndata, and (3) it can be applied to different market conditions, making it\nsuitable for use in both research and financial applications. Our experiments\nshow that the proposed method generates synth",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21518987341772153,
          "p": 0.20238095238095238,
          "f": 0.2085889520599196
        },
        "rouge-2": {
          "r": 0.0625,
          "p": 0.04878048780487805,
          "f": 0.0547945156239449
        },
        "rouge-l": {
          "r": 0.20253164556962025,
          "p": 0.19047619047619047,
          "f": 0.19631901340961286
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.09908v1",
      "true_abstract": "We present a work optimal algorithm for parallel fully batch-dynamic maximal\nmatching against an oblivious adversary. In particular it processes batches of\nupdates (either insertions or deletions of edges) in constant expected\namortized work per edge update, and in $O(\\log^3 m)$ depth per batch whp, where\n$m$ is the maximum number of edges in the graph over time. This greatly\nimproves on the recent result by Ghaffari and Trygub (2024) that requires\n$O(\\log^9 m)$ amortized work per update and $O(\\log^4 m )$ depth per batch,\nboth whp. The algorithm can also be used for hyperedge maximal matching. For\nhypergraphs with rank $r$ (maximum cardinality of any edge) the algorithm\nsupports batches of insertions and deletions with $O(r^3)$ expected amortized\nwork per edge update, and $O(\\log^3 m)$ depth per batch whp. This is a factor\nof $O(r)$ work off of the best sequential algorithm, Assadi and Solomon (2021),\nwhich uses $O(r^2)$ work per update. Ghaffari and Trygub's parallel\nbatch-dynamic algorithm on hypergraphs requires $O(r^8 \\log^9 m)$ amortized\nwork per edge update whp. We leverage ideas from the prior algorithms but\nintroduce substantial new ideas. Furthermore, our algorithm is relatively\nsimple, perhaps even simpler than the sequential hyperedge algorithm. We also\npresent the first work-efficient algorithm for maximal matching on hypergraphs.\nFor a hypergraph with total cardinality $m'$ (i.e., sum over the cardinality of\neach edge), the algorithm runs in $O(m')$ work in expectation and $O(\\log^2 m)$\ndepth whp. The algorithm also has some properties that allow us to use it as a\nsubroutine in the dynamic algorithm to select random edges in the graph to add\nto the matching.",
      "generated_abstract": "In this paper, we propose a novel approach for efficiently computing the\nsolution set of a given CSP with respect to a given constraint system. The\nmain idea behind our approach is to use the information of the constraint\nsystem to construct an appropriate tree. This tree, which is constructed in a\nway that ensures that the solution set is computed efficiently, is then used to\ncompute the solution set. We prove that this approach is sound and complete for\nthe CSP/CS-solving problem. Furthermore, we give an efficient algorithm for\ncomputing the solution set of a given CSP/CS with respect to a given constraint\nsystem, and show that this algorithm is also sound and complete. Finally, we\nprovide a simple example to illustrate our approach.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14864864864864866,
          "p": 0.3283582089552239,
          "f": 0.2046511585003787
        },
        "rouge-2": {
          "r": 0.01702127659574468,
          "p": 0.0392156862745098,
          "f": 0.023738868182339244
        },
        "rouge-l": {
          "r": 0.13513513513513514,
          "p": 0.29850746268656714,
          "f": 0.18604650733758799
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/class-ph/2503.03342v1",
      "true_abstract": "For linear electromagnetic systems possessing time-reversal symmetry, we\npresent an approach to bound ratios of internal fields excited from different\nports, using only the scattering matrix (S matrix), improving upon previous\nrelated bounds by Sounas and Al\\`u (2017). By reciprocity, emitted-wave\namplitudes from internal dipole sources are bounded in a similar way. When\napplied to coupled-resonant systems, our method constrains ratios of resonant\ncoupling/decay coefficients. We also obtain a relation for the relative phase\nof fields excited from the two ports and the ratio of field intensities in a\ntwo-port system. In addition, although lossy systems do not have time-reversal\nsymmetry, we can still approximately bound loss-induced non-unitarity of the S\nmatrix using only the lossless S matrix. We show numerical validations of the\nnear-tightness of our bounds in various scattering systems.",
      "generated_abstract": "t of light-based neuromorphic computing systems has transformed\nthe paradigm of deep learning, with neuromorphic architectures offering\npromising computational advantages. However, the current practice of using\noff-chip memory to store the weights of the networks remains a significant\nbarrier to the scaling of neuromorphic computing systems. In this paper, we\npropose a novel scheme to address this issue by integrating on-chip memories\nwith neuromorphic hardware. This approach leverages the inherent parallelism\nand scalability of neuromorphic hardware, enabling the implementation of\nhigh-dimensional matrix-vector multiplications in parallel across the on-chip\nmemory. The memory is organized into a row-block and column-block structure to\noptimize the parallelism and to address the limitations of conventional\nrow-block memory. The on-chip matrix-vector multiplication is performed in\nparallel across the block-rows, thereby reducing the memory laten",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12903225806451613,
          "p": 0.1518987341772152,
          "f": 0.13953487875405643
        },
        "rouge-2": {
          "r": 0.008264462809917356,
          "p": 0.008547008547008548,
          "f": 0.008403356345953118
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.1518987341772152,
          "f": 0.13953487875405643
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/CO/2501.18901v1",
      "true_abstract": "We introduce sliced optimal transport dataset distance (s-OTDD), a\nmodel-agnostic, embedding-agnostic approach for dataset comparison that\nrequires no training, is robust to variations in the number of classes, and can\nhandle disjoint label sets. The core innovation is Moment Transform Projection\n(MTP), which maps a label, represented as a distribution over features, to a\nreal number. Using MTP, we derive a data point projection that transforms\ndatasets into one-dimensional distributions. The s-OTDD is defined as the\nexpected Wasserstein distance between the projected distributions, with respect\nto random projection parameters. Leveraging the closed form solution of\none-dimensional optimal transport, s-OTDD achieves (near-)linear computational\ncomplexity in the number of data points and feature dimensions and is\nindependent of the number of classes. With its geometrically meaningful\nprojection, s-OTDD strongly correlates with the optimal transport dataset\ndistance while being more efficient than existing dataset discrepancy measures.\nMoreover, it correlates well with the performance gap in transfer learning and\nclassification accuracy in data augmentation.",
      "generated_abstract": "of large language models (LLMs) have been trained on massive\ncrowdsourced data, making them appealing for tasks that require large\nhigh-dimensional data, such as text-based visual question answering (VQA).\nHowever, the inherent challenges of large language models, such as model\nstability and bias, have raised concerns about their ability to solve\ncrowdsourced tasks effectively. We propose a new benchmark, MixedMix-VQA,\ncombining VQA with a mix of diverse datasets and LLMs. The benchmark includes\ntwo datasets: 1) a large-scale, diverse dataset, Mixed-2000, and 2) a smaller,\nmore targeted dataset, Mixed-100. Mixed-100 contains 100 distinct images that\nare collected from a wide range of publicly available sources. Mixed-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11711711711711711,
          "p": 0.15476190476190477,
          "f": 0.1333333284291915
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11711711711711711,
          "p": 0.15476190476190477,
          "f": 0.1333333284291915
        }
      }
    },
    {
      "paper_id": "cond-mat.supr-con.cond-mat/str-el/2503.10085v1",
      "true_abstract": "We consider multiband BCS superconductors that exhibit time-reversal symmetry\nand uniform pairing, and analyze their dynamic density and spin structure\nfactors using linear-response theory within the mean-field BCS-BEC crossover\nframework at zero temperature. Our results for the multi-orbital Hubbard model\nsatisfy the associated f-sum rules in several limits. In particular, in the\nstrong-coupling limit, they coincide with those of a weakly-interacting Bose\ngas of Cooper pairs, where the low-energy collective Goldstone modes serve as\nBogoliubov phonons. We further reveal that the quantum-geometric origin of the\nlow-energy structure factors, along with related observables such as the\nsuperfluid-weight tensor and the effective-mass tensor of Cooper pairs, can be\ntraced all the way back to the effective-mass theorem for Bloch bands in this\nlimit. As an illustration, we investigate the pyrochlore-Hubbard model\nnumerically and demonstrate that the Goldstone modes are the only relevant\ncollective degrees of freedom in the flat-band regime.",
      "generated_abstract": "range magnetism of metallic and semiconducting quantum\nheterostructures is predicted to be a consequence of the interplay between\nsuperconductivity, magnetism, and electron-electron interaction. This\ninterplay, however, is not fully understood, and the origin of superconductivity\nremains unclear. Here, we study the interplay between superconductivity and\nmagnetism in a heterostructure consisting of a thin superconducting\nsemiconducting layer sandwiched between two insulating layers. We find that\nthe superconducting state is stabilized by the spin-orbit coupling and\nsuperconducting gap, and the superconducting state can be protected by\ninterlayer magnetism. We further show that the superconducting state is\ndestabilized by the spin-orbit coupling and the interlayer magnetism, and the\nsuperconduct",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12149532710280374,
          "p": 0.21666666666666667,
          "f": 0.15568861815052543
        },
        "rouge-2": {
          "r": 0.05,
          "p": 0.07865168539325842,
          "f": 0.061135366427032656
        },
        "rouge-l": {
          "r": 0.11214953271028037,
          "p": 0.2,
          "f": 0.14371257024633383
        }
      }
    },
    {
      "paper_id": "eess.SP.cs/IT/2503.10472v1",
      "true_abstract": "In this letter, we propose to deploy rotatable antennas (RAs) at the base\nstation (BS) to enhance both communication and sensing (C&S) performances, by\nexploiting a new spatial degree-of-freedom (DoF) offered by array rotation.\nSpecifically, we formulate a multi-objective optimization problem to\nsimultaneously maximize the sum-rate of multiple communication users and\nminimize the Cram\\'er-Rao bound (CRB) for target angle estimation, by jointly\noptimizing the transmit beamforming vectors and the array rotation angle at the\nBS. To solve this problem, we first equivalently decompose it into two\nsubproblems, corresponding to an inner problem for beamforming optimization and\nan outer problem for array rotation optimization. Although these two\nsubproblems are non-convex, we obtain their high-quality solutions by applying\nthe block coordinate descent (BCD) technique and one-dimensional exhaustive\nsearch, respectively. Moreover, we show that for the communication-only case,\nRAs provide an additional rotation gain to improve communication performance;\nwhile for the sensing-only case, the equivalent spatial aperture can be\nenlarged by RAs for achieving higher sensing accuracy. Finally, numerical\nresults are presented to showcase the performance gains of RAs over\nfixed-rotation antennas in integrated sensing and communications (ISAC).",
      "generated_abstract": "vancements in large language models (LLMs) have enabled\nexciting advancements in natural language understanding (NLU) and generation\n(NLG). However, their use in autonomous driving remains limited due to their\ndifficulties in handling complex scenarios and diverse driving tasks. In this\npaper, we propose a novel framework, AI4Drive, which integrates LLMs with\ndriving-specific knowledge to improve safety, efficiency, and autonomy in\ndriving scenarios. We first introduce the AI4Drive architecture, which\nintegrates the LLM with a driving simulator, a driving dataset, and a\ndriving-specific knowledge base. The LLM uses this knowledge base to generate\nrelevant driving instructions, ensuring that they are aligned with the user's\ndriving goals. We further introduce AI4Drive Driving Data, a novel data\ncollection",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12096774193548387,
          "p": 0.19480519480519481,
          "f": 0.14925372661666805
        },
        "rouge-2": {
          "r": 0.01675977653631285,
          "p": 0.027522935779816515,
          "f": 0.020833328628714412
        },
        "rouge-l": {
          "r": 0.11290322580645161,
          "p": 0.18181818181818182,
          "f": 0.13930347786044914
        }
      }
    },
    {
      "paper_id": "math.ST.stat/CO/2502.17738v1",
      "true_abstract": "Motivated by learning dynamical structures from static snapshot data, this\npaper presents a distribution-on-scalar regression approach for estimating the\ndensity evolution of a stochastic process from its noisy temporal point clouds.\nWe propose an entropy-regularized nonparametric maximum likelihood estimator\n(E-NPMLE), which leverages the entropic optimal transport as a smoothing\nregularizer for the density flow. We show that the E-NPMLE has almost\ndimension-free statistical rates of convergence to the ground truth\ndistributions, which exhibit a striking phase transition phenomenon in terms of\nthe number of snapshots and per-snapshot sample size. To efficiently compute\nthe E-NPMLE, we design a novel particle-based and grid-free coordinate KL\ndivergence gradient descent (CKLGD) algorithm and prove its polynomial\niteration complexity. Moreover, we provide numerical evidence on synthetic data\nto support our theoretical findings. This work contributes to the theoretical\nunderstanding and practical computation of estimating density evolution from\nnoisy observations in arbitrary dimensions.",
      "generated_abstract": "r introduces a new class of univariate Gaussian processes,\nthat we call ``compound'' Gaussian processes. Unlike traditional Gaussian\nprocesses, which are centered at a single point, our compound Gaussian\nprocesses are centered at a single point, but have multiple uncorrelated\nGaussian distributions. Compound Gaussian processes have several advantages:\n(1) they are easy to construct, (2) they have a wide range of applications,\nincluding the analysis of high-dimensional data, and (3) they can be used to\nmodel complex probability distributions that traditional Gaussian processes\ncannot handle. The main challenge in using compound Gaussian processes is\nconstructing an efficient and accurate estimator of the density of the\ncompound Gaussian process. To address this challenge, we propose two\napproaches: (1) the so-called ``sparse'' approach, which uses a sparse\nrepresentation of the density, and (2) the so",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1592920353982301,
          "p": 0.21428571428571427,
          "f": 0.18274111185962033
        },
        "rouge-2": {
          "r": 0.013986013986013986,
          "p": 0.017094017094017096,
          "f": 0.015384610434616978
        },
        "rouge-l": {
          "r": 0.1415929203539823,
          "p": 0.19047619047619047,
          "f": 0.16243654333170154
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/CO/2503.10496v1",
      "true_abstract": "Modeling natural phenomena with artificial neural networks (ANNs) often\nprovides highly accurate predictions. However, ANNs often suffer from\nover-parameterization, complicating interpretation and raising uncertainty\nissues. Bayesian neural networks (BNNs) address the latter by representing\nweights as probability distributions, allowing for predictive uncertainty\nevaluation. Latent binary Bayesian neural networks (LBBNNs) further handle\nstructural uncertainty and sparsify models by removing redundant weights. This\narticle advances LBBNNs by enabling covariates to skip to any succeeding layer\nor be excluded, simplifying networks and clarifying input impacts on\npredictions. Ultimately, a linear model or even a constant can be found to be\noptimal for a specific problem at hand. Furthermore, the input-skip LBBNN\napproach reduces network density significantly compared to standard LBBNNs,\nachieving over 99% reduction for small networks and over 99.9% for larger ones,\nwhile still maintaining high predictive accuracy and uncertainty measurement.\nFor example, on MNIST, we reached 97% accuracy and great calibration with just\n935 weights, reaching state-of-the-art for compression of neural networks.\nFurthermore, the proposed method accurately identifies the true covariates and\nadjusts for system non-linearity. The main contribution is the introduction of\nactive paths, enhancing directly designed global and local explanations within\nthe LBBNN framework, that have theoretical guarantees and do not require post\nhoc external tools for explanations.",
      "generated_abstract": "em of learning the joint distribution of a set of latent variables\nfrom observational data has received increasing attention in recent years,\nparticularly in the context of predictive modeling and forecasting. In this\ncontext, the main challenge is to accurately model the joint distribution of\nthe latent variables given the observed data. A key challenge is that the joint\ndistribution is often unknown or inaccessible, and it is often difficult to\nderive an explicit representation of the joint distribution. This paper\naddresses this challenge by proposing a novel approach for learning the joint\ndistribution of latent variables. The approach is based on the use of\nmulti-variate Gaussian Processes to represent the joint distribution. A\nspecialized loss function is proposed to minimize the log-likelihood of the\ndata under the learned joint distribution. The proposed method is\ncomputationally efficient and is shown to outperform state-of-the-art",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14743589743589744,
          "p": 0.2875,
          "f": 0.19491524975581737
        },
        "rouge-2": {
          "r": 0.0049504950495049506,
          "p": 0.008264462809917356,
          "f": 0.00619194577883781
        },
        "rouge-l": {
          "r": 0.1346153846153846,
          "p": 0.2625,
          "f": 0.1779660972134445
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.19906v1",
      "true_abstract": "Single-channel speech enhancement is a challenging ill-posed problem focused\non estimating clean speech from degraded signals. Existing studies have\ndemonstrated the competitive performance of combining convolutional neural\nnetworks (CNNs) with Transformers in speech enhancement tasks. However,\nexisting frameworks have not sufficiently addressed computational efficiency\nand have overlooked the natural multi-scale distribution of the spectrum.\nAdditionally, the potential of CNNs in speech enhancement has yet to be fully\nrealized. To address these issues, this study proposes a Deep Separable Dilated\nDense Block (DSDDB) and a Group Prime Kernel Feedforward Channel Attention\n(GPFCA) module. Specifically, the DSDDB introduces higher parameter and\ncomputational efficiency to the Encoder/Decoder of existing frameworks. The\nGPFCA module replaces the position of the Conformer, extracting deep temporal\nand frequency features of the spectrum with linear complexity. The GPFCA\nleverages the proposed Group Prime Kernel Feedforward Network (GPFN) to\nintegrate multi-granularity long-range, medium-range, and short-range receptive\nfields, while utilizing the properties of prime numbers to avoid periodic\noverlap effects. Experimental results demonstrate that PrimeK-Net, proposed in\nthis study, achieves state-of-the-art (SOTA) performance on the\nVoiceBank+Demand dataset, reaching a PESQ score of 3.61 with only 1.41M\nparameters.",
      "generated_abstract": "uce a new dataset, Heterogeneous Mixed-Language Speech Audio,\nwith a focus on speech in non-English languages. The dataset consists of\nrecordings from 11 speakers from six different languages (Arabic, Chinese,\nEnglish, French, Portuguese, and Spanish) in mixed-language settings.\nAdditionally, we provide a dataset of English-language speech in the\ncontext of different types of social interactions, such as social chats,\ngroup discussions, and family gatherings. We provide an extensive set of\nfeatures, including acoustic features such as mel-spectrograms, speech\nsegmentation, and speaker identity, as well as social interaction features,\nsuch as sentiment, language, and speaker-group membership. We also provide\nadditional metadata, such as the number of speakers in each conversation, the\nduration of each conversation, and the gender of the participants. The dataset\nis publicly",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1044776119402985,
          "p": 0.17721518987341772,
          "f": 0.13145539439441042
        },
        "rouge-2": {
          "r": 0.005681818181818182,
          "p": 0.008620689655172414,
          "f": 0.0068493102796055244
        },
        "rouge-l": {
          "r": 0.1044776119402985,
          "p": 0.17721518987341772,
          "f": 0.13145539439441042
        }
      }
    },
    {
      "paper_id": "quant-ph.math-ph/2503.10123v1",
      "true_abstract": "Based on the generalized Bloch representation, we study the separability and\ngenuine multipartite entanglement of arbitrary dimensional multipartite quantum\nstates. Some sufficient and some necessary criteria are presented. For certain\nstates, these criteria together are both sufficient and necessary. Detailed\nexamples show that our criteria are better than some existing ones in\nidentifying entanglement. Based on these criteria, the largest separable ball\naround the maximally mixed state for arbitrary multi-qubit systems is found,\nand it is proved that its radius is the constant 1. Furthermore, the criteria\nin this paper can be implemented experimentally.",
      "generated_abstract": "We explore the concept of quantum coherence as a resource for quantum\nmechanical\n  computing. We consider the entanglement of a quantum system, as well as the\nentanglement of its quantum coherence. We argue that entanglement of quantum\ncoherence is a more general form of entanglement than entanglement of\nquantum systems, and we propose a new definition of quantum coherence as a\nquantum resource. We show that the notion of quantum coherence is invariant\nunder unitary transformations, while entanglement of quantum systems is not.\nWe use this definition to compute the quantum coherence of quantum systems and\nquantum coherence of quantum coherence. We demonstrate that the\nentanglement-based resource theory of quantum computing is in fact\nquantum-coherence-based.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22058823529411764,
          "p": 0.3,
          "f": 0.25423728325193923
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.03488372093023256,
          "f": 0.03409090409349247
        },
        "rouge-l": {
          "r": 0.20588235294117646,
          "p": 0.28,
          "f": 0.2372881307095663
        }
      }
    },
    {
      "paper_id": "physics.space-ph.physics/space-ph/2503.08878v1",
      "true_abstract": "Ion measurements made with the Hot Plasma Composition Analyzers of the\nMagnetospheric Multiscale Mission (MMS-HPCAs) during the Mother's Day Storm\n(Gannon Storm) of 10-13 May 2024 yield the first observations of atomic and\nmolecular nitrogen ions in the Earth's dayside outer magnetosphere. A\npopulation of ions identified as doubly charged nitrogen and oxygen was also\nmeasured. These observations were made within a highly compressed magnetosphere\nat a geocentric distance of ~6 Earth Radii during the early recovery phase of\nthe storm. From the ion composition measurements and accompanying magnetic\nfield data, we determine the reconnection rate at the magnetopause; we compare\nthis result to a model reconnection rate that assumes the presence of only\natomic oxygen and hydrogen. The heavy ion-laden-mass density in the\nmagnetosphere was greater than the shocked solar wind mass density in the\nmagnetosheath. Despite these conditions, magnetic reconnection still occurred\nat the magnetopause.",
      "generated_abstract": "ence of large-scale multi-wavelength radio interferometric datasets\nprovides unique opportunities to address critical questions in space weather,\nparticularly in the context of magnetohydrodynamic (MHD) waves. In this work,\nwe introduce a novel approach to extract MHD wave parameters from multi-wavelength\ndata, focusing on the double-peaked solar wind profiles observed during the\n2012-2013 period. By utilizing the recently developed multi-wavelength MHD\nwavelet-based modeling approach, we derive the propagation directions and\nstrengths of the solar wind MHD waves and their corresponding eigenmodes.\nFurthermore, we analyze the phase-space distribution of the wave eigenmodes to\nidentify the preferred locations of MHD waves and their associated eigenmodes,\nwhich are consistent with the findings of previous studies. Our results",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12871287128712872,
          "p": 0.1625,
          "f": 0.14364640390708483
        },
        "rouge-2": {
          "r": 0.03597122302158273,
          "p": 0.04672897196261682,
          "f": 0.04065040158867136
        },
        "rouge-l": {
          "r": 0.10891089108910891,
          "p": 0.1375,
          "f": 0.1215469563932727
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.02942v1",
      "true_abstract": "Semantic information refers to the meaning conveyed through words, phrases,\nand contextual relationships within a given linguistic structure. Humans can\nleverage semantic information, such as familiar linguistic patterns and\ncontextual cues, to reconstruct incomplete or masked speech signals in noisy\nenvironments. However, existing speech enhancement (SE) approaches often\noverlook the rich semantic information embedded in speech, which is crucial for\nimproving intelligibility, speaker consistency, and overall quality of enhanced\nspeech signals. To enrich the SE model with semantic information, we employ\nlanguage models as an efficient semantic learner and propose a comprehensive\nframework tailored for language model-based speech enhancement, called\n\\textit{GenSE}. Specifically, we approach SE as a conditional language modeling\ntask rather than a continuous signal regression problem defined in existing\nworks. This is achieved by tokenizing speech signals into semantic tokens using\na pre-trained self-supervised model and into acoustic tokens using a\ncustom-designed single-quantizer neural codec model. To improve the stability\nof language model predictions, we propose a hierarchical modeling method that\ndecouples the generation of clean semantic tokens and clean acoustic tokens\ninto two distinct stages. Moreover, we introduce a token chain prompting\nmechanism during the acoustic token generation stage to ensure timbre\nconsistency throughout the speech enhancement process. Experimental results on\nbenchmark datasets demonstrate that our proposed approach outperforms\nstate-of-the-art SE systems in terms of speech quality and generalization\ncapability.",
      "generated_abstract": "Audio-visual speech synthesis (AVSS) aims to generate speech from a visual\ntext. However, the limited availability of video data hinders the progress of\nthis field. To overcome this challenge, we propose a novel approach that\nintegrates audio and visual cues for AVSS. Specifically, we design a multi-modal\ndecoder that utilizes audio and video features to enhance the generation of\nspeech. To this end, we introduce an audio-visual speech encoder that\nsynthesizes speech from a video in a multi-modal manner. Additionally, we\ndevelop a video-audio speech decoder to convert the generated speech into a\nvideo. The proposed method enables the generation of speech from a video in a\nunified manner, which is not available in existing methods. Experimental\nresults demonstrate the effectiveness of the proposed method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18620689655172415,
          "p": 0.36486486486486486,
          "f": 0.24657533799128467
        },
        "rouge-2": {
          "r": 0.04716981132075472,
          "p": 0.09090909090909091,
          "f": 0.062111796743953104
        },
        "rouge-l": {
          "r": 0.18620689655172415,
          "p": 0.36486486486486486,
          "f": 0.24657533799128467
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.08826v1",
      "true_abstract": "Integrating BD-RIS into wireless communications systems has attracted\nsignificant interest due to its transformative potential in enhancing system\nperformance. This survey provides a comprehensive analysis of BD-RIS\ntechnology, examining its modeling, structural characteristics, and network\nintegration while highlighting its advantages over traditional diagonal RIS.\nSpecifically, we review various BD-RIS modeling approaches, including multiport\nnetwork theory, graph theory, and matrix theory, and emphasize their\napplication in diverse wireless scenarios. The survey also covers BD-RIS's\nstructural diversity, including different scattering matrix types, transmission\nmodes, intercell architectures, and circuit topologies, showing their\nflexibility in improving network performance. We delve into the potential\napplications of BD-RIS, such as enhancing wireless coverage, improving PLS,\nenabling multi-cell interference cancellation, improving precise sensing and\nlocalization, and optimizing channel manipulation. Further, we explore BD-RIS\narchitectural development, providing insights into new configurations focusing\non channel estimation, optimization, performance analysis, and circuit\ncomplexity perspectives. Additionally, we investigate the integration of BD-RIS\nwith emerging wireless technologies, such as millimeter-wave and terahertz\ncommunications, integrated sensing and communications, mobile edge computing,\nand other cutting-edge technologies. These integrations are pivotal in\nadvancing the capabilities and efficiency of future wireless networks. Finally,\nthe survey identifies key challenges, including channel state information\nestimation, interference modeling, and phase-shift designs, and outlines future\nresearch directions. The survey aims to provide valuable insights into BD-RIS's\npotential in shaping the future of wireless communications systems.",
      "generated_abstract": "r addresses the problem of designing a low-complexity, low-power,\nand low-latency (LCL) system for a wireless sensor network (WSN) operating in\nthe millimeter-wave (mmWave) frequency band. The main challenge in the mmWave\ndomain is the non-ideal propagation channel, which introduces a significant\ndelay and degrades the system performance. To address this challenge, we\npropose a hybrid wireless-radar system architecture, where the radar transmitter\nis embedded in the mmWave baseband processor, and the radar receiver is\nintegrated with the baseband processor. This architecture improves the\nsystem's energy efficiency and reduces the latency in the radar subsystem,\nwhile maintaining the system's low-complexity and low-power requirements. We\npropose a two-stage approach to design the system. The first stage of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1360544217687075,
          "p": 0.25316455696202533,
          "f": 0.1769911458951368
        },
        "rouge-2": {
          "r": 0.004672897196261682,
          "p": 0.009433962264150943,
          "f": 0.0062499955695343905
        },
        "rouge-l": {
          "r": 0.11564625850340136,
          "p": 0.21518987341772153,
          "f": 0.15044247332876512
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/TH/2502.10161v1",
      "true_abstract": "Reasoning about fairness through correlation-based notions is rife with\npitfalls. The 1973 University of California, Berkeley graduate school\nadmissions case from Bickel et. al. (1975) is a classic example of one such\npitfall, namely Simpson's paradox. The discrepancy in admission rates among\nmales and female applicants, in the aggregate data over all departments,\nvanishes when admission rates per department are examined. We reason about the\nBerkeley graduate school admissions case through a causal lens. In the process,\nwe introduce a statistical test for causal hypothesis testing based on Pearl's\ninstrumental-variable inequalities (Pearl 1995). We compare different causal\nnotions of fairness that are based on graphical, counterfactual and\ninterventional queries on the causal model, and develop statistical tests for\nthese notions that use only observational data. We study the logical relations\nbetween notions, and show that while notions may not be equivalent, their\ncorresponding statistical tests coincide for the case at hand. We believe that\na thorough case-based causal analysis helps develop a more principled\nunderstanding of both causal hypothesis testing and fairness.",
      "generated_abstract": "of this paper is to develop a novel method for estimating the\ncovariance matrix of a high-dimensional multivariate random vector. We\nintroduce a novel estimator for the covariance matrix of the multivariate\nrandom vector and demonstrate its properties through simulation studies. We\nalso use the new estimator to study the covariance structure of a high-dimensional\nmultivariate Gaussian process, which is known to have a positive-definite\ncovariance matrix. Finally, we use the proposed estimator to analyze the\ncovariance matrix of the multivariate Gaussian process, demonstrating that the\nestimated covariance matrix is consistent and asymptotically normal.\n  The methodology developed in this paper provides a novel approach for\nestimating the covariance matrix of a high-dimensional multivariate random\nvector. It can be used for a variety of applications, including analyzing the\ncov",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15789473684210525,
          "p": 0.28125,
          "f": 0.20224718640575695
        },
        "rouge-2": {
          "r": 0.024691358024691357,
          "p": 0.0425531914893617,
          "f": 0.03124999535278389
        },
        "rouge-l": {
          "r": 0.14035087719298245,
          "p": 0.25,
          "f": 0.1797752762933974
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.17726v1",
      "true_abstract": "The Musical Instrument Digital Interface (MIDI), introduced in 1983,\nrevolutionized music production by allowing computers and instruments to\ncommunicate efficiently. MIDI files encode musical instructions compactly,\nfacilitating convenient music sharing. They benefit Music Information Retrieval\n(MIR), aiding in research on music understanding, computational musicology, and\ngenerative music. The GigaMIDI dataset contains over 1.4 million unique MIDI\nfiles, encompassing 1.8 billion MIDI note events and over 5.3 million MIDI\ntracks. GigaMIDI is currently the largest collection of symbolic music in MIDI\nformat available for research purposes under fair dealing. Distinguishing\nbetween non-expressive and expressive MIDI tracks is challenging, as MIDI files\ndo not inherently make this distinction. To address this issue, we introduce a\nset of innovative heuristics for detecting expressive music performance. These\ninclude the Distinctive Note Velocity Ratio (DNVR) heuristic, which analyzes\nMIDI note velocity; the Distinctive Note Onset Deviation Ratio (DNODR)\nheuristic, which examines deviations in note onset times; and the Note Onset\nMedian Metric Level (NOMML) heuristic, which evaluates onset positions relative\nto metric levels. Our evaluation demonstrates these heuristics effectively\ndifferentiate between non-expressive and expressive MIDI tracks. Furthermore,\nafter evaluation, we create the most substantial expressive MIDI dataset,\nemploying our heuristic, NOMML. This curated iteration of GigaMIDI encompasses\nexpressively-performed instrument tracks detected by NOMML, containing all\nGeneral MIDI instruments, constituting 31% of the GigaMIDI dataset, totalling\n1,655,649 tracks.",
      "generated_abstract": "ning (DL) is an emerging approach to speech enhancement, but\nthe current state of the art relies on large language models (LLMs) to generate\nspeech enhancement (SE) filters. However, these methods often suffer from\nsuboptimal SE filter selection and performance degradation when LLMs have\nlimited vocabulary. To address these limitations, we introduce Deep-Vocab, a\nnovel DL-based SE framework that employs a multimodal LLM to select SE\nfilters based on the input signal. Our approach leverages multi-modal\ninformation to enhance SE performance, including speech and acoustic scene\nfeatures, word embeddings, and speaker embeddings. In addition, we propose an\nadvanced multimodal SE framework that leverages both acoustic and linguistic\ninformation to enhance SE performance. To evaluate our approach, we conduct\nex",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0967741935483871,
          "p": 0.1744186046511628,
          "f": 0.1244813232106887
        },
        "rouge-2": {
          "r": 0.014285714285714285,
          "p": 0.02654867256637168,
          "f": 0.01857584684411924
        },
        "rouge-l": {
          "r": 0.0967741935483871,
          "p": 0.1744186046511628,
          "f": 0.1244813232106887
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.00850v1",
      "true_abstract": "In this paper we develop the theory of the depth of a simple algebraic\nextension of valued fields $(L/K,v)$. This is defined as the minimal number of\naugmentations appearing in some Mac Lane-Vaqui\\'e chain for the valuation on\n$K[x]$ determined by the choice of some generator of the extension. In the\ndefectless and unibranched case, this concept leads to a generalization of a\nclassical result of Ore about the existence of $p$-regular generators for\nnumber fields. Also, we find what valuation-theoretic conditions characterize\nthe extensions having depth one.",
      "generated_abstract": "aper, we present a generalization of the Painlev\\'e II equation to\nthe case of an arbitrary number of variables. Our approach is based on the\nconstruction of a parametrix for the Painlev\\'e II equation using the\nRicci-flat K\\\"ahler metric of a K\\\"ahler manifold, which is a generalization\nof the metric of the product K\\\"ahler manifold. The construction of the\nparametrix is done via the $L^2$ orthogonality relation between the\nK\\\"ahler-Einstein metric and the K\\\"ahler metric of the K\\\"ahler manifold.\n  We prove that the resulting Painlev\\'e II equation admits a solution that\nis a solution to the Painlev\\'e II equation in the standard K\\\"ahler case. We\nalso prove that the solution of the Painlev\\'e II equation in the case of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19672131147540983,
          "p": 0.24,
          "f": 0.2162162112653195
        },
        "rouge-2": {
          "r": 0.07058823529411765,
          "p": 0.07142857142857142,
          "f": 0.07100591215993873
        },
        "rouge-l": {
          "r": 0.16393442622950818,
          "p": 0.2,
          "f": 0.18018017522928348
        }
      }
    },
    {
      "paper_id": "quant-ph.quant-ph/2503.10400v1",
      "true_abstract": "We elucidate the requirements for quantum operations that achieve\nenvironment-assisted invariance (envariance), a symmetry of entanglement. While\nenvariance has traditionally been studied within the framework of local unitary\noperations, we extend the analysis to consider non-unitary local operations.\nFirst, we investigate the conditions imposed on operators acting on pure\nbipartite entanglement to attain envariance. We show that the local operations\nmust take a direct-sum form in their Kraus operator representations,\nestablishing decoherence-free subspaces. Furthermore, we prove that the unitary\noperation on the system's subspace uniquely determines the corresponding\nunitary operator on the environment's subspace. As an immediate consequence, we\ndemonstrate that environment-assisted shortcuts to adiabaticity cannot be\nachieved through non-unitary operations. In addition, we identify the\nrequirements that local operations must satisfy to ensure that the eternal\nblack hole states remain static in AdS/CFT.",
      "generated_abstract": "t a theoretical framework to characterize the influence of\ninformation processing\nquantum mechanically on the statistical mechanics of classical systems. We\nformulate the problem in terms of the entanglement entropy, a measure of the\nentanglement between quantum systems. We demonstrate that the statistical\nmechanics of the classical system is determined by the entropy of the\ninformation-processing quantum system. We show that this entanglement entropy\nis directly proportional to the quantum coherence of the classical system and\nthat the quantum coherence of the classical system is determined by the\nentanglement entropy. This result establishes a clear connection between the\nstatistical mechanics of classical systems and the entanglement entropy of the\nquantum system. We also demonstrate that the classical entropy of the classical\nsystem is determined by the information-processing quantum entropy. We show that\nthis entanglement entropy is directly proportional to the classical entropy of\nthe classical system",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14772727272727273,
          "p": 0.2765957446808511,
          "f": 0.19259258805377238
        },
        "rouge-2": {
          "r": 0.03968253968253968,
          "p": 0.0625,
          "f": 0.04854368456970543
        },
        "rouge-l": {
          "r": 0.14772727272727273,
          "p": 0.2765957446808511,
          "f": 0.19259258805377238
        }
      }
    },
    {
      "paper_id": "physics.hist-ph.q-bio/OT/2412.18030v1",
      "true_abstract": "The 2024 Nobel Prize in Physics was awarded to John Hopfield and Geoffrey\nHinton, \"for foundational discoveries and inventions that enable machine\nlearning with artificial neural networks.\" As noted by the Nobel committee,\ntheir work moved the boundaries of physics. This is a brief reflection on\nHopfield's work, its implications for the emergence of biological physics as a\npart of physics, the path from his early papers to the modern revolution in\nartificial intelligence, and prospects for the future.",
      "generated_abstract": "opment of modern quantitative methods for biological systems\nrequires the ability to control and manipulate data. The ability to manipulate\ndata has been essential in biology for decades, and has been largely overlooked\nin quantitative methods. This is a significant gap because biological data is\noften characterized by the presence of noise, which can lead to bias and\nuncertainty. Additionally, biological data often comes in the form of highly\nvariable data that can be challenging to manipulate. This paper examines how\nquantitative methods can be used to manipulate biological data and how this\ncan be useful in biology. It starts by examining how the data are often\ncharacterized by noise and the challenges this poses for biological data. It\nthen examines how quantitative methods can be used to remove noise from\nbiological data and analyze it without noise. This is followed by a review of\nhow biological data",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.234375,
          "p": 0.21428571428571427,
          "f": 0.22388059202494995
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.025,
          "f": 0.030303025528008103
        },
        "rouge-l": {
          "r": 0.203125,
          "p": 0.18571428571428572,
          "f": 0.1940298457562933
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.19898v1",
      "true_abstract": "This study utilizes a simulated dataset to establish Python code for Double\nMachine Learning (DML) using Anaconda's Jupyter Notebook and the DML software\npackage from GitHub. The research focuses on causal inference experiments for\nboth binary and continuous treatment variables. The findings reveal that the\nDML model demonstrates relatively stable performance in calculating the Average\nTreatment Effect (ATE) and its robustness metrics. However, the study also\nhighlights that the computation of Conditional Average Treatment Effect (CATE)\nremains a significant challenge for future DML modeling, particularly in the\ncontext of continuous treatment variables. This underscores the need for\nfurther research and development in this area to enhance the model's\napplicability and accuracy.",
      "generated_abstract": "r investigates the role of economic uncertainty in the transmission\nof COVID-19 and the subsequent economic impact on the United States. Using\ndynamic panel data models, we examine the effect of COVID-19 on economic\nactivity and the transmission of the disease through aggregate demand and\nsupply shocks. We also examine the impact of the COVID-19 pandemic on\nunemployment and labor market activity. The results show that the pandemic\ncaused a substantial reduction in economic activity, which persisted for nearly\ntwo years. We also find that the decline in economic activity was more pronounced\nin states with higher levels of economic uncertainty. The reduction in economic\nactivity was concentrated in the service and manufacturing sectors, which\nexhibited large negative effects on aggregate demand. The reduction in demand\nwas primarily driven by a decline in private investment, which also led to\ndeclines in private consumption and net exports. In contrast",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13924050632911392,
          "p": 0.13414634146341464,
          "f": 0.1366459577346555
        },
        "rouge-2": {
          "r": 0.028846153846153848,
          "p": 0.024,
          "f": 0.026200868404493607
        },
        "rouge-l": {
          "r": 0.13924050632911392,
          "p": 0.13414634146341464,
          "f": 0.1366459577346555
        }
      }
    },
    {
      "paper_id": "cs.CL.econ/GN/2412.15239v1",
      "true_abstract": "Understanding when and why consumers engage with stories is crucial for\ncontent creators and platforms. While existing theories suggest that audience\nbeliefs of what is going to happen should play an important role in engagement\ndecisions, empirical work has mostly focused on developing techniques to\ndirectly extract features from actual content, rather than capturing\nforward-looking beliefs, due to the lack of a principled way to model such\nbeliefs in unstructured narrative data. To complement existing feature\nextraction techniques, this paper introduces a novel framework that leverages\nlarge language models to model audience forward-looking beliefs about how\nstories might unfold. Our method generates multiple potential continuations for\neach story and extracts features related to expectations, uncertainty, and\nsurprise using established content analysis techniques. Applying our method to\nover 30,000 book chapters from Wattpad, we demonstrate that our framework\ncomplements existing feature engineering techniques by amplifying their\nmarginal explanatory power on average by 31%. The results reveal that different\ntypes of engagement-continuing to read, commenting, and voting-are driven by\ndistinct combinations of current and anticipated content features. Our\nframework provides a novel way to study and explore how audience\nforward-looking beliefs shape their engagement with narrative media, with\nimplications for marketing strategy in content-focused industries.",
      "generated_abstract": "r explores the use of computational models to understand the\nconventional wisdom that large language models (LLMs) are biased towards\ninformation from their training data, and that this biases their reasoning.\nUsing an online experiment, we demonstrate that LLMs can be trained to\nsystematically favor certain kinds of information over others. We also show that\nthe bias is not just due to training data, but also depends on the type of\nquestion being asked. Furthermore, we show that the bias is not limited to\nspecific types of questions, but is present in all types of questions asked by\nthe LLMs. Finally, we show that this bias is present even when the LLM is not\nactively biased towards biased information, but only when it is trained with\nbiased training data. The results of our study provide a deeper understanding\nof the biases that exist in large language models, and point to potential ways\nto address",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2127659574468085,
          "p": 0.32608695652173914,
          "f": 0.25751072483486537
        },
        "rouge-2": {
          "r": 0.04081632653061224,
          "p": 0.057971014492753624,
          "f": 0.04790418676754325
        },
        "rouge-l": {
          "r": 0.19148936170212766,
          "p": 0.29347826086956524,
          "f": 0.23175965187349198
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.06619v1",
      "true_abstract": "We study the problem of synthetic generation of samples of environmental\nfeatures for autonomous vehicle navigation. These features are described by a\nspatiotemporally varying scalar field that we refer to as a threat field. The\nthreat field is known to have some underlying dynamics subject to process\nnoise. Some \"real-world\" data of observations of various threat fields are also\navailable. The assumption is that the volume of ``real-world'' data is\nrelatively small. The objective is to synthesize samples that are statistically\nsimilar to the data. The proposed solution is a generative artificial\nintelligence model that we refer to as a split variational recurrent neural\nnetwork (S-VRNN). The S-VRNN merges the capabilities of a variational\nautoencoder, which is a widely used generative model, and a recurrent neural\nnetwork, which is used to learn temporal dependencies in data. The main\ninnovation in this work is that we split the latent space of the S-VRNN into\ntwo subspaces. The latent variables in one subspace are learned using the\n``real-world'' data, whereas those in the other subspace are learned using the\ndata as well as the known underlying system dynamics. Through numerical\nexperiments we demonstrate that the proposed S-VRNN can synthesize data that\nare statistically similar to the training data even in the case of very small\nvolume of ``real-world'' training data.",
      "generated_abstract": "The advent of massive data and the rise of big data-driven applications\nhas increased the demand for intelligent, adaptive, and efficient wireless\nsystems. In this paper, we propose a novel algorithmic framework for adaptive\nspectrum sharing in multi-cell multi-user massive MIMO systems. The proposed\nframework leverages the adaptive signal processing techniques to dynamically\nadapt the allocation of spectrum among users in the system. Specifically, the\nproposed algorithmic framework utilizes the spectrum sharing ratio to allocate\nspectrum among users. The proposed algorithmic framework is based on the\nspectral efficiency maximization principle and the signal-to-interference-plus-noise\nratio maximization principle. The performance of the proposed algorithmic\nframework is analyzed through the simulation results. The simulation results\ndemonstrate that the proposed algorithmic framework outperforms existing\nproposed algorithms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14414414414414414,
          "p": 0.22857142857142856,
          "f": 0.1767955753670524
        },
        "rouge-2": {
          "r": 0.031578947368421054,
          "p": 0.057692307692307696,
          "f": 0.04081632195844376
        },
        "rouge-l": {
          "r": 0.14414414414414414,
          "p": 0.22857142857142856,
          "f": 0.1767955753670524
        }
      }
    },
    {
      "paper_id": "math.RA.math/RA/2503.08615v1",
      "true_abstract": "Let $H$ be a multiplicative monoid and $\\mathcal{P}_{{\\rm fin},1}(H)$ be the\nmonoid obtained by endowing the family of all non-empty finite subsets of $H$\ncontaining the identity $1_H$ with the operation of setwise multiplication\ninduced by $H$. We study fundamental aspects of the arithmetic of these\nmonoids, taking into account the possible presence of nontrivial idempotents.\nWe consider for this purpose minimal factorizations into irreducibles, a\nconcept recently introduced in the abstract context of a new general theory of\nfactorization. Among other results, we provide necessary and sufficient\nconditions on $H$ for $\\mathcal{P}_{{\\rm fin},1}(H)$ to admit unique minimal\nfactorizations. Our results generalize and shed new light on recent\ndevelopments on the topic.",
      "generated_abstract": "igate the asymptotic behavior of the solution to the\nsystem of linear equations\n$\\frac{1}{2} A_k (t) \\frac{d^2 \\lambda_k (t)}{dt^2} +\n\\lambda_k (t) = 0$ where $\\lambda_k (t)$ are the eigenvalues of the\nHamiltonian $H_k (t)$ and $A_k (t)$ are the $2 \\times 2$ matrices that\ndetermine the dynamics. We show that the solution to this system is given by\n$\\lambda_k (t) = \\frac{1}{2} (A_k (t) + A_k (t)^T)$. We then consider the\nspectral problem for the system of linear equations $\\frac{1}{2} A_k (t)\n\\frac{d^2 \\lambda_k (t)}{dt^2} + \\",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11538461538461539,
          "p": 0.18,
          "f": 0.14062499523925798
        },
        "rouge-2": {
          "r": 0.009174311926605505,
          "p": 0.014285714285714285,
          "f": 0.011173179594896072
        },
        "rouge-l": {
          "r": 0.10256410256410256,
          "p": 0.16,
          "f": 0.12499999523925799
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2501.07828v1",
      "true_abstract": "To trade tokens in cryptoeconomic systems, automated market makers (AMMs)\ntypically rely on liquidity providers (LPs) that deposit tokens in exchange for\nrewards. To profit from such rewards, LPs must use effective liquidity\nprovisioning strategies. However, LPs lack guidance for developing such\nstrategies, which often leads them to financial losses. We developed a\nmeasurement model based on impermanent loss to analyze the influences of key\nparameters (i.e., liquidity pool type, position duration, position range size,\nand position size) of liquidity provisioning strategies on LPs' returns. To\nreveal the influences of those key parameters on LPs' profits, we used the\nmeasurement model to analyze 700 days of historical liquidity provision data of\nUniswap v3. By uncovering the influences of key parameters of liquidity\nprovisioning strategies on profitability, this work supports LPs in developing\nmore profitable strategies.",
      "generated_abstract": "This paper presents a novel framework for the analysis of credit risk in\nthe context of the stochastic volatility model. We propose a novel approach to\nmodel the stochastic volatility parameter and derive the resulting time\nseries from a parametric formulation. In addition, we propose a methodology to\ncompute the implied volatility, and we introduce a new option pricing model\nbased on this implied volatility. We show the effectiveness of the proposed\nmethodology in the pricing of credit default swaps and in the calibration of\nstochastic volatility models. We also discuss the application of our methodology\nto the pricing of credit default swaps and other financial derivatives.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.25862068965517243,
          "f": 0.20270269793645007
        },
        "rouge-2": {
          "r": 0.01680672268907563,
          "p": 0.022222222222222223,
          "f": 0.019138751077128608
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.25862068965517243,
          "f": 0.20270269793645007
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2410.18024v1",
      "true_abstract": "Systems Biology Graphical Notation (SBGN) is a standardised notational system\nthat visualises biochemical processes as networks. These visualizations lack a\nformal framework, so that the analysis of such networks through modelling and\nsimulation is an entirely separate task, determined by a chosen modelling\nframework (e.g. differential equations, Petri nets, stochastic processes,\ngraphs). A second research gap is the lack of a mathematical framework to\ncompose network representations. The complexity of molecular and cellular\nprocesses forces experimental studies to focus on subsystems. To study the\nfunctioning of biological systems across levels of structural and functional\norganisation, we require tools to compose and organise networks with different\nlevels of detail and abstraction.\n  We address these challenges by introducing a category-theoretic formalism for\nbiochemical processes visualised using SBGN Process Description (SBGN-PD)\nlanguage. Using the theory of structured cospans, we construct a symmetric\nmonoidal double category and demonstrate its horizontal 1-morphisms as SBGN\nProcess Descriptions. We obtain organisational principles such as\n'compositionality' (building a large SBGN-PD from smaller ones) and\n'zooming-out' (abstracting away details in biochemical processes) defined in\ncategory-theoretic terms. We also formally investigate how a particular portion\nof a biochemical network influences the remaining portion of the network and\nvice versa. Throughout the paper, we illustrate our findings using standard\nSBGN-PD examples.",
      "generated_abstract": "tion of bacterial biofilms is a crucial step in pathogenesis. This\napplication of mathematical modeling aims to understand the evolution of\nbiofilms, specifically the relationship between the growth rate of bacteria and\ntheir ability to form biofilms. In this study, we use the model proposed by\nBaumann et al. (2011) to understand the evolution of biofilms. This model\nprovides a mathematical description of how the growth rate of bacteria is\nincreased by the formation of biofilms. This model is important for understanding\nthe evolution of biofilms. The model is a stochastic process with a\nself-consistent equation for the growth rate of bacteria. The model allows for\ndifferent growth rates of bacteria, which can lead to different growth rates of\nbiofilms. We investigate how different growth rates of bacteria lead to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12162162162162163,
          "p": 0.27692307692307694,
          "f": 0.16901408026626122
        },
        "rouge-2": {
          "r": 0.014705882352941176,
          "p": 0.030303030303030304,
          "f": 0.01980197579845211
        },
        "rouge-l": {
          "r": 0.12162162162162163,
          "p": 0.27692307692307694,
          "f": 0.16901408026626122
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/SC/2501.13639v2",
      "true_abstract": "Droplet formation has emerged as an essential concept for the spatiotemporal\norganisation of biomolecules in cells. However, classical descriptions of\ndroplet dynamics based on passive liquid-liquid phase separation cannot capture\nthe complex situations inside cells. This review discusses three general\naspects that are crucial in cells: (i) biomolecules are diverse and\nindividually complex, implying that cellular droplets posses complex internal\nbehaviour, e.g., in terms of their material properties; (ii) the cellular\nenvironment contains many solid-like structures that droplets can wet; (iii)\ncells are alive and use fuel to drive processes out of equilibrium. We\nillustrate how these principles control droplet nucleation, growth, position,\nand count to unveil possible regulatory mechanisms in biological cells and\nother applications of phase separation.",
      "generated_abstract": "very of the double helix structure of DNA has provided the basis\nfor the development of DNA sequencing, genetic engineering, and genomics.\nHowever, the understanding of the physical processes that occur in the\ncomplementary strands of DNA remains limited. In this work, we propose a\ngeneralization of the Helix Model that captures the physical mechanisms that\ndrive the sequencing of DNA molecules, with the goal of developing a novel\napproach to DNA sequencing. Our framework includes both DNA double strand\nconformation and the formation of a twisted helix. The proposed model\nexplicitly accounts for the interactions between the base pairs of the\nsequencing strand and the twist, which are the driving forces behind the\nsequencing of the complementary strand. The model is validated using numerical\nsimulations and experimental data, demonstrating that it captures key\nphysical principles involved in the sequencing of DNA",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1595744680851064,
          "p": 0.18072289156626506,
          "f": 0.16949152044304017
        },
        "rouge-2": {
          "r": 0.008403361344537815,
          "p": 0.008130081300813009,
          "f": 0.008264457811286405
        },
        "rouge-l": {
          "r": 0.11702127659574468,
          "p": 0.13253012048192772,
          "f": 0.12429378033004584
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.06873v1",
      "true_abstract": "We analyze over 44,000 NBER and CEPR working papers from 1980 to 2023 using a\ncustom language model to construct knowledge graphs that map economic concepts\nand their relationships. We distinguish between general claims and those\ndocumented via causal inference methods (e.g., DiD, IV, RDD, RCTs). We document\na substantial rise in the share of causal claims-from roughly 4% in 1990 to\nnearly 28% in 2020-reflecting the growing influence of the \"credibility\nrevolution.\" We find that causal narrative complexity (e.g., the depth of\ncausal chains) strongly predicts both publication in top-5 journals and higher\ncitation counts, whereas non-causal complexity tends to be uncorrelated or\nnegatively associated with these outcomes. Novelty is also pivotal for top-5\npublication, but only when grounded in credible causal methods: introducing\ngenuinely new causal edges or paths markedly increases both the likelihood of\nacceptance at leading outlets and long-run citations, while non-causal novelty\nexhibits weak or even negative effects. Papers engaging with central, widely\nrecognized concepts tend to attract more citations, highlighting a divergence\nbetween factors driving publication success and long-term academic impact.\nFinally, bridging underexplored concept pairs is rewarded primarily when\ngrounded in causal methods, yet such gap filling exhibits no consistent link\nwith future citations. Overall, our findings suggest that methodological rigor\nand causal innovation are key drivers of academic recognition, but sustained\nimpact may require balancing novel contributions with conceptual integration\ninto established economic discourse.",
      "generated_abstract": "This paper introduces a framework for analyzing the potential impact of\nfederal and state policies on the economic performance of local governments.\nThe framework is based on the principle of \"equitable federalism\" that balances\nthe competing interests of federalism and stateism. The framework is illustrated\nwith an example from the United States. The paper shows that federalism is not\nnecessarily a bad thing, but it should be managed carefully. The paper\nhighlights the importance of understanding the local context and balancing\nfederalism and stateism to achieve the best possible outcomes.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09090909090909091,
          "p": 0.26666666666666666,
          "f": 0.13559321654696938
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.26666666666666666,
          "f": 0.13559321654696938
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10126v1",
      "true_abstract": "For a regularized least squares estimation of discrete-valued signals, we\npropose an LiGME regularizer, as a nonconvex regularizer, of designated\nisolated minimizers. The proposed regularizer is designed as a Generalized\nMoreau Enhancement (GME) of the so-called SOAV convex regularizer. Every\ncandidate vector in the discrete-valued set is aimed to be assigned to an\nisolated local minimizer of the proposed regularizer while the overall\nconvexity of the regularized least squares model is maintained. Moreover, a\nglobal minimizer of the proposed model can be approximated iteratively by using\na variant of cLiGME algorithm. To enhance the accuracy of the proposed\nestimation, we also propose a pair of simple modifications, called respectively\nan iterative reweighting and a generalized superiorization. Numerical\nexperiments demonstrate the effectiveness of the proposed model and algorithms\nin a scenario of MIMO signal detection.",
      "generated_abstract": "This paper proposes an efficient and effective distributed energy resource\nsystem (DERS) based on the power splitting scheme. The proposed scheme\nincorporates a novel distributed energy resource management (DERM) algorithm,\nwhich is based on a distributed power splitting scheme. The proposed scheme\nenables the decentralized operation of energy storage and power conversion\ndevices. Moreover, it allows the energy storage to dynamically adjust its\ncharge/discharge current to maximize its storage capacity. This paper\nproposes an efficient algorithm to solve the problem of the power splitting\nscheme, which is implemented to minimize the energy consumption and maximize the\nstored energy. Furthermore, the proposed algorithm is applied to solve the\nproblem of the proposed DERS, which is implemented to minimize the power loss\nand maximize the energy efficiency. Simulation results show that the proposed\nproposed scheme achieves an energy efficiency of 89.88%.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13414634146341464,
          "p": 0.16666666666666666,
          "f": 0.14864864370708564
        },
        "rouge-2": {
          "r": 0.025210084033613446,
          "p": 0.028846153846153848,
          "f": 0.02690582461903609
        },
        "rouge-l": {
          "r": 0.13414634146341464,
          "p": 0.16666666666666666,
          "f": 0.14864864370708564
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.09828v1",
      "true_abstract": "Deep learning has significantly advanced medical imaging analysis, yet\nvariations in image resolution remain an overlooked challenge. Most methods\naddress this by resampling images, leading to either information loss or\ncomputational inefficiencies. While solutions exist for specific tasks, no\nunified approach has been proposed. We introduce a resolution-invariant\nautoencoder that adapts spatial resizing at each layer in the network via a\nlearned variable resizing process, replacing fixed spatial down/upsampling at\nthe traditional factor of 2. This ensures a consistent latent space resolution,\nregardless of input or output resolution. Our model enables various downstream\ntasks to be performed on an image latent whilst maintaining performance across\ndifferent resolutions, overcoming the shortfalls of traditional methods. We\ndemonstrate its effectiveness in uncertainty-aware super-resolution,\nclassification, and generative modelling tasks and show how our method\noutperforms conventional baselines with minimal performance loss across\nresolutions.",
      "generated_abstract": "opment of large language models (LLMs) has revolutionized many\napplications, such as text-to-image generation, text-to-speech conversion, and\nmulti-agent dialogue management. However, these models still struggle with\ncomplex and challenging tasks, such as generating realistic human-like\ncharacters. To address this, we propose DANET, a novel framework that\nintegrates a dynamic attention network (DAN) with an end-to-end LLM for\ngenerating human-like characters. Specifically, DANET dynamically adjusts the\nattention weights of the LLM to focus on specific text-input tokens to enhance\nthe generation quality. To this end, we propose a novel LLM-augmented\nattention mechanism that explicitly models the dependencies among different\ncharacters. Additionally, we introduce a novel attention-based dynamic\nattention network (DAN) that captures the temporal dependencies",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1592920353982301,
          "p": 0.22784810126582278,
          "f": 0.18749999515679264
        },
        "rouge-2": {
          "r": 0.007246376811594203,
          "p": 0.009523809523809525,
          "f": 0.008230447767111598
        },
        "rouge-l": {
          "r": 0.1504424778761062,
          "p": 0.21518987341772153,
          "f": 0.177083328490126
        }
      }
    },
    {
      "paper_id": "math.FA.math/OA/2502.19028v2",
      "true_abstract": "We give a new proof of the Weyl-von Neumann-Berg theorem. Our proof improves\nHalmos' proof in 1972 by observing the fact that every compact set in the\ncomplex plane is the continuous image of a compact set in the real line.",
      "generated_abstract": "In this paper, we prove that for any positive integer $n$, the $n$-th\ncharacter of the group of all matrices with entries in a finite field has a\nunique irreducible representation of length $n$.\n  We also prove that the representation of the $n$-th character of the\nmultiplicative group of a finite field is unique up to an integer multiple of\nthe identity element.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2413793103448276,
          "p": 0.175,
          "f": 0.20289854585171196
        },
        "rouge-2": {
          "r": 0.05405405405405406,
          "p": 0.0392156862745098,
          "f": 0.04545454058109557
        },
        "rouge-l": {
          "r": 0.20689655172413793,
          "p": 0.15,
          "f": 0.17391303860533516
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2502.01461v1",
      "true_abstract": "Computational prediction of enzymatic reactions represents a crucial\nchallenge in sustainable chemical synthesis across various scientific domains,\nranging from drug discovery to materials science and green chemistry. These\nsyntheses rely on proteins that selectively catalyze complex molecular\ntransformations. These protein catalysts exhibit remarkable substrate\nadaptability, with the same protein often catalyzing different chemical\ntransformations depending on its molecular partners. Current approaches to\nprotein representation in reaction prediction either ignore protein structure\nentirely or rely on static embeddings, failing to capture how proteins\ndynamically adapt their behavior to different substrates. We present\nDocking-Aware Attention (DAA), a novel architecture that generates dynamic,\ncontext-dependent protein representations by incorporating molecular docking\ninformation into the attention mechanism. DAA combines physical interaction\nscores from docking predictions with learned attention patterns to focus on\nprotein regions most relevant to specific molecular interactions. We evaluate\nour method on enzymatic reaction prediction, where it outperforms previous\nstate-of-the-art methods, achieving 62.2\\% accuracy versus 56.79\\% on complex\nmolecules and 55.54\\% versus 49.45\\% on innovative reactions. Through detailed\nablation studies and visualizations, we demonstrate how DAA generates\ninterpretable attention patterns that adapt to different molecular contexts.\nOur approach represents a general framework for context-aware protein\nrepresentation in biocatalysis prediction, with potential applications across\nenzymatic synthesis planning. We open-source our implementation and pre-trained\nmodels to facilitate further research.",
      "generated_abstract": "asing complexity of biological data necessitates the development of\ncomputational methods that enable the integration of multiple data types into\na unified representation, enabling the analysis of complex biological systems\nwith greater efficiency. In this paper, we introduce a novel approach to\nintegrate multiple types of data into a single representation. Our approach\ncombines two distinct approaches: (1) data fusion and (2) feature\nrepresentation. Data fusion combines multiple datasets through a similarity\nmeasure to generate a new dataset. The new dataset is then used as a basis for\nfeature representation, which produces a vector of features for each\nsample. This vector of features is then used to train a classification model\nusing a logistic regression classifier. The combination of these two approaches\nenables the integration of multiple types of data into a unified representation\nfor classification, ensuring that the classification model is trained on a\nsingle dataset that combines multiple types of data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11764705882352941,
          "p": 0.21428571428571427,
          "f": 0.1518987296010256
        },
        "rouge-2": {
          "r": 0.009433962264150943,
          "p": 0.015267175572519083,
          "f": 0.011661802859014737
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.21428571428571427,
          "f": 0.1518987296010256
        }
      }
    },
    {
      "paper_id": "eess.SP.math/IT/2503.10472v1",
      "true_abstract": "In this letter, we propose to deploy rotatable antennas (RAs) at the base\nstation (BS) to enhance both communication and sensing (C&S) performances, by\nexploiting a new spatial degree-of-freedom (DoF) offered by array rotation.\nSpecifically, we formulate a multi-objective optimization problem to\nsimultaneously maximize the sum-rate of multiple communication users and\nminimize the Cram\\'er-Rao bound (CRB) for target angle estimation, by jointly\noptimizing the transmit beamforming vectors and the array rotation angle at the\nBS. To solve this problem, we first equivalently decompose it into two\nsubproblems, corresponding to an inner problem for beamforming optimization and\nan outer problem for array rotation optimization. Although these two\nsubproblems are non-convex, we obtain their high-quality solutions by applying\nthe block coordinate descent (BCD) technique and one-dimensional exhaustive\nsearch, respectively. Moreover, we show that for the communication-only case,\nRAs provide an additional rotation gain to improve communication performance;\nwhile for the sensing-only case, the equivalent spatial aperture can be\nenlarged by RAs for achieving higher sensing accuracy. Finally, numerical\nresults are presented to showcase the performance gains of RAs over\nfixed-rotation antennas in integrated sensing and communications (ISAC).",
      "generated_abstract": "er the problem of joint source-channel coding with joint\ndistributions for the source and channel. The source has finite energy,\nwhich is subject to constraints. The channel is assumed to have a finite\ncapacity. The capacity is characterized by a finite-dimensional distribution.\nThis work studies the problem of joint source-channel coding with\nconstraint-satisfying source and channel distributions. The capacity region is\ndefined as the set of all joint source-channel codes that satisfy the constraints.\nWe determine the capacity region in two scenarios: when the source has a\nfinite energy and when the channel has a finite capacity. In the first\nscenario, we show that the capacity region is a convex polytope, and that the\nminimum value of the capacity region is attained by the joint source-channel\ncode that minimizes the sum of the logarithms of the source and channel\ndistributions. In the second scenario, we show that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12096774193548387,
          "p": 0.234375,
          "f": 0.15957446359438673
        },
        "rouge-2": {
          "r": 0.01675977653631285,
          "p": 0.02727272727272727,
          "f": 0.020761240959759692
        },
        "rouge-l": {
          "r": 0.12096774193548387,
          "p": 0.234375,
          "f": 0.15957446359438673
        }
      }
    },
    {
      "paper_id": "physics.optics.q-bio/CB/2412.16427v1",
      "true_abstract": "Compressed streak imaging (CSI), introduced in 2014, has proven to be a\npowerful imaging technology for recording ultrafast phenomena such as light\npropagation and fluorescence lifetimes at over 150 trillion frames per second.\nDespite these achievements, CSI has faced challenges in detecting subtle\nintensity fluctuations in slow-moving, continuously illuminated objects. This\nlimitation, largely attributable to high streak compression and motion blur,\nhas curtailed broader adoption of CSI in applications such as cellular\nfluorescence microscopy. To address these issues and expand the utility of CSI,\nwe present a novel encoding strategy, termed two-axis compressed streak imaging\n(TACSI) that results in significant improvements to the reconstructed image\nfidelity. TACSI introduces a second scanning axis which shuttles a conjugate\nimage of the object with respect to the coded aperture. The moving image\ndecreases the streak compression ratio and produces a flash and shutter\nphenomenon that reduces coded aperture motion blur, overcoming the limitations\nof current CSI technologies. We support this approach with an analytical model\ndescribing the two-axis streak compression ratio, along with both simulated and\nempirical measurements. As proof of concept, we demonstrate the ability of\nTACSI to measure rapid variations in cell membrane potentials using\nvoltage-sensitive dye, which were previously unattainable with conventional\nCSI. This method has broad implications for high-speed photography, including\nthe visualization of action potentials, muscle contractions, and enzymatic\nreactions that occur on microsecond and faster timescales using fluorescence\nmicroscopy.",
      "generated_abstract": "t development of the quantum dot laser (QDL) technology has\npromoted the use of quantum dots as light sources, opening new avenues for\nbio-imaging applications. The use of QDLs in biological systems, however,\nrequires an understanding of the fundamental physics and biological\nrequirements of the system, including the quantum dot's intrinsic properties,\nthe quantum dot's coupling to the biological system, and the coupling to the\nenvironment. Here, we present a comprehensive review of the current\ntechnological advancements and theoretical developments in the field,\nhighlighting the key challenges and advancements in the field. We begin by\nintroducing the quantum dot as a light source, including the fundamental\nprinciples of the quantum dot and its coupling to the light source. We then\ndiscuss the quantum dot's intrinsic properties, including its optical\nproperties, the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12578616352201258,
          "p": 0.2857142857142857,
          "f": 0.17467248483819922
        },
        "rouge-2": {
          "r": 0.026905829596412557,
          "p": 0.05660377358490566,
          "f": 0.03647415976607807
        },
        "rouge-l": {
          "r": 0.10062893081761007,
          "p": 0.22857142857142856,
          "f": 0.13973798702160536
        }
      }
    },
    {
      "paper_id": "cs.IT.stat/OT/2402.08135v1",
      "true_abstract": "Since its introduction in 2011, the partial information decomposition (PID)\nhas triggered an explosion of interest in the field of multivariate information\ntheory and the study of emergent, higher-order (\"synergistic\") interactions in\ncomplex systems. Despite its power, however, the PID has a number of\nlimitations that restrict its general applicability: it scales poorly with\nsystem size and the standard approach to decomposition hinges on a definition\nof \"redundancy\", leaving synergy only vaguely defined as \"that information not\nredundant.\" Other heuristic measures, such as the O-information, have been\nintroduced, although these measures typically only provided a summary statistic\nof redundancy/synergy dominance, rather than direct insight into the synergy\nitself. To address this issue, we present an alternative decomposition that is\nsynergy-first, scales much more gracefully than the PID, and has a\nstraightforward interpretation. Our approach defines synergy as that\ninformation in a set that would be lost following the minimally invasive\nperturbation on any single element. By generalizing this idea to sets of\nelements, we construct a totally ordered \"backbone\" of partial synergy atoms\nthat sweeps systems scales. Our approach starts with entropy, but can be\ngeneralized to the Kullback-Leibler divergence, and by extension, to the total\ncorrelation and the single-target mutual information. Finally, we show that\nthis approach can be used to decompose higher-order interactions beyond just\ninformation theory: we demonstrate this by showing how synergistic combinations\nof pairwise edges in a complex network supports signal communicability and\nglobal integration. We conclude by discussing how this perspective on\nsynergistic structure (information-based or otherwise) can deepen our\nunderstanding of part-whole relationships in complex systems.",
      "generated_abstract": "eld of wireless networks, accurate prediction of channel state\ninformation (CSI) is crucial for accurate estimation of channel coefficients.\nHowever, the CSI prediction process is often complex, involving many steps,\nsuch as channel estimation, channel estimation feedback, channel estimation\nfeedback, and channel estimation feedback, etc. This process is often referred\nto as the CSI prediction pipeline. This paper proposes a novel and\nefficient-in-practice CSI prediction pipeline based on a generative model to\nspeed up the CSI prediction process. This model can generate CSI in an\narbitrary sequence of feedbacks, thereby reducing the number of feedbacks\nrequired to estimate the CSI. In addition, this paper proposes a novel\nconvolutional neural network (CNN) model for the CSI prediction pipeline,\nwhich is efficient and has low computational complexity. Simulation results\ndemonstrate that the proposed model significantly reduces the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11242603550295859,
          "p": 0.24050632911392406,
          "f": 0.15322580211010678
        },
        "rouge-2": {
          "r": 0.01968503937007874,
          "p": 0.043859649122807015,
          "f": 0.02717390876713206
        },
        "rouge-l": {
          "r": 0.11242603550295859,
          "p": 0.24050632911392406,
          "f": 0.15322580211010678
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2411.01319v1",
      "true_abstract": "This paper addresses the estimation of the systemic risk measure known as\nCoVaR, which quantifies the risk of a financial portfolio conditional on\nanother portfolio being at risk. We identify two principal challenges:\nconditioning on a zero-probability event and the repricing of portfolios. To\ntackle these issues, we propose a decoupled approach utilizing smoothing\ntechniques and develop a model-independent theoretical framework grounded in a\nfunctional perspective. We demonstrate that the rate of convergence of the\ndecoupled estimator can achieve approximately $O_{\\rm P}(\\Gamma^{-1/2})$, where\n$\\Gamma$ represents the computational budget. Additionally, we establish the\nsmoothness of the portfolio loss functions, highlighting its crucial role in\nenhancing sample efficiency. Our numerical results confirm the effectiveness of\nthe decoupled estimators and provide practical insights for the selection of\nappropriate smoothing techniques.",
      "generated_abstract": "y investigates the evolution of volatility, risk, and liquidity\nin the cryptocurrency market. We employ the Bayesian latent variable\n(Bayes-LV) model to analyze the evolving relationships between the price of\ncryptocurrencies, the associated risk, and liquidity. Our results demonstrate\nthat the BLV model successfully captures the complex interplay between risk,\nvolatility, and liquidity in cryptocurrency markets. The model's robustness\nis demonstrated by its ability to successfully capture the evolution of these\nkey metrics, even in the presence of outliers and temporal dependencies. The\nfindings of this study provide valuable insights into the evolving relationship\nbetween cryptocurrencies, risk, and liquidity, and offer valuable insights into\nthe dynamics of the cryptocurrency market. The results of this study can be\nused to inform policymakers and investors about the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.19444444444444445,
          "f": 0.17177913617223095
        },
        "rouge-2": {
          "r": 0.024793388429752067,
          "p": 0.027777777777777776,
          "f": 0.02620086837855971
        },
        "rouge-l": {
          "r": 0.13186813186813187,
          "p": 0.16666666666666666,
          "f": 0.14723925887161746
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.19370v1",
      "true_abstract": "In this project, we propose a Variational Inference algorithm to approximate\nposterior distributions. Building on prior methods, we develop the\nGradient-Steered Stein Variational Gradient Descent (G-SVGD) approach. This\nmethod introduces a novel loss function that combines a weighted gradient and\nthe Evidence Lower Bound (ELBO) to enhance convergence speed and accuracy. The\nlearning rate is determined through a suboptimal minimization of this loss\nfunction within a gradient descent framework.\n  The G-SVGD method is compared against the standard Stein Variational Gradient\nDescent (SVGD) approach, employing the ADAM optimizer for learning rate\nadaptation, as well as the Markov Chain Monte Carlo (MCMC) method. We assess\nperformance in two wave prospection models representing low-contrast and\nhigh-contrast subsurface scenarios. To achieve robust numerical approximations\nin the forward model solver, a five-point operator is employed, while the\nadjoint method improves accuracy in computing gradients of the log posterior.\n  Our findings demonstrate that G-SVGD accelerates convergence and offers\nimproved performance in scenarios where gradient evaluation is computationally\nexpensive. The abstract highlights the algorithm's applicability to wave\nprospection models and its potential for broader applications in Bayesian\ninference. Finally, we discuss the benefits and limitations of G-SVGD,\nemphasizing its contribution to advancing computational efficiency in\nuncertainty quantification.",
      "generated_abstract": "t a new method for inference in the framework of multivariate\ncomplex Gaussian distributions with a mean vector and covariance matrix that\ndepend on a parameter of interest. This method is based on a variational\napproximation of the Kullback-Leibler divergence. As a key feature, the\napproximation is unbiased, in the sense that the maximum likelihood estimator\nis obtained when the variational approximation is optimal. We provide a\ntheoretical analysis of the method, which includes a finite-sample bound on the\nmaximum likelihood estimator, as well as a practical recommendation for the\nselection of the number of components of the variational approximation. The\nproposed method can be used to estimate the mean and covariance of a multivariate\nGaussian distribution with unknown covariance, as well as to estimate the mean\nand covariance of a multivariate Gaussian distribution with a known",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13970588235294118,
          "p": 0.2676056338028169,
          "f": 0.1835748747200636
        },
        "rouge-2": {
          "r": 0.031413612565445025,
          "p": 0.05454545454545454,
          "f": 0.03986710499663413
        },
        "rouge-l": {
          "r": 0.1323529411764706,
          "p": 0.2535211267605634,
          "f": 0.17391303897127136
        }
      }
    },
    {
      "paper_id": "physics.med-ph.physics/med-ph/2503.09505v1",
      "true_abstract": "Photoacoustic (PA) imaging of deep tissue tends to employ Q-switched lasers\nwith high pulse energy to generate high optical fluence and therefore high PA\nsignal. Compared to Q-switched lasers, pulsed laser diodes (PLDs) typically\ngenerate low pulse energy. In PA imaging applications with strong acoustic\nattenuation, such as through human skull bone, the broadband PA waves generated\nby nanoseconds laser pulses are significantly reduced in bandwidth during their\npropagation to a detector. As high-frequency PA signal components are not\ntransmitted through skull, we propose to not generate them by increasing\nexcitation pulse duration. Because PLDs are mainly limited in their peak power\noutput, an increase in pulse duration linearly increases pulse energy and\ntherefore PA signal amplitude. Here we show that the optimal pulse duration for\ndeep PA sensing through thick skull bone is far higher than in typical PA\napplications. Counterintuitively, this makes PLD excitation well-suited for\ntranscranial photoacoustics. We show this in PA sensing experiments on ex vivo\nhuman skull bone.",
      "generated_abstract": "In this paper, we develop a novel method for identifying the underlying\nsignals from a complex medical image using a deep learning framework. Our\napproach employs a Convolutional Neural Network (CNN) architecture, which is\ntailored to the specific characteristics of medical images. The CNN model\ncaptures the image structure and generates a set of feature vectors for each\nimage pixel, which are then used to train a classifier to identify the\noriginal signal. Our method is tested on a dataset of 2D images acquired from\nbreast cancer patients. The results demonstrate that the proposed approach\noutperforms existing methods in terms of both accuracy and robustness. The\ncode for the implementation is available at https://github.com/AliShahbaz/Med\nImaging-Signal-Identification-CNN.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14953271028037382,
          "p": 0.19047619047619047,
          "f": 0.16753926208821043
        },
        "rouge-2": {
          "r": 0.006578947368421052,
          "p": 0.008695652173913044,
          "f": 0.007490631800140679
        },
        "rouge-l": {
          "r": 0.14953271028037382,
          "p": 0.19047619047619047,
          "f": 0.16753926208821043
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.16794v1",
      "true_abstract": "Auditory foundation models, including auditory large language models (LLMs),\nprocess all sound inputs equally, independent of listener perception. However,\nhuman auditory perception is inherently selective: listeners focus on specific\nspeakers while ignoring others in complex auditory scenes. Existing models do\nnot incorporate this selectivity, limiting their ability to generate\nperception-aligned responses. To address this, we introduce Intention-Informed\nAuditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM\n(AAD-LLM), a prototype system that integrates brain signals to infer listener\nattention. AAD-LLM extends an auditory LLM by incorporating intracranial\nelectroencephalography (iEEG) recordings to decode which speaker a listener is\nattending to and refine responses accordingly. The model first predicts the\nattended speaker from neural activity, then conditions response generation on\nthis inferred attentional state. We evaluate AAD-LLM on speaker description,\nspeech transcription and extraction, and question answering in multitalker\nscenarios, with both objective and subjective ratings showing improved\nalignment with listener intention. By taking a first step toward\nintention-aware auditory AI, this work explores a new paradigm where listener\nperception informs machine listening, paving the way for future\nlistener-centered auditory systems. Demo and code available:\nhttps://aad-llm.github.io.",
      "generated_abstract": "ork, we propose a novel method for audio source separation using\ncortical auditory representations. The approach is based on the observation\nthat the auditory system reconstructs speech signals from the temporal\nsignals of brain signals. Specifically, the cortical auditory representations\nare generated by the auditory system and are similar to the temporal signals\nof brain signals. To this end, we propose to use the auditory representation\nas a feature descriptor and apply a multilayer perceptron (MLP) on it to\nreconstruct the source signal. To prevent the model from learning the\ntranslation of the source signal to the auditory representation, we design a\ndata augmentation strategy to prevent the model from learning the inverse\ntranslation. Furthermore, we propose to regularize the MLP by using\ncross-entropy loss and adding a weighted sum of negative log-likelihood loss\nterms to the objective function. Experiments",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1506849315068493,
          "p": 0.2894736842105263,
          "f": 0.19819819369531702
        },
        "rouge-2": {
          "r": 0.00546448087431694,
          "p": 0.008620689655172414,
          "f": 0.006688958461765544
        },
        "rouge-l": {
          "r": 0.1506849315068493,
          "p": 0.2894736842105263,
          "f": 0.19819819369531702
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.physics/pop-ph/2501.10701v1",
      "true_abstract": "It is found from textbooks that there are the different versions of the\nschematic diagram related to the Nernst equation, and consequently, it leads to\nsome discussion related to the Nernst equation and the discovery of other\nmeaningful schematic diagrams never appearing in literature. It is also found\nthat through the introduction of a new function, the schematic diagram of the\nNernst equation in the isothermal process of any thermodynamic system can be\ngenerated in a unified way and that the Nernst equation can be re-obtained from\nthe experimental data of low-temperature chemical reactions without any\nartificial additional assumptions. The results obtained here show clearly that\nthe centenary progress from the Nernst theorem to the Nernst statement is\ncompleted.",
      "generated_abstract": "aper, we study the effects of quantum fluctuations on the\nmechanical behavior of nanoporous materials. The underlying physical mechanism\nis based on the non-Markovian dynamics of the fluctuations of the energy\ntransfer rate between the pore and the environment. We find that the\nnon-Markovianity of the pore-environment dynamics leads to significant\ndestabilization of the pore's response to external stimuli, with an associated\ndampening of the pore's response to thermal fluctuations. This non-Markovian\neffect is mediated by the pore's interaction with the environment, which\nleads to non-Markovian dynamics of the pore's energy transfer rate. In\nparticular, we find that the pore's response to thermal fluctuations is\ndestabilized by non-Markovian effects due to the non",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11428571428571428,
          "p": 0.13793103448275862,
          "f": 0.12499999504394552
        },
        "rouge-2": {
          "r": 0.04950495049504951,
          "p": 0.056179775280898875,
          "f": 0.052631573967313494
        },
        "rouge-l": {
          "r": 0.11428571428571428,
          "p": 0.13793103448275862,
          "f": 0.12499999504394552
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.08252v1",
      "true_abstract": "Environmental and mental conditions are known risk factors for dermatitis and\nsymptoms of skin inflammation, but their interplay is difficult to quantify;\nepidemiological studies rarely include both, along with possible confounding\nfactors. Infodemiology leverages large online data sets to address this issue,\nbut fusing them produces strong patterns of spatial and temporal correlation,\nmissingness, and heterogeneity.\n  In this paper, we design a causal network that correctly models these complex\nstructures in large-scale infodemiological data from Google, EPA, NOAA and US\nCensus (434 US counties, 134 weeks). Our model successfully captures known\ncausal relationships between weather patterns, pollutants, mental conditions,\nand dermatitis. Key findings reveal that anxiety accounts for 57.4% of\nexplained variance in dermatitis, followed by NO2 (33.9%), while environmental\nfactors show significant mediation effects through mental conditions. The model\npredicts that reducing PM2.5 emissions by 25% could decrease dermatitis\nprevalence by 18%. Through statistical validation and causal inference, we\nprovide unprecedented insights into the complex interplay between environmental\nand mental health factors affecting dermatitis, offering valuable guidance for\npublic health policies and environmental regulations.",
      "generated_abstract": "variate time-series model (MTM) is a popular approach to the\nmultivariate time-series analysis. In this work, we propose a novel method to\nestimate the MTM parameters. Our method is based on the use of the\nhigh-dimensional multivariate normal distribution (HDMN) to represent the\nmultivariate time-series data. We then apply the generalized method of moments\nto estimate the MTM parameters. In this approach, we use the covariance matrix\nof the HDMN to estimate the covariance matrix of the MTM, which is used to\nestimate the parameters of the MTM. The proposed method is compared to the\nexisting methods such as the linear regression method, the least squares\nmethod, and the Bayesian inference method. The performance of the proposed\nmethod is evaluated through simulation studies. The results show that the\nproposed method is more efficient than the existing methods in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13333333333333333,
          "p": 0.25,
          "f": 0.173913038941399
        },
        "rouge-2": {
          "r": 0.005714285714285714,
          "p": 0.009174311926605505,
          "f": 0.007042248791165645
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.25,
          "f": 0.173913038941399
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2502.12940v1",
      "true_abstract": "Global discrete optimization is notoriously difficult due to the lack of\ngradient information and the curse of dimensionality, making exhaustive search\ninfeasible. Tensor cross approximation is an efficient technique to approximate\nmultivariate tensors (and discretized functions) by tensor product\ndecompositions based on a small number of tensor elements, evaluated on\nadaptively selected fibers of the tensor, that intersect on submatrices of\n(nearly) maximum volume. The submatrices of maximum volume are empirically\nknown to contain large elements, hence the entries selected for cross\ninterpolation can also be good candidates for the globally maximal element\nwithin the tensor. In this paper we consider evolution of epidemics on\nnetworks, and infer the contact network from observations of network nodal\nstates over time. By numerical experiments we demonstrate that the contact\nnetwork can be inferred accurately by finding the global maximum of the\nlikelihood using tensor cross interpolation. The proposed tensor product\napproach is flexible and can be applied to global discrete optimization for\nother problems, e.g. discrete hyperparameter tuning.",
      "generated_abstract": "In this paper, we propose a novel methodology for the identification of\neffective treatments using synthetic data. By integrating a two-stage\nprocedure, our methodology enables the identification of the most effective\ntreatment, which can be used as a baseline for subsequent analyses. In the\nfirst stage, we simulate synthetic data from the underlying treatment distribution\nusing a given treatment effect and a given treatment space. In the second\nstage, we evaluate the performance of the identified treatment by comparing\nthe observed data with the synthetic data. This approach allows us to\nquantify the uncertainty of the identified treatment, which is critical for\napplications where the outcome of interest is time to event. We illustrate the\nproposed methodology with several examples from the fields of oncology and\ncardiology.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1743119266055046,
          "p": 0.25333333333333335,
          "f": 0.20652173430115797
        },
        "rouge-2": {
          "r": 0.025477707006369428,
          "p": 0.035398230088495575,
          "f": 0.029629624762415065
        },
        "rouge-l": {
          "r": 0.1651376146788991,
          "p": 0.24,
          "f": 0.19565216908376665
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.18488v1",
      "true_abstract": "Aquaculture has been the fastest growing food production sector globally due\nto its potential to improve food security, stimulate economic growth, and\nreduce poverty. Its rapid development has been linked to sustainability\nchallenges, many of which are still unresolved and poorly understood.\nSmall-scale producers account for an increasing fraction of aquacultural\noutput. At the same time, many of these producers experience poverty, food\ninsecurity, and rely on unimproved production practices. We develop a stylized\nmathematical model to explore the effects of ecological, social, and economic\nfactors on the dynamics of a small-scale pond aquaculture system. Using\nanalytical and numerical methods, we explore the stability, asymptotic\ndynamics, and bifurcations of the model. Depending on the characteristics of\nthe system, the model exhibits one of three distinct configurations:\nmonostability with a global poverty trap in a nutrient-dominated or\nfish-dominated system; bistability with poverty trap and well-being attractors;\nmultistability with poverty trap and two well-being attractors with different\ncharacteristics. The model results show that intensification can be sustainable\nonly if it takes into account the local social-ecological context. In addition,\nthe heterogeneity of small-scale aquaculture producers matters, as the effects\nof intensification can be unevenly distributed among them. Finally, more is not\nalways better because too high nutrient input or productivity can lead to a\nsuboptimal attractor or system collapse.",
      "generated_abstract": "e the impact of a population-level environmental change on\nthe evolution of a species' phenotype. We assume that the environmental\nchange has a local impact on the population. We develop a model where the\npopulation is subject to selection for a traits that changes locally with the\nenvironment. The traits are not subject to selection for the environment. The\nenvironmental change is a stochastic process with a non-zero mean. We show that\nthe evolution of the phenotype is determined by the mean of the environmental\nchange and by the variance of the environmental change. We show that the mean\nof the environmental change is a function of the mean of the phenotype and of\nthe population size. We also show that the variance of the environmental change\nis a function of the mean of the phenotype and the population size. We also\nshow that the variance of the environmental change is a function of the mean of\nthe phenotype",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13013698630136986,
          "p": 0.41304347826086957,
          "f": 0.1979166630230035
        },
        "rouge-2": {
          "r": 0.034653465346534656,
          "p": 0.08433734939759036,
          "f": 0.04912280288925859
        },
        "rouge-l": {
          "r": 0.13013698630136986,
          "p": 0.41304347826086957,
          "f": 0.1979166630230035
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2407.00420v1",
      "true_abstract": "The potential health risks of cement dust exposure are increasingly raising\nconcern worldwide as the cement industry expands in response to rising cement\ndemand. This necessitates the need to determine the nature of the risks in\norder to develop appropriate measures. This study determined the effects of\ncement dust exposure on the weight and blood glucose levels of people residing\nor working around a cement company in Sokoto, Nigeria. Demographic information\nwas obtained using questionnaires from 72 participants, which included age,\ngender, educational level, exposure hours, occupation, and lifestyle. The blood\nglucose levels and body mass index (BMI) were measured using a Fine Test\nglucometer and a mechanical scale, respectively. The results showed that most\nof the people living or working around the cement company were middle-aged men\n(31-40; 42.06%) with a primary (33.33%) or secondary (45.83%) school education.\nIt showed that 30 (41.69%) of the participants were overweight while 5 (6.94%)\nwere obese. Additionally, 52.78% of the participants were diabetic while 31.94%\nwere prediabetic. Participants that were exposed for long hours (> 15 hours per\nday) were the most diabetic (20% of the participants), followed by smokers\n(15%), and artisans (7%). It can be concluded that exposure to cement dust from\nthe company increased the risk of overweight, obesity, and hyperglycemia among\nthe participants. These health risks were worsened by daily long hours of\nexposure, smoking, and artisanal pollutant exposure. Human settlements and\nartisans should not be located near the cement company, and the company should\nminimize pollutant emissions.",
      "generated_abstract": "rm challenge in the field of synthetic biology is the development of\ncrystalline structures capable of efficiently catalyzing specific chemical\nreactions. This paper presents a simple and efficient approach to design\ncrystalline structures, based on a combination of symmetry-based modeling and\nthe use of a simple molecular network to guide the design of the crystalline\nstructure. The approach is applied to a model system that catalyzes the\ndehydration of an organic solvent into water. This system exhibits a\nsingle-site, alkaline-earth metal-catalyzed reaction, which is well suited for\ncatalysis by crystalline structures. A simple model of the catalytic mechanism\nis constructed, and a series of crystalline structures with varying crystal\nstructures and stoichiometry is synthesized. The results indicate that the\ncry",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0949367088607595,
          "p": 0.2,
          "f": 0.12875536044134187
        },
        "rouge-2": {
          "r": 0.017167381974248927,
          "p": 0.03508771929824561,
          "f": 0.023054750631265948
        },
        "rouge-l": {
          "r": 0.08227848101265822,
          "p": 0.17333333333333334,
          "f": 0.11158797846709295
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.21129v1",
      "true_abstract": "We propose Microscopic Propagator Imaging (MPI) as a novel method to retrieve\nthe indices of the microscopic propagator which is the probability density\nfunction of water displacements due to diffusion within the nervous tissue\nmicrostructures. Unlike the Ensemble Average Propagator indices or the\nDiffusion Tensor Imaging metrics, MPI indices are independent from the\nmesoscopic organization of the tissue such as the presence of multiple axonal\nbundle directions and orientation dispersion. As a consequence, MPI indices are\nmore specific to the volumes, sizes, and types of microstructures, like axons\nand cells, that are present in the tissue. Thus, changes in MPI indices can be\nmore directly linked to alterations in the presence and integrity of\nmicrostructures themselves. The methodology behind MPI is rooted on zonal\nmodeling of spherical harmonics, signal simulation, and machine learning\nregression, and is demonstrated on both synthetic and Human Diffusion MRI data.",
      "generated_abstract": "e-wide association study (GWAS) has become the primary method for\nidentifying genetic variants associated with complex traits. Despite this\nsignificant advancement, the performance of GWAS has been criticized for its\nlack of statistical power, particularly in low-powered studies, which are\ncommonly conducted for rare variants. Here, we propose a novel statistical\nframework for assessing the power of GWAS in identifying rare variants. We\nshow that the average power of GWAS can be expressed as the product of the\npower of each individual GWAS, and we derive the power of GWAS by pooling the\nresults of multiple GWAS. To validate our proposed method, we apply it to a\nstudy of schizophrenia, where we show that the average power of GWAS is\nsignificantly higher than the power of the original GWAS, while the number of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18947368421052632,
          "p": 0.21951219512195122,
          "f": 0.20338982553544652
        },
        "rouge-2": {
          "r": 0.0364963503649635,
          "p": 0.043859649122807015,
          "f": 0.03984063249218328
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.18292682926829268,
          "f": 0.16949152045070076
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2502.02674v1",
      "true_abstract": "We address functional uncertainty quantification for ill-posed inverse\nproblems where it is possible to evaluate a possibly rank-deficient forward\nmodel, the observation noise distribution is known, and there are known\nparameter constraints. We present four constraint-aware confidence intervals\nextending the work of Batlle et al. (2023) by making the intervals both\ncomputationally feasible and less conservative. Our approach first shrinks the\npotentially unbounded constraint set compact in a data-adaptive way, obtains\nsamples of the relevant test statistic inside this set to estimate a quantile\nfunction, and then uses these computed quantities to produce the intervals. Our\ndata-adaptive bounding approach is based on the approach by Berger and Boos\n(1994), and involves defining a subset of the constraint set where the true\nparameter exists with high probability. This probabilistic guarantee is then\nincorporated into the final coverage guarantee in the form of an uncertainty\nbudget. We then propose custom sampling algorithms to efficiently sample from\nthis subset, even when the parameter space is high-dimensional.\nOptimization-based interval methods formulate confidence interval computation\nas two endpoint optimizations, where the optimization constraints can be set to\nachieve different types of interval calibration while seamlessly incorporating\nparameter constraints. However, choosing valid optimization constraints has\nbeen elusive. We show that all four proposed intervals achieve nominal coverage\nfor a particular functional both theoretically and in practice, with numerical\nexamples demonstrating superior performance of our intervals over the OSB\ninterval in terms of both coverage and expected length. In particular, we show\nthe superior performance in a realistic unfolding simulation from high-energy\nphysics that is severely ill-posed and involves a rank-deficient forward model.",
      "generated_abstract": "ntext of studying the distribution of a random variable $X$\nunder a given probability measure $\\mu$, the normal approximation to the\ncumulative distribution function $F_X$ of $X$ is widely used. In this paper,\nwe propose a new approach based on the local polynomial approximation method to\nestimate the cumulative distribution function $F_X$ of $X$ with a fixed\nprobability measure $\\mu$. This approach is based on the local polynomial\napproximation method of the normal approximation to the cumulative distribution\nfunction, and the asymptotic normality of the local polynomial approximation\nmethod is proved. The proposed approach is compared with the normal\napproximation method and the local polynomial approximation method, and the\nresults of simulation studies are used to evaluate the accuracy of the\nproposed approach. The results show that the proposed approach has a higher\naccuracy than the normal approximation method and the local polynomial\napproximation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15476190476190477,
          "p": 0.43333333333333335,
          "f": 0.22807017156048018
        },
        "rouge-2": {
          "r": 0.03543307086614173,
          "p": 0.09183673469387756,
          "f": 0.05113635961841457
        },
        "rouge-l": {
          "r": 0.13690476190476192,
          "p": 0.38333333333333336,
          "f": 0.201754382086796
        }
      }
    },
    {
      "paper_id": "math.NT.math/NT/2503.09207v1",
      "true_abstract": "In this paper, we study the $p$-Selmer groups in the family of $p$-twists of\nan elliptic curve $E$ over a number field $K$. We prove that if $E/K$ is an\nelliptic curve over a number field $K$, and if $d$ is congruent to the\ndimension of the Selmer group of $E/K$ modulo $2$ and is greater than that\ndimension, then there exist infinitely many characters $\\chi \\in\n\\text{Hom}(G_K, \\mu_p)$ such that $\\text{dim}_{\\mathbb{F}_p}(\\text{Sel}_p(E/K,\n\\chi)) = d$ under certain conditions.",
      "generated_abstract": "aper we prove the following: Let $k$ be an algebraically closed\nelementary amenable field of positive characteristic. Then there exists a\nfinite extension of $k$ of degree $d$ with the property that every maximal\nordered field extension of $k$ of degree $d$ contains a maximal order of $k$\nwhose Galois group is not elementary abelian of finite index.\n  For $k=\\mathbb{Q}$ we obtain a similar result for every finite extension\n$k$ of $\\mathbb{Q}$ of degree $d$ with the property that every maximal ordered\nfield extension of $k$ of degree $d$ contains a maximal order of $k$ whose\nGalois group is not elementary abelian of finite index.\n  We also obtain a similar result for $k=\\mathbb{F}_p$ for every prime $p$ with\n$p\\geq 5$ and $d\\geq",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2413793103448276,
          "p": 0.25,
          "f": 0.24561403008925833
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1896551724137931,
          "p": 0.19642857142857142,
          "f": 0.19298245114188992
        }
      }
    },
    {
      "paper_id": "math.OC.q-fin/PM/2411.07949v2",
      "true_abstract": "We consider a simplified model for optimizing a single-asset portfolio in the\npresence of transaction costs given a signal with a certain autocorrelation and\ncross-correlation structure. In our setup, the portfolio manager is given two\none-parameter controls to influence the construction of the portfolio. The\nfirst is a linear filtering parameter that may increase or decrease the level\nof autocorrelation in the signal. The second is a numerical threshold that\ndetermines a symmetric \"no-trade\" zone. Portfolio positions are constrained to\na single unit long or a single unit short. These constraints allow us to focus\non the interplay between the signal filtering mechanism and the hysteresis\nintroduced by the \"no-trade\" zone. We then formulate an optimization problem\nwhere we aim to minimize the frequency of trades subject to a fixed return\nlevel of the portfolio. We show that maintaining a no-trade zone while removing\nautocorrelation entirely from the signal yields a locally optimal solution. For\nany given \"no-trade\" zone threshold, this locally optimal solution also\nachieves the maximum attainable return level, and we derive a quantitative\nlower bound for the amount of improvement in terms of the given threshold and\nthe amount of autocorrelation removed.",
      "generated_abstract": "uce a novel risk management framework for financial markets by\nintegrating stochastic volatility models with multi-period optimal portfolio\ndecision-making. The proposed framework integrates two key concepts: the\nheteroskedasticity of volatility and the stochastic discounting of future\ncash flows. We show that the stochastic discounting of future cash flows\ncorresponds to the classical Cram\\'er-Lundberg model, while the heteroskedastic\nvolatility captures the effect of intrinsic market risk. Our framework\nintegrates these two concepts in a unified framework for risk management by\nintroducing an optimal portfolio decision-making problem with a stochastic\ndiscount factor and a stochastic volatility process. We establish the\nstationarity and consistency of the stochastic volatility process and derive\nthe corresponding asymptotic risk and portfolio value. Furthermore, we",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18421052631578946,
          "p": 0.31343283582089554,
          "f": 0.23204419423216635
        },
        "rouge-2": {
          "r": 0.028735632183908046,
          "p": 0.05,
          "f": 0.0364963457296612
        },
        "rouge-l": {
          "r": 0.17543859649122806,
          "p": 0.29850746268656714,
          "f": 0.2209944704752603
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.06093v1",
      "true_abstract": "Bayesian Optimization (BO) is a well-established method for addressing\nblack-box optimization problems. In many real-world scenarios, optimization\noften involves multiple functions, emphasizing the importance of leveraging\ndata and learned functions from prior tasks to enhance efficiency in the\ncurrent task. To expedite convergence to the global optimum, recent studies\nhave introduced meta-learning strategies, collectively referred to as meta-BO,\nto incorporate knowledge from historical tasks. However, in practical settings,\nthe underlying functions are often heterogeneous, which can adversely affect\noptimization performance for the current task. Additionally, when the number of\nhistorical tasks is large, meta-BO methods face significant scalability\nchallenges. In this work, we propose a scalable and robust meta-BO method\ndesigned to address key challenges in heterogeneous and large-scale meta-tasks.\nOur approach (1) effectively partitions transferred meta-functions into highly\nhomogeneous clusters, (2) learns the geometry-based surrogate prototype that\ncapture the structural patterns within each cluster, and (3) adaptively\nsynthesizes meta-priors during the online phase using statistical\ndistance-based weighting policies. Experimental results on real-world\nhyperparameter optimization (HPO) tasks, combined with theoretical guarantees,\ndemonstrate the robustness and effectiveness of our method in overcoming these\nchallenges.",
      "generated_abstract": "We present a novel method for online learning of a class of functions, called\ngaussian randomized functions, that is designed to work with a small number of\ninputs. The method is based on the notion of the ``gaussian randomized\nfunction class'' (GRFC) and uses a novel gradient estimator for this class.\nThe method is demonstrated in several applications, including the problem of\napproximating the mean of a Gaussian process, the estimation of the\ndistribution of the mean of a Gaussian process, and the problem of approximating\nthe variance of a Gaussian process. The method is applied to the problem of\napproximating the mean of a Gaussian process. The results show that the\nmethodology can be used to approximate the mean of a Gaussian process in a\ncompact set with a probability that approaches one.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1357142857142857,
          "p": 0.296875,
          "f": 0.18627450549788552
        },
        "rouge-2": {
          "r": 0.0223463687150838,
          "p": 0.04081632653061224,
          "f": 0.0288808618535372
        },
        "rouge-l": {
          "r": 0.11428571428571428,
          "p": 0.25,
          "f": 0.15686274079200319
        }
      }
    },
    {
      "paper_id": "math.KT.math/KT/2503.06267v1",
      "true_abstract": "We present the fundamental properties of the K-theory groups of complex\nvector bundles endowed with actions of magnetic groups. In this work we show\nthat the magnetic equivariant K-theory groups define an equivariant cohomology\ntheory, we determine its coefficients, we show Bott's, Thom's and the degree\nshift isomorphism, we present the Atiyah-Hirzeburh spectral sequence, and we\nexplicitly calculate two magnetic equivariant K-theory groups in order to\nshowcase its applicability. These magnetic equivariant K-theory groups are\nrelevant in condensed matter physics since they provide topological invariants\nof gapped Hamiltonians in magnetic crystals.",
      "generated_abstract": "We prove that the universal covering $\\widetilde{X}$ of a compact K\\\"ahler\ne-manifold $X$ admits a canonical meromorphic $2$-form with respect to which\nthe complex structure on $X$ is induced by the natural one on $\\widetilde{X}$.\nThis result is a generalization of the theorem of A.V.Gelfand and M.M.Shilov\n(1995) for the case of K\\\"ahler manifolds. The proof is based on a study of\nthe canonical bundle of $\\widetilde{X}$, which is constructed as the pullback\nof the canonical bundle of $X$ by the canonical projection $\\pi: \\widetilde{X}\n\\to X$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13333333333333333,
          "p": 0.14035087719298245,
          "f": 0.13675213175542428
        },
        "rouge-2": {
          "r": 0.024691358024691357,
          "p": 0.024096385542168676,
          "f": 0.02439023890318365
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.10526315789473684,
          "f": 0.10256409756739011
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.09934v1",
      "true_abstract": "It is time to move on from attempts to make the pharmacy benefit manager\n(PBM) reseller business model more transparent. Time and time again the Big 3\nPBMs have developed opaque alternatives to piece-meal 100% pass-through\nmandates. Time and time again PBMs have demonstrated expertise in finding\nloopholes in state government disclosure laws. The purpose of this paper is to\nprovide quantitative estimates of two transparent insurance business models as\na solution to the PBM agency issue. The key parameter used is an 8% gross\nprofit margin figure disclosed by the Big 3 PBMs themselves. Based on reported\ndrug trend delivered to plans, we use a $1,200 to $1,500 per member per year\n(PMPY) as the range for this key performance indicator (KPI). We propose that\ndiscussions of PBM insurance business models start with the following figures:\n(1) a fixed premium model with medical loss ratio ranging from 92% to 85%; (2)\na fee-for-service model ranging from $96 to $180 PMPY with risk sharing of\ndeviations from a contracted PMPY delivered drug spend.",
      "generated_abstract": "y investigates the effect of climate change on economic development.\nThis paper uses the dynamic panel data model to investigate the effects of\nclimate change on economic development. The results show that the economic\ndevelopment of the country is positively affected by climate change. The study\nalso concludes that the economic development of the country is positively\naffected by the economic development of the region and the urban-rural\ndifference. The study also concludes that the economic development of the\ncountry is positively affected by the economic development of the region and\nthe urban-rural difference. The study also concludes that the economic\ndevelopment of the country is positively affected by the economic development of\nthe region and the urban-rural difference. The study also concludes that the\neconomic development of the country is positively affected by the economic\ndevelopment of the region and the urban-rural difference. The study also concludes\nthat the economic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09401709401709402,
          "p": 0.3055555555555556,
          "f": 0.14379084607458678
        },
        "rouge-2": {
          "r": 0.006172839506172839,
          "p": 0.020833333333333332,
          "f": 0.009523805997280217
        },
        "rouge-l": {
          "r": 0.08547008547008547,
          "p": 0.2777777777777778,
          "f": 0.1307189506497502
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.07900v1",
      "true_abstract": "Cost-effective localization methods for Autonomous Underwater Vehicle (AUV)\nnavigation are key for ocean monitoring and data collection at high resolution\nin time and space. Algorithmic solutions suitable for real-time processing that\nhandle nonlinear measurement models and different forms of measurement\nuncertainty will accelerate the development of field-ready technology. This\npaper details a Bayesian estimation method for landmark-aided navigation using\na Side-scan Sonar (SSS) sensor. The method bounds navigation filter error in\nthe GPS-denied undersea environment and captures the highly nonlinear nature of\nslant range measurements while remaining computationally tractable. Combining a\nnovel measurement model with the chosen statistical framework facilitates the\nefficient use of SSS data and, in the future, could be used in real time. The\nproposed filter has two primary steps: a prediction step using an unscented\ntransform and an update step utilizing particles. The update step performs\nprobabilistic association of sonar detections with known landmarks. We evaluate\nalgorithm performance and tractability using synthetic data and real data\ncollected field experiments. Field experiments were performed using two\ndifferent marine robotic platforms with two different SSS and at two different\nsites. Finally, we discuss the computational requirements of the proposed\nmethod and how it extends to real-time applications.",
      "generated_abstract": "aper, we propose a novel framework for the coordination of\nefficient, safe, and robust motion planning in complex, dynamic, and uncertain\nenvironments. The proposed framework employs a hierarchical architecture,\ncomprising a kinodynamic planner, a state-of-charge (SoC) planner, and a\ncontroller. The SoC planner provides an optimal path based on the SoC of the\nbattery. The kinodynamic planner is used to determine the trajectory along the\noptimal path. The controller is designed to maintain the SoC at the desired\nlevel while ensuring safety and stability of the system. To ensure safety, the\ncontroller employs a collision-avoidance constraint. To ensure stability, we\nintroduce a new stability constraint, which is based on the dynamics of the\nbattery. The stability constraint is formulated as a minimization problem and\nis solved",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11851851851851852,
          "p": 0.2191780821917808,
          "f": 0.15384614929040324
        },
        "rouge-2": {
          "r": 0.015463917525773196,
          "p": 0.02608695652173913,
          "f": 0.01941747105497541
        },
        "rouge-l": {
          "r": 0.1037037037037037,
          "p": 0.1917808219178082,
          "f": 0.13461538005963403
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.16998v1",
      "true_abstract": "We provide a counterexample to the conduct parameter identification result\nestablished in the foundational work of Lau (1982), which generalizes the\nidentification theorem of Bresnahan (1982) by relaxing the linearity\nassumptions. We identify a separable demand function that still permits\nidentification and validate this case both theoretically and through numerical\nsimulations.",
      "generated_abstract": "This paper explores the use of Bayesian nonparametric techniques to\nmeasuring the level of inequality in countries. I propose a novel approach to\nestimating the conditional variance of the top and bottom 10% of income\ndistributions. Using a Bayesian hierarchical model, I demonstrate that the\nconditional variance is a function of the conditional distribution function of\nincome, which is estimated using a Gaussian process (GP). The proposed\nestimator is highly efficient and is robust to misspecification of the\ndistribution function. I demonstrate the method's superior performance by\ncomparing it to alternative methods that are based on linear regression.\nFurther, I show that the proposed method provides a more accurate estimate of\nthe top 10% distribution when compared to the traditional approach based on\nlinear regression.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23809523809523808,
          "p": 0.13513513513513514,
          "f": 0.1724137884839478
        },
        "rouge-2": {
          "r": 0.02,
          "p": 0.00909090909090909,
          "f": 0.012499995703126475
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.0945945945945946,
          "f": 0.12068965055291336
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.07374v1",
      "true_abstract": "Recent statistical postprocessing methods for wind speed forecasts have\nincorporated linear models and neural networks to produce more skillful\nprobabilistic forecasts in the low-to-medium wind speed range. At the same\ntime, these methods struggle in the high-to-extreme wind speed range. In this\nwork, we aim to increase the performance in this range by training using a\nweighted version of the continuous ranked probability score (wCRPS). We develop\nan approach using shifted Gaussian cdf weight functions, whose parameters are\ntuned using a multi-objective hyperparameter tuning algorithm that balances\nperformance on low and high wind speed ranges. We explore this approach for\nboth linear models and convolutional neural network models combined with\nvarious parametric distributions, namely the truncated normal, log-normal, and\ngeneralized extreme value distributions, as well as adaptive mixtures. We apply\nthese methods to forecasts from KNMI's deterministic Harmonie-Arome numerical\nweather prediction model to obtain probabilistic wind speed forecasts in the\nNetherlands for 48 hours ahead. For linear models we observe that even with a\ntuned weight function, training using the wCRPS produces a strong body-tail\ntrade-off, where increased performance on extremes comes at the price of lower\nperformance for the bulk of the distribution. For the best models using\nconvolutional neural networks, we find that using a tuned weight function the\nperformance on extremes can be increased without a significant deterioration in\nperformance on the bulk. The best-performing weight function is shown to be\nmodel-specific. Finally, the choice of distribution has no significant impact\non the performance of our models.",
      "generated_abstract": "In this paper, we propose a novel approach to the problem of detecting\nsignificant outliers in the presence of a large number of observations. The\nkey idea is to identify the outlier clusters and then apply a robust\nstatistical test to identify the outliers. We develop a novel framework for\nthe detection of outliers in a clustered data setting. The proposed framework\nuses a partitioning of the data set into clusters based on the outlier\nstatistics, and then applies a robust statistical test to identify the\noutliers within each cluster. We provide theoretical guarantees for the\noutlier detection performance of the proposed approach. We also discuss the\nimpact of the clustering scheme and the robust statistical test on the\ndetection performance of the proposed approach.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14583333333333334,
          "p": 0.3387096774193548,
          "f": 0.20388349093788302
        },
        "rouge-2": {
          "r": 0.031818181818181815,
          "p": 0.07142857142857142,
          "f": 0.04402515296863298
        },
        "rouge-l": {
          "r": 0.13194444444444445,
          "p": 0.3064516129032258,
          "f": 0.1844660152097277
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2411.13813v3",
      "true_abstract": "I examine the value of information from sell-side analysts by analyzing a\nlarge corpus of their written reports. Using embeddings from state-of-the-art\nlarge language models, I show that textual information in analyst reports\nexplains 10.19% of contemporaneous stock returns out-of-sample, a value that is\neconomically more significant than quantitative forecasts. I then perform a\nShapley value decomposition to assess how much each topic within the reports\ncontributes to explaining stock returns. The results show that analysts' income\nstatement analyses account for more than half of the reports' explanatory\npower. Expressing these findings in economic terms, I estimate that early\nacquisition of analysts' reports can yield significant profits. Analysts'\ninformation value peeks in the first week following earnings announcements,\nhighlighting their vital role in interpreting new financial data.",
      "generated_abstract": "er the problem of pricing a portfolio of callable American options with\na single maturity. The payoff function is given by a finite sum of the form\n$\\sum_{k=1}^K w_k\\mathrm{I}\\left(\\frac{x}{x_k}\\right)$, where $x$ is the\nportfolio's value and $x_k$ is the value of the $k$th contract at maturity.\nThe portfolio's value is assumed to be a function of the current market price\nof the underlying asset, $x$, and the prices of the call options. The\nunderlying asset's price process is assumed to be continuous, and to be\nnon-negative and strictly increasing. We derive the closed-form solution to\nthis problem, using the method of moments. We then prove that the solution is\nunique, and that it coincides with the solution of a linear stochastic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10989010989010989,
          "p": 0.14925373134328357,
          "f": 0.12658227359637897
        },
        "rouge-2": {
          "r": 0.024,
          "p": 0.027777777777777776,
          "f": 0.02575106798799116
        },
        "rouge-l": {
          "r": 0.10989010989010989,
          "p": 0.14925373134328357,
          "f": 0.12658227359637897
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.06413v1",
      "true_abstract": "Despite a plethora of anomaly detection models developed over the years,\ntheir ability to generalize to unseen anomalies remains an issue, particularly\nin critical systems. This paper aims to address this challenge by introducing\nSwift Hydra, a new framework for training an anomaly detection method based on\ngenerative AI and reinforcement learning (RL). Through featuring an RL policy\nthat operates on the latent variables of a generative model, the framework\nsynthesizes novel and diverse anomaly samples that are capable of bypassing a\ndetection model. These generated synthetic samples are, in turn, used to\naugment the detection model, further improving its ability to handle\nchallenging anomalies. Swift Hydra also incorporates Mamba models structured as\na Mixture of Experts (MoE) to enable scalable adaptation of the number of Mamba\nexperts based on data complexity, effectively capturing diverse feature\ndistributions without increasing the model's inference time. Empirical\nevaluations on ADBench benchmark demonstrate that Swift Hydra outperforms other\nstate-of-the-art anomaly detection models while maintaining a relatively short\ninference time. From these results, our research highlights a new and\nauspicious paradigm of integrating RL and generative AI for advancing anomaly\ndetection.",
      "generated_abstract": "the problem of estimating the spectral radius of a random Hermitian\n$n \\times n$ matrix. We consider the following two-step procedure. First,\nwe sample a random Hermitian $n \\times n$ matrix $X$ uniformly at random from\na unitary ensemble, and then estimate the spectral radius of $X$ using\n$\\log_2(\\|X\\|_2^2)$. We show that for the $n=1$ case, the two-step procedure\nyields an estimator with error at most $1/\\log_2(n)$, which is optimal in the\n$n=1$ case, and it is asymptotically optimal in the general case. We also\nprove that the two-step procedure is optimal in the $n=2$ case, and it is\nasymptotically optimal in the general case. We also show that the two-step\nprocedure is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07317073170731707,
          "p": 0.16363636363636364,
          "f": 0.101123591235324
        },
        "rouge-2": {
          "r": 0.005714285714285714,
          "p": 0.012987012987012988,
          "f": 0.007936503692683046
        },
        "rouge-l": {
          "r": 0.07317073170731707,
          "p": 0.16363636363636364,
          "f": 0.101123591235324
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.10718v1",
      "true_abstract": "The escalating challenges of managing vast sensor-generated data,\nparticularly in audio applications, necessitate innovative solutions. Current\nsystems face significant computational and storage demands, especially in\nreal-time applications like gunshot detection systems (GSDS), and the\nproliferation of edge sensors exacerbates these issues. This paper proposes a\ngroundbreaking approach with a near-sensor model tailored for intelligent\naudio-sensing frameworks. Utilizing a Fast Fourier Transform (FFT) module,\nconvolutional neural network (CNN) layers, and HyperDimensional Computing\n(HDC), our model excels in low-energy, rapid inference, and online learning. It\nis highly adaptable for efficient ASIC design implementation, offering superior\nenergy efficiency compared to conventional embedded CPUs or GPUs, and is\ncompatible with the trend of shrinking microphone sensor sizes. Comprehensive\nevaluations at both software and hardware levels underscore the model's\nefficacy. Software assessments through detailed ROC curve analysis revealed a\ndelicate balance between energy conservation and quality loss, achieving up to\n82.1% energy savings with only 1.39% quality loss. Hardware evaluations\nhighlight the model's commendable energy efficiency when implemented via ASIC\ndesign, especially with the Google Edge TPU, showcasing its superiority over\nprevalent embedded CPUs and GPUs.",
      "generated_abstract": "opment of digital audio technology has led to a significant\nchange in music listening habits. The advent of wireless headphones has\ndemocratized music listening, allowing individuals to listen to music anywhere\nand anytime. However, the increased accessibility of wireless headphones has\nalso resulted in an increase in ambient noise, which can negatively affect the\nquality of music listening. This study investigates the impact of ambient\nnoise on the listening experience of wireless headphones. The results of the\nstudy indicate that ambient noise can significantly reduce the listening\nquality of wireless headphones, especially in noisy environments. The study\nprovides insights into the design of effective noise reduction techniques for\nwireless headphones, offering valuable information for the development of\nsound-enhancing technologies. This study contributes to the growing body of\nresearch on the effects of ambient noise on listening experiences, offering\ninsights into how noise can",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10273972602739725,
          "p": 0.189873417721519,
          "f": 0.13333332877669155
        },
        "rouge-2": {
          "r": 0.0056179775280898875,
          "p": 0.008064516129032258,
          "f": 0.0066225117161563376
        },
        "rouge-l": {
          "r": 0.0958904109589041,
          "p": 0.17721518987341772,
          "f": 0.12444443988780264
        }
      }
    },
    {
      "paper_id": "cs.CE.q-fin/CP/2412.18174v1",
      "true_abstract": "Recent advancements have underscored the potential of large language model\n(LLM)-based agents in financial decision-making. Despite this progress, the\nfield currently encounters two main challenges: (1) the lack of a comprehensive\nLLM agent framework adaptable to a variety of financial tasks, and (2) the\nabsence of standardized benchmarks and consistent datasets for assessing agent\nperformance. To tackle these issues, we introduce \\textsc{InvestorBench}, the\nfirst benchmark specifically designed for evaluating LLM-based agents in\ndiverse financial decision-making contexts. InvestorBench enhances the\nversatility of LLM-enabled agents by providing a comprehensive suite of tasks\napplicable to different financial products, including single equities like\nstocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we\nassess the reasoning and decision-making capabilities of our agent framework\nusing thirteen different LLMs as backbone models, across various market\nenvironments and tasks. Furthermore, we have curated a diverse collection of\nopen-source, multi-modal datasets and developed a comprehensive suite of\nenvironments for financial decision-making. This establishes a highly\naccessible platform for evaluating financial agents' performance across various\nscenarios.",
      "generated_abstract": "We propose a novel approach for modeling and predicting the dynamic\ndistribution of uncertainties in real-world systems. This approach leverages\na novel framework based on the concepts of uncertainty sets and uncertainty\nquantifiers. The framework is flexible and adaptable, allowing for the\ndevelopment of generalizable models for uncertainty quantification and\nuncertainty management. We demonstrate the effectiveness of our approach by\nusing it to analyze and predict the distribution of uncertainties in an\nindustrial plant, where the plant's uncertainties are caused by complex\nmechanical and electrical components. Our analysis and predictions allow the\nplant's operators to make more informed decisions and to optimize their\noperations. The results demonstrate the effectiveness of our approach in\nproviding accurate and reliable information for decision-making in real-world\nsystems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12962962962962962,
          "p": 0.1891891891891892,
          "f": 0.15384614902064983
        },
        "rouge-2": {
          "r": 0.006493506493506494,
          "p": 0.009174311926605505,
          "f": 0.007604557884026286
        },
        "rouge-l": {
          "r": 0.12962962962962962,
          "p": 0.1891891891891892,
          "f": 0.15384614902064983
        }
      }
    },
    {
      "paper_id": "econ.TH.cs/GT/2503.06582v1",
      "true_abstract": "The steady rise of e-commerce marketplaces underscores the need to study a\nmarket structure that captures the key features of this setting. To this end,\nwe consider a price-quantity Stackelberg duopoly in which the leader is the\nmarketplace operator and the follower is an independent seller. The objective\nof the marketplace operator is to maximize a weighted sum of profit and a term\ncapturing positive customer experience, whereas the independent seller solely\nseeks to maximize their own profit. Furthermore, the independent seller is\nrequired to share a fraction of their revenue with the marketplace operator for\nthe privilege of selling on the platform. We derive the subgame-perfect Nash\nequilibrium of this game and find that the equilibrium strategies depend on the\nassumed rationing rule. We then consider practical implications for marketplace\noperators. Finally, we show that, under intensity rationing, consumer surplus\nand total welfare in the duopoly marketplace is always at least as high as\nunder an independent seller monopoly, demonstrating that it is socially\nbeneficial for the operator to join the market as a seller.",
      "generated_abstract": "We show that the class of finite non-trivial groups is closed under all\nreasonable forms of homotopy equivalence. This is an extension of the result of\nMatsumoto [Math. Ann. 272 (1978) 567-579",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07920792079207921,
          "p": 0.2962962962962963,
          "f": 0.12499999667114267
        },
        "rouge-2": {
          "r": 0.018518518518518517,
          "p": 0.0967741935483871,
          "f": 0.031088080205106408
        },
        "rouge-l": {
          "r": 0.07920792079207921,
          "p": 0.2962962962962963,
          "f": 0.12499999667114267
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2410.20915v1",
      "true_abstract": "In the literature on stochastic frontier models until the early 2000s, the\njoint consideration of spatial and temporal dimensions was often inadequately\naddressed, if not completely neglected. However, from an evolutionary economics\nperspective, the production process of the decision-making units constantly\nchanges over both dimensions: it is not stable over time due to managerial\nenhancements and/or internal or external shocks, and is influenced by the\nnearest territorial neighbours. This paper proposes an extension of the Fusco\nand Vidoli [2013] SEM-like approach, which globally accounts for spatial and\ntemporal effects in the term of inefficiency. In particular, coherently with\nthe stochastic panel frontier literature, two different versions of the model\nare proposed: the time-invariant and the time-varying spatial stochastic\nfrontier models. In order to evaluate the inferential properties of the\nproposed estimators, we first run Monte Carlo experiments and we then present\nthe results of an application to a set of commonly referenced data,\ndemonstrating robustness and stability of estimates across all scenarios.",
      "generated_abstract": "r introduces a novel model for the estimation of nonlinear\nestimators of the latent Gaussian process of a random field, where the\nobservations are generated from a nonlinear transformation of a Gaussian\nprocess. This nonlinear model is motivated by the use of nonlinear\nregression in econometrics and finance, and is based on the idea that the\nestimator of the latent Gaussian process can be obtained as the solution of an\nordinary differential equation (ODE) with a known initial condition. The\nestimator is constructed using a discretisation of the ODE, and is shown to be\nconsistent and asymptotically normal. The asymptotic properties of the\nestimator are investigated, and a theoretical analysis of the finite-sample\nperformance of the estimator is performed. Theoretical guarantees are provided\nfor the consistency and asymptotic normality of the estimator, and the finite",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15517241379310345,
          "p": 0.23684210526315788,
          "f": 0.187499995217014
        },
        "rouge-2": {
          "r": 0.045454545454545456,
          "p": 0.05982905982905983,
          "f": 0.05166051169837059
        },
        "rouge-l": {
          "r": 0.09482758620689655,
          "p": 0.14473684210526316,
          "f": 0.11458332855034743
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.10169v1",
      "true_abstract": "As part of work to connect phylogenetics with machine learning, there has\nbeen considerable recent interest in vector encodings of phylogenetic trees. We\npresent a simple new ``ordered leaf attachment'' (OLA) method for uniquely\nencoding a binary, rooted phylogenetic tree topology as an integer vector. OLA\nencoding and decoding take linear time in the number of leaf nodes, and the set\nof vectors corresponding to trees is a simply-described subset of integer\nsequences. The OLA encoding is unique compared to other existing encodings in\nhaving these properties. The integer vector encoding induces a distance on the\nset of trees, and we investigate this distance in relation to the NNI and SPR\ndistances.",
      "generated_abstract": "The recent advancements in the field of bioinformatics have paved the way\nfor the integration of genomic data with spatial data. This integration has\ndemonstrated the ability to enhance the understanding of disease mechanisms and\nto provide valuable insights for disease prevention and treatment. This paper\ndescribes a novel approach for integrating spatial data with genomic data in\nthe context of oncogenesis. First, we discuss the methodology used for the\nintegration of genomic data with spatial data. Second, we describe the\napproach for integrating genomic and spatial data, focusing on the integration\nof genomic data with spatial data. Third, we discuss the advantages and\nlimitations of integrating genomic and spatial data. Finally, we provide\nrecommendations for future research in this area.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18421052631578946,
          "p": 0.23728813559322035,
          "f": 0.20740740248669423
        },
        "rouge-2": {
          "r": 0.018691588785046728,
          "p": 0.021505376344086023,
          "f": 0.01999999502450124
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.2033898305084746,
          "f": 0.17777777285706461
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2412.20550v1",
      "true_abstract": "Achaete-scute complex homolog 2 (ASCL2) codes a part of the basic\nhelix-loop-helix (BHLH) transcription factor family. WNTs have been found to\ndirectly affect the stemness of the tumor cells via regulation of ASCL2.\nSwitching off the ASCL2 literally blocks the stemness process of the tumor\ncells and vice versa. In colorectal cancer (CRC) cells treated with\nETC-1922159, ASCL2 was found to be down regulated along with other genes. A\nrecently developed search engine ranked combinations of ASCL2-X (X, a\nparticular gene/protein) at 2nd order level after drug administration. Some\nrankings confirm the already tested combinations, while others point to those\nthat are untested/unexplored. These rankings reveal which ASCL2-X combinations\nmight be working synergistically in CRC. In this research work, I cover\ncombinations of ASCL2 with WNT, transforming growth factor beta (TGFB),\ninterleukin (IL), leucine rich repeat containing G protein-coupled receptor\n(LGR), NOTCH, solute carrier family (SLC), SRY-box transcription factor (SOX),\nsmall nucleolar RNA host gene (SNHG), KIAA, F-box protein (FBXO), family with\nsequence similarity (FAM), B cell CLL/lymphoma (BCL), autophagy related (ATG)\nand Rho GTPase activating protein (ARHGAP) family.",
      "generated_abstract": "ification of the genetic basis of disease is a critical step in\nthe development of new diagnostic and therapeutic tools. However, the\nsignificant amount of data required for this task poses a significant\ncomputational challenge. To address this, we present a novel approach to\nidentify the genetic basis of disease using large-scale multi-omics data. Our\nmethod, which we call Genome-Wide Association of Complex Traits Using\nMulti-Omics Data (Genome-Wide-MOD), combines genotype data from single-nucleotide\npolymorphism (SNP) arrays with multi-omics data from transcriptomics,\nepigenomics, and proteomics. We demonstrate the efficacy of our approach in\nidentifying the genetic basis of Parkinson's disease, using data from the Parkinson\nDisease Gene Expression Omnibus (",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06428571428571428,
          "p": 0.11842105263157894,
          "f": 0.08333332877229106
        },
        "rouge-2": {
          "r": 0.005917159763313609,
          "p": 0.01020408163265306,
          "f": 0.0074906320576835885
        },
        "rouge-l": {
          "r": 0.05714285714285714,
          "p": 0.10526315789473684,
          "f": 0.07407406951303183
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.stat/ML/2503.04454v1",
      "true_abstract": "By leveraging tools from the statistical mechanics of complex systems, in\nthese short notes we extend the architecture of a neural network for\nhetero-associative memory (called three-directional associative memories, TAM)\nto explore supervised and unsupervised learning protocols. In particular, by\nproviding entropic-heterogeneous datasets to its various layers, we predict and\nquantify a new emergent phenomenon -- that we term {\\em layer's\ncooperativeness} -- where the interplay of dataset entropies across network's\nlayers enhances their retrieval capabilities Beyond those they would have\nwithout reciprocal influence. Naively we would expect layers trained with less\ninformative datasets to develop smaller retrieval regions compared to those\npertaining to layers that experienced more information: this does not happen\nand all the retrieval regions settle to the same amplitude, allowing for\noptimal retrieval performance globally. This cooperative dynamics marks a\nsignificant advancement in understanding emergent computational capabilities\nwithin disordered systems.",
      "generated_abstract": "p a theory of nonlinear dynamics in the mean-field limit in\nnetworked\npopulations, where the dynamics are described by stochastic differential\nequations. We prove that the mean-field limit of the networked population\nfollows a diffusive, self-sustained, self-consistent trajectory. The\nmean-field limit is obtained by treating each agent as a Brownian particle in\nthe mean-field approximation, with the dynamics governed by a stochastic\ndifferential equation. We prove that the networked population exhibits a\ndiffusive mean-field limit, where the mean-field approximation leads to the\ndynamics of a single Brownian particle in the mean-field approximation. We\nfurther prove that the networked population exhibits a self-sustained,\nself-consistent mean-field limit, where the mean-field approximation leads to\nthe dynamics of a nonlinear stochastic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09009009009009009,
          "p": 0.20408163265306123,
          "f": 0.1249999957507814
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.038461538461538464,
          "f": 0.02739725568774706
        },
        "rouge-l": {
          "r": 0.08108108108108109,
          "p": 0.1836734693877551,
          "f": 0.1124999957507814
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.09806v1",
      "true_abstract": "Interdependencies between units in online two-sided marketplaces complicate\nestimating causal effects in experimental settings. We propose a novel\nexperimental design to mitigate the interference bias in estimating the total\naverage treatment effect (TATE) of item-side interventions in online two-sided\nmarketplaces. Our Two-Sided Prioritized Ranking (TSPR) design uses the\nrecommender system as an instrument for experimentation. TSPR strategically\nprioritizes items based on their treatment status in the listings displayed to\nusers. We designed TSPR to provide users with a coherent platform experience by\nensuring access to all items and a consistent realization of their treatment by\nall users. We evaluate our experimental design through simulations using a\nsearch impression dataset from an online travel agency. Our methodology closely\nestimates the true simulated TATE, while a baseline item-side estimator\nsignificantly overestimates TATE.",
      "generated_abstract": "uce a novel method for constructing localized, adaptive, and\nreproducible instrumental variable (IV) estimators in a context in which\nstandard IV estimators are not applicable. We propose an alternative method for\nconstructing IV estimators, where the IV is constructed from a subsample of\nobservations that are selected at random. We derive the distribution of the\nestimated IV estimator and derive the distribution of the estimated IV\nestimator under the null hypothesis that the IV is drawn randomly. We also\nintroduce a method for constructing adaptive IV estimators that are adaptive\nto the IV selection process. Our method for constructing adaptive IV estimators\nis based on the estimation of the conditional mean of the IV and the IV\nestimator. We derive the distribution of the estimated IV estimator and\nderive the distribution of the estimated IV estimator under the null\nhypothesis that the IV",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17582417582417584,
          "p": 0.26666666666666666,
          "f": 0.211920525012061
        },
        "rouge-2": {
          "r": 0.024390243902439025,
          "p": 0.03260869565217391,
          "f": 0.027906971848134985
        },
        "rouge-l": {
          "r": 0.16483516483516483,
          "p": 0.25,
          "f": 0.19867549189947822
        }
      }
    },
    {
      "paper_id": "math.ST.econ/EM/2412.18080v1",
      "true_abstract": "There are many nonparametric objects of interest that are a function of a\nconditional distribution. One important example is an average treatment effect\nconditional on a subset of covariates. Many of these objects have a conditional\ninfluence function that generalizes the classical influence function of a\nfunctional of a (unconditional) distribution. Conditional influence functions\nhave important uses analogous to those of the classical influence function.\nThey can be used to construct Neyman orthogonal estimating equations for\nconditional objects of interest that depend on high dimensional regressions.\nThey can be used to formulate local policy effects and describe the effect of\nlocal misspecification on conditional objects of interest. We derive\nconditional influence functions for functionals of conditional means and other\nfeatures of the conditional distribution of an outcome variable. We show how\nthese can be used for locally linear estimation of conditional objects of\ninterest. We give rate conditions for first step machine learners to have no\neffect on asymptotic distributions of locally linear estimators. We also give a\ngeneral construction of Neyman orthogonal estimating equations for conditional\nobjects of interest.",
      "generated_abstract": "We study a dynamic setting where agents interact through an intermittent\nnetwork. Agents' choices depend on the current state of the network, which is\nunknown to them. Agents' beliefs about the network's state are updated based on\nobservations of their interactions with neighbors. Our goal is to design an\ninference algorithm that maximizes the expected utility of the updated beliefs\nusing a convex optimization problem. We consider two settings: (1) the\nintermittent network is the interaction graph of the model and (2) the network\nis a random graph with an independent neighbor selection process. We prove that\nthe problem admits a unique solution and characterize the optimal\ninteraction graph. We prove that our algorithm is optimal in both settings and\nconstruct a simulation-based algorithm that achieves the same optimal\noptimality.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1511627906976744,
          "p": 0.16455696202531644,
          "f": 0.1575757525847568
        },
        "rouge-2": {
          "r": 0.014184397163120567,
          "p": 0.01652892561983471,
          "f": 0.015267170601656532
        },
        "rouge-l": {
          "r": 0.1511627906976744,
          "p": 0.16455696202531644,
          "f": 0.1575757525847568
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.01910v1",
      "true_abstract": "The development of therapeutic antibodies heavily relies on accurate\npredictions of how antigens will interact with antibodies. Existing\ncomputational methods in antibody design often overlook crucial conformational\nchanges that antigens undergo during the binding process, significantly\nimpacting the reliability of the resulting antibodies. To bridge this gap, we\nintroduce dyAb, a flexible framework that incorporates AlphaFold2-driven\npredictions to model pre-binding antigen structures and specifically addresses\nthe dynamic nature of antigen conformation changes. Our dyAb model leverages a\nunique combination of coarse-grained interface alignment and fine-grained flow\nmatching techniques to simulate the interaction dynamics and structural\nevolution of the antigen-antibody complex, providing a realistic representation\nof the binding process. Extensive experiments show that dyAb significantly\noutperforms existing models in antibody design involving changing antigen\nconformations. These results highlight dyAb's potential to streamline the\ndesign process for therapeutic antibodies, promising more efficient development\ncycles and improved outcomes in clinical applications.",
      "generated_abstract": "The discovery of the RNA world has been one of the most significant events\nin the history of life. The RNA world, with its ability to synthesize\nproteins, was a major step towards the development of life. However, the RNA\nworld was not a static system, and its evolution has been the subject of much\nresearch in the past few decades. In this paper, we present a brief review of\nthe evolution of RNA worlds, focusing on the formation and evolution of RNA\nworlds, their relationship with other life forms, and their impact on the\nevolution of other biological systems. We conclude by discussing the implications\nof RNA worlds for future research in the field of life evolution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1308411214953271,
          "p": 0.208955223880597,
          "f": 0.1609195354941209
        },
        "rouge-2": {
          "r": 0.02112676056338028,
          "p": 0.03,
          "f": 0.02479338358035748
        },
        "rouge-l": {
          "r": 0.1308411214953271,
          "p": 0.208955223880597,
          "f": 0.1609195354941209
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2410.00158v1",
      "true_abstract": "Systemic risk is receiving increasing attention in the insurance industry, as\nthese risks can have severe impacts on the entire financial system. In this\npaper, we propose a multi-dimensional L/'{e}vy process-based renewal risk model\nwith heterogeneous insurance claims, where every dimension indicates a business\nline of an insurer. We use the systemic expected shortfall (SES) and marginal\nexpected shortfall (MES) defined with a Value-at-Risk (VaR) target level as the\nmeasurement of systemic risks. Assuming that all the claim sizes are pairwise\nasymptotically independent (PAI), we derive asymptotic formulas for the tail\nprobabilities of discounted aggregate claims and total loss, which holds\nuniformly for all time horizons. We further obtain the asymptotics of the above\nsystemic risk measures. The main technical issues involve the treatment of\nuniform convergence in the dynamic time setting. Finally, we conduct a Monte\nCarlo numerical study and verify that our asymptotics are accurate and\nconvenient in computation.",
      "generated_abstract": "r proposes a novel methodology for assessing the risk of credit\narbitrage and provides a comprehensive framework for the analysis of credit\narbitrage and its implications. The proposed methodology offers a novel\nframework for assessing the risk of credit arbitrage and provides a comprehensive\nframework for the analysis of credit arbitrage and its implications. By\nanalyzing the impact of credit arbitrage on the credit market, this paper\nprovides a comprehensive framework for analyzing the risks associated with\ncredit arbitrage and its implications. This paper provides a comprehensive\nframework for analyzing the risks associated with credit arbitrage and its\nimplications. This paper provides a comprehensive framework for analyzing the\nrisks associated with credit arbitrage and its implications. By analyzing the\nimpact of credit arbitrage on the credit market, this paper provides a\ncomprehensive",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0990990990990991,
          "p": 0.3333333333333333,
          "f": 0.15277777424479175
        },
        "rouge-2": {
          "r": 0.013513513513513514,
          "p": 0.0425531914893617,
          "f": 0.020512816854175536
        },
        "rouge-l": {
          "r": 0.0990990990990991,
          "p": 0.3333333333333333,
          "f": 0.15277777424479175
        }
      }
    },
    {
      "paper_id": "cs.AI.q-bio/NC/2502.18725v1",
      "true_abstract": "Traditional psychological experiments utilizing naturalistic stimuli face\nchallenges in manual annotation and ecological validity. To address this, we\nintroduce a novel paradigm leveraging multimodal large language models (LLMs)\nas proxies to extract rich semantic information from naturalistic images\nthrough a Visual Question Answering (VQA) strategy for analyzing human visual\nsemantic representation. LLM-derived representations successfully predict\nestablished neural activity patterns measured by fMRI (e.g., faces, buildings),\nvalidating its feasibility and revealing hierarchical semantic organization\nacross cortical regions. A brain semantic network constructed from LLM-derived\nrepresentations identifies meaningful clusters reflecting functional and\ncontextual associations. This innovative methodology offers a powerful solution\nfor investigating brain semantic organization with naturalistic stimuli,\novercoming limitations of traditional annotation methods and paving the way for\nmore ecologically valid explorations of human cognition.",
      "generated_abstract": "e a novel framework for the representation of biological networks,\nin which the structural relationships among nodes are represented by\nconnectivity patterns. To represent this data, we develop a novel connectivity\npattern embedding (CPE) method that enables us to represent biological networks\nas high-dimensional, multi-dimensional embeddings, where each node's embedding\nrepresents the connectivity patterns among its neighbors. We apply this method\nto the Dengvaxia dataset, which is a network of 52,499 nodes representing\ndengue virus transmission networks. We compare the performance of our proposed\nmethod against the state-of-the-art method in terms of node embedding\nrepresentations, and we analyze the topological and structural properties of\nthe nodes and edges in the networks. Our results show that our proposed\nmethod outperforms the state-of-the-art method in terms of node embedding\nre",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1509433962264151,
          "p": 0.21052631578947367,
          "f": 0.17582417096002914
        },
        "rouge-2": {
          "r": 0.008130081300813009,
          "p": 0.008620689655172414,
          "f": 0.008368195841112206
        },
        "rouge-l": {
          "r": 0.14150943396226415,
          "p": 0.19736842105263158,
          "f": 0.16483515997101814
        }
      }
    },
    {
      "paper_id": "cs.NI.eess/SY/2503.04637v1",
      "true_abstract": "Sensing is emerging as a vital future service in next-generation wireless\nnetworks, enabling applications such as object localization and activity\nrecognition. The IEEE 802.11bf standard extends Wi-Fi capabilities to\nincorporate these sensing functionalities. However, coexistence with legacy\nWi-Fi in densely populated networks poses challenges, as contention for\nchannels can impair both sensing and communication quality. This paper develops\nan analytical framework and a system-level simulation in ns-3 to evaluate the\ncoexistence of IEEE 802.11bf and legacy 802.11ax in terms of sensing delay and\ncommunication throughput. Forthis purpose, we have developed a dedicated ns-3\nmodule forIEEE 802.11bf, which is made publicly available as open-source. We\nprovide the first coexistence analysis between IEEE 802.11bfand IEEE 802.11ax,\nsupported by link-level simulation in ns-3to assess the impact on sensing delay\nand network performance. Key parameters, including sensing intervals, access\ncategories, network densities, and antenna configurations, are systematically\nanalyzed to understand their influence on the sensing delay and aggregated\nnetwork throughput. The evaluation is further extended to a realistic indoor\noffice environment modeled after the 3GPP TR 38.901 standard. Our findings\nreveal key trade-offs between sensing intervals and throughput and the need for\nbalanced sensing parameters to ensure effective coexistence in Wi-Fi networks.",
      "generated_abstract": "This paper proposes a novel method for the simultaneous localization and\nmapping (SLAM) of multiple unmanned aerial vehicles (UAVs) using a single\ncamera, referred to as the SLAM-UAV system. The SLAM-UAV system is a two-stage\nprocess that integrates an obstacle-avoidance controller with a camera-based\nmapping algorithm. The first stage uses a single camera to estimate the position\nand orientation of the UAVs, while the second stage uses the estimated position\nand orientation to perform a mapping task using a 3D point cloud. This paper\ndescribes the SLAM-UAV system architecture, the system performance analysis,\nand the advantages of the proposed approach compared to existing methods.\nFinally, the paper discusses the challenges of the SLAM-UAV system and\nrecommends future directions for research.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.2361111111111111,
          "f": 0.16346153393491136
        },
        "rouge-2": {
          "r": 0.015463917525773196,
          "p": 0.02830188679245283,
          "f": 0.019999995430223266
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.2222222222222222,
          "f": 0.15384614931952675
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2502.12219v1",
      "true_abstract": "Optimizing chemical properties is a challenging task due to the vastness and\ncomplexity of chemical space. Here, we present a generative energy-based\narchitecture for implicit chemical property optimization, designed to\nefficiently generate molecules that satisfy target properties without explicit\nconditional generation. We use Graph Energy Based Models and a training\napproach that does not require property labels. We validated our approach on\nwell-established chemical benchmarks, showing superior results to\nstate-of-the-art methods and demonstrating robustness and efficiency towards de\nnovo drug design.",
      "generated_abstract": "y examines the impact of climate change on the ecological dynamics\nof the green ants, Myrmecocystus spp. These ants are a key component of the\necosystems of tropical rain forests and are known to exhibit significant\nresilience to drought. However, their ecological resilience is affected by\nclimate change, and this study investigates the effect of global warming on\nant populations. We develop a simple stochastic model of ant population\ndynamics and use it to investigate the impact of climate change on the\nant population in two ecosystems: a tropical forest and a temperate forest.\nWe find that climate change leads to increased ant mortality in both ecosystems,\nwith an increase of 24% in ant mortality in a tropical forest and a 20%\nincrease in a temperate forest. We also find that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15151515151515152,
          "p": 0.1388888888888889,
          "f": 0.144927531241336
        },
        "rouge-2": {
          "r": 0.0125,
          "p": 0.009009009009009009,
          "f": 0.010471199320196335
        },
        "rouge-l": {
          "r": 0.15151515151515152,
          "p": 0.1388888888888889,
          "f": 0.144927531241336
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.06092v1",
      "true_abstract": "The popular choice of using a $direct$ forecasting scheme implies that the\nindividual predictions do not contain information on cross-horizon dependence.\nHowever, this dependence is needed if the forecaster has to construct, based on\n$direct$ density forecasts, predictive objects that are functions of several\nhorizons ($e.g.$ when constructing annual-average growth rates from\nquarter-on-quarter growth rates). To address this issue we propose to use\ncopulas to combine the individual $h$-step-ahead predictive distributions into\na joint predictive distribution. Our method is particularly appealing to\npractitioners for whom changing the $direct$ forecasting specification is too\ncostly. In a Monte Carlo study, we demonstrate that our approach leads to a\nbetter approximation of the true density than an approach that ignores the\npotential dependence. We show the superior performance of our method in several\nempirical examples, where we construct (i) quarterly forecasts using\nmonth-on-month $direct$ forecasts, (ii) annual-average forecasts using monthly\nyear-on-year $direct$ forecasts, and (iii) annual-average forecasts using\nquarter-on-quarter $direct$ forecasts.",
      "generated_abstract": "We consider a model of heterogeneous agents with individual rationality,\nrevealed preferences, and imperfect information. The agents have heterogeneous\npreferences and the information structure is revealed. The agents are\nrevealed preference learners. We propose a new class of estimation methods for\nthe identification of individual rationality and reveal preferences. Our\nestimation methods are based on the identification of a special class of\npartially ordered sets of agents with information structures. We prove that\nthe estimators are consistent and asymptotically normal. We apply our method to\nthe identification of agents' individual rationality and reveal preferences. We\nshow that the agents' individual rationality is not identifiable in a model with\na single agent.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.205607476635514,
          "p": 0.3793103448275862,
          "f": 0.26666666210762174
        },
        "rouge-2": {
          "r": 0.026143790849673203,
          "p": 0.0425531914893617,
          "f": 0.0323886592528978
        },
        "rouge-l": {
          "r": 0.205607476635514,
          "p": 0.3793103448275862,
          "f": 0.26666666210762174
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2411.13965v1",
      "true_abstract": "Universal power laws have been scrutinised in physics and beyond, and a\nlong-standing debate exists in econophysics regarding the strict universality\nof the nonlinear price impact, commonly referred to as the square-root law\n(SRL). The SRL posits that the average price impact $I$ follows a power law\nwith respect to transaction volume $Q$, such that $I(Q) \\propto Q^{\\delta}$\nwith $\\delta \\approx 1/2$. Some researchers argue that the exponent $\\delta$\nshould be system-specific, without universality. Conversely, others contend\nthat $\\delta$ should be exactly $1/2$ for all stocks across all countries,\nimplying universality. However, resolving this debate requires high-precision\nmeasurements of $\\delta$ with errors of around $0.1$ across hundreds of stocks,\nwhich has been extremely challenging due to the scarcity of large microscopic\ndatasets -- those that enable tracking the trading behaviour of all individual\naccounts. Here we conclusively support the universality hypothesis of the SRL\nby a complete survey of all trading accounts for all liquid stocks on the Tokyo\nStock Exchange (TSE) over eight years. Using this comprehensive microscopic\ndataset, we show that the exponent $\\delta$ is equal to $1/2$ within\nstatistical errors at both the individual stock level and the individual trader\nlevel. Additionally, we rejected two prominent models supporting the\nnonuniversality hypothesis: the Gabaix-Gopikrishnan-Plerou-Stanley and the\nFarmer-Gerig-Lillo-Waelbroeck models. Our work provides exceptionally\nhigh-precision evidence for the universality hypothesis in social science and\ncould prove useful in evaluating the price impact by large investors -- an\nimportant topic even among practitioners.",
      "generated_abstract": "se of this paper is to introduce a novel risk-aware model for\ncompliance with the capital requirements of the Securities and Exchange\nCommission (SEC) and to present a risk-aware method for the pricing of\nsecurities under the SEC's rules. The method is based on a risk-aware\nrepresentation of the risk of a given security and on the application of the\nrisk-aware model. The method is based on the representation of the risk of a\nsecurity in terms of its risk of default and the risk of an insolvency event.\nThe risk-aware model is applied to the pricing of securities under the SEC's\nrules. The method has been applied to the pricing of equity securities under\nSEC's rules. The results of the application of the risk-aware model and the\nrisk-aware method to the pricing of equity se",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10191082802547771,
          "p": 0.32653061224489793,
          "f": 0.15533980219954763
        },
        "rouge-2": {
          "r": 0.02631578947368421,
          "p": 0.06818181818181818,
          "f": 0.03797467952571746
        },
        "rouge-l": {
          "r": 0.09554140127388536,
          "p": 0.30612244897959184,
          "f": 0.14563106433547
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.07369v1",
      "true_abstract": "Skeletonization extracts thin representations from images that compactly\nencode their geometry and topology. These representations have become an\nimportant topological prior for preserving connectivity in curvilinear\nstructures, aiding medical tasks like vessel segmentation. Existing compatible\nskeletonization algorithms face significant trade-offs: morphology-based\napproaches are computationally efficient but prone to frequent breakages, while\ntopology-preserving methods require substantial computational resources.\n  We propose a novel framework for training iterative skeletonization\nalgorithms with a learnable component. The framework leverages synthetic data,\ntask-specific augmentation, and a model distillation strategy to learn compact\nneural networks that produce thin, connected skeletons with a fully\ndifferentiable iterative algorithm.\n  Our method demonstrates a 100 times speedup over topology-constrained\nalgorithms while maintaining high accuracy and generalizing effectively to new\ndomains without fine-tuning. Benchmarking and downstream validation in 2D and\n3D tasks demonstrate its computational efficiency and real-world applicability",
      "generated_abstract": "aper, we propose a new method to detect and segment brain tumors\nin MRI scans using a deep learning approach. Our approach integrates\nconvolutional neural networks (CNNs) with transformer-based models for\nsegmentation. In particular, we use a pre-trained Vision Transformer\narchitecture (VisionTransformer) to learn the relationship between features\nand tumor labels. This enables us to predict tumor regions more accurately\nthan traditional deep learning models, such as Convolutional Neural Networks\n(CNNs). Additionally, we utilize a transformer-based architecture to enhance\nthe segmentation of tumor regions in MRI scans. The transformer-based model\nprovides a more accurate representation of tumor-free regions, enabling\nmore precise segmentation. Our approach combines the strengths of both\nconvolutional and transformer-based models, offering a robust",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1415929203539823,
          "p": 0.20512820512820512,
          "f": 0.16753926218360257
        },
        "rouge-2": {
          "r": 0.022388059701492536,
          "p": 0.02727272727272727,
          "f": 0.024590158982801317
        },
        "rouge-l": {
          "r": 0.13274336283185842,
          "p": 0.19230769230769232,
          "f": 0.1570680579951209
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/MS/2503.06010v1",
      "true_abstract": "In this paper, we propose the InfoFusion Controller, an advanced path\nplanning algorithm that integrates both global and local planning strategies to\nenhance autonomous driving in complex urban environments. The global planner\nutilizes the informed Theta-Rapidly-exploring Random Tree Star (Informed-TRRT*)\nalgorithm to generate an optimal reference path, while the local planner\ncombines Model Predictive Control (MPC) and Pure Pursuit algorithms. Mutual\nInformation (MI) is employed to fuse the outputs of the MPC and Pure Pursuit\ncontrollers, effectively balancing their strengths and compensating for their\nweaknesses. The proposed method addresses the challenges of navigating in\ndynamic environments with unpredictable obstacles by reducing uncertainty in\nlocal path planning and improving dynamic obstacle avoidance capabilities.\nExperimental results demonstrate that the InfoFusion Controller outperforms\ntraditional methods in terms of safety, stability, and efficiency across\nvarious scenarios, including complex maps generated using SLAM techniques.\n  The code for the InfoFusion Controller is available at https:\n//github.com/DrawingProcess/InfoFusionController.",
      "generated_abstract": "This paper proposes a multi-robot, multi-target tracking strategy for\ntargeted and distributed object removal. The proposed framework consists of a\ncentral controller that controls multiple agents, each with a targeting module,\nto track and remove targets. Each agent has a targeting module that can\ndetect and track targets. The central controller uses the targeting modules to\ndetect and track targets. It then coordinates the agents to track and remove\nthe targets. The central controller is implemented in the ROS platform. The\ncentral controller uses the targeting modules to detect and track targets. The\ncentral controller coordinates the agents to track and remove the targets. A\nsimulation is conducted to verify the effectiveness of the proposed strategy.\nThe simulation results show that the proposed strategy can effectively track\nand remove targets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11926605504587157,
          "p": 0.22033898305084745,
          "f": 0.1547619002047904
        },
        "rouge-2": {
          "r": 0.020833333333333332,
          "p": 0.03296703296703297,
          "f": 0.025531910147941123
        },
        "rouge-l": {
          "r": 0.11926605504587157,
          "p": 0.22033898305084745,
          "f": 0.1547619002047904
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.08179v3",
      "true_abstract": "Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks.",
      "generated_abstract": "brain is a complex system composed of neurons, synapses, and\ngrowing knowledge of its architecture, cellular and molecular interactions,\nneurochemistry, and developmental processes. However, the computational\nrepresentation of the brain remains challenging. We developed a computational\nmodel based on the neuron-to-neuron connectivity of the human brain. Our model\nincludes the interconnection of neurons and synapses, and the interactions\nbetween neurons, synapses, and glial cells. The model allows for the\ncomputation of the spiking activity of the neurons and the computation of the\nspiking activity of the synapses. The model was trained on a large database of\nbrain images and was validated on a small database of brain images. We also\nanalyzed the computational representation of the brain from a biological point\nof view by comparing our model with the current state",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10596026490066225,
          "p": 0.23529411764705882,
          "f": 0.14611871717937505
        },
        "rouge-2": {
          "r": 0.014423076923076924,
          "p": 0.02857142857142857,
          "f": 0.019169324614930247
        },
        "rouge-l": {
          "r": 0.08609271523178808,
          "p": 0.19117647058823528,
          "f": 0.11872145690540246
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PR/2409.04903v2",
      "true_abstract": "In this paper, we propose a semi-analytical approach to pricing options on\nSOFR futures where the underlying SOFR follows a time-dependent CEV model. By\ndefinition, these options change their type at the beginning of the reference\nperiod: before this time, this is an American option written on a SOFR forward\nprice as an underlying, and after this point, this is an arithmetic Asian\noption with an American style exercise written on the daily SOFR rates. We\ndevelop a new version of the GIT method and solve both problems\nsemi-analytically, obtaining the option price, the exercise boundary, and the\noption Greeks. This work is intended to address the concern that the transfer\nfrom LIBOR to SOFR has resulted in a situation in which the options of the key\nmoney market (i.e., futures on the reference rate) are options without any\npricing model available. Therefore, the trading in options on 3M SOFR futures\ncurrently ends before their reference quarter starts, to eliminate the final\nmetamorphosis into exotic options.",
      "generated_abstract": "r introduces a novel model of financial market microstructure\ninvolving a single investor who can engage in bid-ask spread trading and\nbidirectional order flow trading. We consider the case where the bid-ask spread\nis dependent on the market liquidity, but the bid-ask spread is not\ndeterministic. In this setting, we show that the market liquidity exhibits\nstochastic dependence on the bid-ask spread. As a result, the bid-ask spread\nexhibits a stochastic dependence on the market liquidity. We further derive the\nmean-variance optimization problem and the corresponding dynamic programming\nequation, which can be solved to obtain the optimal bid-ask spread. We\ninvestigate the impact of the bid-ask spread on the equilibrium price. We\nintroduce an equilibrium price that is a function of the bid-ask spread and\ntheir stochastic dependence. Furthermore, we",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20192307692307693,
          "p": 0.29577464788732394,
          "f": 0.23999999517779602
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.05714285714285714,
          "f": 0.04597700668516368
        },
        "rouge-l": {
          "r": 0.19230769230769232,
          "p": 0.28169014084507044,
          "f": 0.2285714237492246
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.07573v1",
      "true_abstract": "We prove an inversion formula for the exterior $k$-plane transform. As a\nconsequence, we show that if $m < k$ then an $m$-current in $\\mathbf R^n$ can\nbe reconstructed from its projections onto $\\mathbf R^k$, which proves a\nconjecture of Solomon.",
      "generated_abstract": "In this paper, we investigate the problem of finding the best possible\nestimate of the number of eigenvalues of a self-adjoint linear operator on a\ncomplex Hilbert space which is related to a given spectral radius of the\noperator. The problem is studied under the assumption that the operator is\nbounded on a subspace of the Hilbert space. In the case when the operator is\nbounded on the whole Hilbert space, we show that for a sufficiently small\nparameter the best possible estimate of the number of eigenvalues is obtained.\nWe also derive the bounds of the number of eigenvalues for a sufficiently\nlarge parameter. As a result of our analysis, we obtain that the spectral\nradius of the operator is an upper bound of the number of eigenvalues of the\noperator. The bounds of the number of eigenvalues are also obtained for a\nsufficiently large parameter.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2894736842105263,
          "p": 0.18333333333333332,
          "f": 0.22448979117034576
        },
        "rouge-2": {
          "r": 0.075,
          "p": 0.03,
          "f": 0.042857138775510596
        },
        "rouge-l": {
          "r": 0.2894736842105263,
          "p": 0.18333333333333332,
          "f": 0.22448979117034576
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/LG/2503.10594v1",
      "true_abstract": "The structural analogies of ResNets and Multigrid (MG) methods such as common\nbuilding blocks like convolutions and poolings where already pointed out by He\net al.\\ in 2016. Multigrid methods are used in the context of scientific\ncomputing for solving large sparse linear systems arising from partial\ndifferential equations. MG methods particularly rely on two main concepts:\nsmoothing and residual restriction / coarsening. Exploiting these analogies, He\nand Xu developed the MgNet framework, which integrates MG schemes into the\ndesign of ResNets. In this work, we introduce a novel neural network building\nblock inspired by polynomial smoothers from MG theory. Our polynomial block\nfrom an MG perspective naturally extends the MgNet framework to Poly-Mgnet and\nat the same time reduces the number of weights in MgNet. We present a\ncomprehensive study of our polynomial block, analyzing the choice of initial\ncoefficients, the polynomial degree, the placement of activation functions, as\nwell as of batch normalizations. Our results demonstrate that constructing\n(quadratic) polynomial building blocks based on real and imaginary polynomial\nroots enhances Poly-MgNet's capacity in terms of accuracy. Furthermore, our\napproach achieves an improved trade-off of model accuracy and number of weights\ncompared to ResNet as well as compared to specific configurations of MgNet.",
      "generated_abstract": "ponent of modern GPUs is the high-bandwidth memory (HBM), which\ndoes not fit into the traditional DDR4 bus. Instead, it must be co-aligned with\nthe host bus, which is significantly slower. This slows down many applications\nlike deep learning and GPU-accelerated simulations. To address this issue, we\npropose a novel approach that leverages the high-bandwidth memory (HBM) as a\n\"pipeline buffer\" to accelerate the execution of the GPU kernel. Specifically,\nwe design a new GPU kernel, called HBM-based Kernel, which leverages the\nHBM-based pipeline buffer to accelerate the execution of the GPU kernel. This\napproach improves the throughput of the GPU kernel by a factor of 2.5 compared\nto a standard kernel. We evaluate the performance of the HBM-based kernel on\nseveral work",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13768115942028986,
          "p": 0.2345679012345679,
          "f": 0.17351597707387262
        },
        "rouge-2": {
          "r": 0.015306122448979591,
          "p": 0.027777777777777776,
          "f": 0.019736837524239288
        },
        "rouge-l": {
          "r": 0.13043478260869565,
          "p": 0.2222222222222222,
          "f": 0.16438355698254845
        }
      }
    },
    {
      "paper_id": "physics.ao-ph.physics/ao-ph/2503.07466v1",
      "true_abstract": "Supercell thunderstorms are the most hazardous thunderstorm category and\nparticularly impactful to society. Their monitoring is challenging and often\nconfined to the radar networks of single countries. By exploiting\nkilometer-scale climate simulations, a first-of-its-kind characterization of\nsupercell occurrence in Europe is derived for the current and a warmer climate.\nDespite previous notions of supercells being uncommon in Europe, the model\nshows ~700 supercells per convective season. Occurrence peaks are co-located\nwith complex topography e.g. the Alps. The absolute frequency maximum lies\nalong the southern Alps with minima over the oceans and flat areas. Contrasting\na current-climate simulation with a pseudo-global-warming +3$^\\circ$C global\nwarming scenario, the future climate simulation shows an average increase of\nsupercell occurrence by 11 %. However, there is a spatial dipole of change with\nstrong increases in supercell frequencies in central and eastern Europe and a\ndecrease in frequency over the Iberian Peninsula and southwestern France.",
      "generated_abstract": "al distribution of wind turbines and their effect on the wind field\nis crucial for their operation and management. However, this topic has\noften been overlooked in the literature, with most studies focusing on\nstatistical properties, such as average wind speeds or the wind turbine\ndensity. In this study, we aim to provide a comprehensive analysis of the\nspatial distribution of wind turbines in a large-scale wind farm. We consider\nthe effects of the farm's spatial distribution on the wind field and its\nvariations in space and time. To this end, we use a global numerical model to\nsimulate the evolution of the wind turbine distribution. We then use the\nconventional statistical tools, such as the mean wind speed and the turbulent\nkinetic energy, to analyze the spatial distribution of wind turbines. In\naddition, we employ the Frenks-Smith model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.20512820512820512,
          "f": 0.1758241709262168
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.20512820512820512,
          "f": 0.1758241709262168
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.08028v1",
      "true_abstract": "Denoising diffusions provide a general strategy to sample from a probability\ndistribution $\\mu$ in $\\mathbb{R}^d$ by constructing a stochastic process\n$(\\hat{\\boldsymbol x}_t:t\\ge 0)$ in ${\\mathbb R}^d$ such that the distribution\nof $\\hat{\\boldsymbol x}_T$ at large times $T$ approximates $\\mu$. The drift\n${\\boldsymbol m}:{\\mathbb R}^d\\times{\\mathbb R}\\to{\\mathbb R}^d$ of this\ndiffusion process is learned from data (samples from $\\mu$) by minimizing the\nso-called score-matching objective. In order for the generating process to be\nefficient, it must be possible to evaluate (an approximation of) ${\\boldsymbol\nm}({\\boldsymbol y},t)$ in polynomial time.\n  Is every probability distribution $\\mu$, for which sampling is tractable,\nalso amenable to sampling via diffusions? We provide evidence to the contrary\nby constructing a probability distribution $\\mu$ for which sampling is easy,\nbut the drift of the diffusion process is intractable -- under a popular\nconjecture on information-computation gaps in statistical estimation. We\nfurther show that any polynomial-time computable drift can be modified in a way\nthat changes minimally the score matching objective and yet results in\nincorrect sampling.",
      "generated_abstract": "aper, we present a new and efficient method for Bayesian inference\nin a Bayesian neural network. We introduce a novel and simple method for\ninference in Bayesian neural networks by proposing a novel algorithm for\ncomputing the posterior distribution of the neural network weights. This\nalgorithm is based on the expectation and variance expansion, and is\ncomputationally efficient due to its use of a truncated Gaussian approximation\nto the posterior distribution. This approach allows for computation of the\nposterior distribution in a computationally efficient manner. We also\nintroduce a novel method for Bayesian inference in a Bayesian neural network.\nThis method allows for the computation of the posterior distribution of the\nneural network weights in a computationally efficient manner, using the\nexpectation and variance expansion. We demonstrate the efficiency of our\nmethods through numerical simulations, and we also provide theoretical\nguidance for the choice of parameters in the expectation and variance\nexpansion",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13761467889908258,
          "p": 0.23809523809523808,
          "f": 0.17441860000878867
        },
        "rouge-2": {
          "r": 0.03225806451612903,
          "p": 0.046296296296296294,
          "f": 0.038022808847894905
        },
        "rouge-l": {
          "r": 0.11926605504587157,
          "p": 0.20634920634920634,
          "f": 0.1511627860553003
        }
      }
    },
    {
      "paper_id": "cs.IR.cs/IR/2503.09223v1",
      "true_abstract": "Query and product relevance prediction is a critical component for ensuring a\nsmooth user experience in e-commerce search. Traditional studies mainly focus\non BERT-based models to assess the semantic relevance between queries and\nproducts. However, the discriminative paradigm and limited knowledge capacity\nof these approaches restrict their ability to comprehend the relevance between\nqueries and products fully. With the rapid advancement of Large Language Models\n(LLMs), recent research has begun to explore their application to industrial\nsearch systems, as LLMs provide extensive world knowledge and flexible\noptimization for reasoning processes. Nonetheless, directly leveraging LLMs for\nrelevance prediction tasks introduces new challenges, including a high demand\nfor data quality, the necessity for meticulous optimization of reasoning\nprocesses, and an optimistic bias that can result in over-recall. To overcome\nthe above problems, this paper proposes a novel framework called the LLM-based\nRElevance Framework (LREF) aimed at enhancing e-commerce search relevance. The\nframework comprises three main stages: supervised fine-tuning (SFT) with Data\nSelection, Multiple Chain of Thought (Multi-CoT) tuning, and Direct Preference\nOptimization (DPO) for de-biasing. We evaluate the performance of the framework\nthrough a series of offline experiments on large-scale real-world datasets, as\nwell as online A/B testing. The results indicate significant improvements in\nboth offline and online metrics. Ultimately, the model was deployed in a\nwell-known e-commerce application, yielding substantial commercial benefits.",
      "generated_abstract": "ng prevalence of large language models (LLMs) has made it crucial to\nevaluate the performance of LLMs. However, evaluating LLMs is challenging due\nto the complex nature of the LLMs and the lack of large-scale datasets that\nsufficiently reflect the real-world scenarios. This paper proposes a novel\nframework named LLM-TU, which can effectively evaluate LLMs in various\nscenarios. LLM-TU adopts the multi-task learning (MTL) framework to construct\na comprehensive evaluation dataset. The dataset contains 56,325 training\nexamples, 1,850 test examples, and 125 evaluation metrics, which can be\napplied to evaluate LLMs in various scenarios. The framework also adopts\nDifferential Privacy (DP) to ensure the fairness of evaluation metrics and\nprevent the leakage",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14465408805031446,
          "p": 0.3108108108108108,
          "f": 0.19742488836928299
        },
        "rouge-2": {
          "r": 0.04205607476635514,
          "p": 0.08571428571428572,
          "f": 0.05642632787217141
        },
        "rouge-l": {
          "r": 0.14465408805031446,
          "p": 0.3108108108108108,
          "f": 0.19742488836928299
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2502.10512v1",
      "true_abstract": "Blockchain technology has revolutionized financial markets by enabling\ndecentralized exchanges (DEXs) that operate without intermediaries. Uniswap V2,\na leading DEX, facilitates the rapid creation and trading of new tokens,\noffering high return potential but exposing investors to significant risks. In\nthis work, we analyze the financial impact of newly created tokens, assessing\ntheir market dynamics, profitability and liquidity manipulations. Our findings\nreveal that a significant portion of market liquidity is trapped in honeypots,\nreducing market efficiency and misleading investors. Applying a simple\nbuy-and-hold strategy, we are able to uncover some major risks associated with\ninvesting in newly created tokens, including the widespread presence of rug\npulls and sandwich attacks. We extract the optimal sandwich amount, revealing\nthat their proliferation in new tokens stems from higher profitability in\nlow-liquidity pools. Furthermore, we analyze the fundamental differences\nbetween token price evolution in swap time and physical time. Using clustering\ntechniques, we highlight these differences and identify typical patterns of\nhoneypot and sellable tokens. Our study provides insights into the risks and\nfinancial dynamics of decentralized markets and their challenges for investors.",
      "generated_abstract": "r investigates the effect of the size of the portfolio and the\nportfolio weighting strategy on the out-of-the-money (OTM) performance of\ninvestors. We propose an analytical formula for the OTM return for the\nportfolio with the maximum and minimum weights. Our findings demonstrate that\nthe OTM return of the portfolio with the maximum weight decreases when the\nportfolio size increases, while the OTM return of the portfolio with the\nminimum weight increases with the portfolio size. Furthermore, our analysis\nshows that the OTM return of the portfolio with the maximum weight decreases\nwhen the portfolio weighting strategy is the first-in, first-out (FIFO)\nstrategy, while the OTM return of the portfolio with the minimum weight\nincreases with the portfolio size when the portfolio weighting strategy is the\nsecond-in, second-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11382113821138211,
          "p": 0.2916666666666667,
          "f": 0.16374268602031403
        },
        "rouge-2": {
          "r": 0.005747126436781609,
          "p": 0.014705882352941176,
          "f": 0.008264458769211728
        },
        "rouge-l": {
          "r": 0.10569105691056911,
          "p": 0.2708333333333333,
          "f": 0.15204677958756552
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.08653v1",
      "true_abstract": "National forest inventory (NFI) data are often costly to collect, which\ninhibits efforts to estimate parameters of interest for small spatial,\ntemporal, or biophysical domains. Traditionally, design-based estimators are\nused to estimate status of forest parameters of interest, but are unreliable\nfor small areas where data are sparse. Additionally, design-based estimates\nconstructed directly from the survey data are often unavailable when sample\nsizes are especially small. Traditional model-based small area estimation\napproaches, such as the Fay-Herriot (FH) model, rely on these direct estimates\nfor inference; hence, missing direct estimates preclude the use of such\napproaches. Here, we detail a Bayesian spatio-temporal small area estimation\nmodel that efficiently leverages sparse NFI data to estimate status and trends\nfor forest parameters. The proposed model bypasses the use of direct estimates\nand instead uses plot-level NFI measurements along with auxiliary data\nincluding remotely sensed tree canopy cover. We produce forest carbon estimates\nfrom the United States NFI over 14 years across the contiguous US (CONUS) and\nconduct a simulation study to assess our proposed model's accuracy, precision,\nand bias, compared to that of a design-based estimator. The proposed model\nprovides improved precision and accuracy over traditional estimation methods,\nand provides useful insights into county-level forest carbon dynamics across\nthe CONUS.",
      "generated_abstract": "r studies the estimation of the mean and the variance of the\nregression coefficient in a random sample of data, assuming the data are\ndistributed according to a multivariate normal distribution. The estimation\nproblem is formulated as a maximum likelihood estimation problem. We provide a\nclosed-form solution for the maximum likelihood estimator of the mean and the\nvariance of the regression coefficient in a random sample of data, assuming\nthat the data are distributed according to a multivariate normal distribution.\nWe also provide an approximation for the maximum likelihood estimator of the\nmean and the variance of the regression coefficient in a random sample of data,\nassuming that the data are distributed according to a multivariate normal\ndistribution. The proposed estimators are evaluated by comparing their\nperformance with the maximum likelihood estimator of the mean and the\nvariance of the regression coefficient in a random sample of data, assuming that\nthe data are distributed according",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13740458015267176,
          "p": 0.3673469387755102,
          "f": 0.19999999603765442
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.04477611940298507,
          "f": 0.023715411125936052
        },
        "rouge-l": {
          "r": 0.12213740458015267,
          "p": 0.32653061224489793,
          "f": 0.1777777738154322
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.08930v1",
      "true_abstract": "We consider the problem of optimizing neural implicit surfaces for 3D\nreconstruction using acoustic images collected with drifting sensor poses. The\naccuracy of current state-of-the-art 3D acoustic modeling algorithms is highly\ndependent on accurate pose estimation; small errors in sensor pose can lead to\nsevere reconstruction artifacts. In this paper, we propose an algorithm that\njointly optimizes the neural scene representation and sonar poses. Our\nalgorithm does so by parameterizing the 6DoF poses as learnable parameters and\nbackpropagating gradients through the neural renderer and implicit\nrepresentation. We validated our algorithm on both real and simulated datasets.\nIt produces high-fidelity 3D reconstructions even under significant pose drift.",
      "generated_abstract": "r proposes an efficient hybrid deep learning-fed-beamforming framework\nfor multi-user massive multiple-input multiple-output (MU-MIMO) systems. The\nproposed method is designed to optimize the beamforming vector (BFV) with\nfederated learning (FL) and minimize the training loss through a gradient\ndescent (GD) process. To improve the learning efficiency, we design a\ndecentralized FL framework to reduce the communication cost. In addition, we\npropose a dynamic beamforming optimization (DBO) framework, which dynamically\noptimizes the BFV based on the current received signal strength indication\n(RSSI) at the receiver. The main contributions of this paper are summarized as\nfollows: (1) We propose a hybrid deep learning-fed-beamforming framework for\nmulti-user MU-MIMO systems,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23170731707317074,
          "p": 0.2261904761904762,
          "f": 0.22891565765132832
        },
        "rouge-2": {
          "r": 0.019230769230769232,
          "p": 0.020202020202020204,
          "f": 0.019704428500571528
        },
        "rouge-l": {
          "r": 0.2073170731707317,
          "p": 0.20238095238095238,
          "f": 0.20481927210915962
        }
      }
    },
    {
      "paper_id": "cs.AI.cs/DS/2503.07545v1",
      "true_abstract": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems.",
      "generated_abstract": "ork, we investigate the problem of constructing a plan of action for\na robot that can navigate a dynamic environment with a limited amount of\ninformation. To do so, we first propose a novel method for planning an action\nsequence that is valid with respect to a given state-action space. This is\nachieved through a novel extension of the action-selection principle, which\nallows us to ensure that the robot does not perform actions that are inconsistent\nwith the given state. We then propose a method for evaluating the quality of\nthese plans using a novel notion of action-plan consistency. Our approach\nenables the robot to perform a plan of action that is valid with respect to a\ngiven state-action space and that is also consistent with the given state. We\nalso present a theoretical framework for evaluating the quality of the\nplans, as well as a practical implementation using a robotic arm to demonstrate\nthe effectiveness",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18253968253968253,
          "p": 0.27710843373493976,
          "f": 0.2200956889915525
        },
        "rouge-2": {
          "r": 0.017045454545454544,
          "p": 0.023622047244094488,
          "f": 0.0198019753287816
        },
        "rouge-l": {
          "r": 0.1349206349206349,
          "p": 0.20481927710843373,
          "f": 0.16267942104896882
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.06964v1",
      "true_abstract": "This paper seeks to clarify whether the processing of two types of messages\nthat promote responsible gambling, namely personalized messages and normative\nmessages, varies across gamblers severity habits and rational thinking",
      "generated_abstract": "y examines the impact of the COVID-19 pandemic on the\nperformance of the Chilean stock market, comparing the performance of\nstocks that were trading at the beginning of the pandemic to those that were\ntrading at the end of the pandemic. The results show that during the pandemic,\nstocks that were trading at the beginning of the pandemic experienced a\nsignificant decline in their performance, while those that were trading at the\nend of the pandemic experienced a significant increase in their performance.\nThese results are explained by the fact that during the pandemic, investors\nexercised caution in their investment decisions due to uncertainty around the\npandemic. The results also show that stocks that were trading at the beginning\nof the pandemic were more volatile than stocks that were trading at the end of\nthe pandemic. The results also show that stocks",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.07547169811320754,
          "f": 0.09876542757506498
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10714285714285714,
          "p": 0.05660377358490566,
          "f": 0.0740740695503737
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/MN/2501.03235v1",
      "true_abstract": "Neural networks based on soft and biological matter constitute an interesting\npotential alternative to traditional implementations based on electric\ncircuits. DNA is a particularly promising system in this context due its\nnatural ability to store information. In recent years, researchers have started\nto construct neural networks that are based on DNA. In this chapter, I provide\na very basic introduction to the concept of DNA neural networks, aiming at an\naudience that is not familiar with biochemistry.",
      "generated_abstract": "ng pandemic has revealed the crucial role of human social networks in\ncommunication, collaboration, and decision-making. Although human networks\nconstitute an essential part of society, it remains unclear how they are shaped\nby social interactions. We propose a novel framework for understanding human\nnetworks by integrating complex networks theory with a novel stochastic model of\nsocial interactions. Our model captures key features of human networks, such as\ndegree distributions and clustering, through a non-linear, stochastic\nrepresentation of social interactions. Using this framework, we investigate\nthe role of social interactions in shaping human networks, particularly their\npower and size distributions. Our analysis reveals that human networks are\npower-law distributed, with a power-law exponent that is strongly dependent on\nthe degree distribution of the network. This dependence is driven by the\ninteraction strength, which we show can be interpreted as the strength of\nsocial interactions. Additionally",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26666666666666666,
          "p": 0.17582417582417584,
          "f": 0.211920525012061
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.015503875968992248,
          "f": 0.01970442886456951
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.16483516483516483,
          "f": 0.19867549189947822
        }
      }
    },
    {
      "paper_id": "cs.MA.econ/TH/2502.16719v2",
      "true_abstract": "Recent research on instant runoff voting (IRV) shows that it exhibits a\nstriking combinatorial property in one-dimensional preference spaces: there is\nan \"exclusion zone\" around the median voter such that if a candidate from the\nexclusion zone is on the ballot, then the winner must come from the exclusion\nzone. Thus, in one dimension, IRV cannot elect an extreme candidate as long as\na sufficiently moderate candidate is running. In this work, we examine the\nmathematical structure of exclusion zones as a broad phenomenon in more general\npreference spaces. We prove that with voters uniformly distributed over any\n$d$-dimensional hyperrectangle (for $d > 1$), IRV has no nontrivial exclusion\nzone. However, we also show that IRV exclusion zones are not solely a\none-dimensional phenomenon. For irregular higher-dimensional preference spaces\nwith fewer symmetries than hyperrectangles, IRV can exhibit nontrivial\nexclusion zones. As a further exploration, we study IRV exclusion zones in\ngraph voting, where nodes represent voters who prefer candidates closer to them\nin the graph. Here, we show that IRV exclusion zones present a surprising\ncomputational challenge: even checking whether a given set of positions is an\nIRV exclusion zone is NP-hard. We develop an efficient randomized approximation\nalgorithm for checking and finding exclusion zones. We also report on\ncomputational experiments with exclusion zones in two directions: (i) applying\nour approximation algorithm to a collection of real-world school friendship\nnetworks, we find that about 60% of these networks have probable nontrivial IRV\nexclusion zones; and (ii) performing an exhaustive computer search of small\ngraphs and trees, we also find nontrivial IRV exclusion zones in most graphs.\nWhile our focus is on IRV, the properties of exclusion zones we establish\nprovide a novel method for analyzing voting systems in metric spaces more\ngenerally.",
      "generated_abstract": "aper, we propose a framework to design the design of fair and\nefficient algorithms for the design of multi-criteria decision-making\nalgorithms. Our design framework provides a systematic approach to address the\ncomplexities of designing fair and efficient algorithms for multi-criteria\ndecision-making problems. The framework can be used to design a fair and\nefficient algorithm for any multi-criteria decision-making problem, provided\nthe decision problem is tractable and the objective function is monotone. We\nprovide a framework for designing a fair and efficient algorithm for any\nmulti-criteria decision-making problem. The framework is based on a\npre-design phase, in which we design a fair and efficient algorithm for a\nspecific problem. The post-design phase, in which we design the fair and\nefficient algorithm for a general problem. We show that the fair and efficient\nalgorithms for the specific problem can be easily extended to the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10982658959537572,
          "p": 0.35185185185185186,
          "f": 0.16740087743134943
        },
        "rouge-2": {
          "r": 0.011583011583011582,
          "p": 0.031578947368421054,
          "f": 0.016949148615500925
        },
        "rouge-l": {
          "r": 0.10982658959537572,
          "p": 0.35185185185185186,
          "f": 0.16740087743134943
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2502.05186v1",
      "true_abstract": "In an era where financial markets are heavily influenced by many static and\ndynamic factors, it has become increasingly critical to carefully integrate\ndiverse data sources with machine learning for accurate stock price prediction.\nThis paper explores a multimodal machine learning approach for stock price\nprediction by combining data from diverse sources, including traditional\nfinancial metrics, tweets, and news articles. We capture real-time market\ndynamics and investor mood through sentiment analysis on these textual data\nusing both ChatGPT-4o and FinBERT models. We look at how these integrated data\nstreams augment predictions made with a standard Long Short-Term Memory (LSTM\nmodel) to illustrate the extent of performance gains. Our study's results\nindicate that incorporating the mentioned data sources considerably increases\nthe forecast effectiveness of the reference model by up to 5%. We also provide\ninsights into the individual and combined predictive capacities of these\nmodalities, highlighting the substantial impact of incorporating sentiment\nanalysis from tweets and news articles. This research offers a systematic and\neffective framework for applying multimodal data analytics techniques in\nfinancial time series forecasting that provides a new view for investors to\nleverage data for decision-making.",
      "generated_abstract": "The financial sector faces challenges in integrating machine learning (ML)\ninto its operations, including the lack of a standardized ML framework and the\nlack of clear policies on how data is used. This paper proposes a framework\nfor the application of machine learning in financial markets and aims to\nclarify the legal and ethical implications of ML use in financial systems. The\npaper also examines the potential benefits and risks of ML adoption in the\nfinancial sector, highlighting potential opportunities for financial firms to\nleverage ML to enhance their operations. The paper discusses the role of\nethical guidelines and regulatory oversight in ensuring the integrity and\ntransparency of ML applications in financial systems. The paper concludes with\nrecommendations for further research and policy development.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18045112781954886,
          "p": 0.3287671232876712,
          "f": 0.23300970416203232
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.05357142857142857,
          "f": 0.041095885682117264
        },
        "rouge-l": {
          "r": 0.16541353383458646,
          "p": 0.3013698630136986,
          "f": 0.21359222843387698
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/DB/2503.05530v1",
      "true_abstract": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems.",
      "generated_abstract": "aper, we propose a novel multi-source, multi-objective, multi-object\ndecision-making framework for the selection of storage devices. The proposed\nframework is inspired by the hierarchical nature of data storage systems. In\nthis framework, we divide the storage devices into three categories, namely,\nraw, raw-reduced, and raw-reduced-reduced categories, and assign a weight to each\nof them based on their performance. We then formulate a multi-objective\noptimization problem for selecting the best storage device for each object,\nbased on the weights assigned to the categories. This problem is solved using\nthe genetic algorithm, which utilizes a selection of genes, namely, genes that\ncan be used to select the best storage device for each object, in addition to\nthe genes that are specific to each category. We evaluate our proposed\nframework through a simulation study, in which the performance of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14035087719298245,
          "p": 0.1927710843373494,
          "f": 0.16243654334716187
        },
        "rouge-2": {
          "r": 0.013793103448275862,
          "p": 0.01639344262295082,
          "f": 0.014981268445343842
        },
        "rouge-l": {
          "r": 0.14035087719298245,
          "p": 0.1927710843373494,
          "f": 0.16243654334716187
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.05974v1",
      "true_abstract": "Contrast enhancement, a key aspect of image-to-image translation (I2IT),\nimproves visual quality by adjusting intensity differences between pixels.\nHowever, many existing methods struggle to preserve fine-grained details, often\nleading to the loss of low-level features. This paper introduces LapLoss, a\nnovel approach designed for I2IT contrast enhancement, based on the Laplacian\npyramid-centric networks, forming the core of our proposed methodology. The\nproposed approach employs a multiple discriminator architecture, each operating\nat a different resolution to capture high-level features, in addition to\nmaintaining low-level details and textures under mixed lighting conditions. The\nproposed methodology computes the loss at multiple scales, balancing\nreconstruction accuracy and perceptual quality to enhance overall image\ngeneration. The distinct blend of the loss calculation at each level of the\npyramid, combined with the architecture of the Laplacian pyramid enables\nLapLoss to exceed contemporary contrast enhancement techniques. This framework\nachieves state-of-the-art results, consistently performing well across\ndifferent lighting conditions in the SICE dataset.",
      "generated_abstract": "tection and classification are essential tasks in computer vision\nand robotics, and are often achieved by convolutional neural networks (CNNs)\nwith large convolutional and pooling layers. However, these methods struggle to\ngeneralize to unseen data, especially in challenging scenarios where\nrepresentations from the previous layer are unavailable. To address this, we\npropose a novel training framework for CNNs that leverages unlabeled data from\nthe pretrained model to learn discriminative representations. Our approach\nenables the network to learn the data distribution in the feature space,\nleveraging the rich information from the pretrained model. This approach\nenables the network to learn the data distribution in the feature space,\nleveraging the rich information from the pretrained model. We introduce a\nvariational autoencoder (VAE) loss to encourage the learned representations to\nbe similar to the pretrained ones.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16216216216216217,
          "p": 0.225,
          "f": 0.1884816705243827
        },
        "rouge-2": {
          "r": 0.03424657534246575,
          "p": 0.04716981132075472,
          "f": 0.0396825348085166
        },
        "rouge-l": {
          "r": 0.15315315315315314,
          "p": 0.2125,
          "f": 0.17801046633590098
        }
      }
    },
    {
      "paper_id": "physics.med-ph.physics/med-ph/2503.06172v1",
      "true_abstract": "Noninvasive brain stimulation can activate neurons in the brain but requires\npower electronics with exceptionally high power in the mega-volt-ampere and\nhigh frequencies in the kilohertz range. Whereas oscillator circuits offered\nonly one or very few pulse shapes, modular power electronics solved a\nlong-standing problem for the first time and enabled arbitrary software-based\ndesign of the temporal shape of stimuli. However, synthesizing arbitrary\nstimuli with a high output quality requires a large number of modules. Systems\nwith few modules and pulse-width modulation may generate apparently smooth\ncurrent shapes in the highly inductive coil, but the stimulation effect of the\nneurons depends on the electric field and the electric field becomes a burst of\nultra-brief rectangular pulses. We propose an alternative solution that\nachieves high-resolution pulse shaping with fewer modules by implementing\nhigh-power wide-bandwidth voltage asymmetry. Rather than equal voltage steps,\nour system strategically assigns different voltages to each module to achieve a\nnear-exponential improvement in resolution. Compared to prior designs, our\nexperimental prototype achieved better output quality, although it uses only\nhalf the number of modules.",
      "generated_abstract": "ntext of the recent COVID-19 pandemic, the development of a rapid\ncomprehensive diagnostic tool for SARS-CoV-2, particularly in low- and middle-\nincome countries, is essential. While rapid antigen tests (RATs) have been\nused in some countries, they are not widely available or widely accepted due to\ntheir higher cost, limited availability, and lack of FDA-approval. The\ndevelopment of a rapid molecular test is also critical, as it can be done\nwithout the need for refrigeration. However, molecular tests are also more\nexpensive, complex, and require specialized equipment. This review discusses the\ncurrent state of rapid molecular tests for SARS-CoV-2. We focus on the\ntechnological limitations and possible solutions, as well as the potential\napplications of rapid molecular tests in low- and middle-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11570247933884298,
          "p": 0.1686746987951807,
          "f": 0.13725489713427544
        },
        "rouge-2": {
          "r": 0.012048192771084338,
          "p": 0.01818181818181818,
          "f": 0.014492748829029102
        },
        "rouge-l": {
          "r": 0.09917355371900827,
          "p": 0.14457831325301204,
          "f": 0.11764705399702058
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.08835v1",
      "true_abstract": "Roll-to-roll (R2R) printing technologies are promising for high-volume\ncontinuous production of substrate-based electronic products. One of the major\nchallenges in R2R flexible electronics printing is achieving tight alignment\ntolerances, as specified by the device resolution (usually at the micro-meter\nlevel), for multi-layer printed electronics. The alignment of the printed\npatterns in different layers is known as registration. Conventional\nregistration control methods rely on real-time feedback controllers, such as\nPID control, to regulate the web tension and the web speed. However, those\nmethods may lose effectiveness in compensating for recurring disturbances and\nsupporting effective mitigation of registration errors. In this paper, we\npropose a Spatial-Terminal Iterative Learning Control (STILC) method integrated\nwith PID control to iteratively learn and reduce registration error\ncycle-by-cycle, converging it to zero. This approach enables unprecedented\nprecision in the creation, integration, and manipulation of multi-layer\nmicrostructures in R2R processes. We theoretically prove the convergence of the\nproposed STILC-PID hybrid approach and validate its effectiveness through a\nsimulated registration error scenario caused by axis mismatch between roller\nand motor, a common issue in R2R systems. The results demonstrate that the\nSTILC-PID hybrid control method can fully eliminate the registration error\nafter a feasible number of iterations. Additionally, we analyze the impact of\ndifferent learning gains on the convergence performance of STILC.",
      "generated_abstract": "This paper proposes a novel reinforcement learning-based algorithm for\nmanaging the dynamics of multi-agent systems in real-world applications. The\nalgorithm employs an offline training phase to optimize the control laws of\neach agent, followed by a continuous-time online learning phase for\nadaptive control. The online learning phase is based on the Q-learning\nalgorithm, which allows for the computation of Q-values for each agent and its\nneighboring agents. The proposed algorithm is designed to efficiently handle\nreal-world problems where the dynamics of each agent are nonlinear and the\naction space is continuous. Numerical simulations are conducted to validate the\neffectiveness of the proposed algorithm in real-world applications. The results\ndemonstrate that the proposed algorithm can effectively manage the dynamics of\nmulti-agent systems, achieving smooth and stable motion while ensuring\ncollision-free behaviors.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.32098765432098764,
          "f": 0.23214285252590888
        },
        "rouge-2": {
          "r": 0.03940886699507389,
          "p": 0.07017543859649122,
          "f": 0.050473181513996976
        },
        "rouge-l": {
          "r": 0.16783216783216784,
          "p": 0.2962962962962963,
          "f": 0.21428570966876606
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.cond-mat/mtrl-sci/2503.10012v1",
      "true_abstract": "Epitaxial thin-film growth is a versatile and powerful technique for\nachieving a precise control of composition, stabilizing non-equilibrium phases,\ntailoring growth orientation, as well as forming heterointerfaces of various\nquantum materials. For synthesis of highly crystalline thin films, in-depth\nunderstanding of epitaxial relationship between the desired thin film and the\nsingle-crystalline substrates is necessary. In this study, we investigate\nepitaxial relationship in thin-film growth of triangular-lattice\nantiferromagnet CrSe on the (001) plane of Al2O3 and the lattice-matched (111)\nplane of yttria-stabilized zirconia (YSZ) substrates. Structural\ncharacterization using out-of-plane and in-plane x-ray diffraction shows that\nthe presence of 19.1o-twisted domains of CrSe significantly dominates the\naligned domain on the Al2O3 substrate while it reveals a single-domain\nformation on the YSZ substrate. The stability of the 19.1o-twisted domain\nrather than the aligned domain can be explained by rotational commensurate\nepitaxy, which is well reproduced by density functional theory calculations.\nThe single-domain CrSe thin film on the YSZ substrate exhibits a superior\nmetallic conductivity compared to the twisted-domain thin film on the Al2O3\nsubstrate, implying contribution of the grain boundary scattering mechanism to\nelectrical transport.",
      "generated_abstract": "The development of nanostructured, high-performance materials has revolutionized\nthe electronics industry, enabling the design of advanced electronic devices\nthat require ultra-thin, low-friction, and low-cost substrates. In recent\nyears, a shift in the focus from bulk materials to thin films has resulted in a\nrapid development in the field of advanced functional materials, with\nparticularly notable advances in graphene-based materials. In this review, we\nfocus on the most recent advancements in the field of graphene-based thin\nfilms and their applications in electronic devices, highlighting the key\ntechnological challenges and future research directions. We provide a\ncomprehensive overview of the current state-of-the-art in graphene-based\nmaterials, including the synthesis techniques, functionalization strategies,\nand device applications of graphene-based thin films.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.2328767123287671,
          "f": 0.17708332862033432
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.02857142857142857,
          "f": 0.02222221746913682
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.1917808219178082,
          "f": 0.14583332862033435
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2410.16101v1",
      "true_abstract": "Humans and other animals coactivate agonist and antagonist muscles in many\nmotor actions. Increases in muscle coactivation are thought to leverage\nviscoelastic properties of skeletal muscles to provide resistance against limb\nmotion. However, coactivation also emerges in scenarios where it seems\nparadoxical because the goal is not to resist limb motion but instead to\nrapidly mobilize the limb(s) or body to launch or correct movements. Here, we\npresent a new perspective on muscle coactivation: to prime the nervous system\nfor fast, task-dependent responses to sensory stimuli. We review distributed\nneural control mechanisms that may allow the healthy nervous system to leverage\nmuscle coactivation to produce fast and flexible responses to sensory feedback.",
      "generated_abstract": "vancements in deep learning and generative models have enabled the\ndevelopment of novel approaches for protein folding prediction. However,\ntraditional methods often struggle to model complex protein structures and\nfolding mechanisms, resulting in suboptimal performance. To address this\nchallenge, we propose FoldNet, a novel generative model that integrates\nstructural information from X-ray crystallography with protein sequence and\nstructure features. FoldNet uses an autoregressive decoder to capture the\nrelationships between residues in a protein sequence and its folded structure.\nThis approach enables the model to capture long-range dependencies, which\nfacilitate the generation of realistic protein structures. Additionally, we\nintroduce a sequence-to-sequence decoder to generate sequence-level features\nthat enhance the model's ability to capture protein-protein interactions.\nFoldNet outperforms existing methods in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12048192771084337,
          "p": 0.12195121951219512,
          "f": 0.12121211621230508
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12048192771084337,
          "p": 0.12195121951219512,
          "f": 0.12121211621230508
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/MM/2503.09938v1",
      "true_abstract": "Vision-and-language navigation (VLN) tasks require agents to navigate\nthree-dimensional environments guided by natural language instructions,\noffering substantial potential for diverse applications. However, the scarcity\nof training data impedes progress in this field. This paper introduces\nPanoGen++, a novel framework that addresses this limitation by generating\nvaried and pertinent panoramic environments for VLN tasks. PanoGen++\nincorporates pre-trained diffusion models with domain-specific fine-tuning,\nemploying parameter-efficient techniques such as low-rank adaptation to\nminimize computational costs. We investigate two settings for environment\ngeneration: masked image inpainting and recursive image outpainting. The former\nmaximizes novel environment creation by inpainting masked regions based on\ntextual descriptions, while the latter facilitates agents' learning of spatial\nrelationships within panoramas. Empirical evaluations on room-to-room (R2R),\nroom-for-room (R4R), and cooperative vision-and-dialog navigation (CVDN)\ndatasets reveal significant performance enhancements: a 2.44% increase in\nsuccess rate on the R2R test leaderboard, a 0.63% improvement on the R4R\nvalidation unseen set, and a 0.75-meter enhancement in goal progress on the\nCVDN validation unseen set. PanoGen++ augments the diversity and relevance of\ntraining environments, resulting in improved generalization and efficacy in VLN\ntasks.",
      "generated_abstract": "r explores the potential of deep learning for image-based\nmechanical characterization. We focus on a new application of the recently\nproposed image-based mechanical properties (IBMPs) framework, the mechanical\nimage-based texture (MINT) analysis. The MINT analysis involves the detection\nof texture patterns in images of mechanical specimens. This paper explores the\npossibility of using deep learning to automatically extract texture patterns\nfrom the images, enabling the analysis of the mechanical properties of the\nmaterials. We propose a novel framework that integrates the state-of-the-art\nimage processing techniques, including deep learning, to perform the MINT\nanalysis. We evaluate the performance of the proposed method on the\nbenchmark datasets. The results show that the proposed framework outperforms\nstate-of-the-art methods in terms of accuracy and robustness. This approach\nprovides a powerful tool for the characterization of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.145985401459854,
          "p": 0.273972602739726,
          "f": 0.19047618594058968
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.04201680672268908,
          "f": 0.034013600623583454
        },
        "rouge-l": {
          "r": 0.1386861313868613,
          "p": 0.2602739726027397,
          "f": 0.18095237641678016
        }
      }
    },
    {
      "paper_id": "math.MG.math/FA/2503.09542v2",
      "true_abstract": "In 1959, Marcus and Ree proved that any bistochastic matrix $A$ satisfies\n$\\Delta_n(A):= \\max_{\\sigma\\in S_n}\\sum_{i=1}^{n}A(i, \\sigma(i))-\\sum_{i,\nj=1}^n A(i, j)^2 \\geq 0$. Erd\\H{o}s asked to characterize the bistochastic\nmatrices satisfying $\\Delta_n(A)=0$. It was recently proved that there are only\nfinitely many such matrices for any $n$. However, a complete list of such\nmatrices was obtained in dimension $n=2, 3$ only recently, arXiv:2306.05518. In\nthis paper, we characterize all $4\\times 4$ bistochastic matrices satisfying\n$\\Delta_4(A)=0$. Furthermore, we show that for $n\\geq 3$, $\\Delta_n(A)=\\alpha$\nhas uncountably many solutions when $\\alpha\\notin \\{0, (n-1)/4\\}$. This answers\na question raised in arXiv:2410.06612. We extend the Marcus$\\unicode{x2013}$Ree\ninequality to infinite bistochastic arrays and bistochastic kernels. Our\ninvestigation into $4\\times 4$ Erd\\H{o}s matrices also raises several\nintriguing questions that are of independent interest. We propose several\nquestions and conjectures and present numerical evidence for them.",
      "generated_abstract": "aper, we consider a family of non-commutative Banach spaces\n$E_n$, $n\\in\\mathbb{N}$, where each $E_n$ is a non-commutative $n$-fold\nproduct of a separable normed space and a Hilbert space. We prove that the\nfamily $E_n$ is a non-commutative $n$-fold product of a separable normed\nspace and a Hilbert space, and we show that each $E_n$ is isomorphic to a\nnon-commutative $n$-fold product of a separable normed space and a Hilbert\nspace. We also show that each $E_n$ is a non-commutative $n$-fold\nproduct of a separable normed space and a Hilbert space, and we show that each\n$E_n$ is isomorphic to a non-commutative $",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09900990099009901,
          "p": 0.3125,
          "f": 0.15037593619537576
        },
        "rouge-2": {
          "r": 0.015037593984962405,
          "p": 0.044444444444444446,
          "f": 0.022471906334428107
        },
        "rouge-l": {
          "r": 0.09900990099009901,
          "p": 0.3125,
          "f": 0.15037593619537576
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/optics/2503.10490v1",
      "true_abstract": "The effect of a constant electric field on two-photon absorption in a\nsemiconductor is calculated using an independent-particle theory. The\ntheoretical framework is an extension of a theory of the one-photon\nFranz-Keldysh effect [Wahlstrand and Sipe, Phys. Rev. B 82, 075206 (2010)]. The\ntheory includes the effect of the constant field, including field-induced\ncoupling between closely spaced bands, in the electronic wavefunctions and\ncalculates optical absorption perturbatively. Numerical calculations are\nperformed using a 14-band $\\mathbf{k} \\cdot\\mathbf{p}$ band structure model for\nGaAs. For all nonzero tensor elements, field-enabled two-photon absorption\n(TPA) below the band gap and Franz-Keldysh oscillations in the TPA spectrum are\npredicted, with a generally larger effect in tensor elements with more\ncomponents parallel to the constant electric field direction. Some tensor\nelements that are zero in the absence of a field become nonzero in the presence\nof the constant electric field and depend on its sign. Notably, these elements\nare linear in the electric field to lowest order and may be substantial away\nfrom band structure critical points at room temperature and/or with a\nnon-uniform field. Electric-field-induced changes in the carrier injection rate\ndue to interference between one- and two-photon absorption are also calculated.\nThe electric field enables this bichromatic coherent control process for\npolarization configurations where it is normally forbidden, and also modifies\nthe spectrum of the process for configurations where it is allowed by crystal\nsymmetry.",
      "generated_abstract": "igate the evolution of an optical beam with a 1D optical system\nin the presence of a linearly polarized magnetic field. In this work, we use a\nnonlinear theory approach to calculate the evolution of the magnetic field in\nterms of the optical beam and the polarization state of the field. The beam is\na linearly polarized Gaussian beam with a constant intensity, and the\npolarization state is given by the Zeeman effect. We investigate the case of a\nmagnetic field in the XY plane. In the first case, we find that the magnetic\nfield can be described by the Bessel function of the first kind, and we show\nthat the magnetic field evolves with time in a similar way to the beam. In the\nsecond case, we find that the magnetic field can be described by a sine function\nand we show that the magnetic field evolves as a complex function. We then\nconsider a magnetic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15789473684210525,
          "p": 0.3181818181818182,
          "f": 0.21105527194868826
        },
        "rouge-2": {
          "r": 0.04477611940298507,
          "p": 0.08035714285714286,
          "f": 0.05750798262470818
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.2878787878787879,
          "f": 0.19095476943612547
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2412.09157v1",
      "true_abstract": "This paper studies the robust reinsurance and investment games for\ncompetitive insurers. Model uncertainty is characterized by a class of\nequivalent probability measures. Each insurer is concerned with relative\nperformance under the worst-case scenario. Insurers' surplus processes are\napproximated by drifted Brownian motion with common and idiosyncratic insurance\nrisks. The insurers can purchase proportional reinsurance to divide the\ninsurance risk with the reinsurance premium calculated by the variance\nprinciple. We consider an incomplete market driven by the 4/2 stochastic\nvolatility mode. This paper formulates the robust mean-field game for a\nnon-linear system originating from the variance principle and the 4/2 model.\nFor the case of an exponential utility function, we derive closed-form\nsolutions for the $n$-insurer game and the corresponding mean-field game. We\nshow that relative concerns lead to new hedging terms in the investment and\nreinsurance strategies. Model uncertainty can significantly change the\ninsurers' hedging demands. The hedging demands in the investment-reinsurance\nstrategies exhibit highly non-linear dependence with the insurers' competitive\ncoefficients, risk aversion and ambiguity aversion coefficients. Finally,\nnumerical results demonstrate the herd effect of competition.",
      "generated_abstract": "We study the problem of finding the optimal trading strategy for a large\nfinancial firm. In this paper, we model the firm as a mean-variance optimizing\nagent who trades stocks across different markets. Our model is based on\nmulti-market portfolio theory, which considers multiple markets with\ndiscrete-time stochastic processes. We solve the problem using an\nalternating-direction method of multipliers (ADMM) algorithm, which is\nill-conditioned in practice. To tackle this issue, we develop a regularized\nADMM algorithm that effectively handles the ill-conditioning of the\ndiscrete-time stochastic process. The proposed algorithm is efficient and\neffective, and its computational complexity is lower than the ADMM algorithm.\nWe also present numerical results to demonstrate the effectiveness of the\nproposed algorithm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17857142857142858,
          "p": 0.24096385542168675,
          "f": 0.20512820023879036
        },
        "rouge-2": {
          "r": 0.018292682926829267,
          "p": 0.02727272727272727,
          "f": 0.021897805413182366
        },
        "rouge-l": {
          "r": 0.16071428571428573,
          "p": 0.21686746987951808,
          "f": 0.1846153797259699
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.14317v2",
      "true_abstract": "We develop a model that captures peer effect heterogeneity by modeling the\nendogenous spillover to be linear in ordered peer outcomes. Unlike the\ncanonical linear-in-means model, our approach accounts for the distribution of\npeer outcomes as well as the size of peer groups. Under a minimal condition,\nour model admits a unique equilibrium and is therefore tractable and\nidentified. Simulations show our estimator has good finite sample performance.\nFinally, we apply our model to educational data from Norway, finding that\nhigher-performing friends disproportionately drive GPA spillovers. Our\nframework provides new insights into the structure of peer effects beyond\naggregate measures.",
      "generated_abstract": "sis of large datasets poses significant challenges, particularly for\nresearchers with limited resources. In this paper, we introduce a novel\nmethodology for multivariate panel data analysis, which combines the\nindependence of random effects (IRE) model with the recently developed\nsemiparametric Bayesian method for linear regression (SBMLR). The IRE model\nprovides a flexible framework for modeling random effects, while SBMLR\naccelerates the estimation process by leveraging the posterior distribution of\nthe latent variables. The proposed methodology is applied to a dataset of\nmultivariate panel data, which includes 43 variables from 1829 observations of\n982 individuals. The empirical results demonstrate that the proposed methodology\ncan accurately estimate the parameters of interest, which is consistent with\ntheoretical expectations. The methodology provides a practical tool for\nmultivariate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23076923076923078,
          "p": 0.20930232558139536,
          "f": 0.21951219013384904
        },
        "rouge-2": {
          "r": 0.010526315789473684,
          "p": 0.00847457627118644,
          "f": 0.009389666419804668
        },
        "rouge-l": {
          "r": 0.20512820512820512,
          "p": 0.18604651162790697,
          "f": 0.19512194623141
        }
      }
    },
    {
      "paper_id": "math.AT.math/AT/2503.02839v1",
      "true_abstract": "We show that the $\\infty$-category of normed algebras in genuine $G$-spectra,\nas introduced by Bachmann-Hoyois, is modelled by strictly commutative algebras\nin $G$-symmetric spectra for any finite group $G$. We moreover provide an\nanalogous description of Schwede's ultra-commutative global ring spectra in\nhigher categorical terms.\n  Using these new descriptions, we exhibit the $\\infty$-category of\nultra-commutative global ring spectra as a partially lax limit of the\n$\\infty$-categories of genuine $G$-spectra for varying $G$, in analogy with the\nnon-multiplicative comparison of Nardin, Pol, and the second author.\n  Along the way, we establish various new results in parametrized higher\nalgebra, which we hope to be of independent interest.",
      "generated_abstract": "We study the $L^p$ norm of the iterated Fourier transform of a $p$-adic\nmanifold with a non-degenerate quadratic form. We prove that the norm is\nbounded, uniformly in the dimension, the $p$-adic point degree, and the\ndiscrepancy of the form. The bounds are sharp in the sense that they are\nachieved for a certain class of $p$-adic manifolds, and in particular, the\nresult does not hold for any compact $p$-adic manifold. We also prove a\n$L^p$ bound for the iterated Fourier transform of the Euclidean $p$-adic\nmanifold, and show that it is sharp in the same sense.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16216216216216217,
          "p": 0.23076923076923078,
          "f": 0.19047618562862195
        },
        "rouge-2": {
          "r": 0.05102040816326531,
          "p": 0.05813953488372093,
          "f": 0.054347821108223525
        },
        "rouge-l": {
          "r": 0.14864864864864866,
          "p": 0.21153846153846154,
          "f": 0.17460316975560608
        }
      }
    },
    {
      "paper_id": "math.NA.stat/CO/2503.05533v1",
      "true_abstract": "Multilevel sampling methods, such as multilevel and multifidelity Monte\nCarlo, multilevel stochastic collocation, or delayed acceptance Markov chain\nMonte Carlo, have become standard uncertainty quantification tools for a wide\nclass of forward and inverse problems. The underlying idea is to achieve faster\nconvergence by leveraging a hierarchy of models, such as partial differential\nequation (PDE) or stochastic differential equation (SDE) discretisations with\nincreasing accuracy. By optimally redistributing work among the levels,\nmultilevel methods can achieve significant performance improvement compared to\nsingle level methods working with one high-fidelity model. Intuitively,\napproximate solutions on coarser levels can tolerate large computational error\nwithout affecting the overall accuracy. We show how this can be used in\nhigh-performance computing applications to obtain a significant performance\ngain.\n  As a use case, we analyse the computational error in the standard multilevel\nMonte Carlo method and formulate an adaptive algorithm which determines a\nminimum required computational accuracy on each level of discretisation. We\nshow two examples of how the inexactness can be converted into actual gains\nusing an elliptic PDE with lognormal random coefficients. Using a low precision\nsparse direct solver combined with iterative refinement results in a simulated\ngain in memory references of up to $3.5\\times$ compared to the reference double\nprecision solver; while using a MINRES iterative solver, a practical speedup of\nup to $1.5\\times$ in terms of FLOPs is achieved. These results provide a step\nin the direction of energy-aware scientific computing, with significant\npotential for energy savings.",
      "generated_abstract": "In this work, we study the maximum likelihood estimator (MLE) for a\nnon-concave non-smooth potential in the framework of functional data analysis\n(FDA). In particular, we propose a novel estimator that is based on a\nconvolution of the original estimator with a function that is non-concave and\nnon-smooth. This approach is motivated by the recent breakthroughs in\nfunctional data analysis (FDA) for non-concave non-smooth potentials, which\nlead to non-concave and non-smooth potentials. We first present a result that\nprovides conditions on the potential function for the convergence of the\nestimator. Then, we present numerical experiments to illustrate the effect of\nthe potential function on the performance of the estimator. Finally, we\npresent some applications to the analysis of high-throughput data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11585365853658537,
          "p": 0.3064516129032258,
          "f": 0.16814158893883635
        },
        "rouge-2": {
          "r": 0.017167381974248927,
          "p": 0.038461538461538464,
          "f": 0.023738868136199905
        },
        "rouge-l": {
          "r": 0.10975609756097561,
          "p": 0.2903225806451613,
          "f": 0.15929203141671247
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.05682v1",
      "true_abstract": "Multi-contrast magnetic resonance imaging (MRI) plays a vital role in brain\ntumor segmentation and diagnosis by leveraging complementary information from\ndifferent contrasts. Each contrast highlights specific tumor characteristics,\nenabling a comprehensive understanding of tumor morphology, edema, and\npathological heterogeneity. However, existing methods still face the challenges\nof multi-level specificity perception across different contrasts, especially\nwith limited annotations. These challenges include data heterogeneity,\ngranularity differences, and interference from redundant information. To\naddress these limitations, we propose a Task-oriented Uncertainty Collaborative\nLearning (TUCL) framework for multi-contrast MRI segmentation. TUCL introduces\na task-oriented prompt attention (TPA) module with intra-prompt and\ncross-prompt attention mechanisms to dynamically model feature interactions\nacross contrasts and tasks. Additionally, a cyclic process is designed to map\nthe predictions back to the prompt to ensure that the prompts are effectively\nutilized. In the decoding stage, the TUCL framework proposes a dual-path\nuncertainty refinement (DUR) strategy which ensures robust segmentation by\nrefining predictions iteratively. Extensive experimental results on limited\nlabeled data demonstrate that TUCL significantly improves segmentation accuracy\n(88.2\\% in Dice and 10.853 mm in HD95). It shows that TUCL has the potential to\nextract multi-contrast information and reduce the reliance on extensive\nannotations. The code is available at:\nhttps://github.com/Zhenxuan-Zhang/TUCL_BrainSeg.",
      "generated_abstract": "f deep learning models in medical imaging has had a significant\neffect on the field of computer-aided detection (CAD). The development of\ndeep learning models has led to significant improvements in CAD accuracy.\nHowever, the rapid increase in the number of models in the deep learning space\nmakes it difficult to determine the most effective model for a given task.\nFurthermore, due to the vast amount of data generated by deep learning models,\nit is difficult to determine which models are the most accurate. This paper\ndescribes a methodology for selecting the most accurate deep learning models\nfor the purpose of CAD. The methodology utilizes the DeepMetrics library, a\nPython library that allows for the comparison of deep learning models. The\nmethodology consists of five steps: data generation, feature extraction, model\nselection, model training, and model evaluation. The methodology was applied\nto several deep learning models in the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14189189189189189,
          "p": 0.2625,
          "f": 0.1842105217605418
        },
        "rouge-2": {
          "r": 0.005,
          "p": 0.008064516129032258,
          "f": 0.006172834781286957
        },
        "rouge-l": {
          "r": 0.13513513513513514,
          "p": 0.25,
          "f": 0.17543859193598044
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2409.10805v1",
      "true_abstract": "Dengue virus (DENV) is a mosquito-borne virus with a significant human health\nconcern. With 390 million infections annually and 96 million showing clinical\nsymptoms, severe dengue can lead to life-threatening conditions like dengue\nhemorrhagic fever (DHF) and dengue shock syndrome (DSS). The only FDA-approved\nvaccine, Dengvaxia, has limitations due to antibody-dependent enhancement\n(ADE), necessitating careful administration. The recent pre-approval of TAK-003\nby WHO in 2024 highlights ongoing efforts to improve vaccine options. This\nreview explores recent advancements in dengue vaccine development, emphasizing\npotential utility of mRNA-based vaccines. By examining current clinical trial\ndata and innovations, we aim to identify promising strategies to address the\nlimitations of existing vaccines and enhance global dengue prevention efforts.",
      "generated_abstract": "olding is a complex process that requires the formation of stable\nprotein structures from a sequence of amino acids. This process is driven by\nthe availability of energy from the environment, which is provided by\ninteractions between the amino acids of the polypeptide chain and the\nexternally available energy sources. The availability of energy is controlled\nby the thermostat, which regulates the energy levels of the system, and by the\nenergy barriers, which restrict the time taken to reach the target energy\nlevel. The energy barriers and thermostat control the rate at which the\npolypeptide chain folds. The thermostat is the most important of the two, as\nit is the rate-limiting step in protein folding. It can be thought of as a\ntype of thermodynamic process, and it is the process that determines the\nshape and structure of the folded protein",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12222222222222222,
          "p": 0.1506849315068493,
          "f": 0.1349693202077611
        },
        "rouge-2": {
          "r": 0.008849557522123894,
          "p": 0.008,
          "f": 0.008403356357251741
        },
        "rouge-l": {
          "r": 0.12222222222222222,
          "p": 0.1506849315068493,
          "f": 0.1349693202077611
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.01698v1",
      "true_abstract": "Fractional vegetation coverage (FVC) and its spatio-temporal variations are\ncritical indicators of regional ecological changes, which are of great\nsignificance to study the laws of surface variation and analyze regional\necosystem. Under the development of RS and GIS technology, this analysis\nemploys Landsat satellite images in 1994, 2008, 2013 and 2016 to estimate FVC\nin Yalong River Basin based on the Dimidiate Pixel Model. With consideration of\nthe vegetation coverage condition and land surface law in the study area, the\nresearch further analyzes the Spatio-temporal variations as well as the\ninfluencing factors of FVC in terms of topography and land use types\nrespectively. The results show that since 1994, FVC in Yalong River Basin has\nexperienced a downward trend yet displaying an uptick from 2013. Moreover,\ndifferent land use types indicate the versatility of land covers in Yalong\nRiver Basin, with grassland and forest performing probably the most important\nfactors that can induce changes to the stability of FVC in whole basin.\nOverall, the research reflects the impact of human activities on vegetation in\nYalong River Basin, and provides available data and theoretical basis for\necological assessment, ecological restoration and environmental protection.",
      "generated_abstract": "aper, we propose an agent-based model that captures the behavior\nof a population of cooperative, social insects, known as ants, that interact\nthrough chemical signals. We focus on the behavior of the ants under conditions\nof heterogeneity, such as the presence of sub-populations of ants with different\nevolutionary histories and levels of cooperation. We develop a model of ants\nthat exhibit differential social behavior, based on the notion of a social\ninteraction space, and study how it affects the population's behavior. Our\nmodel includes the ants' social interactions with their neighbors, as well as\ntheir interactions with predators and prey. In addition, we include a\nnon-cooperative component in the model, which represents the ants' behavior\nunder conditions where they are not cooperating. We explore how the different\ncomponents of the model influence the population",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13333333333333333,
          "p": 0.20253164556962025,
          "f": 0.16080401531274477
        },
        "rouge-2": {
          "r": 0.040229885057471264,
          "p": 0.056451612903225805,
          "f": 0.046979860912572
        },
        "rouge-l": {
          "r": 0.10833333333333334,
          "p": 0.16455696202531644,
          "f": 0.13065326154390058
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/AS/2503.06273v1",
      "true_abstract": "We explore a novel zero-shot Audio-Visual Speech Recognition (AVSR)\nframework, dubbed Zero-AVSR, which enables speech recognition in target\nlanguages without requiring any audio-visual speech data in those languages.\nSpecifically, we introduce the Audio-Visual Speech Romanizer (AV-Romanizer),\nwhich learns language-agnostic speech representations by predicting Roman text.\nThen, by leveraging the strong multilingual modeling capabilities of Large\nLanguage Models (LLMs), we propose converting the predicted Roman text into\nlanguage-specific graphemes, forming the proposed Cascaded Zero-AVSR. Taking it\na step further, we explore a unified Zero-AVSR approach by directly integrating\nthe audio-visual speech representations encoded by the AV-Romanizer into the\nLLM. This is achieved through finetuning the adapter and the LLM using our\nproposed multi-task learning scheme. To capture the wide spectrum of phonetic\nand linguistic diversity, we also introduce a Multilingual Audio-Visual\nRomanized Corpus (MARC) consisting of 2,916 hours of audio-visual speech data\nacross 82 languages, along with transcriptions in both language-specific\ngraphemes and Roman text. Extensive analysis and experiments confirm that the\nproposed Zero-AVSR framework has the potential to expand language support\nbeyond the languages seen during the training of the AV-Romanizer.",
      "generated_abstract": "development of large language models (LLMs) has paved the way\nfor developing more intelligent conversational agents. However, the reliance\non large models and LLMs in these systems has also raised concerns regarding\nprivacy and security. To address these challenges, we propose a novel framework\nfor developing privacy-preserving conversational agents. Our framework\nencompasses three key components: 1) the development of privacy-preserving\nconversational agents, 2) the design of an LLM-free dialogue management\nframework, and 3) the implementation of privacy-preserving dialogue\nmanagement. Our framework not only provides a foundation for the development\nof privacy-preserving conversational agents but also facilitates the\nimplementation of privacy-preserving dialogue management. Through experiments\non a real-world dataset, we demonstrate that our framework achieves superior\nperformance compared to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1487603305785124,
          "p": 0.2465753424657534,
          "f": 0.1855670056153684
        },
        "rouge-2": {
          "r": 0.011834319526627219,
          "p": 0.02,
          "f": 0.01486988380481345
        },
        "rouge-l": {
          "r": 0.14049586776859505,
          "p": 0.2328767123287671,
          "f": 0.1752577272648529
        }
      }
    },
    {
      "paper_id": "nlin.AO.nlin/AO/2503.01812v1",
      "true_abstract": "A novel model for dynamical traps in intermittent human control is proposed.\nIt describes probabilistic, step-wise transitions between two modes of a\nsubject's behavior - active and passive phases in controlling an object's\ndynamics - using an original stochastic differential equation. This equation\ngoverns time variations of a special variable, denoted as $\\zeta$, between two\nlimit values, $\\zeta=0$ and $\\zeta=1$. The introduced trap function,\n$\\Omega(\\Delta)$, quantifies the subject's perception of the object's deviation\nfrom a desired state, thereby determining the relative priority of the two\naction modes. Notably, these transitions - referred to as the subject's action\npoints - occur before the trap function reaches its limit values,\n$\\Omega(\\Delta)=0$ or $\\Omega(\\Delta)=1$. This characteristic enables the\napplication of the proposed model to describe intermittent human control over\nreal objects.",
      "generated_abstract": "tudy, we propose a novel approach to investigate the nonlinear\nsteady state of an adiabatic oscillator. The analysis is based on a\nheteroclinic-type bifurcation diagram of the system and its stability. Our\napproach uses the theory of heteroclinic orbits to determine the nonlinear\nsteady state. The results show that the oscillator can be in either stable or\nunstable steady state. We find that the nonlinear steady state is sensitive to\nthe initial conditions, which is different from the linear steady state. The\nnonlinear steady state is also sensitive to the bifurcation parameters, which\ncan be used to control the oscillator. Furthermore, we show that the oscillator\ncan be in the saddle-resonance or periodic-resonance state depending on the\nbifurcation parameters. Finally, we discuss the impact of the bifurcation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1590909090909091,
          "p": 0.2153846153846154,
          "f": 0.1830065310607033
        },
        "rouge-2": {
          "r": 0.008403361344537815,
          "p": 0.01,
          "f": 0.009132415128961641
        },
        "rouge-l": {
          "r": 0.1590909090909091,
          "p": 0.2153846153846154,
          "f": 0.1830065310607033
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/SR/2503.10415v1",
      "true_abstract": "This article focuses on NGC7538 IRS1, one of the most luminous and studied HC\nHII regions in the northern hemisphere. Our aim is to identify the young\nstellar objects (YSOs) embedded within the ionized gas and study their\nkinematic structures. This work expands on a recent survey called \"Protostellar\nOutflows at the EarliesT Stages\" (POETS), which has been devoted to studying\nyoung outflow emission on scales of 10-100 au near luminous YSOs, before they\nstart photoionizing the surrounding medium. We carried out multi-epoch Very\nLong Baseline Array observations of the 22 GHz water masers toward NGC7538 IRS1\nto measure the maser 3D velocities, which, following POETS' findings, are\nreliable tracers of the protostellar winds. Recently, we reobserved the water\nmasers in NGC7538 IRS1 with sensitive global very long baseline interferometry\n(VLBI) observations to map weaker maser emission. Our study confirms the\npresence of two embedded YSOs, IRS1a and IRS1b, at the center of the two linear\ndistributions of 6.7 GHz methanol masers observed in the southern and northern\ncores of the HC HII region, which have been previously interpreted in terms of\nedge-on rotating disks. The water masers trace an extended (~200 au) stationary\nshock front adjacent to the inner portion of the disk around IRS1a. This shock\nfront corresponds to the edge of the southern tip of the ionized core and might\nbe produced by the interaction of the disk wind ejected from IRS1a with the\ninfalling envelope. The water masers closer to IRS1b follow the same LSR\nvelocity (Vlsr) pattern of the 6.7~GHz masers rotating in the disk, but the\ndirection and amplitude of the water maser proper motions are inconsistent with\nrotation. We propose that these water masers are tracing a photo-evaporated\ndisk wind, where the maser Vlsr traces mainly the disk rotation and the proper\nmotions the poloidal velocity of the wind.",
      "generated_abstract": "h for exoplanets through the transit method is a powerful tool to\nfind and characterise planets around distant stars, with the most recent\nsurvey by the Kepler mission yielding over 300,000 transiting planets. However,\nthe vast majority of these planets are small and orbits close to the host\nstar, making them difficult to detect and characterise. The high-dispersion\ninstruments on the European Southern Observatory (ESO) Very Large Telescope\n(VLT) and the Atacama Large Millimetre/submillimetre Array (ALMA) have been\ndeveloped with the goal of detecting and characterising planets around\ndistant stars. In this work, we present the ALMA/AJA-transit survey of 160\nstars across 15 nearby open clusters. By combining the high-disp",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11351351351351352,
          "p": 0.25301204819277107,
          "f": 0.1567164136347183
        },
        "rouge-2": {
          "r": 0.02527075812274368,
          "p": 0.06666666666666667,
          "f": 0.03664921067336465
        },
        "rouge-l": {
          "r": 0.0972972972972973,
          "p": 0.21686746987951808,
          "f": 0.1343283539332258
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2411.10139v1",
      "true_abstract": "The basic principle of any version of insurance is the paradigm that\nexchanging risk by sharing it in a pool is beneficial for the participants. In\ncase of independent risks with a finite mean this is the case for risk averse\ndecision makers. The situation may be very different in case of infinite mean\nmodels. In that case it is known that risk sharing may have a negative effect,\nwhich is sometimes called the nondiversification trap. This phenomenon is well\nknown for infinite mean stable distributions. In a series of recent papers\nsimilar results for infinite mean Pareto and Fr\\'echet distributions have been\nobtained. We further investigate this property by showing that many of these\nresults can be obtained as special cases of a simple result demonstrating that\nthis holds for any distribution that is more skewed than a Cauchy distribution.\nWe also relate this to the situation of deadly catastrophic risks, where we\nassume a positive probability for an infinite value. That case gives a very\nsimple intuition why this phenomenon can occur for such catastrophic risks. We\nalso mention several open problems and conjectures in this context.",
      "generated_abstract": "In this paper, we propose a risk-sensitive portfolio selection model for\nportfolio allocation with risk aversion and uncertainty. We formulate a\nminimization problem that maximizes the expected return in the limit as the\nuncertainty parameter tends to infinity. We propose a modified Benders decomposition\nmethod to solve this problem. Our approach includes a risk-sensitive\nreinforcement learning mechanism to select the optimal strategy, which\nreinforces the optimal portfolio. We develop a method to select the risk\nparameter, and use it to generate risk-neutral distributions for the portfolio\nvalues and the expected return. Finally, we apply the proposed framework to\nreal-world data, and the results show that the proposed model outperforms the\nalternative methods, especially in terms of return and risk. The empirical\nresults also demonstrate the effectiveness of the risk-sensitive reinforcement\nlearning mechanism.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1926605504587156,
          "p": 0.27631578947368424,
          "f": 0.22702702218612136
        },
        "rouge-2": {
          "r": 0.005494505494505495,
          "p": 0.00847457627118644,
          "f": 0.00666666189422564
        },
        "rouge-l": {
          "r": 0.1743119266055046,
          "p": 0.25,
          "f": 0.20540540056449977
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.02763v1",
      "true_abstract": "The Index of Dissimilarity (ID), widely utilized in economic literature as a\nmeasure of segregation, is inadequate for cross-country or time series studies\ndue to its failure to account for structural variations across countries' labor\nmarkets or changes over time within a single country's labor market. Building\non the works of Karmel and MacLachlan (1988) and Blackburn et al. (1993), we\npropose a new measure - the standardized ID - that isolates structural\ndifferences from true differences in segregation across space or time. A key\nadvantage of our proposed measure lies in its ease of implementation and\ninterpretation, even when working with datasets encompassing a large number of\ncountries or time periods. Moreover, our measure can be consistently applied in\nthe case of lumpy sectors or occupations that account for a large fraction of\nthe workforce. We illustrate the new measure in an analysis of the\ncross-country relationship between economic development (as measured by GDP per\ncapita) and occupational and sectoral gender segregation. Comparing the crude\nID with the standardized ID, we show that the crude ID overestimates the\npositive correlation between income and segregation, especially between low-\nand middle-income countries. This suggests that analyses relying on the crude\nID risk overestimating the importance of income differentials in explaining\ncross-country variation in gender segregation.",
      "generated_abstract": "r examines the role of innovation in the emergence of regional\nsystems of production, focusing on the impact of the introduction of\ntrade-related technologies (TRTs) in the production of textiles and clothing\nin the 1970s. Using the Regional Industrial Production Index (RIPI), a novel\ndataset, we quantify the impact of TRTs on regional productivity. We find that\nthe introduction of TRTs reduced regional productivity by 0.23, but this\nreduction was concentrated in regions that were already producing clothing and\ntextiles. This suggests that TRTs did not have a significant impact on\ninternational trade, but rather on regional production in the textile and\nclothing industry. Our results suggest that the introduction of TRTs was not a\nsuccess story for regional economic integration, with the reduction in\nproductivity in the 197",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12213740458015267,
          "p": 0.2191780821917808,
          "f": 0.15686274050221083
        },
        "rouge-2": {
          "r": 0.035,
          "p": 0.061946902654867256,
          "f": 0.044728429891088474
        },
        "rouge-l": {
          "r": 0.12213740458015267,
          "p": 0.2191780821917808,
          "f": 0.15686274050221083
        }
      }
    },
    {
      "paper_id": "physics.med-ph.physics/med-ph/2503.06131v1",
      "true_abstract": "This study focuses on the rotation of the hips and shoulders during a\nbaseball bat swing, analyzing the time-series changes in rotational angles,\nrotational velocities, and axes using marker position data obtained from a\nmotion capture system with 12 infrared cameras. Previous studies have examined\nfactors such as ground reaction forces, muscle activation patterns, rotational\nenergy, angular velocity, and angles during a swing. However, to the best of\nour knowledge, the hip and shoulder rotational motions have not been adequately\nvisualized or compared. In particular, there is a lack of analysis regarding\nthe coordination and timing differences between hip and shoulder movements\nduring the swing. Therefore, this study aims to quantitatively compare the hip\nand shoulder rotational movements during the swing between skilled and\nunskilled players and visualizes the differences between them. Based on the\nobtained data, the study aims to improve the understanding of bat swing\nmechanics by visualizing the coordinated body movements during the swing.",
      "generated_abstract": "y presents a novel 3D-printed, low-cost, and low-weight robotic\ninterventional imaging system that can be used for minimally invasive surgery\n(MIS). The 3D-printed robotic system comprises a robotic arm, a surgical\ncamera, and an ultrasound probe. The robotic arm is capable of performing\nmultiple types of imaging procedures, including ultrasound-guided fine needle\naspiration, ultrasound-guided fine needle biopsy, and ultrasound-guided\nbiopsy. The surgical camera is a 3D-printed, open-source, and low-cost device\nthat can be used for surgical imaging. The ultrasound probe is a 3D-printed,\nopen-source, and low-cost device that can be used for ultrasound-guided",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.050505050505050504,
          "p": 0.10204081632653061,
          "f": 0.06756756313823988
        },
        "rouge-2": {
          "r": 0.007194244604316547,
          "p": 0.013513513513513514,
          "f": 0.009389666827130846
        },
        "rouge-l": {
          "r": 0.050505050505050504,
          "p": 0.10204081632653061,
          "f": 0.06756756313823988
        }
      }
    },
    {
      "paper_id": "nlin.CG.nlin/CG/2411.03601v3",
      "true_abstract": "A one-dimensional cellular automaton $\\tau : A^\\mathbb{Z} \\to A^\\mathbb{Z}$\nis a transformation of the full shift defined via a finite neighborhood $S\n\\subset \\mathbb{Z}$ and a local function $\\mu : A^S \\to A$. We study the family\nof cellular automata whose finite neighborhood $S$ is an interval containing\n$0$, and there exists a pattern $p \\in A^S$ satisfying that $\\mu(z) = z(0)$ if\nand only if $z \\neq p$; this means that these cellular automata have a unique\n\\emph{active transition}. Despite its simplicity, this family presents\ninteresting and subtle problems, as the behavior of the cellular automaton\ncompletely depends on the structure of $p$. We show that every cellular\nautomaton $\\tau$ with a unique active transition $p \\in A^S$ is either\nidempotent or strictly almost equicontinuous, and we completely characterize\neach one of these situations in terms of $p$. In essence, the idempotence of\n$\\tau$ depends on the existence of a certain subpattern of $p$ with a\ntranslational symmetry.",
      "generated_abstract": "We study the stability of a class of periodic solutions to a three-dimensional\nnonlinear Schr\\\"odinger equation with a time-dependent phase factor, which is\nthe generalization of a model of a pendulum. We show that the stability of a\nperiodic solution can be reduced to that of a periodic orbit. We also give an\nexample of a pendulum-like system with a time-dependent phase factor which\ndoes not admit a periodic solution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0970873786407767,
          "p": 0.25,
          "f": 0.13986013583060308
        },
        "rouge-2": {
          "r": 0.041379310344827586,
          "p": 0.10714285714285714,
          "f": 0.05970148851761122
        },
        "rouge-l": {
          "r": 0.0970873786407767,
          "p": 0.25,
          "f": 0.13986013583060308
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2502.09962v1",
      "true_abstract": "We consider a two-sided matching problem in which the agents on one side have\ndichotomous preferences and the other side representing institutions has strict\npreferences (priorities). It captures several important applications in\nmatching market design in which the agents are only interested in getting\nmatched to an acceptable institution. These include centralized daycare\nassignment and healthcare rationing. We present a compelling new mechanism that\nsatisfies many prominent and desirable properties including individual\nrationality, maximum size, fairness, Pareto-efficiency on both sides,\nstrategyproofness on both sides, non-bossiness and having polynomial time\nrunning time. As a result, we answer an open problem whether there exists a\nmechanism that is agent-strategyproof, maximum, fair and non-bossy.",
      "generated_abstract": "the problem of optimizing the allocation of a set of jobs among a\ncluster of agents. A job can be assigned to one agent, or to several agents.\nAgents can perform the job, or they can wait. The objective is to allocate\njobs to agents such that the agents perform their jobs, maximize the\naggregate revenue from the jobs, and minimize the average waiting time. The\njobs are competitive, and can be completed in a single round. The set of jobs\nis finite, and the agents are restricted to have a finite budget. We provide\nan $\\Omega(n \\log^2 n)$ approximation algorithm for this problem, and a\n$\\tilde{O}(\\log n)$ approximation algorithm for a slightly more general problem\nthat is identical to ours, but where jobs can be completed in multiple rounds\nand the agents can perform the job in the same round, and the budget is\nunrestrict",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19047619047619047,
          "p": 0.20512820512820512,
          "f": 0.19753085920438967
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.03076923076923077,
          "f": 0.0341880292497633
        },
        "rouge-l": {
          "r": 0.19047619047619047,
          "p": 0.20512820512820512,
          "f": 0.19753085920438967
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/TR/2411.05013v1",
      "true_abstract": "This study utilizes machine learning algorithms to analyze and organize\nknowledge in the field of algorithmic trading. By filtering a dataset of 136\nmillion research papers, we identified 14,342 relevant articles published\nbetween 1956 and Q1 2020. We compare traditional practices-such as\nkeyword-based algorithms and embedding techniques-with state-of-the-art topic\nmodeling methods that employ dimensionality reduction and clustering. This\ncomparison allows us to assess the popularity and evolution of different\napproaches and themes within algorithmic trading. We demonstrate the usefulness\nof Natural Language Processing (NLP) in the automatic extraction of knowledge,\nhighlighting the new possibilities created by the latest iterations of Large\nLanguage Models (LLMs) like ChatGPT. The rationale for focusing on this topic\nstems from our analysis, which reveals that research articles on algorithmic\ntrading are increasing at a faster rate than the overall number of\npublications. While stocks and main indices comprise more than half of all\nassets considered, certain asset classes, such as cryptocurrencies, exhibit a\nmuch stronger growth trend. Machine learning models have become the most\npopular methods in recent years. The study demonstrates the efficacy of LLMs in\nrefining datasets and addressing intricate questions about the analyzed\narticles, such as comparing the efficiency of different models. Our research\nshows that by decomposing tasks into smaller components and incorporating\nreasoning steps, we can effectively tackle complex questions supported by case\nanalyses. This approach contributes to a deeper understanding of algorithmic\ntrading methodologies and underscores the potential of advanced NLP techniques\nin literature reviews.",
      "generated_abstract": "y addresses the challenge of synthesizing high-fidelity market\nsynthetic datasets by proposing a novel approach that uses a large language\nmodel (LLM) to generate synthetic datasets from real market data. Our approach\nenables the generation of synthetic datasets that are in line with the\nreal-world market dynamics, while maintaining the same statistical properties\nof the real data. The proposed approach is based on a hybrid approach,\nincorporating a LLM with a GAN-based model for generating synthetic datasets.\nThe LLM generates synthetic data by utilizing the real market data and\nconverting it into text. The GAN then converts the synthetic data into the\nspecified dataset format. The generated synthetic datasets are then\ninterpolated using the real dataset, which yields the synthetic dataset. The\nproposed approach is evaluated through a comprehensive study involving\nnumerical simulations, statistical analysis",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1206896551724138,
          "p": 0.26582278481012656,
          "f": 0.166007900843319
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09770114942528736,
          "p": 0.21518987341772153,
          "f": 0.13438734748363526
        }
      }
    },
    {
      "paper_id": "math.CO.math/AC/2503.07299v1",
      "true_abstract": "Let $\\varphi:V\\times V\\to W$ be a bilinear map of finite vector spaces $V$\nand $W$ over a finite field $\\mathbb{F}_q$. We present asymptotic bounds on the\nnumber of isomorphism classes of bilinear maps under the natural action of\n$\\mathrm{GL}(V)$ and $\\mathrm{GL}(W)$, when $\\dim(V)$ and $\\dim(W)$ are\nlinearly related.\n  As motivations and applications of the results, we present almost tight upper\nbounds on the number of $p$-groups of Frattini class $2$ as first studied by\nHigman (Proc. Lond. Math. Soc., 1960). Such bounds lead to answers for some\nopen questions by Blackburn, Neumann, and Venkataraman (Cambridge Tracts in\nMathematics, 2007). Further applications include sampling matrix spaces with\nthe trivial automorphism group, and asymptotic bounds on the number of\nisomorphism classes of finite cube-zero commutative algebras.",
      "generated_abstract": "hbb{F}$ be a field of arbitrary characteristic and $M$ a\nalgebraic $\\mathbb{F}$-module. We define an invariant $\\mathrm{Ext}^1_M$ of\n$M$ over $\\mathbb{F}$ by\n  \\begin{equation*}\n    \\mathrm{Ext}^1_M(A,B):=\\frac{\\mathrm{Ker}(A\\to B) \\cap \\mathrm{Im}(B\\to\n    A)}{\\mathrm{Im}(A\\to B)}.\n  \\end{equation*}\nThe first result of the paper concerns the relation between $\\mathrm{Ext}^1_M$\nand the $k$-linear extension of $\\mathrm{Ext}^1_M$ from $M$ to $M\\otimes_\\mathbb{F}\nk$. The second result concerns the relation between $\\mathrm{Ext}^1_M$ and\n$\\mathrm{Ext}^1_M$ over",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12359550561797752,
          "p": 0.2619047619047619,
          "f": 0.16793892694132054
        },
        "rouge-2": {
          "r": 0.018018018018018018,
          "p": 0.03508771929824561,
          "f": 0.02380951932610629
        },
        "rouge-l": {
          "r": 0.0898876404494382,
          "p": 0.19047619047619047,
          "f": 0.12213740022376333
        }
      }
    },
    {
      "paper_id": "math.LO.math/LO/2503.08566v1",
      "true_abstract": "We provide a characterization of those relation algebras which are isomorphic\nto the algebras of compatible relations of some $\\Z_2$-set. We further prove\nthat this class is finitely axiomatizable in first-order logic in the language\nof relation algebras.",
      "generated_abstract": "er the class of $\\beta$-separated sets in the real line. It was\nintroduced by Schelp and studied in \\cite{schelp1986infinite}, where the\nclassification of $\\beta$-separated sets was determined. In this paper, we\ndescribe the set of $\\beta$-separated sets and characterize it by the set of\nmeasurable sets $M(\\beta)$, which is the closure of all $\\beta$-separated\nsets under the $\\beta$-measurable topology. In the case of $\\beta = 1$, the\nclass of $\\beta$-separated sets coincides with the class of $\\beta$-measurable\nsets.\n  In this context, we define the set $F_\\beta$ of $\\beta$-finitely separated\nsets. We show that it is an open set in $M(\\beta)$, and $M(\\beta) = F_\\beta",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3103448275862069,
          "p": 0.16363636363636364,
          "f": 0.21428570976473932
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.011363636363636364,
          "f": 0.01612902813735797
        },
        "rouge-l": {
          "r": 0.27586206896551724,
          "p": 0.14545454545454545,
          "f": 0.1904761859552155
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/PR/2405.02170v1",
      "true_abstract": "We consider the Fourier-Laplace transforms of a broad class of polynomial\nOrnstein-Uhlenbeck (OU) volatility models, including the well-known\nStein-Stein, Sch\\\"obel-Zhu, one-factor Bergomi, and the recently introduced\nQuintic OU models motivated by the SPX-VIX joint calibration problem. We show\nthe connection between the joint Fourier-Laplace functional of the log-price\nand the integrated variance, and the solution of an infinite dimensional\nRiccati equation. Next, under some non-vanishing conditions of the\nFourier-Laplace transforms, we establish an existence result for such Riccati\nequation and we provide a discretized approximation of the joint characteristic\nfunctional that is exponentially entire. On the practical side, we develop a\nnumerical scheme to solve the stiff infinite dimensional Riccati equations and\ndemonstrate the efficiency and accuracy of the scheme for pricing SPX options\nand volatility swaps using Fourier and Laplace inversions, with specific\nexamples of the Quintic OU and the one-factor Bergomi models and their\ncalibration to real market data.",
      "generated_abstract": "y explores the effects of the covariance structure of the\ndistribution of realized volatilities on the out-of-sample performance of\nhedge funds. We use a dataset of 4,296 hedge funds from 2006 to 2022, and we\nemploy a machine learning approach to analyze the effects of the covariance\nstructure on hedge fund performance. The results show that hedge funds with\nhigher-variance, less-correlated realized volatilities outperform their\ncounterparts with lower-variance, more-correlated realized volatilities. Our\nfindings also reveal that hedge funds with more-correlated realized volatilities\noutperform their counterparts with lower-correlated realized volatilities, and\nthat hedge funds with higher realized volatility standard deviations outperform\nthose with lower realized volatility standard deviations. The find",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1276595744680851,
          "p": 0.21052631578947367,
          "f": 0.15894039265119964
        },
        "rouge-2": {
          "r": 0.014598540145985401,
          "p": 0.024096385542168676,
          "f": 0.018181813483059063
        },
        "rouge-l": {
          "r": 0.1276595744680851,
          "p": 0.21052631578947367,
          "f": 0.15894039265119964
        }
      }
    },
    {
      "paper_id": "cs.CL.stat/AP/2502.16556v1",
      "true_abstract": "This study examines how Large Language Models (LLMs) perform when tackling\nquantitative management decision problems in a zero-shot setting. Drawing on\n900 responses generated by five leading models across 20 diverse managerial\nscenarios, our analysis explores whether these base models can deliver accurate\nnumerical decisions under varying presentation formats, scenario complexities,\nand repeated attempts. Contrary to prior findings, we observed no significant\neffects of text presentation format (direct, narrative, or tabular) or text\nlength on accuracy. However, scenario complexity -- particularly in terms of\nconstraints and irrelevant parameters -- strongly influenced performance, often\ndegrading accuracy. Surprisingly, the models handled tasks requiring multiple\nsolution steps more effectively than expected. Notably, only 28.8\\% of\nresponses were exactly correct, highlighting limitations in precision. We\nfurther found no significant ``learning effect'' across iterations: performance\nremained stable across repeated queries. Nonetheless, significant variations\nemerged among the five tested LLMs, with some showing superior binary accuracy.\nOverall, these findings underscore both the promise and the pitfalls of\nharnessing LLMs for complex quantitative decision-making, informing managers\nand researchers about optimal deployment strategies.",
      "generated_abstract": "asing availability of large language models (LLMs) has led to an\nvariety of applications in various fields, including text generation.\nHowever, the limitations of LLMs in generating coherent and structured text\nhave been well documented. In this paper, we present a novel approach to\ngenerate structured text using a pre-trained LLM. Our approach is based on a\nhybrid model that combines a transformer-based language model with a\ncontrastive learning framework. The model is trained on both textual and\nsemantic representations, resulting in more effective and efficient text\ngeneration. The proposed approach is evaluated on the task of text generation\nfrom a structured dataset, and the results demonstrate that it is able to\ngenerate coherent, structured, and high-quality text. The model also shows\npromising potential for other tasks, such as text summarization and text\ngeneration from a prompt.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12413793103448276,
          "p": 0.20224719101123595,
          "f": 0.15384614913251532
        },
        "rouge-2": {
          "r": 0.011494252873563218,
          "p": 0.015748031496062992,
          "f": 0.013289031666760435
        },
        "rouge-l": {
          "r": 0.12413793103448276,
          "p": 0.20224719101123595,
          "f": 0.15384614913251532
        }
      }
    },
    {
      "paper_id": "cs.GT.cs/GT/2503.04542v1",
      "true_abstract": "Professional networks are a key determinant of individuals' labor market\noutcomes. They may also play a role in either exacerbating or ameliorating\ninequality of opportunity across demographic groups. In a theoretical model of\nprofessional network formation, we show that inequality can increase even\nwithout exogenous in-group preferences, confirming and complementing existing\ntheoretical literature. Increased inequality emerges from the differential\nleverage privileged and unprivileged individuals have in forming connections\ndue to their asymmetric ex ante prospects. This is a formalization of a source\nof inequality in the labor market which has not been previously explored.\n  We next show how inequality-aware platforms may reduce inequality by\nsubsidizing connections, through link recommendations that reduce costs,\nbetween privileged and unprivileged individuals. Indeed, mixed-privilege\nconnections turn out to be welfare improving, over all possible equilibria,\ncompared to not recommending links or recommending some smaller fraction of\ncross-group links. Taken together, these two findings reveal a stark reality:\nprofessional networking platforms that fail to foster integration in the link\nformation process risk reducing the platform's utility to its users and\nexacerbating existing labor market inequality.",
      "generated_abstract": "aper, we study the problem of finding the best approximation of a\nsingle vertex to a given distance set in a bipartite graph. The set of vertices\nin the distance set is called the support. We propose to approximate the vertex\nby an arbitrary point in $\\mathbb{R}^d$, where $d$ is the dimension of the\ndistance set. We show that the problem of finding the best approximation to a\nsingle vertex to a distance set in a bipartite graph can be solved in\npolynomial time if the distance set is of dimension at most $d=O(n)$, where\n$n$ is the number of vertices in the graph. We show that the problem of finding\nthe best approximation to a single vertex to a distance set in a bipartite\ngraph can be solved in polynomial time if the distance set is of dimension at\nmost $O(\\sqrt{n})$, where $n$ is the number of vertices in the graph",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1015625,
          "p": 0.2653061224489796,
          "f": 0.14689265136327376
        },
        "rouge-2": {
          "r": 0.01744186046511628,
          "p": 0.0375,
          "f": 0.023809519475939313
        },
        "rouge-l": {
          "r": 0.09375,
          "p": 0.24489795918367346,
          "f": 0.13559321633502516
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PR/2412.08987v1",
      "true_abstract": "Computational efficiency is essential for enhancing the accuracy and\npracticality of pricing complex financial derivatives. In this paper, we\ndiscuss Isogeometric Analysis (IGA) for valuing financial derivatives, modeled\nby two nonlinear Black-Scholes PDEs: the Leland model for European call with\ntransaction costs and the AFV model for convertible bonds with default options.\nWe compare the solutions of IGA with finite difference methods (FDM) and finite\nelement methods (FEM). In particular, very accurate solutions can be\nnumerically calculated on far less mesh (knots) than FDM or FEM, by using\nnon-uniform knots and weighted cubic NURBS, which in turn reduces the\ncomputational time significantly.",
      "generated_abstract": "aper, we introduce a novel approach for pricing and hedging\nprincipal protected derivatives, which is a combination of the Black-Scholes\nmodel and the Heston model. The model is characterized by the parameter\n$\\alpha$. The Heston model is used for the volatility, whereas the Black-Scholes\nmodel is used for the forward rate. We use the so-called stochastic volatility\nmodel to model the volatility, which has been developed by Heston. We define\nthe Black-Scholes model and the Heston model as the Black-Scholes model and\nthe Heston model, respectively, with the parameter $\\alpha$ equal to 0.5. The\npricing of the derivative contracts is performed by using the option pricing\nmethod. The pricing of the derivative contracts is performed by using the\noption pricing method. We use the European option and the American option for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1951219512195122,
          "p": 0.2857142857142857,
          "f": 0.23188405314849833
        },
        "rouge-2": {
          "r": 0.02,
          "p": 0.021739130434782608,
          "f": 0.020833328342015085
        },
        "rouge-l": {
          "r": 0.18292682926829268,
          "p": 0.26785714285714285,
          "f": 0.21739129952530992
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.17679v1",
      "true_abstract": "Adverse childhood experiences (ACEs) have been linked to a wide range of\nnegative health outcomes in adulthood. However, few studies have investigated\nwhat specific combinations of ACEs most substantially impact mental health. In\nthis article, we provide the protocol for our observational study of the\neffects of combinations of ACEs on adult depression. We use data from the 2023\nBehavioral Risk Factor Surveillance System (BRFSS) to assess these effects. We\nwill evaluate the replicability of our findings by splitting the sample into\ntwo discrete subpopulations of individuals. We employ data turnover for this\nanalysis, enabling a single team of statisticians and domain experts to\ncollaboratively evaluate the strength of evidence, and also integrating both\nqualitative and quantitative insights from exploratory data analysis. We\noutline our analysis plan using this method and conclude with a brief\ndiscussion of several specifics for our study.",
      "generated_abstract": "We investigate the problem of designing a linear estimator that is robust to\ndifferent outliers. We propose a robust estimator that is based on a\ntwo-stage procedure. The first stage is a standard bootstrap estimator, which\nis robust to outliers. The second stage uses a robust estimator for the\noutlier-free sample mean to estimate the population mean. We show that the\nrobust estimator is asymptotically normal and that the two-stage estimator is\nasymptotically normal. We also show that the two-stage estimator is\nconsistent and asymptotically normally distributed. We illustrate the\neffectiveness of the proposed estimator in a simulation study and in an\napplication to the U.S. Census Bureau.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12,
          "p": 0.21818181818181817,
          "f": 0.1548387050988555
        },
        "rouge-2": {
          "r": 0.0072992700729927005,
          "p": 0.010869565217391304,
          "f": 0.008733619647225238
        },
        "rouge-l": {
          "r": 0.12,
          "p": 0.21818181818181817,
          "f": 0.1548387050988555
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.17431v1",
      "true_abstract": "In this paper, we show exponential dimensional dependence for the Hermite\nmethod of moments as a statistical test for Gaussianity in the case of i.i.d.\nGaussian variables, by constructing a lower bound for the the\nKolmogorov-Smirnov distance and an upper bound for the convex distance.",
      "generated_abstract": "er the problem of estimating a smooth function from a finite number\nof samples. In this setting, the function is a sum of two terms, where the\nfirst term is a zero-mean Gaussian and the second term is a zero-mean\nexponential. In this paper, we consider the problem of estimating the mean of\nthe first term from the samples. We propose a new estimator, which we call\n$\\mathrm{MF-VaR}_{\\alpha}$, that is based on the use of the moment function\nvalued random variable (MF-VaR). Our estimator is based on the idea that the\nestimator of the mean of the first term should be close to the estimator of the\nmean of the second term, and we demonstrate that our estimator is close to the\nestimator of the mean of the first term. We also show that our estimator is\ncloser to the estimator of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3142857142857143,
          "p": 0.18333333333333332,
          "f": 0.2315789427146815
        },
        "rouge-2": {
          "r": 0.06976744186046512,
          "p": 0.030927835051546393,
          "f": 0.04285713860102083
        },
        "rouge-l": {
          "r": 0.2571428571428571,
          "p": 0.15,
          "f": 0.18947367955678682
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2503.00239v1",
      "true_abstract": "Bayesian posterior approximation has become more accessible to practitioners\nthan ever, thanks to modern black-box software. While these tools provide\nhighly accurate approximations with minimal user effort, certain posterior\ngeometries remain notoriously difficult for standard methods. As a result,\nresearch into alternative approximation techniques continues to flourish. In\nmany papers, authors validate their new approaches by testing them on posterior\nshapes deemed challenging or \"wild.\" However, these shapes are not always\ndirectly linked to real-world applications where they naturally occur. In this\nnote, we present examples of practical applications that give rise to some\ncommonly used benchmark posterior shapes.",
      "generated_abstract": "opment of large language models (LLMs) has revolutionized natural\nlanguage processing (NLP). However, their high computational requirements and\nlack of interpretability raise concerns about their potential misuse.\nExisting legal frameworks are based on the premise that an LLM is an\nintelligent agent, and, thus, its output should be treated as an expert\nopinion. However, this approach assumes that the LLM is fully autonomous and\nunderstands its own output. In reality, LLMs are complex systems, whose\nperformance is heavily influenced by the training data, the architecture, the\nhyperparameters, and the training algorithm. In this paper, we propose the\nframework of LLM-aware expert opinions (LEOs) to address the problem of\ninterpreting LLM outputs, in the context of their legal implications. We\npropose a general framework that incorporates two key concepts: (1) an L",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14942528735632185,
          "p": 0.13829787234042554,
          "f": 0.1436464038472576
        },
        "rouge-2": {
          "r": 0.01020408163265306,
          "p": 0.0078125,
          "f": 0.008849552610230621
        },
        "rouge-l": {
          "r": 0.14942528735632185,
          "p": 0.13829787234042554,
          "f": 0.1436464038472576
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.10172v1",
      "true_abstract": "In this paper, for solving nonlinear systems we propose two\npseudoinverse-free greedy block methods with momentum by combining the\nresidual-based weighted nonlinear Kaczmarz and heavy ball methods. Without the\nfull column rank assumptions on Jacobi matrices of nonlinear systems, we\nprovide a thorough convergence analysis, and derive upper bounds for the\nconvergence rates of the new methods. Numerical experiments demonstrate that\nthe proposed methods with momentum are much more effective than the existing\nones.",
      "generated_abstract": "In this paper, we introduce a new method to solve the inverse scattering\nproblems in the three-dimensional anisotropic setting. The inverse scattering\nproblems arise in the study of the scattering properties of materials, such as\nelectromagnetic, acoustic, and elastic materials. We provide a unified\nformulation for the three-dimensional anisotropic scattering problems. The\nunified formulation allows for the modeling of the scattering problem in the\nanisotropic environment, which consists of a three-dimensional scattering\nproblem in a planar environment. Our approach is based on the analysis of\ngeometric and physical constraints on the scattering problem. We also discuss\nthe numerical methods to solve the inverse scattering problems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22807017543859648,
          "p": 0.22807017543859648,
          "f": 0.2280701704385966
        },
        "rouge-2": {
          "r": 0.07042253521126761,
          "p": 0.060240963855421686,
          "f": 0.06493505996542458
        },
        "rouge-l": {
          "r": 0.21052631578947367,
          "p": 0.21052631578947367,
          "f": 0.2105263107894738
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.16800v1",
      "true_abstract": "This paper introduces a new solution concept for the Cooperative Game with\nPublic Externalities, called the w-value, which is characterized by three\nproperties (axioms), namely Pareto-optimality (PO), Market-equilbrium (ME) and\nFiscal-balance (FB). Additionally, the implementation mechanism for w-value is\nalso provided. The w-value exists and is unique. It belongs to the core. And,\nmore specifically, it belongs to the -core. Meanwhile, the computational cost\nof w-value is very low. Therefore, the w-value is a theoretically more\ncompelling solution concept than the existing cooperation game solutions when\nanalyzing cooperative games with public externalities. A numerical illustration\nshows the calculation steps of w-value. Meanwhile, the w-value well explains\nthe reason why the mandatory emission reduction mechanism must be transformed\ninto a \"nationally determined contribution\" mechanism in current international\nclimate negotiations.",
      "generated_abstract": "We study a class of games with two strategies, where the payoff function is\ngiven by a function of the players' utility functions and a non-negative\nfunction of their strategies. We show that the existence of a Nash equilibrium\ncan be characterized by a certain linear inequality constraint. We then\ngeneralize the classical theory of linear inequalities to the games with two\nstrategies, where we show that the existence of a Nash equilibrium can be\ncharacterized by a certain linear inequality constraint, and we characterize\nthe class of such games.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11702127659574468,
          "p": 0.24444444444444444,
          "f": 0.1582733769163088
        },
        "rouge-2": {
          "r": 0.025210084033613446,
          "p": 0.04477611940298507,
          "f": 0.03225805990692632
        },
        "rouge-l": {
          "r": 0.11702127659574468,
          "p": 0.24444444444444444,
          "f": 0.1582733769163088
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2402.01820v1",
      "true_abstract": "We consider a stochastic volatility model where the dynamics of the\nvolatility are given by a possibly infinite linear combination of the elements\nof the time extended signature of a Brownian motion. First, we show that the\nmodel is remarkably universal, as it includes, but is not limited to, the\ncelebrated Stein-Stein, Bergomi, and Heston models, together with some\npath-dependent variants. Second, we derive the joint characteristic functional\nof the log-price and integrated variance provided that some infinite\ndimensional extended tensor algebra valued Riccati equation admits a solution.\nThis allows us to price and (quadratically) hedge certain European and\npath-dependent options using Fourier inversion techniques. We highlight the\nefficiency and accuracy of these Fourier techniques in a comprehensive\nnumerical study.",
      "generated_abstract": "e a new method for portfolio selection in high-dimensional\nportfolio optimization problems. Our approach, which we call\n\"portfolio-optimization with sub-gaussian risk measures\", employs a\nprobability measure $\\mu$ on the space of sub-Gaussian risk measures, and\nintegrates over this measure to obtain an optimal portfolio for a given\nhigh-dimensional risk-neutral distribution. This is done by solving a\nvariational problem, which we show to be equivalent to the standard\noptimization problem in the case of Gaussian risk measures. Our main\ncontribution lies in the theoretical analysis of the optimal portfolio\ndesign, which we present in the context of a mean-variance framework. We\nshow that the optimal portfolio design is unique up to a scaling factor,\nproviding a rigorous foundation for the commonly used \"risk-parity\"\nstrategy, which we also analyze in detail. Our results",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1724137931034483,
          "p": 0.17647058823529413,
          "f": 0.17441859965183898
        },
        "rouge-2": {
          "r": 0.04310344827586207,
          "p": 0.040983606557377046,
          "f": 0.042016801725867414
        },
        "rouge-l": {
          "r": 0.14942528735632185,
          "p": 0.15294117647058825,
          "f": 0.15116278569835062
        }
      }
    },
    {
      "paper_id": "cs.CY.q-fin/EC/2502.12397v1",
      "true_abstract": "Access to digital information is a driver of economic development. But\nalthough 85% of sub-Saharan Africa's population is covered by mobile broadband\nsignal, only 37% use the internet, and those who do seldom use the web. We\ninvestigate whether AI can bridge this gap by analyzing how 469 teachers use an\nAI chatbot in Sierra Leone. The chatbot, accessible via a common messaging app,\nis compared against traditional web search. Teachers use AI more frequently\nthan web search for teaching assistance. Data cost is the most frequently cited\nreason for low internet usage across Africa. The average web search result\nconsumes 3,107 times more data than an AI response, making AI 87% less\nexpensive than web search. Additionally, only 2% of results for corresponding\nweb searches contain content from Sierra Leone. In blinded evaluations, an\nindependent sample of teachers rate AI responses as more relevant, helpful, and\ncorrect than web search results. These findings suggest that AI-driven\nsolutions can cost-effectively bridge information gaps in low-connectivity\nregions.",
      "generated_abstract": "r introduces the concept of a dynamic, multi-agent-based\nsystem to address the challenges of trading in financial markets. The proposed\nmodel is based on a hierarchical structure, comprising three layers: the\ntrader, the trading system, and the market. The trader represents a single\nagent, while the trading system and the market are two-dimensional multi-agent\nsystems. The proposed model incorporates several key features, including a\ndynamic market model, a trading system that adapts to market conditions, and a\ntrader that adjusts its trading strategy to the market's evolution. The\ntrading system and the market model are based on a Dynamic Stochastic\nGrowth Model (DSGM) and a Markov Chain Model (MCM), respectively. The\ntrading system adapts to market conditions through a Dynamic Trading Strategy\n(DTS), while the trad",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07692307692307693,
          "p": 0.12857142857142856,
          "f": 0.09625667980783002
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07692307692307693,
          "p": 0.12857142857142856,
          "f": 0.09625667980783002
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.10491v1",
      "true_abstract": "While music remains a challenging domain for generative models like\nTransformers, recent progress has been made by exploiting suitable\nmusically-informed priors. One technique to leverage information about musical\nstructure in Transformers is inserting such knowledge into the positional\nencoding (PE) module. However, Transformers carry a quadratic cost in sequence\nlength. In this paper, we propose F-StrIPE, a structure-informed PE scheme that\nworks in linear complexity. Using existing kernel approximation techniques\nbased on random features, we show that F-StrIPE is a generalization of\nStochastic Positional Encoding (SPE). We illustrate the empirical merits of\nF-StrIPE using melody harmonization for symbolic music.",
      "generated_abstract": "This paper investigates the real-time speech enhancement using deep\nlearning models. The deep learning models are applied to enhance the speech\nsignals with the help of the transformer network. The deep learning models are\ntrained using the large-scale speech signal datasets and evaluated on the\nacoustic benchmarks. The results show that the proposed models outperform\nconventional models in terms of speech quality and computational efficiency.\nThis paper provides a comprehensive review of the existing research in the\nfield of speech enhancement and deep learning models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12941176470588237,
          "p": 0.22448979591836735,
          "f": 0.16417909983849424
        },
        "rouge-2": {
          "r": 0.01020408163265306,
          "p": 0.013888888888888888,
          "f": 0.011764700999309985
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.20408163265306123,
          "f": 0.14925372670416587
        }
      }
    },
    {
      "paper_id": "eess.IV.q-bio/QM/2503.07104v1",
      "true_abstract": "Whole-brain tractography in diffusion MRI is often followed by a parcellation\nin which each streamline is classified as belonging to a specific white matter\nbundle, or discarded as a false positive. Efficient parcellation is important\nboth in large-scale studies, which have to process huge amounts of data, and in\nthe clinic, where computational resources are often limited. TractCloud is a\nstate-of-the-art approach that aims to maximize accuracy with a local-global\nrepresentation. We demonstrate that the local context does not contribute to\nthe accuracy of that approach, and is even detrimental when dealing with\npathological cases. Based on this observation, we propose PETParc, a new method\nfor Parallel Efficient Tractography Parcellation. PETParc is a\ntransformer-based architecture in which the whole-brain tractogram is randomly\npartitioned into sub-tractograms whose streamlines are classified in parallel,\nwhile serving as global context for each other. This leads to a speedup of up\nto two orders of magnitude relative to TractCloud, and permits inference even\non clinical workstations without a GPU. PETParc accounts for the lack of\nstreamline orientation either via a novel flip-invariant embedding, or by\nsimply using flips as part of data augmentation. Despite the speedup, results\nare often even better than those of prior methods. The code and pretrained\nmodel will be made public upon acceptance.",
      "generated_abstract": "uce a novel framework for the automated and robust segmentation of\nseveral types of cardiac structures, including the left ventricular (LV)\nmyocardium, the atria, and the ventricular septum. The method utilizes a\nhigh-resolution three-dimensional (3D) model of the heart, which enables the\nautomatic segmentation of multiple cardiac structures without the need for\nmanual annotation. The proposed method is based on a multi-scale learning\nframework, enabling the segmentation of structures in different anatomical\nresolutions. It also includes a robust structure-aware guidance mechanism that\ntakes into account the anatomical structure's curvature and the shape of the\nventricular septum. The proposed framework is evaluated in both in vivo and\nex vivo datasets, demonstrating its effectiveness in segmenting multiple cardiac\nstructures in various conditions. The proposed method",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11724137931034483,
          "p": 0.22666666666666666,
          "f": 0.15454545005165304
        },
        "rouge-2": {
          "r": 0.014492753623188406,
          "p": 0.027522935779816515,
          "f": 0.01898733725304546
        },
        "rouge-l": {
          "r": 0.1103448275862069,
          "p": 0.21333333333333335,
          "f": 0.14545454096074392
        }
      }
    },
    {
      "paper_id": "cs.NI.cs/NI/2503.07973v1",
      "true_abstract": "The Satellite-Terrestrial Integrated Network (STIN) enhances end-to-end\ntransmission by simultaneously utilizing terrestrial and satellite networks,\noffering significant benefits in scenarios like emergency response and\ncross-continental communication. Low Earth Orbit (LEO) satellite networks offer\nreduced Round Trip Time (RTT) for long-distance data transmission and serve as\na crucial backup during terrestrial network failures. Meanwhile, terrestrial\nnetworks are characterized by ample bandwidth resources and generally more\nstable link conditions. Therefore, integrating Multipath TCP (MPTCP) into STIN\nis vital for optimizing resource utilization and ensuring efficient data\ntransfer by exploiting the complementary strengths of both networks. However,\nthe inherent challenges of STIN, such as heterogeneity, instability, and\nhandovers, pose difficulties for traditional multipath schedulers, which are\ntypically designed for terrestrial networks. We propose a novel multipath data\nscheduling approach for STIN, Adaptive Latency Compensation Scheduler (ALCS),\nto address these issues. ALCS refines transmission latency estimates by\nincorporating RTT, congestion window size, inflight and queuing packets, and\nsatellite trajectory information. It further employs adaptive mechanisms for\nlatency compensation and proactive handover management. It further employs\nadaptive mechanisms for latency compensation and proactive handover management.\nImplemented in the MPTCP Linux Kernel and evaluated in a simulated STIN\ntestbed, ALCS outperforms existing multipath schedulers, delivering faster data\ntransmission and achieving throughput gains of 9.8% to 44.0% compared to\nbenchmark algorithms.",
      "generated_abstract": "A 2-D map of the city of New York has been digitized and is available as a\nofficial dataset. In this paper, we propose an automated method for identifying\nand extracting the road names from the map, which is used to create a road\nnetwork dataset for the study of the city's transportation system. We apply\nthis dataset to identify the shortest paths between two road names in the city\nof New York. The results of this study are compared with the results of a\ntraditional algorithm and a computer-assisted algorithm. We also show how the\nresults can be used to create a road network model of the city's transportation\nsystem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.265625,
          "f": 0.15668202349083662
        },
        "rouge-2": {
          "r": 0.010050251256281407,
          "p": 0.021505376344086023,
          "f": 0.013698625795882402
        },
        "rouge-l": {
          "r": 0.10457516339869281,
          "p": 0.25,
          "f": 0.14746543362908546
        }
      }
    },
    {
      "paper_id": "math.GT.math/GT/2503.06133v1",
      "true_abstract": "We introduce a new PL invariant, called the balanced genus, for balanced\nnormal $d$-pseudomanifolds. As a key result, we establish that for any\n3-manifold $M$ that is not a sphere, the balanced genus satisfies the lower\nbound $\\mathcal{G}_M \\geq m+3$, where $m$ is the rank of its fundamental group.\nFurthermore, we prove that a 3-manifold $M$ is homeomorphic to the 3-sphere if\nand only if its balanced genus $\\mathcal{G}_M$ is at most 3.\n  For 4-manifolds, we establish a similar characterization: if $M$ is not\nhomeomorphic to a sphere, then its balanced genus is bounded below by\n$\\mathcal{G}_M \\geq 2\\chi(M) + 5m + 11$, where $m$ is the rank of $\\pi_1(M)$.\nAdditionally, we prove that a 4-manifold $M$ is PL-homeomorphic to the 4-sphere\nif and only if its balanced genus satisfies $\\mathcal{G}_M \\leq 2\\chi(M) + 10$.\n  We believe that the balanced genus provides a new perspective in\ncombinatorial topology and will inspire further developments in the field. To\nthis end, we outline several research directions for future exploration.",
      "generated_abstract": "In this paper we prove that the Hopf algebra of Hopf algebras of a\ndifferential graded Lie algebra can be computed as the co-Hopf algebra of a\ndifferential graded Lie algebra. This result was conjectured in \\cite{MR2547626}\nby the author. In this paper we give a direct proof of this result using the\ntheory of the $K$-theory of Hopf algebras and the theory of Hopf algebra\nfunctors.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.2777777777777778,
          "f": 0.15873015464852616
        },
        "rouge-2": {
          "r": 0.022388059701492536,
          "p": 0.058823529411764705,
          "f": 0.032432428438860975
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.2777777777777778,
          "f": 0.15873015464852616
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2501.01454v2",
      "true_abstract": "Infectious diseases remain a critical global health challenge, and the\nintegration of standardized ontologies plays a vital role in managing related\ndata. The Infectious Disease Ontology (IDO) and its extensions, such as the\nCoronavirus Infectious Disease Ontology (CIDO), are essential for organizing\nand disseminating information related to infectious diseases. The COVID-19\npandemic highlighted the need for updating IDO and its virus-specific\nextensions. There is an additional need to update IDO extensions specific to\nbacteria, fungus, and parasite infectious diseases. We adopt the \"hub and\nspoke\" methodology to generate pathogen-specific extensions of IDO: Virus\nInfectious Disease Ontology (VIDO), Bacteria Infectious Disease Ontology\n(BIDO), Mycosis Infectious Disease Ontology (MIDO), and Parasite Infectious\nDisease Ontology (PIDO). The creation of pathogen-specific reference ontologies\nadvances modularization and reusability of infectious disease data within the\nIDO ecosystem. Future work will focus on further refining these ontologies,\ncreating new extensions, and developing application ontologies based on them,\nin line with ongoing efforts to standardize biological and biomedical\nterminologies for improved data sharing and analysis.",
      "generated_abstract": "opment of the next generation of cancer immunotherapies is\nprompting new insights into the biology of cancer immunology. The development\nof cancer immunotherapies relies on the development of immune cell types that\nrespond to cancer and that can also function as antigen presenting cells (APCs).\nThis review focuses on the development of immune cell types that are required\nfor an effective immune response to cancer. We discuss the role of B cells in\nimmune response to cancer and how B cells are regulated during immune response\nto cancer. We discuss the development of T cells during immune response to\ncancer and how T cells are regulated during immune response to cancer. We\ndiscuss the role of dendritic cells in immune response to cancer and how dendritic\ncells are regulated during immune response to cancer. We discuss the role of\nneut",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14150943396226415,
          "p": 0.2830188679245283,
          "f": 0.1886792408385745
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1320754716981132,
          "p": 0.2641509433962264,
          "f": 0.17610062448637326
        }
      }
    },
    {
      "paper_id": "math.LO.math/LO/2503.03551v1",
      "true_abstract": "This is the second of three papers motivated by the author's desire to\nunderstand and explain \"algebraically\" one aspect of Dmitriy Zhuk's proof of\nthe CSP Dichotomy Theorem. In this paper we extend Zhuk's \"bridge\" construction\nto arbitrary meet-irreducible congruences of finite algebras in locally finite\nvarieties with a Taylor term. We then connect bridges to centrality and\nsimilarity. In particular, we prove that Zhuk's bridges and our \"proper\nbridges\" (defined in our first paper) convey the same information in locally\nfinite Taylor varieties.",
      "generated_abstract": "We study the relation between the structure of the Hecke algebra of a\n$p$-adic modular form of level $\\Gamma$ and the cohomology of the pair\n$(X,\\omega)$. In particular, we prove that if $X$ is a modular curve of genus\n$g\\geq 2$, then the Hecke algebra of a weight $k$ modular form of level\n$\\Gamma$ is isomorphic to the algebra of $k$-th powers of the Hecke operator\n$T$ acting on cohomology of $(X,\\omega)$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21311475409836064,
          "p": 0.30952380952380953,
          "f": 0.25242717963615807
        },
        "rouge-2": {
          "r": 0.06172839506172839,
          "p": 0.08620689655172414,
          "f": 0.07194244118006347
        },
        "rouge-l": {
          "r": 0.21311475409836064,
          "p": 0.30952380952380953,
          "f": 0.25242717963615807
        }
      }
    },
    {
      "paper_id": "cs.MM.cs/MA/2503.09448v1",
      "true_abstract": "Proactive virtual reality (VR) streaming requires users to upload\nviewpoint-related information, raising significant privacy concerns. Existing\nstrategies preserve privacy by introducing errors to viewpoints, which,\nhowever, compromises the quality of experience (QoE) of users. In this paper,\nwe first delve into the analysis of the viewpoint leakage probability achieved\nby existing privacy-preserving approaches. We determine the optimal\ndistribution of viewpoint errors that minimizes the viewpoint leakage\nprobability. Our analyses show that existing approaches cannot fully eliminate\nviewpoint leakage. Then, we propose a novel privacy-preserving approach that\nintroduces noise to uploaded viewpoint prediction errors, which can ensure zero\nviewpoint leakage probability. Given the proposed approach, the tradeoff\nbetween privacy preservation and QoE is optimized to minimize the QoE loss\nwhile satisfying the privacy requirement. Simulation results validate our\nanalysis results and demonstrate that the proposed approach offers a promising\nsolution for balancing privacy and QoE.",
      "generated_abstract": "aper, we introduce a novel algorithm to solve the problem of\nestimating the maximum likelihood estimator (MLE) of a target function using\nthe Laplace method. The proposed algorithm is based on the estimation of the\nMLE of a target function using the Laplace method, followed by an iterative\nupdate of the Laplace estimator, which is used to estimate the MLE of the\ntarget function. The proposed algorithm is applicable to any target function,\nand can be used to obtain a better approximation of the MLE of the target\nfunction compared to the use of the Laplace method. The proposed algorithm is\nanalytically studied and numerically validated, and is shown to converge to\nthe MLE of the target function. The proposed algorithm is also shown to be\nrobust to changes in the target function. The proposed algorithm is implemented\nin a Python package and is available at\nhttps://github.com/j",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.18181818181818182,
          "f": 0.14814814331961607
        },
        "rouge-2": {
          "r": 0.014814814814814815,
          "p": 0.02,
          "f": 0.017021271706656
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.18181818181818182,
          "f": 0.14814814331961607
        }
      }
    },
    {
      "paper_id": "cs.GR.eess/IV/2503.02218v1",
      "true_abstract": "Purpose: This study proposes a novel anatomically-driven dynamic modeling\nframework for coronary arteries using skeletal skinning weights computation,\naiming to achieve precise control over vessel deformation while maintaining\nreal-time performance for surgical simulation applications. Methods: We\ndeveloped a computational framework based on biharmonic energy minimization for\nskinning weight calculation, incorporating volumetric discretization through\ntetrahedral mesh generation. The method implements temporal sampling and\ninterpolation for continuous vessel deformation throughout the cardiac cycle,\nwith mechanical constraints and volume conservation enforcement. The framework\nwas validated using clinical datasets from 5 patients, comparing interpolated\ndeformation results against ground truth data obtained from frame-by-frame\nsegmentation across cardiac phases. Results: The proposed framework effectively\nhandled interactive vessel manipulation. Geometric accuracy evaluation showed\nmean Hausdorff distance of 4.96 +- 1.78 mm and mean surface distance of 1.78 +-\n0.75 mm between interpolated meshes and ground truth models. The Branch\nCompleteness Ratio achieved 1.82 +- 0.46, while Branch Continuity Score\nmaintained 0.84 +- 0.06 (scale 0-1) across all datasets. The system\ndemonstrated capability in supporting real-time guidewire-vessel collision\ndetection and contrast medium flow simulation throughout the complete coronary\ntree structure. Conclusion: Our skinning weight-based methodology enhances\nmodel interactivity and applicability while maintaining geometric accuracy. The\nframework provides a more flexible technical foundation for virtual surgical\ntraining systems, demonstrating promising potential for both clinical practice\nand medical education applications. The code is available at\nhttps://github.com/ipoirot/DynamicArtery.",
      "generated_abstract": "vances in vision-language model (VLM) have demonstrated their\ncapabilities in natural language processing. However, traditional VLMs\noversubscribe the resources and overutilize the model's capacity, leading to\nhigh-latency responses, memory leaks, and resource exhaustion. To address these\nchallenges, we propose an end-to-end VLM called VLM-Lite. VLM-Lite uses a\ncombination of fine-tuning, dynamic resource allocation, and efficient\noptimization techniques to reduce resource utilization and improve latency. Our\nexperiments show that VLM-Lite achieves comparable performance to state-of-the-art\nVLMs while reducing latency by 20x to 50x on the same task. In addition,\nVLM-Lite significantly reduces memory leaks and resource exhaustion while\nachieving comparable",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0755813953488372,
          "p": 0.16883116883116883,
          "f": 0.10441766641054194
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.06976744186046512,
          "p": 0.15584415584415584,
          "f": 0.09638553789648573
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.02850v1",
      "true_abstract": "The comparison of different medical treatments from observational studies or\nacross different clinical studies is often biased by confounding factors such\nas systematic differences in patient demographics or in the inclusion criteria\nfor the trials. Propensity score matching is a popular method to adjust for\nsuch confounding. It compares weighted averages of patient responses. The\nweights are calculated from logistic regression models with the intention to\nreduce differences between the confounders in the treatment groups. However,\nthe groups are only \"roughly matched\" with no generally accepted principle to\ndetermine when a match is \"good enough\".\n  In this manuscript, we propose an alternative approach to the matching\nproblem by considering it as a constrained optimization problem. We investigate\nthe conditions for exact matching in the sense that the average values of\nconfounders are identical in the treatment groups after matching. Our approach\nis similar to the matching-adjusted indirect comparison approach by\nSignorovitch et al. (2010) but with two major differences: First, we do not\nimpose any specific functional form on the matching weights; second, the\nproposed approach can be applied to individual patient data from several\ntreatment groups as well as to a mix of individual patient and aggregated data.",
      "generated_abstract": "We study the statistical interpretation of a recently proposed class of\nstatistical models, called {\\em network models}, which generalize Bayesian\nnetworks and represent the conditional independence structure of a set of random\nvariables. We derive the maximum likelihood estimator of the model parameters,\nshow that it is a consistent estimator, and establish its asymptotic normality\nunder a mild regularity condition on the graph structure. We also derive the\nasymptotic normality of the maximum likelihood estimator under a mild\nregularity condition on the probability distribution of the random variables,\nand establish the consistency of the estimator under the same regularity\ncondition.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07936507936507936,
          "p": 0.18181818181818182,
          "f": 0.11049723333842085
        },
        "rouge-2": {
          "r": 0.010582010582010581,
          "p": 0.024691358024691357,
          "f": 0.014814810614816005
        },
        "rouge-l": {
          "r": 0.06349206349206349,
          "p": 0.14545454545454545,
          "f": 0.08839778582460872
        }
      }
    },
    {
      "paper_id": "econ.GN.stat/AP/2502.18253v1",
      "true_abstract": "Participants in online experiments often enroll over time, which can\ncompromise sample representativeness due to temporal shifts in covariates. This\nissue is particularly critical in A/B tests, online controlled experiments\nextensively used to evaluate product updates, since these tests are\ncost-sensitive and typically short in duration. We propose a novel framework\nthat dynamically assesses sample representativeness by dividing the ongoing\nsampling process into three stages. We then develop stage-specific estimators\nfor Population Average Treatment Effects (PATE), ensuring that experimental\nresults remain generalizable across varying experiment durations. Leveraging\nsurvival analysis, we develop a heuristic function that identifies these stages\nwithout requiring prior knowledge of population or sample characteristics,\nthereby keeping implementation costs low. Our approach bridges the gap between\nexperimental findings and real-world applicability, enabling product decisions\nto be based on evidence that accurately represents the broader target\npopulation. We validate the effectiveness of our framework on three levels: (1)\nthrough a real-world online experiment conducted on WeChat; (2) via a synthetic\nexperiment; and (3) by applying it to 600 A/B tests on WeChat in a\nplatform-wide application. Additionally, we provide practical guidelines for\npractitioners to implement our method in real-world settings.",
      "generated_abstract": "This paper provides a novel and general approach to constructing a\nequilibrium solution in the dynamic games, where the players are agents who\nare randomly assigned to play a game and there are multiple equilibria. We\nshow that a unique equilibrium in such a game can be identified by the\nsymmetric positive definite matrix whose eigenvectors are the equilibria. We\nthen apply the proposed approach to construct a Nash equilibrium in the game\nof \"Mostly Harmless Elections\" and identify the equilibrium as a\n$K$-means-type algorithm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11347517730496454,
          "p": 0.2857142857142857,
          "f": 0.16243654415419112
        },
        "rouge-2": {
          "r": 0.010582010582010581,
          "p": 0.02531645569620253,
          "f": 0.01492536897666634
        },
        "rouge-l": {
          "r": 0.10638297872340426,
          "p": 0.26785714285714285,
          "f": 0.15228425989023178
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.cond-mat/str-el/2503.09692v1",
      "true_abstract": "We present a model for interacting electrons in a continuum band structure\nthat resembles a ``trashcan'', with a flat bottom of radius $k_b$ beyond which\nthe dispersion increases rapidly with velocity $v$. The form factors of the\nBloch wavefunctions can be well-approximated by the Girvin-MacDonald-Platzman\nalgebra, which encodes the uniform Berry curvature. We demonstrate how this\nmodel captures the salient features of the low-energy Hamiltonian for\nelectron-doped pristine $n$-layer rhombohedral graphene (R$n$G) for appropriate\nvalues of the displacement field, and provide corresponding expressions for\n$k_b$. In the regime where the Fermi wavevector is close to $k_b$, we\nanalytically solve the Hartree-Fock equations for a gapped Wigner crystal in\nseveral limits of the model. We introduce a new method, the sliver-patch\napproximation, which extends the previous few-patch approaches and is crucial\nin both differentiating even vs odd Chern numbers of the ground state and\ngapping the Hartree-Fock solution. A key parameter is the Berry flux\n$\\varphi_{\\text{BZ}}$ enclosed by the (flat) bottom of the band. We\nanalytically show that there is a ferromagnetic coupling between the signs of\n$\\varphi_{\\text{BZ}}$ and the Chern number $C$ of the putative Wigner crystal.\nWe also study the competition between the $C=0$ and $1$ solutions as a function\nof the interaction potential for parameters relevant to R$n$G. By exhaustive\ncomparison to numerical Hartree-Fock calculations, we demonstrate how the\nanalytic results capture qualitative trends of the phase diagram, as well as\nquantitative details such as the enhancement of the effective velocity. Our\nanalysis paves the way for an analytic and numerical examination of the\nstability and competition beyond mean-field theory of the Wigner crystals in\nthis model.",
      "generated_abstract": "t a theoretical model for the dynamics of non-equilibrium\nstates of strongly correlated electrons in a quantum dot, with a non-trivial\ninteraction between electrons. We formulate the model as a non-equilibrium\nnonequilibrium Green's function (NEGF) theory, and demonstrate its utility for\ndynamics of non-equilibrium states in the strongly correlated regime,\nparticularly in the presence of a strong onsite Coulomb interaction. Our\nresults demonstrate that the dynamics of non-equilibrium states can be\ndescribed by a single-particle Green's function, which can be written as a\nsum over occupation numbers and the Green's functions of two-level systems.\nThe Green's functions of two-level systems, in turn, can be expressed as the\nexpectation value of the NEGF Green's function. The derivation of the NEG",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.36923076923076925,
          "f": 0.21145374040637316
        },
        "rouge-2": {
          "r": 0.04,
          "p": 0.09900990099009901,
          "f": 0.05698005288106457
        },
        "rouge-l": {
          "r": 0.13580246913580246,
          "p": 0.3384615384615385,
          "f": 0.19383259503192388
        }
      }
    },
    {
      "paper_id": "physics.chem-ph.physics/chem-ph/2503.08435v1",
      "true_abstract": "We show how small molecules in an intense and cold molecular beam can be\nhighly nuclear-spin polarized via microwave or infrared rotational excitation\nschemes, followed by hyperfine-induced quantum beats. Repumping schemes can be\nused to achieve polarization above $90\\%$ in cases where single-pumping schemes\nare insufficient. Projected production rates in excess of $10^{21}$\n${\\text{s}^{-1}}$ allow applications including nuclear-magnetic-resonance\nsignal enhancement, and spin-polarized nuclear fusion, where polarized nuclei\nare known to enhance D-T and D-$^3$He fusion cross sections by $50\\%$.",
      "generated_abstract": "al studies of the quantum mechanical and molecular dynamics of\nchemical reactions have traditionally relied on the use of ab initio\nquantum-chemical methods. While these methods are highly accurate, they\ntypically require extremely large amounts of computational time, making them\nunsuitable for practical applications. To address this, we have developed a\nmolecular dynamics simulation method that can be applied to realistic\nmolecular systems. Our method utilizes the L\\\"owdin orbitals to calculate the\natomic orbitals of the system, and the corresponding electronic energies. We\nhave tested this method on a variety of molecules, including CO, H2, H2O, H2O2,\nH2O3, and H2O4. By using this method, we have been able to achieve computational\ntimes that are orders of magnitude faster than traditional methods. Our results\ndemonstrate the potential of this approach",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19402985074626866,
          "p": 0.14130434782608695,
          "f": 0.1635220077022271
        },
        "rouge-2": {
          "r": 0.025974025974025976,
          "p": 0.016,
          "f": 0.019801975480346186
        },
        "rouge-l": {
          "r": 0.1791044776119403,
          "p": 0.13043478260869565,
          "f": 0.15094339135002588
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.02285v1",
      "true_abstract": "Monitoring a process/phenomenon of specific interest is prevalent in\nCyber-Physical Systems (CPS), remote healthcare, smart buildings, intelligent\ntransport, industry 4.0, etc. A key building block of the monitoring system is\na sensor sampling the process and communicating the status updates to a monitor\nfor detecting events of interest. Measuring the freshness of the status updates\nis essential for the timely detection of events, and it has received\nsignificant research interest in recent times. In this paper, we propose a new\nfreshness metric, Age of Detection (AoD), for monitoring the state transitions\nof a Discrete Time Markov Chain (DTMC) source over a lossy wireless channel. We\nconsider the pull model where the sensor samples DTMC state whenever the\nmonitor requests a status update. We formulate a Constrained Markov Decision\nProblem (CMDP) for optimising the AoD subject to a constraint on the average\nsampling frequency and solve it using the Lagrangian MDP formulation and\nRelative Value Iteration (RVI) algorithm. Our numerical results show\ninteresting trade-offs between AoD, sampling frequency, and transmission\nsuccess probability. Further, the AoD minimizing policy provides a lower\nestimation error than the Age of Information (AoI) minimizing policy, thus\ndemonstrating the utility of AoD for monitoring DTMC sources.",
      "generated_abstract": "r proposes a novel multi-layer, multi-task, multi-channel\napproach to compress the frequency-domain information of a wireless channel\nusing the block-coherent (BC) scheme. The proposed method uses a low-rank\napproach to compress the information of the frequency-domain block, which\nenables the efficient encoding of the block-coherent (BC) scheme. This method\ncombines the block-coherent (BC) scheme with a low-rank approximation, which\nallows for efficient compression of the frequency-domain block. This method\nenables efficient encoding of the frequency-domain block, which allows for\nefficient compression of the frequency-domain block. This method provides\nefficient encoding of the frequency-domain block, which allows for efficient\ncompression of the frequency-domain block. This method provides efficient\nencoding of the frequency-domain block, which allows for efficient compression\nof the frequency-domain block",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07407407407407407,
          "p": 0.2564102564102564,
          "f": 0.1149425252576299
        },
        "rouge-2": {
          "r": 0.020833333333333332,
          "p": 0.07142857142857142,
          "f": 0.03225806101977145
        },
        "rouge-l": {
          "r": 0.06666666666666667,
          "p": 0.23076923076923078,
          "f": 0.10344827238406672
        }
      }
    },
    {
      "paper_id": "math.OC.stat/AP/2502.18332v1",
      "true_abstract": "National teams from different continents can play against each other only in\nafew sports competitions. Therefore, a reasonable aim is maximising the number\nof intercontinental games in world cups, as done in basketball and football, in\ncontrast to handball and volleyball. However, this objective requires\nadditional draw constraints that imply the violation of equal treatment. In\naddition, the standard draw mechanism is non-uniformly distributed on the set\nof valid assignments, which may lead to further distortions. Our paper analyses\nthis novel trade-off between attractiveness and fairness through the example of\nthe 2025 World Men's Handball Championship. We introduce a measure of\ninequality, which enables considering 32 sets of reasonable geographical\nrestrictions to determine the Pareto frontier. The proposed methodology can be\nused by policy-makers to select the optimal set of draw constraints.",
      "generated_abstract": "We study the robustness of estimators of the mean and covariance of a\nfunction of several variables. We show that if the estimators are consistent\nwith respect to the supremum norm, they are also consistent with respect to a\nconcave weighted sum of the supremum norm, and the supremum norm is the\nconsistent estimator of the mean. In addition, we prove that if the\nestimators are consistent with respect to the supremum norm, they are also\nconsistent with respect to the average of the supremum norm, and the supremum\nnorm is the consistent estimator of the covariance. These results have a\nsignificant impact on applications of the regression method and the\nregression-based method in the robust estimation of the mean and covariance of\nfunctions of several variables.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1188118811881188,
          "p": 0.24,
          "f": 0.15894039292136322
        },
        "rouge-2": {
          "r": 0.015503875968992248,
          "p": 0.02631578947368421,
          "f": 0.019512190456158162
        },
        "rouge-l": {
          "r": 0.1188118811881188,
          "p": 0.24,
          "f": 0.15894039292136322
        }
      }
    },
    {
      "paper_id": "nlin.PS.nlin/PS/2503.06289v1",
      "true_abstract": "We study a nonlinear magnetic metamaterial modeled as a split-ring resonator\narray, where the standard discrete laplacian is replaced by its fractional\nform. We find a closed-form expression for the dispersion relation as a\nfunction of the fractional exponent s and the gain/loss parameter {\\gamma} and\nexamine the conditions under which stable magneto-inductive waves exist. The\ndensity of states is computed in closed form and suggests that the main effect\nof fractionality is the flattening of the bands, while gain/loss increase tends\nto reduce the bandgaps. The spatial extent of the modes for a finite array is\ncomputed by means of the participation ratio R, which is also obtained in\nclosed form. For a fixed fractionality exponent, an increase in gain/loss\n{\\gamma} decreases the overall R, from the number of sites N towards N/2 at\nlarge {\\gamma}. The nonlinear dynamics of the average magnetic energy on an\ninitial ring during a cycle shows a monotonic increase with {\\gamma}, and it is\nqualitatively similar for all fractional exponents. This is explained as mainly\ndue to the interplay of nonlinearity and PT symmetry.",
      "generated_abstract": "e the concept of a ``global attractor'' in a network of coupled\ndifferential equations. This concept was introduced in the context of\nsystems of ordinary differential equations to study the stability of local\nattractors. We generalise this idea to systems of coupled differential\nequations. We show that for systems of coupled ordinary differential equations,\nthe global attractor is the unique local attractor with the property that it\ncontains all local attractors. This generalisation allows us to extend the\nconcept of global attractor to systems of coupled differential equations,\nproviding a new tool for studying the dynamics of coupled systems. In particular,\nwe show that the global attractor of a system of coupled ordinary differential\nequations can be a global attractor of the original system of ordinary\ndifferential equations. This result is illustrated with an example of a\ncoupled ordinary differential equation system and the associated system of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1504424778761062,
          "p": 0.2698412698412698,
          "f": 0.19318181358535652
        },
        "rouge-2": {
          "r": 0.029069767441860465,
          "p": 0.045871559633027525,
          "f": 0.03558718386342689
        },
        "rouge-l": {
          "r": 0.1415929203539823,
          "p": 0.25396825396825395,
          "f": 0.18181817722172017
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.06776v1",
      "true_abstract": "We address safe multi-robot interaction under uncertainty. In particular, we\nformulate a chance-constrained linear quadratic Gaussian game with coupling\nconstraints and system uncertainties. We find a tractable reformulation of the\ngame and propose a dual ascent algorithm. We prove that the algorithm converges\nto a generalized Nash equilibrium of the reformulated game, ensuring the\nsatisfaction of the chance constraints. We test our method in driving\nsimulations and real-world robot experiments. Our method ensures safety under\nuncertainty and generates less conservative trajectories than single-agent\nmodel predictive control.",
      "generated_abstract": "In robotics, it is crucial to optimize the motion planning process by\napplying the notion of cost-sensitivity. However, this problem is difficult to\nsolve due to the large number of constraints and the lack of a proper\nrepresentation of the cost function. To tackle this challenge, we propose a\nnovel cost-sensitivity representation and a cost-sensitivity optimization\nframework based on the concept of partial cost sensitivity. First, we introduce\na new cost-sensitivity representation, named partial cost sensitivity, to\nquantify the sensitivity of the cost function to a set of variables. Then, we\nformulate the partial cost sensitivity optimization problem as a mixed-integer\nprogramming (MIP) problem, which is solved by a branch-and-bound algorithm.\nFinally, we demonstrate the effectiveness of the proposed approach in two\nsimulated scenarios: motion planning with and without safety constraints.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2153846153846154,
          "p": 0.18181818181818182,
          "f": 0.19718309362725664
        },
        "rouge-2": {
          "r": 0.06097560975609756,
          "p": 0.040983606557377046,
          "f": 0.049019603035371485
        },
        "rouge-l": {
          "r": 0.18461538461538463,
          "p": 0.15584415584415584,
          "f": 0.1690140795427496
        }
      }
    },
    {
      "paper_id": "math.AP.math/CA/2503.05140v1",
      "true_abstract": "In this paper, we investigate the mixed norm estimates for the operator $ T\n$associated with a dilated plane curve $(ut, u\\gamma(t))$, defined by \\[ Tf(x,\nu) := \\int_{0}^{1} f(x_1 - ut, x_2 - u\\gamma(t)) \\, dt, \\] where $ x := (x_1,\nx_2) $ and $\\gamma $ is a general plane curve satisfying appropriate smoothness\nand curvature conditions. Our results partially address a problem posed by\nHickman [J. Funct. Anal. 2016] in the two-dimensional setting. More precisely,\nwe establish the $ L_x^p(\\mathbb{R}^2) \\rightarrow L_x^q L_u^r(\\mathbb{R}^2\n\\times [1, 2]) $ (space-time) estimates for $ T $, whenever\n$(\\frac{1}{p},\\frac{1}{q})$ satisfy \\[ \\max\\left\\{0, \\frac{1}{2p} -\n\\frac{1}{2r}, \\frac{3}{p} - \\frac{r+2}{r}\\right\\} < \\frac{1}{q} \\leq\n\\frac{1}{p} < \\frac{r+1}{2r} \\] and $$1 + (1 + \\omega)\\left(\\frac{1}{q} -\n\\frac{1}{p}\\right) > 0,$$ where $ r \\in [1, \\infty] $ and $ \\omega :=\n\\limsup_{t \\rightarrow 0^+} \\frac{\\ln|\\gamma(t)|}{\\ln t} $. These results are\nsharp, except for certain borderline cases. Additionally, we examine the $\nL_x^p(\\mathbb{R}^2) \\rightarrow L_u^r L_x^q(\\mathbb{R}^2 \\times [1, 2]) $\n(time-space) estimates for $T $, which are especially almost sharp when $p=2$.",
      "generated_abstract": "aper, we investigate the stability of the local-in-space\ngeneralized normal form (GNF) of the vector field $F$ on $M$ defined by\n$F(x,y,z)=(y,z)$. We prove that the GNF is locally asymptotically stable in a\nneighborhood of $0\\in M$ if and only if $F$ is locally asymptotically stable\nin a neighborhood of $0\\in M$. Moreover, we show that if the GNF is\nasymptotically stable, then the vector field $F$ is asymptotically stable. We\nalso investigate the stability of the GNF of the vector field $F$ on\n$M^3$ defined by $F(x,y,z)=(y,z,x^2)$. We prove that if $F$ is\nasymptotically stable, then the vector field $F$",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07377049180327869,
          "p": 0.21428571428571427,
          "f": 0.10975609375074374
        },
        "rouge-2": {
          "r": 0.018518518518518517,
          "p": 0.05084745762711865,
          "f": 0.027149317353044114
        },
        "rouge-l": {
          "r": 0.06557377049180328,
          "p": 0.19047619047619047,
          "f": 0.09756097179952423
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2402.14161v1",
      "true_abstract": "We derive the short-maturity asymptotics for option prices in the local\nvolatility model in a new short-maturity limit $T\\to 0$ at fixed $\\rho = (r-q)\nT$, where $r$ is the interest rate and $q$ is the dividend yield. In cases of\npractical relevance $\\rho$ is small, however our result holds for any fixed\n$\\rho$. The result is a generalization of the Berestycki-Busca-Florent formula\nfor the short-maturity asymptotics of the implied volatility which includes\ninterest rates and dividend yield effects of $O(((r-q) T)^n)$ to all orders in\n$n$. We obtain analytical results for the ATM volatility and skew in this\nasymptotic limit. Explicit results are derived for the CEV model. The\nasymptotic result is tested numerically against exact evaluation in the\nsquare-root model model $\\sigma(S)=\\sigma/\\sqrt{S}$, which demonstrates that\nthe new asymptotic result is in very good agreement with exact evaluation in a\nwide range of model parameters relevant for practical applications.",
      "generated_abstract": "of this study is to examine the role of the US Federal Reserve\nin shaping the stock market. The authors employed a novel approach to evaluate\nthe effects of monetary policy on the stock market by analyzing the price\nactions of the S&P 500 index between 1926 and 2022. Their findings indicate\nthat the Fed has been a significant factor in driving market volatility and\ninfluencing stock price movements, with a particular emphasis on the 1970s,\n1980s, and 1990s. The study also reveals that the Fed's actions have been\nconsistent with its stated objectives, with the central bank primarily aiming\nto maintain a stable and predictable economic environment. However, in recent\nyears, the Fed has increasingly focused on managing inflation, with\nmonetary policy becoming increasingly accommod",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14772727272727273,
          "p": 0.15853658536585366,
          "f": 0.1529411714768168
        },
        "rouge-2": {
          "r": 0.02962962962962963,
          "p": 0.03418803418803419,
          "f": 0.03174602677154273
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.13414634146341464,
          "f": 0.1294117597121109
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.06673v1",
      "true_abstract": "We initiate systematic study of EZ-structures (and associated boundaries) of\ngroups acting on spaces that admit consistent and conical (equivalently,\nconsistent and convex) geodesic bicombings. Such spaces recently drew a lot of\nattention due to the fact that many classical groups act `nicely' on them. We\nrigorously construct EZ-structures, discuss their uniqueness (up to\nhomeomorphism), provide examples, and prove some boundary-related features\nanalogous to the ones exhibited by CAT(0) spaces and groups, which form a\nsubclass of the discussed class of spaces and groups.",
      "generated_abstract": "uct a class of metric spaces which we call \"metrizable\nmetricspaces\" (MMs). These spaces are metric spaces which are reflexive\n(i.e. every point has a neighborhood which is compact) and have the property\nthat any point can be approximated by finitely many points which are also in the\nneighborhood. We also prove that any MM has the structure of a locally compact\ngroup. We also prove that any MM is metrizable. We show that any MM is\n$T_3$. We show that any MM with the property that any point can be approximated\nby finitely many points which are also in the neighborhood is $T_3$. We show\nthat any MM which is $T_3$ is Hausdorff. We show that any MM which is\n$T_3$ is $\\sigma$-compact. We show that any MM which is $T_3$ is closed",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1935483870967742,
          "p": 0.23076923076923078,
          "f": 0.21052631082794718
        },
        "rouge-2": {
          "r": 0.0125,
          "p": 0.012987012987012988,
          "f": 0.012738848505012307
        },
        "rouge-l": {
          "r": 0.1935483870967742,
          "p": 0.23076923076923078,
          "f": 0.21052631082794718
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.06528v1",
      "true_abstract": "This paper integrates the damped harmonic oscillator into DSGE models to\nbetter capture delayed economic adjustments. By introducing a damping\ncoefficient, I model economic recoveries as under-damped, critically damped, or\nover-damped processes. Numerical simulations illustrate how different damping\nlevels affect recovery speed and stability. This approach enhances DSGE models'\nrealism, offering insights into historical economic crises and improving\nmacroeconomic forecasting.",
      "generated_abstract": "We examine the impact of inflation on consumer confidence and the\ninfluence of consumer sentiment on inflation expectations in the context of\nthe Turkish economy. We employ the inflation expectations index (IEI) and the\ninflation expectations index (IEI-2) as proxies for consumer confidence and\ninflation expectations, respectively. We find that the IEI and IEI-2 are\npositively and significantly associated with consumer confidence. Furthermore,\nwe show that the relationship between consumer confidence and IEI-2 is\nstronger than the relationship between consumer confidence and IEI. Our\nfindings suggest that consumer confidence has a significant and positive\ninfluence on inflation expectations, which is consistent with the theory of\nmonetary transmission. Our results suggest that a change in consumer confidence\ncan lead to a change in inflation expectations, which can lead to a change in\nthe inflation rate.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09433962264150944,
          "p": 0.08196721311475409,
          "f": 0.08771929327023728
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09433962264150944,
          "p": 0.08196721311475409,
          "f": 0.08771929327023728
        }
      }
    },
    {
      "paper_id": "math.DS.math/CA/2503.07508v1",
      "true_abstract": "This paper relates to the Fourier decay properties of images of self-similar\nmeasures $\\mu$ on $\\mathbb{R}^k$ under nonlinear smooth maps $f \\colon\n\\mathbb{R}^k \\to \\mathbb{R}$. For example, we prove that if the linear parts of\nthe similarities defining $\\mu$ commute and the graph of $f$ has nonvanishing\nGaussian curvature, then the Fourier dimension of the image measure is at least\n$\\max\\left\\{ \\frac{2(2\\kappa_2 - k)}{4 + 2\\kappa_* - k} , 0 \\right\\}$, where\n$\\kappa_2$ is the lower correlation dimension of $\\mu$ and $\\kappa_*$ is the\nAssouad dimension of the support of $\\mu$. Under some additional assumptions on\n$\\mu$, we use recent breakthroughs in the fractal uncertainty principle to\nobtain further improvements for the decay exponents.\n  We give several applications to nonlinear arithmetic of self-similar sets $F$\nin the line. For example, we prove that if $\\dim_{\\mathrm H} F > (\\sqrt{65} -\n5)/4 = 0.765\\dots$ then the arithmetic product set $F \\cdot F = \\{ xy : x,y \\in\nF \\}$ has positive Lebesgue measure, while if $\\dim_{\\mathrm H} F > (-3 +\n\\sqrt{41})/4 = 0.850\\dots$ then $F \\cdot F \\cdot F$ has non-empty interior. One\nfeature of the above results is that they do not require any separation\nconditions on the self-similar sets.",
      "generated_abstract": "In this paper we study the following problem. Let $K$ be a compact Riemannian\nmanifold. Given two closed, connected, and $C^2$-smooth $2$-dimensional\nsubmanifolds $M_1$ and $M_2$ of $K$ that intersect transversely, we ask what is\nthe maximal dimension $d$ such that the intersection $M_1\\cap M_2$ can be\ncovered by a finite number of $C^d$-smooth $2$-dimensional submanifolds of\n$K$. We answer this question for $d\\leq 2$ and obtain a sharp bound for $d=2$.\nWe also prove that for $d\\geq 3$ the bound is sharp.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09230769230769231,
          "p": 0.2033898305084746,
          "f": 0.1269841226897345
        },
        "rouge-2": {
          "r": 0.0111731843575419,
          "p": 0.025,
          "f": 0.01544401117455135
        },
        "rouge-l": {
          "r": 0.09230769230769231,
          "p": 0.2033898305084746,
          "f": 0.1269841226897345
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.15035v1",
      "true_abstract": "Hair follicles constantly cycle through phases of growth, regression and\nrest, as matrix keratinocytes (MKs), the cells producing hair fibers,\nproliferate, and then undergo spontaneous apoptosis. Damage to MKs and\nperturbations in their normal dynamics result in a shortened growth phase,\nleading to hair loss. Two common factors causing such disruption are hormonal\nimbalance and attacks by the immune system. Androgenetic alopecia (AGA) is hair\nloss caused by high sensitivity to androgens, and alopecia areata (AA) is hair\nloss caused by an autoimmune reaction against MKs. In this study, we inform a\nmathematical model for the human hair cycle with experimental data for the\nlengths of hair cycle phases available from male control subjects and subjects\nwith AGA. We also, connect a mathematical model for AA with estimates for the\nduration of hair cycle phases obtained from the literature. Subsequently, with\neach model we perform parameter screening, uncertainty quantification and\nglobal sensitivity analysis and compare the results within and between the\ncontrol and AGA subject groups as well as among AA, control and AGA conditions.\nThe findings reveal that in AGA subjects there is greater uncertainty\nassociated with the duration of hair growth than in control subjects and that,\ncompared to control and AGA conditions, in AA it is more certain that longer\nhair growth phase could not be expected. The comparison of results also\nindicates that in AA lower proliferation of MKs and weaker communication of the\ndermal papilla with MKs via signaling molecules could be expected than in\nnormal and AGA conditions, and in AA stronger inhibition of MK proliferation by\nregulatory molecules could be expected than in AGA. Finally, the global\nsensitivity analysis highlights the process of MK apoptosis as highly impactful\nfor the length of hair growth only in the AA case, but not for control and AGA\nconditions.",
      "generated_abstract": "spike trains are highly correlated, yet their causal dynamics are\nnot fully understood. In this study, we investigate the spike train dynamics of\na population of neurons in the visual cortex of the mammalian retina using\nintracellular recordings from a single neuron and multisite recordings from a\nfew hundred neurons. We show that the spike train dynamics are significantly\nindependent of the synaptic strength of the connections between neurons, and\nthat the mean firing rate of the neuron is a good indicator of the\ncausal dynamics of the spike trains. We propose that the spike train dynamics\nare generated by a spike train noise model, where the mean firing rate of the\nneuron is the spike train noise model, and that the noise is generated by the\nspike train noise model itself. Our results suggest that the spike train\ndynamics",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13924050632911392,
          "p": 0.3384615384615385,
          "f": 0.19730941290997214
        },
        "rouge-2": {
          "r": 0.023529411764705882,
          "p": 0.0625,
          "f": 0.03418803021404093
        },
        "rouge-l": {
          "r": 0.12658227848101267,
          "p": 0.3076923076923077,
          "f": 0.17937219317903044
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/RO/2503.10554v1",
      "true_abstract": "The evolution from motion capture and teleoperation to robot skill learning\nhas emerged as a hotspot and critical pathway for advancing embodied\nintelligence. However, existing systems still face a persistent gap in\nsimultaneously achieving four objectives: accurate tracking of full upper limb\nmovements over extended durations (Accuracy), ergonomic adaptation to human\nbiomechanics (Comfort), versatile data collection (e.g., force data) and\ncompatibility with humanoid robots (Versatility), and lightweight design for\noutdoor daily use (Convenience). We present a wearable exoskeleton system,\nincorporating user-friendly immersive teleoperation and multi-modal sensing\ncollection to bridge this gap. Due to the features of a novel shoulder\nmechanism with synchronized linkage and timing belt transmission, this system\ncan adapt well to compound shoulder movements and replicate 100% coverage of\nnatural upper limb motion ranges. Weighing 5.2 kg, NuExo supports backpack-type\nuse and can be conveniently applied in daily outdoor scenarios. Furthermore, we\ndevelop a unified intuitive teleoperation framework and a comprehensive data\ncollection system integrating multi-modal sensing for various humanoid robots.\nExperiments across distinct humanoid platforms and different users validate our\nexoskeleton's superiority in motion range and flexibility, while confirming its\nstability in data collection and teleoperation accuracy in dynamic scenarios.",
      "generated_abstract": "aper, we introduce a novel framework for the design and control of\nstructural systems using geometric reinforcement learning (GRL). The framework\nconsists of a two-stage process, which first generates an initial structure\nconfiguration, and then iteratively optimizes it to achieve the desired\nperformance. In the first stage, we introduce a novel reinforcement learning\nalgorithm for generating initial structures from a given input distribution.\nThis algorithm uses a GRL-based objective function, a reparameterized\nvariational inference framework, and an autoencoder to generate initial\nstructures. In the second stage, we design a reinforcement learning controller\nthat optimizes the generated structures to achieve a desired performance\nmetric. This controller leverages a target-conditioned GRL objective function,\nan autoencoder for structure reconstruction, and a GRL-based objective\nfunction to generate control inputs. We illustrate the effectiveness of the\nframework by",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10714285714285714,
          "p": 0.2054794520547945,
          "f": 0.14084506591725643
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.02564102564102564,
          "f": 0.019607838414072647
        },
        "rouge-l": {
          "r": 0.09285714285714286,
          "p": 0.1780821917808219,
          "f": 0.12206572319425175
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.13868v1",
      "true_abstract": "Policy makers need to decide whether to treat or not to treat heterogeneous\nindividuals. The optimal treatment choice depends on the welfare function that\nthe policy maker has in mind and it is referred to as the policy learning\nproblem. I study a general setting for policy learning with semiparametric\nSocial Welfare Functions (SWFs) that can be estimated by locally\nrobust/orthogonal moments based on U-statistics. This rich class of SWFs\nsubstantially expands the setting in Athey and Wager (2021) and accommodates a\nwider range of distributional preferences. Three main applications of the\ngeneral theory motivate the paper: (i) Inequality aware SWFs, (ii) Inequality\nof Opportunity aware SWFs and (iii) Intergenerational Mobility SWFs. I use the\nPanel Study of Income Dynamics (PSID) to assess the effect of attending\npreschool on adult earnings and estimate optimal policy rules based on parental\nyears of education and parental income.",
      "generated_abstract": "r investigates the causal effects of a firm's product quality on\nthe firm's sales, using a new dataset from the United States that measures\nproduct quality through a survey of customers. We employ a Bayesian\nintervention method to estimate the causal effects of product quality on sales.\nOur results suggest that product quality is positively associated with\nsales, with the causal effect being driven by the product's price and\ndistribution. In addition, the causal effect of product quality on sales is\npositively correlated with the firm's price and sales management policies. Our\nfindings suggest that the causal effect of product quality on sales can be\nexplained by factors other than firm's product quality, such as price and\ndistribution, and the effect of price and distribution is more pronounced\nwhen a firm's sales management policies are considered. This study offers a\nnovel framework for causal inference",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17647058823529413,
          "p": 0.24324324324324326,
          "f": 0.20454544967200428
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.045454545454545456,
          "f": 0.03999999507200061
        },
        "rouge-l": {
          "r": 0.1568627450980392,
          "p": 0.21621621621621623,
          "f": 0.18181817694473154
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.10447v1",
      "true_abstract": "In the Feedback Arc Set in Tournaments (Subset-FAST) problem, we are given a\ntournament $D$ and a positive integer $k$, and the objective is to determine\nwhether there exists an arc set $S \\subseteq A(D)$ of size at most $k$ whose\nremoval makes the graph acyclic. This problem is well-known to be equivalent to\na natural tournament ranking problem, whose task is to rank players in a\ntournament such that the number of pairs in which the lower-ranked player\ndefeats the higher-ranked player is no more than $k$. Using the PTAS for\nSubset-FAST [STOC 2007], Bessy et al. [JCSS 2011] present a $(2 +\n\\varepsilon)k$-vertex kernel for this problem, given any fixed $\\varepsilon >\n0$. A generalization of Subset-FAST, called Subset-FAST, further includes an\nadditional terminal subset $T \\subseteq V(D)$ in the input. The goal of\nSubset-FAST is to determine whether there is an arc set $S \\subseteq A(D)$ of\nsize at most $k$ whose removal ensures that no directed cycle passes through\nany terminal in $T$. Prior to our work, no polynomial kernel for Subset-FAST\nwas known. In our work, we show that Subset-FAST admits an $\\mathcal{O}((\\alpha\nk)^{2})$-vertex kernel, provided that Subset-FAST has an approximation\nalgorithm with an approximation ratio $\\alpha$. Consequently, based on the\nknown $\\mathcal{O}(\\log k \\log \\log k)$-approximation algorithm, we obtain an\nalmost quadratic kernel for Subset-FAST.",
      "generated_abstract": "aper, we consider the problem of computing a $(1 + \\epsilon)$-approximation\nfor the minimum vertex cover of a graph $G$, where $\\epsilon \\in (0,1)$ is a\ngiven parameter. We first introduce a new notion of vertex cover called\n$\\epsilon$-cover. We then propose a polynomial-time algorithm for computing\n$\\epsilon$-cover for a graph $G$ using the minimum vertex cover of a\n$1+\\epsilon$-approximate bipartite graph $G'$ on the same parameters. We show\nthat the minimum vertex cover of $G'$ is also a $\\epsilon$-cover for $G$.\nFurthermore, we present an $O(n^3 \\log^4 n)$-time algorithm for computing\n$\\epsilon$-cover for a graph $G$ where $n$ is the number of vertices in $G$.\nOur algorithm runs in $O(n^3)$ time",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14074074074074075,
          "p": 0.31666666666666665,
          "f": 0.19487179061143994
        },
        "rouge-2": {
          "r": 0.02564102564102564,
          "p": 0.05434782608695652,
          "f": 0.03484320121890572
        },
        "rouge-l": {
          "r": 0.14074074074074075,
          "p": 0.31666666666666665,
          "f": 0.19487179061143994
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.02477v1",
      "true_abstract": "Two high-level \"pictures\" of probability theory have emerged: one that takes\nas central the notion of random variable, and one that focuses on distributions\nand probability channels (Markov kernels). While the channel-based picture has\nbeen successfully axiomatized, and widely generalized, using the notion of\nMarkov category, the categorical semantics of the random variable picture\nremain less clear. Simpson's probability sheaves are a recent approach, in\nwhich probabilistic concepts like random variables are allowed vary over a site\nof sample spaces. Simpson has identified rich structure on these sites, most\nnotably an abstract notion of conditional independence, and given examples\nranging from probability over databases to nominal sets. We aim bring this\ndevelopment together with the generality and abstraction of Markov categories:\nWe show that for any suitable Markov category, a category of sample spaces can\nbe defined which satisfies Simpson's axioms, and that a theory of probability\nsheaves can be developed purely synthetically in this setting. We recover\nSimpson's examples in a uniform fashion from well-known Markov categories, and\nconsider further generalizations.",
      "generated_abstract": "We prove that the set of isomorphism classes of simple modules of finite-dimensional\ncorrespondence algebras over a non-associative algebra $A$ is homeomorphic to\nthe set of isomorphism classes of simple $A$-modules.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0625,
          "p": 0.3181818181818182,
          "f": 0.10447760919581206
        },
        "rouge-2": {
          "r": 0.00625,
          "p": 0.041666666666666664,
          "f": 0.010869562948960776
        },
        "rouge-l": {
          "r": 0.0625,
          "p": 0.3181818181818182,
          "f": 0.10447760919581206
        }
      }
    },
    {
      "paper_id": "stat.ML.cs/AI/2503.10496v1",
      "true_abstract": "Modeling natural phenomena with artificial neural networks (ANNs) often\nprovides highly accurate predictions. However, ANNs often suffer from\nover-parameterization, complicating interpretation and raising uncertainty\nissues. Bayesian neural networks (BNNs) address the latter by representing\nweights as probability distributions, allowing for predictive uncertainty\nevaluation. Latent binary Bayesian neural networks (LBBNNs) further handle\nstructural uncertainty and sparsify models by removing redundant weights. This\narticle advances LBBNNs by enabling covariates to skip to any succeeding layer\nor be excluded, simplifying networks and clarifying input impacts on\npredictions. Ultimately, a linear model or even a constant can be found to be\noptimal for a specific problem at hand. Furthermore, the input-skip LBBNN\napproach reduces network density significantly compared to standard LBBNNs,\nachieving over 99% reduction for small networks and over 99.9% for larger ones,\nwhile still maintaining high predictive accuracy and uncertainty measurement.\nFor example, on MNIST, we reached 97% accuracy and great calibration with just\n935 weights, reaching state-of-the-art for compression of neural networks.\nFurthermore, the proposed method accurately identifies the true covariates and\nadjusts for system non-linearity. The main contribution is the introduction of\nactive paths, enhancing directly designed global and local explanations within\nthe LBBNN framework, that have theoretical guarantees and do not require post\nhoc external tools for explanations.",
      "generated_abstract": "t an extension of the Gumbel-Maximization trick for estimating\nthe posterior distribution of a latent variable in a Markov Decision Process\n(MDP). The trick is based on the idea that a random variable $X$ drawn from\nthe posterior distribution of the latent variable $Z$ can be interpreted as a\nrandomized version of $Z$. We then propose a variant of the Gumbel-Maximization\ntrick that allows us to estimate the posterior of $Z$ without drawing $X$.\nWhile the method is not guaranteed to produce a valid posterior, it can be\napplied to a large class of MDPs, including reinforcement learning (RL)\nenvironments. We prove that our method achieves the same convergence rate as the\nGumbel-Maximization trick in the regime where the gradient of the log-likelihood\nfunction is smooth. We also discuss how our method can be",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09615384615384616,
          "p": 0.19230769230769232,
          "f": 0.1282051237606839
        },
        "rouge-2": {
          "r": 0.0049504950495049506,
          "p": 0.008547008547008548,
          "f": 0.0062695878314909465
        },
        "rouge-l": {
          "r": 0.09615384615384616,
          "p": 0.19230769230769232,
          "f": 0.1282051237606839
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.08244v1",
      "true_abstract": "We establish the existence of intermittent two-point dynamics and infinite\nstationary measures for a class of random circle endomorphisms with zero\nLyapunov exponent, as a dynamical characterisation of the transition from\nsynchronisation (negative Lyapunov exponent) to chaos (positive Lyapunov\nexponent).",
      "generated_abstract": "We provide a new characterization of the set of rational numbers in the unit\nreal line, that is the set of real numbers that can be expressed as the limit of\na sequence of rationals. In this setting, we prove that every element of the\nunit interval is an accumulation point of a sequence of rationals. We show that\nthis is not true for any other subset of the unit interval.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.14634146341463414,
          "f": 0.16216215722059915
        },
        "rouge-2": {
          "r": 0.02631578947368421,
          "p": 0.01694915254237288,
          "f": 0.02061855193538212
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.14634146341463414,
          "f": 0.16216215722059915
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08546v1",
      "true_abstract": "Positron Emission Tomography (PET) is a functional imaging modality that\nenables the visualization of biochemical and physiological processes across\nvarious tissues. Recently, deep learning (DL)-based methods have demonstrated\nsignificant progress in directly mapping sinograms to PET images. However,\nregression-based DL models often yield overly smoothed reconstructions lacking\nof details (i.e., low distortion, low perceptual quality), whereas GAN-based\nand likelihood-based posterior sampling models tend to introduce undesirable\nartifacts in predictions (i.e., high distortion, high perceptual quality),\nlimiting their clinical applicability. To achieve a robust\nperception-distortion tradeoff, we propose Posterior-Mean Denoising Diffusion\nModel (PMDM-PET), a novel approach that builds upon a recently established\nmathematical theory to explore the closed-form expression of\nperception-distortion function in diffusion model space for PET image\nreconstruction from sinograms. Specifically, PMDM-PET first obtained\nposterior-mean PET predictions under minimum mean square error (MSE), then\noptimally transports the distribution of them to the ground-truth PET images\ndistribution. Experimental results demonstrate that PMDM-PET not only generates\nrealistic PET images with possible minimum distortion and optimal perceptual\nquality but also outperforms five recent state-of-the-art (SOTA) DL baselines\nin both qualitative visual inspection and quantitative pixel-wise metrics PSNR\n(dB)/SSIM/NRMSE.",
      "generated_abstract": "tection and semantic segmentation are essential tasks in medical\nmedicine, enabling the analysis of medical images for diagnosis and treatment\nplanning. Recent advances in deep learning have revolutionized medical image\nanalysis, with deep neural networks achieving state-of-the-art performance\nacross a variety of medical tasks. However, these models often require\nsignificant computational resources, limiting their use in resource-constrained\nenvironments. In this paper, we introduce the first end-to-end learning model\nfor medical image segmentation, which can be deployed on low-power devices\nwithout compromising performance. Our model is based on a series of lightweight\nconvolutional neural networks (CNNs), each designed for a specific task. These\nCNNs are combined into a single encoder, which generates feature maps that are\nthen processed by a series of decoders, each specialized for a specific\nfunctional",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19727891156462585,
          "p": 0.30851063829787234,
          "f": 0.24066389565675528
        },
        "rouge-2": {
          "r": 0.016304347826086956,
          "p": 0.025,
          "f": 0.019736837326870964
        },
        "rouge-l": {
          "r": 0.17687074829931973,
          "p": 0.2765957446808511,
          "f": 0.21576763009658934
        }
      }
    },
    {
      "paper_id": "math.CV.math/CV/2503.05976v1",
      "true_abstract": "We prove that the (hermitian) rank of $QP^d$ is bounded from below by the\nrank of $P^d$ whenever $Q$ is not identically zero and real-analytic in a\nneighborhood of some point on the zero set of $P$ in $\\mathbb{C}^n$ and $P$ is\na polynomial of bidegree at most $(1,1)$. This result generalizes the theorem\nof D'Angelo and the second author which assumed that $P$ was bihomogeneous.\nExamples show that no hypothesis can be dropped.",
      "generated_abstract": "the existence of bounded solutions of the initial-boundary value\nproblem (IBVP)\n\\begin{equation*}\n\\begin{cases}\nu_{t} - \\Delta u = 0 & \\text{in } (0, \\infty) \\times \\Omega, \\\\\nu = 0 & \\text{on } (0, \\infty) \\times \\Gammac, \\\\\nu = f |_{\\Gammac} & \\text{on } \\Gamma_c, \\\\\nu = g |_{\\Gamma_c} & \\text{on } \\Gamma_l.\n\\end{cases}\n\\end{equation*}\nHere $\\Omega$ is a bounded domain with smooth boundary, $\\Gammac$ is an open\nsubset of $\\partial \\Omega$, and $f$, $g$ are non-zero functions in $L^2(\\Gamma_c)$\nwith $f|_{\\G",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1320754716981132,
          "p": 0.1206896551724138,
          "f": 0.1261261211362716
        },
        "rouge-2": {
          "r": 0.013888888888888888,
          "p": 0.014492753623188406,
          "f": 0.014184392165385794
        },
        "rouge-l": {
          "r": 0.09433962264150944,
          "p": 0.08620689655172414,
          "f": 0.09009008510023565
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2411.12010v2",
      "true_abstract": "The advancement of novel combinatorial CRISPR screening technologies enables\nthe identification of synergistic gene combinations on a large scale. This is\ncrucial for developing novel and effective combination therapies, but the\ncombinatorial space makes exhaustive experimentation infeasible. We introduce\nNAIAD, an active learning framework that efficiently discovers optimal gene\npairs capable of driving cells toward desired cellular phenotypes. NAIAD\nleverages single-gene perturbation effects and adaptive gene embeddings that\nscale with the training data size, mitigating overfitting in small-sample\nlearning while capturing complex gene interactions as more data is collected.\nEvaluated on four CRISPR combinatorial perturbation datasets totaling over\n350,000 genetic interactions, NAIAD, trained on small datasets, outperforms\nexisting models by up to 40\\% relative to the second-best. NAIAD's\nrecommendation system prioritizes gene pairs with the maximum predicted\neffects, resulting in the highest marginal gain in each AI-experiment round and\naccelerating discovery with fewer CRISPR experimental iterations. Our NAIAD\nframework (https://github.com/NeptuneBio/NAIAD) improves the identification of\nnovel, effective gene combinations, enabling more efficient CRISPR library\ndesign and offering promising applications in genomics research and therapeutic\ndevelopment.",
      "generated_abstract": "The study of the population dynamics of microbial cells is a complex and\nfield with numerous methodological issues, such as the identification of\nindividual cells, the quantification of the cell population, and the study of\ntheir interactions. This paper deals with the identification of individual cells\nin a cell population through the analysis of the spectral properties of the\npopulation. The analysis of the population spectra allows the identification of\nindividual cells, as well as the quantification of the cell population. This\npaper proposes a methodology to identify individual cells in a cell population\nbased on the analysis of the spectral properties of the population. We\ndemonstrate the feasibility of our methodology through the analysis of the\nspectra of cell populations of microorganisms and their applications in\ndetection and identification of individual cells in microbial populations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13846153846153847,
          "p": 0.34615384615384615,
          "f": 0.19780219372056523
        },
        "rouge-2": {
          "r": 0.023529411764705882,
          "p": 0.043478260869565216,
          "f": 0.03053434658819484
        },
        "rouge-l": {
          "r": 0.12307692307692308,
          "p": 0.3076923076923077,
          "f": 0.1758241717425433
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2501.02214v1",
      "true_abstract": "One approach to estimating the average treatment effect in binary treatment\nwith unmeasured confounding is the proximal causal inference, which assumes the\navailability of outcome and treatment confounding proxies. The key identifying\nresult relies on the existence of a so-called bridge function. A parametric\nspecification of the bridge function is usually postulated and estimated using\nstandard techniques. The estimated bridge function is then plugged in to\nestimate the average treatment effect. This approach may have two efficiency\nlosses. First, the bridge function may not be efficiently estimated since it\nsolves an integral equation. Second, the sequential procedure may fail to\naccount for the correlation between the two steps. This paper proposes to\napproximate the integral equation with increasing moment restrictions and\njointly estimate the bridge function and the average treatment effect. Under\nsufficient conditions, we show that the proposed estimator is efficient. To\nassist implementation, we propose a data-driven procedure for selecting the\ntuning parameter (i.e., number of moment restrictions). Simulation studies\nreveal that the proposed method performs well in finite samples, and\napplication to the right heart catheterization dataset from the SUPPORT study\ndemonstrates its practical value.",
      "generated_abstract": "y examines the effect of the use of the Expectation-Maximization (EM)\nmethod for sequential estimation on the performance of the Bayesian Adaptive\nMonte Carlo (BAMC) algorithm, which is a Bayesian version of EM. BAMC is a\nwell-known MCMC method for Bayesian inference and has been widely used for\nsimulation studies. The BAMC algorithm requires the evaluation of the\nposterior distribution in each iteration, which is computationally intensive.\nTo address this challenge, we propose a sequential EM algorithm based on the\nexpectation-maximization framework, which can be implemented efficiently using\nthe EM algorithm. The proposed methodology is validated through simulations\nusing the multivariate normal Gaussian distribution. The results demonstrate\nthat the proposed methodology outperforms the EM method in terms of the number\nof iterations, computational efficiency, and efficiency in handling non-Gaussian",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19834710743801653,
          "p": 0.2962962962962963,
          "f": 0.23762375757229692
        },
        "rouge-2": {
          "r": 0.040697674418604654,
          "p": 0.0603448275862069,
          "f": 0.048611106300154805
        },
        "rouge-l": {
          "r": 0.19008264462809918,
          "p": 0.2839506172839506,
          "f": 0.22772276747328704
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/DB/2503.06882v1",
      "true_abstract": "Maximum Inner Product Search (MIPS) for high-dimensional vectors is pivotal\nacross databases, information retrieval, and artificial intelligence. Existing\nmethods either reduce MIPS to Nearest Neighbor Search (NNS) while suffering\nfrom harmful vector space transformations, or attempt to tackle MIPS directly\nbut struggle to mitigate redundant computations due to the absence of the\ntriangle inequality. This paper presents a novel theoretical framework that\nequates MIPS with NNS without requiring space transformation, thereby allowing\nus to leverage advanced graph-based indices for NNS and efficient edge pruning\nstrategies, significantly reducing unnecessary computations. Despite a strong\nbaseline set by our theoretical analysis, we identify and address two\npersistent challenges to further refine our method: the introduction of the\nProximity Graph with Spherical Pathway (PSP), designed to mitigate the issue of\nMIPS solutions clustering around large-norm vectors, and the implementation of\nAdaptive Early Termination (AET), which efficiently curtails the excessive\nexploration once an accuracy bottleneck is reached. Extensive experiments\nreveal the superiority of our method over existing state-of-the-art techniques\nin search efficiency, scalability, and practical applicability. Compared with\nstate-of-the-art graph based methods, it achieves an average 35% speed-up in\nquery processing and a 3x reduction in index size. Notably, our approach has\nbeen validated and deployed in the search engines of Shopee, a well-known\nonline shopping platform. Our code and an industrial-scale dataset for offline\nevaluation will also be released to address the absence of e-commerce data in\npublic benchmarks.",
      "generated_abstract": "This paper introduces the K-Nearest Neighbor DBMS (KNNDBMS) framework for\nK-nearest neighbor (KNN) queries, which uses a hash-based data structure\ninstead of traditional data structures for efficient storage and querying. The\nKNNDBMS framework uses a hash-based data structure to store query inputs and\noutputs, and a hash table to store the nearest neighbor search results. The\nKNNDBMS framework also supports efficient data retrieval and deletion. The\nKNNDBMS framework can be extended to support other types of similarity\nmeasurements, such as cosine similarity and Euclidean distance. The\nimplementations of the KNNDBMS framework are implemented in Java, and the\nframework is evaluated through extensive experiments. The results show that the\nKNNDBMS framework can outperform traditional data structures for KNN queries.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11931818181818182,
          "p": 0.2916666666666667,
          "f": 0.16935483458896994
        },
        "rouge-2": {
          "r": 0.017391304347826087,
          "p": 0.039603960396039604,
          "f": 0.0241691800494709
        },
        "rouge-l": {
          "r": 0.10227272727272728,
          "p": 0.25,
          "f": 0.14516128620187316
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2502.19333v1",
      "true_abstract": "Despite advances in methods to interrogate tumor biology, the observational\nand population-based approach of classical cancer research and clinical\noncology does not enable anticipation of tumor outcomes to hasten the discovery\nof cancer mechanisms and personalize disease management. To address these\nlimitations, individualized cancer forecasts have been shown to predict tumor\ngrowth and therapeutic response, inform treatment optimization, and guide\nexperimental efforts. These predictions are obtained via computer simulations\nof mathematical models that are constrained with data from a patient's cancer\nand experiments. This book chapter addresses the validation of these\nmathematical models to forecast tumor growth and treatment response. We start\nwith an overview of mathematical modeling frameworks, model selection\ntechniques, and fundamental metrics. We then describe the usual strategies\nemployed to validate cancer forecasts in preclinical and clinical scenarios.\nFinally, we discuss existing barriers in validating these predictions along\nwith potential strategies to address them.",
      "generated_abstract": "t a novel approach to the study of cellular motility in complex\nenvironmental conditions by combining a combination of two state-of-the-art\nmethods: molecular dynamics simulations with the molecular dynamics\noptimization technique (MDO) and active particles simulations with the\nmulti-state model (MSM). Our approach allows for the identification of\nmechanisms of cellular swimming by combining state-of-the-art methods. We\ndemonstrate that the proposed approach is able to identify mechanisms of\ncellular swimming, such as active particle-mediated swimming and\nbias-induced swimming, in different types of cells, such as epithelial cells,\nmesenchymal cells, and neural cells. We also show that our approach is\ncapable of distinguishing between different types of motility in different\nenvironments. Our results contribute to the understanding of cellular\nmotility and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13861386138613863,
          "p": 0.2028985507246377,
          "f": 0.16470587753010396
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10891089108910891,
          "p": 0.15942028985507245,
          "f": 0.12941175988304518
        }
      }
    },
    {
      "paper_id": "physics.pop-ph.physics/pop-ph/2501.08583v1",
      "true_abstract": "In this work, I propose a way to help high school students and the general\npopulation understand quantum concepts by adopting a new inherently dual\nrepresentation. Major difficulties in explaining to people the basic concepts\nof quantum mechanics reside in the apparent impossibility of representing\nquantum superposition with examples taken from everyday life. In this context,\nI propose a new pictorial paradigm that illustrates a number of quantum\nconcepts by means of optical illusions, potentially without raising\nmisconceptions. The method is based on \"bistable reversible figures\", which\ninduce in the viewer a multistable perception, conveying a direct understanding\nof superposition, random collapse, and observer effect via a sensorial\nexperience. I present the advantages and discuss the limitations of this\nanalogy, and show how it extends to the concepts of complementarity and quantum\nentanglement, also helping to avoiding misconceptions in quantum teleportation.\nFinally, I also address quantum spin and quantum measurement by using different\ntypes of optical illusions.",
      "generated_abstract": "ard framework of quantum mechanics, which is based on the\nquantum theory of the electron, has been extended to a number of other\nquantum systems, such as atoms and molecules. In this review, we discuss the\nextension of the quantum theory of the electron to the quantum theory of\nmolecules. The extension is based on the assumption that the electron wave\nfunction is represented by a wave function of the atomic orbital in the\nSchr\\\"odinger picture. We also discuss the quantum theory of the\nmolecule, which is based on the Schr\\\"odinger picture. We review the\ngeneralizations of the Schr\\\"odinger equation to the Schr\\\"odinger equation\nwith the electronic interaction. We also discuss the quantum theory of\nmolecules with the electronic interaction, which is based on the\nHamilton-Jacobi theory. The Hamilton-Jacobi theory is a generalization of the\nSchr\\\"",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19230769230769232,
          "p": 0.3508771929824561,
          "f": 0.24844720039504656
        },
        "rouge-2": {
          "r": 0.06206896551724138,
          "p": 0.10112359550561797,
          "f": 0.07692307220943853
        },
        "rouge-l": {
          "r": 0.18269230769230768,
          "p": 0.3333333333333333,
          "f": 0.23602484014659933
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.04480v1",
      "true_abstract": "Research in adversarial machine learning (AML) has shown that statistical\nmodels are vulnerable to maliciously altered data. However, despite advances in\nBayesian machine learning models, most AML research remains concentrated on\nclassical techniques. Therefore, we focus on extending the white-box model\npoisoning paradigm to attack generic Bayesian inference, highlighting its\nvulnerability in adversarial contexts. A suite of attacks are developed that\nallow an attacker to steer the Bayesian posterior toward a target distribution\nthrough the strategic deletion and replication of true observations, even when\nonly sampling access to the posterior is available. Analytic properties of\nthese algorithms are proven and their performance is empirically examined in\nboth synthetic and real-world scenarios. With relatively little effort, the\nattacker is able to substantively alter the Bayesian's beliefs and, by\naccepting more risk, they can mold these beliefs to their will. By carefully\nconstructing the adversarial posterior, surgical poisoning is achieved such\nthat only targeted inferences are corrupted and others are minimally disturbed.",
      "generated_abstract": "ork, we propose a novel approach for the nonparametric estimation of\nnonlinear functions. We consider a general class of nonlinear functions and\nestimate them by means of a stochastic approximation algorithm, based on the\nwell-known stochastic gradient method. Our methodology is based on the use of\na family of probability measures defined on the space of nonlinear functions,\nwhich we call generalized quasi-likelihoods. The proposed approach is\nadapted to a wide range of functions, including piecewise linear and\nnon-convex ones, and it is shown to perform well in a variety of scenarios,\nincluding the estimation of nonlinear functions in high-dimensional settings.\nAdditionally, we consider a case study of estimating the number of principal\ncomponents of a data set using the proposed methodology. Our results show that\nthe methodology is able to estimate the number of principal components with\nhigh accuracy and provides a simple and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11864406779661017,
          "p": 0.1686746987951807,
          "f": 0.13930347773867
        },
        "rouge-2": {
          "r": 0.012738853503184714,
          "p": 0.015748031496062992,
          "f": 0.014084502098047758
        },
        "rouge-l": {
          "r": 0.11864406779661017,
          "p": 0.1686746987951807,
          "f": 0.13930347773867
        }
      }
    },
    {
      "paper_id": "math.ST.cs/CC/2503.02802v1",
      "true_abstract": "In this work, we show the first average-case reduction transforming the\nsparse Spiked Covariance Model into the sparse Spiked Wigner Model and as a\nconsequence obtain the first computational equivalence result between two\nwell-studied high-dimensional statistics models. Our approach leverages a new\nperturbation equivariance property for Gram-Schmidt orthogonalization, enabling\nremoval of dependence in the noise while preserving the signal.",
      "generated_abstract": "t a novel algorithm for the optimal transportation of\ngaussian processes (GPs) between a fixed and a moving domain. The optimal\ntransportation of a GP between two domains is the minimization of the\ndivergence between the two probability measures, subject to the boundary\nconditions. In the fixed-moving case, we show that this problem can be\nrepresented as a bivariate transport problem, which is solved by\nrepresenting the GP as a GP with two dimensions and a fixed Gaussian kernel.\nThis approach allows us to solve the optimal transport problem for\nmultidimensional GPs with arbitrary covariance kernels, including the\ninfinite-dimensional GPs with arbitrary covariance kernels. We show that the\nsolution of the optimal transport problem for a multidimensional GP is a\nGP with two dimensions and a fixed Gaussian kernel. The optimal transport\nsolution for the fixed-mov",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2653061224489796,
          "p": 0.18840579710144928,
          "f": 0.22033897819448445
        },
        "rouge-2": {
          "r": 0.05454545454545454,
          "p": 0.028037383177570093,
          "f": 0.037037032552202954
        },
        "rouge-l": {
          "r": 0.22448979591836735,
          "p": 0.15942028985507245,
          "f": 0.18644067310973872
        }
      }
    },
    {
      "paper_id": "math.RT.math/GN/2502.08847v1",
      "true_abstract": "A representation $\\rho$ of a compact group $\\mathbb{G}$ selects eigenvalues\nif there is a continuous circle-valued map on $\\mathbb{G}$ assigning an\neigenvalue of $\\rho(g)$ to every $g\\in \\mathbb{G}$. For every compact connected\n$\\mathbb{G}$, we characterize the irreducible $\\mathbb{G}$-representations\nwhich select eigenvalues as precisely those annihilating the intersection\n$Z_0(\\mathbb{G})\\cap \\mathbb{G}'$ of the connected center of $\\mathbb{G}$ with\nits derived subgroup. The result applies more generally to finite-spectrum\nrepresentations isotypic on $Z_0(\\mathbb{G})$, and recovers as applications\n(noted in prior work) the existence of a continuous eigenvalue selector for the\nnatural representation of $\\mathrm{SU}(n)$ and the non-existence of such a\nselector for $\\mathrm{U}(n)$.",
      "generated_abstract": "In the recent past, the study of the behavior of the Laplacian on the\n$C^*$-algebra of compact quantum groups has attracted considerable interest.\nThis paper is devoted to a study of the behavior of the Laplacian on the\n$C^*$-algebra of compact quantum groups in the context of the spectrum of the\nLaplacian. In particular, we investigate the behavior of the Laplacian when\nthe group is the universal cover of a compact quantum group. We prove that the\nspectrum of the Laplacian is contained in the union of the spectra of the\nLaplacians on the group and the universal cover of the group.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15492957746478872,
          "p": 0.2682926829268293,
          "f": 0.19642856678730877
        },
        "rouge-2": {
          "r": 0.041666666666666664,
          "p": 0.06153846153846154,
          "f": 0.049689436179160225
        },
        "rouge-l": {
          "r": 0.14084507042253522,
          "p": 0.24390243902439024,
          "f": 0.17857142393016592
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.05418v1",
      "true_abstract": "Integrated sensing and communication (ISAC) has been identified as a\npromising technology for the sixth generation (6G) of communication networks.\nTarget privacy in ISAC is essential to ensure that only legitimate sensors can\ndetect the target while keeping it hidden from malicious ones. In this paper,\nwe consider a downlink reconfigurable intelligent surface (RIS)-assisted ISAC\nsystem capable of protecting a sensing region against an adversarial detector.\nThe RIS consists of both reflecting and sensing elements, adaptively changing\nthe element assignment based on system needs. To achieve this, we minimize the\nmaximum sensing signal-to-interference-plus-noise-ratio (SINR) at the\nadversarial detector within sample points in the sensing region, by optimizing\nthe transmit beamformer at the base station, the RIS phase shift matrix, the\nreceived beamformer at the RIS, and the division between reflecting and\nabsorptive elements at the RIS, where the latter function as sensing elements.\nAt the same time, the system is designed to maintain a minimum sensing SINR at\neach monitored location, as well as minimum communication SINR for each user.\nTo solve this challenging optimization problem, we develop an alternating\noptimization approach combined with a successive convex approximation based\nmethod tailored for each subproblem. Our results show that the proposed\napproach achieves a 25 dB reduction in the maximum sensing SINR at the\nadversarial detector compared to scenarios without sensing area protection.\nAlso, the optimal RIS element assignment can further improve sensing protection\nby 3 dB over RISs with fixed element configuration.",
      "generated_abstract": "r presents a novel design of a high-efficiency hybrid active\nreconfigurable antenna for future 6G networks. Our approach combines the\nflexible reconfigurability of active elements with the efficient operation of\npassive elements. This design provides a highly flexible antenna that is able\nto adapt to changing conditions and the user's preference. The hybrid\nreconfigurable antenna is based on an active layer that has a large array\nspacing, enabling high-gain configurations. The passive layer consists of\nfemto-ring antennas, which provide low-gain configurations. The hybrid\nreconfigurable antenna is designed as a 2x2 mI/Q array, with a total number of\nactive elements of 16 and 24 active elements for 1.8 GHz and 2.6 GHz bands,\nrespectively. To demonstrate the performance of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15894039735099338,
          "p": 0.3157894736842105,
          "f": 0.2114537400392013
        },
        "rouge-2": {
          "r": 0.026785714285714284,
          "p": 0.05454545454545454,
          "f": 0.03592813929506312
        },
        "rouge-l": {
          "r": 0.15894039735099338,
          "p": 0.3157894736842105,
          "f": 0.2114537400392013
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.10055v1",
      "true_abstract": "While 3D point clouds are widely utilized across various vision applications,\ntheir irregular and sparse nature make them challenging to handle. In response,\nnumerous encoding approaches have been proposed to capture the rich semantic\ninformation of point clouds. Yet, a critical limitation persists: a lack of\nconsideration for colored point clouds which are more capable 3D\nrepresentations as they contain diverse attributes: color and geometry. While\nexisting methods handle these attributes separately on a per-point basis, this\nleads to a limited receptive field and restricted ability to capture\nrelationships across multiple points. To address this, we pioneer a point cloud\nencoding methodology that leverages 3D Fourier decomposition to disentangle\ncolor and geometric features while extending the receptive field through\nspectral-domain operations. Our analysis confirms that this encoding approach\neffectively separates feature components, where the amplitude uniquely captures\ncolor attributes and the phase encodes geometric structure, thereby enabling\nindependent learning and utilization of both attributes. Furthermore, the\nspectral-domain properties of these components naturally aggregate local\nfeatures while considering multiple points' information. We validate our point\ncloud encoding approach on point cloud classification and style transfer tasks,\nachieving state-of-the-art results on the DensePoint dataset with improvements\nvia a proposed amplitude-based data augmentation strategy.",
      "generated_abstract": "aper, we investigate a novel approach to enhance the efficiency and\nquality of speech enhancement (SE) systems, leveraging a multi-scale\narchitecture. The proposed model consists of two main components: a\nmulti-scale encoder and a multi-scale decoder. The encoder is a two-branch\nnetwork that extracts high-level features from the input audio signal, while the\ndecoder is a two-branch network that generates enhanced audio signals with\nrespect to the features extracted by the encoder. In the encoder, a dual-path\nattention mechanism is used to enhance multi-scale representations. To\nenhance the multi-scale representations, we introduce a multi-scale\nprojection layer that utilizes a two-branch architecture, with the\nprojection head having a different number of channels than the other branch. We\nalso introduce a multi-scale attention mechanism in the encoder to enhance the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11428571428571428,
          "p": 0.2191780821917808,
          "f": 0.15023473727875875
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11428571428571428,
          "p": 0.2191780821917808,
          "f": 0.15023473727875875
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06370v1",
      "true_abstract": "This paper presents a method for load balancing and dynamic pricing in\nelectric vehicle (EV) charging networks, utilizing reinforcement learning (RL)\nto enhance network performance. The proposed framework integrates a pre-trained\ngraph neural network to predict demand elasticity and inform pricing decisions.\nThe spatio-temporal EV charging demand prediction (EVCDP) dataset from Shenzhen\nis utilized to capture the geographic and temporal characteristics of the\ncharging stations. The RL model dynamically adjusts prices at individual\nstations based on occupancy, maximum station capacity, and demand forecasts,\nensuring an equitable network load distribution while preventing station\noverloads. By leveraging spatially-aware demand predictions and a carefully\ndesigned reward function, the framework achieves efficient load balancing and\nadaptive pricing strategies that respond to localized demand and global network\ndynamics, ensuring improved network stability and user satisfaction. The\nefficacy of the approach is validated through simulations on the dataset,\nshowing significant improvements in load balancing and reduced overload as the\nRL agent iteratively interacts with the environment and learns to dynamically\nadjust pricing strategies based on real-time demand patterns and station\nconstraints. The findings highlight the potential of adaptive pricing and\nload-balancing strategies to address the complexities of EV infrastructure,\npaving the way for scalable and user-centric solutions.",
      "generated_abstract": "This paper studies a general class of distributed control problems involving\nmultiple agents operating in a distributed way. The agents are equipped with a\nnetwork of communication links, which enable them to exchange information with\neach other. The goal of the agents is to reach a given destination while\nminimizing a cost functional function involving the communication delays. In\nthis paper, we consider the case where the communication delays follow an\nexponential distribution. We first propose a distributed optimal control\nalgorithm for this class of problems, and then provide a convergence\nanalysis of the algorithm. Our analysis establishes that the convergence rate\nof the algorithm is sublinear. We also present an algorithmic scheme to\nobtain a sublinear convergence rate, and furthermore, we provide a convergence\nanalysis for this scheme.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13953488372093023,
          "p": 0.23076923076923078,
          "f": 0.1739130387817687
        },
        "rouge-2": {
          "r": 0.010416666666666666,
          "p": 0.017094017094017096,
          "f": 0.01294497911333316
        },
        "rouge-l": {
          "r": 0.13953488372093023,
          "p": 0.23076923076923078,
          "f": 0.1739130387817687
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.09287v1",
      "true_abstract": "We study the properties of macroeconomic survey forecast response averages as\nthe number of survey respondents grows. Such averages are \"portfolios\" of\nforecasts. We characterize the speed and pattern of the gains from\ndiversification and their eventual decrease with portfolio size (the number of\nsurvey respondents) in both (1) the key real-world data-based environment of\nthe U.S. Survey of Professional Forecasters (SPF), and (2) the theoretical\nmodel-based environment of equicorrelated forecast errors. We proceed by\nproposing and comparing various direct and model-based \"crowd size signature\nplots,\" which summarize the forecasting performance of k-average forecasts as a\nfunction of k, where k is the number of forecasts in the average. We then\nestimate the equicorrelation model for growth and inflation forecast errors by\nchoosing model parameters to minimize the divergence between direct and\nmodel-based signature plots. The results indicate near-perfect equicorrelation\nmodel fit for both growth and inflation, which we explicate by showing\nanalytically that, under conditions, the direct and fitted equicorrelation\nmodel-based signature plots are identical at a particular model parameter\nconfiguration, which we characterize. We find that the gains from\ndiversification are greater for inflation forecasts than for growth forecasts,\nbut that both gains nevertheless decrease quite quickly, so that fewer SPF\nrespondents than currently used may be adequate.",
      "generated_abstract": "ntext of the COVID-19 pandemic, it is crucial to assess the effect\nof the economic crisis on the demand for alcoholic beverages. This study\ninvestigates the impact of the pandemic on alcohol consumption in Germany. To\naddress the limitations of existing econometric models, we propose a novel\nmodel that combines dynamic discrete choice (DDC) with dynamic stochastic\ngene-pool (DSGP) models. We develop a novel method to estimate and interpret\nthe DDC and DSGP parameters, which are crucial for the estimation of the\nimpact of the pandemic on alcohol consumption. The results show that the\npandemic has had a significant impact on alcohol consumption in Germany. The\nresults indicate that the impact of the pandemic on alcohol consumption is\nstronger in areas with lower rates of alcohol consumption. The results also\nshow that the impact of the pandemic on alco",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16393442622950818,
          "p": 0.2702702702702703,
          "f": 0.2040816279529364
        },
        "rouge-2": {
          "r": 0.021052631578947368,
          "p": 0.038834951456310676,
          "f": 0.027303749707044526
        },
        "rouge-l": {
          "r": 0.1557377049180328,
          "p": 0.25675675675675674,
          "f": 0.1938775463202833
        }
      }
    },
    {
      "paper_id": "cs.HC.eess/SY/2503.03570v1",
      "true_abstract": "Traditional XR and Metaverse applications prioritize user experience (UX) for\nadoption and success but often overlook a crucial aspect of user interaction:\nemotions. This article addresses this gap by presenting an emotion-aware\nMetaverse application: a Virtual Reality (VR) fire drill simulator designed to\nprepare crews for shipboard emergencies. The simulator detects emotions in real\ntime, assessing trainees responses under stress to improve learning outcomes.\nIts architecture incorporates eye-tracking and facial expression analysis via\nMeta Quest Pro headsets. The system features four levels whose difficulty is\nincreased progressively to evaluate user decision-making and emotional\nresilience. The system was evaluated in two experimental phases. The first\nphase identified challenges, such as navigation issues and lack of visual\nguidance. These insights led to an improved second version with a better user\ninterface, visual cues and a real-time task tracker. Performance metrics like\ncompletion times, task efficiency and emotional responses were analyzed. The\nobtained results show that trainees with prior VR or gaming experience\nnavigated the scenarios more efficiently. Moreover, the addition of\ntask-tracking visuals and navigation guidance significantly improved user\nperformance, reducing task completion times between 14.18\\% and 32.72\\%.\nEmotional responses were captured, revealing that some participants were\nengaged, while others acted indifferently, indicating the need for more\nimmersive elements. Overall, this article provides useful guidelines for\ncreating the next generation of emotion-aware Metaverse applications.",
      "generated_abstract": "r presents a novel distributed framework for the generation of\nsub-carrier-free (SCF) signals. The proposed method is based on the\ndistributed optimization of the orthogonal frequency division multiplexing\n(OFDM) signal design, where each individual user's sub-carrier is assigned to\na dedicated physical layer (PL) channel. We develop a novel algorithm to\noptimize the OFDM signal design for each user, leveraging the knowledge of the\nchannel matrix and the channel state information (CSI) at the PL. The\noptimization problem is solved using the alternating optimization (AO) method,\nwhere each iteration consists of an optimization for the physical layer and a\nregularization step for the channel. We provide theoretical guarantees on the\nconvergence of the proposed algorithm. The proposed framework is applied to the\nspectral efficiency (SE) analysis of a multi-cell multi-user system with\nmultiple-input and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08484848484848485,
          "p": 0.175,
          "f": 0.11428570988754704
        },
        "rouge-2": {
          "r": 0.009174311926605505,
          "p": 0.01652892561983471,
          "f": 0.011799405438868484
        },
        "rouge-l": {
          "r": 0.07878787878787878,
          "p": 0.1625,
          "f": 0.10612244458142459
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.09565v1",
      "true_abstract": "Despite deep neural networks' powerful representation learning capabilities,\ntheoretical understanding of how networks can simultaneously achieve meaningful\nfeature learning and global convergence remains elusive. Existing approaches\nlike the neural tangent kernel (NTK) are limited because features stay close to\ntheir initialization in this parametrization, leaving open questions about\nfeature properties during substantial evolution. In this paper, we investigate\nthe training dynamics of infinitely wide, $L$-layer neural networks using the\ntensor program (TP) framework. Specifically, we show that, when trained with\nstochastic gradient descent (SGD) under the Maximal Update parametrization\n($\\mu$P) and mild conditions on the activation function, SGD enables these\nnetworks to learn linearly independent features that substantially deviate from\ntheir initial values. This rich feature space captures relevant data\ninformation and ensures that any convergent point of the training process is a\nglobal minimum. Our analysis leverages both the interactions among features\nacross layers and the properties of Gaussian random variables, providing new\ninsights into deep representation learning. We further validate our theoretical\nfindings through experiments on real-world datasets.",
      "generated_abstract": "e a new paradigm for Bayesian learning called the Probabilistic\nLearning Machine (PLM), which leverages the power of probabilistic graphical\nmodels to learn a probabilistic model of the data distribution. Unlike existing\nmodels based on the belief-update paradigm, the PLM is probabilistic by\ndefinition and does not require additional assumptions about the data\ndistribution. The PLM has two main components: a graphical model and a\nprobabilistic update rule. The graphical model represents the data distribution\nas a probabilistic graph, which captures the relationships between variables.\nThe probabilistic update rule updates the graphical model using a\nprobabilistic-statistical mechanism. We show that the PLM can learn a wide\nvariety of probabilistic models, including linear, logistic, and Gaussian\nmodels, and nonparametric models such as kernel density estimation. The PLM\nprovides a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15555555555555556,
          "p": 0.2727272727272727,
          "f": 0.1981132029214134
        },
        "rouge-2": {
          "r": 0.017964071856287425,
          "p": 0.02654867256637168,
          "f": 0.021428566614541893
        },
        "rouge-l": {
          "r": 0.1259259259259259,
          "p": 0.22077922077922077,
          "f": 0.16037735386480967
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.18647v1",
      "true_abstract": "Urban overheating, exacerbated by climate change, threatens public health and\nurban sustainability. Traditional approaches, such as numerical simulations and\nfield measurements, face challenges due to uncertainties in input data. This\nstudy integrates field measurements with machine learning models to predict the\nduration and severity of future urban overheating events, focusing on the role\nof urban greening under different global warming (GW) scenarios. Field\nmeasurements were conducted in summer 2024 at an office campus in Ottawa, a\ncold-climate city. Microclimate data were collected from four locations with\nvarying levels of greenery: a large lawn without trees (Lawn), a parking lot\nwithout greenery (Parking), an area with sparsely distributed trees (Tree), and\na fully covered forested area (Forest). Machine learning models, including\nArtificial Neural Networks (ANN), Recurrent Neural Networks (RNN), and Long\nShort-Term Memory (LSTM) networks, were trained on local microclimate data,\nwith LSTM achieving the best predictions. Four GW scenarios were analyzed,\ncorresponding to different Shared Socioeconomic Pathways (SSP) for 2050 and\n2090. Results show that the Universal Thermal Climate Index (UTCI) at the\n\"Parking\" location rises from about 27,\\textdegree C under GW1.0 to\n31,\\textdegree C under GW3.5. Moreover, low health risk conditions (UTCI >\n26,\\textdegree C) increase across all locations due to climate change,\nregardless of greenery levels. However, tree-covered areas such as \"Tree\" and\n\"Forest\" effectively prevent extreme heat conditions (UTCI > 38.9,\\textdegree\nC). These findings highlight the crucial role of urban greening in mitigating\nsevere thermal stress and enhancing thermal comfort under future climate\nscenarios.",
      "generated_abstract": "uce a novel approach to the analysis of complex data, using the\ndata to guide the analysis, and the analysis to guide the data. We call this\napproach ``data-driven analysis,'' and we show that it is a generalization of\ntraditional methods such as parametric and nonparametric techniques. We show\nthat the data-driven approach has several important advantages, including its\nability to handle complex data sets, its ability to handle nonlinear relationships,\nand its ability to handle data sets with missing data. We also show how the\ndata-driven approach can be used to handle high-dimensional data, and how it can\nbe used to handle data sets that are not stationary. We illustrate the\napplicability of the data-driven approach using several examples, including\nthe analysis of genetic data, the analysis of financial data, and the analysis\nof biological data. We also show how the data-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07344632768361582,
          "p": 0.203125,
          "f": 0.10788381352662677
        },
        "rouge-2": {
          "r": 0.012552301255230125,
          "p": 0.02830188679245283,
          "f": 0.017391300090906312
        },
        "rouge-l": {
          "r": 0.07344632768361582,
          "p": 0.203125,
          "f": 0.10788381352662677
        }
      }
    },
    {
      "paper_id": "math.ST.math/ST/2503.09507v1",
      "true_abstract": "For one dimensional stochastic Burgers equation driven by space-time white\nnoise we consider the problem of estimation of the diffusivity parameter in\nfront of the second-order spatial derivative. Based on local observations in\nspace, we study the estimator derived in [Altmeyer, Rei{\\ss}, Ann. Appl.\nProbab.(2021)] for linear stochastic heat equation that has also been used in\n[Altmeyer, Cialenco, Pasemann, Bernoulli (2023)] to cover large class of\nsemilinear SPDEs and has been examined for the stochastic Burgers equation\ndriven by trace class noise. We extend the achieved results by considering the\nspace-time white noise case which has also relevant physical motivations. After\nwe establish new regularity results for the solution, we are able to show that\nour proposed estimator is strongly consistent and asymptotically normal.",
      "generated_abstract": "er the problem of optimal control of a linear stochastic\nbi-Lipschitz Markovian jump process with state-dependent control. The\nstate-dependent control is assumed to be proportional to the state, but not to\nthe state-dependent control. We show that if the control is sufficiently\nstrongly convex and there is an $L$-Lipschitz function $\\varphi$ such that\n$\\varphi(x) - \\varphi(y) \\leq \\langle x-y, x-y \\rangle$ for all $x, y \\in\n\\mathbb{R}^n$ and $\\langle x, x \\rangle \\leq L^2$, then the optimal control\nsatisfies $\\| \\nabla \\varphi(x) \\| \\leq L$ for all $x \\in \\mathbb{R}^n$.\nMoreover, we show that the optimal control is a stationary point of the\ncost functional. We further show that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.19444444444444445,
          "f": 0.1794871745167654
        },
        "rouge-2": {
          "r": 0.043859649122807015,
          "p": 0.052083333333333336,
          "f": 0.047619042655782824
        },
        "rouge-l": {
          "r": 0.15476190476190477,
          "p": 0.18055555555555555,
          "f": 0.1666666616962526
        }
      }
    },
    {
      "paper_id": "q-bio.QM.stat/AP/2503.07664v1",
      "true_abstract": "The Antibiotic Resistance Microbiology Dataset (ARMD) is a de-identified\nresource derived from electronic health records (EHR) that facilitates research\ninto antimicrobial resistance (AMR). ARMD encompasses data from adult patients,\nfocusing on microbiological cultures, antibiotic susceptibilities, and\nassociated clinical and demographic features. Key attributes include organism\nidentification, susceptibility patterns for 55 antibiotics, implied\nsusceptibility rules, and de-identified patient information. This dataset\nsupports studies on antimicrobial stewardship, causal inference, and clinical\ndecision-making. ARMD is designed to be reusable and interoperable, promoting\ncollaboration and innovation in combating AMR. This paper describes the\ndataset's acquisition, structure, and utility while detailing its\nde-identification process.",
      "generated_abstract": "We introduce a novel method for Bayesian estimation of the average treatment\neffect (ATE) in randomized controlled trials, and apply it to a pediatric\nepidemiological study. The ATE is estimated by integrating the marginal\ndistribution of the difference between the treated and control group, and the\njoint distribution of the treatment and outcome variables. We then perform\nposterior inference on the distribution of the integrated ATE, using Markov\nchain Monte Carlo (MCMC) sampling. The simulation study shows that the\ninference is accurate, with a mean relative error of 0.03% and a maximum\nrelative error of 0.36% over 100,000 MCMC iterations. This method is\ncomputationally efficient and practical, and can be applied to many other\nrandomized controlled trials, with application to the pediatric Ebola vaccine\ntrial being described here.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.13793103448275862,
          "f": 0.14035087219452158
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10714285714285714,
          "p": 0.10344827586206896,
          "f": 0.10526315289627601
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/NE/2503.08301v2",
      "true_abstract": "In many-task optimization scenarios, surrogate models are valuable for\nmitigating the computational burden of repeated fitness evaluations across\ntasks. This study proposes a novel meta-surrogate framework to assist many-task\noptimization, by leveraging the knowledge transfer strengths and emergent\ncapabilities of large language models (LLMs). We formulate a unified framework\nfor many-task fitness prediction, by defining a universal model with metadata\nto fit a group of problems. Fitness prediction is performed on metadata and\ndecision variables, enabling efficient knowledge sharing across tasks and\nadaptability to new tasks. The LLM-based meta-surrogate treats fitness\nprediction as conditional probability estimation, employing a unified token\nsequence representation for task metadata, inputs, and outputs. This approach\nfacilitates efficient inter-task knowledge sharing through shared token\nembeddings and captures complex task dependencies via multi-task model\ntraining. Experimental results demonstrate the model's emergent generalization\nability, including zero-shot performance on problems with unseen dimensions.\nWhen integrated into evolutionary transfer optimization (ETO), our framework\nsupports dual-level knowledge transfer -- at both the surrogate and individual\nlevels -- enhancing optimization efficiency and robustness. This work\nestablishes a novel foundation for applying LLMs in surrogate modeling,\noffering a versatile solution for many-task optimization.",
      "generated_abstract": "opment of AI-based systems has become critical in our daily lives.\nHowever, the rapid development of these systems has raised concerns about their\nability to impact society and the environment. AI systems are vulnerable to\nmalicious manipulation, which can lead to malicious behaviors. These\nmalicious behaviors can cause serious harm to individuals and society.\nTherefore, it is important to identify malicious behaviors in AI systems. In\nthis paper, we propose a novel framework for detecting malicious behaviors in\nAI systems. The proposed framework integrates multi-task learning and\ndynamic-graph neural networks to enhance the robustness of AI systems. The\ndynamic-graph neural network is used to capture the dynamic characteristics of\nmalicious behaviors in AI systems. The multi-task learning method is used to\nimprove the generalization ability of the dynamic-graph neural network. The\nproposed framework is valid",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1328125,
          "p": 0.22972972972972974,
          "f": 0.16831682704048637
        },
        "rouge-2": {
          "r": 0.01098901098901099,
          "p": 0.01818181818181818,
          "f": 0.013698625440984907
        },
        "rouge-l": {
          "r": 0.1328125,
          "p": 0.22972972972972974,
          "f": 0.16831682704048637
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.15891v1",
      "true_abstract": "We consider the problem of estimating the number of communities in a weighted\nbalanced Stochastic Block Model. We construct hypothesis tests based on\nsemidefinite programming and with a statistic coming from a GOE matrix to\ndistinguish between any two candidate numbers of communities. This is possible\ndue to a universality result for a semidefinite programming-based function that\nwe also prove. The tests are then used to form a sequential test to estimate\nthe number of communities. Furthermore, we also construct estimators of the\ncommunities themselves.",
      "generated_abstract": "The paper studies the estimation of the average of a sequence of random\nvariables, under the assumption that the sequence is generated by a\nmultivariate time-homogeneous Markov chain with a finite state space. This\nassumption can be relaxed by considering a family of models with a common\ntransition matrix. We derive the asymptotic distribution of the estimator of the\naverage of the sequence. In addition, we propose an estimator that can be\ncomputed efficiently. Our method is based on the use of a randomized algorithm\nto sample a set of states from the state space, and the use of a\ndistribution-free estimator of the conditional expectation of the sequence. We\nshow that the estimator based on the randomized algorithm has a consistent\nestimation of the average. Finally, we provide a computational method for\nestimating the conditional expectation of the sequence.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3,
          "p": 0.25,
          "f": 0.27272726776859507
        },
        "rouge-2": {
          "r": 0.05063291139240506,
          "p": 0.03636363636363636,
          "f": 0.042328037462557595
        },
        "rouge-l": {
          "r": 0.26666666666666666,
          "p": 0.2222222222222222,
          "f": 0.24242423746556482
        }
      }
    },
    {
      "paper_id": "physics.med-ph.eess/SY/2503.06172v1",
      "true_abstract": "Noninvasive brain stimulation can activate neurons in the brain but requires\npower electronics with exceptionally high power in the mega-volt-ampere and\nhigh frequencies in the kilohertz range. Whereas oscillator circuits offered\nonly one or very few pulse shapes, modular power electronics solved a\nlong-standing problem for the first time and enabled arbitrary software-based\ndesign of the temporal shape of stimuli. However, synthesizing arbitrary\nstimuli with a high output quality requires a large number of modules. Systems\nwith few modules and pulse-width modulation may generate apparently smooth\ncurrent shapes in the highly inductive coil, but the stimulation effect of the\nneurons depends on the electric field and the electric field becomes a burst of\nultra-brief rectangular pulses. We propose an alternative solution that\nachieves high-resolution pulse shaping with fewer modules by implementing\nhigh-power wide-bandwidth voltage asymmetry. Rather than equal voltage steps,\nour system strategically assigns different voltages to each module to achieve a\nnear-exponential improvement in resolution. Compared to prior designs, our\nexperimental prototype achieved better output quality, although it uses only\nhalf the number of modules.",
      "generated_abstract": "n of magnetic resonance imaging (MRI) techniques for breast cancer\nis challenging due to the anatomical variability of the breast. We propose a\ncombination of anatomical-based segmentation and image registration techniques\nto improve MRI image quality for breast cancer. In this study, we used the\n3D-BREAST-CAD dataset for segmentation and the BreastCancer dataset for\nimage registration. We compared the performance of a CNN-based model with a\nmulti-scale convolutional neural network (MS-CNN) and a multi-scale\ndeformable-convolutional neural network (MS-DCNN) model. We used the\nCervicalCancer dataset for comparison. The CNN-based model outperformed the\nMS-DCNN model, achieving an average dice coefficient of 0.677 (0.619-0.718) for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09090909090909091,
          "p": 0.16923076923076924,
          "f": 0.1182795653457049
        },
        "rouge-2": {
          "r": 0.024096385542168676,
          "p": 0.0425531914893617,
          "f": 0.030769226152663417
        },
        "rouge-l": {
          "r": 0.08264462809917356,
          "p": 0.15384615384615385,
          "f": 0.10752687717366188
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2502.13785v2",
      "true_abstract": "mRNA-based vaccines have become a major focus in the pharmaceutical industry.\nThe coding sequence as well as the Untranslated Regions (UTRs) of an mRNA can\nstrongly influence translation efficiency, stability, degradation, and other\nfactors that collectively determine a vaccine's effectiveness. However,\noptimizing mRNA sequences for those properties remains a complex challenge.\nExisting deep learning models often focus solely on coding region optimization,\noverlooking the UTRs. We present Helix-mRNA, a structured state-space-based and\nattention hybrid model to address these challenges. In addition to a first\npre-training, a second pre-training stage allows us to specialise the model\nwith high-quality data. We employ single nucleotide tokenization of mRNA\nsequences with codon separation, ensuring prior biological and structural\ninformation from the original mRNA sequence is not lost. Our model, Helix-mRNA,\noutperforms existing methods in analysing both UTRs and coding region\nproperties. It can process sequences 6x longer than current approaches while\nusing only 10% of the parameters of existing foundation models. Its predictive\ncapabilities extend to all mRNA regions. We open-source the model\n(https://github.com/helicalAI/helical) and model weights\n(https://huggingface.co/helical-ai/helix-mRNA).",
      "generated_abstract": "r presents a novel method for the rapid identification of\nspecies within a large biological dataset. Our approach employs a novel\nconvolutional neural network (CNN) architecture that integrates sequence data\nand gene annotations. This methodology enables the rapid classification of\ngenes within a large dataset of microbial genomes. The methodology is based on\nthe use of the UTR (untranslated region) of the genome as a feature. By\nextracting the UTR from the sequences, it enables a rapid and accurate\nclassification of the genes. The methodology is applied to a dataset of\nmicrobial genomes from a wide range of ecological environments. The dataset\nincludes genomes from diverse organisms, including bacteria, archaea, fungi,\nplants, and animals. By integrating the UTR information, the methodology\nenables a rapid classification of the genes",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12307692307692308,
          "p": 0.22535211267605634,
          "f": 0.1592039755303088
        },
        "rouge-2": {
          "r": 0.01744186046511628,
          "p": 0.02857142857142857,
          "f": 0.02166064511201864
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.18309859154929578,
          "f": 0.1293532292616521
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.05908v1",
      "true_abstract": "We explain the fundamental challenges of sampling from multimodal\ndistributions, particularly for high-dimensional problems. We present the major\ntypes of MCMC algorithms that are designed for this purpose, including parallel\ntempering, mode jumping and Wang-Landau, as well as several state-of-the-art\napproaches that have recently been proposed. We demonstrate these methods using\nboth synthetic and real-world examples of multimodal distributions with\ndiscrete or continuous state spaces.",
      "generated_abstract": "We present a novel algorithm for the estimation of the maximum likelihood\n(ML) estimator of the posterior distribution of a model in a Gaussian process\n(GP) regression setting. The algorithm is based on a generalization of the\nmethod of moments for the GPs, where the GP prior is replaced by a GP likelihood\nand a Gaussian process prior is used for the GP posterior. The GP likelihood is\ndefined in terms of a finite sum of terms that are obtained by taking the\nderivatives of the objective function for the GP prior. The resulting GP likelihood\nis a weighted sum of basis functions. We prove that the GP likelihood is a\nconvex function, which is used to derive an iterative scheme for estimating the\nmaximum likelihood estimator. Numerical simulations are presented to demonstrate\nthe performance of the proposed algorithm and its extensions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.14285714285714285,
          "f": 0.15999999507200013
        },
        "rouge-2": {
          "r": 0.03125,
          "p": 0.017241379310344827,
          "f": 0.022222217639507116
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.14285714285714285,
          "f": 0.15999999507200013
        }
      }
    },
    {
      "paper_id": "math.GT.math/QA/2503.08861v1",
      "true_abstract": "We give invariants of flat bundles over 4-manifolds generalizing a result by\nChaidez, Cotler, and Cui (Alg. \\& Geo. Topology '22). We utilize a structure\ncalled a Hopf $G$-triplet for $G$ a group, which generalizes the notion of a\nHopf triplet by Chaidez, Cotler, and Cui. In our construction, we present flat\nbundles over 4-manifolds using colored trisection diagrams: a direct analogue\nof colored Heegaard diagrams as described by Virelizier. Our main result is\nthat involutory Hopf $G$-triplets of finite type yield well-defined invariants\nof $G$-colored trisection diagrams, and that if the monodromy of a flat bundle\nhas image in $G$ we obtain invariants of flat bundles. We also show that a\nspecial Hopf $G$-triplet yields the invariant from Hopf $G$-algebras described\nby Mochida, thus generalizing the construction.",
      "generated_abstract": "aper we investigate the homotopy type of the nerve of the category\nof $\\mathbb{Z}$-graded vector spaces with a distinguished triangle\n$\\mathcal{A}\\to\\mathcal{B}\\to\\mathcal{C}\\to0$ and the functor\n$\\mathcal{A}\\mapsto\\mathcal{A}^{\\otimes\\omega}$ for $\\omega\\in\\mathbb{Z}$.\nHere $\\mathcal{A}$ and $\\mathcal{B}$ are modules over a commutative ring\n$R$ with a $\\mathbb{Z}$-grading, and $\\mathcal{C}$ is a module over a\n$\\mathbb{Z}$-graded commutative $R$-module $\\mathcal{M}$. We obtain a\ndichotomy for the case $\\omega=1$, showing that the nerve of the category of\n$\\mathbb{Z}$-graded vector spaces with a distinguished triangle $\\mathcal{A}\\to\n\\mathcal{B}\\to\\",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1518987341772152,
          "p": 0.2608695652173913,
          "f": 0.1919999953484801
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12658227848101267,
          "p": 0.21739130434782608,
          "f": 0.15999999534848017
        }
      }
    },
    {
      "paper_id": "physics.flu-dyn.nlin/CD/2503.08983v1",
      "true_abstract": "We study decaying turbulence in the 1D Burgers equation (Burgulence) and 3D\nNavier-Stokes (NS) turbulence. We first investigate the decay in time $t$ of\nthe energy $E(t)$ in Burgulence, for a fractional Brownian initial potential,\nwith Hurst exponent $H$, and demonstrate rigorously a self-similar time-decay\nof $E(t)$, previously determined heuristically. This is a consequence of the\nnontrivial boundedness of the energy for any positive time. We define a\nspatially forgetful \\textit{oblivious fractional Brownian motion} (OFBM), with\nHurst exponent $H$, and prove that Burgulence, with an OFBM as initial\npotential $\\varphi_0(x)$, is not only intermittent, but it also displays, a\nhitherto unanticipated, large-scale bifractality or multifractality; the latter\noccurs if we combine OFBMs, with different values of $H$. This is the first\nrigorous proof of genuine multifractality for turbulence in a nonlinear\nhydrodynamical partial differential equation. We then present direct numerical\nsimulations (DNSs) of freely decaying turbulence, capturing some aspects of\nthis multifractality. For Burgulence, we investigate such decay for two cases:\n(A) $\\varphi_0(x)$ a multifractal random walk that crosses over to a fractional\nBrownian motion beyond a crossover scale $\\mathcal{L}$, tuned to go from small-\nto large-scale multifractality; (B) initial energy spectra $E_0(k)$, with\nwavenumber $k$, having one or more power-law regions, which lead, respectively,\nto self-similar and non-self-similar energy decay. Our analogous DNSs of the 3D\nNS equations also uncover self-similar and non-self-similar energy decay.\nChallenges confronting the detection of genuine large-scale multifractality, in\nnumerical and experimental studies of NS and MHD turbulence, are highlighted.",
      "generated_abstract": "ork, we investigate the effect of a time-dependent inertial\neffect on the vortex line dynamics. We perform a series of numerical\nsimulations using the Lattice Boltzmann Method (LBM) to explore the influence\nof the inertial term in the LBM on the vortex line dynamics. We consider two\ndifferent scenarios: (1) the inertial term is of second-order and (2) the\ninertial term is of third-order. We find that the inertial term of third-order\nimproves the numerical stability of the vortex line dynamics. The effect of\ninertial term on the vortex line dynamics is also investigated by performing\nthe vortex line dynamics simulations for a series of inertial terms. The\nresults suggest that the third-order inertial term has a greater influence on\nthe vortex line dynamics than the second-order inert",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.30357142857142855,
          "f": 0.16267942191433357
        },
        "rouge-2": {
          "r": 0.02643171806167401,
          "p": 0.06593406593406594,
          "f": 0.037735844971125004
        },
        "rouge-l": {
          "r": 0.10457516339869281,
          "p": 0.2857142857142857,
          "f": 0.15311004392390293
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.09383v2",
      "true_abstract": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality.",
      "generated_abstract": "y examines the relationship between the government's fiscal\nstrategy and the stock market performance in China. By employing a\nvector-autoregressive model, we investigate whether the government's fiscal\nstrategy can affect the stock market through the economic fundamentals. We\nfind that the government's fiscal strategy has a significant positive\neffect on the stock market, and the government's fiscal strategy is\nsignificantly associated with the stock market's return. In addition, the\nsignificant impact of fiscal policy on the stock market can be explained by\nthe government's economic fundamentals, such as economic growth, inflation,\nand employment. Furthermore, we also find that the stock market's return is\nsignificantly negatively related to the government's fiscal strategy.\nFurthermore, we find that the government's fiscal strategy has a significant\nnegative impact",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12121212121212122,
          "p": 0.3225806451612903,
          "f": 0.17621144977391381
        },
        "rouge-2": {
          "r": 0.009302325581395349,
          "p": 0.022988505747126436,
          "f": 0.01324502901079027
        },
        "rouge-l": {
          "r": 0.10909090909090909,
          "p": 0.2903225806451613,
          "f": 0.15859030439946448
        }
      }
    },
    {
      "paper_id": "cs.CE.econ/GN/2502.12966v1",
      "true_abstract": "Ethereum has adopted a rollup-centric roadmap to scale by making rollups\n(layer 2 scaling solutions) the primary method for handling transactions. The\nfirst significant step towards this goal was EIP-4844, which introduced blob\ntransactions that are designed to meet the data availability needs of layer 2\nprotocols. This work constitutes the first rigorous and comprehensive empirical\nanalysis of transaction- and mempool-level data since the institution of blobs\non Ethereum on March 13, 2024. We perform a longitudinal study of the early\ndays of the blob fee market analyzing the landscape and the behaviors of its\nparticipants. We identify and measure the inefficiencies arising out of\nsuboptimal block packing, showing that at times it has resulted in up to 70%\nrelative fee loss. We hone in and give further insight into two (congested)\npeak demand periods for blobs. Finally, we document a market design issue\nrelating to subset bidding due to the inflexibility of the transaction\nstructure on packing data as blobs and suggest possible ways to fix it. The\nlatter market structure issue also applies more generally for any discrete\nobjects included within transactions.",
      "generated_abstract": "r explores the impact of gender-based inequalities on the\ndynamics of economic growth and the distribution of income. We analyze the\nimpact of gender-based inequalities on the evolution of income inequality in\ndeveloping countries, focusing on the period between 1960 and 2019. We\nestimate a time-varying gravity model to analyze the impact of gender\ninequality on the evolution of the world economy. Our results show that\ngender-based inequalities have a negative effect on economic growth, with a\nsignificant negative effect on GDP per capita and a small positive effect on\nincome inequality. We also find that gender-based inequalities reduce the\nefficiency of economic growth, increasing the gap between the highest and\nlowest income groups. Our analysis also reveals that the effects of gender\ninequality on economic growth and inequality vary according to the type",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09302325581395349,
          "p": 0.18461538461538463,
          "f": 0.12371133575034558
        },
        "rouge-2": {
          "r": 0.016574585635359115,
          "p": 0.028846153846153848,
          "f": 0.021052626943922226
        },
        "rouge-l": {
          "r": 0.08527131782945736,
          "p": 0.16923076923076924,
          "f": 0.11340205739983014
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.16142v1",
      "true_abstract": "In this study, we investigate the integration of a large language model (LLM)\nwith an automatic speech recognition (ASR) system, specifically focusing on\nenhancing rare word recognition performance. Using a 190,000-hour dataset\nprimarily sourced from YouTube, pre-processed with Whisper V3 pseudo-labeling,\nwe demonstrate that the LLM-ASR architecture outperforms traditional\nZipformer-Transducer models in the zero-shot rare word recognition task, after\ntraining on a large dataset. Our analysis reveals that the LLM contributes\nsignificantly to improvements in rare word error rate (R-WER), while the speech\nencoder primarily determines overall transcription performance (Orthographic\nWord Error Rate, O-WER, and Normalized Word Error Rate, N-WER). Through\nextensive ablation studies, we highlight the importance of adapter integration\nin aligning speech encoder outputs with the LLM's linguistic capabilities.\nFurthermore, we emphasize the critical role of high-quality labeled data in\nachieving optimal performance. These findings provide valuable insights into\nthe synergy between LLM-based ASR architectures, paving the way for future\nadvancements in large-scale LLM-based speech recognition systems.",
      "generated_abstract": "years, transformer-based models have been widely adopted for\ncompetitive performance in multilingual text-to-speech (TTS) generation.\nHowever, existing models still suffer from various challenges, including\ndifficulty in handling language variation, limited speaker diversity, and\ndifficulty in generating high-quality speech. In this paper, we propose a\ngeneral-purpose multilingual TTS model, called Multilingual TTS (MTTs), which\nintegrates several multi-lingual models and employs a multi-task learning\nframework to effectively capture language variation. Specifically, we design a\nmulti-lingual speaker encoder based on a pre-trained speaker embedding model\nand a language encoder, respectively. The multi-lingual speaker encoder\ncaptures the speaker characteristics of each language, while the language\nencoder leverages the speaker embeddings to capture speaker characteristics\nac",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17543859649122806,
          "p": 0.25316455696202533,
          "f": 0.20725388117479673
        },
        "rouge-2": {
          "r": 0.020134228187919462,
          "p": 0.02857142857142857,
          "f": 0.023622042394135786
        },
        "rouge-l": {
          "r": 0.14035087719298245,
          "p": 0.20253164556962025,
          "f": 0.16580310397272424
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.02381v1",
      "true_abstract": "We propose a new approach to estimating the random coefficient logit demand\nmodel for differentiated products when the vector of market-product level\nshocks is sparse. Assuming sparsity, we establish nonparametric identification\nof the distribution of random coefficients and demand shocks under mild\nconditions. Then we develop a Bayesian procedure, which exploits the sparsity\nstructure using shrinkage priors, to conduct inference about the model\nparameters and counterfactual quantities. Comparing to the standard BLP (Berry,\nLevinsohn, & Pakes, 1995) method, our approach does not require demand\ninversion or instrumental variables (IVs), thus provides a compelling\nalternative when IVs are not available or their validity is questionable. Monte\nCarlo simulations validate our theoretical findings and demonstrate the\neffectiveness of our approach, while empirical applications reveal evidence of\nsparse demand shocks in well-known datasets.",
      "generated_abstract": "This paper investigates the relationship between the price of oil and the\nprice of gold, two commodities that have been heavily correlated over the\npast decade. We develop a novel time-varying stochastic volatility model that\naccounts for the impact of global events and geopolitical tensions on the\nprices of both commodities. We use a novel methodology to construct a\nconditional vector autoregressive model, which accounts for the dynamic\nrelationship between the two commodities. We find that the price of gold has a\nsignificant positive correlation with the price of oil, and that the price of\ngold is more sensitive to geopolitical events than the price of oil. Our\nfindings provide new insights into the relationship between the two commodities\nand offer valuable insights for investors and policymakers.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.19718309859154928,
          "f": 0.16568046850040277
        },
        "rouge-2": {
          "r": 0.0078125,
          "p": 0.009900990099009901,
          "f": 0.008733619523657938
        },
        "rouge-l": {
          "r": 0.1326530612244898,
          "p": 0.18309859154929578,
          "f": 0.1538461489737756
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.17271v1",
      "true_abstract": "In the context of scientific policy and science management, this study\nexamines the system of nonuniform wage distribution for researchers. A\nnonlinear mathematical model of optimal remuneration for scientific workers has\nbeen developed, considering key and additive aspects of scientific activity:\nbasic qualifications, research productivity, collaborative projects, skill\nenhancement, distinctions, and international collaborations. Unlike traditional\nlinear schemes, the proposed approach is based on exponential and logarithmic\ndependencies, allowing for the consideration of saturation effects and\npreventing artificial wage growth due to mechanical increases in scientific\nproductivity indicators.\n  The study includes detailed calculations of optimal, minimum, and maximum\nwages, demonstrating a fair distribution of remuneration on the basis of\nresearcher productivity. A linear increase in publication activity or grant\nfunding should not lead to uncontrolled salary growth, thus avoiding\ndistortions in the motivational system. The results of this study can be used\nto reform and modernize the wage system for researchers in Kazakhstan and other\ncountries, as well as to optimize grant-based science funding mechanisms. The\nproposed methodology fosters scientific motivation, long-term productivity, and\nthe internationalization of research while also promoting self-actualization\nand ultimately forming an adequate and authentic reward system for the research\ncommunity.\n  Specifically, in resource-limited scientific systems, science policy should\nfocus on the qualitative development of individual researchers rather than\nquantitative expansion (e.g., increasing the number of scientists). This can be\nachieved through the productive progress of their motivation and\nself-actualization.",
      "generated_abstract": "This paper introduces a novel approach to constructing a vector autoregressive\nmodel by integrating the idea of vectorization with the idea of joint\ndistributions. The method is based on the idea that the joint distribution of\nthe vector of exogenous variables and the vector of endogenous variables can be\nobtained by integrating the joint distribution of the vector of exogenous\nvariables and the vector of endogenous variables. The vector autoregressive\nmodel constructed in this way can be written as a vector of vector autoregressive\nmodels, which is an efficient and flexible way to construct a vector autoregressive\nmodel. Theoretical analysis and empirical applications are provided.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12418300653594772,
          "p": 0.36538461538461536,
          "f": 0.185365849872219
        },
        "rouge-2": {
          "r": 0.02643171806167401,
          "p": 0.08108108108108109,
          "f": 0.039867105926425006
        },
        "rouge-l": {
          "r": 0.12418300653594772,
          "p": 0.36538461538461536,
          "f": 0.185365849872219
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.14984v1",
      "true_abstract": "We examine the growing gender gap in venture capital funding, focusing on\naccelerator programs in the U.S. We collect a unique dataset with detailed\ninformation on accelerators and startups. Using a two-stage methodology, we\nfirst estimate a matching model between startups and accelerators, and then use\nits output to analyze the gender gap in post-graduation outcomes through a\ncontrol function approach. Our results show that female-founded startups face a\nsignificant funding disadvantage, primarily due to relocation challenges tied\nto family obligations. However, larger cohorts and higher-quality accelerators\nhelp reduce this gap by offering female founders better networking\nopportunities and mentorship.",
      "generated_abstract": "This paper examines the effect of taxation on the growth and stability of\nthe economy. We introduce a novel taxation framework that enables us to\nquantitatively model the effect of taxation on economic growth and stability.\nUsing this framework, we compute the optimal tax rate for each country,\nassessing its impact on economic growth and stability. Our findings show that\nthe optimal tax rate can have a significant impact on the economy, with\nsignificant implications for policy makers. The results highlight the\nimportance of considering the impact of taxation on economic growth and\nstability when making policy decisions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21518987341772153,
          "p": 0.2982456140350877,
          "f": 0.24999999513083918
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.02631578947368421,
          "f": 0.022988500827058785
        },
        "rouge-l": {
          "r": 0.17721518987341772,
          "p": 0.24561403508771928,
          "f": 0.2058823480720157
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.19006v2",
      "true_abstract": "In this note, I introduce a novel performance rating system called\nPerformance Rating Equilibrium (PRE). A PRE is a vector of hypothetical ratings\nfor each player, such that if these ratings were each player's initial rating\nat the start of a tournament, scoring the same points against the same\nopponents would leave each player's initial rating unchanged. In other words,\nall players' initial ratings perfectly predict their actual scores in the\ntournament. This property, however, does not hold for the well-known Tournament\nPerformance Rating. PRE is defined as a fixed point of a multidimensional\nrating function. I show that such a fixed point, and hence a PRE, exists under\nmild conditions. I provide an implementation of PRE along with several\nempirical applications. PREs have broad applicability, from sports competitions\nto the evaluation of large language models.",
      "generated_abstract": "uce a new class of stochastic control problems with state-dependent\ntargets, where the state is uncertain and the target may depend on it. We\nformulate a general class of stochastic control problems with state-dependent\ntargets, which includes the classical stochastic control problems with\nstate-dependent targets, as a special case. The general class includes\nreinforcement learning problems, where the agent learns to select actions with\nthe goal of maximizing the expected discounted sum of rewards over a finite\nhorizon. We show that the class includes all the classical stochastic control\nproblems with state-dependent targets, as well as all the stochastic control\nproblems with state-dependent targets defined by an optimal control problem\nwith a deterministic cost function. In addition, we show that the class includes\nall the stochastic control problems with state-dependent targets defined by a\ndeterministic cost function and an optimal control",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15306122448979592,
          "p": 0.23076923076923078,
          "f": 0.18404907495953945
        },
        "rouge-2": {
          "r": 0.015748031496062992,
          "p": 0.021505376344086023,
          "f": 0.01818181330124098
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.2153846153846154,
          "f": 0.17177913630923272
        }
      }
    },
    {
      "paper_id": "cs.GT.cs/GT/2503.07558v1",
      "true_abstract": "We introduce the first formal model capturing the elicitation of unverifiable\ninformation from a party (the \"source\") with implicit signals derived by other\nplayers (the \"observers\"). Our model is motivated in part by applications in\ndecentralized physical infrastructure networks (a.k.a. \"DePIN\"), an emerging\napplication domain in which physical services (e.g., sensor information,\nbandwidth, or energy) are provided at least in part by untrusted and\nself-interested parties. A key challenge in these signal network applications\nis verifying the level of service that was actually provided by network\nparticipants.\n  We first establish a condition called source identifiability, which we show\nis necessary for the existence of a mechanism for which truthful signal\nreporting is a strict equilibrium. For a converse, we build on techniques from\npeer prediction to show that in every signal network that satisfies the source\nidentifiability condition, there is in fact a strictly truthful mechanism,\nwhere truthful signal reporting gives strictly higher total expected payoff\nthan any less informative equilibrium. We furthermore show that this truthful\nequilibrium is in fact the unique equilibrium of the mechanism if there is\npositive probability that any one observer is unconditionally honest (e.g., if\nan observer were run by the network owner). Also, by extending our condition to\ncoalitions, we show that there are generally no collusion-resistant mechanisms\nin the settings that we consider.\n  We apply our framework and results to two DePIN applications: proving\nlocation, and proving bandwidth. In the location-proving setting observers\nlearn (potentially enlarged) Euclidean distances to the source. Here, our\ncondition has an appealing geometric interpretation, implying that the source's\nlocation can be truthfully elicited if and only if it is guaranteed to lie\ninside the convex hull of the observers.",
      "generated_abstract": "U.S. midterm elections are a critical test for U.S. democracy and\nthe U.S. political system. A key question is whether the U.S. political system\ncan withstand the demographic changes brought about by the demographic\ntransformation and the polarization of the political system, and whether the\nU.S. political system can survive the challenges posed by the polarization of\nthe political system. In this study, we study the U.S. midterm elections in\n2024 from the perspectives of the demographic transformation and the\npolarization of the political system. We conduct a survey analysis and\nquantitative analysis to explore the potential threats to U.S. democracy and\nthe U.S. political system from the demographic transformation and the\npolarization of the political system. Based on the survey analysis and\nquantitative analysis,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10982658959537572,
          "p": 0.35185185185185186,
          "f": 0.16740087743134943
        },
        "rouge-2": {
          "r": 0.01107011070110701,
          "p": 0.03896103896103896,
          "f": 0.017241375864216566
        },
        "rouge-l": {
          "r": 0.10982658959537572,
          "p": 0.35185185185185186,
          "f": 0.16740087743134943
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2411.04694v1",
      "true_abstract": "Introduction: Circadian rhythm disruption has garnered significant attention\nfor its adverse effects on human health, particularly in reproductive medicine\nand fetal well-being. Assessing pregnancy health often relies on diagnostic\nmarkers such as the labyrinth zone (LZ) proportion within the placenta. This\nstudy aimed to investigate the impact of disrupted circadian rhythms on\nplacental health and fetal development using animal models. Methods and\nResults: Employing unstained photo-acoustic microscopy (PAM) and hematoxylin\nand eosin (HE)-stained images, we found them mutually reinforcing. Our images\nrevealed the role of MCRD on the LZ and fetus weight: a decrease in LZ area\nfrom 5.01-HE(4.25-PAM) mm2 to 3.58-HE (2.62-PAM) mm2 on day 16 and\n6.48-HE(5.16-PAM) mm2 to 4.61-HE (3.03-PAM) mm2 on day 18, resulting in 0.71\ntimes lower fetus weights. We have discriminated a decrease in the mean LZ to\nplacenta area ratio from 64% to 47% on day 18 in mice with disrupted circadian\nrhythms with PAM. Discussion: The study highlights the negative influence of\ncircadian rhythm disruption on placental development and fetal well-being.\nReduced LZ area and fetal weights in the MCRD group suggest compromised\nplacental function under disrupted circadian rhythms. PAM imaging proved to be\nan efficient technique for assessing placental development, offering advantages\nover traditional staining methods. These findings contribute to understanding\nthe mechanisms underlying circadian disruption's effects on reproductive health\nand fetal development, emphasizing the importance of maintaining normal\ncircadian rhythms for optimal pregnancy outcomes. Further research is needed to\nexplore interventions to mitigate these effects and improve pregnancy outcomes.",
      "generated_abstract": "pt of the \"unconscious mind\" is based on the idea that the human\nbrain can perform calculations without conscious awareness. However, the\ninteraction between conscious and unconscious processes has been a topic of\ncontroversy. In this paper, we review the basic premises of the unconscious\nmind, the concept of \"psychic energy\", and the theory of the \"subconscious\nmemory\" developed by Carl Jung (1930). We then discuss the existence of a\nconscious mind and unconscious processes in general, as well as the existence\nof \"conscious energy\" and \"psychic energy\". We also briefly review the\ninteraction between conscious and unconscious processes, the existence of\n\"psychic energy\", and the theory of the \"subconscious memory\". Finally, we\ndiscuss the existence of \"subconscious processes\" and \"subconscious energy\".",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0736196319018405,
          "p": 0.1875,
          "f": 0.10572686819771406
        },
        "rouge-2": {
          "r": 0.008547008547008548,
          "p": 0.021052631578947368,
          "f": 0.012158050603746733
        },
        "rouge-l": {
          "r": 0.06134969325153374,
          "p": 0.15625,
          "f": 0.08810572282326476
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/OT/2406.10612v1",
      "true_abstract": "A key output of network meta-analysis (NMA) is the relative ranking of the\ntreatments; nevertheless, it has attracted a lot of criticism. This is mainly\ndue to the fact that ranking is an influential output and prone to\nover-interpretations even when relative effects imply small differences between\ntreatments. To date, common ranking methods rely on metrics that lack a\nstraightforward interpretation, while it is still unclear how to measure their\nuncertainty. We introduce a novel framework for estimating treatment\nhierarchies in NMA. At first, we formulate a mathematical expression that\ndefines a treatment choice criterion (TCC) based on clinically important\nvalues. This TCC is applied to the study treatment effects to generate paired\ndata indicating treatment preferences or ties. Then, we synthesize the paired\ndata across studies using an extension of the so-called \"Bradley-Terry\" model.\nWe assign to each treatment a latent variable interpreted as the treatment\n\"ability\" and we estimate the ability parameters within a regression model.\nHigher ability estimates correspond to higher positions in the final ranking.\nWe further extend our model to adjust for covariates that may affect treatment\nselection. We illustrate the proposed approach and compare it with alternatives\nin two datasets: a network comparing 18 antidepressants for major depression\nand a network comparing 6 antihypertensives for the incidence of diabetes. Our\napproach provides a robust and interpretable treatment hierarchy which accounts\nfor clinically important values and is presented alongside with uncertainty\nmeasures. Overall, the proposed framework offers a novel approach for ranking\nin NMA based on concrete criteria and preserves from over-interpretation of\nunimportant differences between treatments.",
      "generated_abstract": "This paper proposes a novel method to compute the posterior distribution of\nthe number of latent components $K$ in Gaussian Mixture Models (GMMs) using\nthe generalized inverse of a non-conjugate log-likelihood. We demonstrate the\nmethod's computational efficiency and efficiency of estimation, demonstrating\nits ability to compute the posterior distribution of $K$ when the model has\nhighly non-Gaussian components. The method also allows for the estimation of\nthe posterior distribution of $K$ directly from the observations, thus\nproviding a means for estimating the number of components without having to\nspecify the number of components in advance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11320754716981132,
          "p": 0.3157894736842105,
          "f": 0.1666666627816359
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.05063291139240506,
          "f": 0.024464828139794243
        },
        "rouge-l": {
          "r": 0.09433962264150944,
          "p": 0.2631578947368421,
          "f": 0.13888888500385813
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.04929v1",
      "true_abstract": "Planning and control for high-dimensional robot manipulators in cluttered,\ndynamic environments require both computational efficiency and robust safety\nguarantees. Inspired by recent advances in learning configuration-space\ndistance functions (CDFs) as robot body representations, we propose a unified\nframework for motion planning and control that formulates safety constraints as\nCDF barriers. A CDF barrier approximates the local free configuration space,\nsubstantially reducing the number of collision-checking operations during\nmotion planning. However, learning a CDF barrier with a neural network and\nrelying on online sensor observations introduce uncertainties that must be\nconsidered during control synthesis. To address this, we develop a\ndistributionally robust CDF barrier formulation for control that explicitly\naccounts for modeling errors and sensor noise without assuming a known\nunderlying distribution. Simulations and hardware experiments on a 6-DoF xArm\nmanipulator show that our neural CDF barrier formulation enables efficient\nplanning and robust real-time safe control in cluttered and dynamic\nenvironments, relying only on onboard point-cloud observations.",
      "generated_abstract": "e of Artificial Intelligence (AI), the ability to collaborate\nand cooperate with others has become essential for achieving sustainable\nsystems. However, collaborative decision-making in complex environments remains\nchallenging due to the complex interactions between multiple agents,\ninefficiencies in communication, and uncertainty in the system's state.\nExisting methods often rely on large models, leading to slow convergence,\ncomplex training procedures, and limited generalization. To address these\nchallenges, we propose a novel method that integrates two key concepts:\nCollaborative Partially Observable Markov Decision Processes (CPO-MDPs) and\nGraph Neural Networks (GNNs). CPO-MDPs enable the formation of collaborative\nagents that can communicate and collaborate with each other, while GNNs\nenhance the communication between agents and the environment. This approach",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.16304347826086957,
          "f": 0.15228425898116432
        },
        "rouge-2": {
          "r": 0.02054794520547945,
          "p": 0.02631578947368421,
          "f": 0.023076918152663772
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.16304347826086957,
          "f": 0.15228425898116432
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/DB/2503.10036v1",
      "true_abstract": "Concurrency control (CC) algorithms are important in modern transactional\ndatabases, as they enable high performance by executing transactions\nconcurrently while ensuring correctness. However, state-of-the-art CC\nalgorithms struggle to perform well across diverse workloads, and most do not\nconsider workload drifts.\n  In this paper, we propose CCaaLF (Concurrency Control as a Learnable\nFunction), a novel learned concurrency control algorithm designed to achieve\nhigh performance across varying workloads. The algorithm is quick to optimize,\nmaking it robust against dynamic workloads. CCaaLF learns an agent function\nthat captures a large number of design choices from existing CC algorithms. The\nfunction is implemented as an efficient in-database lookup table that maps\ndatabase states to concurrency control actions. The learning process is based\non a combination of Bayesian optimization and a novel graph reduction\nalgorithm, which converges quickly to a function that achieves high transaction\nthroughput. We compare CCaaLF against five state-of-the-art CC algorithms and\nshow that our algorithm consistently outperforms them in terms of transaction\nthroughput and optimization time.",
      "generated_abstract": "st decade, the popularity of Graph Neural Networks (GNNs) has\ndemonstrated the potential of leveraging graph data for machine learning\napplications. However, the challenges of dealing with large-scale,\nhigh-dimensional graphs have remained a major obstacle in the adoption of GNNs\nin industry. In this paper, we present a novel framework, called\nGraph-LSTM-GCN (G-LSTM-GCN), that introduces a Graph-LSTM layer in the\ntraditional Graph Convolutional Network (GCN) architecture for efficiently\nprocessing large-scale graph data. The G-LSTM-GCN framework is designed to\nperform both node and graph-level reasoning by leveraging the advantages of\nboth GCNs and LSTMs. We demonstrate the effectiveness of G-LSTM-GCN through\ncomprehensive experiments",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.25316455696202533,
          "f": 0.20618556218248496
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.06060606060606061,
          "f": 0.04705881877923924
        },
        "rouge-l": {
          "r": 0.16521739130434782,
          "p": 0.24050632911392406,
          "f": 0.19587628383196953
        }
      }
    },
    {
      "paper_id": "physics.acc-ph.physics/plasm-ph/2503.09557v1",
      "true_abstract": "We present a novel approach for generating collider-quality electron bunches\nusing a plasma photoinjector. The approach leverages recently developed\ntechniques for the spatiotemporal control of laser pulses to produce a moving\nionization front in a nonlinear plasma wave. The moving ionization front\ngenerates an electron bunch with a current profile that balances the\nlongitudinal electric field of an electron beam-driven plasma wave, creating a\nuniform accelerating field across the bunch. Particle-in-cell (PIC) simulations\nof the ionization stage show the formation of an electron bunch with 220 pC\ncharge and low emittance ($\\epsilon_x = 171$ nm-rad, $\\epsilon_y = 76$ nm-rad).\nQuasistatic PIC simulations of the acceleration stage show that the bunch is\nefficiently accelerated to 20 GeV over 2 meters with a final energy spread of\nless than 1\\% and emittances of $\\epsilon_x = 177$ nm-rad and $\\epsilon_y = 82$\nnm-rad. This high-quality electron bunch meets the requirements outlined by the\nSnowmass process for intermediate-energy colliders and compares favorably to\nthe beam quality of proposed and existing accelerator facilities. The results\nestablish the feasibility of plasma photoinjectors for future collider\napplications making a significant step towards the realization of\nhigh-luminosity, compact accelerators for particle physics research.",
      "generated_abstract": "imental study of ultrafast processes in plasmas is a key to\nunderstanding and controlling them. The use of femtosecond lasers to study\nultrafast processes in plasmas is nowadays a well-established method, which\nprovides a wealth of information about the dynamics of the plasma, including\nthe generation, propagation, and decay of fast particles. This review aims to\nsummarize the main experimental techniques used to study ultrafast processes in\nplasmas, including laser-driven electron and ion acceleration, electron\nand ion heating, and laser-driven plasma acceleration. The review also\nconsiders the most recent developments in the field, such as the use of\nultrafast laser pulses in plasmas, laser-driven electron and ion acceleration\nin plasmas, and laser-driven electron heating in plas",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11904761904761904,
          "p": 0.234375,
          "f": 0.15789473237451535
        },
        "rouge-2": {
          "r": 0.011111111111111112,
          "p": 0.020833333333333332,
          "f": 0.014492749086327823
        },
        "rouge-l": {
          "r": 0.10317460317460317,
          "p": 0.203125,
          "f": 0.13684210079556802
        }
      }
    },
    {
      "paper_id": "astro-ph.HE.astro-ph/HE/2503.09562v1",
      "true_abstract": "The prompt emission of Gamma-Ray Bursts (GRBs) could be composed of different\nspectral components, such as a dominant non-thermal Band component in the\nkeV-MeV range, a subdominant quasi-thermal component, and an additional hard\nnon-thermal component extending into the GeV range. The existence and\nevolutionary behaviors of these components could place strong implication on\nphysical models, such as ejecta composition and dissipation processes. Although\nnumerous GRBs have been found to exhibit one or two spectral components,\nreports of GRBs containing all three components remain rare. In this letter,\nbased on the joint observations of GRB 240825A from multiple gamma-ray\ntelescopes, we conduct a comprehensive temporal and spectral analysis to\nidentify the presence and evolution of all three components. The bulk Lorentz\nfactor of this bright and relatively short-duration burst is independently\ncalculated using the thermal and hard non-thermal components, supporting a jet\npenetration scenario. The multi-segment broken powerlaw feature observed in the\nflux light curves suggests the presence of an early afterglow in the keV-MeV\nband and hints at a possible two-jet structure. Furthermore, the observed\ntransition from positive to negative on the spectral lag can be interpreted as\nan independent evolution of the soft and hard components, leading to\nmisalignment in the cross-correlation function (CCF) analysis of pulses.",
      "generated_abstract": "t the results of a detailed analysis of the X-ray emission from\nthe young star-forming region G39.5-0.1 in the Large Magellanic Cloud (LMC). The\nobservations were carried out with the {\\it XMM-Newton} space telescope in\n2023-2024, in the {\\it XMM-Newton} Advanced Analysis and Discovery Tool\n(XADDT) mode, with the {\\it XMM-Newton} EPIC-pn camera. We analyzed 31\nobservational periods of X-ray light curves obtained over a period of 6.2\nhours. We used the {\\tt XSPEC} package for the analysis of the data. We\ndiscussed the results in the context of the general theory of X-ray emission,\nusing the X-ray spectrum and the spectral parameters. In particular, we",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11851851851851852,
          "p": 0.2222222222222222,
          "f": 0.15458936744381446
        },
        "rouge-2": {
          "r": 0.030927835051546393,
          "p": 0.06315789473684211,
          "f": 0.041522486936220106
        },
        "rouge-l": {
          "r": 0.0962962962962963,
          "p": 0.18055555555555555,
          "f": 0.1256038601974377
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.17072v2",
      "true_abstract": "General Equilibrium Theory is the benchmark of economics, especially its\nresults concerning the efficient allocation of resources, known as the First\nand Second Welfare Theorems. Yet, General Equilibrium Theory is beyond the\nscope of most economists. This paper is pitched as the first entry point into\nthe theory. General Equilibrium Theory proves that at least one state of\nequilibrium always exists. In its most general approach, it uses fixed-point\ntheorems to this end. This paper discusses the assumptions on individuals'\nbehaviour and the structure of the system of exchange that guarantee that the\nconditions of the fixed-point theorems are satisfied. The purpose is to lay\nbare the role each plays in proving the existence of equilibrium and provide a\nclear picture of the relationship between the assumptions and the result. The\ndiscussion is presented in the simplest possible setting that captures the\nfundamental features of commodity exchange.",
      "generated_abstract": "the problem of allocating scarce resources among multiple\nequally desirable goods, where each good has an additional quality $Q$, and\neach individual is given a preference $x_i$ over the $Q$ goods. The\nrevenue-maximizing allocation is a single good, but the revenue from each\nindividual's single-good allocation is $x_i$ times the revenue from the\nsingle-good allocation of another individual. We show that the revenue-maximizing\nallocation is a single-good allocation that is also revenue-equivalent to the\nmaximum of the revenue-equivalent allocations of all other individuals. We\nshow that the revenue-equivalent allocation is a single-good allocation that\nmaximizes the sum of the revenue from the single-good allocations of all\nindividuals, and the revenue-equivalent allocation is a single-good allocation\nthat",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10869565217391304,
          "p": 0.18181818181818182,
          "f": 0.1360544170854738
        },
        "rouge-2": {
          "r": 0.03007518796992481,
          "p": 0.047619047619047616,
          "f": 0.03686635470194799
        },
        "rouge-l": {
          "r": 0.09782608695652174,
          "p": 0.16363636363636364,
          "f": 0.1224489749086031
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.02075v1",
      "true_abstract": "Aligning a lens system relative to an imager is a critical challenge in\ncamera manufacturing. While optimal alignment can be mathematically computed\nunder ideal conditions, real-world deviations caused by manufacturing\ntolerances often render this approach impractical. Measuring these tolerances\ncan be costly or even infeasible, and neglecting them may result in suboptimal\nalignments. We propose a reinforcement learning (RL) approach that learns\nexclusively in the pixel space of the sensor output, eliminating the need to\ndevelop expert-designed alignment concepts. We conduct an extensive benchmark\nstudy and show that our approach surpasses other methods in speed, precision,\nand robustness. We further introduce relign, a realistic, freely explorable,\nopen-source simulation utilizing physically based rendering that models optical\nsystems with non-deterministic manufacturing tolerances and noise in robotic\nalignment movement. It provides an interface to popular machine learning\nframeworks, enabling seamless experimentation and development. Our work\nhighlights the potential of RL in a manufacturing environment to enhance\nefficiency of optical alignments while minimizing the need for manual\nintervention.",
      "generated_abstract": "This paper presents a method for generating collision-free trajectories for\nsubsystems composed of a robotic arm and a mobile manipulator. The proposed\napproach uses a graph-based method to identify the optimal collision-free\ntrajectory of the subsystem. The graph-based method is capable of handling\nlarge-scale and complex collision-free trajectories. To generate collision-free\ntrajectories, the proposed method uses the graph-based method to identify\ncollision-free trajectories for the robotic arm and the mobile manipulator.\nThe graph-based method is capable of handling large-scale and complex\ncollision-free trajectories. To generate collision-free trajectories, the\nproposed method uses the graph-based method to identify collision-free trajectories\nfor the robotic arm and the mobile manipulator. The results of the proposed\nmethod are demonstrated in simulation and validated through experiments.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09917355371900827,
          "p": 0.2727272727272727,
          "f": 0.14545454154343446
        },
        "rouge-2": {
          "r": 0.00625,
          "p": 0.014285714285714285,
          "f": 0.008695647939510567
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.25,
          "f": 0.13333332942222234
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.00450v1",
      "true_abstract": "We propose and study three confidence intervals (CIs) centered at an\nestimator that is intentionally biased to reduce mean squared error. The first\nCI simply uses an unbiased estimator's standard error; compared to centering at\nthe unbiased estimator, this CI has higher coverage probability for confidence\nlevels above 91.7%, even if the biased and unbiased estimators have equal mean\nsquared error. The second CI trades some of this \"excess\" coverage for shorter\nlength. The third CI is centered at a convex combination of the two estimators\nto further reduce length. Practically, these CIs apply broadly and are simple\nto compute.",
      "generated_abstract": "In this paper, we extend the Cox-Ingersoll-Ross (CIR) model by introducing\na non-stationary component in the model. The model is based on a generalized\nCIR (GCIR) model with non-stationary coefficients and non-stationary regressors.\nWe propose an efficient estimation and inference method for the GCIR model\nusing a sequential Monte Carlo (SMC) approach. We evaluate the finite-sample\nperformance of our proposed method in terms of the log-likelihood ratio and\nempirical coverage probabilities. We demonstrate the superiority of our\nproposed method in terms of finite-sample performance compared to existing\nmethods in the literature.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.23333333333333334,
          "f": 0.21538461041420132
        },
        "rouge-2": {
          "r": 0.03125,
          "p": 0.036585365853658534,
          "f": 0.033707860199470495
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.23333333333333334,
          "f": 0.21538461041420132
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2410.03976v1",
      "true_abstract": "Asynchronous Boolean networks are a type of discrete dynamical system in\nwhich each variable can take one of two states, and a single variable state is\nupdated in each time step according to pre-selected rules. Boolean networks are\npopular in systems biology due to their ability to model long-term biological\nphenotypes within a qualitative, predictive framework. Boolean networks model\nphenotypes as attractors, which are closely linked to minimal trap spaces\n(inescapable hypercubes in the system's state space). In biological\napplications, attractors and minimal trap spaces are typically in one-to-one\ncorrespondence. However, this correspondence is not guaranteed: motif-avoidant\nattractors (MAAs) that lie outside minimal trap spaces are possible.\n  MAAs are rare and (despite recent efforts) poorly understood. Here we\nsummarize the current state of knowledge regarding MAAs and present several\nnovel observations regarding their response to node deletion reductions and\nlinear extensions of edges. We conduct large-scale computational studies on an\nensemble of 14,000 models derived from published Boolean models of biological\nsystems, and more than 100 million Random Boolean Networks. Our findings\nquantify the rarity of MAAs (in particular, we found no MAAs in the biological\nmodels), but highlight the role of network reduction in introducing MAAs into\nthe dynamics. We also show that MAAs are fragile to linear extensions: in\nsparse networks, even a single linear node can disrupt virtually all MAAs.\nMotivated by this observation, we improve the upper bound on the number of\ndelays needed to disrupt a motif-avoidant attractor.",
      "generated_abstract": "advancement of high-throughput sequencing technologies has\nprompted an explosion in the production of microbial genomes and the\ndevelopment of bioinformatics tools that facilitate their analysis. The\nproliferation of data has also challenged the ability of traditional statistical\nmethods to handle the increasing complexity. In this work, we present a\nnovel approach to analyze microbial genomic data. The approach integrates\nhigh-throughput sequencing data with statistical methods to address the\ncomplexity of data. Our method relies on the combination of machine learning\nalgorithms and Bayesian inference to identify biological relationships between\ngenes. We applied our approach to the genome of the soil bacterium\nDeinococcus radiodurans, which was used as a model organism for radiation\nresistance. Our results show that the combined analysis of genome-wide\nsequencing data and Bayesian",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1568627450980392,
          "p": 0.2857142857142857,
          "f": 0.20253164099343063
        },
        "rouge-2": {
          "r": 0.012987012987012988,
          "p": 0.025423728813559324,
          "f": 0.017191972601539738
        },
        "rouge-l": {
          "r": 0.13725490196078433,
          "p": 0.25,
          "f": 0.17721518529722813
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2412.17354v3",
      "true_abstract": "In this study, we introduce a novel methodological framework called Bayesian\nPenalized Empirical Likelihood (BPEL), designed to address the computational\nchallenges inherent in empirical likelihood (EL) approaches. Our approach has\ntwo primary objectives: (i) to enhance the inherent flexibility of EL in\naccommodating diverse model conditions, and (ii) to facilitate the use of\nwell-established Markov Chain Monte Carlo (MCMC) sampling schemes as a\nconvenient alternative to the complex optimization typically required for\nstatistical inference using EL. To achieve the first objective, we propose a\npenalized approach that regularizes the Lagrange multipliers, significantly\nreducing the dimensionality of the problem while accommodating a comprehensive\nset of model conditions. For the second objective, our study designs and\nthoroughly investigates two popular sampling schemes within the BPEL context.\nWe demonstrate that the BPEL framework is highly flexible and efficient,\nenhancing the adaptability and practicality of EL methods. Our study highlights\nthe practical advantages of using sampling techniques over traditional\noptimization methods for EL problems, showing rapid convergence to the global\noptima of posterior distributions and ensuring the effective resolution of\ncomplex statistical inference challenges.",
      "generated_abstract": "This paper presents a novel method for the analysis of panel data in\nmechanism design problems. We propose a novel, flexible estimator for the\nexpected payoff function under a given mechanism. Our estimator is based on the\nwell-known method of moments, and we derive a closed-form expression for the\nestimator. Additionally, we derive a bootstrap-based estimator for the\nexpected payoff function. We demonstrate the performance of our estimators in\nsimulation studies and an empirical application to the design of incentives\nfor the treatment of breast cancer patients.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14049586776859505,
          "p": 0.3148148148148148,
          "f": 0.19428571001861233
        },
        "rouge-2": {
          "r": 0.022988505747126436,
          "p": 0.05263157894736842,
          "f": 0.03199999576832056
        },
        "rouge-l": {
          "r": 0.14049586776859505,
          "p": 0.3148148148148148,
          "f": 0.19428571001861233
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.12024v1",
      "true_abstract": "Mean field equilibrium (MFE) has emerged as a computationally tractable\nsolution concept for large dynamic games. However, computing MFE remains\nchallenging due to nonlinearities and the absence of contraction properties,\nlimiting its reliability for counterfactual analysis and comparative statics.\nThis paper focuses on MFE in dynamic models where agents interact through a\nscalar function of the population distribution, referred to as the\n\\textit{scalar interaction function}. Such models naturally arise in a wide\nrange of applications in operations and economics, including quality ladder\nmodels, inventory competition, online marketplaces, and heterogeneous-agent\nmacroeconomic models. The main contribution of this paper is to introduce\niterative algorithms that leverage the scalar interaction structure and are\nguaranteed to converge to the MFE under mild assumptions. Unlike existing\napproaches, our algorithms do not rely on monotonicity or contraction\nproperties, significantly broadening their applicability. Furthermore, we\nprovide a model-free algorithm that learns the MFE by employing simulation and\nreinforcement learning techniques such as Q-learning and policy gradient\nmethods without requiring prior knowledge of payoff or transition functions. We\nestablish finite-time performance bounds for this algorithm under technical\nLipschitz continuity assumptions. We apply our algorithms to classic models of\ndynamic competition, such as capacity competition, and to competitive models\nmotivated by online marketplaces, including ridesharing, dynamic reputation,\nand inventory competition, as well as to social learning models. Using our\nalgorithms, we derive reliable comparative statics results that illustrate how\nkey market parameters influence equilibrium outcomes in these stylized models,\nproviding insights that could inform the design of competitive systems in these\ncontexts.",
      "generated_abstract": "er a two-sided market with a single buyer and a single seller. The\nbuyer is required to pay a fee to the seller to use the market. We show that if\nthe buyer pays a non-negative fee to the seller, the seller's price is\noptimal if and only if the buyer's fee is an additive function of the seller's\nprice. We also show that if the buyer pays zero fee to the seller, the seller's\nprice is optimal if and only if the buyer's fee is a non-negative linear\ncombination of the seller's price and a constant. Finally, we show that if the\nbuyer pays a non-negative fee to the seller, the seller's price is optimal if\nand only if the buyer's fee is a non-negative linear combination of the seller's",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07272727272727272,
          "p": 0.3,
          "f": 0.1170731675907199
        },
        "rouge-2": {
          "r": 0.012295081967213115,
          "p": 0.04477611940298507,
          "f": 0.019292601121163527
        },
        "rouge-l": {
          "r": 0.07272727272727272,
          "p": 0.3,
          "f": 0.1170731675907199
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.15655v1",
      "true_abstract": "We study the local geometry of empirical risks in high dimensions via the\nspectral theory of their Hessian and information matrices. We focus on settings\nwhere the data, $(Y_\\ell)_{\\ell =1}^n\\in \\mathbb R^d$, are i.i.d. draws of a\n$k$-component Gaussian mixture model, and the loss depends on the projection of\nthe data into a fixed number of vectors, namely $\\mathbf{x}^\\top Y$, where\n$\\mathbf{x}\\in \\mathbb{R}^{d\\times C}$ are the parameters, and $C$ need not\nequal $k$. This setting captures a broad class of problems such as\nclassification by one and two-layer networks and regression on multi-index\nmodels. We prove exact formulas for the limits of the empirical spectral\ndistribution and outlier eigenvalues and eigenvectors of such matrices in the\nproportional asymptotics limit, where the number of samples and dimension\n$n,d\\to\\infty$ and $n/d=\\phi \\in (0,\\infty)$. These limits depend on the\nparameters $\\mathbf{x}$ only through the summary statistic of the $(C+k)\\times\n(C+k)$ Gram matrix of the parameters and class means, $\\mathbf{G} =\n(\\mathbf{x},\\mathbf{\\mu})^\\top(\\mathbf{x},\\mathbf{\\mu})$. It is known that\nunder general conditions, when $\\mathbf{x}$ is trained by stochastic gradient\ndescent, the evolution of these same summary statistics along training\nconverges to the solution of an autonomous system of ODEs, called the effective\ndynamics. This enables us to connect the spectral theory to the training\ndynamics. We demonstrate our general results by analyzing the effective\nspectrum along the effective dynamics in the case of multi-class logistic\nregression. In this setting, the empirical Hessian and information matrices\nhave substantially different spectra, each with their own static and even\ndynamical spectral transitions.",
      "generated_abstract": "We present a method for the estimation of the spectral density of a\nprocess with the goal of recovering its mean and variance. This estimator is\nintended to be used in applications where the mean and variance of the process\nare unknown. The estimator is based on the estimation of the mean and variance\nof a Gamma process. The Gamma process has been shown to be well suited for\nestimating the spectral density of complex-valued random processes. We derive\nthe asymptotic properties of the proposed estimator and show that it converges\nin probability to a Gamma process. We also prove that the proposed estimator\nachieves a $\\sqrt{n}$-consistency rate. Finally, we present simulations to\ndemonstrate the efficiency of the proposed estimator.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11875,
          "p": 0.3064516129032258,
          "f": 0.17117116714552402
        },
        "rouge-2": {
          "r": 0.02553191489361702,
          "p": 0.06382978723404255,
          "f": 0.03647416005210641
        },
        "rouge-l": {
          "r": 0.11875,
          "p": 0.3064516129032258,
          "f": 0.17117116714552402
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2503.07811v2",
      "true_abstract": "The theory of optimal transportation has developed into a powerful and\nelegant framework for comparing probability distributions, with wide-ranging\napplications in all areas of science. The fundamental idea of analyzing\nprobabilities by comparing their underlying state space naturally aligns with\nthe core idea of causal inference, where understanding and quantifying\ncounterfactual states is paramount. Despite this intuitive connection, explicit\nresearch at the intersection of optimal transport and causal inference is only\nbeginning to develop. Yet, many foundational models in causal inference have\nimplicitly relied on optimal transport principles for decades, without\nrecognizing the underlying connection. Therefore, the goal of this review is to\noffer an introduction to the surprisingly deep existing connections between\noptimal transport and the identification of causal effects with observational\ndata -- where optimal transport is not just a set of potential tools, but\nactually builds the foundation of model assumptions. As a result, this review\nis intended to unify the language and notation between different areas of\nstatistics, mathematics, and econometrics, by pointing out these existing\nconnections, and to explore novel problems and directions for future work in\nboth areas derived from this realization.",
      "generated_abstract": "We propose a novel method to estimate and test the causal effect of an\ntarget variable on a response variable under the presence of an unobserved\nconfounder. We construct a new estimator of the causal effect of the target\nvariable on the response variable by combining the estimator for the causal\neffect of the unobserved confounder on the response variable with the\nestimator for the causal effect of the target variable on the unobserved\nconfounder. We provide theoretical properties of our estimator and demonstrate\nits efficiency through simulations. We further apply our method to estimate and\ntest the causal effect of the number of children on the number of siblings\nobserved in the same family, and we demonstrate its robustness in the presence\nof confounding.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10483870967741936,
          "p": 0.26,
          "f": 0.14942528326066862
        },
        "rouge-2": {
          "r": 0.005681818181818182,
          "p": 0.01282051282051282,
          "f": 0.007874011492345285
        },
        "rouge-l": {
          "r": 0.08870967741935484,
          "p": 0.22,
          "f": 0.1264367775135422
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/DC/2503.08976v1",
      "true_abstract": "Federated Ranking Learning (FRL) is a state-of-the-art FL framework that\nstands out for its communication efficiency and resilience to poisoning\nattacks. It diverges from the traditional FL framework in two ways: 1) it\nleverages discrete rankings instead of gradient updates, significantly reducing\ncommunication costs and limiting the potential space for malicious updates, and\n2) it uses majority voting on the server side to establish the global ranking,\nensuring that individual updates have minimal influence since each client\ncontributes only a single vote. These features enhance the system's scalability\nand position FRL as a promising paradigm for FL training.\n  However, our analysis reveals that FRL is not inherently robust, as certain\nedges are particularly vulnerable to poisoning attacks. Through a theoretical\ninvestigation, we prove the existence of these vulnerable edges and establish a\nlower bound and an upper bound for identifying them in each layer. Based on\nthis finding, we introduce a novel local model poisoning attack against FRL,\nnamely the Vulnerable Edge Manipulation (VEM) attack. The VEM attack focuses on\nidentifying and perturbing the most vulnerable edges in each layer and\nleveraging an optimization-based approach to maximize the attack's impact.\nThrough extensive experiments on benchmark datasets, we demonstrate that our\nattack achieves an overall 53.23% attack impact and is 3.7x more impactful than\nexisting methods. Our findings highlight significant vulnerabilities in\nranking-based FL systems and underline the urgency for the development of new\nrobust FL frameworks.",
      "generated_abstract": "guage models (LLMs) are widely used in numerous applications,\nincluding chatbots, text-to-speech (TTS), and natural language generation\n(NLG). However, their performance on multilingual text can be limited by the\nlimited availability of multilingual resources. Recently, transformer-based\nmodels have shown great potential in multilingual text processing, but their\nperformance on multilingual text is still limited by the limited availability\nof multilingual resources. In this paper, we propose a novel multilingual\ntransformer-based model named Multilingual Transformer Language Model (MTLM).\nMTLM leverages a multilingual transformer model, which is pre-trained on\nmultilingual resources, to enhance the performance on the target language,\nwhile preserving the performance on the source language. Additionally, we\npropose a multilingual training strategy for M",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.26666666666666666,
          "f": 0.17021276161158908
        },
        "rouge-2": {
          "r": 0.012987012987012988,
          "p": 0.03225806451612903,
          "f": 0.018518514425583894
        },
        "rouge-l": {
          "r": 0.11875,
          "p": 0.25333333333333335,
          "f": 0.16170212331371675
        }
      }
    },
    {
      "paper_id": "math.NT.cs/DS/2503.10158v1",
      "true_abstract": "Integral linear systems $Ax=b$ with matrices $A$, $b$ and solutions $x$ are\nalso required to be in integers, can be solved using invariant factors of $A$\n(by computing the Smith Canonical Form of $A$). This paper explores a new\nproblem which arises in applications, that of obtaining conditions for solving\nthe Modular Linear System $Ax=b\\rem n$ given $A,b$ in $\\zz_n$ for $x$ in\n$\\zz_n$ along with the constraint that the value of the linear function\n$\\phi(x)=<w,x>$ is coprime to $n$ for some solution $x$. In this paper we\ndevelop decomposition of the system to coprime moduli $p^{r(p)}$ which are\ndivisors of $n$ and show how such a decomposition simplifies the computation of\nSmith form. This extends the well known index calculus method of computing the\ndiscrete logarithm where the moduli over which the linear system is reduced\nwere assumed to be prime (to solve the reduced systems over prime fields) to\nthe case when the factors of the modulus are prime powers $p^{r(p)}$. It is\nshown how this problem can be addressed effciently using the invariant factors\nand Smith form of the augmented matrix $[A,-p^{r(p)}I]$ and conditions modulo\n$p$ satisfied by $w$, where $p^{r(p)}$ vary over all divisors of $n$ with $p$\nprime.",
      "generated_abstract": "In the last few years, the study of the behavior of random walks on\ngraphs has gained a lot of interest. A central topic in this area is the\ndetermination of the average time required for the first exit of the walk from\na set of vertices. This paper provides a unified approach to study this\nproblem and we present an algorithm to determine the asymptotic behavior of the\naverage time for the first exit of a random walk on a graph when the number of\nvertices tends to infinity. We also present an algorithm to compute the first\nexit time of a random walk on a finite graph for some specific cases of the\ngraph.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15789473684210525,
          "p": 0.29508196721311475,
          "f": 0.20571428117289803
        },
        "rouge-2": {
          "r": 0.026455026455026454,
          "p": 0.053763440860215055,
          "f": 0.035460988487249684
        },
        "rouge-l": {
          "r": 0.14912280701754385,
          "p": 0.2786885245901639,
          "f": 0.1942857097443266
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.15346v1",
      "true_abstract": "Drug discovery remains a slow and expensive process that involves many steps,\nfrom detecting the target structure to obtaining approval from the Food and\nDrug Administration (FDA), and is often riddled with safety concerns. Accurate\nprediction of how drugs interact with their targets and the development of new\ndrugs by using better methods and technologies have immense potential to speed\nup this process, ultimately leading to faster delivery of life-saving\nmedications. Traditional methods used for drug-target interaction prediction\nshow limitations, particularly in capturing complex relationships between drugs\nand their targets. As an outcome, deep learning models have been presented to\novercome the challenges of interaction prediction through their precise and\nefficient end results. By outlining promising research avenues and models, each\nwith a different solution but similar to the problem, this paper aims to give\nresearchers a better idea of methods for even more accurate and efficient\nprediction of drug-target interaction, ultimately accelerating the development\nof more effective drugs. A total of 180 prediction methods for drug-target\ninteractions were analyzed throughout the period spanning 2016 to 2025 using\ndifferent frameworks based on machine learning, mainly deep learning and graph\nneural networks. Additionally, this paper discusses the novelty, architecture,\nand input representation of these models.",
      "generated_abstract": "y investigates the role of the prefrontal cortex (PFC) in the\ncognitive control of self-relevant information and emotional responses to\ndifferent stimuli in the context of a social game. We hypothesize that\nexecutive control processes in the PFC are associated with cognitive control\nmechanisms that reduce the influence of self-relevant information on emotional\nresponses, thereby enhancing social cognition. To test this hypothesis, we\nexperimentally manipulated the PFC's functioning by increasing or decreasing\nthe number of available tasks. In the context of a social game, the\nexecutive control network of the PFC was activated by increasing the number of\navailable tasks, which reduced the influence of self-relevant information on\nemotional responses. In contrast, decreasing the number of available tasks\nactivated the executive control network of the PFC, which enhanced",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08888888888888889,
          "p": 0.1791044776119403,
          "f": 0.1188118767547301
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08888888888888889,
          "p": 0.1791044776119403,
          "f": 0.1188118767547301
        }
      }
    },
    {
      "paper_id": "cs.AI.nlin/CD/2503.09858v1",
      "true_abstract": "This paper investigates the complex interplay between AI developers,\nregulators, users, and the media in fostering trustworthy AI systems. Using\nevolutionary game theory and large language models (LLMs), we model the\nstrategic interactions among these actors under different regulatory regimes.\nThe research explores two key mechanisms for achieving responsible governance,\nsafe AI development and adoption of safe AI: incentivising effective regulation\nthrough media reporting, and conditioning user trust on commentariats'\nrecommendation. The findings highlight the crucial role of the media in\nproviding information to users, potentially acting as a form of \"soft\"\nregulation by investigating developers or regulators, as a substitute to\ninstitutional AI regulation (which is still absent in many regions). Both\ngame-theoretic analysis and LLM-based simulations reveal conditions under which\neffective regulation and trustworthy AI development emerge, emphasising the\nimportance of considering the influence of different regulatory regimes from an\nevolutionary game-theoretic perspective. The study concludes that effective\ngovernance requires managing incentives and costs for high quality\ncommentaries.",
      "generated_abstract": "r studies the problem of estimating the mean and variance of a\nregression function from a finite number of data points. The regression function\nis assumed to be defined on a compact interval, and the data points are\nassumed to be distributed uniformly within this interval. We consider a\nnonparametric approach, where the regression function is estimated by\nmaximizing a nonparametric estimator of the expectation of the regression\nfunction. The estimation process is described by a stochastic differential\nequation. We provide necessary and sufficient conditions for the uniqueness of\nthe solution to this stochastic differential equation. We also show that the\nestimator is asymptotically efficient, and that its variance is asymptotically\nas small as the variance of the regression function. We establish the\nasymptotic optimality of the estimator in the regime of large sample sizes. We\nalso discuss the asymptotic optimality of the estimator in the regime of small",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14035087719298245,
          "p": 0.2222222222222222,
          "f": 0.17204300600763106
        },
        "rouge-2": {
          "r": 0.013245033112582781,
          "p": 0.017094017094017096,
          "f": 0.014925368214804474
        },
        "rouge-l": {
          "r": 0.12280701754385964,
          "p": 0.19444444444444445,
          "f": 0.15053762966354506
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06070v1",
      "true_abstract": "This paper bridges optimization and control, and presents a novel closed-loop\ncontrol framework based on natural gradient descent, offering a\ntrajectory-oriented alternative to traditional cost-function tuning. By\nleveraging the Fisher Information Matrix, we formulate a preconditioned\ngradient descent update that explicitly shapes system trajectories. We show\nthat, in sharp contrast to traditional controllers, our approach provides\nflexibility to shape the system's low-level behavior. To this end, the proposed\nmethod parameterizes closed-loop dynamics in terms of stationary covariance and\nan unknown cost function, providing a geometric interpretation of control\nadjustments. We establish theoretical stability conditions. The simulation\nresults on a rotary inverted pendulum benchmark highlight the advantages of\nnatural gradient descent in trajectory shaping.",
      "generated_abstract": "aper, we consider a large-scale multi-agent system, where agents\nare connected to a shared communication network and transmit their messages\nover a shared frequency band. Each agent is equipped with a radio frequency\ntransmitter, and its communication channel is characterized by the channel\ngain $K \\in \\mathbb{C}^{N \\times N}$, where $N$ is the number of agents. The\nshared frequency band is divided into $M$ sub-bands, and each agent's transmission\npower is limited to a given constraint $\\alpha \\in \\mathbb{R}^{N}$. In\nparticular, the objective is to maximize the total sum rate of all agents,\nwhile satisfying the power constraint on each agent. We propose a\ntwo-stage algorithm for solving this problem, where the first stage aims to\nidentify the optimal transmission power allocation, while the second stage\noptimizes the channel gain and the sub-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11235955056179775,
          "p": 0.11627906976744186,
          "f": 0.1142857092871839
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10112359550561797,
          "p": 0.10465116279069768,
          "f": 0.1028571378586125
        }
      }
    },
    {
      "paper_id": "cs.CG.cs/CG/2503.08863v1",
      "true_abstract": "We study three fundamental three-dimensional (3D) geometric packing problems:\n3D (Geometric) Bin Packing (3D-BP), 3D Strip Packing (3D-SP), and Minimum\nVolume Bounding Box (3D-MVBB), where given a set of 3D (rectangular) cuboids,\nthe goal is to find an axis-aligned nonoverlapping packing of all cuboids. In\n3D-BP, we need to pack the given cuboids into the minimum number of unit cube\nbins. In 3D-SP, we need to pack them into a 3D cuboid with a unit square base\nand minimum height. Finally, in 3D-MVBB, the goal is to pack into a cuboid box\nof minimum volume.\n  It is NP-hard to even decide whether a set of rectangles can be packed into a\nunit square bin -- giving an (absolute) approximation hardness of 2 for 3D-BP\nand 3D-SP. The previous best (absolute) approximation for all three problems is\nby Li and Cheng (SICOMP, 1990), who gave algorithms with approximation ratios\nof 13, $46/7$, and $46/7+\\varepsilon$, respectively, for 3D-BP, 3D-SP, and\n3D-MVBB. We provide improved approximation ratios of 6, 6, and $3+\\varepsilon$,\nrespectively, for the three problems, for any constant $\\varepsilon > 0$.\n  For 3D-BP, in the asymptotic regime, Bansal, Correa, Kenyon, and Sviridenko\n(Math.~Oper.~Res., 2006) showed that there is no asymptotic polynomial-time\napproximation scheme (APTAS) even when all items have the same height. Caprara\n(Math.~Oper.~Res., 2008) gave an asymptotic approximation ratio of\n$T_{\\infty}^2 + \\varepsilon\\approx 2.86$, where $T_{\\infty}$ is the well-known\nHarmonic constant in Bin Packing. We provide an algorithm with an improved\nasymptotic approximation ratio of $3 T_{\\infty}/2 +\\varepsilon \\approx 2.54$.\nFurther, we show that unlike 3D-BP (and 3D-SP), 3D-MVBB admits an APTAS.",
      "generated_abstract": "This paper addresses the problem of assigning a fixed number of vertices to\nedge-colored graphs. We propose a new class of randomized algorithms that\nachieve near-linear time in the number of vertices while preserving the\nsublinear-time bound on the number of edges. Our algorithms use a\ncombinatorial representation of the graph that makes them particularly\nefficient for handling large graphs. Additionally, we show that these\nalgorithms are polynomial-time approximable, which implies the near-linear\ntime bound on the number of vertices. Our algorithms also provide a\nconstructive construction of the class of randomized algorithms. We show that\nthe randomized algorithms can be constructed by first drawing a random graph\nfrom a certain class of random graphs and then constructing the randomized\nalgorithm from the resulting random graph. This construction is polynomial in\nthe number of vertices, making it a natural class of randomized algorithms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13548387096774195,
          "p": 0.27631578947368424,
          "f": 0.18181817740297235
        },
        "rouge-2": {
          "r": 0.020491803278688523,
          "p": 0.04310344827586207,
          "f": 0.02777777340987723
        },
        "rouge-l": {
          "r": 0.11612903225806452,
          "p": 0.23684210526315788,
          "f": 0.15584415142894636
        }
      }
    },
    {
      "paper_id": "physics.geo-ph.physics/geo-ph/2503.06490v1",
      "true_abstract": "Seismic data acquisition is often affected by various types of noise, which\ndegrade data quality and hinder subsequent interpretation. Recovery of seismic\ndata becomes particularly challenging in the presence of strong noise, which\nsignificantly impacts both data accuracy and geological analysis. This study\nproposes a novel single-encoder, multiple-decoder network based on Nash\nequalization (SEMD-Nash) for effective strong noise attenuation in seismic\ndata. The main contributions of this method are as follows: First, we design a\nshared encoder-multi-decoder architecture, where an improved encoder extracts\nkey features from the noisy data, and three parallel decoders reconstruct the\ndenoised seismic signal from different perspectives. Second, we develop a\nmulti-objective optimization system that integrates three loss functions-Mean\nSquared Error (MSE), Perceived Loss, and Structural Similarity Index (SSIM)-to\nensure effective signal reconstruction, high-order feature preservation, and\nstructural integrity. Third, we introduce the Nash Equalization Weight\nOptimizer, which dynamically adjusts the weights of the loss functions,\nbalancing the optimization objectives to improve the models robustness and\ngeneralization. Experimental results demonstrate that the proposed method\neffectively suppresses strong noise while preserving the geological\ncharacteristics of the seismic data.",
      "generated_abstract": "play of the Earth's mantle, crust, and upper mantle is governed\nby the Earth's crustal structure. The Earth's crust has a thickness of about\n20 km, and its density is about 2.8 g/cm3. It consists of a lower mantle of\nabout 100 km thickness and an upper mantle of about 400 km thickness. The upper\nmantle is believed to be a solid, but its structure remains poorly understood.\nHowever, the crustal structure, in particular, its thickness, has been\nestablished through geophysical methods. The current study is to investigate\nthe interplay of the crustal structure with the upper mantle, using a\ntwo-dimensional (2D) finite-difference model. The model is constructed based on\nthe crustal thickness derived from seism",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10606060606060606,
          "p": 0.20588235294117646,
          "f": 0.13999999551200015
        },
        "rouge-2": {
          "r": 0.011428571428571429,
          "p": 0.019230769230769232,
          "f": 0.014336912886526621
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.17647058823529413,
          "f": 0.11999999551200018
        }
      }
    },
    {
      "paper_id": "cs.NI.math/NA/2503.09869v1",
      "true_abstract": "A well-known expression for the saturation throughput of heterogeneous\ntransmitting nodes in a wireless network using p-CSMA, derived from Renewal\nTheory, implicitly assumes that all transmitting nodes are in range of, and\ntherefore conflicting with, each other. This expression, as well as simple\nmodifications of it, does not correctly capture the saturation throughput\nvalues when an arbitrary topology is specified for the conflict graph between\ntransmitting links. For example, we show numerically that calculations based on\nrenewal theory can underestimate throughput by 48-62% for large packet sizes\nwhen the conflict graph is represented by a star topology. This is problematic\nbecause real-world wireless networks, such as wireless IoT mesh networks, are\noften deployed over a large area, resulting in non-complete conflict graphs.\n  To address this gap, we present a computational approach based on a novel\nMarkov chain formulation that yields the exact saturation throughput for each\nnode in the general network case for any given set of access probabilities, as\nwell as a more compact expression for the special case where the packet length\nis twice the slot length. Using our approach, we show how the transmit\nprobabilities could be optimized to maximize weighted utility functions of the\nsaturation throughput values. This would allow a wireless system designer to\nset transmit probabilities to achieve desired throughput trade-offs in any\ngiven deployment.",
      "generated_abstract": "The concept of a model-based control system is useful for designing control\nsystems that can adapt to changing conditions. In this paper, we propose an\napproach for designing a model-based control system by using a neural network\nto predict the system's response to disturbances. Our approach uses a\nfeed-forward neural network to predict the system's response to disturbances\nand to compute the control input. The neural network is trained using\ntraining data collected from the system, which is then applied to the system.\nThe neural network is retrained using the system's response to disturbances\ncollected after the system has been operating for a while. This process is\nrepeated until the neural network converges. We demonstrate the effectiveness\nof our approach by applying it to a system with a known response and then to a\nsystem with unknown responses.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14388489208633093,
          "p": 0.2857142857142857,
          "f": 0.19138755535358634
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1366906474820144,
          "p": 0.2714285714285714,
          "f": 0.18181817736315575
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.12141v2",
      "true_abstract": "The FAO-GAEZ crop productivity data are widely used in Economics. However,\nthe existence of measurement error is rarely recognized in the empirical\nliterature. We propose a novel method to partially identify the effect of\nagricultural productivity, deriving bounds that allow for nonclassical\nmeasurement error by leveraging two proxies. These bounds exhaust all the\ninformation contained in the first two moments of the data. We reevaluate three\ninfluential studies, documenting that measurement error matters and that the\nimpact of agricultural productivity on economic outcomes may be smaller than\npreviously reported. Our methodology has broad applications in empirical\nresearch involving mismeasured variables.",
      "generated_abstract": "ean Union's (EU) Strategic Framework for Research and Innovation\n(SFRi) 2021-2027 aims to make the EU a global leader in research and\ninnovation. To achieve this, the framework aims to improve the effectiveness of\nresearch and innovation (R&I) activities, enhance the impact of R&I, and\nfacilitate a more innovative and sustainable European economy. The framework\nalso aims to promote international collaboration and coordination among EU\nmember states, as well as with non-EU countries. This paper presents an overview\nof the EU's SFRi and its implementation through the Horizon Europe\nprogram. Additionally, it discusses the challenges and opportunities faced by\nthe European R&I system in the context of the framework. Finally, it\nhighlights the importance of international collaboration for the future",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14666666666666667,
          "p": 0.13580246913580246,
          "f": 0.14102563603303764
        },
        "rouge-2": {
          "r": 0.042105263157894736,
          "p": 0.03571428571428571,
          "f": 0.038647338028892794
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.12345679012345678,
          "f": 0.12820512321252486
        }
      }
    },
    {
      "paper_id": "cs.CL.cs/CL/2503.10515v1",
      "true_abstract": "Discourse understanding is essential for many NLP tasks, yet most existing\nwork remains constrained by framework-dependent discourse representations. This\nwork investigates whether large language models (LLMs) capture discourse\nknowledge that generalizes across languages and frameworks. We address this\nquestion along two dimensions: (1) developing a unified discourse relation\nlabel set to facilitate cross-lingual and cross-framework discourse analysis,\nand (2) probing LLMs to assess whether they encode generalizable discourse\nabstractions. Using multilingual discourse relation classification as a\ntestbed, we examine a comprehensive set of 23 LLMs of varying sizes and\nmultilingual capabilities. Our results show that LLMs, especially those with\nmultilingual training corpora, can generalize discourse information across\nlanguages and frameworks. Further layer-wise analyses reveal that language\ngeneralization at the discourse level is most salient in the intermediate\nlayers. Lastly, our error analysis provides an account of challenging relation\nclasses.",
      "generated_abstract": "r presents a new text-to-speech (TTS) system that can generate\nlanguage-specific speech with a diverse range of accents. The system is\ntrained using a large dataset of voice recordings. The system uses a combination\nof text-to-speech (TTS) models and voice conversion models to produce speech\nwith a diverse range of accents. The system is able to generate speech with\ndifferent accents, such as British, American, and Australian, as well as\nstandard English. The system also produces speech with varying voice qualities,\nsuch as nasal, nasal-low, and neutral. The system also produces speech with\ndifferent accents, such as British, American, and Australian, as well as\nstandard English. The system also produces speech with varying voice qualities,\nsuch as nasal, nasal-low, and neutral. The system is also able to produce\nspeech with",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11320754716981132,
          "p": 0.24,
          "f": 0.15384614949046693
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11320754716981132,
          "p": 0.24,
          "f": 0.15384614949046693
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.03131v1",
      "true_abstract": "There is growing recognition in both the experimental and modelling\nliterature of the importance of spatial structure to the dynamics of viral\ninfections in tissues. Aided by the evolution of computing power and motivated\nby recent biological insights, there has been an explosion of new,\nspatially-explicit models for within-host viral dynamics in recent years. This\ndevelopment has only been accelerated in the wake of the COVID-19 pandemic.\nSpatially-structured models offer improved biological realism and can account\nfor dynamics which cannot be well-described by conventional, mean-field\napproaches. However, despite their growing popularity, spatially-structured\nmodels of viral dynamics are underused in biological applications. One major\nobstacle to the wider application of such models is the huge variety in\napproaches taken, with little consensus as to which features should be included\nand how they should be implemented for a given biological context. Previous\nreviews of the field have focused on specific modelling frameworks or on models\nfor particular viral species. Here, we instead apply a scoping review approach\nto the literature of spatially-structured viral dynamics models as a whole to\nprovide an exhaustive update of the state of the field. Our analysis is\nstructured along two axes, methodology and viral species, in order to examine\nthe breadth of techniques used and the requirements of different biological\napplications. We then discuss the contributions of mathematical and\ncomputational modelling to our understanding of key spatially-structured\naspects of viral dynamics, and suggest key themes for future model development\nto improve robustness and biological utility.",
      "generated_abstract": "opment of artificial intelligence (AI) is accelerating at a\nrapid rate, with significant advancements in neuro-symbolic AI (NSA) and\nneuro-plastic AI (NPA), which rely on the neural representations of the brain\nto solve complex problems. In this work, we explore the integration of\nNSA/NPA into brain-inspired deep learning (BIDL) for a more robust\ncomputational model of the brain. We introduce the BIDL-NSA/NPA architecture,\nwhich combines the BIDL architecture with the NSA/NPA paradigm. The BIDL-NSA/NPA\narchitecture is trained on the BIDL dataset, which contains 110,870\nmulti-modal brain images and 46,698 multi-modal neuro-symbolic representations\nof brain. The results demonstrate that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10273972602739725,
          "p": 0.22058823529411764,
          "f": 0.14018691155210075
        },
        "rouge-2": {
          "r": 0.008620689655172414,
          "p": 0.021505376344086023,
          "f": 0.012307688222297214
        },
        "rouge-l": {
          "r": 0.0958904109589041,
          "p": 0.20588235294117646,
          "f": 0.1308411171595774
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06411v1",
      "true_abstract": "This paper examines the intricate interplay among AI safety, security, and\ngovernance by integrating technical systems engineering with principles of\nmoral imagination and ethical philosophy. Drawing on foundational insights from\nWeapons of Math Destruction and Thinking in Systems alongside contemporary\ndebates in AI ethics, we develop a comprehensive multi-dimensional framework\ndesigned to regulate AI technologies deployed in high-stakes domains such as\ndefense, finance, healthcare, and education. Our approach combines rigorous\ntechnical analysis, quantitative risk assessment, and normative evaluation to\nexpose systemic vulnerabilities inherent in opaque, black-box models. Detailed\ncase studies, including analyses of Microsoft Tay (2016) and the UK A-Level\nGrading Algorithm (2020), demonstrate how security lapses, bias amplification,\nand lack of accountability can precipitate cascading failures that undermine\npublic trust. We conclude by outlining targeted strategies for enhancing AI\nresilience through adaptive regulatory mechanisms, robust security protocols,\nand interdisciplinary oversight, thereby advancing the state of the art in\nethical and technical AI governance.",
      "generated_abstract": "asing demand for renewable energy sources, particularly solar energy,\nhas led to the development of advanced hybrid power systems that combine\nconventional fossil-fueled power plants with renewable energy sources.\nTraditional optimization methods are unable to address the high-dimensional\nconstraints of these systems. In this work, we propose an integrated\noptimization framework that combines a Genetic Algorithm (GA) with a\nMultiobjective Genetic Algorithm (MOGA) to address the challenges of power\nsystems with renewable energy integration. We develop a multi-objective\nconstrained optimization model that incorporates renewable energy integration\nconstraints, such as the grid-connected solar PV inverter, energy storage\nsystem, and battery energy storage system, as well as the power system's\nconventional power generation constraints. The proposed framework is\noptimized using the MOGA algorithm, which is a genetic algorithm that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13709677419354838,
          "p": 0.20987654320987653,
          "f": 0.1658536537565736
        },
        "rouge-2": {
          "r": 0.013071895424836602,
          "p": 0.017543859649122806,
          "f": 0.014981268514919746
        },
        "rouge-l": {
          "r": 0.13709677419354838,
          "p": 0.20987654320987653,
          "f": 0.1658536537565736
        }
      }
    },
    {
      "paper_id": "cs.IT.math/IT/2503.08451v1",
      "true_abstract": "Early neural channel coding approaches leveraged dense neural networks with\none-hot encodings to design adaptive encoder-decoder pairs, improving block\nerror rate (BLER) and automating the design process. However, these methods\nstruggled with scalability as the size of message sets and block lengths\nincreased. TurboAE addressed this challenge by focusing on bit-sequence inputs\nrather than symbol-level representations, transforming the scalability issue\nassociated with large message sets into a sequence modeling problem. While\nrecurrent neural networks (RNNs) were a natural fit for sequence processing,\ntheir reliance on sequential computations made them computationally expensive\nand inefficient for long sequences. As a result, TurboAE adopted convolutional\nnetwork blocks, which were faster to train and more scalable, but lacked the\nsequential modeling advantages of RNNs. Recent advances in efficient RNN\narchitectures, such as minGRU and minLSTM, and structured state space models\n(SSMs) like S4 and S6, overcome these limitations by significantly reducing\nmemory and computational overhead. These models enable scalable sequence\nprocessing, making RNNs competitive for long-sequence tasks. In this work, we\nrevisit RNNs for Turbo autoencoders by integrating the lightweight minGRU model\nwith a Mamba block from SSMs into a parallel Turbo autoencoder framework. Our\nresults demonstrate that this hybrid design matches the performance of\nconvolutional network-based Turbo autoencoder approaches for short sequences\nwhile significantly improving scalability and training efficiency for long\nblock lengths. This highlights the potential of efficient RNNs in advancing\nneural channel coding for long-sequence scenarios.",
      "generated_abstract": "r presents a novel multi-carrier transmission system for\ncommunication over a linearly polarized multistatic aerial system. The\ntransmitter is equipped with an arbitrary number of antennas, each of which\nmay be linearly polarized. The received signals are composed of multiple\ncarriers, which are all combined through a single linear combiner. We prove\nthat the optimal linear polarization strategy is to transmit on all the\npossible linearly polarized subcarriers. This is in contrast to the traditional\nlinear polarization strategy, in which the transmitter first selects one\nlinearly polarized subcarrier and then transmits on all the remaining\nsubcarriers. Furthermore, we analyze the achievable rate region of the\nmulti-carrier system and show that the achievable rate region is the union of\nthe rate regions of the individual subcarriers. We then present a method for\ndesigning the transmitter's polar",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08974358974358974,
          "p": 0.175,
          "f": 0.11864406331513949
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08333333333333333,
          "p": 0.1625,
          "f": 0.11016948704395307
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2501.16522v1",
      "true_abstract": "With limited resources, competition is widespread, yet cooperation persists\nacross taxa, from microorganisms to large mammals. Recent observations reveal\ncontingent factors often drive cooperative interactions, with the intensity\nheterogeneously distributed within species. While cooperation has beneficial\noutcomes, it may also incur significant costs, largely depending on species\ndensity. This creates a dilemma that is pivotal in shaping sustainable\ncooperation strategies. Understanding how cooperation intensity governs the\ncost-benefit balance, and whether an optimal strategy exists for species\nsurvival, is a fundamental question in ecological research, and the focus of\nthis study. We develop a novel mathematical model within the Lotka-Volterra\nframework to explore the dynamics of cost-associated partial cooperation, which\nremains relatively unexplored in ODE model-based studies. Our findings\ndemonstrate that partial cooperation benefits ecosystems up to a certain\nintensity, beyond which costs become dominant, leading to system collapse via\nheteroclinic bifurcation. This outcome captures the cost-cooperation dilemma,\nproviding insights for adopting sustainable strategies and resource management\nfor species survival. We propose a novel mathematical approach to detect and\ntrack heteroclinic orbits in predator-prey systems. Moreover, we show that\nintroducing fear of predation can protect the regime shift, even with a type-I\nfunctional response, challenging traditional ecological views. Although fear is\nknown to resolve the \"paradox of enrichment,\" our results suggest that certain\nlevels of partial cooperation can reestablish this dynamic even at higher fear\nintensity. Finally, we validate the system's dynamical robustness across\nfunctional responses through structural sensitivity analysis.",
      "generated_abstract": "of the physical properties of cells and tissues is essential for\nstudying their interactions with the environment, their response to drugs and\ntherapeutic agents, and their response to various stressors, such as hypoxia\nor metabolic perturbations. In this article, we introduce a new framework to\nstudy the physics of cells and tissues, which includes the physical properties\nof cells and tissues, and their interactions with the environment. This new\nframework is based on the modeling of the physical properties of cells and tissues\nas a system of stochastic differential equations, and the coupling between\ncellular dynamics and the environment as a stochastic differential equation.\nThe modeling of the physical properties of cells and tissues as stochastic\ndifferential equations is crucial for understanding the interactions between\ncellular dynamics and the environment, as well as the response of cells to\ndrug treatments, metabolic perturb",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09941520467836257,
          "p": 0.2698412698412698,
          "f": 0.14529914136423416
        },
        "rouge-2": {
          "r": 0.01282051282051282,
          "p": 0.030303030303030304,
          "f": 0.0180180138397867
        },
        "rouge-l": {
          "r": 0.08771929824561403,
          "p": 0.23809523809523808,
          "f": 0.12820512427021707
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.08991v1",
      "true_abstract": "We prove that cw-hyperbolic homeomorphisms with jointly continuous\nstable/unstable holonomies satisfy the periodic shadowing property and, if they\nare topologically mixing, the periodic specification property. We discuss\ndifficulties to adapt Bowen's techniques to obtain a measure of maximal entropy\nfor cw-hyperbolic homeomorphisms, exhibit the unique measure of maximal entropy\nfor Walter's pseudo-Anosov diffeomorphism of $\\mathbb{S}^2$, and prove it can\nbe obtained, as in the expansive case, as the weak* limit of an average of\nDirac measures on periodic orbits. As an application, we exhibit the unique\nmeasure of maximal entropy for the homeomorphism on the Sierpi\\'nski Carpet\ndefined in [12], which does not satisfy the specification property.",
      "generated_abstract": "igate the structure of the $k$-th symmetric products $S_n^k$ for\n$n \\geq 3$ and $k \\geq 1$. For $n=3$ we obtain a complete description of the\nsymmetric products, which consists of a single element. In general, the\nsymmetric products are not irreducible, and we show that they are not\nconjugate to the symmetric group. We also investigate the structure of the\n$k$-th powers of the symmetric products. For $k=1$ we obtain a complete\ndescription of the $k$-th powers. For $k=2$ we obtain a complete description\nof the $k$-th powers for $n=4$ and $n=5$, and we show that they are not\nconjugate to the symmetric group. For $k=3$ we obtain a complete description\nof the $k$-th powers for $n=",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.2916666666666667,
          "f": 0.2372881307670211
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.028985507246376812,
          "f": 0.02484471559893619
        },
        "rouge-l": {
          "r": 0.17142857142857143,
          "p": 0.25,
          "f": 0.20338982568227532
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2407.21190v2",
      "true_abstract": "Predictive values are measures of the clinical accuracy of a binary\ndiagnostic test, and depend on the sensitivity and the specificity of the test\nand on the disease prevalence among the population being studied. This article\nstudies hypothesis tests to simultaneously compare the predictive values of two\nbinary diagnostic tests in the presence of missing data. The hypothesis tests\nwere solved applying two computational methods: the EM and SEM algorithms and\nmultiple imputation. Simulation experiments were carried out to study the sizes\nand the power of the hypothesis tests, giving some general rules of\napplication. Two R programmes were written to apply each method, and they are\navailable as supplementary material for the manuscript. The results were\napplied to the diagnosis of Alzheimer's disease.",
      "generated_abstract": "aimed to investigate the relationship between the 2019-2020\nH1N1 seasonal influenza and the 2019-2020 COVID-19 pandemic in China. The\nstudy utilized a mixed-methods approach, including descriptive statistics,\nfactor analysis, principal components analysis, and multiple regression\nanalysis. Results showed that the H1N1 seasonal influenza was positively\nassociated with COVID-19 infections, while the COVID-19 pandemic was negatively\nassociated with the H1N1 seasonal influenza. The results also revealed that\nfemale and young people were more susceptible to COVID-19 infection. The\nfindings of this study highlight the complex and interrelated nature of the\ninfluenza and COVID-19 pandemics, offering valuable insights into the\nperspectives of public health",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1375,
          "p": 0.16666666666666666,
          "f": 0.15068492655282434
        },
        "rouge-2": {
          "r": 0.03418803418803419,
          "p": 0.043478260869565216,
          "f": 0.038277507033264525
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.15151515151515152,
          "f": 0.13698629641583807
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.00564v1",
      "true_abstract": "For a many-to-one market with substitutable preferences on the firm's side,\nbased on the Aizerman-Malishevski decomposition, we define an associated\none-to-one market. Given that the usual notion of stability for a one-to-one\nmarket does not fit well for this associated one-to-one market, we introduce a\nnew notion of stability. This notion allows us to establish an isomorphism\nbetween the set of stable matchings in the many-to-one market and the matchings\nin the associated one-to-one market that meet this new stability criterion.\nFurthermore, we present an adaptation of the well-known deferred acceptance\nalgorithm to compute a matching that satisfies this new notion of stability for\nthe associated one-to-one market.",
      "generated_abstract": "igate the role of public good provision in a model of political\ndemocracy, where citizens can vote for public goods and public goods can be\nprovided by the government. We first examine the optimal level of public goods\nprovided by the government, under the assumption that citizens are rational\nutility maximisers. We show that citizens can be more or less rational\ndepending on the distribution of the public goods received by the government.\nSecond, we investigate the impact of public good provision on political\ninvolvement. We show that, under certain assumptions, a higher level of\npublic goods provision leads to higher political involvement. We also show that\nthe optimal level of public goods provision depends on the distribution of\npublic goods received by the government. Finally, we investigate the\nimpact of political involvement on the optimal level of public goods provision.\nWe find that the optimal level of public goods provision depends on the\ndistribution of public goods received by the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1724137931034483,
          "p": 0.1694915254237288,
          "f": 0.17094016594053635
        },
        "rouge-2": {
          "r": 0.033707865168539325,
          "p": 0.031914893617021274,
          "f": 0.03278688024963498
        },
        "rouge-l": {
          "r": 0.15517241379310345,
          "p": 0.15254237288135594,
          "f": 0.1538461488465193
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.00254v1",
      "true_abstract": "Growth curve analysis (GCA) has a wide range of applications in various\nfields where growth trajectories need to be modeled. Heteroscedasticity is\noften present in the error term, which can not be handled with sufficient\nflexibility by standard linear fixed or mixed-effects models. One situation\nthat has been addressed is where the error variance is characterized by a\nlinear predictor with certain covariates. A frequently encountered scenario in\nGCA, however, is one in which the variance is a smooth function of the mean\nwith known shape restrictions. A naive application of standard linear\nmixed-effects models would underestimate the variance of the fixed effects\nestimators and, consequently, the uncertainty of the estimated growth curve. We\npropose to model the variance of the response variable as a shape-restricted\n(increasing/decreasing; convex/concave) function of the marginal or conditional\nmean using shape-restricted splines. A simple iteratively reweighted fitting\nalgorithm that takes advantage of existing software for linear mixed-effects\nmodels is developed. For inference, a parametric bootstrap procedure is\nrecommended. Our simulation study shows that the proposed method gives\nsatisfactory inference with moderate sample sizes. The utility of the method is\ndemonstrated using two real-world applications.",
      "generated_abstract": "em of predicting the mean of a stochastic process is of central\ninterest in many applications. A popular approach to this problem is the\nMonte Carlo method, which estimates the mean from a sample of random values\ndrawn from a distribution. The accuracy of the method is determined by the\nsampling distribution, and the choice of the sampling distribution is often\nbased on the assumptions of a certain theory of stochastic processes. In this\npaper, we consider the problem of predicting the mean of a stochastic process\nthat follows a Gaussian distribution. We propose a general method of predicting\nthe mean of a stochastic process by a finite sum of Gaussian random variables\ndrawn from a given distribution. The method is based on a new concept of\npredictor. We also consider the case when the mean is unknown. Our results show\nthat the mean of a stochastic process can be estimated from a finite sum of\nGaussian random variables, using only",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2066115702479339,
          "p": 0.3246753246753247,
          "f": 0.2525252477721662
        },
        "rouge-2": {
          "r": 0.04597701149425287,
          "p": 0.06504065040650407,
          "f": 0.05387204901948825
        },
        "rouge-l": {
          "r": 0.2066115702479339,
          "p": 0.3246753246753247,
          "f": 0.2525252477721662
        }
      }
    },
    {
      "paper_id": "math.GT.math/GT/2503.05151v1",
      "true_abstract": "In a previous note, it is claimed that every surface-link consisting of\ntrivial components and having at most one non-sphere component is a ribbon\nsurface-link, but it was false. In this revised note, this claim is replaced by\nthe claim that a surface-link $L$ with trivial components is a ribbon\nsurface-link if and only if the surface-link obtained from $L$ by every fusion\nis a ribbon surface-link if and only if the surface-link obtained from $L$ by\nany one fusion is a ribbon surface-link. For any closed oriented disconnected\nsurface ${\\mathbf F}$ containing at least two non-sphere components, there is a\npair of a ribbon ${\\mathbf F}$-link $L$ consisting of trivial components and a\nnon-ribbon ${\\mathbf F}$-link $L'$ consisting of trivial components such that\nthe fundamental groups of $L$ and $L'$ are the same group up to\nmeridian-preserving isomorphisms and the pair of the ${\\mathbf F}'$-links $K$\nand $K'$ obtained from $L$ and $L'$ by every corresponding fusion is a pair of\na ribbon surface-link and a non-ribbon surface-link such that the fundamental\ngroups of $K$ and $K'$ are the same group up to meridian-preserving\nisomorphisms.",
      "generated_abstract": "We study the moduli of stable maps from a projective line to a surface of\ncurve number $g \\geq 3$. In this case, the moduli spaces of stable maps are\nhomeomorphic to the moduli space of stable curves of genus $g$, and the stable\ncurves are parametrized by the space of stable maps. We give a new proof of\nMatsumoto's theorem for stable curves. In addition, we give a new proof of\nMatsumoto's theorem for stable maps using the theory of $\\text{GL}(2)$\nrepresentations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15714285714285714,
          "p": 0.2558139534883721,
          "f": 0.19469026077218277
        },
        "rouge-2": {
          "r": 0.023809523809523808,
          "p": 0.047619047619047616,
          "f": 0.031746027301587924
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.23255813953488372,
          "f": 0.17699114572793495
        }
      }
    },
    {
      "paper_id": "hep-ph.nucl-ex/2503.07055v1",
      "true_abstract": "The valence quark parton distribution functions (PDFs) of all ground state\nheavy mesons that composed of $b$ or $c$ quarks, are discussed; namely, the\npseudoscalar $\\eta_c(1S)$, $\\eta_b(1S)$ and $B_c$, together with the\ncorresponding vector ones, $J/\\psi$, $\\Upsilon(1S)$ and $B_c^\\ast$. We use a\nQCD-inspired constituent quark model, which has been applied with success to\nconventional heavy mesons, so that one advantage here is that all parameters\nhave already been fixed by previous studies. The wave functions of the heavy\nmesons in the rest frame are obtained by solving the Schr\\\"odinger equation,\nthen boosted to its light-front based on Susskind's Lorentz transformation. The\nPDFs at the hadron scale, are then obtained by integrating out the transverse\nmomenta of the modulus square of the light-front wave function. Our study shows\nhow the valence quark distributions differ between pseudoscalar and vector\nmesons, as well as among charmonia, bottomonia and bottom-charmed mesons.\nComparisons with other theoretical calculations demonstrate that the PDFs\nobtained herein are in general narrower but align well with the expected\npatterns. Moreover, each PDF's point-wise behavior is squeezed with respect to\nthe scale-free parton-like PDF.",
      "generated_abstract": "S experiment has measured the differential cross sections for\n$e^+e^-\\to\\mu^+\\mu^-\\pi^+\\pi^-$, $e^+e^-\\to\\mu^+\\mu^-\\eta\\pi^0$,\n$e^+e^-\\to\\mu^+\\mu^-\\eta\\eta'$, $e^+e^-\\to\\mu^+\\mu^-\\omega\\eta'$ and\n$e^+e^-\\to\\mu^+\\mu^-\\eta'\\eta'$ in the resonant region. The data were taken\nat the $e^+e^-$ storage ring of the HERA accelerator in Hamburg, Germany, in\nthe energy region of 81 GeV. The measured differential cross sections are\nsummarized in Table 1. The measured differential cross sections are compared\nwith the theoretical calculations. The theoretical calculations are based on\nthe nonrelativistic quantum mechanics",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.104,
          "p": 0.2765957446808511,
          "f": 0.15116278672593306
        },
        "rouge-2": {
          "r": 0.03409090909090909,
          "p": 0.09523809523809523,
          "f": 0.05020920113863583
        },
        "rouge-l": {
          "r": 0.104,
          "p": 0.2765957446808511,
          "f": 0.15116278672593306
        }
      }
    },
    {
      "paper_id": "physics.med-ph.q-bio/TO/2502.20406v1",
      "true_abstract": "In computational modelling of coronary haemodynamics, imposing\npatient-specific flow conditions is paramount, yet often impractical due to\nresource and time constraints, limiting the ability to perform a large number\nof simulations particularly for diseased cases. We aimed to compare coronary\nhaemodynamics quantified using a simplified flow-split strategy with varying\nexponents against the clinically verified but computationally intensive\nmultiscale simulations under both resting and hyperaemic conditions in arteries\nwith varying degrees of stenosis.\n  Six patient-specific left coronary artery trees were segmented and\nreconstructed, including three with severe (>70%) and three with mild (<50%)\nfocal stenoses. Simulations were performed for the entire coronary tree to\naccount for the flow-limiting effects from epicardial artery stenoses. Both a\n0D-3D coupled multiscale model and a flow-split approach with four different\nexponents (2.0, 2.27, 2.33, and 3.0) were used. The resulting prominent\nhaemodynamic metrics were statistically compared between the two methods.\n  Flow-split and multiscale simulations did not significantly differ under\nresting conditions regardless of the stenosis severity. However, under\nhyperaemic conditions, the flow-split method significantly overestimated the\ntime-averaged wall shear stress by up to 16.8 Pa (p=0.031) and underestimate\nthe fractional flow reserve by 0.327 (p=0.043), with larger discrepancies\nobserved in severe stenoses than in mild ones. Varying the exponent from 2.0 to\n3.0 within the flow-split methods did not significantly affect the haemodynamic\nresults (p>0.141).\n  Flow-split strategies with exponents between 2.0 and 3.0 are appropriate for\nmodelling stenosed coronaries under resting conditions. Multiscale simulations\nare recommended for accurate modelling of hyperaemic conditions, especially in\nseverely stenosed arteries.",
      "generated_abstract": "udy of tumor heterogeneity, the distribution of biomarkers in the\ntumor microenvironment (TME) is of critical importance. This study examines\nthe distribution of two molecular biomarkers, PD-L1 and CD163, in a 3D TME of\nhuman glioblastoma (GBM). We employ a combination of machine learning,\ndeep learning, and diffusion modeling to extract the TME microstructure,\nidentify the spatial and temporal distribution of biomarkers in the TME, and\nstudy their correlations. Our analysis reveals that the PD-L1 and CD163\nmicrostructures are distinct from each other, with the former preferentially\noccupying the tumor surface and the latter being more abundant in the deep\ntissue. We also find that the PD-L1 microstructure is associated with the\nsp",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07547169811320754,
          "p": 0.16901408450704225,
          "f": 0.10434782181890377
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07547169811320754,
          "p": 0.16901408450704225,
          "f": 0.10434782181890377
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2410.21295v1",
      "true_abstract": "Gene regulatory networks (GRNs) play a crucial role in the control of\ncellular functions. Numerous methods have been developed to infer GRNs from\ngene expression data, including mechanism-based approaches, information-based\napproaches, and more recent deep learning techniques, the last of which often\noverlooks the underlying gene expression mechanisms. In this work, we introduce\nTRENDY, a novel GRN inference method that integrates transformer models to\nenhance the mechanism-based WENDY approach. Through testing on both simulated\nand experimental datasets, TRENDY demonstrates superior performance compared to\nexisting methods. Furthermore, we apply this transformer-based approach to\nthree additional inference methods, showcasing its broad potential to enhance\nGRN inference.",
      "generated_abstract": "gical origin of the human genome and its evolutionary\ndevelopment are among the most important unresolved questions in the history of\nbiology. The recent advent of next-generation sequencing technologies has\nenabled the sequencing of the complete genomes of more than 1000 organisms,\nincluding bacteria, fungi, plants, and animals. In this review, we discuss\nthe molecular evolution of the human genome, focusing on the origins of the\ngenome and the origin of the mtDNA genome. We also discuss the origin and\nevolution of the mitochondrial genome, including the origin of the first\nmitochondrial DNA genome, the mitochondrial DNA duplication event, and the\nevolution of the mitochondrial genome. Finally, we discuss the evolution of\nthe nuclear genome, including the origin of the first nuclear genome",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14634146341463414,
          "p": 0.2033898305084746,
          "f": 0.17021276109048852
        },
        "rouge-2": {
          "r": 0.02,
          "p": 0.02197802197802198,
          "f": 0.02094240338806621
        },
        "rouge-l": {
          "r": 0.13414634146341464,
          "p": 0.1864406779661017,
          "f": 0.156028363927368
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.15173v1",
      "true_abstract": "Stable and efficient food markets are crucial for global food security, yet\ninternational staple food markets are increasingly exposed to complex risks,\nincluding intensified risk contagion and escalating external uncertainties.\nThis paper systematically investigates risk spillovers in global staple food\nmarkets and explores the key determinants of these spillover effects, combining\ninnovative decomposition-reconstruction techniques, risk connectedness\nanalysis, and random forest models. The findings reveal that short-term\ncomponents exhibit the highest volatility, with futures components generally\nmore volatile than spot components. Further analysis identifies two main risk\ntransmission patterns, namely cross-grain and cross-timescale transmission, and\nclarifies the distinct roles of each component in various net risk spillover\nnetworks. Additionally, price drivers, external uncertainties, and core\nsupply-demand indicators significantly influence these spillover effects, with\nheterogeneous importance of varying factors in explaining different risk\nspillovers. This study provides valuable insights into the risk dynamics of\nstaple food markets, offers evidence-based guidance for policymakers and market\nparticipants to enhance risk warning and mitigation efforts, and supports the\nstabilization of international food markets and the safeguarding of global food\nsecurity.",
      "generated_abstract": "This paper provides a detailed analysis of the implications of heterogeneous\nbargaining power for the optimal allocation of public goods under a\nnon-ideal utilitarian framework. We first derive the optimal allocation when\nthe bargaining power of each agent is assumed to be a constant function of\ntheir utility. We show that the optimal allocation is a strict preference\nelicitation of the agents' utility and is characterized by a unique solution to\na monotonicity-preserving optimization problem. We then show that this\noptimization problem admits a solution for which the utility of an agent is\nincreased when their bargaining power is increased. This result is further\nillustrated through a simple application to a public goods game where the\nbargaining power of the agents is assumed to be a non-monotonic function of\ntheir utility.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10084033613445378,
          "p": 0.18461538461538463,
          "f": 0.13043477803934328
        },
        "rouge-2": {
          "r": 0.012269938650306749,
          "p": 0.018867924528301886,
          "f": 0.014869883700337348
        },
        "rouge-l": {
          "r": 0.10084033613445378,
          "p": 0.18461538461538463,
          "f": 0.13043477803934328
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.cond-mat/other/2503.06672v1",
      "true_abstract": "Ferroelastic materials (materials with switchable spontaneous strain) often\nare centrosymmetric, but their domain walls are always polar, as their internal\nstrain gradients cause polarization via flexoelectricity. This polarization is\ngenerally not switchable by an external electric field, because reversing the\ndomain wall polarity would require reversing the strain gradient, which in turn\nwould require switching the spontaneous strain of the adjacent domains,\ndestroying the domain wall in the process. However, domain wall polarization\ncan also arise from biquadratic coupling between polar and non-polar order\nparameters (e.g. octahedral tilts in perovskites). Such coupling is independent\nof the sign of the polarization and thus allows switching between +P and -P. In\nthis work, we seek to answer the question of whether the polarization of domain\nwalls in ferroelastic perovskites is switchable, as per the symmetric\nbiquadratic term, or non-switchable due to the unipolar flexoelectric bias.\nUsing perovskite calcium titanate (CaTiO3) as a paradigm, molecular dynamics\ncalculations indicate that high electric fields broaden the ferroelastic domain\nwalls, thereby reducing flexoelectricity (as the domain wall strain gradient is\ninversely proportional to the wall width), eventually enabling switching. The\npolarization switching, however, is not ferroelectric-like with a simple\nhysteresis loop, but antiferroelectric-like with a double hysteresis loop.\nFerroelastic domain walls thus behave as functional antiferroelectric elements,\nand also as nucleation points for a bulk phase transition to a polar state.",
      "generated_abstract": "The emergence of non-Fermi-liquid behavior in quantum materials has been\ndiscovered in the last decade. This phenomenon can be attributed to the\ndeviation of the Fermi-liquid theory from the Kondo effect and has been\ndiscussed in various contexts. In this review, we focus on the Kondo effect in\nanisotropic Heisenberg models with antiferromagnetic correlations. We show that\nthe Kondo effect can be modified by the anisotropy of the Heisenberg interaction\nand the spin-orbit coupling. This leads to non-Fermi-liquid behavior in the\nanisotropic Heisenberg model. We discuss the implications of this behavior for\nthe transport properties of the quantum materials, which is a critical issue\nfor their applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15492957746478872,
          "p": 0.3333333333333333,
          "f": 0.21153845720599124
        },
        "rouge-2": {
          "r": 0.018957345971563982,
          "p": 0.0425531914893617,
          "f": 0.02622950393249196
        },
        "rouge-l": {
          "r": 0.13380281690140844,
          "p": 0.2878787878787879,
          "f": 0.18269230335983738
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2502.11449v3",
      "true_abstract": "We study Walrasian economies (or general equilibrium models) and their\nsolution concept, the Walrasian equilibrium. A key challenge in this domain is\nidentifying price-adjustment processes that converge to equilibrium. One such\nprocess, t\\^atonnement, is an auction-like algorithm first proposed in 1874 by\nL\\'eon Walras. While continuous-time variants of t\\^atonnement are known to\nconverge to equilibrium in economies satisfying the Weak Axiom of Revealed\nPreferences (WARP), the process fails to converge in a pathological Walrasian\neconomy known as the Scarf economy. To address these issues, we analyze\nWalrasian economies using variational inequalities (VIs), an optimization\nframework. We introduce the class of mirror extragradient algorithms, which,\nunder suitable Lipschitz-continuity-like assumptions, converge to a solution of\nany VI satisfying the Minty condition in polynomial time. We show that the set\nof Walrasian equilibria of any balanced economy-which includes among others\nArrow-Debreu economies-corresponds to the solution set of an associated VI that\nsatisfies the Minty condition but is generally discontinuous. Applying the\nmirror extragradient algorithm to this VI we obtain a class of\nt\\^atonnement-like processes, which we call the mirror extrat\\^atonnement\nprocess. While our VI formulation is generally discontinuous, it is\nLipschitz-continuous in variationally stable Walrasian economies with bounded\nelasticity-including those satisfying WARP and the Scarf economy-thus\nestablishing the polynomial-time convergence of mirror extrat\\^atonnement in\nthese economies. We validate our approach through experiments on large\nArrow-Debreu economies with Cobb-Douglas, Leontief, and CES consumers, as well\nas the Scarf economy, demonstrating fast convergence in all cases without\nfailure.",
      "generated_abstract": "r examines the role of interventions in solving resource allocation\nproblems in the context of the economic impact of the COVID-19 pandemic. The\nstudy investigates the effect of different interventions on the economic\nimpact of the pandemic. The interventions include a public health intervention\nthat reduces the population's exposure to the virus, a business intervention\nthat reduces the number of people who are employed, and an economic intervention\nthat reduces the economic impact of the pandemic. The economic impact of the\npandemic is evaluated by using the ADB-IDF index and the Gini coefficient. The\nresults indicate that the economic impact of the pandemic is reduced when\nbusiness intervention is implemented. The economic impact of the pandemic is\nreduced by 11.6% when the public health intervention is implemented. The\nresults also indicate that the economic impact of the pandemic is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0979020979020979,
          "p": 0.24561403508771928,
          "f": 0.1399999959245001
        },
        "rouge-2": {
          "r": 0.013574660633484163,
          "p": 0.033707865168539325,
          "f": 0.019354834616233958
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.22807017543859648,
          "f": 0.12999999592450015
        }
      }
    },
    {
      "paper_id": "stat.ML.q-fin/ST/2411.16666v2",
      "true_abstract": "We introduce CatNet, an algorithm that effectively controls False Discovery\nRate (FDR) and selects significant features in LSTM with the Gaussian Mirror\n(GM) method. To evaluate the feature importance of LSTM in time series, we\nintroduce a vector of the derivative of the SHapley Additive exPlanations\n(SHAP) to measure feature importance. We also propose a new kernel-based\ndependence measure to avoid multicollinearity in the GM algorithm, to make a\nrobust feature selection with controlled FDR. We use simulated data to evaluate\nCatNet's performance in both linear models and LSTM models with different link\nfunctions. The algorithm effectively controls the FDR while maintaining a high\nstatistical power in all cases. We also evaluate the algorithm's performance in\ndifferent low-dimensional and high-dimensional cases, demonstrating its\nrobustness in various input dimensions. To evaluate CatNet's performance in\nreal world applications, we construct a multi-factor investment portfolio to\nforecast the prices of S\\&P 500 index components. The results demonstrate that\nour model achieves superior predictive accuracy compared to traditional LSTM\nmodels without feature selection and FDR control. Additionally, CatNet\neffectively captures common market-driving features, which helps informed\ndecision-making in financial markets by enhancing the interpretability of\npredictions. Our study integrates of the Gaussian Mirror algorithm with LSTM\nmodels for the first time, and introduces SHAP values as a new feature\nimportance metric for FDR control methods, marking a significant advancement in\nfeature selection and error control for neural networks.",
      "generated_abstract": "This paper presents a novel method for generating synthetic financial\ndata that incorporates trading-specific features, such as the current\nvolatility of the underlying stock. This methodology leverages an ensemble of\nML models to generate synthetic trading volumes, which are then integrated into\na traditional Markov Chain Monte Carlo (MCMC) framework. Our approach\nenables the generation of a vast range of trading environments, providing a\ncomprehensive understanding of how the markets react to market conditions. We\ndemonstrate the effectiveness of our methodology by generating synthetic data\nfor the S&P 500 index, covering the period from January 1, 2007, to March 31,\n2023.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15492957746478872,
          "p": 0.2857142857142857,
          "f": 0.20091323744959452
        },
        "rouge-2": {
          "r": 0.009433962264150943,
          "p": 0.020202020202020204,
          "f": 0.012861731994501106
        },
        "rouge-l": {
          "r": 0.11267605633802817,
          "p": 0.2077922077922078,
          "f": 0.1461187169016494
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/OT/2411.19902v1",
      "true_abstract": "We propose a pair of completely data-driven algorithms for unsupervised\nclassification and dimension reduction, and we empirically study their\nperformance on a number of data sets, both simulated data in three-dimensions\nand images from the COIL-20 data set. The algorithms take as input a set of\npoints sampled from a uniform distribution supported on a metric space, the\nlatter embedded in an ambient metric space, and they output a clustering or\nreduction of dimension of the data. They work by constructing a natural family\nof graphs from the data and selecting the graph which maximizes the relative\nvon Neumann entropy of certain normalized heat operators constructed from the\ngraphs. Once the appropriate graph is selected, the eigenvectors of the graph\nLaplacian may be used to reduce the dimension of the data, and clusters in the\ndata may be identified with the kernel of the associated graph Laplacian.\nNotably, these algorithms do not require information about the size of a\nneighborhood or the desired number of clusters as input, in contrast to popular\nalgorithms such as $k$-means, and even more modern spectral methods such as\nLaplacian eigenmaps, among others.\n  In our computational experiments, our clustering algorithm outperforms\n$k$-means clustering on data sets with non-trivial geometry and topology, in\nparticular data whose clusters are not concentrated around a specific point,\nand our dimension reduction algorithm is shown to work well in several simple\nexamples.",
      "generated_abstract": "r presents a novel approach to the estimation of a Markov Chain\nMixing Time (MCMT) process. The proposed estimator is based on the\n$\\lambda$-transform, a powerful tool in nonparametric statistics, and can\nhandle various types of Markov chains. It is particularly useful for\nestimating MCMTs with time-varying transition probabilities. The proposed\nestimator is based on the $\\lambda$-transform, a powerful tool in nonparametric\nstatistics, and can handle various types of Markov chains. It is particularly\nuseful for estimating MCMTs with time-varying transition probabilities. The\nproposed estimator is based on the $\\lambda$-transform, a powerful tool in\nnonparametric statistics, and can handle various types of Markov chains. It is\nparticularly useful for estimating MCMTs with time-varying",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08148148148148149,
          "p": 0.2558139534883721,
          "f": 0.12359550195366759
        },
        "rouge-2": {
          "r": 0.004629629629629629,
          "p": 0.02040816326530612,
          "f": 0.007547166797010815
        },
        "rouge-l": {
          "r": 0.08148148148148149,
          "p": 0.2558139534883721,
          "f": 0.12359550195366759
        }
      }
    },
    {
      "paper_id": "math.CA.math/SP/2503.06957v1",
      "true_abstract": "Volterra integral and integro-differential equations have been extensively\nstudied in both pure mathematics and applied science. In one direction,\ndevelopments in analysis have yielded far-ranging existence, uniqueness, and\nregularity results. In the other, applications in science have inspired a\nsubstantial library of practical techniques to deal with such equations.\n  The present work connects these research areas by examining five large\nclasses of linear Volterra equations: integral and integro-differential\nequations with completely monotone (CM) kernels, corresponding to linear\nviscoelastic models; those with positive definite (PD) kernels, corresponding\nto partially-observed quantum systems; difference equations with PD kernels; a\nclass of generalized delay differential equations; and a class of generalized\nfractional differential equations. We develop a system of correspondences\nbetween these problems, showing that all five can be understood within the\nsame, spectral theory. We leverage this theory to recover practical,\nclosed-form solutions of all five classes, and we show that interconversion\nyields a natural, continuous involution within each class. Our work unifies\nseveral results from science: the interconversion formula of Gross, recent\nresults in viscoelasticity and operator theory for integral equations of the\nsecond type, classical formulas for Prony series and fractional differential\nequations, and the convergence of Prony series to CM kernels. Finally, our\ntheory yields a novel, geometric construction of the regularized Hilbert\ntransform, extends it to a wide class of infinite measures, and reveals a\nnatural connection to delay and fractional differential equations.\n  We leverage our theory to develop a powerful, spectral method to handle\nscalar Volterra equations numerically, and illustrate it with a number of\npractical examples.",
      "generated_abstract": "We give a new proof of the famous Poisson--Boltzmann equation (PBE)\nrepresenting the Poisson equation with a surface term. The proof is based on\nthe Poisson--Boltzmann equation with a flat surface and the new Poisson--Boltzmann\nequation with a surface term. The main difference between the new PBE with a\nsurface term and the PBE with a flat surface is the surface term. We also show\nthat the new PBE with a surface term can be used to construct a new surface\nPBE. As a consequence, we prove the uniqueness of the new surface PBE.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09803921568627451,
          "p": 0.38461538461538464,
          "f": 0.1562499967626954
        },
        "rouge-2": {
          "r": 0.021367521367521368,
          "p": 0.07936507936507936,
          "f": 0.03367003032751793
        },
        "rouge-l": {
          "r": 0.0915032679738562,
          "p": 0.358974358974359,
          "f": 0.1458333300960287
        }
      }
    },
    {
      "paper_id": "math.QA.math/QA/2503.06280v1",
      "true_abstract": "Hopf braces are the quantum analogues of skew braces and, as such, their\ncocommutative counterparts provide solutions to the quantum Yang-Baxter\nequation. We investigate various properties of categories related to Hopf\nbraces. In particular, we prove that the category of Hopf braces is accessible\nwhile the category of cocommutative Hopf braces is even locally presentable. We\nalso show that functors forgetting multiple antipodes and/or multiplications\ndown to coalgebras are monadic. Colimits in the category of cocommutative Hopf\nbraces are described explicitly and a free cocommutative Hopf brace on an\narbitrary cocommutative Hopf algebra is constructed.",
      "generated_abstract": "We show that the existence of a $3$-dimensional irreducible representation of\n$SU(4)$ implies that the associated $3$-dimensional irreducible representation\nof $SO(3)$ is reducible, and the same holds for $SO(6)$ and $SU(6)$. This\ngeneralizes a result of Braden, which showed that the $SU(2)$ irreducible\nrepresentation of $SU(4)$ is reducible. We also obtain some explicit\n$4$-dimensional irreducible representations of $SU(4)$, in the case that $k$ is\nodd and $k/2$ is even. We also obtain a $5$-dimensional irreducible\nrepresentation of $SU(5)$ and $SO(7)$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18032786885245902,
          "p": 0.2391304347826087,
          "f": 0.20560747173377597
        },
        "rouge-2": {
          "r": 0.06329113924050633,
          "p": 0.07462686567164178,
          "f": 0.06849314571870932
        },
        "rouge-l": {
          "r": 0.18032786885245902,
          "p": 0.2391304347826087,
          "f": 0.20560747173377597
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.02242v1",
      "true_abstract": "Approaches for improving generative adversarial networks (GANs) training\nunder a few samples have been explored for natural images. However, these\nmethods have limited effectiveness for synthetic aperture radar (SAR) images,\nas they do not account for the unique electromagnetic scattering properties of\nSAR. To remedy this, we propose a physics-inspired regularization method dubbed\n$\\Phi$-GAN, which incorporates the ideal point scattering center (PSC) model of\nSAR with two physical consistency losses. The PSC model approximates SAR\ntargets using physical parameters, ensuring that $\\Phi$-GAN generates SAR\nimages consistent with real physical properties while preventing discriminator\noverfitting by focusing on PSC-based decision cues. To embed the PSC model into\nGANs for end-to-end training, we introduce a physics-inspired neural module\ncapable of estimating the physical parameters of SAR targets efficiently. This\nmodule retains the interpretability of the physical model and can be trained\nwith limited data. We propose two physical loss functions: one for the\ngenerator, guiding it to produce SAR images with physical parameters consistent\nwith real ones, and one for the discriminator, enhancing its robustness by\nbasing decisions on PSC attributes. We evaluate $\\Phi$-GAN across several\nconditional GAN (cGAN) models, demonstrating state-of-the-art performance in\ndata-scarce scenarios on three SAR image datasets.",
      "generated_abstract": "r proposes a novel approach for the development of an automatic\nestimator of the total transmission distance required for the transmission of\nan image in a wireless network. The proposed method is based on the analysis of\nthe image signal propagation characteristics, which are obtained through the\nsimulation of the propagation of the image signal in a wireless medium. The\nproposed method is based on the use of the total transmission distance required\nto transmit the image signal, as a signal parameter, to the wireless network,\nas the target parameter to be estimated. The proposed method is used for the\nsimulation of the propagation characteristics of the image signal in a\nwireless medium and the simulation of the propagation characteristics of the\nimage signal in the wireless medium. The simulation results show that the\nproposed method can estimate the total transmission distance required to\ntransmit the image signal in a wireless network with a high degree of accuracy.\nThe results also show that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.2833333333333333,
          "f": 0.17346938350687224
        },
        "rouge-2": {
          "r": 0.010869565217391304,
          "p": 0.02040816326530612,
          "f": 0.014184392628138864
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.26666666666666666,
          "f": 0.1632653018742192
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2502.20877v1",
      "true_abstract": "Quantitative magnetic resonance imaging (qMRI) requires multi-phase\nacqui-sition, often relying on reduced data sampling and reconstruction\nalgorithms to accelerate scans, which inherently poses an ill-posed inverse\nproblem. While many studies focus on measuring uncertainty during this process,\nfew explore how to leverage it to enhance reconstruction performance. In this\npaper, we in-troduce PUQ, a novel approach that pioneers the use of uncertainty\ninfor-mation for qMRI reconstruction. PUQ employs a two-stage reconstruction\nand parameter fitting framework, where phase-wise uncertainty is estimated\nduring reconstruction and utilized in the fitting stage. This design allows\nuncertainty to reflect the reliability of different phases and guide\ninformation integration during parameter fitting. We evaluated PUQ on in vivo\nT1 and T2 mapping datasets from healthy subjects. Compared to existing qMRI\nreconstruction methods, PUQ achieved the state-of-the-art performance in\nparameter map-pings, demonstrating the effectiveness of uncertainty guidance.\nOur code is available at https://anonymous.4open.science/r/PUQ-75B2/.",
      "generated_abstract": "e of 3D sensing, advancements in optical sensing technologies have\ndemonstrated remarkable potential in 3D imaging. This paper focuses on\nthree-dimensional (3D) optical sensing and imaging, namely, optical coherence\ntomography (OCT), optical coherence tomography angiography (OCTA), and\noptical coherence tomography electrophysiology (OCTEP), in which the three\ndimensional imaging is performed using optical signals. In OCT, the Fourier\ntransform of the transmitted optical signal is used to obtain the optical\nsignal's amplitude and phase at different points along the optical path. In OCTA,\nthe Fourier transform of the transmitted optical signal is used to obtain the\noptical signal's amplitude and phase at different points along the optical path\nand an additional phase signal is obtained by rotating the transmitted optical",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12962962962962962,
          "p": 0.22950819672131148,
          "f": 0.16568046875949732
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12962962962962962,
          "p": 0.22950819672131148,
          "f": 0.16568046875949732
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2408.01185v1",
      "true_abstract": "We introduce a new class of anticipative backward stochastic differential\nequations with a dependence of McKean type on the law of the solution, that we\nname MKABSDE. We provide existence and uniqueness results in a general\nframework with relatively general regularity assumptions on the coefficients.\nWe show how such stochastic equations arise within the modern paradigm of\nderivative pricing where a central counterparty (CCP) requires the members to\ndeposit variation and initial margins to cover their exposure. In the case when\nthe initial margin is proportional to the Conditional Value-at-Risk (CVaR) of\nthe contract price, we apply our general result to define the price as a\nsolution of a MKABSDE. We provide several linear and non-linear simpler\napproximations, which we solve using different numerical (deterministic and\nMonte-Carlo) methods.",
      "generated_abstract": "This paper considers the estimation of the stochastic volatility model\nunder the assumption that the volatility is non-stationary, with a special\nfocus on the case of negative correlations. We derive an exact formula for the\nestimated volatility and present the asymptotic properties of the estimator.\nThe results are applied to a real-world dataset of US stocks. Our results\ndemonstrate the efficiency of our estimator, while its asymptotic properties\nare in line with those of the seminal L\\'evy model. Furthermore, we present a\nrobust and efficient Monte Carlo approach that allows for estimation in the\npresence of outliers.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1797752808988764,
          "p": 0.24615384615384617,
          "f": 0.2077922029136449
        },
        "rouge-2": {
          "r": 0.032520325203252036,
          "p": 0.043010752688172046,
          "f": 0.037037032133488315
        },
        "rouge-l": {
          "r": 0.14606741573033707,
          "p": 0.2,
          "f": 0.168831163952606
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/AS/2502.14893v1",
      "true_abstract": "Symbolic music is represented in two distinct forms: two-dimensional,\nvisually intuitive score images, and one-dimensional, standardized text\nannotation sequences. While large language models have shown extraordinary\npotential in music, current research has primarily focused on unimodal symbol\nsequence text. Existing general-domain visual language models still lack the\nability of music notation understanding. Recognizing this gap, we propose NOTA,\nthe first large-scale comprehensive multimodal music notation dataset. It\nconsists of 1,019,237 records, from 3 regions of the world, and contains 3\ntasks. Based on the dataset, we trained NotaGPT, a music notation visual large\nlanguage model. Specifically, we involve a pre-alignment training phase for\ncross-modal alignment between the musical notes depicted in music score images\nand their textual representation in ABC notation. Subsequent training phases\nfocus on foundational music information extraction, followed by training on\nmusic notation analysis. Experimental results demonstrate that our NotaGPT-7B\nachieves significant improvement on music understanding, showcasing the\neffectiveness of NOTA and the training pipeline. Our datasets are open-sourced\nat https://huggingface.co/datasets/MYTH-Lab/NOTA-dataset.",
      "generated_abstract": "r presents a novel method for performing multidimensional\nsignal processing on the entire image plane with the use of a set of low-rank\nmatrices, known as the basis matrices, which are used to represent the input\nimage. In this work, we use the basis matrices to represent the image in a\ncompressed manner, which is then used for processing the image. The proposed\nmethod is an extension of the recently introduced multidimensional image\nrepresentation (MIR) method for signal processing. We propose a new method for\nperforming image compression using the basis matrices. We first introduce a\nbasis matrix which is used to represent the image. We then present a novel\nmethod for compressing the image by using the basis matrices. We also present a\nnovel method for decompressing the compressed image using the basis matrices.\nWe evaluate the performance of our proposed method using both synthetic and\nreal-world datasets. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14049586776859505,
          "p": 0.2463768115942029,
          "f": 0.178947363795568
        },
        "rouge-2": {
          "r": 0.012658227848101266,
          "p": 0.017094017094017096,
          "f": 0.014545449656596685
        },
        "rouge-l": {
          "r": 0.1322314049586777,
          "p": 0.2318840579710145,
          "f": 0.1684210480060943
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/SC/2502.03290v1",
      "true_abstract": "Flagellar motors enable bacteria to navigate their environments by switching\nrotation direction in response to external cues with high sensitivity. Previous\nwork suggested that ultrasensitivity of the flagellar motor originates from\nconformational spread, in which subunits of the switching complex are strongly\ncoupled to their neighbors as in an equilibrium Ising model. However, dynamic\nsingle-motor measurements indicated that rotation switching is driven out of\nequilibrium, and the mechanism for this dissipative driving remains unknown.\nHere, based on recent cryo-EM structures, we propose that local mechanical\ntorques on motor subunits can affect their conformation dynamics. This gives\nrise to a tug of war between stator-associated subunits, which produces\ncooperative, non-equilibrium switching responses without requiring\nnearest-neighbor interactions. Since subunits are effectively coupled at a\ndistance, we call this mechanism ``Global Mechanical Coupling.\" Our model makes\na qualitatively new prediction that the motor response cooperativity grows with\nthe number of stators driving rotation. Re-analyzing published motor\ndose-response curves in varying load conditions, we find tentative experimental\nevidence for this prediction. Finally, we show that operating out of\nequilibrium enables motors to achieve high cooperativity with faster responses\ncompared to equilibrium motors. Our results suggest a general role for\nmechanics in sensitive chemical regulation.",
      "generated_abstract": "activity is critical for many brain functions, yet the precise\nmechanisms driving its dynamics remain largely unknown. In this work, we\nintroduce a novel neural dynamics model that accounts for the nonlinear\ninteractions between ion channels and their regulation by membrane potentials.\nOur model is based on the action-potential dynamics and is formulated in the\nlanguage of dynamical systems, allowing for a systematic analysis of its\nstability. The resulting dynamics exhibit a rich variety of features, such as\noscillations, chaotic dynamics, and emergence of attractors, all of which are\ncontrolled by the interplay between membrane potentials and ion channels. We\ninvestigate the role of the membrane potential in regulating ion channel\nactivation, finding that the membrane potential drives a switch between\nsteady-state and transient activation of ion channels. In addition, we study the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1897810218978102,
          "p": 0.3023255813953488,
          "f": 0.23318385176375966
        },
        "rouge-2": {
          "r": 0.02030456852791878,
          "p": 0.031496062992125984,
          "f": 0.024691353258078953
        },
        "rouge-l": {
          "r": 0.18248175182481752,
          "p": 0.29069767441860467,
          "f": 0.22421524189828881
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.05664v1",
      "true_abstract": "In financial applications, we often observe both global and local factors\nthat are modeled by a multi-level factor model. When detecting unknown local\ngroup memberships under such a model, employing a covariance matrix as an\nadjacency matrix for local group memberships is inadequate due to the\npredominant effect of global factors. Thus, to detect a local group structure\nmore effectively, this study introduces an inverse covariance matrix-based\nfinancial adjacency matrix (IFAM) that utilizes negative values of the inverse\ncovariance matrix. We show that IFAM ensures that the edge density between\ndifferent groups vanishes, while that within the same group remains\nnon-vanishing. This reduces falsely detected connections and helps identify\nlocal group membership accurately. To estimate IFAM under the multi-level\nfactor model, we introduce a factor-adjusted GLASSO estimator to address the\nprevalent global factor effect in the inverse covariance matrix. An empirical\nstudy using returns from international stocks across 20 financial markets\ndemonstrates that incorporating IFAM effectively detects latent local groups,\nwhich helps improve the minimum variance portfolio allocation performance.",
      "generated_abstract": "the asymptotic properties of two class of generalized\nmoment estimators for the mean of a heteroskedasticity-in-variance model,\nincluding the well-known OLS and the recently proposed\n(un)biased-variance estimators. We establish consistency of the estimators and\nshow that the asymptotic variances of the estimators depend on the parameters\nand the variance of the error term. In contrast to the classical unbiased-variance\nestimator, the proposed estimators have finite variance when the variance of\nthe error term is large. We show that the asymptotic variances of the proposed\nestimators are asymptotically equivalent to the variances of the classical\nbiased-variance estimator and the unbiased-variance estimator, respectively.\nThese results are obtained under some mild conditions on the parameter and\nthe variance of the error term. We",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.27586206896551724,
          "f": 0.18823528962214545
        },
        "rouge-2": {
          "r": 0.03184713375796178,
          "p": 0.056818181818181816,
          "f": 0.04081632192719752
        },
        "rouge-l": {
          "r": 0.11607142857142858,
          "p": 0.22413793103448276,
          "f": 0.15294117197508664
        }
      }
    },
    {
      "paper_id": "eess.SP.cs/ET/2503.08062v1",
      "true_abstract": "Orthogonal frequency division multiplexing (OFDM), which has been the\ndominating waveform for contemporary wireless communications, is also regarded\nas a competitive candidate for future integrated sensing and communication\n(ISAC) systems. Existing works on OFDM-ISAC usually assume that the maximum\nsensing range should be limited by the cyclic prefix (CP) length since\ninter-symbol interference (ISI) and inter-carrier interference (ICI) should be\navoided. However, in this paper, we provide rigorous analysis to reveal that\nthe random data embedded in OFDM-ISAC signal can actually act as a free ``mask\"\nfor ISI, which makes ISI/ICI random and hence greatly attenuated after radar\nsignal processing. The derived signal-to-interference-plus-noise ratio (SINR)\nin the range profile demonstrates that the maximum sensing range of OFDM-ISAC\ncan greatly exceed the ISI-free distance that is limited by the CP length,\nwhich is validated by simulation results. To further mitigate power degradation\nfor long-range targets, a novel sliding window sensing method is proposed,\nwhich iteratively detects and cancels short-range targets before shifting the\ndetection window. The shifted detection window can effectively compensate the\npower degradation due to insufficient CP length for long-range targets. Such\nresults provide valuable guidance for the CP length design in OFDM-ISAC\nsystems.",
      "generated_abstract": "r introduces a novel approach to the generation of multi-modal\n(audio and text) data for multimodal machine learning (MMML) models. The\nproposed method utilizes the text-to-audio generation capabilities of pre-trained\nmodels, such as OpenAI's GPT-4, to generate audio segments that are subsequently\naugmented with multi-modal audio-text data to enhance the model's understanding\nof the audio content. This approach enhances the capabilities of MMML models,\nparticularly those designed for speech-to-text (S2T) and text-to-speech\n(T2S) translation, by leveraging the additional textual context provided by\nmulti-modal data. The effectiveness of this approach is demonstrated through\nexperimental results, which show that the generated audio segments outperform\ncontrol audio when used as input for S2T and T2S translation tasks",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12698412698412698,
          "p": 0.19753086419753085,
          "f": 0.15458936721697136
        },
        "rouge-2": {
          "r": 0.0111731843575419,
          "p": 0.018018018018018018,
          "f": 0.013793098723188301
        },
        "rouge-l": {
          "r": 0.12698412698412698,
          "p": 0.19753086419753085,
          "f": 0.15458936721697136
        }
      }
    },
    {
      "paper_id": "cs.AR.cs/AR/2503.06862v1",
      "true_abstract": "Weight-only quantization has emerged as a promising solution to the\ndeployment challenges of large language models (LLMs). However, it necessitates\nFP-INT operations, which make implementation on general-purpose hardware like\nGPUs difficult. In this paper, we propose FIGLUT, an efficient look-up table\n(LUT)-based GEMM accelerator architecture. Instead of performing traditional\narithmetic operations, FIGLUT retrieves precomputed values from an LUT based on\nweight patterns, significantly reducing the computational complexity. We also\nintroduce a novel LUT design that addresses the limitations of conventional\nmemory architectures. To further improve LUT-based operations, we propose a\nhalf-size LUT combined with a dedicated decoding and multiplexing unit. FIGLUT\nefficiently supports different bit precisions and quantization methods using a\nsingle fixed hardware configuration. For the same 3-bit weight precision,\nFIGLUT demonstrates 59% higher TOPS/W and 20% lower perplexity than\nstate-of-the-art accelerator designs. When targeting the same perplexity,\nFIGLUT achieves 98% higher TOPS/W by performing 2.4-bit operations.",
      "generated_abstract": "aper, we propose a novel framework, called TRIF, which integrates\ntranslation-invariant feature transformation (TIFT), a novel pre-processing\ntechnique, and transformer-based image generation model. The proposed TRIF\nframework aims to enhance the generation of high-quality images. TIFT\ntransforms the input image into a transformed image, which is then passed to\nthe transformer-based image generation model for generating the final\ntranslation-invariant image. We introduce a novel feature transformation\ntechnique, called Diffusion-based Translation-Invariant Feature\nTransformer (DTIFT), which is inspired by the diffusion model and is able to\ngenerate high-quality images. TIFT is trained on a large-scale dataset, which\nis divided into training, validation, and testing phases. We conduct extensive\nexperiments on various datasets, including Manga2",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11016949152542373,
          "p": 0.18055555555555555,
          "f": 0.13684210055623283
        },
        "rouge-2": {
          "r": 0.034482758620689655,
          "p": 0.04950495049504951,
          "f": 0.04065040166402332
        },
        "rouge-l": {
          "r": 0.11016949152542373,
          "p": 0.18055555555555555,
          "f": 0.13684210055623283
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2503.05185v1",
      "true_abstract": "Finance decision-making often relies on in-depth data analysis across various\ndata sources, including financial tables, news articles, stock prices, etc. In\nthis work, we introduce FinTMMBench, the first comprehensive benchmark for\nevaluating temporal-aware multi-modal Retrieval-Augmented Generation (RAG)\nsystems in finance. Built from heterologous data of NASDAQ 100 companies,\nFinTMMBench offers three significant advantages. 1) Multi-modal Corpus: It\nencompasses a hybrid of financial tables, news articles, daily stock prices,\nand visual technical charts as the corpus. 2) Temporal-aware Questions: Each\nquestion requires the retrieval and interpretation of its relevant data over a\nspecific time period, including daily, weekly, monthly, quarterly, and annual\nperiods. 3) Diverse Financial Analysis Tasks: The questions involve 10\ndifferent tasks, including information extraction, trend analysis, sentiment\nanalysis and event detection, etc. We further propose a novel TMMHybridRAG\nmethod, which first leverages LLMs to convert data from other modalities (e.g.,\ntabular, visual and time-series data) into textual format and then incorporates\ntemporal information in each node when constructing graphs and dense indexes.\nIts effectiveness has been validated in extensive experiments, but notable gaps\nremain, highlighting the challenges presented by our FinTMMBench.",
      "generated_abstract": "the impact of portfolio rebalancing on the distribution of\ndistribution of realized return and realized volatility. We consider a fixed\nnumber of assets and an arbitrary number of rebalancing periods. In addition,\nwe allow for portfolio rebalancing at the same time during the rebalancing\nperiods, which can occur during the trading hours or outside the trading\nhours. We investigate whether the realized return and realized volatility\ndistributions change under these different scenarios. We find that the\ndistributions of realized return and realized volatility do not change in all\ncases, and that the realized return distribution is significantly different\nfrom the realized volatility distribution in all cases. When portfolio rebalancing\noccurred at the trading hours, the realized return distribution tends to be\nmore skewed toward higher values than the realized volatility distribution,\nwhile when portfolio rebalancing occurred at the same time",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11258278145695365,
          "p": 0.23943661971830985,
          "f": 0.15315314880245123
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10596026490066225,
          "p": 0.22535211267605634,
          "f": 0.1441441397934422
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.05300v1",
      "true_abstract": "This article introduces a subbagging (subsample aggregating) approach for\nvariable selection in regression within the context of big data. The proposed\nsubbagging approach not only ensures that variable selection is scalable given\nthe constraints of available computational resources, but also preserves the\nstatistical efficiency of the resulting estimator. In particular, we propose a\nsubbagging loss function that aggregates the least-squares approximations of\nthe loss function for each subsample. Subsequently, we penalize the subbagging\nloss function via an adaptive LASSO-type regularizer, and obtain a regularized\nestimator to achieve variable selection. We then demonstrate that the\nregularized estimator exhibits $\\sqrt{N}$-consistency and possesses the oracle\nproperties, where $N$ represents the size of the full sample in the big data.\nIn addition, we propose a subbagging Bayesian information criterion to select\nthe regularization parameter, ensuring that the regularized estimator achieves\nselection consistency. Simulation experiments are conducted to demonstrate the\nnumerical performance. A U.S. census dataset is analyzed to illustrate the\nusefulness and computational scalability of the subbagging variable selection\nmethod.",
      "generated_abstract": "e a novel methodology for constructing predictive models for\nmeasuring change in a continuous variable based on a finite number of samples\nfrom a multivariate normal distribution. This methodology is based on the\nLaplace approximation of the normal distribution, which can be applied to a\ncontinuous response variable and a set of covariates, which can be observed\nexperimentally or estimated from data. This approach allows for the\nprediction of the response variable at any point in time or in a set of\npossible future time points. The proposed methodology is used to construct\npredictive models for the change in a continuous variable based on a finite\nnumber of samples from a multivariate normal distribution. The methodology is\napplied to the analysis of a dataset of 243 patients with chronic obstructive\npulmonary disease (COPD), in which the response variable is the exacerbation\nfrequency (",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14705882352941177,
          "p": 0.21428571428571427,
          "f": 0.17441859982422944
        },
        "rouge-2": {
          "r": 0.013605442176870748,
          "p": 0.01834862385321101,
          "f": 0.015624995110169987
        },
        "rouge-l": {
          "r": 0.12745098039215685,
          "p": 0.18571428571428572,
          "f": 0.15116278587074108
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.09541v1",
      "true_abstract": "The paper studies the problem of detecting and locating change points in\nmultivariate time-evolving data. The problem has a long history in statistics\nand signal processing and various algorithms have been developed primarily for\nsimple parametric models. In this work, we focus on modeling the data through\nfeed-forward neural networks and develop a detection strategy based on the\nfollowing two-step procedure. In the first step, the neural network is trained\nover a prespecified window of the data, and its test error function is\ncalibrated over another prespecified window. Then, the test error function is\nused over a moving window to identify the change point. Once a change point is\ndetected, the procedure involving these two steps is repeated until all change\npoints are identified. The proposed strategy yields consistent estimates for\nboth the number and the locations of the change points under temporal\ndependence of the data-generating process. The effectiveness of the proposed\nstrategy is illustrated on synthetic data sets that provide insights on how to\nselect in practice tuning parameters of the algorithm and in real data sets.\nFinally, we note that although the detection strategy is general and can work\nwith different neural network architectures, the theoretical guarantees\nprovided are specific to feed-forward neural architectures.",
      "generated_abstract": "the problem of learning a sparse linear classifier in a\nrandomized online setting. Specifically, we consider a sequence of randomized\nonline algorithms that use a fixed number of online queries to construct a\nsparse linear classifier, and the empirical risk of this classifier.\n  Our main result is a polynomial-time algorithm that uses at most $\\frac{n\n\\log n}{\\varepsilon^2}$ online queries to obtain an $O(\\varepsilon)$-sparse\nlinear classifier with high probability.  Our algorithm relies on the\n$O(\\varepsilon)$-sparse linear classifier that we obtain in the previous\nsection, and on a variant of the online algorithm for sparse linear regression\ndue to Pach and Schapire.  We also obtain a sublinear-time algorithm for\nsparse linear classifiers, and a polynomial-time algorithm for sparse linear\nregression.  Our results provide a new framework for analyzing online\nlearning",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14754098360655737,
          "p": 0.2727272727272727,
          "f": 0.19148935714576742
        },
        "rouge-2": {
          "r": 0.026737967914438502,
          "p": 0.045871559633027525,
          "f": 0.03378377913098129
        },
        "rouge-l": {
          "r": 0.13114754098360656,
          "p": 0.24242424242424243,
          "f": 0.17021276140108657
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.02030v1",
      "true_abstract": "We study policy evaluation problems in multi-task reinforcement learning (RL)\nunder a low-rank representation setting. In this setting, we are given $N$\nlearning tasks where the corresponding value function of these tasks lie in an\n$r$-dimensional subspace, with $r<N$. One can apply the classic\ntemporal-difference (TD) learning method for solving these problems where this\nmethod learns the value function of each task independently. In this paper, we\nare interested in understanding whether one can exploit the low-rank structure\nof the multi-task setting to accelerate the performance of TD learning. To\nanswer this question, we propose a new variant of TD learning method, where we\nintegrate the so-called truncated singular value decomposition step into the\nupdate of TD learning. This additional step will enable TD learning to exploit\nthe dominant directions due to the low rank structure to update the iterates,\ntherefore, improving its performance. Our empirical results show that the\nproposed method significantly outperforms the classic TD learning, where the\nperformance gap increases as the rank $r$ decreases.\n  From the theoretical point of view, introducing the truncated singular value\ndecomposition step into TD learning might cause an instability on the updates.\nWe provide a theoretical result showing that the instability does not happen.\nSpecifically, we prove that the proposed method converges at a rate\n$\\mathcal{O}(\\frac{\\ln(t)}{t})$, where $t$ is the number of iterations. This\nrate matches that of the standard TD learning.",
      "generated_abstract": "r proposes a novel solution for controlling the amount of\ngeneration from renewable energy sources in a distributed energy system. The\nproposed approach is based on the use of a hybrid controller, integrating a\nlinear-quadratic regulator with an optimal controller, to control the generation\nof renewable energy sources and the integration of renewable energy sources into\nthe power system. The hybrid controller combines the linear-quadratic regulator\n(LQR) and optimal controller in an adaptive manner to minimize the total\nenergy consumption of the system. The proposed approach is evaluated using a\ntwo-zonal power system model, where the optimal controller is designed for\ndifferent system characteristics, including the voltage level, the number of\ngenerators, and the renewable energy source contribution. The results show that\nthe proposed hybrid controller is able to reduce the overall energy consumption\nby 22% compared to the case",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13076923076923078,
          "p": 0.2361111111111111,
          "f": 0.16831682709538293
        },
        "rouge-2": {
          "r": 0.05853658536585366,
          "p": 0.09917355371900827,
          "f": 0.07361962723380662
        },
        "rouge-l": {
          "r": 0.12307692307692308,
          "p": 0.2222222222222222,
          "f": 0.15841583699637304
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2503.02923v1",
      "true_abstract": "Diverse organisms exploit the geomagnetic field (GMF) for migration.\nMigrating birds employ an intrinsically quantum mechanical mechanism for\ndetecting the geomagnetic field: absorption of a blue photon generates a\nradical pair whose two electrons precess at different rates in the magnetic\nfield, thereby sensitizing cells to the direction of the GMF. In this work,\nusing an in vitro injury model, we discovered a quantum-based mechanism of\ncellular migration. Specifically, we show that migrating cells detect the GMF\nvia an optically activated, electron spin-based mechanism. Cell injury provokes\nacute emission of blue photons, and these photons sensitize muscle progenitor\ncells to the magnetic field. We show that the magnetosensitivity of muscle\nprogenitor cells is (a) activated by blue light, but not by green or red light,\nand (b) disrupted by the application of an oscillatory field at the frequency\ncorresponding to the energy of the electron-spin/magnetic field interaction. A\ncomprehensive analysis of protein expression reveals that the ability of blue\nphotons to promote cell motility is mediated by activation of calmodulin\ncalcium sensors. Collectively, these data suggest that cells possess a\nlight-dependent magnetic compass driven by electron spin dynamics.",
      "generated_abstract": "caused by viruses are a significant public health concern, with\nresearch efforts targeting their molecular mechanisms, vaccine design, and\ntherapeutics. However, despite the critical importance of viral protein-protein\ninteractions, our understanding of their structure and dynamics remains\nlimited. Here, we present a computational analysis of the structure and dynamics\nof viral protein-protein interactions in the form of hemagglutinin (HA)\nstructures of influenza A (H1N1) and H5N1 viruses. By employing density-free\ndiscretization and a force-field-independent potential, we derived the\nequilibrium structures and dynamics of these complex systems, providing a\nframework for future structural and dynamical studies of viral protein-protein\ninteractions. Our results show that the equilibrium structures of H1N1 and H5N1\nHA exhib",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11666666666666667,
          "p": 0.18666666666666668,
          "f": 0.14358973885601592
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.05102040816326531,
          "f": 0.03663003202780184
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.16,
          "f": 0.12307691834319545
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.00358v1",
      "true_abstract": "This paper considers nonparametric estimation and inference in first-order\nautoregressive (AR(1)) models with deterministically time-varying parameters. A\nkey feature of the proposed approach is to allow for time-varying stationarity\nin some time periods, time-varying nonstationarity (i.e., unit root or\nlocal-to-unit root behavior) in other periods, and smooth transitions between\nthe two. The estimation of the AR parameter at any time point is based on a\nlocal least squares regression method, where the relevant initial condition is\nendogenous. We obtain limit distributions for the AR parameter estimator and\nt-statistic at a given point $\\tau$ in time when the parameter exhibits unit\nroot, local-to-unity, or stationary/stationary-like behavior at time $\\tau$.\nThese results are used to construct confidence intervals and median-unbiased\ninterval estimators for the AR parameter at any specified point in time. The\nconfidence intervals have correct asymptotic coverage probabilities with the\ncoverage holding uniformly over stationary and nonstationary behavior of the\nobservations.",
      "generated_abstract": "We propose a two-stage sequential regression modeling approach to analyze\ndata with non-normal, possibly stochastic, responses. The first stage is an\nempirical analysis of the data, including the identification of the\nregression model and the estimation of the parameters. The second stage aims\nto select the model that best fits the data, which may require additional\nregression models, in particular those for covariates. The proposed approach\nis flexible and can be used in a variety of settings, including those where\nexisting regression models are inappropriate.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.3333333333333333,
          "f": 0.24999999531250006
        },
        "rouge-2": {
          "r": 0.028368794326241134,
          "p": 0.04938271604938271,
          "f": 0.036036031401266724
        },
        "rouge-l": {
          "r": 0.16,
          "p": 0.26666666666666666,
          "f": 0.19999999531250012
        }
      }
    },
    {
      "paper_id": "math.CO.math/AT/2503.05385v1",
      "true_abstract": "A Bier sphere is a simplicial sphere obtained as the deleted join of a\nsimplicial complex and its combinatorial Alexander dual. We focus on particular\nclasses of full subcomplexes of Bier spheres, and determine their topological\ntypes. As applications, we explicitly describe the cohomology of real toric\nspaces associated with Bier spheres.",
      "generated_abstract": "We prove the localization theorem for the stable homotopy category of\n$\\mathbb{Z}$-graded K\\\"ahler manifolds. This theorem states that the stable\nhomotopy category of a $\\mathbb{Z}$-graded K\\\"ahler manifold can be\nidentified with the stable homotopy category of the associated graded complex\nof the K\\\"ahler potential. In particular, this implies that the stable\nhomotopy category of a K\\\"ahler manifold is a quasi-category. Our proof is\nbased on the theory of stable homotopy categories developed in the recent\npapers by the first and the second author. The latter paper also contains a\nproof of the localization theorem for the stable homotopy category of the\ncoarse moduli space of K3 surfaces.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23809523809523808,
          "p": 0.17543859649122806,
          "f": 0.20202019713498634
        },
        "rouge-2": {
          "r": 0.04,
          "p": 0.02564102564102564,
          "f": 0.03124999523925854
        },
        "rouge-l": {
          "r": 0.21428571428571427,
          "p": 0.15789473684210525,
          "f": 0.18181817693296617
        }
      }
    },
    {
      "paper_id": "math.AG.math/AG/2503.10564v1",
      "true_abstract": "We prove that holomorphic maps from an open subset of a complex smooth\nprojective curve to a complex smooth projective rationally simply connected\nvariety can be approximated by algebraic maps for the compact-open topology.\nThis theorem can be applied in particular when the target is a smooth\nhypersurface of degree d in P^n with n greater than or equal to d^2-1. We\ndeduce it from a more general result: the tight approximation property holds\nfor rationally simply connected varieties over function fields of complex\ncurves.",
      "generated_abstract": "of the cohomology of the Grassmannian of subspaces of a vector\nspace is one of the most important topics in algebra. In the case of the\nGrassmannian of subspaces of a vector space over a field $\\mathbb{K}$, the\ntheory of linear forms on this Grassmannian is well developed. In this paper,\nwe consider the case of the Grassmannian of subspaces of an arbitrary vector\nspace over a field $\\mathbb{K}$. We show that the cohomology of the Grassmannian\nof subspaces of a vector space over a field $\\mathbb{K}$ is isomorphic to the\ntensor algebra of the space of linear forms on the Grassmannian. We prove that\nthe cohomology of the Grassmannian of subspaces of an arbitrary vector space\nover a field $\\mathbb{K}$ is isomorphic to the tensor algebra of the space of\nlinear forms on",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1774193548387097,
          "p": 0.275,
          "f": 0.2156862697424069
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.046875,
          "f": 0.04225351617536262
        },
        "rouge-l": {
          "r": 0.14516129032258066,
          "p": 0.225,
          "f": 0.1764705834678971
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.09155v1",
      "true_abstract": "We consider time-invariant nonlinear $n$-dimensional strongly $2$-cooperative\nsystems, that is, systems that map the set of vectors with up to weak sign\nvariation to its interior. Strongly $2$-cooperative systems enjoy a strong\nPoincare-Bendixson property: bounded solutions that maintain a positive\ndistance from the set of equilibria converge to a periodic solution. For\nstrongly $2$-cooperative systems whose trajectories evolve in a bounded and\ninvariant set that contains a single unstable equilibrium, we provide a simple\ncriterion for the existence of periodic trajectories. Moreover, we explicitly\ncharacterize a positive-measure set of initial conditions which yield solutions\nthat asymptotically converge to a periodic trajectory. We demonstrate our\ntheoretical results using two models from systems biology, the $n$-dimensional\nGoodwin oscillator and a $4$-dimensional biomolecular oscillator with\nRNA-mediated regulation, and provide numerical simulations that verify the\ntheoretical results.",
      "generated_abstract": "In this paper, we construct an analogue of the Farkas-Levitan lemma for\nthe $n$-dimensional affine space $\\mathbb{A}^n$ by using the method of\nquasi-linear algebra. We also provide an alternative proof of this lemma,\nwhich is based on the notion of the Farkas-Levitan lemma for a linear space.\nMoreover, we show that the Farkas-Levitan lemma for $\\mathbb{A}^n$ holds true\nfor a general linear space $L$ over a field $\\mathbb{K}$ of characteristic\n$\\neq 2$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.2553191489361702,
          "f": 0.18320610226909867
        },
        "rouge-2": {
          "r": 0.02459016393442623,
          "p": 0.04838709677419355,
          "f": 0.032608691183838044
        },
        "rouge-l": {
          "r": 0.13095238095238096,
          "p": 0.23404255319148937,
          "f": 0.16793892669657962
        }
      }
    },
    {
      "paper_id": "cs.LG.q-fin/CP/2502.17518v1",
      "true_abstract": "This paper presents a comprehensive study on the use of ensemble\nReinforcement Learning (RL) models in financial trading strategies, leveraging\nclassifier models to enhance performance. By combining RL algorithms such as\nA2C, PPO, and SAC with traditional classifiers like Support Vector Machines\n(SVM), Decision Trees, and Logistic Regression, we investigate how different\nclassifier groups can be integrated to improve risk-return trade-offs. The\nstudy evaluates the effectiveness of various ensemble methods, comparing them\nwith individual RL models across key financial metrics, including Cumulative\nReturns, Sharpe Ratios (SR), Calmar Ratios, and Maximum Drawdown (MDD). Our\nresults demonstrate that ensemble methods consistently outperform base models\nin terms of risk-adjusted returns, providing better management of drawdowns and\noverall stability. However, we identify the sensitivity of ensemble performance\nto the choice of variance threshold {\\tau}, highlighting the importance of\ndynamic {\\tau} adjustment to achieve optimal performance. This study emphasizes\nthe value of combining RL with classifiers for adaptive decision-making, with\nimplications for financial trading, robotics, and other dynamic environments.",
      "generated_abstract": "This paper introduces a new algorithm, called FMA (Fast Multiplication of\nAscending Numbers), to solve the problem of finding the $k$th largest element\nin ascending order. FMA is a new algorithm that achieves $O(1)$ runtime for\nfinding the $k$th largest element in ascending order. FMA is the first algorithm\nto use the concept of rank and rank-1 vectors to accelerate the algorithm. We\npresent experimental results and analysis for FMA, demonstrating its\nsignificance in solving real-world problems in finance and data science.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1,
          "p": 0.2222222222222222,
          "f": 0.13793103020214045
        },
        "rouge-2": {
          "r": 0.00625,
          "p": 0.014084507042253521,
          "f": 0.008658004400219481
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.2222222222222222,
          "f": 0.13793103020214045
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.06441v1",
      "true_abstract": "Company financial risks pose a significant threat to personal wealth and\nnational economic stability, stimulating increasing attention towards the\ndevelopment of efficient andtimely methods for monitoring them. Current\napproaches tend to use graph neural networks (GNNs) to model the momentum\nspillover effect of risks. However, due to the black-box nature of GNNs, these\nmethods leave much to be improved for precise and reliable explanations towards\ncompany risks. In this paper, we propose CF3, a novel Counterfactual and\nFactual learning method for company Financial risk detection, which generates\nevidence subgraphs on company knowledge graphs to reliably detect and explain\ncompany financial risks. Specifically, we first propose a meta-path attribution\nprocess based on Granger causality, selecting the meta-paths most relevant to\nthe target node labels to construct an attribution subgraph. Subsequently, we\npropose anedge-type-aware graph generator to identify important edges, and we\nalso devise a layer-based feature masker to recognize crucial node features.\nFinally, we utilize counterfactual-factual reasoning and a loss function based\non attribution subgraphs to jointly guide the learning of the graph generator\nand feature masker. Extensive experiments on three real-world datasets\ndemonstrate the superior performance of our method compared to state-of-the-art\napproaches in the field of financial risk detection.",
      "generated_abstract": "umber of recent studies have shown that generative AI models can\ngenerate high-quality speech with diverse speech characteristics. However,\nthese models often produce speech with limited expressiveness, which hinders\ntheir application in various scenarios. To address this issue, we propose\nSpeechGAN++, a framework that integrates a speech generation module and a\nspeech expression module. The speech generation module uses a pre-trained\nspeech model to generate speech samples with desired characteristics, and the\nspeech expression module converts the generated speech samples into\nexpressive speech. The two modules are integrated into the generative model\nthrough a latent space. Our experiments demonstrate that SpeechGAN++ can\ngenerate diverse speech samples with high expressiveness. Moreover, the\nintegration of the speech generation module with the speech expression module\nenhances the model's capability to generate speech with diverse expressiveness\nand better adaptability to various speech characteristics",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11450381679389313,
          "p": 0.19480519480519481,
          "f": 0.14423076456777012
        },
        "rouge-2": {
          "r": 0.015544041450777202,
          "p": 0.02586206896551724,
          "f": 0.019417471038637092
        },
        "rouge-l": {
          "r": 0.11450381679389313,
          "p": 0.19480519480519481,
          "f": 0.14423076456777012
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2411.07674v2",
      "true_abstract": "We prove that a two-cycle equilibrium in a general equilibrium model with\ninfinitely-lived agents also constitutes an equilibrium in an overlapping\ngenerations (OLG) model. Conversely, an equilibrium in an OLG model that\nsatisfies additional conditions is part of an equilibrium in a general\nequilibrium model with infinitely-lived agents. Applying this result, we\ndemonstrate that equilibrium indeterminacy and rational asset price bubbles may\narise in both types of models.",
      "generated_abstract": "The emergence of artificial intelligence (AI) has significantly influenced\nthe financial sector, especially with the rise of machine learning models\nthat enable automation in trading and risk management. These advancements\npresent both challenges and opportunities for financial institutions to\noptimize their strategies and adapt to changing market conditions. By\ncomprehensively analyzing the impact of AI on trading and risk management, this\npaper provides a comprehensive overview of the current state of AI in the\nfinancial sector and identifies areas for future research and innovation.\nAdditionally, we highlight key challenges and opportunities for financial\ninstitutions to leverage AI in their operations, providing practical\nrecommendations for future research and innovation in this field.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2619047619047619,
          "p": 0.1506849315068493,
          "f": 0.19130434318941408
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.21428571428571427,
          "p": 0.1232876712328767,
          "f": 0.15652173449376197
        }
      }
    },
    {
      "paper_id": "nlin.CD.nlin/CD/2503.05462v1",
      "true_abstract": "The wave kinetic equation has become an important tool in different fields of\nphysics. In particular, for surface gravity waves, it is the backbone of wave\nforecasting models. Its derivation is based on the Hamiltonian dynamics of\nsurface gravity waves. Only at the end of the derivation are the\nnon-conservative effects, such as forcing and dissipation, included as\nadditional terms to the collision integral. In this paper, we present a first\nattempt to derive the wave kinetic equation when the dissipation/forcing is\nincluded in the deterministic dynamics. If, in the dynamical equations, the\ndissipation/forcing is one order of magnitude smaller than the nonlinear\neffect, then the classical wave action balance equation is obtained and the\nkinetic time scale corresponds to the dissipation/forcing time scale. However,\nif we assume that the nonlinearity and the dissipation/forcing act on the same\ndynamical time scale, we find that the dissipation/forcing dominates the\ndynamics and the resulting collision integral appears in a modified form, at a\nhigher order.",
      "generated_abstract": "We present a simple and efficient method for the numerical integration of\nthe generalized Boussinesq equations. The method is based on the\nrepresentation of the wave function as a sum of a Fourier series and a\nHamilton-Jacobi equation, which is solved iteratively. We demonstrate the\nmethod on a number of test cases, including the wave equation with the\nSchr\\\"odinger-Poisson equation, the wave equation with a constant background\npotential, and the wave equation with a Gaussian background potential. The\nnumerical results confirm the reliability of the method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13829787234042554,
          "p": 0.2765957446808511,
          "f": 0.18439715867612308
        },
        "rouge-2": {
          "r": 0.04794520547945205,
          "p": 0.0945945945945946,
          "f": 0.06363635917190115
        },
        "rouge-l": {
          "r": 0.13829787234042554,
          "p": 0.2765957446808511,
          "f": 0.18439715867612308
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2502.16108v1",
      "true_abstract": "The binary expansions of irrational algebraic numbers can serve as\nhigh-quality pseudorandom binary sequences. This study presents an efficient\nmethod for computing the exact binary expansions of real quadratic algebraic\nintegers using Newton's method. To this end, we clarify conditions under which\nthe first $N$ bits of the binary expansion of an irrational number match those\nof its upper rational approximation. Furthermore, we establish that the\nworst-case time complexity of generating a sequence of length $N$ with the\nproposed method is equivalent to the complexity of multiplying two $N$-bit\nintegers, showing its efficiency compared to a previously proposed true orbit\ngenerator. We report the results of numerical experiments on computation time\nand memory usage, highlighting in particular that the proposed method\nsuccessfully accelerates true orbit pseudorandom number generation. We also\nconfirm that a generated pseudorandom sequence successfully passes all the\nstatistical tests included in RabbitFile of TestU01.",
      "generated_abstract": "We present a novel and efficient method for the computation of the\ncovariance matrix of a multivariate Gaussian process. Our approach is based on\nthe eigen-decomposition of the covariance kernel. We propose a fast algorithm\nthat is based on a combination of fast Fourier transform and an efficient\nmultiplication by an Hermitian matrix. We provide theoretical guarantees for\nthe convergence of our method and we show that it is superior to the method\nproposed in [22",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1717171717171717,
          "p": 0.3617021276595745,
          "f": 0.23287670796303256
        },
        "rouge-2": {
          "r": 0.03597122302158273,
          "p": 0.07142857142857142,
          "f": 0.04784688549712732
        },
        "rouge-l": {
          "r": 0.16161616161616163,
          "p": 0.3404255319148936,
          "f": 0.21917807782604626
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/ET/2503.07599v1",
      "true_abstract": "Generative AI is transforming education by enabling personalized, on-demand\nlearning experiences. However, AI tutors lack the ability to assess a learner's\ncognitive state in real time, limiting their adaptability. Meanwhile,\nelectroencephalography (EEG)-based neuroadaptive systems have successfully\nenhanced engagement by dynamically adjusting learning content. This paper\npresents NeuroChat, a proof-of-concept neuroadaptive AI tutor that integrates\nreal-time EEG-based engagement tracking with generative AI. NeuroChat\ncontinuously monitors a learner's cognitive engagement and dynamically adjusts\ncontent complexity, response style, and pacing using a closed-loop system. We\nevaluate this approach in a pilot study (n=24), comparing NeuroChat to a\nstandard LLM-based chatbot. Results indicate that NeuroChat enhances cognitive\nand subjective engagement but does not show an immediate effect on learning\noutcomes. These findings demonstrate the feasibility of real-time cognitive\nfeedback in LLMs, highlighting new directions for adaptive learning, AI\ntutoring, and human-AI interaction.",
      "generated_abstract": "years, the evolution of AI-powered digital assistants (DA) has\ncreated new opportunities for education. DA can be used to enhance student\nexperiences in many areas, including assessment, learning, and teaching. DA\nfacilitates student engagement, helps students understand course content, and\nenhances teaching. However, there is a lack of research on the role of DA in\neducation, particularly in higher education. This research aims to fill this gap\nby investigating the role of DA in higher education. The research is based on\nthe theory of augmented reality (AR) and virtual reality (VR). The study\nconducted a survey of 300 students from different educational levels in\nSharjah, UAE. The results show that DA has a significant positive effect on\nstudent engagement and learning. The study also found that students preferred\nDA over traditional learning",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23809523809523808,
          "p": 0.28735632183908044,
          "f": 0.260416661710612
        },
        "rouge-2": {
          "r": 0.014814814814814815,
          "p": 0.016260162601626018,
          "f": 0.015503870979810512
        },
        "rouge-l": {
          "r": 0.22857142857142856,
          "p": 0.27586206896551724,
          "f": 0.24999999504394538
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2412.17005v1",
      "true_abstract": "This study examines the phytochemical characteristics of Ayurvedic products.\nAn analysis was performed on Kottakkal Ayurveda Triphala (T), Kottakkal\nAyurveda Hinguvachadi Churnam (H), and Kottakkal Ayurveda Jirakadyarishtam (J)\nusing GC-MS and LC-MS techniques to determine their bioactive constituents,\nwhile also assessing their antimicrobial, docking, anticancer, and\nanti-diabetic activities. The GC-MS analysis identified 30, 45, and 8 chemical\ncomponents in Kottakkal Ayurveda Triphala (T), Kottakkal Ayurveda Hinguvachadi\nChurnam (H), and Kottakkal Ayurveda Jirakadyarishtam (J), respectively. The\nLC-MS analysis produced 15, 20, and 16 peaks for Kottakkal Ayurveda Triphala\n(T), Kottakkal Ayurveda Hinguvachadi Churnam (H), and Kottakkal Ayurveda\nJirakadyarishtam (J), with m/z values of 982, 981, 972, and 933; 987, 985, 974,\nand 945; and 969, 965, 951, and 941, respectively, confirming their precision.\nMoreover, characterization of the Ayurvedic products was carried out using\nFT-IR, UV-vis, and 1H-NMR spectroscopy to identify significant functional\ngroups and chemical substances. Kottakkal Ayurveda Triphala (T) was evaluated\nfor antibacterial activity against Gram-positive bacteria (Streptococcus\npneumoniae and Staphylococcus aureus) along with Gram-negative bacteria\n(Escherichia coli and Klebsiella pneumoniae), yielding a P value of 0.0650 (P <\n0.0001). Both Kottakkal Ayurveda Hinguvachadi Churnam (H) and Kottakkal\nAyurveda Jirakadyarishtam (J) were subjected to analysis for their\neffectiveness against Aspergillus niger and Aspergillus fumigatus, also\nrevealing a P value within the acceptable range of 0.0650 (P < 0.0001). The\nanti-diabetic properties of Kottakkal Ayurveda Triphala (T) were assessed using\nthe {\\alpha}-glucosidase inhibitory method, which exhibited a significant\ninhibitory effect on {\\alpha}-glucosidase, resulting in an average P value of\n0.001 (P < 0.0001).",
      "generated_abstract": "The current understanding of the developmental processes of the vertebrate\nspecies is still very limited. The lack of detailed and accurate information on\nthe molecular mechanisms involved in the formation of the nervous system is\none of the most important issues. A comprehensive understanding of the\ndevelopment of the nervous system is essential for the development of the\nneural stem cell-based therapy. In this review, we mainly discuss the\ndevelopmental mechanisms of the vertebrate neural stem cells and the\ndevelopmental mechanisms of the vertebrate neural tissues. In addition, we\ndiscuss the molecular mechanisms of the development of the vertebrate nervous\nsystem and the development of the neural tissues. Finally, we summarize the\ncurrent progress of the developmental mechanisms of the vertebrate neural\nstem cells and the developmental mechanisms of the vertebrate neural tissues.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.050359712230215826,
          "p": 0.13725490196078433,
          "f": 0.07368420659889219
        },
        "rouge-2": {
          "r": 0.0049261083743842365,
          "p": 0.013157894736842105,
          "f": 0.0071684548173863456
        },
        "rouge-l": {
          "r": 0.050359712230215826,
          "p": 0.13725490196078433,
          "f": 0.07368420659889219
        }
      }
    },
    {
      "paper_id": "cond-mat.supr-con.cond-mat/stat-mech/2503.10146v1",
      "true_abstract": "We find nonequilibrium phase transitions accompanied by multiple (nested)\nhysteresis behaviors in superconductors coupled to baths under a time-periodic\nlight driving.The transitions are demonstrated with a full phase diagram in the\ndomain of the driving amplitude and frequency by means of the Floquet many-body\ntheory. In the weak driving regime with a frequency smaller than half of the\nsuperconducting gap, excited quasiparticles are accumulated at the far edges of\nthe bands, realizing a distribution reminiscent of the Eliashberg effect, which\nsuddenly becomes unstable in the strong driving regime due to\nmulti-photon-assisted tunneling across the gap mediated by the in-gap Floquet\nsidebands. We also show that superconductivity is enhanced in the weak driving\nregime without effective cooling, which is attributed to the modulation of the\nspectrum due to Floquet sidebands.",
      "generated_abstract": "We present a simple, non-perturbative, and non-perturbative-like approach\nto the study of the dynamical properties of quantum critical points. We\ndemonstrate that the classical trajectory of quantum critical points can be\ndetermined from the quantum critical point itself by means of a non-perturbative\nprocedure. We illustrate the method by means of a quantum critical point of\nthe Ising model, which we find to be at a quantum critical point. Our approach\nenables us to determine the quantum critical point of the Ising model in the\nstrong-coupling regime, which is not accessible to perturbative methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17073170731707318,
          "p": 0.2692307692307692,
          "f": 0.20895521913120974
        },
        "rouge-2": {
          "r": 0.05217391304347826,
          "p": 0.08108108108108109,
          "f": 0.06349205872735962
        },
        "rouge-l": {
          "r": 0.12195121951219512,
          "p": 0.19230769230769232,
          "f": 0.14925372659389638
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.00326v1",
      "true_abstract": "BART (Bayesian additive regression trees) has been established as a leading\nsupervised learning method, particularly in the field of causal inference. This\npaper explores the use of BART models for learning conditional average\ntreatment effects (CATE) from regression discontinuity designs, where treatment\nassignment is based on whether an observed covariate (called the running\nvariable) exceeds a pre-specified threshold. A purpose-built version of BART\nthat uses linear regression leaf models (of the running variable and treatment\nassignment dummy) is shown to out-perform off-the-shelf BART implementations as\nwell as a local polynomial regression approach and a CART-based approach. The\nnew method is evaluated in thorough simulation studies as well as an empirical\napplication looking at the effect of academic probation on student performance.",
      "generated_abstract": "se of this paper is to analyze the relationship between the\nrandom variables $X_1,X_2,\\dots,X_n$ with the joint density function $f(x_1,\nx_2,\\dots,x_n)$ and the random variable $Y$ with the marginal density function\n$g(y)$, given the values of the random variables $X_1,X_2,\\dots,X_n$. The\ncorresponding problem is called the joint distributional regression problem. We\npresent the joint density function and its Fourier series representation and\nestimate the parameters of the density function. The maximum likelihood\nestimator of the density function is obtained. The efficiency of the estimator\nis analyzed. The influence of the sample size is studied. The problem is\nsolved for the case of the linear density function. We consider the\nregression problem with the logarithmic density function. The problem is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12643678160919541,
          "p": 0.19642857142857142,
          "f": 0.1538461490811288
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09195402298850575,
          "p": 0.14285714285714285,
          "f": 0.11188810712308692
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.18662v1",
      "true_abstract": "The peer-review process is broken and the problem is getting worse,\nespecially in AI: large conferences like NeurIPS increasingly struggle to\nadequately review huge numbers of paper submissions. I propose a scalable\nsolution that, foremost, recognizes reviewing as important, necessary,\n\\emph{work} and rewards it with crypto-coins owned and managed by the\nconferences themselves. The idea is at its core quite simple: paper submissions\nrequire work (reviews, meta-reviews, etc.) to be done, and therefore the\nsubmitter must pay for that work. Each reviewer submits their review to be\napproved by some designated conference officer (e.g. PC chair, Area Chair,\netc.), and upon approval is paid a single coin for a single review. If three\nreviews are required, the cost of submission should be three coins + a tax that\ncovers payments to all the volunteers who organize the conference. After some\none-time startup costs to fairly distribute coins, the process should be\nrelatively stable with new coins minted only when a conference grows.",
      "generated_abstract": "r explores the role of incentives in the formation of a consensus\nin a multidisciplinary team of researchers. The authors analyze how\nconsensus-forming processes can be influenced by the presence of incentives in a\nteam of researchers. They use a game-theoretical approach to investigate how\nincentives influence the formation of a consensus and the subsequent\ndissemination of research results. The study focuses on the emergence of a\nconsensus in the form of a shared understanding of a complex problem and its\nsolution. The authors show that incentives can influence the formation of\nconsensus, but only in moderate amounts. They conclude that the formation of\nconsensus is a complex process that is influenced by the individual preferences\nof the team members. The results of the study provide valuable insights into the\nprocess of consensus formation in multidisciplinary research teams and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13333333333333333,
          "p": 0.24615384615384617,
          "f": 0.1729729684149015
        },
        "rouge-2": {
          "r": 0.01875,
          "p": 0.026785714285714284,
          "f": 0.022058818685122168
        },
        "rouge-l": {
          "r": 0.11666666666666667,
          "p": 0.2153846153846154,
          "f": 0.1513513467932799
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.02118v2",
      "true_abstract": "An increase in availability of Software Defined Radios (SDRs) has caused a\ndramatic shift in the threat landscape of legacy satellite systems, opening\nthem up to easy spoofing attacks by low-budget adversaries. Physical-layer\nauthentication methods can help improve the security of these systems by\nproviding additional validation without modifying the space segment. This paper\nextends previous research on Radio Frequency Fingerprinting (RFF) of satellite\ncommunication to the Orbcomm satellite formation. The GPS and Iridium\nconstellations are already well covered in prior research, but the feasibility\nof transferring techniques to other formations has not yet been examined, and\nraises previously undiscussed challenges.\n  In this paper, we collect a novel dataset containing 8992474 packets from the\nOrbcom satellite constellation using different SDRs and locations. We use this\ndataset to train RFF systems based on convolutional neural networks. We achieve\nan ROC AUC score of 0.53 when distinguishing different satellites within the\nconstellation, and 0.98 when distinguishing legitimate satellites from SDRs in\na spoofing scenario. We also demonstrate the possibility of mixing datasets\nusing different SDRs in different physical locations.",
      "generated_abstract": "r presents a novel system for real-time tracking of targets\nin complex environments using multi-sensor fusion. The proposed system is\nbased on a two-stage approach: (i) a high-resolution camera is used for\ninitial target detection, and (ii) a passive radar is used to refine the\ntarget-tracking result. The radar is a passive array consisting of a single\nelement, which is used for the refinement of the initial target detection. The\npassive radar is also used to track targets in the presence of clutter, which\nis often present in complex environments. The proposed system uses a\nstate-of-the-art feature extraction and feature fusion technique, which is\ndesigned to extract the features of the object of interest from the image\nsources and fuse them together, resulting in a more robust and accurate\ntarget-tracking result. The system is evaluated through extensive",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1076923076923077,
          "p": 0.18666666666666668,
          "f": 0.1365853612135635
        },
        "rouge-2": {
          "r": 0.028735632183908046,
          "p": 0.04310344827586207,
          "f": 0.034482753820690326
        },
        "rouge-l": {
          "r": 0.1076923076923077,
          "p": 0.18666666666666668,
          "f": 0.1365853612135635
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.05915v1",
      "true_abstract": "Multilevel regression and poststratification (MRP) is a computationally\nefficient indirect estimation method that can quickly produce improved\npopulation-adjusted estimates with limited data. Recent computational\nadvancements allow efficient, relatively simple, and quick approximate Bayesian\nestimation for MRP. As population health outcomes of interest including\nvaccination uptake are known to have spatial structure, precision may be gained\nby including space in the model. We test a recently proposed spatial MRP method\nthat includes a BYM2 spatial term that smooths across demographics and\ngeographic areas using a large, unrepresentative survey. We produce California\ncounty-level estimates of first-dose COVID-19 vaccination up to June 2021 using\nclassic and spatial MRP models, and poststratify using data from the American\nCommunity Survey (US Census Bureau). We assess validity using reported\nfirst-dose vaccination counts from the Centers for Disease Control (CDC).\nNeither classic nor spatial MRP models performed well, highlighting: 1. spatial\nMRP may be most appropriate for richer data contexts, 2. some demographics in\nthe survey data are over-sampled and -aggregated, producing model\nover-smoothing, and 3. a need for survey producers to share user-representative\nmetrics to better benchmark estimates.",
      "generated_abstract": "In recent years, the development of deep learning has brought significant\nadvances in medical image analysis, particularly in the field of magnetic\nresonance imaging (MRI). However, despite the promising results achieved in\nthis field, there is still a lack of consensus on the optimal parameters for\ntraining deep learning models. This paper aims to address this issue by\nestablishing a systematic method for determining the optimal training parameters\nfor MRI segmentation models. First, we review the various training methods\navailable for deep learning models. We then describe the methodology for\nevaluating the performance of these models. Next, we introduce a novel method\nfor determining the optimal training parameters for MRI segmentation models.\nFinally, we validate the effectiveness of our proposed method on three\nreal-world datasets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0967741935483871,
          "p": 0.15384615384615385,
          "f": 0.11881187644740729
        },
        "rouge-2": {
          "r": 0.005747126436781609,
          "p": 0.009174311926605505,
          "f": 0.00706713307295951
        },
        "rouge-l": {
          "r": 0.0967741935483871,
          "p": 0.15384615384615385,
          "f": 0.11881187644740729
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.05758v1",
      "true_abstract": "Lipreading is an important technique for facilitating human-computer\ninteraction in noisy environments. Our previously developed self-supervised\nlearning method, AV2vec, which leverages multimodal self-distillation, has\ndemonstrated promising performance in speaker-independent lipreading on the\nEnglish LRS3 dataset. However, AV2vec faces challenges such as high training\ncosts and a potential scarcity of audio-visual data for lipreading in languages\nother than English, such as Chinese. Additionally, most studies concentrate on\nspeakerindependent lipreading models, which struggle to account for the\nsubstantial variation in speaking styles across di?erent speakers. To address\nthese issues, we propose a comprehensive approach. First, we investigate\ncross-lingual transfer learning, adapting a pre-trained AV2vec model from a\nsource language and optimizing it for the lipreading task in a target language.\nSecond, we enhance the accuracy of lipreading for specific target speakers\nthrough a speaker adaptation strategy, which is not extensively explored in\nprevious research. Third, after analyzing the complementary performance of\nlipreading with lip region-of-interest (ROI) and face inputs, we introduce a\nmodel ensembling strategy that integrates both, signi?cantly boosting model\nperformance. Our method achieved a character error rate (CER) of 77.3% on the\nevaluation set of the ChatCLR dataset, which is lower than the top result from\nthe 2024 Chat-scenario Chinese Lipreading Challenge.",
      "generated_abstract": "vances in audio-to-speech conversion models have significantly\nreduced the voice conversion error rate to less than 0.1% while achieving\nhigh-quality speech, significantly surpassing the original speech-to-speech\nconversion models. This paper presents a new state-of-the-art method for\nvoice conversion, the Speech-to-Speech (SSS) Conversion, which is designed to\nachieve a 10% reduction in the conversion error rate by incorporating a\nmultilingual adaptation module and a multilingual speech synthesis module. The\nmultilingual adaptation module improves the quality of the converted speech by\nadapting the model's parameters to the target language. The multilingual\nspeech synthesis module generates speech from a speech model that is trained on\nthe target language, which ensures that the converted speech sounds natural\nand native to the target language.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14685314685314685,
          "p": 0.29577464788732394,
          "f": 0.19626167780897905
        },
        "rouge-2": {
          "r": 0.03553299492385787,
          "p": 0.06796116504854369,
          "f": 0.046666662157556
        },
        "rouge-l": {
          "r": 0.13986013986013987,
          "p": 0.28169014084507044,
          "f": 0.18691588341645574
        }
      }
    },
    {
      "paper_id": "cs.AI.econ/GN/2502.16879v1",
      "true_abstract": "This paper pioneers a novel approach to economic and public policy analysis\nby leveraging multiple Large Language Models (LLMs) as heterogeneous artificial\neconomic agents. We first evaluate five LLMs' economic decision-making\ncapabilities in solving two-period consumption allocation problems under two\ndistinct scenarios: with explicit utility functions and based on intuitive\nreasoning. While previous research has often simulated heterogeneity by solely\nvarying prompts, our approach harnesses the inherent variations in analytical\ncapabilities across different LLMs to model agents with diverse cognitive\ntraits. Building on these findings, we construct a Multi-LLM-Agent-Based (MLAB)\nframework by mapping these LLMs to specific educational groups and\ncorresponding income brackets. Using interest-income taxation as a case study,\nwe demonstrate how the MLAB framework can simulate policy impacts across\nheterogeneous agents, offering a promising new direction for economic and\npublic policy analysis by leveraging LLMs' human-like reasoning capabilities\nand computational power.",
      "generated_abstract": "er a market with a single buyer and a single seller. The buyer's\nbuying behavior is characterized by an infinite-dimensional stochastic process\nthat follows an unknown non-Markovian transition kernel. The seller's\nproduction and consumption processes are modeled as a linear stochastic\ndifference equation and a linear stochastic first-order differential equation\nrespectively. The buyer's consumption process is assumed to be a martingale and\nthe seller's consumption process is assumed to be a submartingale. The\nseller's consumption process is assumed to be a martingale and the buyer's\nconsumption process is assumed to be a submartingale. We characterize the\nstationary distribution of the buyer's consumption process and analyze its\nstochastic domination property with respect to the stationary distribution of\nthe seller's consumption process. Our analysis is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09433962264150944,
          "p": 0.17857142857142858,
          "f": 0.12345678559975631
        },
        "rouge-2": {
          "r": 0.007407407407407408,
          "p": 0.012195121951219513,
          "f": 0.009216585160018538
        },
        "rouge-l": {
          "r": 0.09433962264150944,
          "p": 0.17857142857142858,
          "f": 0.12345678559975631
        }
      }
    },
    {
      "paper_id": "cs.IT.math/AC/2503.03421v1",
      "true_abstract": "In this paper, we study the unit graph $ G(\\mathbb{Z}_n) $, where $ n $ is of\nthe form $n = p_1^{n_1} p_2^{n_2} \\dots p_r^{n_r}$, with $ p_1, p_2, \\dots, p_r\n$ being distinct prime numbers and $ n_1, n_2, \\dots, n_r $ being positive\nintegers. We establish the connectivity of $ G(\\mathbb{Z}_n) $, show that its\ndiameter is at most three, and analyze its edge connectivity. Furthermore, we\nconstruct $ q $-ary linear codes from the incidence matrix of $ G(\\mathbb{Z}_n)\n$, explicitly determining their parameters and duals. A primary contribution of\nthis work is the resolution of two conjectures from \\cite{Jain2023} concerning\nthe structural and coding-theoretic properties of $ G(\\mathbb{Z}_n) $. These\nresults extend the study of algebraic graph structures and highlight the\ninterplay between number theory, graph theory, and coding theory.",
      "generated_abstract": "er the problem of designing optimal synchronization protocols to\nensure that a number of identical time-varying wireless nodes are in synchrony.\nThe nodes are assumed to be equipped with a limited number of available\ncommunication slots, and the goal is to schedule the available slots so as to\nensure that the nodes synchronize with each other. We focus on the case where\nthe nodes are equipped with a limited number of available communication slots,\nand we consider both the case where the communication slots are controllable by\nthe nodes and the case where the communication slots are constrained by a\ngiven maximum communication time. We show that, in the case where the nodes\nare equipped with a limited number of available communication slots, the\noptimality of a given synchronization protocol can be characterized by\nevaluating the asymptotic performance of the synchronization protocol. We also\nshow that, in the case",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12643678160919541,
          "p": 0.1746031746031746,
          "f": 0.14666666179466684
        },
        "rouge-2": {
          "r": 0.008,
          "p": 0.00980392156862745,
          "f": 0.008810567738557767
        },
        "rouge-l": {
          "r": 0.11494252873563218,
          "p": 0.15873015873015872,
          "f": 0.13333332846133353
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/DC/2503.09799v1",
      "true_abstract": "As we scale to more massive machine learning models, the frequent\nsynchronization demands inherent in data-parallel approaches create significant\nslowdowns, posing a critical challenge to further scaling. Recent work develops\nan approach (DiLoCo) that relaxes synchronization demands without compromising\nmodel quality. However, these works do not carefully analyze how DiLoCo's\nbehavior changes with model size. In this work, we study the scaling law\nbehavior of DiLoCo when training LLMs under a fixed compute budget. We focus on\nhow algorithmic factors, including number of model replicas, hyperparameters,\nand token budget affect training in ways that can be accurately predicted via\nscaling laws. We find that DiLoCo scales both predictably and robustly with\nmodel size. When well-tuned, DiLoCo scales better than data-parallel training\nwith model size, and can outperform data-parallel training even at small model\nsizes. Our results showcase a more general set of benefits of DiLoCo than\npreviously documented, including increased optimal batch sizes, improved\ndownstream generalization with scale, and improved evaluation loss for a fixed\ntoken budget.",
      "generated_abstract": "ence of autonomous driving has led to a surge in the demand for\ncomputer vision (CV) models. However, the increased computational requirements\nof modern CV models for real-time inference pose a significant challenge in\ndeploying them on edge devices. In this paper, we present a novel framework for\ncomputational efficiency enhancement of the proposed self-driving\nsystem. Our approach integrates a multi-level pruning technique with a\ncomprehensive loss-based optimization framework. The proposed method enables\nthe training of the model at different levels, ranging from high-level\narchitecture to low-level inference, thereby enhancing the efficiency of the\nsystem. Our evaluation on the self-driving system demonstrates that the\npruning-based framework outperforms the conventional loss-based optimization\nmethod in terms of both inference speed and model accuracy. The performance\nimprovement is especially pronounced at lower resolutions",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2033898305084746,
          "p": 0.2696629213483146,
          "f": 0.23188405306914991
        },
        "rouge-2": {
          "r": 0.006369426751592357,
          "p": 0.008064516129032258,
          "f": 0.007117432791381439
        },
        "rouge-l": {
          "r": 0.1864406779661017,
          "p": 0.24719101123595505,
          "f": 0.21256038157156537
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.11462v1",
      "true_abstract": "Deep learning based end-to-end multi-channel speech enhancement methods have\nachieved impressive performance by leveraging sub-band, cross-band, and spatial\ninformation. However, these methods often demand substantial computational\nresources, limiting their practicality on terminal devices. This paper presents\na lightweight multi-channel speech enhancement network with decoupled fully\nconnected attention (LMFCA-Net). The proposed LMFCA-Net introduces time-axis\ndecoupled fully-connected attention (T-FCA) and frequency-axis decoupled\nfully-connected attention (F-FCA) mechanisms to effectively capture long-range\nnarrow-band and cross-band information without recurrent units. Experimental\nresults show that LMFCA-Net performs comparably to state-of-the-art methods\nwhile significantly reducing computational complexity and latency, making it a\npromising solution for practical applications.",
      "generated_abstract": "r presents a new method to generate synthetic speech data for\nlearning to generate speech. This method, called MUSGAN-2-TTS, is based on\nthe MUSGAN-2 model, which is a state-of-the-art method for generating speech\nsynthesis. The MUSGAN-2-TTS method introduces several modifications to the\noriginal MUSGAN-2 model, including adding a text-to-speech (TTS) module,\nintroducing a new text-to-speech model, and incorporating a TTS model to the\noriginal TTS model. These modifications are designed to enhance the\ngeneration of speech data for TTS training and improve the quality of\ngenerated speech. The results of the experiments show that the MUSGAN-2-TTS\nmethod generates high-quality speech data, which is useful for training\nTTS models and generating speech",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1951219512195122,
          "p": 0.26229508196721313,
          "f": 0.22377621888405314
        },
        "rouge-2": {
          "r": 0.021052631578947368,
          "p": 0.020202020202020204,
          "f": 0.020618551703157764
        },
        "rouge-l": {
          "r": 0.18292682926829268,
          "p": 0.2459016393442623,
          "f": 0.20979020489803915
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.hep-ex/2503.09420v1",
      "true_abstract": "Diamond's exceptional properties make it highly suited for applications in\nchallenging radiation environments. Understanding radiation-induced damage in\ndiamond is crucial for enabling its practical applications and advancing\nmaterials science. However, direct imaging of radiation-induced crystal defects\nat the atomic scale remains rare due to diamond's compact lattice structure.\nHere, we report the atomic-level characterization of crystal defects induced by\nhigh-flux fast neutron radiation (up to $3 \\times10^{17}$ n/$cm^2$) in\nsingle-crystal chemical vapor deposition diamonds. Through Raman spectroscopy,\nthe phase transition from carbon $sp^3$ to $sp^2$ hybridization was identified,\nprimarily associated with the formation of dumbbell-shaped interstitial\ndefects. Using electron energy loss spectroscopy and aberration-corrected\ntransmission electron microscopy, we observed a clustering trend in defect\ndistribution, where $sp^2$ rich clusters manifested as dislocation structures\nwith a density up to $10^{14}$ $cm^{-2}$. Lomer-Cottrell junctions were\nidentified, offering a possible explanation for defect cluster formation.\nRadiation-induced point defects were found to be dispersed throughout the\ndiamond lattice, highlighting the widespread nature of primary defect\nformation. Vacancy defects, along with $\\langle 111 \\rangle$ and $\\langle 100\n\\rangle$ oriented dumbbell-shaped interstitial defects induced by high-dose\nneutron irradiation, were directly imaged, providing microscopic structural\nevidence that complements spectroscopic studies of point defects. Dynamical\nsimulations combined with an adiabatic recombination-based damage model\nprovided insights into the correlation between irradiation dose and resulting\ncrystal damage. These findings advance our understanding of neutron-induced\ndamage mechanisms in diamond and contribute to the development of\nradiation-resistant diamond materials.",
      "generated_abstract": "t a comprehensive investigation of the structure, electronic\nenergy, and magnetic properties of 2D and 3D metallic MoS2 single crystals\nobtained by the in-situ PLD technique at low temperatures. We focus on\nquantifying the crystal structure, electronic band structure, and magnetic\nproperties using X-ray diffraction, energy dispersive X-ray spectroscopy, and\nmagnetic susceptibility measurements. The structure of the crystals is\nwell-defined and consists of hexagonal MoS2 layers with a 2D hexagonal lattice\nwith unit cell parameters of a = b = 5.138(5) \u00c5 and c = 25.801(2) \u00c5. The\nelectronic band structure shows a strong gap of 0.66 eV at the Fermi level,\nconfirming the Dirac point,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08284023668639054,
          "p": 0.19718309859154928,
          "f": 0.11666666250034737
        },
        "rouge-2": {
          "r": 0.008733624454148471,
          "p": 0.0196078431372549,
          "f": 0.012084587881090576
        },
        "rouge-l": {
          "r": 0.07692307692307693,
          "p": 0.18309859154929578,
          "f": 0.10833332916701405
        }
      }
    },
    {
      "paper_id": "cs.DL.stat/OT/2405.20156v1",
      "true_abstract": "This paper seeks to bridge the gap between archival text analysis and network\nanalysis by applying network clustering methods to analyze the coverage of\nBulgaria in 123 issues of the newspaper Osservatore Romano published between\nJanuary and May 1877. Utilizing optical character recognition and generalized\nhomogeneity blockmodeling, the study constructs networks of relevant keywords.\nThose including the sets Bulgaria and Russia are rather isomorphic and they\nlargely overlap with those for Germany, Britain, and War. In structural terms,\nthe blockmodel of the two networks exhibits a clear\ncore-semiperiphery-periphery structure that reflects relations between concepts\nin the newpaper's coverage. The newspaper's lexical choices effectively\ndelegitimised the Bulgarian national revival, highlighting the influence of the\nHoly See on the newspaper's editorial line.",
      "generated_abstract": "ntext of large-scale data analysis, it is essential to have a\nunderstanding of the statistical properties of the data to be analyzed.\nStatistical properties are usually defined in terms of measures, which are\nindependent and identically distributed random variables. We define the\nfollowing measures for large-scale data: the empirical distribution function,\nthe empirical cumulative distribution function, the empirical quantile\nfunction, and the empirical distribution function of the empirical distribution\nfunction. We show that the empirical distribution function and the\nempirical quantile function are asymptotically normal in the sense that they\nconverge in distribution to the corresponding asymptotically normal random\nvariables. Moreover, the empirical quantile function is asymptotically\nindependent of the sample size. We show that the empirical cumulative distribution\nfunction is asymptotically central for a large class of distributions. We show\nthat the empirical",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.1724137931034483,
          "f": 0.13513513036888256
        },
        "rouge-2": {
          "r": 0.017094017094017096,
          "p": 0.020618556701030927,
          "f": 0.01869158382871998
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.1724137931034483,
          "f": 0.13513513036888256
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/SC/2312.05876v3",
      "true_abstract": "Some proteins can find their targets on DNA faster than by pure diffusion in\nthe three-dimensional cytoplasm, through the process of facilitated diffusion:\nThey can loosely bind to DNA and temporarily slide along it, thus being guided\nby the DNA molecule itself to the target. This chapter examines this process in\nmathematical detail with a focus on including the effect of DNA coiling on the\nsearch process.",
      "generated_abstract": "eld of drug discovery, the use of computational methods for\nchemical space exploration is gaining popularity. However, a critical\nchallenge remains: the reliance on manually curated databases to generate\nbenchmarks, which can be time-consuming and difficult to scale. Here, we\nintroduce the Chemical Benchmarking Dataset, a novel database of 3D-coordinates\nof 3,591 compounds, designed to address these limitations. Our dataset\nencompasses a diverse range of organic compounds, including small molecules,\ndrugs, and natural products. We further developed an automated method to\ngenerate benchmarks from this dataset using an innovative, data-driven\napproach. This method was validated through extensive benchmarking of 10,000\nrandomly selected compounds. The results demonstrate that our approach\nachieves an accuracy of 87",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22,
          "p": 0.11827956989247312,
          "f": 0.15384614929825435
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.18,
          "p": 0.0967741935483871,
          "f": 0.12587412132622638
        }
      }
    },
    {
      "paper_id": "cs.AR.eess/SY/2503.04581v1",
      "true_abstract": "Most Wearable Ultrasound (WUS) devices lack the computational power to\nprocess signals at the edge, instead relying on remote offload, which\nintroduces latency, high power consumption, and privacy concerns. We present\nMaestro, a RISC-V SoC with unified Vector-Tensor Unit (VTU) and memory-coupled\nFast Fourier Transform (FFT) accelerators targeting edge processing for\nwearable ultrasound devices, fabricated using low-cost TSMC 65nm CMOS\ntechnology. The VTU achieves peak 302GFLOPS/W and 19.8GFLOPS at FP16, while the\nmulti-precision 16/32-bit floating-point FFT accelerator delivers peak\n60.6GFLOPS/W and 3.6GFLOPS at FP16, We evaluate Maestro on a US-based gesture\nrecognition task, achieving 1.62GFLOPS in signal processing at 26.68GFLOPS/W,\nand 19.52GFLOPS in Convolutional Neural Network (CNN) workloads at\n298.03GFLOPS/W. Compared to a state-of-the-art SoC with a similar mission\nprofile, Maestro achieves a 5x speedup while consuming only 12mW, with an\nenergy consumption of 2.5mJ in a wearable US channel preprocessing and ML-based\npostprocessing pipeline.",
      "generated_abstract": "This paper introduces a novel approach for multi-cell multi-input multi-output\n(MIMO) power allocation in a multi-cell orthogonal frequency-division\nmultiplexing (OFDM) system, where each cell in the system has a different\nspectral efficiency (SE). The proposed approach addresses the multi-cell\nMIMO-OFDM system by allocating the received signals from multiple cells to each\nof the user terminals in the system, while ensuring that the total power\nconsumption does not exceed the available power. The problem of multi-cell\nMIMO-OFDM system with limited power allocation is formulated as an optimization\nproblem and is solved through a stochastic gradient descent (SGD) algorithm.\nSimulation results demonstrate that the proposed approach effectively\nallocates the received signals to each of the user terminals while ensuring\nthat the total power consumption does not exceed the available power.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.20270270270270271,
          "f": 0.15463917053884593
        },
        "rouge-2": {
          "r": 0.006711409395973154,
          "p": 0.00980392156862745,
          "f": 0.007968122665357437
        },
        "rouge-l": {
          "r": 0.11666666666666667,
          "p": 0.1891891891891892,
          "f": 0.14432989218833048
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ME/2503.09494v2",
      "true_abstract": "In the era of big data, large-scale, multi-modal datasets are increasingly\nubiquitous, offering unprecedented opportunities for predictive modeling and\nscientific discovery. However, these datasets often exhibit complex\nheterogeneity, such as covariate shift, posterior drift, and missing\nmodalities, that can hinder the accuracy of existing prediction algorithms. To\naddress these challenges, we propose a novel Representation Retrieval ($R^2$)\nframework, which integrates a representation learning module (the representer)\nwith a sparsity-induced machine learning model (the learner). Moreover, we\nintroduce the notion of \"integrativeness\" for representers, characterized by\nthe effective data sources used in learning representers, and propose a\nSelective Integration Penalty (SIP) to explicitly improve the property.\nTheoretically, we demonstrate that the $R^2$ framework relaxes the conventional\nfull-sharing assumption in multi-task learning, allowing for partially shared\nstructures, and that SIP can improve the convergence rate of the excess risk\nbound. Extensive simulation studies validate the empirical performance of our\nframework, and applications to two real-world datasets further confirm its\nsuperiority over existing approaches.",
      "generated_abstract": "We propose a novel framework for designing learning-based algorithms for\ncomplex stochastic control problems with a linear state-action dynamics,\nunderstanding the role of state-dependent noise. In particular, we focus on\nthe case of a discrete-time linear stochastic differential equation, and\ndescribe the main challenges arising from this setting. We present a novel\nframework for designing algorithms based on the stochastic gradient method,\nwithin the framework of a general stochastic control problem. The algorithm\nproposed is based on a recursive recursion, and is shown to be\noptimality-preserving under mild assumptions. We also propose a novel\noptimization-based approach for the problem of minimizing the expected total\ncost of control, which is shown to be equivalent to solving the\nstochastic-gradient-based problem. We provide theoretical guarantees for both\nmethods, and demonstrate the effectiveness of our approach in simulation and\nnumerical experiments.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15702479338842976,
          "p": 0.2261904761904762,
          "f": 0.18536584882141593
        },
        "rouge-2": {
          "r": 0.02531645569620253,
          "p": 0.032520325203252036,
          "f": 0.028469745967250507
        },
        "rouge-l": {
          "r": 0.1322314049586777,
          "p": 0.19047619047619047,
          "f": 0.15609755613848914
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.07934v1",
      "true_abstract": "Counterfactual explanations indicate the smallest change in input that can\ntranslate to a different outcome for a machine learning model. Counterfactuals\nhave generated immense interest in high-stakes applications such as finance,\neducation, hiring, etc. In several use-cases, the decision-making process often\nrelies on an ensemble of models rather than just one. Despite significant\nresearch on counterfactuals for one model, the problem of generating a single\ncounterfactual explanation for an ensemble of models has received limited\ninterest. Each individual model might lead to a different counterfactual,\nwhereas trying to find a counterfactual accepted by all models might\nsignificantly increase cost (effort). We propose a novel strategy to find the\ncounterfactual for an ensemble of models using the perspective of entropic risk\nmeasure. Entropic risk is a convex risk measure that satisfies several\ndesirable properties. We incorporate our proposed risk measure into a novel\nconstrained optimization to generate counterfactuals for ensembles that stay\nvalid for several models. The main significance of our measure is that it\nprovides a knob that allows for the generation of counterfactuals that stay\nvalid under an adjustable fraction of the models. We also show that a limiting\ncase of our entropic-risk-based strategy yields a counterfactual valid for all\nmodels in the ensemble (worst-case min-max approach). We study the trade-off\nbetween the cost (effort) for the counterfactual and its validity for an\nensemble by varying degrees of risk aversion, as determined by our risk\nparameter knob. We validate our performance on real-world datasets.",
      "generated_abstract": "r proposes a novel modeling framework for the analysis of multi-\nmodel, multi-agent, and multi-objective control problems. The framework\nconsiders a set of models (representing different levels of abstraction) and\nmultiple agents, each with its own objective, to address a complex control\nproblem. The proposed modeling framework is designed to handle both the\ntraditional and the extended multi-objective control problem (i.e.,\noptimization over multiple objectives). The proposed framework is designed to\nenable the identification of the optimal set of models and agents. To address\nthe challenge of identifying the optimal set of models and agents, we\nintroduce a novel objective function that integrates the constraints of\noptimality, stability, and fairness. The objective function is derived from\nthe concept of model diversity and is designed to address the challenges of\nidentifying the optimal set of models and agents. The proposed model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1323529411764706,
          "p": 0.25,
          "f": 0.173076918550296
        },
        "rouge-2": {
          "r": 0.0182648401826484,
          "p": 0.03571428571428571,
          "f": 0.02416917981252536
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.2361111111111111,
          "f": 0.16346153393491136
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.04941v2",
      "true_abstract": "Assessing the economic impacts of artificial intelligence requires\nintegrating insights from both computer science and economics. We present the\nGrowth and AI Transition Endogenous model (GATE), a dynamic integrated\nassessment model that simulates the economic effects of AI automation. GATE\ncombines three key ingredients that have not been brought together in previous\nwork: (1) a compute-based model of AI development, (2) an AI automation\nframework, and (3) a semi-endogenous growth model featuring endogenous\ninvestment and adjustment costs. The model allows users to simulate the\neconomic effects of the transition to advanced AI across a range of potential\nscenarios. GATE captures the interactions between economic variables, including\ninvestment, automation, innovation, and growth, as well as AI-related inputs\nsuch as compute and algorithms. This paper explains the model's structure and\nfunctionality, emphasizing AI development for economists and economic modeling\nfor the AI community. The model is implemented in an interactive sandbox,\nenabling users to explore the impact of AI under different parameter choices\nand policy interventions. The modeling sandbox is available at:\nwww.epoch.ai/GATE.",
      "generated_abstract": "y investigates the determinants of economic development in\nHong Kong SAR, China. Using panel data from 2016 to 2022, the findings reveal\nthat economic development is mainly driven by the economic growth of\nforeign-owned enterprises and the domestic market. The effects of foreign\ninvestment, economic growth, and the domestic market on economic development\nare significant and positive, while the effects of the domestic market on\neconomic development are significant and negative. The findings indicate that\neconomic growth and the domestic market have positive effects on economic\ndevelopment, while economic growth and the foreign-owned enterprises have\nnegative effects. The results also show that economic development is driven by\nthe economic growth of foreign-owned enterprises, the domestic market, and the\neconomic growth of the domestic market. The domestic market is more\nimportant than the domestic market in the economic development",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1452991452991453,
          "p": 0.3090909090909091,
          "f": 0.19767441425432675
        },
        "rouge-2": {
          "r": 0.018404907975460124,
          "p": 0.03333333333333333,
          "f": 0.02371541043603332
        },
        "rouge-l": {
          "r": 0.11965811965811966,
          "p": 0.2545454545454545,
          "f": 0.1627906933240942
        }
      }
    },
    {
      "paper_id": "math.FA.math/FA/2503.08590v1",
      "true_abstract": "In this paper we study the essential spectra of the Toeplitz operator on the\nHardy space $H^1$. We give a counterexample to show that the Toeplitz operator\nwith symbol is not Fredholm, which gives a counterexample to the conjecture by\nJ.A. Virtanen J A in 2006.",
      "generated_abstract": "We prove that the family of all $p$-adic valuations on the rationals is a\nstructure\n  $(\\mathcal{V}_p,\\mathsf{Val})$. This is the first example of a family of\nstructures that has a canonical structure of a structure. The key is to show\nthat the family of all $p$-adic valuations is a structure. In the case of\n$p$-adic numbers, we give a characterization of the structure $\\mathcal{V}_p$\nand prove that it is a structure. In the case of the rationals, we give a\ncharacterization of the structure $\\mathsf{Val}$ and prove that it is a\nstructure. As a corollary, we obtain a characterization of the structure of\nthe $p$-adic valuation ring of $\\mathcal{V}_p$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3333333333333333,
          "p": 0.2926829268292683,
          "f": 0.3116883067093945
        },
        "rouge-2": {
          "r": 0.14634146341463414,
          "p": 0.08571428571428572,
          "f": 0.10810810344939555
        },
        "rouge-l": {
          "r": 0.3333333333333333,
          "p": 0.2926829268292683,
          "f": 0.3116883067093945
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.07940v1",
      "true_abstract": "Recent advances in deep learning-based point cloud registration have improved\ngeneralization, yet most methods still require retraining or manual parameter\ntuning for each new environment. In this paper, we identify three key factors\nlimiting generalization: (a) reliance on environment-specific voxel size and\nsearch radius, (b) poor out-of-domain robustness of learning-based keypoint\ndetectors, and (c) raw coordinate usage, which exacerbates scale discrepancies.\nTo address these issues, we present a zero-shot registration pipeline called\nBUFFER-X by (a) adaptively determining voxel size/search radii, (b) using\nfarthest point sampling to bypass learned detectors, and (c) leveraging\npatch-wise scale normalization for consistent coordinate bounds. In particular,\nwe present a multi-scale patch-based descriptor generation and a hierarchical\ninlier search across scales to improve robustness in diverse scenes. We also\npropose a novel generalizability benchmark using 11 datasets that cover various\nindoor/outdoor scenarios and sensor modalities, demonstrating that BUFFER-X\nachieves substantial generalization without prior information or manual\nparameter tuning for the test datasets. Our code is available at\nhttps://github.com/MIT-SPARK/BUFFER-X.",
      "generated_abstract": "video processing is essential for applications such as\neconomic surveillance, public safety, and military security. However,\nconventional image processing methods struggle to meet the requirements of\nreal-time video processing due to their high computational complexity and\nlarge memory footprint. To address these challenges, we propose a new video\nprocessing method based on the Neural Radiance Field (NeRF) framework. NeRF\nuses the implicit representation of the scene to estimate the scene geometry\nand lighting, which enables efficient and real-time rendering. By integrating\nNeRF with a deep learning-based model, we achieve real-time video processing.\nExperiments on the synthetic data demonstrate that our method can process\nreal-time videos with high accuracy and efficiency, and the results show that\nthe proposed method can achieve real-time video processing at a frame rate of\n30 FPS. Additionally, we analyze the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15625,
          "p": 0.2222222222222222,
          "f": 0.18348623368403347
        },
        "rouge-2": {
          "r": 0.025974025974025976,
          "p": 0.032520325203252036,
          "f": 0.028880861488616624
        },
        "rouge-l": {
          "r": 0.15625,
          "p": 0.2222222222222222,
          "f": 0.18348623368403347
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PR/2409.06289v1",
      "true_abstract": "Despite significant progress in deep learning for financial trading, existing\nmodels often face instability and high uncertainty, hindering their practical\napplication. Leveraging advancements in Large Language Models (LLMs) and\nmulti-agent architectures, we propose a novel framework for quantitative stock\ninvestment in portfolio management and alpha mining. Our framework addresses\nthese issues by integrating LLMs to generate diversified alphas and employing a\nmulti-agent approach to dynamically evaluate market conditions. This paper\nproposes a framework where large language models (LLMs) mine alpha factors from\nmultimodal financial data, ensuring a comprehensive understanding of market\ndynamics. The first module extracts predictive signals by integrating numerical\ndata, research papers, and visual charts. The second module uses ensemble\nlearning to construct a diverse pool of trading agents with varying risk\npreferences, enhancing strategy performance through a broader market analysis.\nIn the third module, a dynamic weight-gating mechanism selects and assigns\nweights to the most relevant agents based on real-time market conditions,\nenabling the creation of an adaptive and context-aware composite alpha formula.\nExtensive experiments on the Chinese stock markets demonstrate that this\nframework significantly outperforms state-of-the-art baselines across multiple\nfinancial metrics. The results underscore the efficacy of combining\nLLM-generated alphas with a multi-agent architecture to achieve superior\ntrading performance and stability. This work highlights the potential of\nAI-driven approaches in enhancing quantitative investment strategies and sets a\nnew benchmark for integrating advanced machine learning techniques in financial\ntrading can also be applied on diverse markets.",
      "generated_abstract": "We propose a novel approach to the problem of optimal control in\nmarket-based portfolio management. In particular, we show that a\nreinforcement learning agent can be trained to dynamically allocate a given\nportfolio of assets to a set of risk-neutral market portfolios. This enables\nthe agent to dynamically adapt its allocations based on the market's current\nconditions, providing a framework for risk-based portfolio management.\nSpecifically, we demonstrate that this approach allows the agent to achieve\noptimal risk-return profiles, as well as minimize transaction costs and\nvolatility. We validate our approach through numerical simulations in the\ncontext of a general portfolio problem, demonstrating that the proposed\nframework provides a viable alternative to more traditional approaches in\nportfolio management.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18787878787878787,
          "p": 0.3974358974358974,
          "f": 0.2551440285627191
        },
        "rouge-2": {
          "r": 0.0635593220338983,
          "p": 0.13513513513513514,
          "f": 0.0864553270609342
        },
        "rouge-l": {
          "r": 0.17575757575757575,
          "p": 0.3717948717948718,
          "f": 0.2386831232129249
        }
      }
    },
    {
      "paper_id": "q-bio.SC.q-bio/SC/2308.13877v2",
      "true_abstract": "In the modern world, technology is at its peak. Different avenues in\nprogramming and technology have been explored for data analysis, automation,\nand robotics. Machine learning is key to optimize data analysis, make accurate\npredictions, and hasten/improve existing functions. Thus, presently, the field\nof machine learning in artificial intelligence is being developed and its uses\nin varying fields are being explored. One field in which its uses stand out is\nthat of microbial biosynthesis. In this paper, a comprehensive overview of the\ndiffering machine learning programs used in biosynthesis is provided, alongside\nbrief descriptions of the fields of machine learning and microbial biosynthesis\nseparately. This information includes past trends, modern developments, future\nimprovements, explanations of processes, and current problems they face. Thus,\nthis paper's main contribution is to distill developments in, and provide a\nholistic explanation of, 2 key fields and their applicability to improve\nindustry/research. It also highlights challenges and research directions,\nacting to instigate more research and development in the growing fields.\nFinally, the paper aims to act as a reference for academics performing\nresearch, industry professionals improving their processes, and students\nlooking to understand the concept of machine learning in biosynthesis.",
      "generated_abstract": "metabolism is a fundamental process that shapes organismal fitness,\nand it is a primary target for the development of antibiotics. Despite\nimportant advances in metabolic profiling techniques, the mechanisms underlying\nthe development of antibiotic resistance remain poorly understood. Here, we\nemploy a combination of high-throughput metabolomics and computational\nanalyses to identify key metabolic pathways and metabolites that are\nassociated with antibiotic resistance. Our results reveal that the metabolic\nsignatures associated with antibiotic resistance are associated with increased\nproduction of 3-hydroxy-3-methylglutaryl-coenzyme A reductase (HMGR), an\nenzyme involved in the metabolism of hydrogen, which is a potent\nantibiotic. Furthermore, we identify that the metabol",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10655737704918032,
          "p": 0.19696969696969696,
          "f": 0.13829786778406533
        },
        "rouge-2": {
          "r": 0.0055248618784530384,
          "p": 0.011111111111111112,
          "f": 0.007380069364526569
        },
        "rouge-l": {
          "r": 0.09016393442622951,
          "p": 0.16666666666666666,
          "f": 0.1170212720393845
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2502.19213v1",
      "true_abstract": "We consider an optimal investment-consumption problem for a\nutility-maximizing investor who has access to assets with different liquidity\nand whose consumption rate as well as terminal wealth are subject to\nlower-bound constraints. Assuming utility functions that satisfy standard\nconditions, we develop a methodology for deriving the optimal strategies in\nsemi-closed form. Our methodology is based on the generalized martingale\napproach and the decomposition of the problem into subproblems. We illustrate\nour approach by deriving explicit formulas for agents with power-utility\nfunctions and discuss potential extensions of the proposed framework.",
      "generated_abstract": "aper, we revisit the problem of pricing American options on a\nhistorical market with a continuous-time Black-Scholes model. While the\nstandard Black-Scholes model assumes a constant volatility, we consider a\ndynamic volatility model with volatility that changes over time. We develop a\ndynamic pricing framework where the underlying dynamics are incorporated into\nthe Black-Scholes model. We then develop an algorithm for the dynamic\npricing problem, which is a special case of the continuous-time optimal\ncontrol problem. Our algorithm is based on a numerical method that employs a\nfinite difference scheme for the derivative-free optimization. We develop\nnumerical methods for solving the Black-Scholes model with volatility that\nchanges over time. We compare our algorithm with a numerical method that uses\na finite difference scheme for the Black-Scholes model and an approximation\nmethod for the volatility. Our numerical",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3333333333333333,
          "p": 0.3484848484848485,
          "f": 0.3407407357432099
        },
        "rouge-2": {
          "r": 0.05747126436781609,
          "p": 0.04672897196261682,
          "f": 0.05154638680571841
        },
        "rouge-l": {
          "r": 0.3333333333333333,
          "p": 0.3484848484848485,
          "f": 0.3407407357432099
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2412.08342v1",
      "true_abstract": "We consider an economic environment where a seller wants to sell an\nindivisible unit of good to a buyer. We show that revenue from any\nstrategy-proof and individually rational mechanism defined on closed intervals\nof rich single crossing domains considered in \\citep{Goswami1}, can be\napproximated by the revenue from a sequence of strategy-proof and individually\nrational mechanisms with finite range. Thus while studying optimal mechanisms\nwithout loss of generality we can study mechanisms with finite range.",
      "generated_abstract": "We consider the problem of constructing a Nash equilibrium in a game with\ndifferent types of agents. We show that if the types of agents are\nrepresented by an abelian group, then the problem can be solved in polynomial\ntime. We extend our result to a class of games with more than two types of\nagents. We also consider the problem of finding a Nash equilibrium in a game\nwith a given type of agent. We show that the problem can be solved in polynomial\ntime if the types of agents are represented by an abelian group, and in\nexponential time if the types of agents are represented by a graph. We also\nextend our results to a class of games with more than two types of agents.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26785714285714285,
          "p": 0.3125,
          "f": 0.28846153349112436
        },
        "rouge-2": {
          "r": 0.07352941176470588,
          "p": 0.06944444444444445,
          "f": 0.07142856643265341
        },
        "rouge-l": {
          "r": 0.26785714285714285,
          "p": 0.3125,
          "f": 0.28846153349112436
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.cond-mat/str-el/2503.10330v1",
      "true_abstract": "Motivated by the appearance of Majorana fermions in a broad range of\ncorrelated and topological electronic systems, we develop a general method to\ncompute the dynamical response of interacting Majorana fermions in the\nrandom-phase approximation (RPA). This can be applied self-consistently on top\nof Majorana mean-field theory (MFT) backgrounds, thereby in particular\nproviding a powerful tool to analyse $\\textit{generic}$ behaviour in the\nvicinity of (various heavily studied) exactly soluble models. Prime examples\nare quantum spin liquids (QSL) with emergent Majorana excitations, with the\ncelebrated exact solution of Kitaev. We employ the RPA to study in considerable\ndetail phase structure and dynamics of the extended Kitaev honeycomb\n$KJ\\Gamma$-model, with and without an applied field. First, we benchmark our\nmethod with Kitaev's exactly soluble model, finding a remarkable agreement. The\ninteractions between Majorana fermions even turn out to mimic the effect of\nlocal $\\mathbb{Z}_2$ flux excitations, which we explain analytically. Second,\nwe show how small non-Kitaev couplings $J$ and $\\Gamma$ induce Majorana bound\nstates, resulting in sharp features in the dynamical structure factor in the\npresence of fractionalisation: such 'spinon excitons' naturally appear, and can\ncoexist and interact with the broad Majorana continuum. Third, for increasing\ncouplings or field, our theory predicts instabilities of the KQSL triggered by\nthe condensation of the sharp modes. From the high symmetry momenta of the\ncondensation we can deduce which magnetically ordered phases surround the KQSL,\nin good agreement with previous finite-size numerics. We discuss implications\nfor experiments and the broad range of applicability of our method to other QSL\nand Majorana systems.",
      "generated_abstract": "igate the dynamics of a quantum gas of $N$ bosons confined in a\nexperimentally realistic one-dimensional box. We find that, at zero temperature,\nthe system evolves in a single-particle ground state, which can be mapped to a\ntwo-dimensional (2D) quantum gas. The ground-state energy is determined by the\nrelative overlap between the initial state and the 2D gas, while the particle\nnumber is determined by the initial overlap between the initial state and the\nboson wave function. We further investigate the evolution of the system in the\nthermal bath, finding that the system exhibits non-trivial features such as\nexponential decay of the occupation of the single-particle ground state, and a\nnon-monotonic evolution of the occupation of the ground state in the presence\nof the thermal bath. These results indicate that, in the absence of a\nthermal bath, the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11728395061728394,
          "p": 0.25333333333333335,
          "f": 0.16033754841638637
        },
        "rouge-2": {
          "r": 0.038135593220338986,
          "p": 0.08333333333333333,
          "f": 0.05232557708761529
        },
        "rouge-l": {
          "r": 0.09876543209876543,
          "p": 0.21333333333333335,
          "f": 0.13502109272018387
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/CP/2501.05975v1",
      "true_abstract": "In energy markets, joint historical and implied calibration is of paramount\nimportance for practitioners yet notoriously challenging due to the need to\nalign historical correlations of futures contracts with implied volatility\nsmiles from the option market. We address this crucial problem with a\nparsimonious multiplicative multi-factor Heath-Jarrow-Morton (HJM) model for\nforward curves, combined with a stochastic volatility factor coming from the\nLifted Heston model. We develop a sequential fast calibration procedure\nleveraging the Kemna-Vorst approximation of futures contracts: (i) historical\ncorrelations and the Variance Swap (VS) volatility term structure are captured\nthrough Level, Slope, and Curvature factors, (ii) the VS volatility term\nstructure can then be corrected for a perfect match via a fixed-point\nalgorithm, (iii) implied volatility smiles are calibrated using Fourier-based\ntechniques. Our model displays remarkable joint historical and implied\ncalibration fits - to both German power and TTF gas markets - and enables\nrealistic interpolation within the implied volatility hypercube.",
      "generated_abstract": "r investigates the impact of the Fed's interest rate cuts on\nthe U.S. Treasury market. We employ a stochastic volatility framework to\nanalyze the impact of the Fed's interest rate cuts on the Treasury yield\ncurve. In particular, we investigate the effect of the Fed's interest rate\ncuts on the Treasury yield curve and investigate the impact of the Fed's\ninterest rate cuts on the yield curve's slope. We also explore the impact of\nthe Fed's interest rate cuts on the market's fundamental beliefs regarding the\nfuture path of interest rates. Our findings show that the Fed's interest rate\ncuts have a positive impact on the Treasury yield curve and its slope, but\nthere is a negative impact on the market's fundamental beliefs regarding the\nfuture path of interest rates. The results also show that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11538461538461539,
          "p": 0.21818181818181817,
          "f": 0.1509433917012778
        },
        "rouge-2": {
          "r": 0.02158273381294964,
          "p": 0.04,
          "f": 0.028037378624771462
        },
        "rouge-l": {
          "r": 0.11538461538461539,
          "p": 0.21818181818181817,
          "f": 0.1509433917012778
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10472v1",
      "true_abstract": "In this letter, we propose to deploy rotatable antennas (RAs) at the base\nstation (BS) to enhance both communication and sensing (C&S) performances, by\nexploiting a new spatial degree-of-freedom (DoF) offered by array rotation.\nSpecifically, we formulate a multi-objective optimization problem to\nsimultaneously maximize the sum-rate of multiple communication users and\nminimize the Cram\\'er-Rao bound (CRB) for target angle estimation, by jointly\noptimizing the transmit beamforming vectors and the array rotation angle at the\nBS. To solve this problem, we first equivalently decompose it into two\nsubproblems, corresponding to an inner problem for beamforming optimization and\nan outer problem for array rotation optimization. Although these two\nsubproblems are non-convex, we obtain their high-quality solutions by applying\nthe block coordinate descent (BCD) technique and one-dimensional exhaustive\nsearch, respectively. Moreover, we show that for the communication-only case,\nRAs provide an additional rotation gain to improve communication performance;\nwhile for the sensing-only case, the equivalent spatial aperture can be\nenlarged by RAs for achieving higher sensing accuracy. Finally, numerical\nresults are presented to showcase the performance gains of RAs over\nfixed-rotation antennas in integrated sensing and communications (ISAC).",
      "generated_abstract": "This paper presents a novel adaptive beamforming design for the downlink\nunderwater wireless communication system using the sparse Gaussian channel\nmodel. The proposed adaptive beamforming scheme is based on the sparsity\nprinciple and the sparsity-aided channel estimation, and it is capable of\nsparsely recovering the channels of the desired and interfering users. The\nproposed adaptive beamforming is shown to achieve an average sum-rate that is\nhigher than that of the conventional full-rank beamforming design under the\nsame power constraint. Theoretical results show that the proposed adaptive\nbeamforming scheme achieves a higher sum-rate than the conventional full-rank\nbeamforming design under the same power constraint.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13709677419354838,
          "p": 0.288135593220339,
          "f": 0.18579234535758019
        },
        "rouge-2": {
          "r": 0.01675977653631285,
          "p": 0.03571428571428571,
          "f": 0.022813683865316216
        },
        "rouge-l": {
          "r": 0.11290322580645161,
          "p": 0.23728813559322035,
          "f": 0.1530054601116786
        }
      }
    },
    {
      "paper_id": "physics.flu-dyn.cond-mat/soft/2503.10261v1",
      "true_abstract": "Flow birefringence measurement is an emerging technique for visualizing\nstress fields in fluid flows. This study investigates flow birefringence in the\nsteady radial Hele-Shaw flow of a shear-thinning fluid. In this flow\nconfiguration, stress is dominant along the optical axis, challenging the\napplicability of the conventional stress-optic law (SOL). We conduct flow\nbirefringence measurements at various flow rates and compare the results with\ntheoretical predictions. The observed phase retardation cannot be\nquantitatively explained using the conventional SOL, but is successfully\ndescribed using the second-order SOL, which accounts for stress along the\noptical direction, using the results of rheo-optical measurements. Furthermore,\nwe investigate the shear-thinning effects on phase retardation from two\nperspectives: (i) stress changes resulting from viscosity variations and (ii)\nthe variation of the shear-dependent stress-optic coefficient in the\nsecond-order SOL. Our findings indicate that the latter is more significant and\nshear-thinning behavior suppresses radial variations in phase retardation. This\nstudy demonstrates that the combination of the second-order SOL and\nrheo-optical measurements is essential for an accurate interpretation of flow\nbirefringence in Hele-Shaw flow, providing a noninvasive approach for stress\nfield analysis in high-aspect-ratio geometries.",
      "generated_abstract": "the flow of an incompressible, viscous fluid in a circular cylinder\nand observe the formation of a bubble of infinite radius. We investigate the\ninfluence of the cylinder's radius and viscosity on the bubble's shape and\nsize. We observe that increasing the cylinder's radius decreases the bubble's\nradius, while increasing the viscosity increases the bubble's radius. Increasing\nthe viscosity has the opposite effect on the bubble's radius: it increases it\nwhen the cylinder's radius is small, and reduces it when the cylinder's radius\nis large. Increasing the viscosity also increases the bubble's area, while\ndecreasing it decreases it. Increasing the viscosity has a different effect on\nthe bubble's size: it increases it when the cylinder's radius",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12962962962962962,
          "p": 0.2916666666666667,
          "f": 0.17948717522682459
        },
        "rouge-2": {
          "r": 0.024539877300613498,
          "p": 0.047619047619047616,
          "f": 0.032388659479093876
        },
        "rouge-l": {
          "r": 0.12037037037037036,
          "p": 0.2708333333333333,
          "f": 0.16666666240631176
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.06743v2",
      "true_abstract": "Structural changes in main retinal blood vessels serve as critical biomarkers\nfor the onset and progression of glaucoma. Identifying these vessels is vital\nfor vascular modeling yet highly challenging. This paper proposes X-GAN, a\ngenerative AI-powered unsupervised segmentation model designed for extracting\nmain blood vessels from Optical Coherence Tomography Angiography (OCTA) images.\nThe process begins with the Space Colonization Algorithm (SCA) to rapidly\ngenerate a skeleton of vessels, featuring their radii. By synergistically\nintegrating generative adversarial networks (GANs) with biostatistical modeling\nof vessel radii, X-GAN enables a fast reconstruction of both 2D and 3D\nrepresentations of the vessels. Based on this reconstruction, X-GAN achieves\nnearly 100\\% segmentation accuracy without relying on labeled data or\nhigh-performance computing resources. Also, to address the Issue, data scarity,\nwe introduce GSS-RetVein, a high-definition mixed 2D and 3D glaucoma retinal\ndataset. GSS-RetVein provides a rigorous benchmark due to its exceptionally\nclear capillary structures, introducing controlled noise for testing model\nrobustness. Its 2D images feature sharp capillary boundaries, while its 3D\ncomponent enhances vascular reconstruction and blood flow prediction,\nsupporting glaucoma progression simulations. Experimental results confirm\nGSS-RetVein's superiority in evaluating main vessel segmentation compared to\nexisting datasets. Code and dataset are here:\nhttps://github.com/VikiXie/SatMar8.",
      "generated_abstract": "This paper presents a method for estimating the optical properties of\nmaterials, such as the refractive index and index of refraction, from the\nradiative properties of the same material. The method is based on the\ndetermination of the scattering coefficients, which are related to the\nradiative properties through the scattering matrix. The method is applicable to\nboth passive and active materials, and can be used to estimate the properties of\na material in the visible and near-infrared spectral regions. The method is\nillustrated through two examples: the refractive index of a glass material and\nthe index of refraction of an active material. The method is implemented in a\nPython package, refractive.py.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1103448275862069,
          "p": 0.27586206896551724,
          "f": 0.15763546389866304
        },
        "rouge-2": {
          "r": 0.010309278350515464,
          "p": 0.021505376344086023,
          "f": 0.013937277849192213
        },
        "rouge-l": {
          "r": 0.10344827586206896,
          "p": 0.25862068965517243,
          "f": 0.14778324714989458
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2502.08443v1",
      "true_abstract": "The use of valid surrogate endpoints is an important stake in clinical\nresearch to help reduce both the duration and cost of a clinical trial and\nspeed up the evaluation of interesting treatments. Several methods have been\nproposed in the statistical literature to validate putative surrogate\nendpoints. Two main approaches have been proposed: the meta-analytic approach\nand the mediation analysis approach. The former uses data from meta-analyses to\nderive associations measures between the surrogate and the final endpoint at\nthe individual and trial levels. The latter rather uses the proportion of the\ntreatment effect on the final endpoint through the surrogate as a measure of\nsurrogacy in a causal inference framework. Both approaches have remained\nseparated as the meta-analytic approach does not estimate the treatment effect\non the final endpoint through the surrogate while the mediation analysis\napproach have been limited to single-trial setting. However, these two\napproaches are complementary. In this work we propose an approach that combines\nthe meta-analytic and mediation analysis approaches using joint modeling for\nsurrogate validation. We focus on the cases where the final endpoint is a\ntime-to-event endpoint (such as time-to-death) and the surrogate is either a\ntime-to-event or a longitudinal biomarker. Two new joint models were proposed\ndepending on the nature of the surrogate. These model are implemented in the R\npackage frailtypack. We illustrate the developed approaches in three\napplications on real datasets in oncology.",
      "generated_abstract": "We introduce a novel method for inferring the joint distribution of\nregression coefficients and a predictive model based on a kernel density\nestimator. The method is based on the integration of a kernel density estimator\nin the log-likelihood, and provides a joint distribution of both parameters\nthrough a joint posterior. We derive the resulting posterior distribution and\nits properties, including the asymptotic normality and the Bayes factor. The\nposterior distribution is estimated through a Metropolis-Hastings algorithm,\nand the proposed method is applied to simulated and real datasets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1484375,
          "p": 0.3877551020408163,
          "f": 0.21468926153276524
        },
        "rouge-2": {
          "r": 0.025252525252525252,
          "p": 0.06666666666666667,
          "f": 0.0366300326450111
        },
        "rouge-l": {
          "r": 0.1171875,
          "p": 0.30612244897959184,
          "f": 0.16949152141977092
        }
      }
    },
    {
      "paper_id": "cs.HC.stat/OT/2407.14072v2",
      "true_abstract": "Psychological research often involves understanding psychological constructs\nthrough conducting factor analysis on data collected by a questionnaire, which\ncan comprise hundreds of questions. Without interactive systems for\ninterpreting factor models, researchers are frequently exposed to subjectivity,\npotentially leading to misinterpretations or overlooked crucial information.\nThis paper introduces FAVis, a novel interactive visualization tool designed to\naid researchers in interpreting and evaluating factor analysis results. FAVis\nenhances the understanding of relationships between variables and factors by\nsupporting multiple views for visualizing factor loadings and correlations,\nallowing users to analyze information from various perspectives. The primary\nfeature of FAVis is to enable users to set optimal thresholds for factor\nloadings to balance clarity and information retention. FAVis also allows users\nto assign tags to variables, enhancing the understanding of factors by linking\nthem to their associated psychological constructs. Our user study demonstrates\nthe utility of FAVis in various tasks.",
      "generated_abstract": "We study the task of estimating the maximum likelihood estimator (MLE) of a\nprobability distribution $P$ given a set of observed samples $(X_1, Y_1),\n\\dots, (X_n, Y_n)$, with $X_i \\in \\mathbb{R}^d$ and $Y_i \\in \\mathbb{R}$, and\na constant number of samples $n \\geq 1$. We show that under mild assumptions,\nthe empirical distribution of the MLE estimator $\\hat{P}$ converges in\nprobability to a uniform distribution on $[0, 1",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0891089108910891,
          "p": 0.18,
          "f": 0.11920529358361491
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07920792079207921,
          "p": 0.16,
          "f": 0.10596026047103217
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/CR/2503.10171v1",
      "true_abstract": "Graph databases have garnered extensive attention and research due to their\nability to manage relationships between entities efficiently. Today, many graph\nsearch services have been outsourced to a third-party server to facilitate\nstorage and computational support. Nevertheless, the outsourcing paradigm may\ninvade the privacy of graphs. PeGraph is the latest scheme achieving encrypted\nsearch over social graphs to address the privacy leakage, which maintains two\ndata structures XSet and TSet motivated by the OXT technology to support\nencrypted conjunctive search. However, PeGraph still exhibits limitations\ninherent to the underlying OXT. It does not provide transparent search\ncapabilities, suffers from expensive computation and result pattern leakages,\nand it fails to support search over dynamic encrypted graph database and\nresults verification. In this paper, we propose SecGraph to address the first\ntwo limitations, which adopts a novel system architecture that leverages an\nSGX-enabled cloud server to provide users with secure and transparent search\nservices since the secret key protection and computational overhead have been\noffloaded to the cloud server. Besides, we design an LDCF-encoded XSet based on\nthe Logarithmic Dynamic Cuckoo Filter to facilitate efficient plaintext\ncomputation in trusted memory, effectively mitigating the risks of result\npattern leakage and performance degradation due to exceeding the limited\ntrusted memory capacity. Finally, we design a new dynamic version of TSet named\nTwin-TSet to enable conjunctive search over dynamic encrypted graph database.\nIn order to support verifiable search, we further propose VSecGraph, which\nutilizes a procedure-oriented verification method to verify all data structures\nloaded into the trusted memory, thus bypassing the computational overhead\nassociated with the client's local verification.",
      "generated_abstract": "The current standard method for evaluating and comparing the performance of\ncomputer vision (CV) models is to compare the performance of different models\non the same task. However, this approach does not provide a clear understanding\nof the differences between models. We present a new framework that measures\nmodel performance by comparing the performance of models trained on different\ndata. This framework, called Differential Model Performance, evaluates models\non the task they were designed to perform, and uses the data they were trained\non to compare performance. We evaluate Differential Model Performance on 10\ndatasets and show that it provides a more accurate and comprehensive measure of\nmodel performance compared to existing methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.2898550724637681,
          "f": 0.17467248487252351
        },
        "rouge-2": {
          "r": 0.016877637130801686,
          "p": 0.04,
          "f": 0.02373886822988741
        },
        "rouge-l": {
          "r": 0.1125,
          "p": 0.2608695652173913,
          "f": 0.1572052359642266
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.08458v1",
      "true_abstract": "The information criterion AIC has been used successfully in many areas of\nstatistical modeling, and since it is derived based on the Taylor expansion of\nthe log-likelihood function and the asymptotic distribution of the maximum\nlikelihood estimator, it is not directly justified for likelihood functions\nthat include non-differentiable points such as the Laplace distribution. In\nfact, it is known to work effectively in many such cases. In this paper, we\nattempt to evaluate the bias correction directly for the case where the true\nmodel or the model to be estimated is a simple Laplace distribution model. As a\nresult, an approximate expression for the bias correction term was obtained.\nNumerical results show that the AIC approximations are relatively good except\nwhen the Gauss distribution model is fitted to data following the Laplace\ndistribution.",
      "generated_abstract": "y investigates the performance of an Adaptive Sampling Algorithm\n(ASA) for a Bayesian Nonparametric (BNP) model, which is a Bayesian\nnonparametric Bayesian regression model that is often used to predict\nnon-linear functions. The ASA has been shown to outperform the traditional\nMonte Carlo Markov Chain Monte Carlo (MCMC) method in terms of predictive\nperformance, particularly for high-dimensional data. However, the performance\nof ASA in terms of predictive accuracy can be further enhanced by incorporating\na priori knowledge of the model. In this paper, we propose a method that\nenables ASA to make use of the prior knowledge by using a kernel density\nestimator (KDE) to estimate the posterior distribution of the model parameters.\nThe KDE estimates are then used to generate a set of candidate posterior\ndistributions for the parameters and the AS",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27586206896551724,
          "p": 0.27906976744186046,
          "f": 0.2774566423990111
        },
        "rouge-2": {
          "r": 0.08264462809917356,
          "p": 0.08264462809917356,
          "f": 0.08264462309917386
        },
        "rouge-l": {
          "r": 0.25287356321839083,
          "p": 0.2558139534883721,
          "f": 0.2543352551157741
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.20067v1",
      "true_abstract": "The emergence of audio language models is empowered by neural audio codecs,\nwhich establish critical mappings between continuous waveforms and discrete\ntokens compatible with language model paradigms. The evolutionary trends from\nmulti-layer residual vector quantizer to single-layer quantizer are beneficial\nfor language-autoregressive decoding. However, the capability to handle\nmulti-domain audio signals through a single codebook remains constrained by\ninter-domain distribution discrepancies. In this work, we introduce UniCodec, a\nunified audio codec with a single codebook to support multi-domain audio data,\nincluding speech, music, and sound. To achieve this, we propose a partitioned\ndomain-adaptive codebook method and domain Mixture-of-Experts strategy to\ncapture the distinct characteristics of each audio domain. Furthermore, to\nenrich the semantic density of the codec without auxiliary modules, we propose\na self-supervised mask prediction modeling approach. Comprehensive objective\nand subjective evaluations demonstrate that UniCodec achieves excellent audio\nreconstruction performance across the three audio domains, outperforming\nexisting unified neural codecs with a single codebook, and even surpasses\nstate-of-the-art domain-specific codecs on both acoustic and semantic\nrepresentation capabilities.",
      "generated_abstract": "and reliable speech enhancement (SE) is essential for improving\nspeech quality and understanding, yet current SE approaches often struggle\nwith limited resources and complex signal processing tasks. To address these\nchallenges, we propose a novel framework, SE-Vision, that leverages the power of\nvision for SE. Specifically, we design a video-based SE system that extracts\nspeech features from video frames, enabling efficient SE processing without\nthe need for additional computational resources. Additionally, we introduce a\nnovel vision-language model (VLM) for SE, enabling speech enhancement\nacross different languages. Through extensive experiments, we demonstrate that\nour SE-Vision system outperforms existing SE methods, offering a viable alternative\nto traditional SE approaches. In particular, our results demonstrate that SE-Vision\nachieves comparable or better performance across all metrics, including\nspeech quality, speech separation, and speech",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19008264462809918,
          "p": 0.25274725274725274,
          "f": 0.2169811271755964
        },
        "rouge-2": {
          "r": 0.03125,
          "p": 0.04032258064516129,
          "f": 0.0352112626859757
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.24175824175824176,
          "f": 0.2075471649114455
        }
      }
    },
    {
      "paper_id": "math.AC.math/RA/2503.07271v1",
      "true_abstract": "A module $M$ is said to be stable if it has no nonzero projective direct\nsummand. For a ring $ R $, we study the conditions under which every $R$-module\n$M$ within a specific class can be decomposed into a direct sum of a projective\nmodule and a stable module, focusing on identifying the types of rings and the\nclass of $R$-modules where this property holds. Some well-known classes of\nrings over which every finitely presented module can be decomposed into a\ndirect sum of a projective submodule and a stable submodule are semiperfect\nrings or semilocal rings or rings satisfying some finiteness conditions like\nhaving finite uniform dimension or hollow dimension or being Noetherian or\nArtinian. By using the Auslander-Bridger transpose of modules, we prove that\nevery finitely presented right $R$-module over a left semihereditary ring $R$\nhas such a decomposition; note that the semihereditary condition is on the\nopposite side. Our main focus in this article is to give examples where such a\ndecomposition fails. We give some ring examples over which there exists an\ninfinitely generated or finitely generated or cyclic module or finitely\npresented module or cyclically presented module where such a decomposition\nfails. Our main example is a cyclically presented module $M$ over a commutative\nring such that~$M$ has no such decomposition and $M$ is not projectively\nequivalent to a stable module.",
      "generated_abstract": "We investigate a model for the nonlinear Schrodinger equation, where the\nenergy level is described by a compact Riemannian manifold. We construct a\nfamily of positive solutions of the nonlinear Schrodinger equation in a compact\nRiemannian manifold and prove that there exists a unique solution to the\nnonlinear Schrodinger equation if the compact Riemannian manifold is\nnon-compact.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12389380530973451,
          "p": 0.4,
          "f": 0.18918918557797668
        },
        "rouge-2": {
          "r": 0.010582010582010581,
          "p": 0.043478260869565216,
          "f": 0.017021273447171244
        },
        "rouge-l": {
          "r": 0.12389380530973451,
          "p": 0.4,
          "f": 0.18918918557797668
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.05880v1",
      "true_abstract": "Likelihood inference for max-stable random fields is in general impossible\nbecause their finite-dimensional probability density functions are unknown or\ncannot be computed efficiently. The weighted composite likelihood approach that\nutilizes lower dimensional marginal likelihoods (typically pairs or triples of\nsites that are not too distant) is rather favored. In this paper, we consider\nthe family of spatial max-stable Brown-Resnick random fields associated with\nisotropic fractional Brownian fields. We assume that the sites are given by\nonly one realization of a homogeneous Poisson point process restricted to\n$\\mathbf{C}=(-1/2,1/2]^{2}$ and that the random field is observed at these\nsites. As the intensity increases, we study the asymptotic properties of the\ncomposite likelihood estimators of the scale and Hurst parameters of the\nfractional Brownian fields using different weighting strategies: we exclude\neither pairs that are not edges of the Delaunay triangulation or triples that\nare not vertices of triangles.",
      "generated_abstract": "We consider a random variable $X$ with a finite collection of discrete\n$K$-subsets, where the cardinality of each $K$ is known. The aim is to obtain a\nrandom variable $Y$ such that $X$ and $Y$ have the same conditional distribution\nconditioned on the observation of a finite collection of random variables $Z$\nwhich are also known. We prove that the distribution of $Y$ can be obtained from\nthe distribution of $X$ via an iterative process of sampling $Z$ from the\ndistribution of $X$ and conditioning on $Z$. We show that this process can be\ncarried out in polynomial time and we provide a lower bound on the number of\niterations needed to obtain the conditional distribution of $Y$ from the\ndistribution of $X$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1919191919191919,
          "p": 0.296875,
          "f": 0.2331288295863601
        },
        "rouge-2": {
          "r": 0.015151515151515152,
          "p": 0.02,
          "f": 0.01724137440547107
        },
        "rouge-l": {
          "r": 0.1717171717171717,
          "p": 0.265625,
          "f": 0.20858895228574664
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/CB/2502.18947v1",
      "true_abstract": "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
      "generated_abstract": "states are the basic units of biological activity. They can be\neither single-cell phenotypes (e.g., transcript levels, protein levels, or\nenzymatic activities) or population-level phenotypes (e.g., cell numbers,\npopulation densities, or fitness values). A crucial question in biology is\nhow cells decide on the precise phenotype to express. A key component of cell\ndecision-making is signal processing. In this work, we propose a new concept\nof cellular signal processing. A cell can process a signal through a network of\ncells, which may be in close proximity to one another or may be far apart.\nThe cellular signal processing network is a graph that consists of two parts:\nthe input graph and the output graph. The input graph is a directed graph\nconnecting input cells to a set of signal-processing cells. The output graph is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15217391304347827,
          "p": 0.25301204819277107,
          "f": 0.19004524417845675
        },
        "rouge-2": {
          "r": 0.021164021164021163,
          "p": 0.031746031746031744,
          "f": 0.025396820596826304
        },
        "rouge-l": {
          "r": 0.15217391304347827,
          "p": 0.25301204819277107,
          "f": 0.19004524417845675
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.12752v1",
      "true_abstract": "Using novel establishment-level observational data from Switzerland, we\nempirically examine whether the usage of key technologies of Industry 4.0\ndistinguishes across firms with different types of organizational culture.\nBased on the Technology-Organization-Environment and the Competing Values\nframework, we hypothesize that the developmental culture has the greatest\npotential to promote the usage of Industry 4.0 technologies. We also\nhypothesize that companies with a hierarchical or rational culture are\nespecially likely to make use of automation technologies, such as AI and\nrobotics. By means of descriptive statistics and multiple regression analysis,\nwe find empirical support for our first hypothesis, while we cannot con-firm\nour second hypothesis. Our empirical results provide important implications for\nmanagerial decision-makers. Specifically, the link between organizational\nculture and the implementation of Industry 4.0 technologies is relevant for\nmanagers, as this knowledge helps them to cope with digital transformation in\nturbulent times and keep their businesses competitive.",
      "generated_abstract": "2020 pandemic has had a disproportionate impact on women's labor\ninvestments, which have been disproportionately affected by gendered\ninteractions between labor market policies and the pandemic. Using a\ndynamic panel data model with time-varying policy shocks, I analyze the\nimpact of gendered labor market policies on women's labor investments. Using\nthe Women, Infants, and Children (WIC) nutrition program as a case study, I\nanalyze how pandemic-related policy interventions, such as changes in eligibility\nstandards and payment rates, affected women's labor market behavior. I find\nthat the pandemic significantly increased women's participation in the\nWIC program, resulting in a 7.2 percent increase in the labor force participation\nrate. The pandemic also increased women's use of tele",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1588785046728972,
          "p": 0.22666666666666666,
          "f": 0.18681318196775765
        },
        "rouge-2": {
          "r": 0.028985507246376812,
          "p": 0.0380952380952381,
          "f": 0.03292180579180076
        },
        "rouge-l": {
          "r": 0.14953271028037382,
          "p": 0.21333333333333335,
          "f": 0.17582417097874667
        }
      }
    },
    {
      "paper_id": "math-ph.math/DS/2503.08412v1",
      "true_abstract": "The article presents the concept of a cumulant representation for\ndistribution functions describing the states of many-particle systems with\ntopological nearest-neighbor interaction. A solution to the Cauchy problem for\nthe hierarchy of nonlinear evolution equations for the cumulants of\ndistribution functions of such systems is constructed. The connection between\nthe constructed solution and the series expansion structure for a solution to\nthe Cauchy problem of the BBGKY hierarchy has been established. Furthermore,\nthe expansion structure for a solution to the Cauchy problem of the hierarchy\nof evolution equations for reduced observables of topologically interacting\nparticles is established.",
      "generated_abstract": "aper, we study the properties of the class of unital $C^*$-algebras\n$\\mathcal{A}({\\mathbb D}_n)$ for $n\\geq 2$ and their universal enveloping\nalgebras $U(\\mathcal{A}({\\mathbb D}_n))$. We prove that $\\mathcal{A}({\\mathbb\nD}_n)$ is an example of a group-like $C^*$-algebra. In particular, we show that\n$\\mathcal{A}({\\mathbb D}_n)$ is an example of a group-like $C^*$-algebra with\nnon-trivial center. We also show that $U(\\mathcal{A}({\\mathbb D}_n))$ is\nnon-isomorphic to a universal enveloping algebra of a group-like $C^*$-algebra\nwith non-trivial center if and only if $\\mathcal{A}({\\mathbb D}_n)$ is a\ngroup-like $",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16326530612244897,
          "p": 0.18604651162790697,
          "f": 0.17391303849952755
        },
        "rouge-2": {
          "r": 0.02631578947368421,
          "p": 0.03389830508474576,
          "f": 0.029629624708917137
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.16279069767441862,
          "f": 0.15217390806474496
        }
      }
    },
    {
      "paper_id": "physics.chem-ph.physics/chem-ph/2503.10538v1",
      "true_abstract": "Given the power of large language and large vision models, it is of profound\nand fundamental interest to ask if a foundational model based on data and\nparameter scaling laws and pre-training strategies is possible for learned\nsimulations of chemistry and materials. The scaling of large and diverse\ndatasets and highly expressive architectures for chemical and materials\nsciences should result in a foundation model that is more efficient and broadly\ntransferable, robust to out-of-distribution challenges, and easily fine-tuned\nto a variety of downstream observables, when compared to specific training from\nscratch on targeted applications in atomistic simulation. In this Perspective\nwe aim to cover the rapidly advancing field of machine learned interatomic\npotentials (MLIP), and to illustrate a path to create chemistry and materials\nMLIP foundation models at larger scale.",
      "generated_abstract": "ty to measure the concentration of molecules in solution is\na key requirement for many applications, such as in chemical analysis,\npharmacokinetics, and environmental monitoring. Current methods for\nconcentration measurement rely on the use of chemical reagents, which are\nexpensive, hazardous, and potentially toxic. In this paper, we introduce a\nnovel technique based on magnetic resonance imaging (MRI) for the measurement of\nthe concentration of chemicals in solution. MRI is a non-invasive technique\nthat can be used for the measurement of molecular concentrations, providing\nhigh-resolution information about the distribution of chemicals in solution.\nUsing a MRI scanner, we were able to measure the concentration of a\nflame-retardant compound, polybrominated diphenyl ethers (PBDEs), in solution.\nThe technique is highly scal",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18888888888888888,
          "p": 0.2236842105263158,
          "f": 0.2048192721439978
        },
        "rouge-2": {
          "r": 0.016,
          "p": 0.019417475728155338,
          "f": 0.01754385469567699
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.19736842105263158,
          "f": 0.1807228866018291
        }
      }
    },
    {
      "paper_id": "physics.app-ph.eess/SP/2503.07239v1",
      "true_abstract": "We present the \"Virtual VNA 3.0\" technique for estimating the scattering\nmatrix of a \\textit{non-reciprocal}, linear, passive, time-invariant device\nunder test (DUT) with $N$ monomodal ports using a single measurement setup\ninvolving a vector network analyzer (VNA) with only $N_\\mathrm{A}<N$ ports --\nthus eliminating the need for any reconnections. We partition the DUT ports\ninto $N_\\mathrm{A}$ \"accessible\" and $N_\\mathrm{S}$ \"not-directly-accessible\"\n(NDA) ports. We connect the accessible ports to the VNA and the NDA ports to\nthe \"virtual VNA ports\" of a VNA Extension Kit. This kit enables each NDA port\nto be terminated with three distinct individual loads or connected to\nneighboring DUT ports via coupled loads. We derive both a closed-form and a\ngradient-descent method to estimate the complete scattering matrix of the\nnon-reciprocal DUT from measurements conducted with the $N_\\mathrm{A}$-port VNA\nunder various NDA-port terminations. We validate both methods experimentally\nfor $N_\\mathrm{A}=N_\\mathrm{S}=4$, where our DUT is a complex eight-port\ntransmission-line network comprising circulators. Altogether, the presented\n\"Virtual VNA 3.0\" technique constitutes a scalable approach to unambiguously\ncharacterize a many-port \\textit{non-reciprocal} DUT with a few-port VNA (only\n$N_\\mathrm{A}>1$ is required) -- without any tedious and error-prone manual\nreconnections susceptible to inaccuracies. The VNA Extension Kit requirements\nmatch those for the \"Virtual VNA 2.0\" technique that was limited to reciprocal\nDUTs.",
      "generated_abstract": "This paper presents a novel, nonlinear frequency-domain method for estimating\nthe phase of a time-varying, time-invariant system using a limited number of\nmeasurements. The proposed method is based on the application of a\nnonlinear least-squares optimization technique to a system of nonlinear\nequations. The method is demonstrated on a simulated system that exhibits a\ntime-varying phase, and the effectiveness of the proposed method is validated\nthrough a comparison with a well-known linear least-squares method. The\ncomputational efficiency of the proposed method is also examined through\nsimulations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13636363636363635,
          "p": 0.36,
          "f": 0.19780219381717193
        },
        "rouge-2": {
          "r": 0.03535353535353535,
          "p": 0.09090909090909091,
          "f": 0.050909086877091225
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.36,
          "f": 0.19780219381717193
        }
      }
    },
    {
      "paper_id": "cs.IT.math/IT/2503.09991v1",
      "true_abstract": "A finite-field multiple-access (FFMA) system separates users within a finite\nfield by utilizing different element-pairs (EPs) as virtual resources. The\nCartesian product of distinct EPs forms an EP code, which serves as the input\nto a finite-field multiplexing module (FF-MUX), allowing the FFMA technique to\ninterchange the order of channel coding and multiplexing. This flexibility\nenables the FFMA system to support a large number of users with short packet\ntraffic, addressing the finite block length (FBL) problem in multiuser reliable\ntransmission. Designing EP codes is a central challenge in FFMA systems. In\nthis paper, we construct EP codes based on a bit(s)-to-codeword transformation\napproach and define the corresponding EP code as a codeword-wise EP (CWEP)\ncode. We then investigate the encoding process of EP codes, and propose unique\nsum-pattern mapping (USPM) structural property constraints to design uniquely\ndecodable CWEP codes. Next, we present the \\(\\kappa\\)-fold ternary orthogonal\nmatrix \\({\\bf T}_{\\rm o}(2^{\\kappa}, 2^{\\kappa})\\) over GF\\((3^m)\\), where \\(m\n= 2^{\\kappa}\\), and the ternary non-orthogonal matrix \\({\\bf T}_{\\rm no}(3,2)\\)\nover GF\\((3^2)\\), for constructing specific CWEP codes. Based on the proposed\nCWEP codes, we introduce three FFMA modes: channel codeword multiple access\n(FF-CCMA), code division multiple access (FF-CDMA), and non-orthogonal multiple\naccess (FF-NOMA). Simulation results demonstrate that all three modes\neffectively support massive user transmissions with strong error performance.",
      "generated_abstract": "nt trend in the wireless communications industry is to shift\nfrom the traditional orthogonal frequency division multiplexing (OFDM) modulation\nto the time-division multiplexing (TDMA) modulation, which can reduce the\noverall energy consumption of wireless networks. However, the performance\ndegradation due to the TDMA modulation introduces additional challenges in\nestablishing reliable communication links, particularly when the wireless\nenvironment is insecure. To address this issue, we propose a novel two-stage\nscheme for the secure transmission in the TDMA network. In the first stage, we\nintroduce a novel key-dependent block-coherent beamforming design to ensure\nfuture channel state information (CSI) reliability. Then, in the second stage,\nwe design a secure communication protocol to protect the transmitted signal\nagainst the interference from the eavesdropper. In",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14,
          "p": 0.25301204819277107,
          "f": 0.18025750614304936
        },
        "rouge-2": {
          "r": 0.004830917874396135,
          "p": 0.009009009009009009,
          "f": 0.00628930363178209
        },
        "rouge-l": {
          "r": 0.11333333333333333,
          "p": 0.20481927710843373,
          "f": 0.1459227421945515
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2407.00022v1",
      "true_abstract": "Entropy is a very useful concept from physics that tries to explain how a\nsystem behaves from a point of view of the thermodynamics. However, there are\ntwo ways to explain entropy, and it depends on if we are studying a microsystem\nor a microsystem. From a macroscopically point of view, it is important to\ndescribe if the system is a reversible system or not. However, form the\nmicroscopically point of view, the concept of chaos is related to entropy. In\nsuch case, entropy measures the amount of disorder into the system. Otherwise,\nthe idea of connecting at the same time the analysis of the macro and micro\nsystem with the use of entropy it is not very common.",
      "generated_abstract": "In this paper, we propose a method for identifying and pricing risks in\n(inter)bank lending markets. The method combines a novel pricing model with\nexisting risk models and a novel risk measurement method. We use a\nsimulation-based approach to assess the performance of the risk measurement\nmethod. We also show that the new method can be applied to other types of\nrisk. Finally, we compare the results of our method with those of a well-known\nrisk measurement method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15151515151515152,
          "p": 0.20408163265306123,
          "f": 0.1739130385875238
        },
        "rouge-2": {
          "r": 0.009174311926605505,
          "p": 0.014084507042253521,
          "f": 0.011111106333952672
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.1836734693877551,
          "f": 0.1565217342396977
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SP/2503.10345v1",
      "true_abstract": "Online conformal prediction enables the runtime calibration of a pre-trained\nartificial intelligence model using feedback on its performance. Calibration is\nachieved through set predictions that are updated via online rules so as to\nensure long-term coverage guarantees. While recent research has demonstrated\nthe benefits of incorporating prior knowledge into the calibration process,\nthis has come at the cost of replacing coverage guarantees with less tangible\nregret guarantees based on the quantile loss. This work introduces intermittent\nmirror online conformal prediction (IM-OCP), a novel runtime calibration\nframework that integrates prior knowledge, while maintaining long-term coverage\nand achieving sub-linear regret. IM-OCP features closed-form updates with\nminimal memory complexity, and is designed to operate under potentially\nintermittent feedback.",
      "generated_abstract": "r proposes a novel multi-agent reinforcement learning (MARL)\nframework to optimize the energy consumption of autonomous underwater\nvehicles (AUVs) during dynamic missions. Our approach combines a\nreinforcement learning agent with a probabilistic model-based energy estimator\nto mitigate the uncertainty in the AUVs' energy consumption. The reinforcement\nlearning agent selects the best path to the target, ensuring the AUVs reach the\ntarget within a specified time frame. The energy estimator estimates the\ncurrent battery state of the AUVs and provides an energy consumption\nestimation. We propose a novel energy consumption estimation method, which\nincorporates the trajectory deviation and energy consumption of previous\nmissions, to ensure the accuracy of the energy estimator. The estimator is\ntrained by a reinforcement learning agent that selects the optimal path for the\nAU",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12790697674418605,
          "p": 0.1506849315068493,
          "f": 0.13836477490763832
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.018691588785046728,
          "f": 0.018433174724459298
        },
        "rouge-l": {
          "r": 0.12790697674418605,
          "p": 0.1506849315068493,
          "f": 0.13836477490763832
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.08795v1",
      "true_abstract": "We propose a stochastic Model Predictive Control (MPC) framework that ensures\nclosed-loop chance constraint satisfaction for linear systems with general\nsub-Gaussian process and measurement noise. By considering sub-Gaussian noise,\nwe can provide guarantees for a large class of distributions, including\ntime-varying distributions. Specifically, we first provide a new\ncharacterization of sub-Gaussian random vectors using matrix variance proxies,\nwhich can more accurately represent the predicted state distribution. We then\nderive tail bounds under linear propagation for the new characterization,\nenabling tractable computation of probabilistic reachable sets of linear\nsystems. Lastly, we utilize these probabilistic reachable sets to formulate a\nstochastic MPC scheme that provides closed-loop guarantees for general\nsub-Gaussian noise. We further demonstrate our approach in simulations,\nincluding a challenging task of surgical planning from image observations.",
      "generated_abstract": "r presents a novel multi-agent system (MAS) approach for the\nnonlinear vehicle routing problem (VRP) using the multi-agent reinforcement\nlearning (MARL) framework. The MAS includes a set of vehicles with different\ncapacities and driving styles. These vehicles are coordinated by an agent that\nprovides the vehicles with the desired routing directions. The agent also\nmonitors the vehicles' driving behaviors and provides feedback to the vehicles\nto improve their driving styles. The agent's objective is to maximize the\nvehicles' utility while ensuring safety. To address the challenges of\ncomputational complexity and time-consuming training of reinforcement learning\n(RL) agents, we propose a hybrid RL/MARAL approach. The MARAL agent learns the\nagent's policy through reinforcement learning, and the RL agent provides\nfeedback to the MARAL agent to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.18181818181818182,
          "f": 0.16666666170138905
        },
        "rouge-2": {
          "r": 0.025210084033613446,
          "p": 0.02631578947368421,
          "f": 0.02575106796367685
        },
        "rouge-l": {
          "r": 0.13186813186813187,
          "p": 0.15584415584415584,
          "f": 0.14285713789186524
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.10524v1",
      "true_abstract": "We discuss invariants which are helpful for the computation of the vanishing\nlocus of a finitely presented functor $\\mathcal{G}$, i.e., the set of points in\nthe Ziegler spectrum on which $\\mathcal{G}$ vanishes. These invariants are: the\nrank of $\\mathcal{G}$, the supports of its co- and contravariant defect, and\nthe class of $\\mathcal{G}$ in the Grothendieck group of the category of\nfinitely presented functors. We show that these invariants determine the\nvanishing locus in the case of a finitely presented functor over a Dedekind\ndomain.",
      "generated_abstract": "In this paper, we give a complete description of the localized Fourier\nmoments of the modular form $S_k(T)$ for $k \\geq 4$ from the modular $L$-series\nof $L(s,S_k(T))$ when $T$ is a generic unramified Hecke operator of weight $k$.\nWe also give a description of the localized Fourier moments of the modular form\n$S_k(T)$ for $k \\leq 3$ when $T$ is a generic unramified Hecke operator of\nweight $k$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1,
          "p": 0.1388888888888889,
          "f": 0.11627906489994612
        },
        "rouge-2": {
          "r": 0.013333333333333334,
          "p": 0.023255813953488372,
          "f": 0.016949147910084575
        },
        "rouge-l": {
          "r": 0.08,
          "p": 0.1111111111111111,
          "f": 0.0930232509464578
        }
      }
    },
    {
      "paper_id": "cs.MM.eess/AS/2502.03897v2",
      "true_abstract": "As a natural multimodal content, audible video delivers an immersive sensory\nexperience. Consequently, audio-video generation systems have substantial\npotential. However, existing diffusion-based studies mainly employ relatively\nindependent modules for generating each modality, which lack exploration of\nshared-weight generative modules. This approach may under-use the intrinsic\ncorrelations between audio and visual modalities, potentially resulting in\nsub-optimal generation quality. To address this, we propose UniForm, a unified\ndiffusion transformer designed to enhance cross-modal consistency. By\nconcatenating auditory and visual information, UniForm learns to generate audio\nand video simultaneously within a unified latent space, facilitating the\ncreation of high-quality and well-aligned audio-visual pairs. Extensive\nexperiments demonstrate the superior performance of our method in joint\naudio-video generation, audio-guided video generation, and video-guided audio\ngeneration tasks. Our demos are available at https://uniform-t2av.github.io/.",
      "generated_abstract": "speech recognition (ASR) is a fundamental task in speech\nprocessing, with widespread applications in speech communication. Despite\nadvancements in deep learning, conventional ASR models still struggle with\nlanguage understanding (LU) tasks, such as speaker verification (SV). To address\nthis limitation, we propose a novel model, Speaker-Guided LU (SGLU), which\nintroduces a speaker-specific LU module for SV. The model first uses a\nspeaker-specific transformer encoder to extract speaker-specific representations\nfrom the input audio, followed by a speaker-specific LU module to estimate\nspeaker embeddings. The speaker embeddings are then fused with the speaker\nembeddings from the encoder to form the final LU representation. Extensive\nexperiments on both public and private data demonstrate that SGLU outperforms\nstate-of-the-art",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1509433962264151,
          "p": 0.19047619047619047,
          "f": 0.16842104769861513
        },
        "rouge-2": {
          "r": 0.024,
          "p": 0.028846153846153848,
          "f": 0.026200868404493607
        },
        "rouge-l": {
          "r": 0.1320754716981132,
          "p": 0.16666666666666666,
          "f": 0.14736841611966775
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/RM/2503.08693v1",
      "true_abstract": "We construct liquidity-adjusted return and volatility using purposely\ndesigned liquidity metrics (liquidity jump and liquidity diffusion) that\nincorporate additional liquidity information. Based on these measures, we\nintroduce a liquidity-adjusted ARMA-GARCH framework to address the limitations\nof traditional ARMA-GARCH models, which are not effectively in modeling\nilliquid assets with high liquidity variability, such as cryptocurrencies. We\ndemonstrate that the liquidity-adjusted model improves model fit for\ncryptocurrencies, with greater volatility sensitivity to past shocks and\nreduced volatility persistence of erratic past volatility. Our model is\nvalidated by the empirical evidence that the liquidity-adjusted mean-variance\n(LAMV) portfolios outperform the traditional mean-variance (TMV) portfolios.",
      "generated_abstract": "er the problem of learning an optimal mean-variance portfolio strategy\nfrom noisy, incomplete data. We propose a novel method for estimating the\noptimal strategy in the mean-variance framework. The key idea is to estimate\nthe optimal strategy from the available noisy data and then learn the\noptimization problem from the available noisy data. We derive a new\nwell-posedness result for this estimator, which is in a sense a generalization\nof the standard (well-posedness) result for the standard mean-variance\noptimization problem. We establish the consistency and asymptotic normality of\nthe estimator, and establish a convergence rate for the estimator. We show that\nthe estimator has a strong consistency property, which is crucial for the\nasymptotic normality result. We also provide a convergence rate for the\noptimization problem in the mean-variance framework. Finally, we illustrate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19444444444444445,
          "p": 0.21875,
          "f": 0.20588234795847765
        },
        "rouge-2": {
          "r": 0.010309278350515464,
          "p": 0.009433962264150943,
          "f": 0.009852211758598949
        },
        "rouge-l": {
          "r": 0.18055555555555555,
          "p": 0.203125,
          "f": 0.19117646560553647
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/TH/2503.03206v1",
      "true_abstract": "We developed an analytical framework for understanding how the learned\ndistribution evolves during diffusion model training. Leveraging the Gaussian\nequivalence principle, we derived exact solutions for the gradient-flow\ndynamics of weights in one- or two-layer linear denoiser settings with\narbitrary data. Remarkably, these solutions allowed us to derive the generated\ndistribution in closed form and its KL divergence through training. These\nanalytical results expose a pronounced power-law spectral bias, i.e., for\nweights and distributions, the convergence time of a mode follows an inverse\npower law of its variance. Empirical experiments on both Gaussian and image\ndatasets demonstrate that the power-law spectral bias remains robust even when\nusing deeper or convolutional architectures. Our results underscore the\nimportance of the data covariance in dictating the order and rate at which\ndiffusion models learn different modes of the data, providing potential\nexplanations for why earlier stopping could lead to incorrect details in image\ngenerative models.",
      "generated_abstract": "er the problem of recovering the joint distribution of a set of\n$n$ independent random variables from a single sample, $y_1, \\ldots, y_n$. In\nthis paper, we focus on the Gaussian case, where $y_i \\sim N(\\mu, \\sigma^2 I)$\nfor $i = 1, \\ldots, n$. The most common method for this task is to estimate\nthe mean $\\mu$ and covariance $\\sigma I$ and then apply the Gaussian\nKernel Density Estimator (GKDE) to estimate the joint distribution of the\nvariables. However, this method suffers from the curse of dimensionality,\nparticularly for large $n$. In this paper, we introduce a novel method for\nestimating the joint distribution of a set of variables using a kernel\ndensity estimator and a support vector machine (SVM). The kernel density\nest",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10526315789473684,
          "p": 0.15789473684210525,
          "f": 0.12631578467368437
        },
        "rouge-2": {
          "r": 0.019867549668874173,
          "p": 0.02830188679245283,
          "f": 0.023346298655241814
        },
        "rouge-l": {
          "r": 0.10526315789473684,
          "p": 0.15789473684210525,
          "f": 0.12631578467368437
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SP/2503.03338v1",
      "true_abstract": "We offer a new in-depth investigation of global path planning (GPP) for\nunmanned ground vehicles, an autonomous mining sampling robot named ROMIE. GPP\nis essential for ROMIE's optimal performance, which is translated into solving\nthe traveling salesman problem, a complex graph theory challenge that is\ncrucial for determining the most effective route to cover all sampling\nlocations in a mining field. This problem is central to enhancing ROMIE's\noperational efficiency and competitiveness against human labor by optimizing\ncost and time. The primary aim of this research is to advance GPP by\ndeveloping, evaluating, and improving a cost-efficient software and web\napplication. We delve into an extensive comparison and analysis of Google\noperations research (OR)-Tools optimization algorithms. Our study is driven by\nthe goal of applying and testing the limits of OR-Tools capabilities by\nintegrating Reinforcement Learning techniques for the first time. This enables\nus to compare these methods with OR-Tools, assessing their computational\neffectiveness and real-world application efficiency. Our analysis seeks to\nprovide insights into the effectiveness and practical application of each\ntechnique. Our findings indicate that Q-Learning stands out as the optimal\nstrategy, demonstrating superior efficiency by deviating only 1.2% on average\nfrom the optimal solutions across our datasets.",
      "generated_abstract": "s driving requires continuous feedback from the environment,\nautomatically identifying objects and estimating their locations. In\nconventional vision-based systems, this process is typically achieved using a\nsingle camera to provide depth information and a single sensor to provide\nobject information. However, these methods often fail to capture the entire\nscene, limiting the ability to identify and localize objects. This limitation\ncan be mitigated by employing multi-sensor fusion to provide additional\ninformation. In this work, we propose a novel method for multi-sensor fusion\nthat utilizes a single camera and a single lidar sensor. By integrating these\nsensors, we enhance the detection and localization of objects in the scene.\nAdditionally, we introduce a novel fusion method that integrates depth and\nlidar data to improve object detection and localization. Our experiments\ndemonstrate that our method outperforms traditional methods in terms of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14893617021276595,
          "p": 0.2441860465116279,
          "f": 0.18502202172524218
        },
        "rouge-2": {
          "r": 0.015151515151515152,
          "p": 0.023809523809523808,
          "f": 0.018518513765433318
        },
        "rouge-l": {
          "r": 0.1347517730496454,
          "p": 0.22093023255813954,
          "f": 0.16740087635079287
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2503.10117v1",
      "true_abstract": "Using introduced concept of the exchange and inflation rates adequacy, the\nrelevance of them to the determining factors is found. We established close\npositive relation between hryvnia / dollar exchange and inflation rates, fiscal\ndeficit, price level of energy sources, and money supply. On this basis, we\ngive proposals for state macroeconomic policy to stabilize Ukrainian economy.",
      "generated_abstract": "aper, we investigate the optimal allocation of fixed-income securities\nin a continuous-time market with heterogeneous investors. We consider an\ninvestor who has a risk aversion parameter and the ability to hold the\nsecurities. The investor's decision is to either buy or sell at any time and\nthe security's price changes continuously. The investor can also choose to\ninvest in a fractional amount of securities. In this scenario, the optimal\nallocation depends on the investor's risk aversion parameter and the\ninvestor's fractional amount of securities. The optimal allocation is computed\nby solving a partial differential equation (PDE) model, which is a partial\ndifferential equation with a fractional derivative. The PDE model is a\nhighly non-linear and non-linear differential equation. We develop a numerical\nmethod to solve the PDE",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1875,
          "p": 0.1232876712328767,
          "f": 0.14876032579195425
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1875,
          "p": 0.1232876712328767,
          "f": 0.14876032579195425
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.09172v1",
      "true_abstract": "Low Ambiguity Zone (LAZ) sequences play a pivotal role in modern integrated\nsensing and communication (ISAC) systems. Recently, Wang et al.[1] proposed a\ndefinition of locally perfect nonlinear functions (LPNFs) and constructed three\nclasses of both periodic and aperiodic LAZ sequence sets with flexible\nparameters by applying such functions and interleaving method. Some of these\nLAZ sequence sets are asymptotically optimal with respect to the\nYe-Zhou-Liu-Fan-Lei-Tang bounds undercertain conditions. In this paper, we\nproceed with the construction of LPNFs with new parameters. By using these\nLPNFs, we also present a series of LAZ sequence sets with more flexible\nparameters, addressing the limitations of existing parameter choices.\nFurthermore, our results show that one of these classes is asymptotically\noptimal in both the periodic and aperiodic cases, respectively.",
      "generated_abstract": "r presents the design and implementation of a low-power wireless\nnetwork, built on a low-power radio, based on a novel approach to\nmulti-hop-based wireless communication. The proposed system is based on\nlow-power RFID technology to provide wireless connectivity. The system\nconsists of a low-power radio and a low-power processor, and is\nadaptable to various environments. The system is designed to operate on a\nlow-power radio, and the power consumption is reduced by 90% compared to\ntraditional wireless networks. The system also includes a low-power processor\nto reduce power consumption further. The system is designed to operate in\nlow-power mode, thereby reducing power consumption by 90%. The system also\nincludes a low-power processor to reduce power consumption further. The\nsystem is designed to operate in low-power mode, thereby reducing power\nconsumption by 90%. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12359550561797752,
          "p": 0.20754716981132076,
          "f": 0.15492957278615366
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12359550561797752,
          "p": 0.20754716981132076,
          "f": 0.15492957278615366
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.21141v1",
      "true_abstract": "How do transport infrastructures shape economic transformation and social\nchange? We examine the impact of railway expansion in nineteenth-century\nDenmark on local population growth, occupational shifts, and the diffusion of\nideas. Using a historical panel dataset and a difference-in-differences\napproach, we document that railway access significantly increased population\ngrowth and accelerated structural change. Moreover, railway-connected areas\nwere more likely to establish key institutions linked to civic engagement and\nthe cooperative movement. These findings suggest that improved market access\nwas not only a driver of economic modernization but also a catalyst for\ninstitutional and cultural transformation.",
      "generated_abstract": "We introduce a novel approach to the study of the dynamics of the global\nmarket for a good. We use a time-consistent model with repeated\ninteractions between agents who trade goods. The goods are embedded in\ncomplex networks, and we quantify the effect of these networks on the\ntrade dynamics. Our findings highlight the critical role of interactions in\ninfluencing the global market dynamics, and highlight the potential of\nnetworks to reshape trade dynamics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15789473684210525,
          "p": 0.2553191489361702,
          "f": 0.19512194649745532
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14473684210526316,
          "p": 0.23404255319148937,
          "f": 0.17886178389582932
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.05695v1",
      "true_abstract": "We develop a Functional Augmented Vector Autoregression (FunVAR) model to\nexplicitly incorporate firm-level heterogeneity observed in more than one\ndimension and study its interaction with aggregate macroeconomic fluctuations.\nOur methodology employs dimensionality reduction techniques for tensor data\nobjects to approximate the joint distribution of firm-level characteristics.\nMore broadly, our framework can be used for assessing predictions from\nstructural models that account for micro-level heterogeneity observed on\nmultiple dimensions. Leveraging firm-level data from the Compustat database, we\nuse the FunVAR model to analyze the propagation of total factor productivity\n(TFP) shocks, examining their impact on both macroeconomic aggregates and the\ncross-sectional distribution of capital and labor across firms.",
      "generated_abstract": "the joint estimation and inference of vector autoregressive (VAR)\nmodels with two or more dependent variables, a task that is inherently\nnon-linear and challenging for conventional methods such as the GMM estimator.\nWe introduce a new estimator, called the GMM-VAR-Mixture estimator, that\napproximates the joint distribution of the model parameters by means of a\nmixture of two GMM distributions. By leveraging the fact that the joint\ndistribution of the parameters can be expressed as a Gaussian mixture, the\nestimator is able to exploit the latent structure of the data, enabling more\nefficient estimation and inference. We demonstrate the effectiveness of the\nestimator through a series of simulation studies. Furthermore, we apply our\nmethodology to the VAR model with two dependent variables and show its\nsignificant potential in capturing the cross-dependency between the two\nvariables. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24705882352941178,
          "p": 0.25609756097560976,
          "f": 0.25149700098963756
        },
        "rouge-2": {
          "r": 0.038834951456310676,
          "p": 0.032520325203252036,
          "f": 0.035398225127653615
        },
        "rouge-l": {
          "r": 0.23529411764705882,
          "p": 0.24390243902439024,
          "f": 0.239520953085446
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.04453v1",
      "true_abstract": "Given the need to elucidate the mechanisms underlying illnesses and their\ntreatment, as well as the lack of harmonization of acquisition and\npost-processing protocols among different magnetic resonance system vendors,\nthis work is to determine if metabolite concentrations obtained from different\nsessions, machine models and even different vendors of 3 T scanners can be\nhighly reproducible and be pooled for diagnostic analysis, which is very\nvaluable for the research of rare diseases. Participants underwent magnetic\nresonance imaging (MRI) scanning once on two separate days within one week (one\nsession per day, each session including two proton magnetic resonance\nspectroscopy (1H-MRS) scans with no more than a 5-minute interval between scans\n(no off-bed activity)) on each machine. were analyzed for reliability of\nwithin- and between- sessions using the coefficient of variation (CV) and\nintraclass correlation coefficient (ICC), and for reproducibility of across the\nmachines using correlation coefficient. As for within- and between- session,\nall CV values for a group of all the first or second scans of a session, or for\na session were almost below 20%, and most of the ICCs for metabolites range\nfrom moderate (0.4-0.59) to excellent (0.75-1), indicating high data\nreliability. When it comes to the reproducibility across the three scanners,\nall Pearson correlation coefficients across the three machines approached 1\nwith most around 0.9, and majority demonstrated statistical significance\n(P<0.01). Additionally, the intra-vendor reproducibility was greater than the\ninter-vendor ones.",
      "generated_abstract": "st decade, Deep Learning (DL) has made remarkable progress in\nconducting complex tasks in multiple domains. However, its success is often\ninherently limited by its inherent inability to represent complex and\nintertwined relationships. In this paper, we propose a novel paradigm of\nrepresenting such relationships in DL, termed as Relational DL (RDL). RDL is a\nnovel paradigm for modeling and reasoning about complex and intertwined\nrelationships. It is built upon the idea that a Relational DL model can be\nviewed as a DL model that encodes the underlying relations between input\nattributes, and a learning task is formulated as a task of optimizing the\ndistribution of the learned model parameters. We present an overview of the\ncore ideas of RDL, including the definition of the learning task and the\nframework of the Relational DL model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09032258064516129,
          "p": 0.16470588235294117,
          "f": 0.11666666209201407
        },
        "rouge-2": {
          "r": 0.008733624454148471,
          "p": 0.016666666666666666,
          "f": 0.011461313539299483
        },
        "rouge-l": {
          "r": 0.07096774193548387,
          "p": 0.12941176470588237,
          "f": 0.09166666209201414
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/SC/2309.06907v3",
      "true_abstract": "Plasma membrane calcium influx through ion channels is crucial for many\nevents in cellular physiology. Cell surface stimuli lead to the production of\ninositol 1,4,5-trisphosphate (IP3), which binds to IP3 receptors in the\nendoplasmic reticulum (ER) to release calcium pools from the ER lumen. This\nleads to depletion of ER calcium pools which has been termed store-depletion.\nStore-depletion leads the dissociation of calcium ions from the EF-hand motif\nof the ER calcium sensor Stromal Interaction Molecule 1 (STIM1). This leads to\na conformational change in STIM1 which helps it to interact with a plasma\nmembrane (PM) at ER:PM junctions. At these ER:PM junctions, STIM1 binds to and\nactivates a calcium channel known as Orai1 to form calcium-release activated\ncalcium (CRAC) channels. Activation of Orai1 leads to calcium influx, known as\nstore-operated calcium entry (SOCE). In addition to Orai1 and STIM1, the\nhomologs of Orai1 and STIM1, such as Orai2/3 and STIM2 also play a crucial role\nin calcium homeostasis. The influx of calcium through the Orai channel\nactivates a calcium current that has been termed CRAC currents. CRAC channels\nform multimers and cluster together in large macromolecular assemblies termed\npuncta. How these CRAC channels form puncta has been contentious since their\ndiscovery. In this review, we will outline the history of SOCE, the molecular\nplayers involved in this process (Orai and STIM proteins, TRP channels,\nSOCE-associated regulatory factor etc.), as well as the models that have been\nproposed to explain this important mechanism in cellular physiology.",
      "generated_abstract": "ence of complex systems, from bacterial colonies to human societies,\nis a complex and interdisciplinary problem that requires the integration of\ndifferent theoretical and computational approaches. Recent advances in machine\nlearning have made it possible to model complex biological systems using\nartificial neural networks (ANNs). These models have been shown to exhibit\nremarkable performance in several tasks, including prediction of network\ntopology and dynamics, and detection of disease states. However, the use of\nthese models for more complex tasks, such as inferring the evolutionary\nhistory of a given system, remains limited due to the lack of a robust and\nscalable framework for this task. In this paper, we propose an approach that\nintegrates the capabilities of ANNs with the scalability and flexibility of\ndeep learning frameworks. Our approach, based on a hybrid neural network (HNN)\nmodel, is able to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1527777777777778,
          "p": 0.22,
          "f": 0.18032786401504985
        },
        "rouge-2": {
          "r": 0.022321428571428572,
          "p": 0.0364963503649635,
          "f": 0.02770082631532987
        },
        "rouge-l": {
          "r": 0.1527777777777778,
          "p": 0.22,
          "f": 0.18032786401504985
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/PR/2410.04748v2",
      "true_abstract": "We introduce a fairly general, recombining trinomial tree model in the\nnatural world. Market-completeness is ensured by considering a market\nconsisting of two risky assets, a riskless asset, and a European option. The\ntwo risky assets consist of a stock and a perpetual derivative of that stock.\nThe option has the stock and its derivative as its underlying. Using a\nreplicating portfolio, we develop prices for European options and generate the\nunique relationships between the risk-neutral and real-world parameters of the\nmodel. We discuss calibration of the model to empirical data in the cases in\nwhich the risky asset returns are treated as either arithmetic or logarithmic.\nFrom historical price and call option data for select large cap stocks, we\ndevelop implied parameter surfaces for the real-world parameters in the model.",
      "generated_abstract": "We introduce the concept of a price process in the framework of\nMarkov processes with a single state. This concept allows us to generalize the\nclass of Markov processes with a single state to more general classes of\nMarkov processes. We prove that any Markov process with a single state is a\nprice process. Moreover, we prove that any price process is a Markov process\nwith a single state. We also show that the class of Markov processes with a\nsingle state is closed under the product of two Markov processes with a single\nstate. In addition, we characterize the class of Markov processes with a single\nstate as the class of Markov processes with a single state and a state-independent\nPoisson process.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1728395061728395,
          "p": 0.32558139534883723,
          "f": 0.22580644708246628
        },
        "rouge-2": {
          "r": 0.041666666666666664,
          "p": 0.07142857142857142,
          "f": 0.052631574293629226
        },
        "rouge-l": {
          "r": 0.16049382716049382,
          "p": 0.3023255813953488,
          "f": 0.20967741482440178
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.04129v1",
      "true_abstract": "This work aims to synthesize a controller that ensures that an unknown\ndiscrete-time system is incrementally input-to-state stable ($\\delta$-ISS). In\nthis work, we introduce the notion of $\\delta$-ISS control Lyapunov function\n($\\delta$-ISS-CLF), which, in conjunction with the controller, ensures that the\nclosed-loop system is incrementally ISS. To address the unknown dynamics of the\nsystem, we parameterize the controller as well as the $\\delta$-ISS-CLF as\nneural networks and learn them by utilizing the sampled data from the state\nspace of the unknown system. To formally verify the obtained $\\delta$-ISS-CLF,\nwe develop a validity condition and incorporate the condition into the training\nframework to ensure a provable correctness guarantee at the end of the training\nprocess. Finally, the usefulness of the proposed approach is proved using\nmultiple case studies - the first one is a scalar system with a non-affine\nnon-polynomial structure, the second example is a one-link manipulator system,\nthe third system is a nonlinear Moore-Grietzer model of the jet engine and the\nfinal one is a rotating rigid spacecraft model.",
      "generated_abstract": "r presents a novel adaptive multi-agent reinforcement learning\n(MARL) framework for an uncertain, nonlinear multi-agent system, aiming to\nachieve optimal control while accounting for uncertainty in the system\nparameters. The proposed framework includes a learning agent that learns the\nparameter uncertainty, and an adaptive controller that exploits this\nuncertainty to achieve optimal control. The learning agent is trained with\ndiverse problem instances, and is capable of generalizing to new problem\ninstances with limited data. The adaptive controller is designed to exploit the\ngeneralization capability of the learning agent, and is capable of adapting to\ndifferent problem instances with limited data. The effectiveness of the\nproposed framework is verified through simulations, where the learning agent\neffectively learns the parameter uncertainty, and the adaptive controller\neffectively exploits this uncertainty to achieve optimal control. This\nframework not only provides a novel solution",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18867924528301888,
          "p": 0.2857142857142857,
          "f": 0.2272727224819216
        },
        "rouge-2": {
          "r": 0.025806451612903226,
          "p": 0.03669724770642202,
          "f": 0.030303025454833193
        },
        "rouge-l": {
          "r": 0.16037735849056603,
          "p": 0.24285714285714285,
          "f": 0.19318181339101254
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.09998v1",
      "true_abstract": "We numerically investigate the sensitivity of the scattered wave field to\nperturbations in the shape of a scattering body illuminated by an incident\nplane wave. This study is motivated by recent work on the inverse problem of\nreconstructing a scatterer shape from measurements of the scattered wave at\nlarge distances from the scatterer. For this purpose we consider star-shaped\nscatterers represented using cubic splines, and our approach is based on a\nNystr\\\"om method-based discretisation of the shape derivative. Using the\nsingular value decomposition, we identify fundamental geometric modes that most\nstrongly influence the scattered wave, providing insight into the most visible\nboundary features in scattering data.",
      "generated_abstract": "In this paper, we study the numerical approximation of the solution to\nthe coupled system of Stokes and wave equations with the Dirichlet or\nNeumann boundary condition. We provide a numerical scheme to solve the\ncoupled system using the finite element method. The main difficulty in the\nnumerical solution of the coupled system is that the solution to the Stokes\nequation is difficult to obtain, and the solution to the wave equation is\ndifficult to obtain. In order to solve the coupled system, we need to solve\nseveral coupled linear systems. To solve these linear systems, we propose a\ncoupled iterative scheme. We show that this coupled iterative scheme converges\nexponentially. This paper provides a novel numerical scheme to solve the\ncoupled Stokes-wave system and the coupled Stokes-wave-Neumann system.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21052631578947367,
          "p": 0.25806451612903225,
          "f": 0.23188405302247436
        },
        "rouge-2": {
          "r": 0.020202020202020204,
          "p": 0.02,
          "f": 0.02010049751269032
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.1935483870967742,
          "f": 0.1739130385297208
        }
      }
    },
    {
      "paper_id": "math.OA.math/OA/2503.03498v1",
      "true_abstract": "In this paper, we provide a comprehensive analysis of involutive quantales,\nwith a particular focus on quantic frames. We extend the axiomatic foundations\nof quantale-enriched topological spaces to include closure under the\nanti-homomorphic involution, facilitating a balanced topologization of the\nspectrum of unital $C^*$-algebras that encompasses both closed right and left\nideals through the concept of quantic frames. Specifically, certain subspaces\nof pure states are identified as strongly Hausdorff separated quantale-enriched\ninvolutive topological spaces.",
      "generated_abstract": "e a smooth projective variety over an algebraically closed field\n$k$ of characteristic $0$. We define the cohomological Hilbert-Chow module of\n$X$ to be the quotient of the Hilbert-Chow group of $X$ by the action of the\ngroup of ample divisors on $X$. We also define the Hilbert-Chow group of $X$\nto be the quotient of the group of divisors on $X$ by the action of the\ngroup of ample divisors on $X$.\n  We prove that the cohomological Hilbert-Chow module of a smooth projective\nvariety is isomorphic to the Hilbert-Chow group of the normalization of the\nvariety. We also prove that the Hilbert-Chow group of a smooth projective\nvariety is isomorphic to the quotient of the group of divisors on the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13793103448275862,
          "p": 0.2222222222222222,
          "f": 0.17021276123132653
        },
        "rouge-2": {
          "r": 0.014084507042253521,
          "p": 0.017543859649122806,
          "f": 0.015624995059816014
        },
        "rouge-l": {
          "r": 0.1206896551724138,
          "p": 0.19444444444444445,
          "f": 0.1489361654866457
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/MN/2410.02463v1",
      "true_abstract": "In this study, the predominant lactic acid bacteria (LAB) isolates were\nobtained from Gouda, Jack, Cheddar, and Parmesan cheeses produced in Uganda.\nThe isolates were identified through Gram staining, catalase and oxidase tests,\nand 16S rDNA sequencing. Approximately 90% of the isolates were cocci (n=192),\nincluding Streptococcus, Enterococcus, and Lactococcus. The remaining 10% were\nidentified as rod-shaped bacteria, primarily belonging to the Lactobacillus\nspecies (n=23). BLAST analysis revealed that Pediococcus pentosaceus dominated\nin all cheese samples (23.7%, of the total 114 isolates). This was followed by\nuncultured bacterium (15.8%), uncultured Pediococcus species (13.2%),\nLacticaseibacillus rhamnosus (8.8%) among others",
      "generated_abstract": "the dynamics of a biological system as a Markov chain in a\nstate space $X$ with a discrete set of transitions $\\Delta$ determined by the\nenvironment. The dynamics are described by a transition kernel $P$ and the\nstate space is endowed with a probability measure $\\mu$ on $\\Delta$. We\nintroduce a random initial state $X_0$ and assume that the system starts in a\nrandom state $X_0$ and evolves according to $P$ and $\\mu$. The goal is to\ndetermine the transition kernel $P$ and the probability measure $\\mu$ under\nwhich the system evolves to a stationary distribution $\\pi$ that is\narbitrarily close to $\\mu$ in the sense of the Wasserstein metric. This problem\nis a generalization of the well-known Markov chain Monte Carlo problem and\nmotivated by applications to epidemiology and stochastic control",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11904761904761904,
          "p": 0.14705882352941177,
          "f": 0.1315789424238229
        },
        "rouge-2": {
          "r": 0.010309278350515464,
          "p": 0.008695652173913044,
          "f": 0.00943395730019841
        },
        "rouge-l": {
          "r": 0.11904761904761904,
          "p": 0.14705882352941177,
          "f": 0.1315789424238229
        }
      }
    },
    {
      "paper_id": "physics.acc-ph.physics/acc-ph/2503.05192v1",
      "true_abstract": "Symplectic integrator plays a pivotal role in the long-term tracking of\ncharged particles within accelerators. To get symplectic maps in accurate\nsimulation of single-particle trajectories, two key components are addressed:\nprecise analytical expressions for arbitrary electromagnetic fields and a\nrobust treatment of the equations of motion. In a source-free region, the\nelectromagnetic fields can be decomposed into harmonic functions, applicable to\nboth scalar and vector potentials, encompassing both straight and curved\nreference trajectories. These harmonics are constructed according to the\nboundary surface's field data due to uniqueness theorem. Finding generating\nfunctions to meet the Hamilton-Jacobi equation via a perturbative ansatz, we\nderive symplectic maps that are independent of the expansion order. This method\nyields a direct mapping from initial to final coordinates in a large step,\nbypassing the transition of intermediate coordinates. Our developed\nparticle-tracking algorithm translates the field harmonics into beam\ntrajectories, offering a high-resolution integration method in accelerator\nsimulations.",
      "generated_abstract": "t a novel method for measuring the energy deposited by charged\ntrajectories in a magnetic field, which we call the energy-deposition method.\nThis method is based on a simple modification of the well-known energy-deposition\nmethod for charged particles, where the method is applied to charged trajectories\ninstead of charged particles. The method is applicable to any magnetic field\nconfiguration, regardless of the symmetry of the magnetic field, which means\nthat it is applicable to any magnetic field configuration. The method allows\nfor the measurement of the energy deposited by charged trajectories in any\nmagnetic field configuration, including magnetic field configurations with\nnon-zero components perpendicular to the magnetic field. The method is based on\nthe fact that the energy deposited by a charged trajectory in a magnetic field\nis a function of the magnetic field component perpendicular to the trajectory\nand the magnetic field direction. We demonstrate the method's",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15315315315315314,
          "p": 0.2786885245901639,
          "f": 0.19767441402717695
        },
        "rouge-2": {
          "r": 0.047619047619047616,
          "p": 0.06862745098039216,
          "f": 0.05622489476169781
        },
        "rouge-l": {
          "r": 0.14414414414414414,
          "p": 0.26229508196721313,
          "f": 0.18604650705043277
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.10230v1",
      "true_abstract": "Lucatelli Nunes obtained a 2-categorical version of the adjoint triangle\ntheorem of Dubuc using the descent object of a specific diagram. In some cases,\nsuch a diagram can be filled with an extra cell. We show then how to obtain a\nbiadjoint as an inverter of this additional datum (under suitable hypotheses).\nThe problem addressed here is slightly different however: we still have a\ntriangle of pseudofunctors but the lifted biadjoint is not the same. The\nconstruction is simplified when the pseudofunctor whose left biadjoint is\nsought is fully faithful. As an example, we get the biadjoint of the inclusion\npseudofunctor of a bicategory associated to a KZ-monad preserving\npseudomonicity.",
      "generated_abstract": "uce a framework for studying the geometric properties of the\nfinite group\ngenerated by the two elements $a,b$ in a finite group $G$ satisfying certain\nconditions. We obtain a formula for the number of subgroups of a fixed size in\nthis group. We also give a complete description of the groups for which this\nformula is non-zero. We also give a formula for the number of conjugacy classes\nof a fixed size in this group. We obtain a formula for the number of\nsubconjugacy classes of a fixed size in this group. We obtain a formula for the\nnumber of conjugacy classes of a fixed size in the direct product of two finite\ngroups. We give a formula for the number of subconjugacy classes of a fixed\nsize in the direct product of two finite groups. We also give a formula for the\nnumber of conjugacy classes of a fixed size in the direct product of two finite",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09090909090909091,
          "p": 0.16666666666666666,
          "f": 0.11764705425605555
        },
        "rouge-2": {
          "r": 0.02830188679245283,
          "p": 0.043478260869565216,
          "f": 0.03428570950922516
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.16666666666666666,
          "f": 0.11764705425605555
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.02156v1",
      "true_abstract": "WiFi-based mobility monitoring in urban environments can provide valuable\ninsights into pedestrian and vehicle movements. However, MAC address\nrandomization introduces a significant obstacle in accurately estimating\ncongestion levels and path trajectories. To this end, we consider radio\nfrequency fingerprinting and re-identification for attributing WiFi traffic to\nemitting devices without the use of MAC addresses.\n  We present MobRFFI, an AI-based device fingerprinting and re-identification\nframework for WiFi networks that leverages an encoder deep learning model to\nextract unique features based on WiFi chipset hardware impairments. It is\nentirely independent of frame type. When evaluated on the WiFi fingerprinting\ndataset WiSig, our approach achieves 94% and 100% device accuracy in multi-day\nand single-day re-identification scenarios, respectively.\n  We also collect a novel dataset, MobRFFI, for granular multi-receiver WiFi\ndevice fingerprinting evaluation. Using the dataset, we demonstrate that the\ncombination of fingerprints from multiple receivers boosts re-identification\nperformance from 81% to 100% on a single-day scenario and from 41% to 100% on a\nmulti-day scenario.",
      "generated_abstract": "This paper presents a novel 4-dimensional (4D) joint beamforming and channel\ninverse problem (CIP) framework for a multi-user massive multiple-input\nmultiple-output (MIMO) system with time-varying channels. In the proposed\nframework, the channel state information (CSI) is assumed to be available\nduring the beamforming design phase. The 4D CIP problem is formulated as a\nconstrained optimization problem, where the constraints are derived from the\nchannel CSI. To tackle the challenging nonconvexity of the problem, a\nsubmodular optimization method is proposed to approximate the problem and\nidentify the optimal beamforming design. Simulation results demonstrate the\neffectiveness of the proposed method in improving the performance of the\nsystem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12612612612612611,
          "p": 0.19718309859154928,
          "f": 0.1538461490876707
        },
        "rouge-2": {
          "r": 0.012987012987012988,
          "p": 0.020202020202020204,
          "f": 0.01581027191613823
        },
        "rouge-l": {
          "r": 0.12612612612612611,
          "p": 0.19718309859154928,
          "f": 0.1538461490876707
        }
      }
    },
    {
      "paper_id": "physics.geo-ph.physics/geo-ph/2503.02815v1",
      "true_abstract": "Full waveform inversion (FWI) plays an important role in velocity modeling\ndue to its high-resolution advantages. However, its highly non-linear\ncharacteristic leads to numerous local minimums, which is known as the\ncycle-skipping problem. Therefore, effectively addressing the cycle-skipping\nissue is crucial to the success of FWI. Well-log data contain rich information\nabout subsurface medium parameters, providing inherent advantages for velocity\nmodeling. Traditional well-log data interpolation methods to build velocity\nmodels have limited accuracy and poor adaptability to complex geological\nstructures. This study introduces a well interpolation algorithm based on a\ngenerative diffusion model (GDM) to generate initial models for FWI, addressing\nthe cycle-skipping problem. Existing convolutional neural network (CNN)-based\nmethods face difficulties in handling complex feature distributions and lack\neffective uncertainty quantification, limiting the reliability of their\noutputs. The proposed GDM-based approach overcomes these challenges by\nproviding geologically consistent well interpolation while incorporating\nuncertainty assessment. Numerical experiments demonstrate that the method\nproduces accurate and reliable initial models, enhancing FWI performance and\nmitigating cycle-skipping issues.",
      "generated_abstract": "r presents a new approach to the simulation of the transport of\nparticles in a three-dimensional fluid flow field. The methodology is based on\nthe use of the Finite-Volume Method (FVM) in the time domain, with the\nassumption that the flow field is characterised by a three-dimensional\nthree-dimensional Fourier-domain velocity field. The method is demonstrated for\nthe simulation of the flow of a particle in the context of a fluid flow over a\nchannel. The methodology is applied to the simulation of the flow of a\nparticle in a channel. The results demonstrate that the methodology is\neffective for the simulation of the flow of a particle in a channel with\nsmooth flow fields, with a reasonable accuracy. The methodology is also\napplicable to the simulation of the flow of a particle in a channel with\nparticularly sharp flow fields. The methodology is applied to the simulation of\nthe flow",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.144,
          "p": 0.3333333333333333,
          "f": 0.20111731422240264
        },
        "rouge-2": {
          "r": 0.025477707006369428,
          "p": 0.045454545454545456,
          "f": 0.032653056621075205
        },
        "rouge-l": {
          "r": 0.144,
          "p": 0.3333333333333333,
          "f": 0.20111731422240264
        }
      }
    },
    {
      "paper_id": "cs.CR.q-bio/GN/2411.16744v1",
      "true_abstract": "Counting distinct permutations with replacement, especially when involving\nmultiple subwords, is a longstanding challenge in combinatorial analysis, with\ncritical applications in cryptography, bioinformatics, and statistical\nmodeling. This paper introduces a novel framework that presents closed-form\nformulas for calculating distinct permutations with replacement, fundamentally\nreducing the time complexity from exponential to linear relative to the\nsequence length for single-subword calculations. We then extend our\nfoundational formula to handle multiple subwords through the development of an\nadditional formula. Unlike traditional methods relying on brute-force\nenumeration or recursive algorithms, our approach leverages novel combinatorial\nconstructs and advanced mathematical techniques to achieve unprecedented\nefficiency. This comprehensive advancement in reducing computational complexity\nnot only simplifies permutation counting but also establishes a new benchmark\nfor scalability and versatility. We also demonstrate the practical utility of\nour formulas through diverse applications, including the simultaneous\nidentification of multiple genetic motifs in DNA sequences and complex pattern\nanalysis in cryptographic systems, using a computer program that runs the\nproposed formulae.",
      "generated_abstract": "ntext of the COVID-19 pandemic, rapid identification of\ncovariates for disease progression, and the development of early-warning\nsystems to predict the onset of disease, is of paramount importance. We\npresent a novel framework for the identification of covariates that are\nassociated with the progression of COVID-19, and are indicative of the onset of\nthe disease. We develop a statistical framework that incorporates\nmulti-stage-diagnostic-analysis (MSDA) to identify covariates that are\nassociated with the progression of COVID-19, and that are indicative of the\nonset of the disease. Our proposed framework consists of three stages. The\nfirst stage, MSDA-1, involves a pre-processing step that involves the\nidentification of covariates that are associated with the progression of the\ndisease, and the identification of covariates",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13445378151260504,
          "p": 0.3076923076923077,
          "f": 0.18713449869156332
        },
        "rouge-2": {
          "r": 0.03821656050955414,
          "p": 0.07407407407407407,
          "f": 0.05042016357707829
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.2692307692307692,
          "f": 0.16374268582606627
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.09325v1",
      "true_abstract": "We initiate the study of $\\lambda$-fold near-factorizations of groups with\n$\\lambda > 1$. While $\\lambda$-fold near-factorizations of groups with $\\lambda\n= 1$ have been studied in numerous papers, this is the first detailed treatment\nfor $\\lambda > 1$. We establish fundamental properties of $\\lambda$-fold\nnear-factorizations and introduce the notion of equivalence. We prove various\nnecessary conditions of $\\lambda$-fold near-factorizations, including upper\nbounds on $\\lambda$. We present three constructions of infinite families of\n$\\lambda$-fold near-factorizations, highlighting the characterization of two\nsubfamilies of $\\lambda$-fold near-factorizations. We discuss a computational\napproach to $\\lambda$-fold near-factorizations and tabulate computational\nresults for abelian groups of small order.",
      "generated_abstract": "aper, we introduce a new class of $\\mathcal{N}=2$ vector bundles\n(called the $\\mathcal{N}=2$ vector bundles over a base) and investigate their\ngeometric properties. In particular, we characterize the global sections of\nthese vector bundles over a base, and we define a notion of $\\mathcal{N}=2$\nvector bundles over a base. We show that the vector bundles over a base that\nsatisfy a certain geometric condition are isomorphic to the $\\mathcal{N}=2$\nvector bundles over a base, and we classify the $\\mathcal{N}=2$ vector bundles\nover a base. In particular, we show that the $\\mathcal{N}=2$ vector bundles\nover a base are isomorphic to the $\\mathcal{N}=1$ vector bundles over a base.\nFinally, we show that the $\\mathcal{N}=2",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14516129032258066,
          "p": 0.21428571428571427,
          "f": 0.17307691826183447
        },
        "rouge-2": {
          "r": 0.011904761904761904,
          "p": 0.015873015873015872,
          "f": 0.013605437278913326
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.19047619047619047,
          "f": 0.15384614903106522
        }
      }
    },
    {
      "paper_id": "math.CO.math/RT/2503.06975v1",
      "true_abstract": "Given two affine permutations, some results of Lascoux and Deodhar, and\nindependently Jacon-Lecouvey, allow to decide if they are comparable for the\nstrong Bruhat order. These permutations are associated with tuples of core\npartitions, and the preceding problem is equivalent to compare the Young\ndiagrams in each components for the inclusion. Using abaci, we give an easy\nrule to compute these Young diagrams one another. We deduce a procedure to\ncompare, for the Bruhat order, two affine permutations in the window notation.",
      "generated_abstract": "igate the global dynamics of a class of discrete dynamical systems\ndefined by a finite set of nonlinear equations with non-smooth and non-linear\ncoefficients. We introduce a novel approach to study these systems, based on\nthe construction of a natural space of solutions. We prove that these systems\nadmit a non-trivial global attractor, and we characterize its structure. We\nalso provide a novel way to define a Lyapunov exponent associated with the\ndynamics of these systems. The latter is given by a functional of the\ncorresponding Lyapunov function, and its properties are analyzed in detail.\nFinally, we apply our results to a particular class of systems, whose\ndynamics is governed by a recurrent pattern of oscillations. We show that the\nLyapunov exponent associated with these oscillations can be expressed in terms\nof the fundamental matrix of the system. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22950819672131148,
          "p": 0.1728395061728395,
          "f": 0.19718309369073608
        },
        "rouge-2": {
          "r": 0.012987012987012988,
          "p": 0.007936507936507936,
          "f": 0.009852212040090581
        },
        "rouge-l": {
          "r": 0.22950819672131148,
          "p": 0.1728395061728395,
          "f": 0.19718309369073608
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.04565v1",
      "true_abstract": "Panoramic imagery, with its 360{\\deg} field of view, offers comprehensive\ninformation to support Multi-Object Tracking (MOT) in capturing spatial and\ntemporal relationships of surrounding objects. However, most MOT algorithms are\ntailored for pinhole images with limited views, impairing their effectiveness\nin panoramic settings. Additionally, panoramic image distortions, such as\nresolution loss, geometric deformation, and uneven lighting, hinder direct\nadaptation of existing MOT methods, leading to significant performance\ndegradation. To address these challenges, we propose OmniTrack, an\nomnidirectional MOT framework that incorporates Tracklet Management to\nintroduce temporal cues, FlexiTrack Instances for object localization and\nassociation, and the CircularStatE Module to alleviate image and geometric\ndistortions. This integration enables tracking in large field-of-view\nscenarios, even under rapid sensor motion. To mitigate the lack of panoramic\nMOT datasets, we introduce the QuadTrack dataset--a comprehensive panoramic\ndataset collected by a quadruped robot, featuring diverse challenges such as\nwide fields of view, intense motion, and complex environments. Extensive\nexperiments on the public JRDB dataset and the newly introduced QuadTrack\nbenchmark demonstrate the state-of-the-art performance of the proposed\nframework. OmniTrack achieves a HOTA score of 26.92% on JRDB, representing an\nimprovement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing the\nbaseline by 6.81%. The dataset and code will be made publicly available at\nhttps://github.com/xifen523/OmniTrack.",
      "generated_abstract": "r presents a novel framework for 3D object segmentation in\nsegmentation-based multi-view image synthesis (SB-MVIs), which integrates\nsemantic segmentation and synthesis by leveraging the 3D segmentation result.\nThe proposed method consists of two stages: the first stage is a semantic\nsegmentation stage that predicts the 3D object masks and the second stage is a\nsynthesis stage that generates the 3D objects. First, we introduce a\nsemantic-guided 3D object mask generation method. In the semantic-guided\n3D object mask generation method, we first predict the semantic masks by a\nsemantic-guided 3D mask prediction network, and then use these semantic masks\nto generate the 3D object masks. Second, we introduce a 3D synthesis network\nthat synthesizes the 3D objects based on the semantic mask",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12025316455696203,
          "p": 0.31666666666666665,
          "f": 0.17431192261594153
        },
        "rouge-2": {
          "r": 0.019138755980861243,
          "p": 0.041237113402061855,
          "f": 0.026143786519501763
        },
        "rouge-l": {
          "r": 0.10759493670886076,
          "p": 0.2833333333333333,
          "f": 0.15596329876273052
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.07997v1",
      "true_abstract": "Autonomous stores leverage advanced sensing technologies to enable\ncashier-less shopping, real-time inventory tracking, and seamless customer\ninteractions. However, these systems face significant challenges, including\nocclusion in vision-based tracking, scalability of sensor deployment, theft\nprevention, and real-time data processing. To address these issues, researchers\nhave explored multi-modal sensing approaches, integrating computer vision,\nRFID, weight sensing, vibration-based detection, and LiDAR to enhance accuracy\nand efficiency. This survey provides a comprehensive review of sensing\ntechnologies used in autonomous retail environments, highlighting their\nstrengths, limitations, and integration strategies. We categorize existing\nsolutions across inventory tracking, environmental monitoring, people-tracking,\nand theft detection, discussing key challenges and emerging trends. Finally, we\noutline future directions for scalable, cost-efficient, and privacy-conscious\nautonomous store systems.",
      "generated_abstract": "r presents a novel approach for the estimation of the phase\ndistribution of the surface of the sphere with a finite number of measurements.\nThis problem is of fundamental importance in many applications, such as\ncommunications, radar, and seismology. It is well-known that the phase\ndistribution of the sphere can be expressed as a sum of an infinite number of\nindependent circular waves, each of which is characterized by a phase angle\n$\\theta$. The problem is to estimate the phase angles $\\theta$ of the circular\nwaves, from a finite number of measurements. In this paper, we focus on the\ncase of the sphere with a finite number of measurements. We present a\nrepresentation of the problem as a quadratic optimization problem, which is\nsolved using the classical method of Lagrange multipliers. We also present an\nalternative representation of the problem as a problem of finding the\namplitude of the phase angle at each point of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09574468085106383,
          "p": 0.11392405063291139,
          "f": 0.10404623781215568
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09574468085106383,
          "p": 0.11392405063291139,
          "f": 0.10404623781215568
        }
      }
    },
    {
      "paper_id": "cs.SC.cs/SC/2503.07342v1",
      "true_abstract": "In this work, we introduce a novel variant of the multivariate quadratic\nproblem, which is at the core of one of the most promising post-quantum\nalternatives: multivariate cryptography. In this variant, the solution of a\ngiven multivariate quadratic system must also be regular, i.e. if it is split\ninto multiple blocks of consecutive entries with the same fixed length, then\neach block has only one nonzero entry. We prove the NP-completeness of this\nvariant and show similarities and differences with other computational problems\nused in cryptography. Then we analyze its hardness by reviewing the most common\nsolvers for polynomial systems over finite fields, derive asymptotic formulas\nfor the corresponding complexities and compare the different approaches.",
      "generated_abstract": "r proposes a novel approach for the design of scalable and\nfederated learning (FL) models that can effectively utilize data from multiple\ndistinct data sources while preserving privacy and protecting sensitive data.\nThe key idea is to design a novel framework that integrates the\nfederated learning (FL) framework with data integration and federated\nprivacy preserving (FPP) techniques to enhance the effectiveness of FL\napplications. This framework, named Federated PIMPL, is designed to enable\nFederated Learning models to operate effectively in multi-source data\nenvironments. It combines the FL framework with data integration and FPP\ntechniques to enhance the effectiveness of FL applications. This framework\nprovides a comprehensive solution for enhancing the performance of FL models in\nmulti-source data environments by integrating data integration and FPP\ntechniques. In",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1511627906976744,
          "p": 0.19402985074626866,
          "f": 0.16993463559998306
        },
        "rouge-2": {
          "r": 0.018018018018018018,
          "p": 0.019801980198019802,
          "f": 0.01886791953942816
        },
        "rouge-l": {
          "r": 0.12790697674418605,
          "p": 0.16417910447761194,
          "f": 0.14379084475030987
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.06638v1",
      "true_abstract": "In the paper the joint optimization of uplink multiuser power and resource\nblock (RB) allocation are studied, where each user has quality of service (QoS)\nconstraints on both long- and short-blocklength transmissions. The objective is\nto minimize the consumption of RBs for meeting the QoS requirements, leading to\na mixed-integer nonlinear programming (MINLP) problem. We resort to deep\nlearning to solve the problem with low inference complexity. To provide a\nperformance benchmark for learning based methods, we propose a hierarchical\nalgorithm to find the global optimal solution in the single-user scenario,\nwhich is then extended to the multiuser scenario. The design of the learning\nmethod, however, is challenging due to the discrete policy to be learned, which\nresults in either vanishing or exploding gradient during neural network\ntraining. We introduce two types of smoothing functions to approximate the\ninvolved discretizing processes and propose a smoothing parameter adaption\nmethod. Another critical challenge lies in guaranteeing the QoS constraints. To\naddress it, we design a nonlinear function to intensify the penalties for minor\nconstraint violations. Simulation results demonstrate the advantages of the\nproposed method in reducing the number of occupied RBs and satisfying QoS\nconstraints reliably.",
      "generated_abstract": "r presents a novel, multi-channel, multilayered deep learning (MLDL)\nmodel for the multi-channel, multi-layered (MCML) prediction of electrocardiogram\n(ECG) signals. The proposed MCDL model integrates four different\nMLDL-based predictors, each with its own specificities, into a unified\nframework. The predictors are based on a novel architecture that combines\nneural networks and transformer models. Each predictor, which is trained on\nindependent subsets of the ECG dataset, is characterized by a specific\narchitecture. The architecture of the predictor is trained on a subset of the\ndataset. The architecture of the predictor is then used to predict the next\nECG signal. The four predictors are then fused using an attention mechanism.\nThe fused predictor is used to predict the next ECG signal. The MCDL model is\nvalid",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14615384615384616,
          "p": 0.25333333333333335,
          "f": 0.18536584901844153
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.038834951456310676,
          "f": 0.027491404361309667
        },
        "rouge-l": {
          "r": 0.14615384615384616,
          "p": 0.25333333333333335,
          "f": 0.18536584901844153
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/PL/2503.07328v1",
      "true_abstract": "Reachability Types (RT) are a qualified type system for tracking aliasing and\nseparation in functional and higher-order programming. By formalizing resource\nreachability with a sound static type system, RT enable higher-order\nprogramming patterns with runtime safety and non-interference guarantees.\nHowever, previous RT systems have been based on calculi that restrict cyclic\ndependencies and are shown to be terminating in the absence of built-in\nrecursive constructs. While termination is sometimes a desirable property,\nsimplifying reasoning and ensuring predictable behavior, it implies an\ninability to encode expressive programs involving non-termination and advanced\nrecursive patterns, such as mutual recursion and various fixed-point\ncombinators.\n  In this paper, we address this limitation by extending RT with an expressive\ncyclic reference type that permits the formation of cyclic dependencies through\nthe store, thereby allowing the system to encode recursive programming patterns\nwithout relying on extra built-in constructs. In addition, we redesign\nqualifier typing in the reference introduction rule, allowing separate\nreferences to point to a shared and tracked referent. We formalize the system\nas the $\\lambda^{\\circ}_{<:}$-calculus, with a mechanized soundness proof via\nthe standard progress and preservation lemmas. As a demonstration, we implement\na well-typed fixpoint operator, proving that recursive patterns can be encoded\nusing the novel cyclic reference type.",
      "generated_abstract": "We present a novel, simple, and scalable approach to optimizing the size of\ncode in functional languages. Our approach leverages a data structure called\nPiecewise-Partial (PWP), which is a recursive data structure with a single\nnode for each function. Using PWP, we maintain a list of PWPs, which represent\nthe partial derivatives of a function with respect to different input\nvariables. We then use these partial derivatives to prune the search tree of\nthe functional interpreter, resulting in a smaller code size. We demonstrate\nthat our approach outperforms standard optimization techniques such as\noptimizing the interpreter's tail call elimination and the use of tail call\noptimizations, both of which can increase the code size.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13333333333333333,
          "p": 0.25,
          "f": 0.173913038941399
        },
        "rouge-2": {
          "r": 0.015463917525773196,
          "p": 0.027777777777777776,
          "f": 0.019867545074339906
        },
        "rouge-l": {
          "r": 0.1259259259259259,
          "p": 0.2361111111111111,
          "f": 0.1642512031926067
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.08311v1",
      "true_abstract": "Inference in linear panel data models is complicated by the presence of fixed\neffects when (some of) the regressors are not strictly exogenous. Under\nasymptotics where the number of cross-sectional observations and time periods\ngrow at the same rate, the within-group estimator is consistent but its limit\ndistribution features a bias term. In this paper we show that a panel version\nof the moving block bootstrap, where blocks of adjacent cross-sections are\nresampled with replacement, replicates the limit distribution of the\nwithin-group estimator. Confidence ellipsoids and hypothesis tests based on the\nreverse-percentile bootstrap are thus asymptotically valid without the need to\ntake the presence of bias into account.",
      "generated_abstract": "This paper develops a stochastic frontier model for estimating the\nparameters of a frontier model. The model is structurally similar to the\nframeworks of Cattaneo and Sarkozy (2001) and Cattaneo and Sarkozy (2004). The\nmodel accounts for the multivariate dependency structure of the frontier\nparameters. The paper shows that the model can be estimated with the standard\nCattaneo and Sarkozy estimator, and also with a more flexible estimator. The\npaper presents simulation results to illustrate the theoretical findings and\ndemonstrate the performance of the estimator. The results show that the\nestimator performs well, especially when the multivariate dependency structure\nis moderate.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15,
          "p": 0.22641509433962265,
          "f": 0.18045112302560928
        },
        "rouge-2": {
          "r": 0.019801980198019802,
          "p": 0.023255813953488372,
          "f": 0.021390369363723304
        },
        "rouge-l": {
          "r": 0.15,
          "p": 0.22641509433962265,
          "f": 0.18045112302560928
        }
      }
    },
    {
      "paper_id": "eess.IV.q-bio/TO/2411.00749v1",
      "true_abstract": "Accurate survival prediction is essential for personalized cancer treatment.\nHowever, genomic data - often a more powerful predictor than pathology data -\nis costly and inaccessible. We present the cross-modal genomic feature\ntranslation and alignment network for enhanced survival prediction from\nhistopathology images (PathoGen-X). It is a deep learning framework that\nleverages both genomic and imaging data during training, relying solely on\nimaging data at testing. PathoGen-X employs transformer-based networks to align\nand translate image features into the genomic feature space, enhancing weaker\nimaging signals with stronger genomic signals. Unlike other methods, PathoGen-X\ntranslates and aligns features without projecting them to a shared latent space\nand requires fewer paired samples. Evaluated on TCGA-BRCA, TCGA-LUAD, and\nTCGA-GBM datasets, PathoGen-X demonstrates strong survival prediction\nperformance, emphasizing the potential of enriched imaging models for\naccessible cancer prognosis.",
      "generated_abstract": "he widespread adoption of Magnetic Resonance Imaging (MRI) in\npatients with brain tumors, their clinical significance is often obscured by\nthe complexity of the tumor environment. In this work, we propose a novel\ndeep learning framework to analyze the tumor microenvironment (TME) of\ndiffuse-inflamed astrocytoma (DIA) and oligodendroglioma (OLiG). By leveraging\nthe inherent geometric and structural similarity between MRI scans of DIA and\nOLiG, our method is able to distinguish DIA and OLiG with an average area under\nthe receiver operating characteristic curve (AUROC) of 0.827 and 0.818,\nrespectively, surpassing existing state-of-the-art methods. The model's\narchitecture is based on a dual-t",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12371134020618557,
          "p": 0.14457831325301204,
          "f": 0.13333332836358042
        },
        "rouge-2": {
          "r": 0.015625,
          "p": 0.02,
          "f": 0.017543854724532005
        },
        "rouge-l": {
          "r": 0.1134020618556701,
          "p": 0.13253012048192772,
          "f": 0.12222221725246933
        }
      }
    },
    {
      "paper_id": "math.AP.math/AP/2503.09307v1",
      "true_abstract": "We consider a broad class of nonlinear integro-differential equations with a\nkernel whose differentiability order is described by a general function $\\phi$.\nThis class includes not only the fractional $p$-Laplace equations, but also\nborderline cases when the fractional order approaches $1$. Under mild\nassumptions on $\\phi$, we establish sharp Sobolev-Poincar\\'e type inequalities\nfor the associated Sobolev spaces, which are connected to a question raised by\nBrezis (Russian Math. Surveys 57:693--708, 2002). Using these inequalities, we\nprove H\\\"older regularity and Harnack inequalities for weak solutions to such\nnonlocal equations. All the estimates in our results remain stable as the\nassociated nonlocal energy functional approaches its local counterpart.",
      "generated_abstract": "This paper studies the evolution of a discrete-time Markov chain whose\nstates are represented by a partition of a set $X$. The evolution is\ndeterministic and the transition kernel is given by a finite matrix. The\ndynamics of this system is investigated in the case of a partition of $X$ into\ntwo disjoint sets. The main result is that, when the partitions are\n$\\mathbb{Z}_n$-shifts of each other, the chain converges weakly to a\nsuper-diffusive Markov chain on $X$ with drift and diffusion coefficient\ninversely proportional to the number of vertices of the partition. This\nconvergence is independent of the choice of the transition kernel. This\nresult is also valid in the case of partitions of $X$ into two disjoint\nsubspaces. It is proved by a reduction to the case of a Markov chain on the\nunion of two subspaces.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1839080459770115,
          "p": 0.23529411764705882,
          "f": 0.206451607978356
        },
        "rouge-2": {
          "r": 0.029411764705882353,
          "p": 0.02654867256637168,
          "f": 0.027906971757275092
        },
        "rouge-l": {
          "r": 0.16091954022988506,
          "p": 0.20588235294117646,
          "f": 0.1806451563654528
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.cond-mat/mtrl-sci/2503.10373v1",
      "true_abstract": "Topotactic reduction of perovskite oxides offers a powerful approach for\ndiscovering novel phenomena, such as superconducting infinite-layer nickelates\nand polar metallicity, and is commonly accompanied by the emergence of multiple\nvalence states and/or complex crystal fields of transition metals. However,\nunderstanding the complex interplay between crystal chemistry, electronic\nstructure, and physical properties at the spin- and orbital-resolved levels in\nthese reduced systems remains elusive. Here, we combine x-ray absorption\nspectroscopy, resonant inelastic x-ray scattering (RIXS), and density\nfunctional theory calculations to uncover topotactic metal-insulator transition\nand orbital-specific crystal field excitations in brownmillerite\nLa0.67Ca0.33MnO2.5 thin films. We reveal the Mn valence states to be\npredominantly Mn2+/Mn3+, along with their corresponding populations at\noctahedral and tetrahedral sites, which effectively weaken the Mn-O\nhybridization compared to the parent perovskite phase. As a result,\nLa0.67Ca0.33MnO2.5 films exhibit an antiferromagnetic insulating ground state.\nMoreover, by combining the RIXS measurements on selected single-valence\nmanganites, specifically MnO, LaMnO3, and CaMnO3, with orbital- and\nspin-resolved density-of-states calculations, we identify the dd excitations of\noctahedrally and tetrahedrally coordinated Mn2+/Mn3+ ions, directly linking the\nmicroscopic electronic structure to the macroscopic magnetic/electrical\nproperties.",
      "generated_abstract": "ence of magnetic skyrmions, which are the key building blocks of\nstructures with novel magnetic and spin transport properties, is a fundamental\nfeature of spintronics. Recent studies have demonstrated the emergence of\nmagnetic skyrmions in epitaxial thin films of Bi2Se3, which are a promising\nmaterial for spintronics applications. In this work, we perform an extensive\nstudy of the formation, growth, and stabilization of magnetic skyrmions in\nBi2Se3 films by combining first-principles calculations with ab initio\nmagnetic moments simulations. Our results show that, depending on the\npreparation conditions and the film thickness, magnetic skyrmions can form in\nthe film, exhibit anisotropic and non-monotonic behavior of their spin\npolarization, and exhibit a non-trivial spin texture. Furthermore, we demonstrate\nthat",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14788732394366197,
          "p": 0.25925925925925924,
          "f": 0.1883408025490158
        },
        "rouge-2": {
          "r": 0.02197802197802198,
          "p": 0.03636363636363636,
          "f": 0.027397255577970403
        },
        "rouge-l": {
          "r": 0.11267605633802817,
          "p": 0.19753086419753085,
          "f": 0.14349775322166156
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.physics/comp-ph/2503.09328v1",
      "true_abstract": "A Schr\\\"odinger bridge is the most probable time-dependent probability\ndistribution that connects an initial probability distribution $w_{i}$ to a\nfinal one $w_{f}$. The problem has been solved and widely used for the case of\nsimple Brownian evolution (non-interacting particles). It is related to the\nproblem of entropy regularized Wasserstein optimal transport. In this article,\nwe generalize Brownian bridges to systems of interacting particles. We derive\nsome equations for the forward and backward single particle ``wave-functions''\nwhich allow to compute the most probable evolution of the single-particle\nprobability between the initial and final distributions.",
      "generated_abstract": "y investigates the impact of anisotropy and correlation length\non the evolution of the critical point of a two-dimensional (2D) Ising model.\nBy introducing anisotropy, we study the phase transition of the model with\nincreasing anisotropy. The correlation length is found to be an important\nparameter in determining the critical behavior. We show that, under the\npresence of anisotropy, the correlation length is anisotropic, and, as a\nresult, the critical point shifts to the upper half of the phase diagram. We\nalso discuss the influence of anisotropy on the critical behavior of the model\nwithin a given correlation length. We observe that anisotropy has a non-trivial\neffect on the critical behavior of the model, leading to a non-monotonic\ndependence on the anisotropy. Finally, we study the critical behavior",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19402985074626866,
          "p": 0.19696969696969696,
          "f": 0.19548871680479407
        },
        "rouge-2": {
          "r": 0.045454545454545456,
          "p": 0.039603960396039604,
          "f": 0.042328037351698475
        },
        "rouge-l": {
          "r": 0.1791044776119403,
          "p": 0.18181818181818182,
          "f": 0.18045112281983164
        }
      }
    },
    {
      "paper_id": "cs.NI.cs/NI/2503.06883v1",
      "true_abstract": "Semantic communication has emerged as a transformative paradigm in\nnext-generation communication systems, leveraging advanced artificial\nintelligence (AI) models to extract and transmit semantic representations for\nefficient information exchange. Nevertheless, the presence of unpredictable\nsemantic noise, such as ambiguity and distortions in transmitted\nrepresentations, often undermines the reliability of received information.\nConventional approaches primarily adopt adversarial training with noise\ninjection to mitigate the adverse effects of noise. However, such methods\nexhibit limited adaptability to varying noise levels and impose additional\ncomputational overhead during model training. To address these challenges, this\npaper proposes Noise-Resilient \\textbf{Se}mantic Communication with\n\\textbf{Hi}gh-and-\\textbf{Lo}w Frequency Decomposition (Se-HiLo) for image\ntransmission. The proposed Se-HiLo incorporates a Finite Scalar Quantization\n(FSQ) based noise-resilient module, which bypasses adversarial training by\nenforcing encoded representations within predefined spaces to enhance noise\nresilience. While FSQ improves robustness, it compromise representational\ndiversity. To alleviate this trade-off, we adopt a transformer-based\nhigh-and-low frequency decomposition module that decouples image\nrepresentations into high-and-low frequency components, mapping them into\nseparate FSQ representation spaces to preserve representational diversity.\nExtensive experiments demonstrate that Se-HiLo achieves superior noise\nresilience and ensures accurate semantic communication across diverse noise\nenvironments.",
      "generated_abstract": "In this work, we present a novel approach to automatically generating\nconventional and advanced test cases for cyber-physical systems (CPS). Our\napproach, called COMPASS, leverages the knowledge of the test harness and\nautomatically generates test cases based on its specifications. We demonstrate\nthe effectiveness of COMPASS through a case study on a CPS that includes a\nplanning module to control the robotic arm. We show that COMPASS significantly\nimproves the reliability of the test suite and enables more reliable and\nefficient testing. We also evaluate the effectiveness of COMPASS in\nsimulation-based testing and demonstrate the value of COMPASS in\nautomated-development-environments (ADEs) that rely on simulation-based testing.\nFinally, we discuss the limitations of COMPASS and future directions for\nenhancing COMPASS.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1223021582733813,
          "p": 0.2328767123287671,
          "f": 0.16037735397516922
        },
        "rouge-2": {
          "r": 0.011111111111111112,
          "p": 0.018691588785046728,
          "f": 0.013937277553450075
        },
        "rouge-l": {
          "r": 0.1079136690647482,
          "p": 0.2054794520547945,
          "f": 0.14150942944686737
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/MF/2411.11522v1",
      "true_abstract": "This paper presents comparison results and establishes risk bounds for credit\nportfolios within classes of Bernoulli mixture models, assuming conditionally\nindependent defaults that are stochastically increasing with a common risk\nfactor. We provide simple and interpretable conditions for conditional default\nprobabilities that imply a comparison of credit portfolio losses in convex\norder. In the case of threshold models, the ranking of portfolio losses is\nbased on a pointwise comparison of the underlying copulas. Our setting includes\nas special case the well-known Gaussian copula model but allows for general\ntail dependencies, which are crucial for modeling credit portfolio risks.\nMoreover, our results extend the classical parameterized models, such as the\nindustry models CreditMetrics and KMV Portfolio Manager, to a robust setting\nwhere individual parameters or the copula modeling the dependence structure can\nbe ambiguous. A simulation study and a real data example under model\nuncertainty offer evidence supporting the effectiveness of our approach.",
      "generated_abstract": "r introduces a novel method for identifying portfolio managers\nwith minimal information. It provides a principled approach to constructing\nindependent and identically distributed (i.i.d.) portfolio managers, even\nwithout knowledge of the underlying financial markets. The method is based on\nthe assumption that the return process of a single stock follows a\ndistribution-free, multivariate stable process. This assumption allows for a\nsingle-parameter approximation of the return process of each portfolio\nmanager. The approximation is obtained by approximating the return process of\neach stock with the return process of a single-parameter stable process. The\nproposed method is applied to the case of portfolio managers that invest in\ndifferent asset classes, including stocks, bonds, and commodities, and\ncompared with a method based on the stochastic volatility of the underlying\nstock market. The proposed method yields portfolio managers",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1743119266055046,
          "p": 0.25333333333333335,
          "f": 0.20652173430115797
        },
        "rouge-2": {
          "r": 0.060810810810810814,
          "p": 0.07964601769911504,
          "f": 0.0689655123312932
        },
        "rouge-l": {
          "r": 0.1559633027522936,
          "p": 0.22666666666666666,
          "f": 0.18478260386637535
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.19947v1",
      "true_abstract": "Motivated by studying the effects of marriage prospects on students' college\nmajor choices, this paper develops a new econometric test for analyzing the\neffects of an unobservable factor in a setting where this factor potentially\ninfluences both agents' decisions and a binary outcome variable. Our test is\nbuilt upon a flexible copula-based estimation procedure and leverages the\nordered nature of latent utilities of the polychotomous choice model. Using the\nproposed method, we demonstrate that marriage prospects significantly influence\nthe college major choices of college graduates participating in the National\nLongitudinal Study of Youth (97) Survey. Furthermore, we validate the\nrobustness of our findings with alternative tests that use stated marriage\nexpectation measures from our data, thereby demonstrating the applicability and\nvalidity of our testing procedure in real-life scenarios.",
      "generated_abstract": "We consider a repeated cross-sectional experiment in which subjects are\ninvested in assets, and we want to predict the next period's assets given the\nprevious period's assets and investment history. We propose a novel model for\nthis setting, which we call the Investment-Augmented Difference-in-Differences\n(ID-DID) estimator, which improves on the Difference-in-Differences estimator\nby including an augmented set of variables that captures the interaction of\ninvestment and asset prices with the investment history. We show that the\nID-DID estimator is consistent, asymptotically normal, and has finite\nvariance. We then apply the ID-DID estimator to a simulated data set and to a\nreal-world data set, finding that it outperforms the Difference-in-Differences\nestimator.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16483516483516483,
          "p": 0.21428571428571427,
          "f": 0.18633539881177436
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10989010989010989,
          "p": 0.14285714285714285,
          "f": 0.12422359756953841
        }
      }
    },
    {
      "paper_id": "math.SG.math/SG/2503.10283v1",
      "true_abstract": "Given a closed connected symplectic manifold $(M,\\omega)$, we construct an\nalternating $\\mathbb{R}$-bilinear form\n$\\mathfrak{b}=\\mathfrak{b}_{\\mu_{\\mathrm{Sh}}}$ on the real first cohomology of\n$M$ from Shelukhin's quasimorphism $\\mu_{\\mathrm{Sh}}$. Here\n$\\mu_{\\mathrm{Sh}}$ is defined on the universal cover of the group of\nHamiltonian diffeomorphisms on $(M,\\omega)$. This bilinear form is invariant\nunder the symplectic mapping class group action, and $\\mathfrak{b}$ yields a\nconstraint on the fluxes of commuting two elements in the group of\nsymplectomorphisms on $(M,\\omega)$. These results might be seen as an analog of\nRousseau's result for an open connected symplectic manifold, where he recovered\nthe symplectic pairing from the Calabi homomorphism. Furthermore,\n$\\mathfrak{b}$ controls the extendability of Shelukhin's quasimorphisms, as\nwell as the triviality of a characteristic class of Reznikov. To construct\n$\\mathfrak{b}$, we build general machinery for a group $G$ of producing a\nreal-valued $\\mathbb{Z}$-bilinear form $\\mathfrak{b}_{\\mu}$ from a\n$G$-invariant quasimorphism $\\mu$ on the commutator subgroup of $G$.",
      "generated_abstract": "In this paper we extend the results of [Gue18",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.04395604395604396,
          "p": 0.4444444444444444,
          "f": 0.07999999836200003
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.04395604395604396,
          "p": 0.4444444444444444,
          "f": 0.07999999836200003
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.09767v1",
      "true_abstract": "Classical unsupervised learning methods like clustering and linear\ndimensionality reduction parametrize large-scale geometry when it is discrete\nor linear, while more modern methods from manifold learning find low\ndimensional representation or infer local geometry by constructing a graph on\nthe input data. More recently, topological data analysis popularized the use of\nsimplicial complexes to represent data topology with two main methodologies:\ntopological inference with geometric complexes and large-scale topology\nvisualization with Mapper graphs -- central to these is the nerve construction\nfrom topology, which builds a simplicial complex given a cover of a space by\nsubsets. While successful, these have limitations: geometric complexes scale\npoorly with data size, and Mapper graphs can be hard to tune and only contain\nlow dimensional information. In this paper, we propose to study the problem of\nlearning covers in its own right, and from the perspective of optimization. We\ndescribe a method for learning topologically-faithful covers of geometric\ndatasets, and show that the simplicial complexes thus obtained can outperform\nstandard topological inference approaches in terms of size, and Mapper-type\nalgorithms in terms of representation of large-scale topology.",
      "generated_abstract": "aper, we propose a novel framework for learning probabilistic\npolynomial-time approximation schemes for linear regression with Gaussian\nnoise. Our approach is inspired by recent advancements in probabilistic\napproximation of regression problems with random design and Gaussian noise. We\ndemonstrate that the probabilistic polynomial-time approximation scheme we\npropose achieves the optimal convergence rate of $\\mathcal{O}(\\sqrt{T/K})$ with\na constant $K$ independent of $T$, where $T$ is the total number of\niterations. To the best of our knowledge, our work is the first to achieve\nprobabilistic polynomial-time approximation scheme with such a rate in the\ncontext of linear regression with Gaussian noise. Furthermore, we propose a\nwell-balanced randomized gradient algorithm for the proposed scheme, which\nachieves an average iteration complexity of $O(K/\\sqrt{T})$. Our framework\ngeneralizes the well-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14655172413793102,
          "p": 0.22972972972972974,
          "f": 0.1789473636653741
        },
        "rouge-2": {
          "r": 0.023121387283236993,
          "p": 0.03669724770642202,
          "f": 0.028368789583774243
        },
        "rouge-l": {
          "r": 0.14655172413793102,
          "p": 0.22972972972972974,
          "f": 0.1789473636653741
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.04488v1",
      "true_abstract": "In a recent article [13], G. Janelidze introduced the concept of ideally\nexact categories as a generalization of semi-abelian categories, aiming to\nincorporate relevant examples of non-pointed categories, such as the categories\n$\\textbf{Ring}$ and $\\textbf{CRing}$ of unitary (commutative) rings. He also\nextended the notion of action representability to this broader framework,\nproving that both $\\textbf{Ring}$ and $\\textbf{CRing}$ are action\nrepresentable.\n  This article investigates the representability of actions of unitary\nnon-associative algebras. After providing a detailed description of the monadic\nadjunction associated with any category of unitary algebra, we use the\nconstruction of the external weak actor [4] in order to prove that the\ncategories of unitary (commutative) associative algebras and that of unitary\nalternative algebras are action representable. The result is then extended for\nunitary (commutative) Poisson algebras, where the explicit construction of the\nuniversal strict general actor is employed.",
      "generated_abstract": "We study a class of non-commutative linear operators acting on a separable\n(or, in general, non-commutative) Hilbert space. The operators are defined by\ntheir actions on a set of random variables. We show that such operators can be\ndescribed in terms of a system of nonlinear ordinary differential equations,\nwhich is a non-trivial generalization of the case where the operators act on\na Hilbert space. We show that the linear operators that arise in this way are\nin a one-to-one correspondence with those in the class of non-commutative\nlinear operators on a separable Hilbert space which can be obtained by\nnormalizing an operator from this class.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16279069767441862,
          "p": 0.2413793103448276,
          "f": 0.19444443963348781
        },
        "rouge-2": {
          "r": 0.032,
          "p": 0.04395604395604396,
          "f": 0.03703703216092314
        },
        "rouge-l": {
          "r": 0.11627906976744186,
          "p": 0.1724137931034483,
          "f": 0.13888888407793226
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.cond-mat/str-el/2503.09717v1",
      "true_abstract": "We clarify the lore that anomaly-free symmetries are either on-site or can be\ntransformed into on-site symmetries. We prove that any finite, internal,\nanomaly-free symmetry in a 1+1d lattice Hamiltonian system can be disentangled\ninto an on-site symmetry by introducing ancillas and applying conjugation via a\nfinite-depth quantum circuit. We provide an explicit construction of the\ndisentangling circuit using Gauss's law operators and emphasize the necessity\nof adding ancillas. Our result establishes the converse to a generalized\nLieb-Schultz-Mattis theorem by demonstrating that any anomaly-free symmetry\nadmits a trivially gapped Hamiltonian.",
      "generated_abstract": "t a detailed theoretical study of the spin-orbit coupling in\nstrontium calcium ferrite (SrCaFe2O4), using density functional theory\ncalculations and a phenomenological model to explain the observed spin\npolarization of the magnetic moments in this material. We find that the\nsignature of the spin-orbit interaction in SrCaFe2O4 is a strong reduction of\nthe spin-orbit splitting, with a maximum of 1.02 meV at the Fermi level, and a\nsignificant reduction in the spin-orbit splitting compared to iron-based\nsuperconductors. This reduction in the spin-orbit splitting results in a\nsignificant reduction in the spin-polarization of the magnetic moments, with\nthe spin-polarization of the magnetic moments in SrCaFe2O4 being 2.0 times\nsmaller than in iron-based super",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14516129032258066,
          "p": 0.13846153846153847,
          "f": 0.14173227846735711
        },
        "rouge-2": {
          "r": 0.023255813953488372,
          "p": 0.02247191011235955,
          "f": 0.022857137858613336
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.12307692307692308,
          "f": 0.12598424697129415
        }
      }
    },
    {
      "paper_id": "cs.CY.econ/GN/2501.19407v2",
      "true_abstract": "Surnames often convey implicit markers of social status, wealth, and lineage,\nshaping perceptions in ways that can perpetuate systemic biases and\nintergenerational inequality. This study is the first of its kind to\ninvestigate whether and how surnames influence AI-driven decision-making,\nfocusing on their effects across key areas such as hiring recommendations,\nleadership appointments, and loan approvals. Using 72,000 evaluations of 600\nsurnames from the United States and Thailand, two countries with distinct\nsociohistorical contexts and surname conventions, we classify names into four\ncategories: Rich, Legacy, Normal, and phonetically similar Variant groups. Our\nfindings show that elite surnames consistently increase AI-generated\nperceptions of power, intelligence, and wealth, which in turn influence\nAI-driven decisions in high-stakes contexts. Mediation analysis reveals\nperceived intelligence as a key mechanism through which surname biases\ninfluence AI decision-making process. While providing objective qualifications\nalongside surnames mitigates most of these biases, it does not eliminate them\nentirely, especially in contexts where candidate credentials are low. These\nfindings highlight the need for fairness-aware algorithms and robust policy\nmeasures to prevent AI systems from reinforcing systemic inequalities tied to\nsurnames, an often-overlooked bias compared to more salient characteristics\nsuch as race and gender. Our work calls for a critical reassessment of\nalgorithmic accountability and its broader societal impact, particularly in\nsystems designed to uphold meritocratic principles while counteracting the\nperpetuation of intergenerational privilege.",
      "generated_abstract": "We develop a model of the interaction between an agent and a large\nsystem of agents in which the agents' behaviors are determined by a\nrepresentation of the state of the system. We show that the interaction\nstrategy of the agent depends on the state of the system, and the system can\nadapt to the agent's strategy. We also show that the system adapts to the agent's\nstrategy, but it does not adapt to the agent's strategy. Our model\nillustrates the concept of a self-organizing system, which is a model of a\ncomplex system that adapts to the actions of its constituent elements.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1130952380952381,
          "p": 0.38,
          "f": 0.1743119230704487
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.05,
          "f": 0.026666662755556132
        },
        "rouge-l": {
          "r": 0.1130952380952381,
          "p": 0.38,
          "f": 0.1743119230704487
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ph/2503.10299v1",
      "true_abstract": "In this work, we systematically study the interactions of the $S$-wave\n$D^{(*)}\\bar{B}^{(*)}$ systems within the framework of chiral effective field\ntheory in heavy hadron formalism. We calculate the $D^{(*)}\\bar{B}^{(*)}$\neffective potentials up to next-to-leading order, explore the bound state\nformations, and investigate the $D^{(*)}\\bar{B}^{(*)}$ scattering properties\nsuch as scattering rate, scattering length, and effective range. Our results\nshow that all $I=1$ $D^{(*)}\\bar{B}^{(*)}$ potentials are repulsive, preventing\nthe formation of bound states, while the $I=0$ potentials are generally\nattractive. Specifically, we get two important observations: first, the shallow\nbound state is more likely to exist in the $D\\bar{B}[I(J^{P})=0(0^{+})]$ system\nthan in the $D\\bar{B}^{*}[I(J^{P})=0(1^{+})]$ system; second,\n$D^{*}\\bar{B}^{*}[I(J^{P})=0(0^{+})]$ and $D^{*}\\bar{B}^{*}[I(J^{P})=0(1^{+})]$\nsystems possess relatively large binding energies and positive scattering\nlengths, which suggests strong bound state formations in these channels. So the\nattractions in the $D^{*}\\bar{B}^{*}[I=0]$ systems are deeper than those in the\n$D\\bar{B}^{(*)}[I=0]$ systems, thus we strongly recommend the future experiment\nto search for the $D^{*}\\bar{B}^{*}[I=0]$ tetraquark systems. In addition, we\nalso investigate the dependencies of the $D\\bar{B}^{(*)}$ binding energies on\nthe contact low-energy coupling constants (LECs).",
      "generated_abstract": "We study the effects of quantum gravity on the evolution of the\nfluctuations of the Higgs field in the Standard Model (SM) by introducing\nnon-renormalizable operators in the Lagrangian. We consider the case of a\nnearly flat background and compare the predictions with those obtained in the\nconformal limit. The results are consistent with the experimental data on the\nHiggs boson. Our analysis reveals that the quantum effects are of order one\npercent in the present scenario, which is smaller than the current experimental\nconstraints. In the case of a curved background, we obtain a non-negligible\ncontribution to the fluctuations of the Higgs field. This effect is of\nsignificance if the curvature scale is above the electroweak scale.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.2602739726027397,
          "f": 0.20320855139008848
        },
        "rouge-2": {
          "r": 0.024691358024691357,
          "p": 0.038834951456310676,
          "f": 0.030188674493129973
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.2465753424657534,
          "f": 0.1925133642243131
        }
      }
    },
    {
      "paper_id": "quant-ph.stat/TH/2502.14950v1",
      "true_abstract": "Inferring causal models from observed correlations is a challenging task,\ncrucial to many areas of science. In order to alleviate the effort, it is\nimportant to know whether symmetries in the observations correspond to\nsymmetries in the underlying realization. Via an explicit example, we answer\nthis question in the negative. We use a tripartite probability distribution\nover binary events that is realized by using three (different) independent\nsources of classical randomness. We prove that even removing the condition that\nthe sources distribute systems described by classical physics, the requirements\nthat i) the sources distribute the same physical systems, ii) these physical\nsystems respect relativistic causality, and iii) the correlations are the\nobserved ones, are incompatible.",
      "generated_abstract": "We consider the problem of learning the joint distribution of a random sample\nfrom a Gaussian process (GP) using a single observation of the GP. We show that\nthe learning problem can be reduced to a regression problem with an additive\nnoise model, and propose a novel algorithm that can be implemented in\nexperimental settings. Our method provides an effective solution to the\nlearning problem, even when the GP is highly non-Gaussian or the observations\nare noisy. In this paper, we focus on the case where the GP is Gaussian, but\nour results can be easily extended to other cases. Our approach is based on the\nLagrangian method and the Newton-Raphson algorithm, and we propose a\nnon-convex regularization term that stabilizes the algorithm. Numerical\nexperiments demonstrate that our method outperforms existing methods in terms\nof the number of iterations and the quality of the solution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2345679012345679,
          "p": 0.21348314606741572,
          "f": 0.22352940677577862
        },
        "rouge-2": {
          "r": 0.01834862385321101,
          "p": 0.015037593984962405,
          "f": 0.01652892066901316
        },
        "rouge-l": {
          "r": 0.2222222222222222,
          "p": 0.20224719101123595,
          "f": 0.21176470089342572
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2501.00002v2",
      "true_abstract": "In this paper we present a QUBO formulation for the Takuzu game (or Binairo),\nfor the most recent LinkedIn game, Tango, and for its generalizations. We\noptimize the number of variables needed to solve the combinatorial problem,\nmaking it suitable to be solved by quantum devices with fewer resources.",
      "generated_abstract": "pt of a \"living robot\" is one of the most prominent ideas in the\nrobots' worldview. Living robots are able to sense, perceive, think, and\nact. However, the concept is not entirely novel. In the 1970s, the idea of\nembodied intelligence was presented in the works of philosophers, including\nGottlob Frege, Gottlob Frege, Gottlob Frege, and J\u00fcrgen Mittelstra\u00df. These\nworks described a kind of intelligence that is not bound by the laws of physics.\nThis idea was later expanded by Alan Turing, who proposed a theory of artificial\nintelligence that incorporated both embodied intelligence and physical\nintelligence. The latter has since been termed \"embodied cognition.\"\nEmbodied cognition has been used to describe the ways in which artificial\nintelligence is able to actively interact with the world. However,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20930232558139536,
          "p": 0.1111111111111111,
          "f": 0.14516128579214374
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.00847457627118644,
          "f": 0.01212120804701698
        },
        "rouge-l": {
          "r": 0.20930232558139536,
          "p": 0.1111111111111111,
          "f": 0.14516128579214374
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.12935v1",
      "true_abstract": "In this review, we examine computational models that explore the role of\nneural oscillations in speech perception, spanning from early auditory\nprocessing to higher cognitive stages. We focus on models that use rhythmic\nbrain activities, such as gamma, theta, and delta oscillations, to encode\nphonemes, segment speech into syllables and words, and integrate linguistic\nelements to infer meaning. We analyze the mechanisms underlying these models,\ntheir biological plausibility, and their potential applications in processing\nand understanding speech in real time, a computational feature that is achieved\nby the human brain but not yet implemented in speech recognition models.\nReal-time processing enables dynamic adaptation to incoming speech, allowing\nsystems to handle the rapid and continuous flow of auditory information\nrequired for effective communication, interactive applications, and accurate\nspeech recognition in a variety of real-world settings. While significant\nprogress has been made in modeling the neural basis of speech perception,\nchallenges remain, particularly in accounting for the complexity of semantic\nprocessing and the integration of contextual influences. Moreover, the high\ncomputational demands of biologically realistic models pose practical\ndifficulties for their implementation and analysis. Despite these limitations,\nthese models provide valuable insights into the neural mechanisms of speech\nperception. We conclude by identifying current limitations, proposing future\nresearch directions, and suggesting how these models can be further developed\nto achieve a more comprehensive understanding of speech processing in the human\nbrain.",
      "generated_abstract": "The study of social behaviors in populations is essential for understanding\ntheir evolutionary and functional aspects. A central challenge is to identify\nmechanisms that drive the emergence of cooperative behaviors. Theoretical\nmodels have traditionally focused on evolutionary processes, but there is a\ngap between the empirical understanding of social interactions and the\nunderlying mechanisms of cooperation. Here, we introduce a novel framework\nbased on the concept of cooperation-specific selection, which allows us to\ntheoretically explain how cooperative behaviors emerge in populations by\nexploiting the cooperative potential of individuals. This framework allows us to\nexplain how cooperative behaviors evolve without any explicit cooperation\nselection. This approach provides a theoretical framework for understanding\nhow cooperative behaviors arise in nature and can be applied to a wide range of\nsocial interactions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14965986394557823,
          "p": 0.27848101265822783,
          "f": 0.19469026093938457
        },
        "rouge-2": {
          "r": 0.018433179723502304,
          "p": 0.03508771929824561,
          "f": 0.024169179774190565
        },
        "rouge-l": {
          "r": 0.1292517006802721,
          "p": 0.24050632911392406,
          "f": 0.1681415883730129
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.06431v1",
      "true_abstract": "The kidney paired donation (KPD) program provides an innovative solution to\novercome incompatibility challenges in kidney transplants by matching\nincompatible donor-patient pairs and facilitating kidney exchanges. To address\nunequal access to transplant opportunities, there are two widely used fairness\ncriteria: group fairness and individual fairness. However, these criteria do\nnot consider protected patient features, which refer to characteristics legally\nor ethically recognized as needing protection from discrimination, such as race\nand gender. Motivated by the calibration principle in machine learning, we\nintroduce a new fairness criterion: the matching outcome should be\nconditionally independent of the protected feature, given the sensitization\nlevel. We integrate this fairness criterion as a constraint within the KPD\noptimization framework and propose a computationally efficient solution.\nTheoretically, we analyze the associated price of fairness using random graph\nmodels. Empirically, we compare our fairness criterion with group fairness and\nindividual fairness through both simulations and a real-data example.",
      "generated_abstract": "The concept of a \"smooth\" or \"smoothness\" measure is commonly used in\ndata analysis, and it is commonly assumed that the smoothness of a function\nis constant across its domain. However, this assumption is not necessarily\nvalid. We show that if a function is Lipschitz continuous and satisfies the\nLipschitz condition on its domain, then it is smooth on its domain. Furthermore,\nwe show that if a function is Lipschitz continuous and satisfies the\nLipschitz condition on its domain, then it is smooth on its domain and\n$C^\\infty$ on its domain. This implies that the smoothness measure and the\nLipschitz measure are not equivalent, and we propose a new notion of smoothness\nthat is more general and also useful in practice.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1415929203539823,
          "p": 0.2807017543859649,
          "f": 0.18823528966020772
        },
        "rouge-2": {
          "r": 0.013793103448275862,
          "p": 0.023255813953488372,
          "f": 0.017316012642193124
        },
        "rouge-l": {
          "r": 0.1415929203539823,
          "p": 0.2807017543859649,
          "f": 0.18823528966020772
        }
      }
    },
    {
      "paper_id": "physics.soc-ph.econ/TH/2501.09778v2",
      "true_abstract": "We propose a disaggregated representation of production through an\nagent-based fund-flow model (NGR-ADAPT) within which inefficiencies, such as\nfactor idleness and production instability, emerge from endogenous frictions.\nThe model incorporates productivity dynamics (learning and depreciation) and is\nextended with time-saving process innovations. Specifically, we assume that\nworkers possess inherent creativity that flourishes during idle periods. The\nfirm, rather than laying off idle workers, is assumed to exploit this potential\nby involving them in the innovation process. Results show that a firm's\norganizational and managerial decisions, the temporal structure of the\nproduction system, the speed at which workers learn and forget, and the pace of\ninnovation are critical factors influencing production efficiency in both the\nshort and long run. The co-evolution of production and innovation processes\nemerges in our model through the two-sided effects of idleness: whereas it\ndrives skill decay it is also a condition for creative thinking that can be\nleveraged for innovation. In doing so, we question the utilization of labour as\nan adjustment variable in a productive organisation. The paper concludes by\ndiscussing potential solutions to this issue and suggesting avenues for future\nresearch.",
      "generated_abstract": "We investigate the role of information asymmetry in economic outcomes,\nusing a simple model of information diffusion and information sharing. We\npropose that asymmetric information may lead to heterogeneous welfare\ndistributions, and that this heterogeneity can be explained through the\ninterplay between the cost of information and the amount of information\navailable. Using a stylized dataset of economic transactions in the 19th\ncentury, we find that the cost of information is generally low, and that the\namount of information available varies widely across economic transactions.\nThese results suggest that asymmetric information can have substantial\neffects on welfare, and that the effects are heterogeneous across economic\ntransactions. These findings have important implications for the design of\npublic policies aimed at increasing welfare.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15503875968992248,
          "p": 0.2857142857142857,
          "f": 0.20100502056513733
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.05,
          "f": 0.03508771474299844
        },
        "rouge-l": {
          "r": 0.15503875968992248,
          "p": 0.2857142857142857,
          "f": 0.20100502056513733
        }
      }
    },
    {
      "paper_id": "cs.NE.cs/NE/2503.10387v1",
      "true_abstract": "Progress in neuromorphic computing requires efficient implementation of\nstandard computational problems, like adding numbers. Here we implement one\nsequential and two parallel binary adders in the Lava software framework, and\ndeploy them to the neuromorphic chip Loihi 2. We describe the time complexity,\nneuron and synaptic resources, as well as constraints on the bit width of the\nnumbers that can be added with the current implementations. Further, we measure\nthe time required for the addition operation on-chip. Importantly, we encounter\ntrade-offs in terms of time complexity and required chip resources for the\nthree considered adders. While sequential adders have linear time complexity\n$\\bf\\mathcal{O}(n)$ and require a linearly increasing number of neurons and\nsynapses with number of bits $n$, the parallel adders have constant time\ncomplexity $\\bf\\mathcal{O}(1)$ and also require a linearly increasing number of\nneurons, but nonlinearly increasing synaptic resources (scaling with $\\bf n^2$\nor $\\bf n \\sqrt{n}$). This trade-off between compute time and chip resources\nmay inform decisions in application development, and the implementations we\nprovide may serve as a building block for further progress towards efficient\nneuromorphic algorithms.",
      "generated_abstract": "In the recent years, deep learning-based methods have demonstrated their\npower to perform advanced machine learning tasks in areas such as computer\nvision and speech recognition. However, the scalability and generalization\ncapabilities of these models have been limited by their dependency on large\ntraining datasets, which are often expensive to obtain. In this work, we\nintroduce a new dataset that addresses this limitation by leveraging existing\nand publicly available datasets. We introduce a novel data augmentation\ntechnique that enhances the dataset's generalization capabilities,\nincreasing its utility for training neural networks. Our results show that\nour dataset enhances the performance of state-of-the-art models across a range\nof tasks, including image classification, object detection, and speech\nrecognition, highlighting its potential to significantly improve the\nperformance of deep learning models in various domains.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12612612612612611,
          "p": 0.14583333333333334,
          "f": 0.13526569550934697
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11711711711711711,
          "p": 0.13541666666666666,
          "f": 0.12560385976055471
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.10496v1",
      "true_abstract": "Modeling natural phenomena with artificial neural networks (ANNs) often\nprovides highly accurate predictions. However, ANNs often suffer from\nover-parameterization, complicating interpretation and raising uncertainty\nissues. Bayesian neural networks (BNNs) address the latter by representing\nweights as probability distributions, allowing for predictive uncertainty\nevaluation. Latent binary Bayesian neural networks (LBBNNs) further handle\nstructural uncertainty and sparsify models by removing redundant weights. This\narticle advances LBBNNs by enabling covariates to skip to any succeeding layer\nor be excluded, simplifying networks and clarifying input impacts on\npredictions. Ultimately, a linear model or even a constant can be found to be\noptimal for a specific problem at hand. Furthermore, the input-skip LBBNN\napproach reduces network density significantly compared to standard LBBNNs,\nachieving over 99% reduction for small networks and over 99.9% for larger ones,\nwhile still maintaining high predictive accuracy and uncertainty measurement.\nFor example, on MNIST, we reached 97% accuracy and great calibration with just\n935 weights, reaching state-of-the-art for compression of neural networks.\nFurthermore, the proposed method accurately identifies the true covariates and\nadjusts for system non-linearity. The main contribution is the introduction of\nactive paths, enhancing directly designed global and local explanations within\nthe LBBNN framework, that have theoretical guarantees and do not require post\nhoc external tools for explanations.",
      "generated_abstract": "vancements in deep learning have enabled the development of\nnovel generative models, such as the LLMs. The LLMs, however, face limitations\nin handling large-scale datasets. These limitations are primarily attributed\nto their large language modeling (LLM) capacity and computational power. The\ncapacity issue is particularly pronounced in LLMs with larger size. To\naddress this challenge, we propose a novel framework for generating large-scale\ndata in a more efficient manner. The framework is based on the use of\ninvertible latent representations and an efficient sampling process. The\nframework enables the generation of data with higher fidelity and reduces the\ntime required to generate large datasets. We demonstrate the efficacy of our\nframework through various experiments on several datasets. The results\ndemonstrate that our framework not only improves the efficiency of the data\ngeneration process but also yields data with higher fidelity and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11538461538461539,
          "p": 0.1956521739130435,
          "f": 0.14516128565556727
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09615384615384616,
          "p": 0.16304347826086957,
          "f": 0.12096773726847053
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2502.21276v1",
      "true_abstract": "Boosting has emerged as a useful machine learning technique over the past\nthree decades, attracting increased attention. Most advancements in this area,\nhowever, have primarily focused on numerical implementation procedures, often\nlacking rigorous theoretical justifications. Moreover, these approaches are\ngenerally designed for datasets with fully observed data, and their validity\ncan be compromised by the presence of missing observations. In this paper, we\nemploy semiparametric estimation approaches to develop boosting prediction\nmethods for data with missing responses. We explore two strategies for\nadjusting the loss functions to account for missingness effects. The proposed\nmethods are implemented using a functional gradient descent algorithm, and\ntheir theoretical properties, including algorithm convergence and estimator\nconsistency, are rigorously established. Numerical studies demonstrate that the\nproposed methods perform well in finite sample settings.",
      "generated_abstract": "r introduces a novel approach to Bayesian estimation of the\nregression coefficients of a structural equation model, employing a latent\nvariable model for the error terms. The latent variable model accounts for\nmissing data and provides a unified framework for handling both missingness\nand heteroskedasticity. We propose a novel method for inference on the\nregression coefficients of the model, leveraging the variational inference\nframework to derive efficient and accurate estimates. The proposed method\neliminates the need for a prior distribution on the regression coefficients,\nenabling a more flexible modeling approach. Simulation studies and real data\nanalyses demonstrate the efficacy of the proposed method, with results\ncompared favorably to traditional approaches. This novel approach offers\nadvantages over existing methods, including improved interpretability and\nflexibility, while also providing a more computationally efficient solution.\nThe proposed method is applicable to a wide range of structural equation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18867924528301888,
          "p": 0.22988505747126436,
          "f": 0.2072538810588205
        },
        "rouge-2": {
          "r": 0.016,
          "p": 0.015748031496062992,
          "f": 0.01587301087333239
        },
        "rouge-l": {
          "r": 0.16981132075471697,
          "p": 0.20689655172413793,
          "f": 0.18652849245778422
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.07924v1",
      "true_abstract": "We study a fundamental challenge in the economics of innovation: an inventor\nmust reveal details of a new idea to secure compensation or funding, yet such\ndisclosure risks expropriation. We present a model in which a seller (inventor)\nand buyer (investor) bargain over an information good under the threat of\nhold-up. In the classical setting, the seller withholds disclosure to avoid\nmisappropriation, leading to inefficiency. We show that trusted execution\nenvironments (TEEs) combined with AI agents can mitigate and even fully\neliminate this hold-up problem. By delegating the disclosure and payment\ndecisions to tamper-proof programs, the seller can safely reveal the invention\nwithout risking expropriation, achieving full disclosure and an efficient ex\npost transfer. Moreover, even if the invention's value exceeds a threshold that\nTEEs can fully secure, partial disclosure still improves outcomes compared to\nno disclosure. Recognizing that real AI agents are imperfect, we model \"agent\nerrors\" in payments or disclosures and demonstrate that budget caps and\nacceptance thresholds suffice to preserve most of the efficiency gains.\n  Our results imply that cryptographic or hardware-based solutions can function\nas an \"ironclad NDA,\" substantially mitigating the fundamental\ndisclosure-appropriation paradox first identified by Arrow (1962) and Nelson\n(1959). This has far-reaching policy implications for fostering R&D, technology\ntransfer, and collaboration.",
      "generated_abstract": "uce a game-theoretic model for the allocation of scarce resources in\na network. A network is represented by a weighted graph, where each node\nrepresents a resource and the weight of the edge between two nodes is the\ncost of using the resource. The cost of using a resource can be determined by\na cost function, and we consider two types of cost functions: the first is\ndetermined by the price of a good, while the second is determined by the\ndemand for the good. We show that the equilibrium allocation of resources is\ndetermined by the equilibrium price of the good, which is determined by the\nequilibrium price of the good. We also show that the equilibrium allocation\nof resources can be represented by a convex combination of the equilibrium\nprice of the good and the equilibrium demand for the good. We further show that\nthe equilibrium allocation of resources can be represented by the convex\ncombination of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10738255033557047,
          "p": 0.2857142857142857,
          "f": 0.1560975570046402
        },
        "rouge-2": {
          "r": 0.0196078431372549,
          "p": 0.04040404040404041,
          "f": 0.026402635864458467
        },
        "rouge-l": {
          "r": 0.087248322147651,
          "p": 0.23214285714285715,
          "f": 0.1268292643217134
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.06389v1",
      "true_abstract": "Network estimation has been a critical component of single-cell\ntranscriptomic data analysis, which can provide crucial insights into the\ncomplex interplay among genes, facilitating uncovering the biological basis of\nhuman life at single-cell resolution. Despite notable achievements, existing\nmethodologies often falter in their practicality, primarily due to their narrow\nfocus on simplistic linear relationships and inadequate handling of cellular\nheterogeneity. To bridge these gaps, we propose a joint regularized deep neural\nnetwork method incorporating a Mahalanobis distance-based K-means clustering\n(JRDNN-KM) to estimate multiple networks for various cell subgroups\nsimultaneously, accounting for both unknown cellular heterogeneity and\nzero-inflation and, more importantly, complex nonlinear relationships among\ngenes. We innovatively introduce a selection layer for network construction and\ndevelop homogeneous and heterogeneous hidden layers to accommodate commonality\nand specificity across multiple networks. Through simulations and applications\nto real single-cell transcriptomic data for multiple tissues and species, we\nshow that JRDNN-KM constructs networks with more accuracy and biological\ninterpretability and, meanwhile, identifies more accurate cell subgroups\ncompared to the state-of-the-art methods in the literature. Building on the\nnetwork construction, we further find hub genes with important biological\nimplications and modules with statistical enrichment of biological processes.",
      "generated_abstract": "advancement of Artificial Intelligence (AI) has created new\nrisks for national security. Traditional countermeasures such as cryptography\nand hardware security modules have limited effectiveness against AI-enabled\nattacks. To address this, we introduce the first end-to-end AI-enabled\ninformation security framework, called AI-Sec, that integrates both\nintelligence-driven and deep learning-based defense mechanisms. AI-Sec\nidentifies vulnerabilities in AI systems and employs advanced machine learning\nand deep learning techniques to detect and mitigate them. By integrating\ninformation security technologies with machine learning and deep learning,\nAI-Sec significantly enhances the effectiveness of existing security\nmechanisms while providing a scalable, adaptive, and end-to-end solution for\nprotecting critical AI-based systems. The proposed framework improves security\nperformance by",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13970588235294118,
          "p": 0.2261904761904762,
          "f": 0.17272726800661167
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11029411764705882,
          "p": 0.17857142857142858,
          "f": 0.13636363164297538
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.11552v1",
      "true_abstract": "We explore the interplay between sovereign debt default/renegotiation and\nenvironmental factors (e.g., pollution from land use, natural resource\nexploitation). Pollution contributes to the likelihood of natural disasters and\ninfluences economic growth rates. The country can default on its debt at any\ntime while also deciding whether to invest in pollution abatement. The\nframework provides insights into the credit spreads of sovereign bonds and\nexplains the observed relationship between bond spread and a country's climate\nvulnerability. Through calibration for developing and low-income countries, we\ndemonstrate that there is limited incentive for these countries to address\nclimate risk, and the sensitivity of bond spreads to climate vulnerability\nremains modest. Climate risk does not play a relevant role on the decision to\ndefault on sovereign debt. Financial support for climate abatement expenditures\ncan effectively foster climate adaptation actions, instead renegotiation\nconditional upon pollution abatement does not produce any effect.",
      "generated_abstract": "r explores the impact of the COVID-19 pandemic on the labor\nmarket in the United States, focusing on the labor shortage and the\nshortage of skilled workers. Using data from the U.S. Department of Labor's\nCensus of Federal Labor Statistics, we examine the labor market conditions\nduring the pandemic and compare them with the pre-pandemic period. The results\nshow that the labor market was severely affected by the pandemic, with\nsignificant decreases in the labor force participation rate and the unemployment\nrate. The labor shortage and the shortage of skilled workers were also\nsignificantly affected by the pandemic. The results indicate that the\npandemic caused a significant decline in the labor force participation rate,\nreducing the number of employed workers by 1.9 million, and a significant\nincrease in the unemployment rate, with a 3",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10576923076923077,
          "p": 0.15942028985507245,
          "f": 0.12716762526245465
        },
        "rouge-2": {
          "r": 0.02112676056338028,
          "p": 0.02830188679245283,
          "f": 0.024193543492456764
        },
        "rouge-l": {
          "r": 0.10576923076923077,
          "p": 0.15942028985507245,
          "f": 0.12716762526245465
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2410.21090v1",
      "true_abstract": "One of goals in soft robotics is to achive spontaneous behavior like real\norganisms. To gain a clue to achieve this, we examined the long (16-hour)\nspontaneous exploratory locomotion of snails. The active forager snail, Tegula\nnigerrima, from an intertidal rocky shore was selected to test the general\nhypothesis that nervous systems are inherently near a critical state, which is\nself-organized to drive spontaneous animal behavior. This hypothesis, known as\nthe critical brain hypothesis, was originally proposed for vertebrate species,\nbut it might be applicable to other invertebrate species as well. We first\ninvestigated the power spectra of the speed of locomotion of the snails\n($N=39$). The spectra showed $1/{f^\\alpha}$ fluctuation, which is one of the\nsignatures of self-organized criticality. The $\\alpha$ was estimated to be\nabout 0.9. We further examined whether the spatial and temporal quantities show\nmultiple power-laws and scaling relations, which are rigorous criteria of\ncriticality. Although the satisfaction of these criteria is limited to a\ntruncated region and provides limited evidence to demonstrate the aspect of\nself-organization, the multiple power-laws and the scaling relations were\noverall satisfied. Therefore, these results additionally support the generality\nof the critical brain hypothesis.",
      "generated_abstract": "ence of COVID-19 as a global pandemic has led to widespread\nchallenges in access to healthcare and healthcare systems. The growing\nimpact of COVID-19 on the global economy has led to a decrease in the\navailability of healthcare professionals, particularly in low-income countries.\nThis paper investigates the impact of the COVID-19 pandemic on healthcare\nprofessionals, specifically in low-income countries. We utilized the Open Science\nFramework (OSF) to conduct a literature review of studies on the impact of the\nCOVID-19 pandemic on healthcare professionals in low-income countries. We\nextracted the information from 45 studies that were published between 2019 and\n2021. We analyzed the studies using a systematic review methodology and\nextracted the main findings of the studies. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.23333333333333334,
          "f": 0.15053763003815482
        },
        "rouge-2": {
          "r": 0.010869565217391304,
          "p": 0.020833333333333332,
          "f": 0.014285709779593255
        },
        "rouge-l": {
          "r": 0.09523809523809523,
          "p": 0.2,
          "f": 0.12903225369406884
        }
      }
    },
    {
      "paper_id": "cs.CV.q-bio/GN/2411.16793v1",
      "true_abstract": "Spatial transcriptomics (ST) provides high-resolution pathological images and\nwhole-transcriptomic expression profiles at individual spots across whole-slide\nscales. This setting makes it an ideal data source to develop multimodal\nfoundation models. Although recent studies attempted to fine-tune visual\nencoders with trainable gene encoders based on spot-level, the absence of a\nwider slide perspective and spatial intrinsic relationships limits their\nability to capture ST-specific insights effectively. Here, we introduce\nST-Align, the first foundation model designed for ST that deeply aligns\nimage-gene pairs by incorporating spatial context, effectively bridging\npathological imaging with genomic features. We design a novel pretraining\nframework with a three-target alignment strategy for ST-Align, enabling (1)\nmulti-scale alignment across image-gene pairs, capturing both spot- and\nniche-level contexts for a comprehensive perspective, and (2) cross-level\nalignment of multimodal insights, connecting localized cellular characteristics\nand broader tissue architecture. Additionally, ST-Align employs specialized\nencoders tailored to distinct ST contexts, followed by an Attention-Based\nFusion Network (ABFN) for enhanced multimodal fusion, effectively merging\ndomain-shared knowledge with ST-specific insights from both pathological and\ngenomic data. We pre-trained ST-Align on 1.3 million spot-niche pairs and\nevaluated its performance through two downstream tasks across six datasets,\ndemonstrating superior zero-shot and few-shot capabilities. ST-Align highlights\nthe potential for reducing the cost of ST and providing valuable insights into\nthe distinction of critical compositions within human tissue.",
      "generated_abstract": "y introduces the first benchmark for real-time 3D face and facial\npart recognition, developed as part of the Lung Cancer Research Challenge 2023.\nThe benchmark is built on the CelebA-HQ dataset, which is one of the largest\nfaces-only datasets with over 4.6 million images, covering diverse ethnicities\nand ages. The CelebA-HQ dataset is highly used for research in computer vision\nand deep learning, and is a widely used benchmark for face recognition and\nsegmentation. This study aims to bridge the gap between the research and\nindustry communities by developing a real-time 3D face and facial part\nrecognition benchmark. The benchmark was built using a combination of\npublicly available models, pre-trained on large datasets, and new models\ndeveloped in-house. The benchmark was tested on real-time 3D face and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09433962264150944,
          "p": 0.189873417721519,
          "f": 0.12605041573299927
        },
        "rouge-2": {
          "r": 0.004608294930875576,
          "p": 0.008771929824561403,
          "f": 0.006042291556670437
        },
        "rouge-l": {
          "r": 0.09433962264150944,
          "p": 0.189873417721519,
          "f": 0.12605041573299927
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.physics/space-ph/2503.00705v1",
      "true_abstract": "The first severe (G4) geomagnetic storm of Solar Cycle 25 occurred on 23-24\nApril 2023, following the arrival of a Coronal Mass Ejection (CME) on 23 April.\nThe characteristics of this CME, measured from coronagraphs (speed and mass),\ndid not indicate that it would trigger such an intense geomagnetic storm. In\nthis work, our aim is to understand why this CME led to such a geoeffective\noutcome. Our analysis spans from the source active region to the corona and\ninner heliosphere through 1 au using multiwavelength, multi-viewpoint remote\nsensing observations and in situ data. We find that rotation and possibly\ndeflection of the CME resulted in an axial magnetic field nearly parallel to\nthe ecliptic plane during the Earth encounter, which might explain the storm's\nseverity. Additionally, we find that imaging away from the Sun-Earth line is\ncrucial in hindcasting the CME Time-of-Arrival at Earth. The position (0.39 au)\nand detailed images from the SoloHI telescope onboard the Solar Orbiter\nmission, in combination with SOHO and STEREO images, helped decisively with the\nthree-dimensional (3D) reconstruction of the CME.",
      "generated_abstract": "y investigates the formation of binary neutron star systems\nin the Galactic center using an analytical model. We present a new approach\nthat allows us to determine the dynamical and tidal effects of the Galactic\ncenter on the binary systems through the calculation of the tidal deformation\nand the gravitational interaction. Our study provides a fundamental framework\nfor understanding the Galactic center and the formation of neutron star binaries\nin the Galactic center. The results show that the Galactic center has a large\ntidal field and has a significant effect on the tidal deformation of the\nbinary system, which may influence the dynamical evolution of the system. We\nalso find that the tidal deformation of the binary system may change the\norbital eccentricity of the system, which may affect the stability of the\nbinary system. Our results suggest that the Galactic center may play a\nsignificant role in the formation and evolution",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1328125,
          "p": 0.25,
          "f": 0.17346938322365693
        },
        "rouge-2": {
          "r": 0.011764705882352941,
          "p": 0.018018018018018018,
          "f": 0.014234870665266736
        },
        "rouge-l": {
          "r": 0.1328125,
          "p": 0.25,
          "f": 0.17346938322365693
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.16524v2",
      "true_abstract": "This study demonstrates the persistent dominance of identity based voting\nacross democratic systems, using the United States as a primary case and\ncomparative analyses of 19 other democracies as counterfactuals. Drawing solely\non election data from the Roper Center (1976 through recent cycles), we employ\nOLS regression, ANOVA, and correlation tests to show that race remains the\nstrongest predictor of party affiliation in the US (p < 0.001), with White\nvoters favoring Republicans and Black voters consistently supporting Democrats\n(85% since 1988). Income, education, and gender exemplified by gaps like 10\npoints in 2020 further shape voting patterns, yet racial identity predominates.\nComparative evidence from majoritarian (e.g., India), proportional (e.g.,\nGermany through 2025), and hybrid (e.g., South Korea with a 25 point gender\ngap) systems reveals no democracy where issue based voting fully supplants\nidentity based voting. Digital mobilization amplifies this trend globally.\nThese findings underscore identity enduring role in electoral behavior,\nchallenging assumptions of policy driven democratic choice.",
      "generated_abstract": "e the impact of a 2014 tax increase on the value of rental units in\nNew York City, a city with a high concentration of small rental units. Using\nestimates of the marginal tax rate of the tax increase from a recent\npublication, we find that the tax increase reduced the value of rental units\nby 1.2 percent. This reduction in the value of rental units is concentrated\nin the most expensive neighborhoods, where the value of units was reduced by\nup to 3.6 percent. The tax increase also reduced the average monthly rental\nrate by 0.2 percent, with the largest reductions in the most expensive\nneighborhoods. This reduction in rental rates was larger in the most expensive\nneighborhoods, where the rental rates were reduced by up to 3.3 percent.\nThese findings suggest that the tax increase did",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1297709923664122,
          "p": 0.26153846153846155,
          "f": 0.17346938332205342
        },
        "rouge-2": {
          "r": 0.0189873417721519,
          "p": 0.028846153846153848,
          "f": 0.022900758571179836
        },
        "rouge-l": {
          "r": 0.11450381679389313,
          "p": 0.23076923076923078,
          "f": 0.15306122005674733
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2409.00780v1",
      "true_abstract": "The main purpose of this work is the derivation of a functional partial\ndifferential equation (FPDE) for the calculations of equity-linked insurance\npolicies, where the payment stream may depend on the whole past history of the\nfinancial asset. To this end, we employ variational techniques from the theory\nof functional It\\^o calculus.",
      "generated_abstract": "We propose a novel method for predicting the market price of an asset in\nunderlying assets using a deep learning model. The proposed method utilizes\nGANs to generate synthetic data of the underlying assets and applies a\nvariational autoencoder (VAE) to learn a distribution of the synthetic data.\nThe VAE is trained to maximize the Kullback-Leibler divergence between the\ndistribution of the synthetic data and the distribution of the underlying\nassets. The VAE is also trained to minimize the Kullback-Leibler divergence\nbetween the synthetic data and the generated data. The VAE is trained with the\nunderlying asset data and the synthetic data. The proposed method can be\napplied to any asset and in any market.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1951219512195122,
          "p": 0.16,
          "f": 0.17582417087308314
        },
        "rouge-2": {
          "r": 0.0196078431372549,
          "p": 0.012658227848101266,
          "f": 0.015384610616569523
        },
        "rouge-l": {
          "r": 0.17073170731707318,
          "p": 0.14,
          "f": 0.15384614889506115
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2502.21106v1",
      "true_abstract": "Recent advancements in AI and medical imaging offer transformative potential\nin emergency head CT interpretation for reducing assessment times and improving\naccuracy in the face of an increasing request of such scans and a global\nshortage in radiologists. This study introduces a 3D foundation model for\ndetecting diverse neuro-trauma findings with high accuracy and efficiency.\nUsing large language models (LLMs) for automatic labeling, we generated\ncomprehensive multi-label annotations for critical conditions. Our approach\ninvolved pretraining neural networks for hemorrhage subtype segmentation and\nbrain anatomy parcellation, which were integrated into a pretrained\ncomprehensive neuro-trauma detection network through multimodal fine-tuning.\nPerformance evaluation against expert annotations and comparison with CT-CLIP\ndemonstrated strong triage accuracy across major neuro-trauma findings, such as\nhemorrhage and midline shift, as well as less frequent critical conditions such\nas cerebral edema and arterial hyperdensity. The integration of neuro-specific\nfeatures significantly enhanced diagnostic capabilities, achieving an average\nAUC of 0.861 for 16 neuro-trauma conditions. This work advances foundation\nmodels in medical imaging, serving as a benchmark for future AI-assisted\nneuro-trauma diagnostics in emergency radiology.",
      "generated_abstract": "r proposes a novel 3D deep learning-based method for automated\nbrain tumor segmentation in pediatric CT images. Our approach integrates a\nmulti-scale fully convolutional neural network (FCN) with a spatial attention\nmodule to enhance the segmentation performance. The FCN is composed of two\nsub-networks: a feature extractor and a classifier. The feature extractor\nextracts key features, such as texture and structure, from the brain tumor\nregion. The classifier is designed to predict the tumor's malignant status\nusing these features. To improve the model's robustness to small tumors, we\nintroduce a self-supervised contrastive learning approach that enables the\nclassifier to learn to predict the tumor's malignant status based on the\ndifferences in the feature representations of the tumor and surrounding\nregions. Additionally, we",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15873015873015872,
          "p": 0.24096385542168675,
          "f": 0.19138755502026064
        },
        "rouge-2": {
          "r": 0.023391812865497075,
          "p": 0.03508771929824561,
          "f": 0.02807017063859731
        },
        "rouge-l": {
          "r": 0.1349206349206349,
          "p": 0.20481927710843373,
          "f": 0.16267942104896882
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2503.02389v1",
      "true_abstract": "We propose a method for accurately detecting bioacoustic sound events that is\nrobust to overlapping events, a common issue in domains such as ethology,\necology and conservation. While standard methods employ a frame-based,\nmulti-label approach, we introduce an onset-based detection method which we\nname Voxaboxen. It takes inspiration from object detection methods in computer\nvision, but simultaneously takes advantage of recent advances in\nself-supervised audio encoders. For each time window, Voxaboxen predicts\nwhether it contains the start of a vocalization and how long the vocalization\nis. It also does the same in reverse, predicting whether each window contains\nthe end of a vocalization, and how long ago it started. The two resulting sets\nof bounding boxes are then fused using a graph-matching algorithm. We also\nrelease a new dataset designed to measure performance on detecting overlapping\nvocalizations. This consists of recordings of zebra finches annotated with\ntemporally-strong labels and showing frequent overlaps. We test Voxaboxen on\nseven existing data sets and on our new data set. We compare Voxaboxen to\nnatural baselines and existing sound event detection methods and demonstrate\nSotA results. Further experiments show that improvements are robust to frequent\nvocalization overlap.",
      "generated_abstract": "a universal language that has the potential to bridge people from\nvarious cultures, religions, and backgrounds. Music synthesis has emerged as a\npromising avenue to bridge cultures and languages. However, there remains a\nsignificant gap in the research and development of music generation systems\nthat can effectively integrate multiple languages. To address this gap, we\npresent a novel multilingual music synthesis framework, MUSIQ, that integrates\nmusic generation with large language models (LLMs). MUSIQ first generates\nlatent representations for each language and then uses these representations to\ngenerate music. This approach enables MUSIQ to synthesize music in multiple\nlanguages while preserving musical structure and harmonic coherence. Additionally,\nwe introduce the Cross-lingual LLM-based Style Transfer (CLST) module, which\ntransfers the style of one language to another, enhancing",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13076923076923078,
          "p": 0.18888888888888888,
          "f": 0.15454544971074397
        },
        "rouge-2": {
          "r": 0.005376344086021506,
          "p": 0.008264462809917356,
          "f": 0.0065146532045998375
        },
        "rouge-l": {
          "r": 0.12307692307692308,
          "p": 0.17777777777777778,
          "f": 0.14545454061983487
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.06790v1",
      "true_abstract": "Recent research applying text-to-image (T2I) diffusion models to real-world\nsuper-resolution (SR) has achieved remarkable success. However, fundamental\nmisalignments between T2I and SR targets result in a dilemma between inference\nspeed and detail fidelity. Specifically, T2I tasks prioritize multi-step\ninversion to synthesize coherent outputs aligned with textual prompts and\nshrink the latent space to reduce generating complexity. Contrariwise, SR tasks\npreserve most information from low-resolution input while solely restoring\nhigh-frequency details, thus necessitating sufficient latent space and fewer\ninference steps. To bridge the gap, we present a one-step diffusion model for\ngenerative detail restoration, GenDR, distilled from a tailored diffusion model\nwith larger latent space. In detail, we train a new SD2.1-VAE16 (0.9B) via\nrepresentation alignment to expand latent space without enlarging the model\nsize. Regarding step-distillation, we propose consistent score identity\ndistillation (CiD) that incorporates SR task-specific loss into score\ndistillation to leverage more SR priors and align the training target.\nFurthermore, we extend CiD with adversarial learning and representation\nalignment (CiDA) to enhance perceptual quality and accelerate training. We also\npolish the pipeline to achieve a more efficient inference. Experimental results\ndemonstrate that GenDR achieves state-of-the-art performance in both\nquantitative metrics and visual fidelity.",
      "generated_abstract": "r proposes a novel end-to-end framework for visual question answering\n(VQA) that leverages large language models (LLMs) to solve visual reasoning\ntasks. By integrating LLMs into the VQA framework, our approach enhances\nvisual reasoning by enabling faster and more efficient question generation and\nquestion-answering. Additionally, we introduce an image-to-question\ntransformer, which is capable of generating multiple questions for a single\nimage. This transformer is trained with a novel multi-task learning (MTL)\nframework, enabling it to generate questions that capture the essence of the\nimage while maintaining the integrity of its content. By integrating LLMs and\nmulti-task learning, our approach enables efficient and effective visual\nreasoning. We evaluate our approach on the Visual Genome dataset, demonstrating\nthat our method achieves significant improvements in accuracy and efficiency.\nAdditionally,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14084507042253522,
          "p": 0.22988505747126436,
          "f": 0.17467248437138896
        },
        "rouge-2": {
          "r": 0.005235602094240838,
          "p": 0.00847457627118644,
          "f": 0.0064724871884493124
        },
        "rouge-l": {
          "r": 0.14084507042253522,
          "p": 0.22988505747126436,
          "f": 0.17467248437138896
        }
      }
    },
    {
      "paper_id": "physics.ed-ph.physics/ed-ph/2503.03850v1",
      "true_abstract": "Although physics has become increasingly computational, with computing even\nbeing considered the third pillar of physics [1], it is still not well\nintegrated into physics education [2]. Research suggests that integrating\nComputational Thinking (CT) into physics enhances conceptual understanding and\nstrengthens students ability to model and analyze phenomena [3]. Building on\nthis, we designed a didactic sequence for K9 students to foster specific CT\npractices while reinforcing fundamental kinematics concepts. Assessments\nhighlight student's ability to apply CT skills to analyze accelerated motion.\nThis activity can be seamlessly integrated into introductory kinematics\ncourses.",
      "generated_abstract": "This paper presents a novel, hands-on, immersive experience that enables\nstudents to develop their understanding of quantum mechanics through a\ncollaborative learning process. The curriculum focuses on building a cohesive\nconceptual framework and applying it to solve physics problems through\ninteractive activities. It is designed to engage students in a\nhands-on, interactive learning environment that emphasizes collaboration and\ncreativity. The curriculum is aligned with the International Physics Undergraduate\nLearning Outcomes (IPUL). It is available for free download at\nhttps://bit.ly/qm-experience.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22077922077922077,
          "p": 0.2698412698412698,
          "f": 0.24285713790714292
        },
        "rouge-2": {
          "r": 0.011363636363636364,
          "p": 0.012987012987012988,
          "f": 0.012121207143436388
        },
        "rouge-l": {
          "r": 0.2077922077922078,
          "p": 0.25396825396825395,
          "f": 0.2285714236214287
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.04337v1",
      "true_abstract": "In this paper, a compressor system is analyzed in order to show its\ncharacteristics and design a control scheme to improve its efficiency. A\nmathematical technique has been created to forecast the onset of surge and\ninstability in a compressor chart, drawing from the nonlinear Greitzer and\nMoore model. This approach employs the phase plane and Jacobian matrix to\nidentify both stable and unstable regions within the compressor, as well as to\ncapture the limit cycle within the unstable region. A predictive analytical\napproach for anticipating compressor surge and instability is of great\nimportance in system instrumentation and control. State space model is built up\nby nonlinear Greitzer equations. Validation from previous study about especial\ncompressor will be considered for evaluation of mathematic method. Upstream\nflow acts as a disturbance to control loop and controller cannot satisfy\ndesired requirements with flow variances, ergo it is essential that controller\nis adapted to new conditions. Since control signal is linearly related to\nsystem output, a PD controller is used to control compressor system. An\nadaptive PD controller is designed with MRAS method based on a reference model.\nAdaptive controller can stabilize compressor and increase its efficiency in the\npresence of any disturbances. Simulation results shows that an adaptive\ncontroller can provide good performance and convergence in case of speed\nchanges by adapting gain parameters, and adaptive will be compared with normal\nPID. Finally, controller stability is investigated.",
      "generated_abstract": "r focuses on the design of a robust and energy-efficient\nsolar energy system with a battery storage system (BSS) in a multi-agent\nsystem. A multi-agent system is a complex system comprising several\ninteracting agents that interact with each other. This paper develops a\ncoordinated multi-agent system (CoMAS) to address the challenges of\nmulti-agent interaction and distributed energy resources (DERs) integration.\nThe CoMAS model is formulated as a nonlinear programming problem with\nconstraints that ensure the stability of the system. The objective of the\nCoMAS model is to maximize the energy storage capacity of the BSS while\nminimizing the energy consumption of the solar energy system. This paper\nintroduces a distributed control strategy for the BSS to minimize the\nenergy consumption while ensuring the stability of the system. The proposed\ncontrol strategy includes a distributed control strategy",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13986013986013987,
          "p": 0.2898550724637681,
          "f": 0.18867924089222154
        },
        "rouge-2": {
          "r": 0.01809954751131222,
          "p": 0.03571428571428571,
          "f": 0.024024019559740747
        },
        "rouge-l": {
          "r": 0.13286713286713286,
          "p": 0.2753623188405797,
          "f": 0.17924527862807058
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.15104v2",
      "true_abstract": "In both artificial and biological systems, the centered kernel alignment\n(CKA) has become a widely used tool for quantifying neural representation\nsimilarity. While current CKA estimators typically correct for the effects of\nfinite stimuli sampling, the effects of sampling a subset of neurons are\noverlooked, introducing notable bias in standard experimental scenarios. Here,\nwe provide a theoretical analysis showing how this bias is affected by the\nrepresentation geometry. We then introduce a novel estimator that corrects for\nboth input and feature sampling. We use our method for evaluating both\nbrain-to-brain and model-to-brain alignments and show that it delivers reliable\ncomparisons even with very sparsely sampled neurons. We perform within-animal\nand across-animal comparisons on electrophysiological data from visual cortical\nareas V1, V4, and IT data, and use these as benchmarks to evaluate\nmodel-to-brain alignment. We also apply our method to reveal how object\nrepresentations become progressively disentangled across layers in both\nbiological and artificial systems. These findings underscore the importance of\ncorrecting feature-sampling biases in CKA and demonstrate that our\nbias-corrected estimator provides a more faithful measure of representation\nalignment. The improved estimates increase our understanding of how neural\nactivity is structured across both biological and artificial systems.",
      "generated_abstract": "vances in high-throughput sequencing technologies have allowed for\nthe measurement of thousands of single-cell genes in each cell type, but the\nhigh-dimensional data challenge posed by single-cell transcriptomics can be\nfaced by dimensionality reduction techniques. Dimensionality reduction is\nessential for the analysis of single-cell data due to the sheer volume of\ninformation, but it must be performed in a way that preserves information\ncritical for the interpretation of the data. This is particularly important in\nthe case of microbiome studies, where the number of cells and the number of\ngrowth conditions are so large that it is essential to preserve the\ncorresponding information in the data. In this article, we present a novel\ndimensionality reduction technique that leverages the non-negative orthant,\nwhich is a set of vectors in the Euclidean space that is orthogonal to the\nEuclidean space",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13636363636363635,
          "p": 0.21951219512195122,
          "f": 0.1682242943383703
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.024193548387096774,
          "f": 0.019169324289113054
        },
        "rouge-l": {
          "r": 0.11363636363636363,
          "p": 0.18292682926829268,
          "f": 0.14018691116080023
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/SY/2503.10419v1",
      "true_abstract": "In motion simulation, motion cueing algorithms are used for the trajectory\nplanning of the motion simulator platform, where workspace limitations prevent\ndirect reproduction of reference trajectories. Strategies such as motion\nwashout, which return the platform to its center, are crucial in these\nsettings. For serial robotic MSPs with highly nonlinear workspaces, it is\nessential to maximize the efficient utilization of the MSPs kinematic and\ndynamic capabilities. Traditional approaches, including classical washout\nfiltering and linear model predictive control, fail to consider\nplatform-specific, nonlinear properties, while nonlinear model predictive\ncontrol, though comprehensive, imposes high computational demands that hinder\nreal-time, pilot-in-the-loop application without further simplification. To\novercome these limitations, we introduce a novel approach using deep\nreinforcement learning for motion cueing, demonstrated here for the first time\nin a 6-degree-of-freedom setting with full consideration of the MSPs kinematic\nnonlinearities. Previous work by the authors successfully demonstrated the\napplication of DRL to a simplified 2-DOF setup, which did not consider\nkinematic or dynamic constraints. This approach has been extended to all 6 DOF\nby incorporating a complete kinematic model of the MSP into the algorithm, a\ncrucial step for enabling its application on a real motion simulator. The\ntraining of the DRL-MCA is based on Proximal Policy Optimization in an\nactor-critic implementation combined with an automated hyperparameter\noptimization. After detailing the necessary training framework and the\nalgorithm itself, we provide a comprehensive validation, demonstrating that the\nDRL MCA achieves competitive performance against established algorithms.\nMoreover, it generates feasible trajectories by respecting all system\nconstraints and meets all real-time requirements with low...",
      "generated_abstract": "This paper addresses the challenges associated with the emergence of a high\nnumber of connected vehicles and the need to support the safety and efficiency\nof traffic. To address these issues, this paper proposes a hybrid control\nscheme that combines a reinforcement learning (RL) algorithm with a\nconstrained optimization (CO) to control traffic flow. The RL algorithm\nidentifies a set of traffic-flow control rules based on the available data,\nand the CO analyzes the impact of these rules on traffic flow. The proposed\nscheme is validated through simulations, where it demonstrates its ability to\nachieve higher flow rates and reduced delays compared to a traditional\napproach that relies on a single rule. The results show that the proposed\nscheme can reduce traffic delays by up to 60% and increase traffic flow rates\nby up to 40% compared to a traditional approach that relies on a single rule.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13872832369942195,
          "p": 0.2891566265060241,
          "f": 0.18749999561798106
        },
        "rouge-2": {
          "r": 0.024390243902439025,
          "p": 0.046875,
          "f": 0.0320855569950534
        },
        "rouge-l": {
          "r": 0.12716763005780346,
          "p": 0.26506024096385544,
          "f": 0.17187499561798106
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/EC/2502.17906v2",
      "true_abstract": "In econophysics, there are several enigmatic empirical laws: (i)~the\nmarket-order flow has strong persistence (long-range order-sign correlation),\nwell formulated as the Lillo-Mike-Farmer model. This phenomenon seems\nparadoxical given the diffusive and unpredictable price dynamics; (ii)~the\nprice impact $I(Q)$ of a large metaorder $Q$ follows the square-root law,\n$I(Q)\\propto \\sqrt{Q}$. In this Letter, we propose an exactly solvable model of\nthe nonlinear price-impact dynamics that unifies these enigmas. We generalize\nthe Lillo-Mike-Farmer model to nonlinear price-impact dynamics, which is mapped\nto an exactly solvable L\\'evy-walk model. Our exact solution and numerical\nsimulations reveal three important points: First, the price dynamics remains\ndiffusive under the square-root law, even under the long-range correlation.\nSecond, price-movement statistics follows truncated power laws with typical\nexponent around three. Third, volatility has long memory. While this simple\nmodel lacks adjustable free parameters, it naturally aligns even with other\nenigmatic empirical laws, such as (iii)~the inverse-cubic law for price\nstatistics and (iv)~volatility clustering. This work illustrates the crucial\nrole of the square-root law in understanding rich and complex financial price\ndynamics from a single coherent viewpoint.",
      "generated_abstract": "ng number of climate-related risks posed by extreme weather events\n(e.g., droughts, floods, wildfires, and storm surges) is increasingly\nhighlighting the need for climate-aware financial risk management. However,\ncurrent methods to quantify climate-related risks often overlook the complex\nrelationship between weather and climate, resulting in inaccurate\nestimations of climate-related risk. In this paper, we propose a novel\nframework for evaluating climate-related financial risk by integrating\nclimate-related weather risk factors into a multi-step framework that\nconsiders both weather and climate. We develop a model for assessing climate\nrisk based on the integration of climate-related weather risk factors into the\nframework. The framework enables the incorporation of climate risk factors into\nthe analysis of extreme weather events, providing a more complete assessment\nof the potential impacts of climate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1328125,
          "p": 0.21518987341772153,
          "f": 0.1642512030096386
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.026785714285714284,
          "f": 0.021660645002542483
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.20253164556962025,
          "f": 0.15458936726084638
        }
      }
    },
    {
      "paper_id": "math.CV.math/CV/2503.05855v1",
      "true_abstract": "We first study subextensions of m-subharmonic functions in weighted energy\nclasses with given boundary values. The results are used to approximate an\nm-subharmonic function in weighted energy classes with given boundary values by\nan increasing sequence of m-subharmonic functions defined on larger domains.",
      "generated_abstract": "the set of solutions to the $n$-th order linear differential\nequations of the form\n$\\sum_{i=1}^n a_i(x) \\frac{\\partial^2}{\\partial x_i^2} + b_i(x) \\frac{\\partial} %{\\bf p}\n}{\\partial x_i} = 0$, where $a_i,b_i$ are given functions. We prove that the\npolynomials $a_i(x)$ and $b_i(x)$ are the coefficients of a monic polynomial in\n$x$ with integer coefficients. We prove that the set of solutions of this\nequation is equal to the set of solutions of the equation\n$\\sum_{i=1}^n \\frac{\\partial^2}{\\partial x_i^2} u = 0$ with $u$ a polynomial.\n  We prove that for $n=2$ the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26666666666666666,
          "p": 0.1509433962264151,
          "f": 0.19277107972129492
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.26666666666666666,
          "p": 0.1509433962264151,
          "f": 0.19277107972129492
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.07615v1",
      "true_abstract": "Climate change is increasing the frequency and severity of natural disasters\nworldwide. Media coverage of these events may be vital to generate empathy and\nmobilize global populations to address the common threat posed by climate\nchange. Using a dataset of 466 news sources from 123 countries, covering 135\nmillion news articles since 2016, we apply an event study framework to measure\ncross-border media activity following natural disasters. Our results shows that\nwhile media attention rises after disasters, it is heavily skewed towards\ncertain events, notably earthquakes, accidents, and wildfires. In contrast,\nclimatologically salient events such as floods, droughts, or extreme\ntemperatures receive less coverage. This cross-border disaster reporting is\nstrongly related to the number of deaths associated with the event, especially\nwhen the affected populations share strong social ties or genetic similarities\nwith those in the reporting country. Achieving more balanced media coverage\nacross different types of natural disasters may be essential to counteract\nskewed perceptions. Further, fostering closer social connections between\ncountries may enhance empathy and mobilize the resources necessary to confront\nthe global threat of climate change.",
      "generated_abstract": "opment of blockchain technology has emerged as a powerful tool for\nincreasing efficiency and transparency in global supply chains. By linking\nmultiple actors in the supply chain, blockchain offers a robust framework for\nmonitoring and tracing products and ensuring their authenticity. However,\nexisting blockchain-based traceability systems face several challenges,\nincluding data quality and scalability. To address these limitations, this\npaper proposes a novel framework for blockchain-based traceability that\nintegrates blockchain technologies with data-driven methods. The proposed\nframework uses a data-driven approach to enhance the accuracy and efficiency\nof blockchain-based traceability systems. The framework incorporates\nblockchain-based technologies to verify and store data related to products,\nsuch as product codes, barcodes, and labels. The framework also utilizes\nmachine learning techniques to analyze and interpret the data to identify\ntr",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13076923076923078,
          "p": 0.19767441860465115,
          "f": 0.15740740261488356
        },
        "rouge-2": {
          "r": 0.017543859649122806,
          "p": 0.024793388429752067,
          "f": 0.020547940352084088
        },
        "rouge-l": {
          "r": 0.11538461538461539,
          "p": 0.1744186046511628,
          "f": 0.13888888409636505
        }
      }
    },
    {
      "paper_id": "math.PR.math/PR/2503.09550v1",
      "true_abstract": "We prove that the limit profile of a sequence of reversible Markov chains\nexhibiting total variation cutoff is a continuous function, under a computable\ncondition involving the spectrum of the transition matrix and the cutoff\nwindow.",
      "generated_abstract": "In this paper, we consider the case when the vector field $V$ on the\ngraph $G$ is not only a linear combination of the edge vectors, but also a\nlinear combination of the vertex vectors. We give an explicit formula for the\nHessian of the energy functional with respect to the Hessian of the\nLagrangian function. Our formula is valid for any parameter $t>0$. We also\nderive a formula for the Hessian of the Lagrangian function with respect to the\nHessian of the energy functional. Our results give a new proof of the\nnon-existence of equilibrium of the energy functional with respect to the\nHessian of the Lagrangian function.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17857142857142858,
          "p": 0.09615384615384616,
          "f": 0.12499999545000016
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.013888888888888888,
          "f": 0.01869158438291657
        },
        "rouge-l": {
          "r": 0.17857142857142858,
          "p": 0.09615384615384616,
          "f": 0.12499999545000016
        }
      }
    },
    {
      "paper_id": "math.NT.math/NT/2503.10443v1",
      "true_abstract": "We prove a completely explicit and effective upper bound for the\nN\\'eron--Tate height of rational points of curves of genus at least $2$ over\nnumber fields, provided that they have enough automorphisms with respect to the\nMordell--Weil rank of their jacobian. Our arguments build on Arakelov theory\nfor arithmetic surfaces. Our bounds are practical, and we illustrate this by\nexplicitly computing the rational points of a certain genus $2$ curve whose\njacobian has Mordell--Weil rank $2$.",
      "generated_abstract": "that the number of non-isomorphic finite subgraphs of a graph $G$\nis polynomial in the chromatic number $\\chi(G)$ of $G$. This implies that\n$\\chi(G)$ is exponential in the order $e(G)$ of an edge-deletion graph of $G$.\nIn particular, it is exponential in the order of the complement of the\ngraph-theoretic dual of a graph. We also show that $\\chi(G)$ is polynomial in\nthe order of the complement of the graph-theoretic dual of a graph $H$ if and\nonly if $H$ is a subgraph of a graph $G$ with $\\chi(G) = \\chi(H)$.\n  We also give a new proof of the so-called \"subgraph theorem\" for graph\nisomorphism. We show that there are polynomial-time algorithms to decide\ngraph isomorphism in two classes of graphs:",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1896551724137931,
          "p": 0.19298245614035087,
          "f": 0.19130434282646513
        },
        "rouge-2": {
          "r": 0.013888888888888888,
          "p": 0.011363636363636364,
          "f": 0.012499995050001959
        },
        "rouge-l": {
          "r": 0.13793103448275862,
          "p": 0.14035087719298245,
          "f": 0.13913042978298695
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.10846v1",
      "true_abstract": "The digitalization of public administration has advanced significantly on a\nglobal scale. Many governments now view digital platforms as essential for\nimproving the delivery of public services and fostering direct communication\nbetween citizens and public institutions. However, this view overlooks the role\nplayed by digital intermediaries significantly shape the provision of\ne-government services. Using Chile as a case study, we analyze these\nintermediaries through a national survey on digitalization, we find five types\nof intermediaries: family members, peers, political figures, bureaucrats, and\ncommunity leaders. The first two classes comprise close intermediaries, while\nthe latter three comprise hierarchical intermediaries. Our findings suggest\nthat all these intermediaries are a critical but underexplored element in the\ndigitalization of public administration.",
      "generated_abstract": "This paper develops a dynamic model of the global economy that incorporates\nthe impact of trade and migration. We show that the model can capture a\nnumber of important features of the economy, including the existence of a\ntrade deficit, a trade-dependent monetary regime, and a growing population. The\nmodel predicts that trade and migration have a relatively weak impact on the\nglobal economy, with the trade deficit and migration affecting the economy\nthroughout the model's time period. The model also suggests that the global\neconomy is likely to experience a period of instability following a\nmonetary transition, with the trade deficit and migration being key factors\nthat contribute to instability. Finally, the model shows that migration can\nhave a positive impact on the economy, particularly when the monetary regime\nis fixed.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09302325581395349,
          "p": 0.12307692307692308,
          "f": 0.10596025999736876
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09302325581395349,
          "p": 0.12307692307692308,
          "f": 0.10596025999736876
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/CB/2407.11453v1",
      "true_abstract": "Telomeres are repetitive sequences of nucleotides at the end of chromosomes,\nwhose evolution over time is intrinsically related to biological ageing. In\nmost cells, with each cell division, telomeres shorten due to the so-called end\nreplication problem, which can lead to replicative senescence and a variety of\nage-related diseases. On the other hand, in certain cells, the presence of the\nenzyme telomerase can lead to the lengthening of telomeres, which may delay or\nprevent the onset of such diseases but can also increase the risk of cancer.In\nthis article, we propose a stochastic representation of this biological model,\nwhich takes into account multiple chromosomes per cell, the effect of\ntelomerase, different cell types and the dependence of the distribution of\ntelomere length on the dynamics of the process. We study theoretical properties\nof this model, including its long-term behaviour. In addition, we investigate\nnumerically the impact of the model parameters on biologically relevant\nquantities, such as the Hayflick limit and the Malthusian parameter of the\npopulation of cells.",
      "generated_abstract": "ty of cells to respond to extracellular cues is crucial for\ncellular communication and the maintenance of homeostasis. This process is\naccompanied by the evolution of cellular structures that serve as sensors,\ntransmitters, and regulators of cellular signals. This review explores the\nemergence of these cellular structures and their role in signal transduction\nthrough the lens of cellular organization. We focus on the following key\nquestions: How does cellular organization facilitate the formation of\nstructures, such as transduction channels, that allow cells to sense extracellular\nsignals? What are the different types of sensors and how do they interact to\ntransform signals into specific responses? How are signals transmitted from\nsensors to transduction channels, and how are responses regulated? Finally, we\nexplore the emergence of cellular structures, such as tubules and c",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1592920353982301,
          "p": 0.22784810126582278,
          "f": 0.18749999515679264
        },
        "rouge-2": {
          "r": 0.025157232704402517,
          "p": 0.03389830508474576,
          "f": 0.028880861535534978
        },
        "rouge-l": {
          "r": 0.07079646017699115,
          "p": 0.10126582278481013,
          "f": 0.08333332849012616
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/OT/2407.05572v2",
      "true_abstract": "This study addresses important issues of traffic congestion and vehicle\nemissions in urban areas by developing a comprehensive mathematical framework\nto evaluate Park-and-Ride (PnR) systems. The proposed approach integrates\nqueueing theory and emissions modeling to simultaneously assess waiting times,\ntravel times, and vehicle emissions under various PnR usage scenarios. The\nmethodology employs a novel combination of Monte Carlo simulation and matrix\ngeometric analytic methods to analyze a queueing network representing PnR\nfacilities and road traffic. A case study of Tsukuba, Japan demonstrates the\nmodel's applicability, revealing potential reductions in social costs related\nto total trip time and emissions through optimized PnR policies. Specifically,\nthe study found that implementing optimal bus frequency and capacity policies\ncould reduce total social costs by up to 30\\% compared to current conditions.\nThis research contributes to the literature by providing a unified framework\nfor evaluating PnR systems that considers both time and environmental costs,\noffering valuable insights for urban planners and policymakers seeking to\nimprove transportation sustainability. The proposed model utilizes a single\nserver queue with a deterministic service time and multiple arrival streams to\nrepresent traffic flow, incorporating both private cars and public buses.\nEmissions are calculated using the Methodologies for Estimating Air Pollutant\nEmissions from Transport (MEET) framework. The social cost of emissions and\ntotal trip time (SCETT) is introduced as a comprehensive metric for evaluating\nPnR system performance.",
      "generated_abstract": "A common challenge in data science is finding a statistical model that fits\nthe data well, and often a statistical model that is interpretable is preferred.\nThere are many different ways to express a model, and the choice depends on the\nobjective of the analysis. In this paper, we first review the classical\napproaches to interpretable models and then introduce the notion of\n\"interpretability\" for statistical models, and we propose an alternative\napproach for interpretable models, namely the approach of \"model\ninference\". This approach allows the analysis of models without\ninterpretation, and we provide an example of this approach for a model of\ninterest to us, namely the Random Forest. We then discuss the relationship\nbetween the approaches of \"interpretability\" and \"model inference\".",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09333333333333334,
          "p": 0.2028985507246377,
          "f": 0.12785387696253223
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08666666666666667,
          "p": 0.18840579710144928,
          "f": 0.11872145687120803
        }
      }
    },
    {
      "paper_id": "cs.IR.cs/IR/2503.10166v1",
      "true_abstract": "With the proliferation of images in online content, language-guided image\nretrieval (LGIR) has emerged as a research hotspot over the past decade,\nencompassing a variety of subtasks with diverse input forms. While the\ndevelopment of large multimodal models (LMMs) has significantly facilitated\nthese tasks, existing approaches often address them in isolation, requiring the\nconstruction of separate systems for each task. This not only increases system\ncomplexity and maintenance costs, but also exacerbates challenges stemming from\nlanguage ambiguity and complex image content, making it difficult for retrieval\nsystems to provide accurate and reliable results. To this end, we propose\nImageScope, a training-free, three-stage framework that leverages collective\nreasoning to unify LGIR tasks. The key insight behind the unification lies in\nthe compositional nature of language, which transforms diverse LGIR tasks into\na generalized text-to-image retrieval process, along with the reasoning of LMMs\nserving as a universal verification to refine the results. To be specific, in\nthe first stage, we improve the robustness of the framework by synthesizing\nsearch intents across varying levels of semantic granularity using\nchain-of-thought (CoT) reasoning. In the second and third stages, we then\nreflect on retrieval results by verifying predicate propositions locally, and\nperforming pairwise evaluations globally. Experiments conducted on six LGIR\ndatasets demonstrate that ImageScope outperforms competitive baselines.\nComprehensive evaluations and ablation studies further confirm the\neffectiveness of our design.",
      "generated_abstract": "net of Things (IoT) has transformed our daily lives by enabling\nsmart homes, smart cities, and intelligent factories. In this paper, we\nexplore how machine learning (ML) can be leveraged to understand and manage\nthe large volumes of IoT data. We propose a hybrid approach, which combines\nML with classical data analytics (e.g., statistical modeling, text\nanalytics, and data mining). Our approach aims to address the challenges of\nincreasing data volumes and complexities in IoT systems. We introduce a\nfusion-based approach, where we combine the predictive capabilities of ML with\nthe insights provided by classical data analytics. This fusion approach is\ndesigned to improve the predictive accuracy of ML models, while also\nenhancing the interpretability and explainability of the resulting models. We\ndemonstrate the effectiveness of our approach through",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15853658536585366,
          "p": 0.28888888888888886,
          "f": 0.20472440487320984
        },
        "rouge-2": {
          "r": 0.0273972602739726,
          "p": 0.048,
          "f": 0.034883716303576874
        },
        "rouge-l": {
          "r": 0.13414634146341464,
          "p": 0.24444444444444444,
          "f": 0.17322834188108388
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2503.06251v1",
      "true_abstract": "Short-term patterns in financial time series form the cornerstone of many\nalgorithmic trading strategies, yet extracting these patterns reliably from\nnoisy market data remains a formidable challenge. In this paper, we propose an\nentropy-assisted framework for identifying high-quality, non-overlapping\npatterns that exhibit consistent behavior over time. We ground our approach in\nthe premise that historical patterns, when accurately clustered and pruned, can\nyield substantial predictive power for short-term price movements. To achieve\nthis, we incorporate an entropy-based measure as a proxy for information gain.\nPatterns that lead to high one-sided movements in historical data, yet retain\nlow local entropy, are more informative in signaling future market direction.\nCompared to conventional clustering techniques such as K-means and Gaussian\nMixture Models (GMM), which often yield biased or unbalanced groupings, our\napproach emphasizes balance over a forced visual boundary, ensuring that\nquality patterns are not lost due to over-segmentation. By emphasizing both\npredictive purity (low local entropy) and historical profitability, our method\nachieves a balanced representation of Buy and Sell patterns, making it better\nsuited for short-term algorithmic trading strategies.",
      "generated_abstract": "r introduces a novel, non-parametric framework for the pricing of\ndifferentiable European call options in a continuous-time stochastic\ndeterministic framework. The framework, which we call the continuous-time\nstochastic deterministic pricing of options (CTSDPO), is based on the\ncontinued-time stochastic deterministic pricing of options (CTSDPO) framework\nfor the pricing of European call options, but extends it to the continuous\ntime case. The CTSDPO framework is based on the notion of continuous-time\nstochastic processes and the corresponding continuous-time stochastic\ndeterministic price processes. The CTSDPO framework is based on the notion of\nstochastic processes and deterministic price processes. The CTSDPO framework\ncan be seen as a continuous-time extension of the CTSDPO framework, which\noriginally dealt with the pricing of European put",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11194029850746269,
          "p": 0.29411764705882354,
          "f": 0.16216215816859034
        },
        "rouge-2": {
          "r": 0.011560693641618497,
          "p": 0.02531645569620253,
          "f": 0.015873011568721247
        },
        "rouge-l": {
          "r": 0.1044776119402985,
          "p": 0.27450980392156865,
          "f": 0.1513513473577795
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2501.17096v1",
      "true_abstract": "Estimating market impact and transaction costs of large trades (metaorders)\nis a very important topic in finance. However, using models of price and trade\nbased on public market data provide average price trajectories which are\nqualitatively different from what is observed during real metaorder executions:\nthe price increases linearly, rather than in a concave way, during the\nexecution and the amount of reversion after its end is very limited. We claim\nthat this is a generic phenomenon due to the fact that even sophisticated\nstatistical models are unable to correctly describe the origin of the\nautocorrelation of the order flow. We propose a modified Transient Impact Model\nwhich provides more realistic trajectories by assuming that only a fraction of\nthe metaorder trading triggers market order flow. Interestingly, in our model\nthere is a critical condition on the kernels of the price and order flow\nequations in which market impact becomes permanent.",
      "generated_abstract": "opment of financial markets, particularly in emerging markets, has\noften been accompanied by increased volatility and uncertainty. These factors\ncan undermine the effectiveness of financial regulation, leading to increased\nrisks of financial crises. To address these challenges, this paper proposes a\nsystematic framework for evaluating the resilience of emerging markets to\nfinancial crises. The framework uses the volatility shocks generated by the\nBlack-Scholes model to simulate the impact of financial shocks on stock\nreturns and the exchange rate. It then analyzes the systemic risks posed by\nvolatility shocks to emerging market economies. The results show that the\nvolatility shocks generated by the Black-Scholes model can significantly\naffect the exchange rate, which is an important indicator of macroeconomic\nstability. This paper provides an innovative framework for assess",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17525773195876287,
          "p": 0.21794871794871795,
          "f": 0.19428570934465317
        },
        "rouge-2": {
          "r": 0.007142857142857143,
          "p": 0.009009009009009009,
          "f": 0.007968122556787865
        },
        "rouge-l": {
          "r": 0.16494845360824742,
          "p": 0.20512820512820512,
          "f": 0.18285713791608174
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.physics/bio-ph/2503.06239v1",
      "true_abstract": "Interactions between crawling cells, which are essential for many biological\nprocesses, can be quantified by measuring cell-cell collisions. Conventionally,\nexperiments of cell-cell collisions are conducted on two-dimensional flat\nsubstrates, where colliding cells repolarize and move away upon contact with\none another in \"contact inhibition of locomotion\" (CIL). Inspired by recent\nexperiments that show cells on suspended nanofibers have qualitatively\ndifferent CIL behaviors than those on flat substrates, we develop a phase field\nmodel of cell motility and two-cell collisions in fiber geometries. Our model\nincludes cell-cell and cell-fiber adhesion, and a simple positive feedback\nmechanism of cell polarity. We focus on cell collisions on two parallel fibers,\nfinding that larger cell deformability (lower membrane tension), larger\npositive feedback of polarization, and larger fiber spacing promote more\noccurrences of cells walking past one another. We can capture this behavior\nusing a simple linear stability analysis on the cell-cell interface upon\ncollision.",
      "generated_abstract": "ethodology for the generation of three-dimensional (3D) human\ncell-based in vitro tissue models of human disease is presented. This approach\nis based on the integration of a novel 3D cell culture system and a\ncomputational model that predicts cell fate decisions in response to\nindividual cellular signals. The 3D tissue model is generated using an\nadvanced 3D cell culture system that facilitates the growth of cells in a\nmulti-dimensional culture matrix. The system allows for the integration of\nindividual cellular signals and provides a detailed view of the 3D tissue\norganization, which can be visualized and analyzed using an advanced\ncomputational model. The proposed approach provides a robust and scalable\napproach for the generation of 3D human cell-based tissue models for the\nstudy of disease progression and therapeutic response. The approach offers",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14423076923076922,
          "p": 0.22727272727272727,
          "f": 0.17647058348512124
        },
        "rouge-2": {
          "r": 0.027972027972027972,
          "p": 0.03571428571428571,
          "f": 0.03137254409350328
        },
        "rouge-l": {
          "r": 0.1346153846153846,
          "p": 0.21212121212121213,
          "f": 0.1647058776027683
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.15422v1",
      "true_abstract": "We study the classical object reallocation problem under strict preferences,\nwith a focus on characterizing \"TTC domains\" -- preference domains on which the\nTop Trading Cycles (TTC) mechanism is the unique mechanism satisfying\nindividual rationality, Pareto efficiency, and strategyproofness. We introduce\na sufficient condition for a domain to be a TTC domain, which we call the\ntop-two condition. This condition requires that, within any subset of objects,\nif two objects can each be most-preferred, they can also be the top-two\nmost-preferred objects (in both possible orders). A weaker version of this\ncondition, applying only to subsets of size three, is shown to be necessary.\nThese results provide a complete characterization of TTC domains for the case\nof three objects, unify prior studies on specific domains such as single-peaked\nand single-dipped preferences, and classify several previously unexplored\ndomains as TTC domains or not.",
      "generated_abstract": "er a market where buyers and sellers can communicate via a\ncommunication channel, which is open to both buyers and sellers. The\ncommunication channel is open only to the current buyer or seller, and\nclosed to the previous buyer or seller. The buyer or seller can communicate\ndirectly to the seller or directly to the buyer. We study the price-setting\nstrategies of buyers and sellers, and analyze the effect of communication\nprivacy on price formation. We derive a general price-setting strategy that\ndepends only on the current buyer and the previous buyer. We also consider\ntwo-sided markets and extend the analysis to the two-sided case. We also\npropose a novel market design that includes a third seller, who is only\ninterested in the price of the product, but not in the product itself. We\nderive a price-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16831683168316833,
          "p": 0.2537313432835821,
          "f": 0.2023809475857428
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.02631578947368421,
          "f": 0.023809518854876314
        },
        "rouge-l": {
          "r": 0.16831683168316833,
          "p": 0.2537313432835821,
          "f": 0.2023809475857428
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2412.18563v3",
      "true_abstract": "Artificial intelligence is transforming financial investment decision-making\nframeworks, with deep reinforcement learning demonstrating substantial\npotential in robo-advisory applications. This paper addresses the limitations\nof traditional portfolio optimization methods in dynamic asset weight\nadjustment through the development of a deep reinforcement learning-based\ndynamic optimization model grounded in practical trading processes. The\nresearch advances two key innovations: first, the introduction of a novel\nSharpe ratio reward function engineered for Actor-Critic deep reinforcement\nlearning algorithms, which ensures stable convergence during training while\nconsistently achieving positive average Sharpe ratios; second, the development\nof an innovative comprehensive approach to portfolio optimization utilizing\ndeep reinforcement learning, which significantly enhances model optimization\ncapability through the integration of random sampling strategies during\ntraining with image-based deep neural network architectures for\nmulti-dimensional financial time series data processing, average Sharpe ratio\nreward functions, and deep reinforcement learning algorithms. The empirical\nanalysis validates the model using randomly selected constituent stocks from\nthe CSI 300 Index, benchmarking against established financial econometric\noptimization models. Backtesting results demonstrate the model's efficacy in\noptimizing portfolio allocation and mitigating investment risk, yielding\nsuperior comprehensive performance metrics.",
      "generated_abstract": "This paper studies the dynamic portfolio allocation problem with the\nportfolio optimization objective. We first derive a linear programming (LP)\noptimal solution for the problem when the portfolio optimization objective\nincludes the risk-free rate. We then consider a general nonlinear portfolio\noptimization objective. We present a linear programming (LP) solution for this\nnonlinear portfolio optimization problem. In addition, we propose a new\nalgorithm to solve the portfolio optimization problem under nonlinear portfolio\noptimization objective, which is an iterative scheme. We show that the\niterative algorithm converges to a unique optimal portfolio. We also study the\nportfolio allocation problem with the nonlinear portfolio optimization\nobjective under the stochastic risk assumption. We propose a stochastic\nalgorithm to solve the portfolio optimization problem with the nonlinear\nportfolio optimization objective. We show that the stochastic algorithm\nconverges to a unique optimal portfolio.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11023622047244094,
          "p": 0.2545454545454545,
          "f": 0.15384614962866813
        },
        "rouge-2": {
          "r": 0.018292682926829267,
          "p": 0.034482758620689655,
          "f": 0.02390437794066846
        },
        "rouge-l": {
          "r": 0.09448818897637795,
          "p": 0.21818181818181817,
          "f": 0.13186812765064618
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/ST/2412.04263v1",
      "true_abstract": "A simple model-free and distribution-free statistic, the functional\nrelationship between the number of \"effective\" degrees of freedom and portfolio\nsize, or N*(N), is used to discriminate between two alternative models for the\ncorrelation of daily cryptocurrency returns within a retail universe of defined\nby the list of tradable assets available to account holders at the Robinhood\nbrokerage. The average pairwise correlation between daily cryptocurrency\nreturns is found to be high (of order 60%) and the data collected supports\ndescription of the cross-section of returns by a simple isotropic correlation\nmodel distinct from a decomposition into a linear factor model with additive\nnoise with high confidence. This description appears to be relatively stable\nthrough time.",
      "generated_abstract": "uce a novel approach to the design of hedging strategies in the\nmarket for American options. The hedging strategy is a combination of an\narbitrage-free strategy and an option-hedging strategy. The arbitrage-free\nstrategy is a risk-free arbitrage that hedges the underlying option against a\nvolatility risk measure. The option-hedging strategy is a risk-averse hedging\nstrategy that hedges the underlying option against the volatility risk\nmeasure. We show that the hedging strategy is equivalent to a hedging strategy\nfor the underlying option. We propose a new hedging strategy that uses a\nmultiplicative volatility risk measure, i.e., we hedge the underlying option\nagainst a multiplicative volatility risk measure. We show that the hedging\nstrategy is equivalent to a hedging strategy for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10256410256410256,
          "p": 0.1702127659574468,
          "f": 0.12799999530752015
        },
        "rouge-2": {
          "r": 0.00909090909090909,
          "p": 0.012987012987012988,
          "f": 0.010695182321486937
        },
        "rouge-l": {
          "r": 0.07692307692307693,
          "p": 0.1276595744680851,
          "f": 0.09599999530752024
        }
      }
    },
    {
      "paper_id": "stat.ML.q-fin/ST/2502.11310v1",
      "true_abstract": "We tackle the challenges of modeling high-dimensional data sets, particularly\nthose with latent low-dimensional structures hidden within complex, non-linear,\nand noisy relationships. Our approach enables a seamless integration of\nconcepts from non-parametric regression, factor models, and neural networks for\nhigh-dimensional regression. Our approach introduces PCA and Soft PCA layers,\nwhich can be embedded at any stage of a neural network architecture, allowing\nthe model to alternate between factor modeling and non-linear transformations.\nThis flexibility makes our method especially effective for processing\nhierarchical compositional data. We explore ours and other techniques for\nimposing low-rank structures on neural networks and examine how architectural\ndesign impacts model performance. The effectiveness of our method is\ndemonstrated through simulation studies, as well as applications to forecasting\nfuture price movements of equity ETF indices and nowcasting with macroeconomic\ndata.",
      "generated_abstract": "We consider the problem of estimating the mean and variance of the\ndistribution of a large set of data points. For a large number of data\npoints, the mean and variance are known, and we are interested in estimating\nthem accurately. We propose a novel data-driven method for this problem that\nuses a randomized convex optimization algorithm. We demonstrate that our method\nis competitive in terms of estimation accuracy, and it outperforms existing\nmethods when the number of data points is moderate. We also show that our\nmethod can be applied to other problems in the statistics and machine learning\nliterature. Our work paves the way for future research in this area.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16,
          "p": 0.2318840579710145,
          "f": 0.18934910759427204
        },
        "rouge-2": {
          "r": 0.031007751937984496,
          "p": 0.039603960396039604,
          "f": 0.03478260376975495
        },
        "rouge-l": {
          "r": 0.16,
          "p": 0.2318840579710145,
          "f": 0.18934910759427204
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.07527v1",
      "true_abstract": "This paper presents a novel method for real-time lifting-load estimation to\nenhance the control strategies of upper-limb assistive exoskeletons. By\nleveraging cost-effective insole pressure sensors, the proposed system extracts\ndifferential pressure data that minimizes disturbances from variations in body\nweight and sensor placement. Two modeling approaches are explored: a\nchannel-based method that employs traditional regression techniques-Elastic\nNet, Support Vector Regression (SVR), and Multi-Layer Perceptron (MLP)-and a\nmap-based method that utilizes transfer learning with a pre-trained MobileNetV2\nmodel. The experiment is in the preliminary test stage, covering load ranges\nfrom 2 kg to 10 kg in increments of 0.5 kg, and collecting data from three\nsubjects to test the approach. In the Channel-based method, the average\nWeighted Mean Absolute Percentage Error(WMAPE) for three subjects showed that\nthe SVR achieved 13.46%, with the MLP performing similarly. In the Map-based\nmethod, using data from one subject, the Fully Fine-Tuned MobileNetV2 model\nreached a WMAPE of 9.74%. The results indicate that the integration of insole\nsensor technology with advanced machine learning models provides an effective\nsolution for dynamic load estimation, potentially reducing the risks of over-\nand under-compensation in exoskeleton control.",
      "generated_abstract": "t of artificial intelligence (AI) has revolutionized the automation\nof complex industrial processes. However, the control of AI-driven systems is\noften challenging due to their complex behavior and the limited availability of\nwell-defined control laws. To address these limitations, we introduce a\ndistributed control framework that leverages the distributed knowledge of\nmultiple autonomous agents to optimize control strategies. The framework\nincorporates the concept of distributed knowledge as a means of reducing\ncommunication overhead. Our approach aims to enable distributed control in\ndistributed AI systems by leveraging knowledge from multiple autonomous agents.\nThis knowledge is distributed across the system and is used to inform control\nstrategies. We demonstrate the effectiveness of our approach through two\nsimulation examples, where we compare the performance of our approach with\nexisting methods. We show that our approach achieves significant gains in\nperformance compared to existing methods,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13533834586466165,
          "p": 0.20689655172413793,
          "f": 0.16363635885495884
        },
        "rouge-2": {
          "r": 0.01092896174863388,
          "p": 0.015151515151515152,
          "f": 0.012698407829480325
        },
        "rouge-l": {
          "r": 0.12781954887218044,
          "p": 0.19540229885057472,
          "f": 0.1545454497640497
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09919v1",
      "true_abstract": "We provide a family of $5$-dimensional prismatoids whose width grows linearly\nin the number of vertices. This provides a new infinite family of\ncounter-examples to the Hirsch conjecture whose excess width grows linearly in\nthe number of vertices, and answers a question of Matschke, Santos and Weibel.",
      "generated_abstract": "We prove that the group of automorphisms of the compact Lie group $G$ acting\ntransitively on itself is a semidirect product of the group of automorphisms\nof the orbit space of $G$ and a group of automorphisms of the quotient space\n$G/G'$. We show that the orbit space of $G$ is homeomorphic to a compact\nproduct of the group of automorphisms of the circle and the group of\nautomorphisms of the torus. We prove that $G/G'$ is homeomorphic to a\n$2$-dimensional compact orientable surface of genus $1$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1935483870967742,
          "p": 0.18181818181818182,
          "f": 0.18749999500488296
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1935483870967742,
          "p": 0.18181818181818182,
          "f": 0.18749999500488296
        }
      }
    },
    {
      "paper_id": "hep-ex.physics/data-an/2503.06727v1",
      "true_abstract": "The Precision Reactor Oscillation and Spectrum Experiment, PROSPECT, was a\nsegmented antineutrino detector that successfully operated at the High Flux\nIsotope Reactor in Oak Ridge, TN, during its 2018 run. Despite challenges with\nphotomultiplier tube base failures affecting some segments, innovative machine\nlearning approaches were employed to perform position and energy\nreconstruction, and particle classification. This work highlights the\neffectiveness of convolutional neural networks and graph convolutional networks\nin enhancing data analysis. By leveraging these techniques, a 3.3% increase in\neffective statistics was achieved compared to traditional methods, showcasing\ntheir potential to improve performance. Furthermore, these machine learning\nmethodologies offer promising applications for other segmented particle\ndetectors, underscoring their versatility and impact.",
      "generated_abstract": "experiment, with its sensitivity to the time-dependent decay of\nB mesons into a $\\Lambda\\bar{\\Lambda}$ system, has the potential to provide\nprecise measurements of the decay branching fractions of B mesons in the\n$\\Lambda\\bar{\\Lambda}$ final state. We present a detailed analysis of the\nBelle-II $\\Lambda\\bar{\\Lambda}$ data sample used by LHCb, including\nreconstructed B meson candidates, their decay branching fractions, and\nsystematic uncertainties. We find that the LHCb-like sensitivity to\n$\\mathcal{B}(\\Lambda\\bar{\\Lambda})_{B\\to\\Lambda\\bar{\\Lambda}}$ can be as\nhigh as $1.6\\%$, which is comparable to the LHCb-like sensitivity to\n$\\mathcal{B}(\\Lambda\\bar{\\Lambda})_{B\\to K\\bar{K",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13978494623655913,
          "p": 0.22033898305084745,
          "f": 0.17105262682912065
        },
        "rouge-2": {
          "r": 0.009009009009009009,
          "p": 0.013157894736842105,
          "f": 0.010695182340932714
        },
        "rouge-l": {
          "r": 0.11827956989247312,
          "p": 0.1864406779661017,
          "f": 0.14473683735543644
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.09794v1",
      "true_abstract": "As Augmented Reality (AR) and Artificial Intelligence (AI) continue to\nconverge, new opportunities emerge for AI agents to actively support human\ncollaboration in immersive environments. While prior research has primarily\nfocused on dyadic human-AI interactions, less attention has been given to\nHuman-AI Teams (HATs) in AR, where AI acts as an adaptive teammate rather than\na static tool. This position paper takes the perspective of team dynamics and\nwork organization to propose that AI agents in AR should not only interact with\nindividuals but also recognize and respond to team-level needs in real time. We\nargue that spatially aware AI agents should dynamically generate the resources\nnecessary for effective collaboration, such as virtual blackboards for\nbrainstorming, mental map models for shared understanding, and memory recall of\nspatial configurations to enhance knowledge retention and task coordination.\nThis approach moves beyond predefined AI assistance toward context-driven AI\ninterventions that optimize team performance and decision-making.",
      "generated_abstract": "In this work, we introduce a novel method to predict the behavior of a\nfinancial market in the presence of news events. We propose a model that combines\na deep learning approach with an ensemble of simple models to provide a\nmore accurate forecast. Our approach combines a deep learning model to predict\nthe probability of positive or negative changes in the price of a financial\ninstrument, with a simple model to predict the probability of a news event\noccurring in the next 24 hours. Our results show that our method is more\naccurate than simple models like ARIMA and Naive Bayes, achieving an accuracy of\n85.3% on the 1-year rolling window. The model is able to capture the dynamics\nof the financial markets and predict the next day's trend with high accuracy.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13333333333333333,
          "p": 0.21333333333333335,
          "f": 0.16410255936883644
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.21333333333333335,
          "f": 0.16410255936883644
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.cond-mat/soft/2503.09564v1",
      "true_abstract": "Under an externally applied load, granular packings form force chains that\ndepend on the contact network and moduli of the grains. In this work, we\ninvestigate packings of variable modulus (VM) particles, where we can direct\nforce chains by changing the Young's modulus of individual particles within the\npacking on demand. Each VM particle is made of a silicone shell that\nencapsulates a core made of a low-melting-point metallic alloy (Field's metal).\nBy sending an electric current through a co-located copper heater, the Field's\nmetal internal to each particle can be melted via Joule heating, which softens\nthe particle. As the particle cools to room temperature, the alloy solidifies\nand the particle recovers its original modulus. To optimize the mechanical\nresponse of granular packings containing both soft and stiff particles, we\nemploy an evolutionary algorithm coupled with discrete element method\nsimulations to predict the patterns of particle moduli that will yield specific\nforce outputs on the assembly boundaries. The predicted patterns of particle\nmoduli from the simulations were realized in experiments using 2D assemblies of\nVM particles and the force outputs on the assembly boundaries were measured\nusing photoelastic techniques. These studies represent a step towards making\nrobotic granular metamaterials that can dynamically adapt their mechanical\nproperties in response to different environmental conditions or perform\nspecific tasks on demand.",
      "generated_abstract": "t a theoretical model for the coarsening of non-equilibrium\nsurfaces of non-equilibrium systems, with particular emphasis on the\ncoarsening of rough surfaces. We focus on the case of a system of Brownian\nparticles in an inertial potential. We derive a general form for the\ncoarsening rate of rough surfaces, and show that the roughness exponent is\nrelated to the inverse temperature of the inertial potential. We also present\na numerical study of rough surfaces in a one-dimensional random walker model.\nWe find that the roughness exponent is strongly dependent on the inverse\ntemperature, and that the roughness exponent exhibits a rich phase diagram,\nwith a crossover to the random walker model at low temperatures, and an\ninstability at high temperatures. We also study the coarsening of rough\nsurfaces in a random walker model with a mean-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10071942446043165,
          "p": 0.21875,
          "f": 0.1379310301652553
        },
        "rouge-2": {
          "r": 0.014925373134328358,
          "p": 0.029411764705882353,
          "f": 0.01980197573179201
        },
        "rouge-l": {
          "r": 0.08633093525179857,
          "p": 0.1875,
          "f": 0.11822659666771837
        }
      }
    },
    {
      "paper_id": "cs.GT.stat/OT/2502.11645v1",
      "true_abstract": "Many real-world multi-agent or multi-task evaluation scenarios can be\nnaturally modelled as normal-form games due to inherent strategic (adversarial,\ncooperative, and mixed motive) interactions. These strategic interactions may\nbe agentic (e.g. players trying to win), fundamental (e.g. cost vs quality), or\ncomplementary (e.g. niche finding and specialization). In such a formulation,\nit is the strategies (actions, policies, agents, models, tasks, prompts, etc.)\nthat are rated. However, the rating problem is complicated by redundancy and\ncomplexity of N-player strategic interactions. Repeated or similar strategies\ncan distort ratings for those that counter or complement them. Previous work\nproposed ``clone invariant'' ratings to handle such redundancies, but this was\nlimited to two-player zero-sum (i.e. strictly competitive) interactions. This\nwork introduces the first N-player general-sum clone invariant rating, called\ndeviation ratings, based on coarse correlated equilibria. The rating is\nexplored on several domains including LLMs evaluation.",
      "generated_abstract": "r introduces the Splitting Method for Markov Decision Processes,\na novel algorithm for sampling from a general class of Markov decision processes.\nSpecifically, we consider the problem of learning the optimal policy from a\nMarkov decision process that is a subgraph of a graph of Markov decision\nprocesses. We show that the optimal policy can be found by solving a\nconstrained optimization problem. However, in the case of the Markov\ndecision process being a subgraph of a graph of Markov decision processes, the\nconstrained optimization problem is non-convex. To solve the problem, we\nintroduce the Splitting Method, which is a novel splitting-and-merge procedure\nthat allows the optimization problem to be solved efficiently. The\nsplitting-and-merge procedure is shown to be equivalent to the standard\nalternating-direction method of multipliers, and is implemented by a\nparallel-gradient algorithm",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.22058823529411764,
          "f": 0.16393442155931812
        },
        "rouge-2": {
          "r": 0.02097902097902098,
          "p": 0.027522935779816515,
          "f": 0.02380951890054271
        },
        "rouge-l": {
          "r": 0.13043478260869565,
          "p": 0.22058823529411764,
          "f": 0.16393442155931812
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/NI/2503.06785v1",
      "true_abstract": "As reliance on space systems continues to increase, so does the need to\nensure security for them. However, public work in space standards have\nstruggled with defining security protocols that are well tailored to the domain\nand its risks. In this work, we investigate various space networking paradigms\nand security approaches, and identify trade-offs and gaps. Furthermore, we\ndescribe potential existing security protocol approaches that fit well into the\nspace network paradigm in terms of both functionality and security. Finally, we\nestablish future directions for enabling strong security for space\ncommunication.",
      "generated_abstract": "t a new framework for modelling and analyzing the dynamic\nchallenges posed by online social networks, particularly in the context of\nnetworked conflict. The framework is based on a novel notion of networked\nconflict, which we call \"conflict space\" and which allows us to characterize\nand analyze the interconnectedness of different types of social conflicts. We\napply our framework to the analysis of the 2014-2018 conflict in Syria, using\nthe \"conflict space\" approach to model and analyze the dynamic nature of the\nconflict. Our findings show that the conflict in Syria is characterized by a\ncomplex, multi-dimensional dynamic with multiple layers of interconnectedness\nthat create a complex web of social conflicts and social networks, and\nresult in a complex networked conflict space. We also identify key\ncharacteristics of this complex networked conflict space, including the\npresence of a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19402985074626866,
          "p": 0.16883116883116883,
          "f": 0.18055555057966835
        },
        "rouge-2": {
          "r": 0.011363636363636364,
          "p": 0.00819672131147541,
          "f": 0.009523804654877774
        },
        "rouge-l": {
          "r": 0.1791044776119403,
          "p": 0.15584415584415584,
          "f": 0.16666666169077946
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.01298v2",
      "true_abstract": "Measuring inequality of opportunities has long been a challenging and open\nproblem, primarily due to the limitations associated with individual-level\ndata. In this study, we utilize data obtained from vehicle license plates in a\ncomprehensive survey (17258 vehicles from 6 major cities in China) to evaluate\nthe inequality of opportunities in the country. In our context, we define\ninequality of opportunity as the scenario where relatively expensive vehicles\nhave a higher likelihood of being paired with license plates featuring 'Lucky\nNumbers'. To quantify this, we propose a lucky-number-based opportunity Gini\ncoefficient. Through the calculation of the opportunity Gini coefficient, we\nobserve a significant and positive correlation between opportunity inequality\nand income inequality. Particularly noteworthy is our finding that the\nadvancement of technology, exemplified by the widespread adoption of new energy\nvehicles, can substantially reduce the inequality of opportunity. Taking\nincorporation of a random lottery process before acquiring a motor vehicle in\nBeijing and Shanghai as a natural experiment, our empirical results support the\nargument that, in terms of equality, employing random drawing is a fair and\nequitable approach for allocating scarce resources.",
      "generated_abstract": "r develops a novel framework for analyzing the long-run impact of\nsignificant public investments. We introduce a novel measure of\nstagnation-to-decline, defined as the difference between a government's\nbudget deficit and its GDP, which we term the \"stagnation-to-decline\nindex.\" We then develop a model for the long-run impact of these\ninvestments. We find that a government's stagnation-to-decline index\nsignificantly impacts its long-run growth rate. For example, if the\ngovernment's stagnation-to-decline index is 0.75, its growth rate drops by\n0.17 percentage points. We also find that a government's stagnation-to-decline\nindex impacts its long-run debt-to-GDP ratio. For example,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1,
          "p": 0.2033898305084746,
          "f": 0.13407820787116523
        },
        "rouge-2": {
          "r": 0.005780346820809248,
          "p": 0.012345679012345678,
          "f": 0.007874011403995204
        },
        "rouge-l": {
          "r": 0.08333333333333333,
          "p": 0.1694915254237288,
          "f": 0.11173183915608144
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2410.01378v1",
      "true_abstract": "This paper studies robust forward investment and consumption preferences and\noptimal strategies for a risk-averse and ambiguity-averse agent in an\nincomplete financial market with drift and volatility uncertainties. We focus\non non-zero volatility and constant relative risk aversion (CRRA) forward\npreferences. Given the non-convexity of the Hamiltonian with respect to\nuncertain volatilities, we first construct robust randomized forward\npreferences through endogenous randomization in an auxiliary market. We derive\nthe corresponding optimal and robust investment and consumption strategies.\nFurthermore, we show that such forward preferences and strategies, developed in\nthe auxiliary market, remain optimal and robust in the physical market,\noffering a comprehensive framework for forward investment and consumption under\nmodel uncertainty.",
      "generated_abstract": "uce a novel framework for multi-period, multi-asset portfolio\nselection and optimization. We develop an extension of the portfolio selection\nmodel with incomplete information (PII) to incorporate a single portfolio\nwithin a portfolio of multiple assets. We establish a duality relationship\nbetween the optimization problem of minimizing a portfolio's expected\nutility and the optimization problem of finding the optimal portfolio. We also\nderive a closed-form solution for the optimal portfolio in the case where the\nutility function is linear. We then study the case where the utility function\nis nonlinear and derive the optimal utility function and the corresponding\noptimal portfolio. We show that the optimal utility function is non-smooth\nand non-convex and further develop a smoothened utility function and a\nsmoothened optimization problem. We also derive the optimal policy of\nmanipulating the utility function to obtain a desired distribution of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2647058823529412,
          "p": 0.28125,
          "f": 0.2727272677318642
        },
        "rouge-2": {
          "r": 0.07142857142857142,
          "p": 0.06481481481481481,
          "f": 0.06796116006032651
        },
        "rouge-l": {
          "r": 0.23529411764705882,
          "p": 0.25,
          "f": 0.2424242374288339
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2412.00986v1",
      "true_abstract": "We study a problem of optimal irreversible investment and emission reduction\nformulated as a nonzero-sum dynamic game between an investor with environmental\npreferences and a firm. The game is set in continuous time on an infinite-time\nhorizon. The firm generates profits with a stochastic dynamics and may spend\npart of its revenues towards emission reduction (e.g., renovating the\ninfrastructure). The firm's objective is to maximize the discounted expectation\nof a function of its profits. The investor participates in the profits and may\ndecide to invest to support the firm's production capacity. The investor uses a\nprofit function which accounts for both financial and environmental factors.\nNash equilibria of the game are obtained via a system of variational\ninequalities. We formulate a general verification theorem for this system in a\ndiffusive setup and construct an explicit solution in the zero-noise limit. Our\nexplicit results and numerical approximations show that both the investor's and\nthe firm's optimal actions are triggered by moving boundaries that increase\nwith the total amount of emission abatement.",
      "generated_abstract": "This paper studies the problem of optimal portfolio selection with time-varying\nportfolio constraints. We consider a model in which the stock price fluctuates\nover a finite time horizon and the investor has a limited budget for holding\nthe stock. The budget constraint is time-varying and it is dependent on the\nprice fluctuation. In this paper, we first study the case when the budget\nconstraint is independent of the price fluctuation. We then consider the case\nwhen the budget constraint is time-varying. We first establish a necessary and\nsufficient condition for the optimal budget constraint to be time-varying. We\nthen derive a dynamic programming equation for computing the optimal budget\nconstraint. We then use this equation to derive a dynamic programming\nequation for computing the optimal portfolio selection. Our results extend\nprevious results in the literature on optimal budget constraints.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21904761904761905,
          "p": 0.3484848484848485,
          "f": 0.26900584321329646
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.046296296296296294,
          "f": 0.036630031848006096
        },
        "rouge-l": {
          "r": 0.20952380952380953,
          "p": 0.3333333333333333,
          "f": 0.257309936780548
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2404.07658v1",
      "true_abstract": "This paper extends the valuation and optimal surrender framework for variable\nannuities with guaranteed minimum benefits in a L\\'evy equity market\nenvironment by incorporating a stochastic interest rate described by the\nHull-White model. This approach frames a more dynamic and realistic financial\nsetting compared to previous literature. We exploit a robust valuation\nmechanism employing a hybrid numerical method that merges tree methods for\ninterest rate modeling with finite difference techniques for the underlying\nasset price. This method is particularly effective for addressing the\ncomplexities of variable annuities, where periodic fees and mortality risks are\nsignificant factors. Our findings reveal the influence of stochastic interest\nrates on the strategic decision-making process concerning the surrender of\nthese financial instruments. Through comprehensive numerical experiments, and\nby comparing our results with those obtained through the Longstaff-Schwartz\nMonte Carlo method, we illustrate how our refined model can guide insurers in\ndesigning contracts that equitably balance the interests of both parties. This\nis particularly relevant in discouraging premature surrenders while adapting to\nthe realistic fluctuations of financial markets. Lastly, a comparative statics\nanalysis with varying interest rate parameters underscores the impact of\ninterest rates on the cost of the optimal surrender strategy, emphasizing the\nimportance of accurately modeling stochastic interest rates.",
      "generated_abstract": "the portfolio selection problem for a family of time-dependent\nportfolios and a set of fixed portfolios. In this setup, we consider a\nportfolio selection model with the optimal consumption function, which is\ncharacterized by a functional of the expected utility. We assume that the\nutility is convex in time. The utility is convex in the expected utility and\nconcave in the payoff function. The payoff function is convex in the expected\nutility. We provide an analytical solution for the optimal portfolio selection\nproblem, and obtain the explicit expression for the optimal portfolio\nselection problem. We study the portfolio selection problem for a family of\ntime-dependent portfolios and a set of fixed portfolios. We consider a\nportfolio selection model with the optimal consumption function, which is\ncharacterized by a functional of the expected utility. We assume that the\nutility is convex in time. The utility is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1037037037037037,
          "p": 0.2857142857142857,
          "f": 0.15217390913575152
        },
        "rouge-2": {
          "r": 0.015463917525773196,
          "p": 0.03896103896103896,
          "f": 0.022140217334187372
        },
        "rouge-l": {
          "r": 0.1037037037037037,
          "p": 0.2857142857142857,
          "f": 0.15217390913575152
        }
      }
    },
    {
      "paper_id": "math.QA.math/OA/2502.19876v3",
      "true_abstract": "This paper explores Frobenius subalgebra posets within abelian monoidal\ncategories and shows that they form lattices under certain conditions,\nincluding all semisimple tensor categories. Furthermore, it extends Watatani's\ntheorem on the finiteness of intermediate subfactors, proving that these\nlattices are finite under weak positivity constraints, encompassing all\nsemisimple tensor categories as well. The primary tools employed in this paper\nare semisimplification and a concept of formal angle. Additionally, we have\nbroadened several intermediate results, such as the exchange relation theorem\nand Landau's theorem, to apply to abelian monoidal categories. Key applications\nof our findings include a stronger version of the Ino-Watatani result: we show\nthat the finiteness of intermediate C*-algebras holds in a finite-index unital\nirreducible inclusion of C*-algebras without requiring the simple assumption.\nMoreover, for a finite-dimensional semisimple Hopf algebra H, we demonstrate\nthat H* contains a finite number of Frobenius subalgebra objects in Rep(H).\nFinally, we explore a range of applications, including abstract spin chains,\nvertex operator algebras, and speculations on quantum arithmetic involving the\ngeneralization of Ore's theorem, Euler's totient and sigma functions, and RH.",
      "generated_abstract": "We present a new approach to the construction of certain spaces of\nconstrained functions, which are in many cases different from those\nconstructed in the literature. We use this approach to study the spaces of\nfunctions that are the sums of two functions that are in some sense more\nconstrained than the original functions. In particular, we consider the spaces\nof functions that are the sums of functions that are in some sense more\nconstrained than a given function. We show that these spaces are in many cases\ndifferent from those constructed in the literature.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10743801652892562,
          "p": 0.3023255813953488,
          "f": 0.15853658149687697
        },
        "rouge-2": {
          "r": 0.011904761904761904,
          "p": 0.03225806451612903,
          "f": 0.017391300409830758
        },
        "rouge-l": {
          "r": 0.10743801652892562,
          "p": 0.3023255813953488,
          "f": 0.15853658149687697
        }
      }
    },
    {
      "paper_id": "cs.GL.cs/GL/2403.05592v1",
      "true_abstract": "As we keep rapidly advancing toward an era where artificial intelligence is a\nconstant and normative experience for most of us, we must also be aware of what\nthis vision and this progress entail. By first approximating neural connections\nand activities in computer circuits and then creating more and more\nsophisticated versions of this crude approximation, we are now facing an age to\ncome where modern deep learning-based artificial intelligence systems can\nrightly be called thinking machines, and they are sometimes even lauded for\ntheir emergent behavior and black-box approaches. But as we create more\npowerful electronic brains, with billions of neural connections and parameters,\ncan we guarantee that these mammoths built of artificial neurons will be able\nto forget the data that we store in them? If they are at some level like a\nbrain, can the right to be forgotten still be protected while dealing with\nthese AIs? The essential gap between machine learning and the RTBF is explored\nin this article, with a premonition of far-reaching conclusions if the gap is\nnot bridged or reconciled any time soon. The core argument is that deep\nlearning models, due to their structure and size, cannot be expected to forget\nor delete a data as it would be expected from a tabular database, and they\nshould be treated more like a mechanical brain, albeit still in development.",
      "generated_abstract": "This paper explores the potential of artificial intelligence (AI) to\nimprove the performance of large language models (LLMs). Specifically, we\nconsider the use of LLMs in the task of text-to-image generation. We propose a\nframework that uses an LLM as a model-agnostic layer in a generative model,\nwhere the LLM serves as a \"black box\" for image generation. We compare the\nperformance of this architecture with a traditional method that employs a LLM\nas an intermediate layer between the text generator and the final image\nrepresentation. The results demonstrate that using an LLM in this way improves\nthe quality and accuracy of generated images, highlighting the potential of\nthis approach for improving LLM performance in other domains.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.2465753424657534,
          "f": 0.16589861304678388
        },
        "rouge-2": {
          "r": 0.027522935779816515,
          "p": 0.056074766355140186,
          "f": 0.03692307250632006
        },
        "rouge-l": {
          "r": 0.11805555555555555,
          "p": 0.2328767123287671,
          "f": 0.15668202318503274
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2502.10631v1",
      "true_abstract": "Large Language Models (LLMs) employ three popular training approaches: Masked\nLanguage Models (MLM), Causal Language Models (CLM), and Sequence-to-Sequence\nModels (seq2seq). However, each approach has its strengths and limitations, and\nfaces challenges in addressing specific tasks that require controllable and\nbidirectional generation, such as drug optimization. To address this challenge,\ninspired by the biological processes of growth and evolution, which involve the\nexpansion, shrinking, and mutation of sequences, we introduce ControllableGPT.\nThis initiative represents the first effort to combine the advantages of MLM,\nCLM, and seq2seq into a single unified, controllable GPT framework. It enables\nthe precise management of specific locations and ranges within a sequence,\nallowing for expansion, reduction, or mutation over chosen or random lengths,\nwhile maintaining the integrity of any specified positions or subsequences. In\nthis work, we designed ControllableGPT for drug optimization from the ground\nup, which included proposing the Causally Masked Seq2seq (CMS) objective,\ndeveloping the training corpus, introducing a novel pre-training approach, and\ndevising a unique generation process. We demonstrate the effectiveness and\ncontrollability of ControllableGPT by conducting experiments on drug\noptimization tasks for both viral and cancer benchmarks, surpassing competing\nbaselines.",
      "generated_abstract": "t of next-generation sequencing technologies has transformed\nthe study of gene expression into a highly quantitative field. The\nimproved resolution of sequencing technologies has enabled the analysis of\ntranscriptomes at the individual nucleotide level. This information has led to\nthe discovery of thousands of genes and their regulatory interactions,\nsignificantly expanding the field of gene regulatory network (GRN) modeling.\nHowever, it is challenging to interpret and visualize these complex networks\nwithout the aid of a computational framework. In this paper, we introduce\nGRNet, a novel deep learning-based approach for analyzing GRNs. GRNet is\nbased on a novel Transformer-based architecture, which enables efficient\ncomputation while maintaining high performance. This architecture incorporates\na novel attention mechanism to enhance the model's ability to capture the\ninterdependencies between nodes. By leveraging this attention mechanism",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16296296296296298,
          "p": 0.23404255319148937,
          "f": 0.19213973315154187
        },
        "rouge-2": {
          "r": 0.0273224043715847,
          "p": 0.04065040650406504,
          "f": 0.032679733754325965
        },
        "rouge-l": {
          "r": 0.15555555555555556,
          "p": 0.22340425531914893,
          "f": 0.1834061086973934
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/DB/2503.08717v1",
      "true_abstract": "The ability of tracing states of logistic transportations requires an\nefficient storage and retrieval of the state of logistic transportations and\nlocations of logistic objects. However, the restriction of sharing states and\nlocations of logistic objects across organizations from different countries\nmakes it hard to deploy a centralized database for implementing the\ntraceability in a cross-border logistic system. This paper proposes a semantic\ndata model on Blockchain to represent a logistic process based on the Semantic\nLink Network model where each semantic link represents a logistic\ntransportation of a logistic object between two parties. A state representation\nmodel is designed to represent the states of a logistic transportation with\nsemantic links. It enables the locations of logistic objects to be derived from\nthe link states. A mapping from the semantic links to the blockchain\ntransactions is designed to enable schema of semantic links and states of\nsemantic links to be published in blockchain transactions. To improve the\nefficiency of tracing a path of semantic links on blockchain platform, an\nalgorithm is designed to build shortcuts along the path of semantic links to\nenable a query on the path of a logistic object to reach the target in\nlogarithmic steps on the blockchain platform. A reward-penalty policy is\ndesigned to allow participants to confirm the state of links on blockchain.\nAnalysis and simulation demonstrate the flexibility, effectiveness and the\nefficiency of Semantic Link Network on immutable blockchain for implementing\nlogistic traceability.",
      "generated_abstract": "This paper presents a methodology for the analysis of the impact of\ntransactions in a transactional database. The methodology is based on the\napproach of the transactional database model (TDMM), but it is not limited to\nTDMM. The proposed methodology consists of three steps. In the first step, the\ntransactions of a database are described, and in the second step, the\nrelationship between the transactions and the database is established. In the\nthird step, the results of the analysis are presented. In this paper, the\ntransactions are described according to the principles of TDMM, but the\nrelationship between the transactions and the database is established according\nto the principles of the database model. The analysis is performed in\npseudocode. The results of the analysis are presented in the form of\nstatements. The results of the analysis are used for the analysis of the\nimpact of transactions on the database.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18269230769230768,
          "p": 0.34545454545454546,
          "f": 0.2389937061666865
        },
        "rouge-2": {
          "r": 0.04395604395604396,
          "p": 0.07766990291262135,
          "f": 0.05614034626137311
        },
        "rouge-l": {
          "r": 0.17307692307692307,
          "p": 0.32727272727272727,
          "f": 0.2264150898144853
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.07896v2",
      "true_abstract": "The effect of a negative sectoral shock on GDP depends on how important the\nshocked sector is as a direct and indirect supplier and how easily sectors can\nsubstitute inputs. Past estimates of the parameters that determine these\nqualities in the US have been restrictive: they have not been allowed to vary\nacross industries or across time. This paper uses a novel empirical strategy to\nrelax those restrictions, by exploiting variation in input expenditure share\nshifts within industries rather than across industries. The resulting estimates\nexhibit significant sectoral and temporal heterogeneity, and are dynamically\ncorrelated with weighted patents. In a calibrated GE model of multi-sector\nproduction, this heterogeneity (1) raises[lowers] the GDP effect of negative\nshocks to sectors whose customers are less[more] able to substitute inputs\n(e.g. the GDP effect of \"Chemical products\" shocks rises), (2) raises[lowers]\nthe GDP effect of negative sectoral shocks in years where sectors are\nless[more] able to substitute inputs, and (3) raises[lowers] the GDP effect of\nnegative shocks to sectors as they become more[less] central input suppliers\n(e.g. between 1997 and 2023 the GDP effect of \"Paper products\" shocks fell and\nthe GDP effect of \"Computer and electronic products\" shocks rose due to changes\nin their importance as input suppliers).",
      "generated_abstract": "r presents a novel methodology for identifying economic agents that\ncould be involved in a financial crisis. By utilizing a network approach, we\nexplore the influence of these economic agents on the financial system. Our\napproach enables us to identify the main players and their roles in the\nfinancial system. We use a multi-agent system approach to identify the\neconomic agents and their interactions. We then use the network approach to\nexplore the relationships between these economic agents and identify their\ninfluence on the financial system. We apply this methodology to the financial\ncrisis of 2008 and identify the main economic agents and their roles in the\nfinancial system. We find that these economic agents had a significant impact\non the financial system and that they were also important in causing the\ncrisis. Our methodology can be used to identify the main economic agents and\ntheir roles in financial crises,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.2786885245901639,
          "f": 0.1888888844080248
        },
        "rouge-2": {
          "r": 0.011560693641618497,
          "p": 0.01904761904761905,
          "f": 0.014388484507790986
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.22950819672131148,
          "f": 0.1555555510746915
        }
      }
    },
    {
      "paper_id": "cs.NI.eess/SY/2503.07935v1",
      "true_abstract": "Unmanned aerial vehicles (UAVs) enhance coverage and provide flexible\ndeployment in 5G and next-generation wireless networks. The performance of such\nwireless networks can be improved by developing new navigation and wireless\nadaptation approaches in digital twins (DTs). However, challenges such as\ncomplex propagation conditions and hardware complexities in real-world\nscenarios introduce a realism gap with the DTs. Moreover, while using real-time\nfull-stack protocols in DTs enables subsequent deployment and testing in a\nreal-world environment, development in DTs requires high computational\ncomplexity and involves a long development time. In this paper, to accelerate\nthe development cycle, we develop a measurement-calibrated Matlab-based\nsimulation framework to replicate performance in a full-stack UAV wireless\nnetwork DT. In particular, we use the DT from the NSF AERPAW platform and\ncompare its reports with those generated by our developed simulation framework\nin wireless networks with similar settings. In both environments, we observe\ncomparable results in terms of RSRP measurement, hence motivating iterative use\nof the developed simulation environment with the DT.",
      "generated_abstract": "t of 6G will bring significant advancements in wireless communication\ntechnologies, including massive connectivity, enhanced data rates, and\nintelligent sensing capabilities. As a result, the spectrum efficiency and\nperformance of 6G networks are expected to significantly improve compared to\nthose of 5G networks. However, the challenges posed by the high-frequency band\n(HF) remain significant. The key challenge is to design the optimal network\narchitectures to meet the ever-increasing demands for spectral efficiency and\nperformance. This paper explores the impact of the HF band on network\nperformance, focusing on the trade-off between spectral efficiency and\nperformance. We first introduce a comprehensive framework to evaluate the\nperformance of 6G networks under different scenarios, including the impact of\nthe HF band. Then, we investigate the impact of the HF band on network",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1743119266055046,
          "p": 0.2375,
          "f": 0.20105819617591905
        },
        "rouge-2": {
          "r": 0.03205128205128205,
          "p": 0.04672897196261682,
          "f": 0.03802280886177395
        },
        "rouge-l": {
          "r": 0.1743119266055046,
          "p": 0.2375,
          "f": 0.20105819617591905
        }
      }
    },
    {
      "paper_id": "cs.DS.stat/ML/2503.01766v1",
      "true_abstract": "We provide the first $\\widetilde{\\mathcal{O}}\\left(d\\right)$-sample algorithm\nfor sampling from unbounded Gaussian distributions under the constraint of\n$\\left(\\varepsilon, \\delta\\right)$-differential privacy. This is a quadratic\nimprovement over previous results for the same problem, settling an open\nquestion of Ghazi, Hu, Kumar, and Manurangsi.",
      "generated_abstract": "We present an efficient and scalable algorithm for learning a binary\npath in a tree, which we term the \"binary-tree-based path learning (BTPL)\nalgorithm.\" We first describe the path learning problem and the BTPL algorithm.\nThen, we describe the main complexity results for the BTPL algorithm,\nincluding its computational complexity, query complexity, and approximation\ncomplexity. Finally, we discuss the algorithm's use in real-world applications,\nincluding its use in large-scale data mining, and the challenges it faces in\nthese applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24324324324324326,
          "p": 0.18,
          "f": 0.20689654683577766
        },
        "rouge-2": {
          "r": 0.075,
          "p": 0.0410958904109589,
          "f": 0.05309734055916713
        },
        "rouge-l": {
          "r": 0.21621621621621623,
          "p": 0.16,
          "f": 0.1839080410886512
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/PM/2501.06701v2",
      "true_abstract": "This paper investigates the investment problem of constructing an optimal\nno-short sequential portfolio strategy in a market with a latent dependence\nstructure between asset prices and partly unobservable side information, which\nis often high-dimensional. The results demonstrate that a dynamic strategy,\nwhich forms a portfolio based on perfect knowledge of the dependence structure\nand full market information over time, may not grow at a higher rate infinitely\noften than a constant strategy, which remains invariant over time.\nSpecifically, if the market is stationary, implying that the dependence\nstructure is statistically stable, the growth rate of an optimal dynamic\nstrategy, utilizing the maximum capacity of the entire market information,\nalmost surely decays over time into an equilibrium state, asymptotically\nconverging to the growth rate of a constant strategy.\n  Technically, this work reassesses the common belief that a constant strategy\nonly attains the optimal limiting growth rate of dynamic strategies when the\nmarket process is identically and independently distributed. By analyzing the\ndynamic log-optimal portfolio strategy as the optimal benchmark in a stationary\nmarket with side information, we show that a random optimal constant strategy\nalmost surely exists, even when a limiting growth rate for the dynamic strategy\ndoes not. Consequently, two approaches to learning algorithms for portfolio\nconstruction are discussed, demonstrating the safety of removing side\ninformation from the learning process while still guaranteeing an asymptotic\ngrowth rate comparable to that of the optimal dynamic strategy.",
      "generated_abstract": "r investigates the robustness of portfolio selection and optimization\nunder asymmetric information by considering two scenarios: (i) the initial\ninformation is unknown, and (ii) the investor has incomplete information. The\nincomplete information can be either due to market uncertainty or due to\nmis-specification of the model. We first characterize the performance of the\nportfolio selection problem and optimize the portfolio allocation. We then\nderive the robustness of the optimal portfolio. We show that, when the initial\ninformation is unknown, the robust portfolio is not necessarily unique, but\nthere is a unique robust portfolio for any given initial information. When the\nincomplete information is due to market uncertainty, we show that the robust\nportfolio is unique, but the robust portfolio selection problem is not\nconvex. We then consider the case where the investor has mis-specification of\nthe model. We show that, when the initial",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16,
          "p": 0.29850746268656714,
          "f": 0.20833332878960512
        },
        "rouge-2": {
          "r": 0.035,
          "p": 0.06796116504854369,
          "f": 0.046204615974469176
        },
        "rouge-l": {
          "r": 0.136,
          "p": 0.2537313432835821,
          "f": 0.17708332878960517
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/AS/2503.06805v1",
      "true_abstract": "Emotion recognition and sentiment analysis are pivotal tasks in speech and\nlanguage processing, particularly in real-world scenarios involving\nmulti-party, conversational data. This paper presents a multimodal approach to\ntackle these challenges on a well-known dataset. We propose a system that\nintegrates four key modalities/channels using pre-trained models: RoBERTa for\ntext, Wav2Vec2 for speech, a proposed FacialNet for facial expressions, and a\nCNN+Transformer architecture trained from scratch for video analysis. Feature\nembeddings from each modality are concatenated to form a multimodal vector,\nwhich is then used to predict emotion and sentiment labels. The multimodal\nsystem demonstrates superior performance compared to unimodal approaches,\nachieving an accuracy of 66.36% for emotion recognition and 72.15% for\nsentiment analysis.",
      "generated_abstract": "r introduces a novel approach to enhancing the performance of\nsignature verification systems in the presence of adversarial attacks. We propose\na robust signature verification framework that leverages the power of Deep\nNeural Networks (DNNs) to learn and incorporate diverse patterns of\nsignature patterns. Our framework integrates two main components: (1) a\nmulti-task DNN architecture that integrates signature pattern learning and\nadversarial attack detection, and (2) a data augmentation strategy that\nenhances the robustness of the signature pattern learning process. Our\napproach provides a comprehensive approach for improving signature verification\nsystems against adversarial attacks, offering a novel approach that combines\nDNNs with data augmentation to address the challenges of signature verification\nin the presence of adversarial attacks. The proposed framework is evaluated on\nthe SVC-Net dataset and demonstrates significant improvements in accuracy",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25287356321839083,
          "p": 0.2894736842105263,
          "f": 0.2699386453295195
        },
        "rouge-2": {
          "r": 0.036036036036036036,
          "p": 0.034782608695652174,
          "f": 0.035398225090062574
        },
        "rouge-l": {
          "r": 0.22988505747126436,
          "p": 0.2631578947368421,
          "f": 0.24539876802890598
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/RM/2502.13148v1",
      "true_abstract": "This paper explores key theoretical frameworks instrumental in understanding\nthe relationship between sustainability and institutional investment decisions.\nThe study identifies and analyzes various theories, including Behavioral\nFinance Theory, Modern Portfolio Theory, Risk Management Theory, and others, to\nexplain how sustainability considerations increasingly influence investment\nchoices. By examining these frameworks, the paper highlights how investors\nintegrate Environmental, Social, and Governance (ESG) factors to optimize\nfinancial outcomes and align with broader societal goals.",
      "generated_abstract": "er a multi-agent system with a set of agents that form a portfolio\nand a set of agents that form a market maker. We consider an agent-dependent\nrisk-neutral measure, and the agents' portfolio and market-making strategies\nare dependent on the agent's risk-neutral measure. The risk-neutral measures\nare dependent on the market's price and the agents' risk-neutral measures. We\nstudy the portfolio and market-making strategies of the system. We also study\nthe impact of the risk-neutral measures on the portfolio and market-making\nstrategies of the system. We consider a general model for the risk-neutral\nmeasures and the market's price. We derive the optimal portfolio and market-making\nstrategies of the system. We derive the optimal portfolio and market-making\nstrategies of the system under the risk",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0847457627118644,
          "p": 0.11627906976744186,
          "f": 0.09803921080930436
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0847457627118644,
          "p": 0.11627906976744186,
          "f": 0.09803921080930436
        }
      }
    },
    {
      "paper_id": "cs.AR.cs/AR/2503.07778v1",
      "true_abstract": "The future of artificial intelligence (AI) acceleration demands a paradigm\nshift beyond the limitations of purely electronic or photonic architectures.\nPhotonic analog computing delivers unmatched speed and parallelism but\nstruggles with data movement, robustness, and precision. Electronic\nprocessing-in-memory (PIM) enables energy-efficient computing by co-locating\nstorage and computation but suffers from endurance and reconfiguration\nconstraints, limiting it to static weight mapping. Neither approach alone\nachieves the balance needed for adaptive, efficient AI. To break this impasse,\nwe study a hybrid electronic-photonic-PIM computing architecture and introduce\nH3PIMAP, a heterogeneity-aware mapping framework that seamlessly orchestrates\nworkloads across electronic and optical tiers. By optimizing workload\npartitioning through a two-stage multi-objective exploration method, H3PIMAP\nharnesses light speed for high-throughput operations and PIM efficiency for\nmemory-bound tasks. System-level evaluations on language and vision models show\nH3PIMAP achieves a 2.74x energy efficiency improvement and a 3.47x latency\nreduction compared to homogeneous architectures and naive mapping strategies.\nThis proposed framework lays the foundation for hybrid AI accelerators,\nbridging the gap between electronic and photonic computation for\nnext-generation efficiency and scalability.",
      "generated_abstract": "t of the Internet of Things (IoT) has enabled the integration of\nmicrocontrollers (MCUs) into many diverse systems. In recent years,\nprogramming languages for embedded systems have evolved to support the\nintegration of MCUs, enabling developers to write code that can execute\ndirectly on the microcontroller. In this paper, we present a novel\nframework that aims to address the challenge of providing developers with\neffective tools for programming embedded systems. To address this challenge,\nwe propose a systematic methodology for designing an embedded system using\nthe Ada programming language. We first explore the design space of Ada and\nidentify the most promising approaches for building a system. Then, we use\nthese approaches to develop a prototype system that implements a simple\nemulator for a microcontroller. We then present a set of design principles\nthat guide the development of embedded systems in Ada. Finally, we present\nthe results of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10687022900763359,
          "p": 0.15730337078651685,
          "f": 0.12727272245495885
        },
        "rouge-2": {
          "r": 0.005813953488372093,
          "p": 0.0070921985815602835,
          "f": 0.006389771406877439
        },
        "rouge-l": {
          "r": 0.09923664122137404,
          "p": 0.14606741573033707,
          "f": 0.11818181336404979
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.16214v1",
      "true_abstract": "This paper analyzes nonlinearities in the international transmission of\nfinancial shocks originating in the US. To do so, we develop a flexible\nnonlinear multi-country model. Our framework is capable of producing\nasymmetries in the responses to financial shocks for shock size and sign, and\nover time. We show that international reactions to US-based financial shocks\nare asymmetric along these dimensions. Particularly, we find that adverse\nshocks trigger stronger declines in output, inflation, and stock markets than\nbenign shocks. Further, we investigate time variation in the estimated dynamic\neffects and characterize the responsiveness of three major central banks to\nfinancial shocks.",
      "generated_abstract": "This paper develops an analytical framework for understanding how economic\nenvironmental conditions can shape the economic behavior of firms and their\nemployees. We introduce a new econometric model that simultaneously\npredicts the probability of firms' bankruptcy and the number of employees\nreducing their work hours during a recession. Using data from 1992 to 2017, we\nempirically test our model against a wide range of alternative models. Our\nfindings suggest that firms' probability of bankruptcy is negatively\nassociated with the extent of environmental degradation and that employees'\nproactive behavior during recessions is positively associated with their\nreduction in work hours. These findings have important implications for\nempirical research and policymaking in both developing and developed countries.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.2,
          "f": 0.21052631080332423
        },
        "rouge-2": {
          "r": 0.010869565217391304,
          "p": 0.009009009009009009,
          "f": 0.009852211792572077
        },
        "rouge-l": {
          "r": 0.20833333333333334,
          "p": 0.1875,
          "f": 0.19736841606648212
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PM/2502.00415v1",
      "true_abstract": "MarketSenseAI is a novel framework for holistic stock analysis which\nleverages Large Language Models (LLMs) to process financial news, historical\nprices, company fundamentals and the macroeconomic environment to support\ndecision making in stock analysis and selection. In this paper, we present the\nlatest advancements on MarketSenseAI, driven by rapid technological expansion\nin LLMs. Through a novel architecture combining Retrieval-Augmented Generation\nand LLM agents, the framework processes SEC filings and earnings calls, while\nenriching macroeconomic analysis through systematic processing of diverse\ninstitutional reports. We demonstrate a significant improvement in fundamental\nanalysis accuracy over the previous version. Empirical evaluation on S\\&P 100\nstocks over two years (2023-2024) shows MarketSenseAI achieving cumulative\nreturns of 125.9% compared to the index return of 73.5%, while maintaining\ncomparable risk profiles. Further validation on S\\&P 500 stocks during 2024\ndemonstrates the framework's scalability, delivering a 33.8% higher Sortino\nratio than the market. This work marks a significant advancement in applying\nLLM technology to financial analysis, offering insights into the robustness of\nLLM-driven investment strategies.",
      "generated_abstract": "r investigates the effectiveness of using deep learning to forecast\nthe daily closing prices of 20,000 S&P 500 stocks using the S&P 500 Stock Price\nIndex. The dataset consists of daily closing prices from 1950 to 2024, and\nthe goal is to develop a deep learning model that can forecast the closing\nprice within the next 30 minutes. To achieve this goal, we use LSTM,\nGRU, and BiLSTM layers to model the time series of daily closing prices. We\ntested a variety of learning methods, including feedforward neural networks,\nand compared the results with a baseline model that uses only the daily closing\nprices. Our results show that using deep learning models to forecast the\nclosing prices of S&P 500 stocks can improve the accuracy of the model by up to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1076923076923077,
          "p": 0.19444444444444445,
          "f": 0.13861385679835325
        },
        "rouge-2": {
          "r": 0.012121212121212121,
          "p": 0.01834862385321101,
          "f": 0.014598535354841999
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.18055555555555555,
          "f": 0.12871286669934337
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.q-bio/SC/2412.20570v1",
      "true_abstract": "Characterizing the local voltage distribution within nanophysiological\ndomains, driven by ionic currents through membrane channels, is crucial for\nstudying cellular activity in modern biophysics, yet it presents significant\nexperimental and theoretical challenges. Theoretically, the complexity arises\nfrom the difficulty of solving electro-diffusion equations in three-dimensional\ndomains. Currently, there are no methods available for obtaining asymptotic\ncomputations or approximated solutions of nonlinear equations, and numerically,\nit is challenging to explore solutions across both small and large spatial\nscales. In this work, we develop a method to solve the Poisson-Nernst-Planck\nequations with ionic currents entering and exiting through two narrow, circular\nwindow channels located on the boundary. The inflow through the first window is\ncomposed of a single cation, while the outflow maintains a constant ionic\ndensity satisfying local electro-neutrality conditions. Employing regular\nexpansions and Green's function representations, we derive the ionic profiles\nand voltage drops in both small and large charge regimes. We explore how local\nsurface curvature and window channels size influence voltage dynamics and\nvalidate our theoretical predictions through numerical simulations, assessing\nthe accuracy of our asymptotic computations. These novel relationships between\ncurrent, voltage, concentrations and geometry can enhance the characterization\nof physiological behaviors of nanodomains.",
      "generated_abstract": "igate the influence of ionic strength and pH on the thermodynamic\nproperties of a model biomolecular system, with particular focus on the\nconcentration dependence of the thermodynamic properties of the system, such as\nthe Gibbs free energy and entropy. We have investigated the effects of ionic\nstrength and pH on the equilibrium constant of the reaction, and the thermodynamic\nstability of the solution. We have also studied the influence of ionic strength\nand pH on the thermodynamic properties of the reaction. We have also investigated\nthe influence of ionic strength and pH on the enthalpy of reaction. We have\nobserved that the concentration dependence of the equilibrium constant, the\nenthalpy and the entropy of reaction depend on ionic strength and pH. The\nconcentration dependence of the equilibrium constant, the enthalpy and the\nentropy of reaction of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07857142857142857,
          "p": 0.25,
          "f": 0.11956521375236306
        },
        "rouge-2": {
          "r": 0.010526315789473684,
          "p": 0.027777777777777776,
          "f": 0.015267171586738409
        },
        "rouge-l": {
          "r": 0.07142857142857142,
          "p": 0.22727272727272727,
          "f": 0.10869564853497177
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.00467v1",
      "true_abstract": "Recent advancements in convolutional neural network (CNN)-based techniques\nfor remote sensing pansharpening have markedly enhanced image quality. However,\nconventional convolutional modules in these methods have two critical\ndrawbacks. First, the sampling positions in convolution operations are confined\nto a fixed square window. Second, the number of sampling points is preset and\nremains unchanged. Given the diverse object sizes in remote sensing images,\nthese rigid parameters lead to suboptimal feature extraction. To overcome these\nlimitations, we introduce an innovative convolutional module, Adaptive\nRectangular Convolution (ARConv). ARConv adaptively learns both the height and\nwidth of the convolutional kernel and dynamically adjusts the number of\nsampling points based on the learned scale. This approach enables ARConv to\neffectively capture scale-specific features of various objects within an image,\noptimizing kernel sizes and sampling locations. Additionally, we propose ARNet,\na network architecture in which ARConv is the primary convolutional module.\nExtensive evaluations across multiple datasets reveal the superiority of our\nmethod in enhancing pansharpening performance over previous techniques.\nAblation studies and visualization further confirm the efficacy of ARConv.",
      "generated_abstract": "ning-based video retrieval has emerged as a promising approach for\nhigh-quality visual search. However, the inherent limitations of conventional\nmethods, such as data sparsity and inconsistent visual features, have\nsignificantly hindered their performance in real-world scenarios. To address\nthese challenges, we propose a novel video retrieval framework, named\nVideo-QA, which incorporates a video question-answering (QA) module and\nefficient visual feature extraction. Specifically, the video QA module extracts\nvisual features from the video clip using a pre-trained visual encoder and\nattends over them with the question text to generate visual-text pairs. To\noptimize the visual feature extraction, we propose a novel contrastive\nlearning strategy that leverages the contrastive learning framework to learn\nvisual features with a positive sample consistency, which can effectively\nenhance the visual feature extra",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1885245901639344,
          "p": 0.26436781609195403,
          "f": 0.22009568892012557
        },
        "rouge-2": {
          "r": 0.011976047904191617,
          "p": 0.017391304347826087,
          "f": 0.01418439233313378
        },
        "rouge-l": {
          "r": 0.18032786885245902,
          "p": 0.25287356321839083,
          "f": 0.21052631092969498
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.02726v1",
      "true_abstract": "Deep learning scaling laws predict how performance improves with increased\nmodel and dataset size. Here we identify measurement noise in data as another\nperformance scaling axis, governed by a distinct logarithmic law. We focus on\nrepresentation learning models of biological single cell genomic data, where a\ndominant source of measurement noise is due to molecular undersampling. We\nintroduce an information-theoretic metric for cellular representation model\nquality, and find that it scales with sampling depth. A single quantitative\nrelationship holds across several model types and across several datasets. We\nshow that the analytical form of this relationship can be derived from a simple\nGaussian noise model, which in turn provides an intuitive interpretation for\nthe scaling law. Finally, we show that the same relationship emerges in image\nclassification models with respect to two types of imaging noise, suggesting\nthat measurement noise scaling may be a general phenomenon. Scaling with noise\ncan serve as a guide in generating and curating data for deep learning models,\nparticularly in fields where measurement quality can vary dramatically between\ndatasets.",
      "generated_abstract": "udy of protein dynamics, the time evolution of protein conformations\nis often compared to a given initial condition. The time evolution of the\nconformations can be expressed in terms of a master equation, which is an\nalternative formulation of the time evolution of the system. The master equation\nis typically solved by numerical methods, and the results are used to\ncharacterise the protein dynamics. However, the number of parameters in the\nmaster equation can be large, making the computational cost prohibitive. In\nthis paper, we propose a new approach to solve the master equation, based on a\nnovel method for the calculation of the expectation values of the master\nequation. This method is based on the concept of the quantum expectation\nvalue, which is a generalisation of the classical expectation value. This new\napproach is illustrated by a specific example, and is then applied to a general\ncase of the master equation for the time evolution of a generalised",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1504424778761062,
          "p": 0.2125,
          "f": 0.17616579825498685
        },
        "rouge-2": {
          "r": 0.023809523809523808,
          "p": 0.03125,
          "f": 0.02702702211833544
        },
        "rouge-l": {
          "r": 0.1504424778761062,
          "p": 0.2125,
          "f": 0.17616579825498685
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.10653v1",
      "true_abstract": "This paper proposes a framework for selecting policies that maximize expected\nbenefit in the presence of estimation uncertainty, by controlling for\nestimation risk and incorporating risk aversion. The proposed method explicitly\nbalances the size of the estimated benefit against the uncertainty inherent in\nits estimation, ensuring that chosen policies meet a reporting guarantee,\nnamely that the actual benefit of the implemented policy is guaranteed not to\nfall below the reported estimate with a pre-specified confidence level. This\napproach applies to a variety of settings, including the selection of policy\nrules that allocate individuals to treatments based on observed\ncharacteristics, using both experimental and non-experimental data; and the\nallocation of limited budgets among competing social programs; as well as many\nothers. Across these applications, the framework offers a principled and robust\nmethod for making data-driven policy choices under uncertainty. In broader\nterms, it focuses on policies that are on the efficient decision frontier,\ndescribing policies that offer maximum estimated benefit for a given acceptable\nlevel of estimation risk.",
      "generated_abstract": "We study the identification of non-linear autoregressive (NLA) models in\nnonlinear regression, which are popular in econometrics and finance. In\npractice, NLA models are often estimated using least squares, but we show that\nthe usual identification strategy, based on the conditional expectation\nestimation (CEE), is inappropriate in the nonlinear setting. We develop a new\nidentification strategy that is based on the sample mean of the nonlinear\npredictor. We illustrate our results by applying them to a simple empirical\nexample of a stock market model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14414414414414414,
          "p": 0.26229508196721313,
          "f": 0.18604650705043277
        },
        "rouge-2": {
          "r": 0.0375,
          "p": 0.075,
          "f": 0.049999995555555954
        },
        "rouge-l": {
          "r": 0.12612612612612611,
          "p": 0.22950819672131148,
          "f": 0.16279069309694444
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2501.17490v1",
      "true_abstract": "Leveraging a unique dataset of carbon futures option prices traded on the ICE\nmarket from December 2015 until December 2020, we present the results from an\nunprecedented calibration exercise. Within a multifactor stochastic volatility\nframework with jumps, we employ a three-dimensional pricing kernel compensating\nfor equity and variance components' risk to derive an analytically tractable\nand numerically practical approach to pricing. To the best of our knowledge, we\nare the first to provide an estimate of the equity and variance risk premia for\nthe carbon futures option market. We gain insights into daily option and\nfutures dynamics by exploiting the information from tick-by-tick futures trade\ndata. Decomposing the realized measure of futures volatility into continuous\nand jump components, we employ them as auxiliary variables for estimating\nfutures dynamics via indirect inference. Our approach provides a realistic\ndescription of carbon futures price, volatility, and jump dynamics and an\ninsightful understanding of the carbon option market.",
      "generated_abstract": "uce a new dataset for the research of financial market dynamics and\nintroduce a new method for the analysis of the dynamics of the spread of\ncurrencies. The dataset consists of 30000 trading days of the USD/EUR, USD/JPY\nand EUR/JPY markets, covering the period from 2015 to 2023. The dataset is\ncomposed of 100000 trading days of the USD/EUR and 10000 trading days of the\nUSD/JPY markets, covering the period from 2015 to 2023. The dataset is\npartitioned into training and testing sets, and the classification task is\nachieved by the use of a machine learning model, namely, the Random Forest\nClassifier. The proposed methodology allows the analysis of the spread dynamics\nand the classification",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13829787234042554,
          "p": 0.22807017543859648,
          "f": 0.17218542576378243
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.03488372093023256,
          "f": 0.026431713355198883
        },
        "rouge-l": {
          "r": 0.1276595744680851,
          "p": 0.21052631578947367,
          "f": 0.15894039265119964
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2502.11199v1",
      "true_abstract": "Although the methodology of Design Science Research (DSR) is playing an\nincreasingly important role with the emergence of the \"sciences of the\nartificial\", the validity of the resulting artifacts is occasionally\nquestioned. This paper compares three influential DSR frameworks to assess\ntheir support for artifact validity. Using five essential validity types\n(instrument validity, technical validity, design validity, purpose validity and\ngeneralization), the qualitative analysis reveals that while purpose validity\nis explicitly emphasized, instrument and design validity remain the least\ndeveloped. Their implicit treatment in all frameworks poses a risk of\noverlooked validation, and the absence of mandatory instrument validity can\nlead to invalid artifacts, threatening research credibility. Beyond these\nfindings, the paper contributes (a) a comparative overview of each framework's\nstrengths and weaknesses and (b) a revised DSR framework incorporating all five\nvalidity types with definitions and examples. This ensures systematic artifact\nevaluation and improvement, reinforcing the rigor of DSR.",
      "generated_abstract": "t a novel approach to modeling the dynamics of a continuous-time\nsystem as a discrete-time system with a finite number of state variables,\nwithin a continuous-time Markov chain (CTMC) framework. This approach\nenables the modeling of the dynamics of the continuous-time system as a\ndynamical system with a finite number of state variables within the CTMC\nframework, where the CTMC is a discrete-time Markov chain with a finite number\nof state variables. We show that the continuous-time system can be expressed\nas the transition probability matrix of the discrete-time CTMC, and vice versa.\nThis is a significant advantage, as it enables the use of standard\napproximation techniques for the continuous-time system, such as the\nMonte Carlo method, within the discrete-time CTMC framework. The approach is\napplied to a model of a continuous-time system in which the system states",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13,
          "p": 0.20634920634920634,
          "f": 0.15950919771161895
        },
        "rouge-2": {
          "r": 0.006896551724137931,
          "p": 0.009708737864077669,
          "f": 0.008064511272440488
        },
        "rouge-l": {
          "r": 0.12,
          "p": 0.19047619047619047,
          "f": 0.1472392590613122
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.physics/bio-ph/2503.09319v1",
      "true_abstract": "We present PolyMorph, a lightweight standalone C++ program that extends its\npredecessor PolyHoop by a finite-difference solver for multi-component\nreaction-advection-diffusion equations. PolyMorph simulates two integral parts\nof tissue morphogenesis in two dimensions: 1) the mechanics of cellular\ndeformation, growth and proliferation, and 2) transport and reaction of an\narbitrary number of chemical species. Both of these components are\nbidirectionally coupled, allowing cells to base their behavior on local\ninformation on concentrations and flow, and allowing the chemical transport and\nreaction kinetics to depend on spatial information such as the local cell type.\nThis bidirectional feedback makes PolyMorph a versatile tool to study a variety\nof cellular morphogenetic processes such as chemotaxis, cell sorting, tissue\npatterning with morphogen gradients, Turing patterning, and diffusion- or\nsupply-limited growth with sub-cellular resolution.",
      "generated_abstract": "t a unified theoretical framework for the description of\ndynamic heterogeneous media in which a network of self-assembling monolayers\n(SAMs) is embedded in a bulk fluid. We focus on the dynamics of SAMs that are\naligned parallel to the bulk fluid and we show that they can be described in\nterms of a self-consistent theory for the interaction between the SAMs and the\nfluid. Our theory is based on the analysis of the mean-field interaction\nbetween SAMs, which can be calculated using the well-known Lennard-Jones\npotential. We provide a rigorous derivation of this mean-field interaction,\nshowing that it is non-linear in the SAMs' interactions with the fluid. Our\ntheory is validated with a series of simple experiments. We demonstrate that\nthe mean-field interaction can capture the main features of the interaction\nbetween SAM",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12903225806451613,
          "p": 0.16216216216216217,
          "f": 0.14371256991502043
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.16216216216216217,
          "f": 0.14371256991502043
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.09740v1",
      "true_abstract": "This paper addresses the challenge of forecasting corporate distress, a\nproblem marked by three key statistical hurdles: (i) right censoring, (ii)\nhigh-dimensional predictors, and (iii) mixed-frequency data. To overcome these\ncomplexities, we introduce a novel high-dimensional censored MIDAS (Mixed Data\nSampling) logistic regression. Our approach handles censoring through inverse\nprobability weighting and achieves accurate estimation with numerous\nmixed-frequency predictors by employing a sparse-group penalty. We establish\nfinite-sample bounds for the estimation error, accounting for censoring, the\nMIDAS approximation error, and heavy tails. The superior performance of the\nmethod is demonstrated through Monte Carlo simulations. Finally, we present an\nextensive application of our methodology to predict the financial distress of\nChinese-listed firms. Our novel procedure is implemented in the R package\n'Survivalml'.",
      "generated_abstract": "This paper develops a simple and flexible estimator for the mean-preserving\nestimator of a linear mean-preserving function. The estimator can be used to\nimprove the mean-preservation of the estimator of a nonlinear mean-preserving\nfunction. We show that the mean-preservation of the mean-preserving estimator\nof the mean-preserving function is the same as the mean-preservation of the\noriginal estimator of the nonlinear mean-preserving function. We also discuss\nhow the mean-preservation of the mean-preserving estimator of the mean-preserving\nfunction can be used to improve the mean-preservation of the original estimator\nof the nonlinear mean-preserving function.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11458333333333333,
          "p": 0.34375,
          "f": 0.17187499625000008
        },
        "rouge-2": {
          "r": 0.025,
          "p": 0.06382978723404255,
          "f": 0.035928139667969906
        },
        "rouge-l": {
          "r": 0.11458333333333333,
          "p": 0.34375,
          "f": 0.17187499625000008
        }
      }
    },
    {
      "paper_id": "cs.LO.cs/LO/2503.06036v1",
      "true_abstract": "Consistent Hoare, Smyth and Plotkin power domains are introduced and\ndiscussed by Yuan and Kou. The consistent algebraic operation $+$ defined by\nthem is a binary partial Scott continuous operation satisfying the requirement:\n$a+b$ exists whenever there exists a $c$ which is greater than $a$ and $b$. We\nextend the consistency to be a categorical concept and obtain an approach to\ngenerating consistent monads from monads on dcpos whose images equipped with\nsome algebraic operations. Then we provide two new power constructions over\ndomains: the consistent Plotkin index power domain and the consistent\nprobabilistic power domain. Moreover, we verify these power constructions are\nfree.",
      "generated_abstract": "t the first theoretical model for the development of a general\nlanguage\n  for formal verification, where the verification model is built using\n  mathematical proofs rather than formal specifications. This is a natural\nextension of the formal verification approach, which focuses on mathematical\nproofs of correctness rather than the specification of correctness. Our\napproach extends the verification process by enabling the development of a\nformal specification of the desired correctness. This formal specification\ncaptures the desired correctness and its verification through mathematical\nproofs. We use this approach for the first time to develop the verification\nmodel for the development of a formal specification of the correctness of the\nrecently proposed language for formal verification, called LF. We present a\nfoundation of this formal specification through mathematical proofs and\ndemonstrate its effectiveness in the verification of the recently proposed\nlanguage for formal verification, called LF",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14666666666666667,
          "p": 0.1896551724137931,
          "f": 0.16541352891627578
        },
        "rouge-2": {
          "r": 0.01,
          "p": 0.010101010101010102,
          "f": 0.010050246256410154
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.1724137931034483,
          "f": 0.1503759349313134
        }
      }
    },
    {
      "paper_id": "quant-ph.cs/CC/2503.03600v1",
      "true_abstract": "Bosonic quantum systems operate in an infinite-dimensional Hilbert space,\nunlike discrete-variable quantum systems. This distinct mathematical structure\nleads to fundamental differences in quantum information processing, such as an\nexponentially greater complexity of state tomography [MMB+24] or a factoring\nalgorithm in constant space [BCCRK24]. Yet, it remains unclear whether this\nstructural difference of bosonic systems may also translate to a practical\ncomputational advantage over finite-dimensional quantum computers. Here we take\na step towards answering this question by showing that universal bosonic\nquantum computations can be simulated in exponential time on a classical\ncomputer, significantly improving the best previous upper bound requiring\nexponential memory [CJMM24]. In complexity-theoretic terms, we improve the best\nupper bound on $\\textsf{CVBQP}$ from $\\textsf{EXPSPACE}$ to $\\textsf{EXP}$.\nThis result is achieved using a simulation strategy based on finite energy\ncutoffs and approximate coherent state decompositions. While we propose ways to\npotentially refine this bound, we also present arguments supporting the\nplausibility of an exponential computational advantage of bosonic quantum\ncomputers over their discrete-variable counterparts. Furthermore, we emphasize\nthe role of circuit energy as a resource and discuss why it may act as the\nfundamental bottleneck in realizing this advantage in practical\nimplementations.",
      "generated_abstract": "We study the robustness of classical communication in the context of\nattacking quantum channels. We show that, for certain types of attacks,\nclassical communication can allow for a significant increase in robustness,\nespecially for attacks where the target is a quantum channel. We furthermore\npropose a novel attack on quantum channel robustness, which we refer to as\nquantum-channel-robustness-attack, and we show that it can break quantum\nchannel robustness, which is a fundamental property of quantum channels. We\nalso show that quantum-channel-robustness-attack is a form of quantum\nchannel-attack.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13846153846153847,
          "p": 0.35294117647058826,
          "f": 0.198895023576814
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13846153846153847,
          "p": 0.35294117647058826,
          "f": 0.198895023576814
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.06711v1",
      "true_abstract": "This paper touches on several interaction points of semigroups and\nconstructions from category theory: An adjunction is established between\ncategories with selected arrows and semigroups. Regular semigroups are\ncharacterized by split epi - split mono factorization of the Karoubi envelope.\nWe investigate how semigroupads (monads without requirement of unit\ntransformation) map semigroups to semigroups and ensure certain properties\nprovided they hold on meta level.",
      "generated_abstract": "In this paper, we study the problem of approximating a given function $f$ by a\nfunction of the form $f(x)=g(x) \\exp(-\\lambda x)$, where $g$ is a polynomial\nof degree $k$ and $\\lambda \\in [0,1",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09259259259259259,
          "p": 0.17857142857142858,
          "f": 0.12195121501487226
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.030303030303030304,
          "f": 0.021052627044876322
        },
        "rouge-l": {
          "r": 0.09259259259259259,
          "p": 0.17857142857142858,
          "f": 0.12195121501487226
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.06327v1",
      "true_abstract": "We investigate the return-forecasting and volatility-forecasting power of\nintraday on-chain flow data for BTC, ETH, and USDT, and the associated option\nstrategies. First, we find that USDT net inflow into cryptocurrency exchanges\npositively forecasts future returns of both BTC and ETH, with the strongest\neffect at the 1-hour frequency. Second, we find that ETH net inflow into\ncryptocurrency exchanges negatively forecasts future returns of ETH. Third, we\nfind that BTC net inflow into cryptocurrency exchanges does not significantly\nforecast future returns of BTC. Finally, we confirm that selling 0DTE ETH call\noptions is a profitable trading strategy when the net inflow into\ncryptocurrency exchanges is high. Our study lends new insights into the\nemerging literature that studies the on-chain activities and their\nasset-pricing impact in the cryptocurrency market.",
      "generated_abstract": "This paper develops a novel model for predicting individual-level labor\napproval in a large-scale setting with multiple actors. The model accounts for\nthe complex dynamics of political attitudes, party identification, and\npersonality traits, while also incorporating information about the demographic\nand socioeconomic characteristics of voters. We demonstrate the model's\neffectiveness by simulating a real-world data set, and then using it to\npredict the approval for the 2016 U.S. presidential election. The results\ndemonstrate that the model's predictions are in line with the actual election\noutcome, offering a promising framework for future research on political\nparticipation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14473684210526316,
          "p": 0.1527777777777778,
          "f": 0.14864864365230113
        },
        "rouge-2": {
          "r": 0.009433962264150943,
          "p": 0.010752688172043012,
          "f": 0.010050246277621716
        },
        "rouge-l": {
          "r": 0.14473684210526316,
          "p": 0.1527777777777778,
          "f": 0.14864864365230113
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/ME/2503.06401v1",
      "true_abstract": "Distribution-as-response regression problems are gaining wider attention,\nespecially within biomedical settings where observation-rich patient specific\ndata sets are available, such as feature densities in CT scans (Petersen et\nal., 2021) actigraphy (Ghosal et al., 2023), and continuous glucose monitoring\n(Coulter et al., 2024; Matabuena et al., 2021). To accommodate the complex\nstructure of such problems, Petersen and M\\\"uller (2019) proposed a regression\nframework called Fr\\'echet regression which allows non-Euclidean responses,\nincluding distributional responses. This regression framework was further\nextended for variable selection by Tucker et al. (2023), and Coulter et al.\n(2024) (arXiv:2403.00922 [stat.AP]) developed a fast variable selection\nalgorithm for the specific setting of univariate distributional responses\nequipped with the 2-Wasserstein metric (2-Wasserstein space). We present\n\"fastfrechet\", an R package providing fast implementation of these Fr\\'echet\nregression and variable selection methods in 2-Wasserstein space, with\nresampling tools for automatic variable selection. \"fastfrechet\" makes\ndistribution-based Fr\\'echet regression with resampling-supplemented variable\nselection readily available and highly scalable to large data sets, such as the\nUK Biobank (Doherty et al., 2017).",
      "generated_abstract": "The statistical analysis of large-scale genome-wide association studies\n(GWAS) is challenging due to the increasing complexity of the data,\nparticularly when multiple genome-wide association studies (GWAS) are\nperformed. In this work, we propose a novel approach to the analysis of\nmulti-GWAS data using Gaussian process regression. This method leverages the\nGaussian process framework to allow for the simultaneous inference of\nmultiple GWAS parameters and a shared common factor. We demonstrate the\neffectiveness of our method by applying it to 1000 Genomes Project (1000GP)\nand European Genome-phenome Archive (EGA) data sets. The results demonstrate\nthat our method achieves comparable accuracy to existing methods, while\nproviding a more intuitive and interpretable analysis of complex GWAS data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13559322033898305,
          "p": 0.20253164556962025,
          "f": 0.1624365434193101
        },
        "rouge-2": {
          "r": 0.012903225806451613,
          "p": 0.01904761904761905,
          "f": 0.015384610569528133
        },
        "rouge-l": {
          "r": 0.1271186440677966,
          "p": 0.189873417721519,
          "f": 0.1522842591553507
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.09127v1",
      "true_abstract": "This research presents Spiritus, an AI-assisted creation tool designed to\nstreamline 2D character animation creation while enhancing creative\nflexibility. By integrating natural language processing and diffusion models,\nusers can efficiently transform natural language descriptions into personalized\n2D characters and animations. The system employs automated segmentation,\nlayered costume techniques, and dynamic mesh-skeleton binding solutions to\nsupport flexible adaptation of complex costumes and additional components.\nSpiritus further achieves real-time animation generation and efficient\nanimation resource reuse between characters through the integration of BVH data\nand motion diffusion models. Experimental results demonstrate Spiritus's\neffectiveness in reducing technical barriers, enhancing creative freedom, and\nsupporting resource universality. Future work will focus on optimizing user\nexperience and further exploring the system's human-computer collaboration\npotential.",
      "generated_abstract": "r introduces a novel approach to automated systematic review\nanalysis, aimed at addressing the gap between current practice and the\nexisting research agenda. The paper proposes a novel methodology for systematic\nreview data analysis that aims to address the need for improved data\ninterpretation and the development of new research agendas. The methodology\ninvolves the development of a novel systematic review data analysis framework\nthat integrates qualitative and quantitative analysis and enables the\nsystematic identification of key findings. This framework provides a\ncomprehensive understanding of the systematic review data and highlights key\ninsights that can inform future research. The proposed methodology provides a\nstep-by-step framework for systematic review data analysis that is suitable for\nuse by systematic reviewers, researchers, and other stakeholders. The methodology\nprovides a practical approach for analyzing systematic review data",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11458333333333333,
          "p": 0.15942028985507245,
          "f": 0.13333332846721782
        },
        "rouge-2": {
          "r": 0.008695652173913044,
          "p": 0.009174311926605505,
          "f": 0.008928566432161597
        },
        "rouge-l": {
          "r": 0.09375,
          "p": 0.13043478260869565,
          "f": 0.1090909042247936
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2410.12648v1",
      "true_abstract": "Fingerprints, otherwise known as dermatoglyphs, are most commonly thought of\nin the context of identification, but have myriad other roles in human biology.\nThey are formed by the restricted ability of ridges and furrows of the\nepidermis to flatten. The patterns these ridges and furrows make can be\nrepresented as 2D fingerprints, but also as 3D structures with cross-sectional\nshapes that may add new levels of detail to identification, forensic, and\nbehavioral uses/studies. Surface metrology techniques better allow for the\nquantification of these features, though it is unclear what tool and what scale\nis most appropriate. A Sensofar S Neox white light reflectance confocal\nmicroscope and a Gelsight Mobile 2 were used to independently measure the\nsurface roughness of the fingerprints of four individuals from preserved\ncadaveric remains. Scale-sensitive fractal analyses (SSFA) were performed on\nthe data from the S Neox (a small area), Gelsight (a larger area), and the same\nGelsight datasets cropped down to the size of the S Neox scan size. Though\nfewer SSFA parameters identified differences between individuals from the\nsmaller, extracted Gelsight area, all three forms of measurement found\nsignificant differences between some individuals from the study. No significant\ndifferences were found that differ between fingers themselves. Though only an\ninitial step, these data suggest that a variety of surface metrology techniques\nmay be useful in differentiating individuals.",
      "generated_abstract": "Cancer metastasis is a complex process that involves multiple steps and\nvarious types of cells. To understand these processes, it is essential to\nstudy the interaction between cells at different stages of the metastatic\nprocess. In this paper, we propose a new method to analyze the metastasis of\ncancer using the interaction network of the metastatic process. This method\ncombines the network analysis and the dynamic programming approach. This\napproach allows us to analyze the evolution of the metastasis process by\nconstructing a dynamic network with the interaction of cells at different\nstages. We illustrate the methodology and demonstrate its effectiveness on two\nreal-world datasets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1103448275862069,
          "p": 0.25396825396825395,
          "f": 0.15384614962324344
        },
        "rouge-2": {
          "r": 0.014423076923076924,
          "p": 0.03296703296703297,
          "f": 0.02006688539770338
        },
        "rouge-l": {
          "r": 0.0896551724137931,
          "p": 0.20634920634920634,
          "f": 0.12499999577708965
        }
      }
    },
    {
      "paper_id": "cs.CG.cs/DS/2503.07769v1",
      "true_abstract": "We study the problem of computing the diameter and the mean distance of a\ncontinuous graph, i.e., a connected graph where all points along the edges,\ninstead of only the vertices, must be taken into account. It is known that for\ncontinuous graphs with $m$ edges these values can be computed in roughly\n$O(m^2)$ time. In this paper, we use geometric techniques to obtain\nsubquadratic time algorithms to compute the diameter and the mean distance of a\ncontinuous graph for two well-established classes of sparse graphs. We show\nthat the diameter and the mean distance of a continuous graph of treewidth at\nmost $k$ can be computed in $O(n\\log^{O(k)} n)$ time, where $n$ is the number\nof vertices in the graph. We also show that computing the diameter and mean\ndistance of a continuous planar graph with $n$ vertices and $F$ faces takes\n$O(n F \\log n)$ time.",
      "generated_abstract": "t a new algorithm for finding a minimum-weight perfect matching in\nthe bipartite graph induced by a balanced binary tree. We first construct a\nsufficient condition for the existence of a minimum-weight perfect matching in\nthe tree and then construct a polynomial-time algorithm to check whether this\ncondition is satisfied. Our algorithm is based on the fact that the minimum\nweight of a minimum-weight perfect matching is at least the minimum weight of\na minimum weight matching in the tree. We also provide a polynomial-time\nalgorithm to check whether the minimum weight of a minimum-weight perfect matching\nis equal to the minimum weight of a minimum-weight matching in the tree. We\nprovide a polynomial-time algorithm to check whether the minimum weight of a\nminimum-weight perfect matching in the tree is equal to the minimum weight of a\nminimum-weight matching in a balanced binary tree. We also provide a\npolynomial-time algorithm to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1686746987951807,
          "p": 0.30434782608695654,
          "f": 0.21705425897722502
        },
        "rouge-2": {
          "r": 0.03305785123966942,
          "p": 0.05555555555555555,
          "f": 0.0414507725243636
        },
        "rouge-l": {
          "r": 0.14457831325301204,
          "p": 0.2608695652173913,
          "f": 0.18604650703924053
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/CP/2412.16067v1",
      "true_abstract": "We use modifications of the Adams method and very fast and accurate\nsinh-acceleration method of the Fourier inversion (iFT) (S.Boyarchenko and\nLevendorski\\u{i}, IJTAF 2019, v.22) to evaluate prices of vanilla options; for\noptions of moderate and long maturities and strikes not very far from the spot,\nthousands of prices can be calculated in several msec. with relative errors of\nthe order of 0.5\\% and smaller running Matlab on a Mac with moderate\ncharacteristics. We demonstrate that for the calibrated set of parameters in\nEuch and Rosenbaum, Math. Finance 2019, v. 29, the correct implied volatility\nsurface is significantly flatter and fits the data very poorly, hence, the\ncalibration results in op.cit. is an example of the {\\em ghost calibration}\n(M.Boyarchenko and Levendorki\\u{i}, Quantitative Finance 2015, v. 15): the\nerrors of the model and numerical method almost cancel one another. We explain\nhow calibration errors of this sort are generated by each of popular versions\nof numerical realizations of iFT (Carr-Madan, Lipton-Lewis and COS methods)\nwith prefixed parameters of a numerical method, resulting in spurious\nvolatility smiles and skews. We suggest a general {\\em Conformal Bootstrap\nprinciple} which allows one to avoid ghost calibration errors. We outline\nschemes of application of Conformal Bootstrap principle and the method of the\npaper to the design of accurate and fast calibration procedures.",
      "generated_abstract": "uce a novel model for pricing European options in a nonlinear\nsystem with memory. The model is based on the so-called quadratic exponential\nmodel, which is a generalization of the classical P\\'olya-Szeg\\\"o model. The\nquadratic exponential model exhibits a richer behavior than the classical P\\'olya\nSzeg\\\"o model and is also more flexible than the standard linear model. We\nderive a closed-form solution for the optimal exercise price in the quadratic\nexponential model and use the solution to numerically solve the pricing\nproblem. Our numerical results show that the quadratic exponential model\nprovides a better approximation of the true price of the option when compared\nto the standard linear model. The quadratic exponential model also provides a\nmore accurate option price in volatile markets. These findings suggest that\nthe quadratic exponential model is a more accurate model for pricing European\noptions in nonlinear systems with",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.2857142857142857,
          "f": 0.19512194672218927
        },
        "rouge-2": {
          "r": 0.01904761904761905,
          "p": 0.03636363636363636,
          "f": 0.024999995488282065
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.2571428571428571,
          "f": 0.17560975160023806
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.00818v1",
      "true_abstract": "Sample size determination is crucial in experimental design, especially in\ntraffic and transport research. Frequentist statistics require a fixed sample\nsize determined by power analysis, which cannot be adjusted once the experiment\nstarts. Bayesian sample size determination, with proper priors, offers an\nalternative. Bayesian optional stopping (BOS) allows experiments to stop when\nstatistical targets are met. We introduce predictive Bayesian optional stopping\n(pBOS), combining BOS with Bayesian rehearsal simulations to predict future\ndata and stop experiments if targets are unlikely to be met within resource\nconstraints. We identified and corrected a bias in predictions using multiple\nlinear regression. pBOS shows up to 118% better cost benefit than traditional\nBOS and is more efficient than frequentist methods. pBOS allows researchers to,\nunder certain conditions, stop experiments when resources are insufficient or\nwhen enough data is collected, optimizing resource use and cost savings.",
      "generated_abstract": "of this work is to develop an algorithm that provides a\nmethod to perform the following tasks: (1) a linear regression of the\noutcome variable against one or more predictor variables using the least\nsquares method; (2) a linear regression of the outcome variable against one or\nmore predictor variables using the least squares method, but with the\nobservations excluded from the analysis; (3) a nonlinear regression of the\noutcome variable against one or more predictor variables using the least\nsquares method; and (4) a nonlinear regression of the outcome variable against\none or more predictor variables using the least squares method, but with the\nobservations excluded from the analysis. The algorithm is designed to be\nefficient, robust to outliers, and able to handle scenarios where the\nobservations are either missing or very noisy. The algorithm is designed to\nprovide the following results: (1) a linear regression",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13861386138613863,
          "p": 0.22950819672131148,
          "f": 0.17283950147767121
        },
        "rouge-2": {
          "r": 0.014814814814814815,
          "p": 0.024390243902439025,
          "f": 0.01843317502176849
        },
        "rouge-l": {
          "r": 0.1188118811881188,
          "p": 0.19672131147540983,
          "f": 0.14814814345297986
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2502.17455v1",
      "true_abstract": "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
      "generated_abstract": "In this study, we propose a novel framework to model the impact of\nchanging the environment on the evolution of biological species. We develop a\nstatistical mechanics model to quantify the environmental effects on the\ndynamics of a population of organisms. We first formulate the problem as a\nstatistical mechanics model and develop a mean-field approach to analytically\nsolve for the population dynamics. The model can be used to investigate the\ndynamics of a population of organisms under a changing environment and to\nanalyze how changes in the environment affect the evolution of the population\nover time. Our model can be used to understand the effect of environmental\nchange on biological populations and provide insights into how the environment\naffects the evolution of biological species.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19101123595505617,
          "p": 0.2786885245901639,
          "f": 0.226666661840889
        },
        "rouge-2": {
          "r": 0.04504504504504504,
          "p": 0.05,
          "f": 0.047393359942499574
        },
        "rouge-l": {
          "r": 0.16853932584269662,
          "p": 0.2459016393442623,
          "f": 0.19999999517422232
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2502.19397v1",
      "true_abstract": "In chemical reaction network theory, ordinary differential equations are used\nto model the temporal change of chemical species concentration. As the\nfunctional form of these ordinary differential equations systems is derived\nfrom an empirical model of the reaction network, it may be incomplete. Our\napproach aims to elucidate these hidden insights in the reaction network by\ncombining dynamic modelling with deep learning in the form of neural ordinary\ndifferential equations. Our contributions not only help to identify the\nshortcomings of existing empirical models but also assist the design of future\nreaction networks.",
      "generated_abstract": "The integration of genetics and computational biology is essential for\nrevealing the mechanisms underlying complex biological phenomena. In this\narticle, we present a framework for integrating genetics and computational\nbiology by combining the concepts of phenotype-genotype interaction networks and\ngenome-wide association studies (GWAS) with the Bayesian inference modeling\nframework. The framework integrates GWAS with the Bayesian inference modeling\nframework and provides a practical way to integrate GWAS and genetics into\ncomputational biology. We demonstrate the usefulness of the framework in\npredicting the genetic susceptibility to cancer by integrating GWAS and\nBayesian inference modeling.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16129032258064516,
          "p": 0.18181818181818182,
          "f": 0.17094016595806866
        },
        "rouge-2": {
          "r": 0.024096385542168676,
          "p": 0.025,
          "f": 0.024539872302308217
        },
        "rouge-l": {
          "r": 0.14516129032258066,
          "p": 0.16363636363636364,
          "f": 0.15384614886405157
        }
      }
    },
    {
      "paper_id": "math.PR.q-fin/MF/2412.16436v1",
      "true_abstract": "We consider a microstructure foundation for rough volatility models driven by\nPoisson random measures. In our model the volatility is driven by self-exciting\narrivals of market orders as well as self-exciting arrivals of limit orders and\ncancellations. The impact of market order on future order arrivals is captured\nby a Hawkes kernel with power law decay, and is hence persistent. The impact of\nlimit orders on future order arrivals is temporary, yet possibly long-lived.\nAfter suitable scaling the volatility process converges to a fractional Heston\nmodel driven by an additional Poisson random measure. The random measure\ngenerates occasional spikes and clusters of spikes in the volatility process.\nOur results are based on novel existence and uniqueness of solutions results\nfor stochastic path-dependent Volterra equations driven by Poisson random\nmeasures.",
      "generated_abstract": "This paper studies a stochastic control problem with a linear utility\nreward function under a dynamic price of money and a dynamic interest rate. The\nunderlying stochastic process is a semimartingale process with a jump-diffusion\nterm. We establish the existence of a unique strong solution to the\nsemimartingale-valued stochastic control problem. Moreover, we establish the\nstrong uniqueness of the solution in a neighborhood of the origin. Then, we\nprove the existence of the invariant measure for the value function, which is\nalso the unique strong solution to the martingale-valued stochastic control\nproblem. We also show that the invariant measure is the unique weak solution\nto the martingale-valued stochastic control problem. Finally, we derive the\noptimal control policy and present numerical examples.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21333333333333335,
          "p": 0.25806451612903225,
          "f": 0.23357663738078757
        },
        "rouge-2": {
          "r": 0.009433962264150943,
          "p": 0.010416666666666666,
          "f": 0.009900985111266114
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.24193548387096775,
          "f": 0.2189780972348022
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.03648v1",
      "true_abstract": "The work aims to propose a new nonlinear characteristics model for a wideband\nradio amplifier of variable supply voltage. An extended Rapp model proposal is\npresented. The proposed model has been verified by measurements of three\ndifferent amplifiers. This model can be used to design frontend-aware 6G\nsystems.\n  --\n  Praca ma na celu zaproponowanie nowego modelu dla nieliniowej charakterystyki\nwzmacniacza radiowego ze zmiennym napi\\k{e}ciem zasilania pracuj\\k{a}cym w\nszerokim zakresie cz\\k{e}stotliwo\\'sci. Przedstawiona zosta{\\l}a propozycja\nrozszerzonego modelu Rappa. Zaproponowany model zweryfikowano na podstawie\npomiar\\'ow charakterystyk trzech r\\'o\\.znych wzmacniaczy. Model ten mo\\.ze\nby\\'c wykorzystany do projektowania system\\'ow 6G \"\\'swiadomych\"\nniedoskona{\\l}o\\'sci uk{\\l}ad\\'ow wej\\'sciowo-wyj\\'sciowych.",
      "generated_abstract": "r presents a novel wireless communication system that combines\ncommunication and energy harvesting for both uplink and downlink communication\nin a heterogeneous network. A single base station (BS) serves two types of\nusers, namely, user 1 (U1) and user 2 (U2), which are associated with different\nsignals, with the former transmitting in the frequency band 1 and the latter in\nthe band 2. The energy harvesting (EH) units of the two types of users are\nlocated at the edge of the network. In addition, the EH units of the users in\nthe band 2 are located further away than the EH units of the users in the band\n1, which allows the users to communicate in the band 1 while harvesting energy\nfrom the band 2. The BS then simultaneously transmits the signal of both\nusers in the band 1 and the band",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06741573033707865,
          "p": 0.08450704225352113,
          "f": 0.07499999506328157
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.06741573033707865,
          "p": 0.08450704225352113,
          "f": 0.07499999506328157
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.06376v1",
      "true_abstract": "Over-the-air federated learning (OTA-FL) offers an exciting new direction\nover classical FL by averaging model weights using the physics of analog signal\npropagation. Since each participant broadcasts its model weights concurrently\nin time and frequency, this paradigm conserves communication bandwidth and\nmodel upload latency. Despite its potential, there is no prior large-scale\ndemonstration on a real-world experimental platform. This paper proves for the\nfirst time that OTA-FL can be deployed in a cellular network setting within the\nconstraints of a 5G compliant waveform. To achieve this, we identify challenges\ncaused by multi-path fading effects, thermal noise at the radio devices, and\nmaintaining highly precise synchronization across multiple clients to perform\ncoherent OTA combining. To address these challenges, we propose a unified\nframework for real-time channel estimation, model weight to OFDM symbol mapping\nand dual-layer synchronization interface to perform OTA model training. We\nexperimentally validate OTA-FL using two relevant applications - Channel\nEstimation and Object Classification, at a large-scale on ORBIT Testbed and a\nportable setup respectively, along with analyzing the benefits from the\nperspective of a telecom operator. Under specific experimental conditions,\nOTA-FL achieves equivalent model performance, supplemented with 43 times\nimprovement in spectrum utilization and 7 times improvement in energy\nefficiency over classical FL when considering 5 nodes.",
      "generated_abstract": "r proposes a novel deep learning-based algorithm for predicting\nthe arrival time of radio-frequency (RF) signals in time-frequency (T-F)\nspectrum sensing (TSS) systems. The proposed algorithm is based on a\nrepresentation of the TSS problem as a binary classification problem, and\nemploys a Convolutional Neural Network (CNN) to model the TSS problem by\ncomparing the RF signal with a predefined reference signal. The CNN is\ntrained using the TSS data from a previously acquired experiment, and the\ntraining data is then used to predict the TSS results for unknown RF signals.\nThe performance of the proposed algorithm is evaluated using a simulated TSS\nsystem, and the results show that the algorithm can achieve a significantly\nhigher classification accuracy than traditional machine learning approaches.\nThe results also indicate that the proposed algorithm can be used to enhance",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1464968152866242,
          "p": 0.27380952380952384,
          "f": 0.1908713647533618
        },
        "rouge-2": {
          "r": 0.014925373134328358,
          "p": 0.02459016393442623,
          "f": 0.01857584669229192
        },
        "rouge-l": {
          "r": 0.12101910828025478,
          "p": 0.2261904761904762,
          "f": 0.15767634400647385
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.04924v2",
      "true_abstract": "The integration of artificial intelligence (AI) into the workplace is\nadvancing rapidly, necessitating robust metrics to evaluate its tangible impact\non the labour market. Existing measures of AI occupational exposure largely\nfocus on AI's theoretical potential to substitute or complement human labour on\nthe basis of technical feasibility, providing limited insight into actual\nadoption and offering inadequate guidance for policymakers. To address this\ngap, we introduce the AI Startup Exposure (AISE) index-a novel metric based on\noccupational descriptions from O*NET and AI applications developed by startups\nfunded by the Y Combinator accelerator. Our findings indicate that while\nhigh-skilled professions are theoretically highly exposed according to\nconventional metrics, they are heterogeneously targeted by startups. Roles\ninvolving routine organizational tasks-such as data analysis and office\nmanagement-display significant exposure, while occupations involving tasks that\nare less amenable to AI automation due to ethical or high-stakes, more than\nfeasibility, considerations -- such as judges or surgeons -- present lower AISE\nscores. By focusing on venture-backed AI applications, our approach offers a\nnuanced perspective on how AI is reshaping the labour market. It challenges the\nconventional assumption that high-skilled jobs uniformly face high AI risks,\nhighlighting instead the role of today's AI players' societal\ndesirability-driven and market-oriented choices as critical determinants of AI\nexposure. Contrary to fears of widespread job displacement, our findings\nsuggest that AI adoption will be gradual and shaped by social factors as much\nas by the technical feasibility of AI applications. This framework provides a\ndynamic, forward-looking tool for policymakers and stakeholders to monitor AI's\nevolving impact and navigate the changing labour landscape.",
      "generated_abstract": "This paper studies the effects of the 2016 U.S. presidential election on\nthe stock market and the employment of college graduates. We find that the\npresidential election is a significant determinant of stock market returns,\nwhile the presidential election has no significant impact on employment of\ncollege graduates. The election impact on stock market returns is driven by\nhigh-frequency trading, which is not impacted by the election. Our findings\ndemonstrate that the U.S. election does not affect the stock market in a\nsystematic way, and the effect of the election on employment of college\ngraduates is limited.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09444444444444444,
          "p": 0.3469387755102041,
          "f": 0.14847161235674383
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.05194805194805195,
          "f": 0.024316105837160203
        },
        "rouge-l": {
          "r": 0.09444444444444444,
          "p": 0.3469387755102041,
          "f": 0.14847161235674383
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/PF/2503.08973v1",
      "true_abstract": "Reducing the memory footprint of Machine Learning (ML) models, especially\nDeep Neural Networks (DNNs), is imperative to facilitate their deployment on\nresource-constrained edge devices. However, a notable drawback of DNN models\nlies in their susceptibility to adversarial attacks, wherein minor input\nperturbations can deceive them. A primary challenge revolves around the\ndevelopment of accurate, resilient, and compact DNN models suitable for\ndeployment on resource-constrained edge devices. This paper presents the\noutcomes of a compact DNN model that exhibits resilience against both black-box\nand white-box adversarial attacks. This work has achieved this resilience\nthrough training with the QKeras quantization-aware training framework. The\nstudy explores the potential of QKeras and an adversarial robustness technique,\nJacobian Regularization (JR), to co-optimize the DNN architecture through\nper-layer JR methodology. As a result, this paper has devised a DNN model\nemploying this co-optimization strategy based on Stochastic Ternary\nQuantization (STQ). Its performance was compared against existing DNN models in\nthe face of various white-box and black-box attacks. The experimental findings\nrevealed that, the proposed DNN model had small footprint and on average, it\nexhibited better performance than Quanos and DS-CNN MLCommons/TinyML (MLC/T)\nbenchmarks when challenged with white-box and black-box attacks, respectively,\non the CIFAR-10 image and Google Speech Commands audio datasets.",
      "generated_abstract": "We present a novel approach to the problem of estimating a parameter of a\nregression model from incomplete data. We consider a simple case where the\ndata are generated from a multivariate normal distribution, and we estimate the\nunknown parameter $\\theta$ from a finite set of data points, where each data\npoint is sampled from the same distribution. Our approach relies on the\nidentifiability of the parameter $\\theta$ and the covariance function of the\ndata generating distribution. We show that in this case, the estimated\nparameter is not only consistent but also asymptotically normal, and we provide\nan explicit expression for the asymptotic variance of the estimator. We also\nprove that the estimated parameter is asymptotically normal with respect to\nthe asymptotic distribution of the data generating distribution. We illustrate\nour approach on simulated and real-world data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10218978102189781,
          "p": 0.1891891891891892,
          "f": 0.13270141724669274
        },
        "rouge-2": {
          "r": 0.010362694300518135,
          "p": 0.016666666666666666,
          "f": 0.012779547987630488
        },
        "rouge-l": {
          "r": 0.10218978102189781,
          "p": 0.1891891891891892,
          "f": 0.13270141724669274
        }
      }
    },
    {
      "paper_id": "physics.space-ph.physics/space-ph/2503.07905v1",
      "true_abstract": "We present the results of the first multi-event study of the normalized\nreconnection rate integrating events spanning the three primary regimes of\nreconnection observed by the Magnetospheric Multiscale (MMS) mission. We\nutilize a new method for determining the normalized reconnection rate with\nfewer sources of uncertainty by estimating the diffusion region aspect ratio\nwith magnetic field gradients, which are very well measured by MMS. After\ndemonstrating our technique is valid in the guide field and asymmetric regimes\nof reconnection, we investigate any relationships between the normalized rate\non guide field, upstream magnetic field variability, and magnetic field and\ndensity asymmetry. Our results suggest that under typical magnetospheric\nconditions, the normalized reconnection rate is constant, which may be\nsignificant in predicting the terrestrial effects of space weather by providing\ninsight into the efficiency of solar wind-magnetospheric coupling.",
      "generated_abstract": "The first observations of the diffuse Galactic cosmic-ray (CR) flux from the\ngalactic plane were made by the ESRF-SIS-2 beamline in 1987. The SIS-2\nconfiguration, which consisted of a 35-cm-diameter double-screw target,\nallowed for the simultaneous observation of both the northern and southern\nhemispheres. This paper describes the technical details of the SIS-2\nconfiguration and the first observations of the diffuse CR flux. The\ncharacteristics of the observed fluxes are compared to theoretical models. The\nobserved fluxes are found to be consistent with the theoretical models for\nthe diffuse CR flux.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14130434782608695,
          "p": 0.23214285714285715,
          "f": 0.17567567097151215
        },
        "rouge-2": {
          "r": 0.024390243902439025,
          "p": 0.03896103896103896,
          "f": 0.029999995264500746
        },
        "rouge-l": {
          "r": 0.13043478260869565,
          "p": 0.21428571428571427,
          "f": 0.16216215745799867
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2410.13265v2",
      "true_abstract": "An automated market maker where the price can cross the zero bound into the\nnegative price domain with applications in electricity, energy, and derivatives\nmarkets is presented. A unique feature involves the ability to swap both\nnegatively and positively priced assets between one another, which unlike\ntraditional markets requires a numeraire in the form of a currency. Model\nextensions to skew and concentrate liquidity are shown. The liquidity\nfingerprint, payoff, and invariant are compared to the Black-Scholes covered\ncall and the Logarithmic Market Scoring Rule invariants.",
      "generated_abstract": "We propose a novel framework for financial risk management using the\ntrade-off between risk and profitability as a design principle. The framework\nintegrates a risk-sensitive production function with a risk-adjusted profit\nmeasure. We first establish a general framework for deriving the optimal\nproduction function and profit measure under a risk-sensitive production\nfunction. Second, we formulate the optimal risk-sensitive production function\nand risk-adjusted profit measure as a constrained optimization problem with\nthe optimal objective function and profit measure constrained by a risk\nsensitive production function. Third, we provide a gradient-based optimization\nalgorithm to solve the constrained optimization problem. Finally, we apply our\nframework to the case of a market maker and show that the market maker\nefficiently manages its risk while maximizing its profit.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14705882352941177,
          "p": 0.16129032258064516,
          "f": 0.1538461488568049
        },
        "rouge-2": {
          "r": 0.03529411764705882,
          "p": 0.0297029702970297,
          "f": 0.03225805955312829
        },
        "rouge-l": {
          "r": 0.1323529411764706,
          "p": 0.14516129032258066,
          "f": 0.13846153347218954
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2403.03862v1",
      "true_abstract": "In December 2023 the Florida State Seminoles became the first Power 5 school\nto have an undefeated season and miss selection for the College Football\nPlayoff. In order to assess this decision, we employed an Elo ratings model to\nrank the teams and found that the selection committee's decision was justified\nand that Florida State were not one of the four best teams in college football\nin that season (ranking only 11th!). We extended this analysis to all other\nyears of the CFP and found that the top four teams by Elo ratings differ\ngreatly from the four teams selected in almost every year of the CFP's\nexistence. Furthermore, we found that there have been more egregious\nnon-selections including when Alabama was ranked first by Elo ratings in 2022\nand were not selected. The analysis suggests that the current criteria are too\nsubjective and a ratings model should be implemented to provide transparency\nfor the sport, its teams, and its fans.",
      "generated_abstract": "p a unified framework for analyzing the effect of both the number\nof variables and the variance of the errors on the performance of machine\nlearning models. We show that the number of variables affects the performance\nof models in two ways: (1) by increasing the variance of the errors, which\nreduces the bias of the model and increases its accuracy; and (2) by\ndecreasing the variance of the errors, which decreases the bias of the model\nand increases its accuracy. We also show that the variance of the errors\naffects the performance of models in two ways: (1) by increasing the variance\nof the errors, which reduces the bias of the model and increases its accuracy;\nand (2) by decreasing the variance of the errors, which decreases the bias of\nthe model and increases its accuracy. We discuss the impact of the number of\nvariables and the variance of the errors on the performance of a model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.24444444444444444,
          "f": 0.1527777734809029
        },
        "rouge-2": {
          "r": 0.013888888888888888,
          "p": 0.028985507246376812,
          "f": 0.018779338342922402
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.24444444444444444,
          "f": 0.1527777734809029
        }
      }
    },
    {
      "paper_id": "cs.NI.cs/NI/2503.07935v1",
      "true_abstract": "Unmanned aerial vehicles (UAVs) enhance coverage and provide flexible\ndeployment in 5G and next-generation wireless networks. The performance of such\nwireless networks can be improved by developing new navigation and wireless\nadaptation approaches in digital twins (DTs). However, challenges such as\ncomplex propagation conditions and hardware complexities in real-world\nscenarios introduce a realism gap with the DTs. Moreover, while using real-time\nfull-stack protocols in DTs enables subsequent deployment and testing in a\nreal-world environment, development in DTs requires high computational\ncomplexity and involves a long development time. In this paper, to accelerate\nthe development cycle, we develop a measurement-calibrated Matlab-based\nsimulation framework to replicate performance in a full-stack UAV wireless\nnetwork DT. In particular, we use the DT from the NSF AERPAW platform and\ncompare its reports with those generated by our developed simulation framework\nin wireless networks with similar settings. In both environments, we observe\ncomparable results in terms of RSRP measurement, hence motivating iterative use\nof the developed simulation environment with the DT.",
      "generated_abstract": "In this work, we present the first systematic study of the behavior of\nthe AI-driven decision-making process of an intelligent driver assist system.\nWe propose a method to detect the presence of human drivers by using the\nGPS data from the vehicle and their corresponding driving behavior. We also\nintroduce a method to detect the presence of a vehicle-human interaction,\nincluding pedestrians, by using the GPS data and their corresponding driving\nbehavior. The presence of human drivers is detected by employing a\nmachine-learning algorithm, while the presence of a vehicle-human interaction\nis detected by employing the method proposed by Meng et al. [2023",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12844036697247707,
          "p": 0.24561403508771928,
          "f": 0.16867469428581808
        },
        "rouge-2": {
          "r": 0.02564102564102564,
          "p": 0.05194805194805195,
          "f": 0.03433475952329258
        },
        "rouge-l": {
          "r": 0.11009174311926606,
          "p": 0.21052631578947367,
          "f": 0.14457830874364944
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/IT/2503.08632v1",
      "true_abstract": "This study investigates secret-key generation for device authentication using\nphysical identifiers, such as responses from physical unclonable functions\n(PUFs). The system includes two legitimate terminals (encoder and decoder) and\nan eavesdropper (Eve), each with access to different measurements of the\nidentifier. From the device identifier, the encoder generates a secret key,\nwhich is securely stored in a private database, along with helper data that is\nsaved in a public database accessible by the decoder for key reconstruction.\nEve, who also has access to the public database, may use both her own\nmeasurements and the helper data to attempt to estimate the secret key and\nidentifier. Our setup focuses on authentication scenarios where channel\nstatistics are uncertain, with the involved parties employing multiple antennas\nto enhance signal reception. Our contributions include deriving inner and outer\nbounds on the optimal trade-off among secret-key, storage, and privacy-leakage\nrates for general discrete sources, and showing that these bounds are tight for\nGaussian sources.",
      "generated_abstract": "vances in deep learning have led to improved text-to-speech\n(TTS) performance. However, current systems often rely on large, pre-trained\nmodels to achieve high quality, requiring extensive data and computational\nresources. This paper introduces a novel approach for TTS generation using\nsmaller models that can be trained in a more efficient manner. Our method\ncombines text-to-text translation and self-attention, employing a 3-layer\nmultimodal transformer to generate speech from text. The model is trained\nend-to-end using only text, without requiring additional data or large-scale\npre-training. We evaluate our approach on the WAV2TTS benchmark, demonstrating\nthat our model can achieve comparable quality to larger models while\nconsiderably reducing training costs. Our model also shows strong performance\nacross diverse text-to-speech tasks, demonstrating its potential",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1565217391304348,
          "p": 0.18947368421052632,
          "f": 0.17142856647392307
        },
        "rouge-2": {
          "r": 0.012903225806451613,
          "p": 0.01680672268907563,
          "f": 0.01459853523229959
        },
        "rouge-l": {
          "r": 0.14782608695652175,
          "p": 0.17894736842105263,
          "f": 0.16190475695011353
        }
      }
    },
    {
      "paper_id": "math.LO.math/LO/2503.05360v1",
      "true_abstract": "Sandqvist's base-extension semantics (B-eS) for intuitionistic sentential\nlogic grounds meaning relative to bases (rather than, say, models), which are\narbitrary sets of permitted inferences over sentences. While his soundness\nproof is standard, his completeness proof, is quite unusual. It closely\nparallels a method introduced much earlier by Mints, who developed a\nresolution-based approach to intuitionistic logic using a systematic\ntranslation of formulas into sentential counterparts. In this short note, we\nhighlight the connection between these two approaches and show that the\nsoundness and completeness of B-eS follow directly from Mints' theorem. While\nthe result is modest, it reinforces the relevance of proof-search to\nproof-theoretic semantics and suggests that resolution methods have a deeper\nconceptual role in constructive reasoning than is often acknowledged.",
      "generated_abstract": "e a finite group and $A$ be a subgroup of $G$. A homomorphism $f:A\\to\nG$ is called a $G$-{\\em homotopy} if $f(a)f(b)=af(a^{-1}b)$ for all $a,b\\in A$.\nThis paper studies $G$-homotopic conjugacy classes of finite groups. We\nconsider conjugacy classes of subgroups of $G$ that are either abelian or\ntorsion-free, and prove that they are in bijection with $G$-homotopy classes of\n$G$-homotopic conjugacy classes of subgroups of $G$. We also study $G$-homotopic\nconjugacy classes of subgroups of $G$ that are not conjugate under the\n$G$-action. We prove that they are in bijection with conjugacy classes of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09278350515463918,
          "p": 0.16981132075471697,
          "f": 0.11999999543022241
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09278350515463918,
          "p": 0.16981132075471697,
          "f": 0.11999999543022241
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.05807v1",
      "true_abstract": "This paper introduces a novel multi-stage decision-making model that\nintegrates hypothesis testing and dynamic programming algorithms to address\ncomplex decision-making scenarios.Initially,we develop a sampling inspection\nscheme that controls for both Type I and Type II errors using a simple random\nsampling method without replacement,ensuring the randomness and\nrepresentativeness of the sample while minimizing selection bias.Through the\napplication of hypothesis testing theory,a hypothesis testing model concerning\nthe defect rate is established,and formulas for the approximate distribution of\nthe sample defect rate and the minimum sample size required under two different\nscenarios are derived. Subsequently,a multi-stage dynamic programming decision\nmodel is constructed.This involves defining the state transition functions and\nstage-specific objective functions,followed by obtaining six optimal decision\nstrategies under various conditions through backward recursion.The results\ndemonstrate the model's potent capability for multi-stage decision-making and\nits high interpretability,offering significant advantages in practical\napplications.",
      "generated_abstract": "r investigates the application of reinforcement learning (RL) to\ncontrol a network of robotic manipulators in a multi-robot system. The\nmulti-robot system comprises multiple robots, which are collectively responsible\nfor manipulating a common object. In this paper, we focus on the case where the\nobject is a cylinder. Robots are equipped with a robotic arm that can move\naround the cylinder, and each robot has its own unique position and orientation\n(rotation and translation). The objective is to maximize the number of\ncollisions between robots and the cylinder. The RL approach is applied to\noptimize the control inputs of the robotic arms in order to minimize the\nnumber of collisions between robots and the cylinder. The RL approach is\nfurther extended to optimize the position and orientation of the robotic arms in\norder to minimize the number of coll",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13861386138613863,
          "p": 0.18421052631578946,
          "f": 0.1581920854952282
        },
        "rouge-2": {
          "r": 0.029411764705882353,
          "p": 0.036036036036036036,
          "f": 0.032388659018834046
        },
        "rouge-l": {
          "r": 0.12871287128712872,
          "p": 0.17105263157894737,
          "f": 0.14689265046697966
        }
      }
    },
    {
      "paper_id": "eess.SP.math/FA/2503.10274v1",
      "true_abstract": "This paper devotes to combine the chirp basis function transformation and\nsymplectic coordinates transformation to yield a novel Wigner distribution (WD)\nassociated with the linear canonical transform (LCT), named as the symplectic\nWD in the LCT domain (SWDL). It incorporates the merits of the symplectic WD\n(SWD) and the WD in the LCT domain (WDL), achieving stronger capability in the\nlinear frequency-modulated (LFM) signal frequency rate feature extraction while\nmaintaining the same level of computational complexity. Some essential\nproperties of the SWDL are derived, including marginal distributions, energy\nconservations, unique reconstruction, Moyal formula, complex conjugate\nsymmetry, time reversal symmetry, scaling property, time translation property,\nfrequency modulation property, and time translation and frequency modulation\nproperty. Heisenberg's uncertainty principles of the SWDL are formulated,\ngiving rise to three kinds of lower bounds attainable respectively by Gaussian\nenveloped complex exponential signal, Gaussian signal and Gaussian enveloped\nchirp signal. The optimal symplectic matrices corresponding to the highest\ntime-frequency resolution are generated by solving the lower bound optimization\n(minimization) problem. The time-frequency resolution of the SWDL is compared\nwith those of the SWD and WDL to demonstrate its superiority in LFM signals\ntime-frequency energy concentration. A synthesis example is also carried out to\nverify the feasibility and reliability of the theoretical analysis.",
      "generated_abstract": "the estimation of a Gaussian process (GP) in the presence of a\ngiven covariance function and unknown mean function. This problem is of\nparticular interest in many scientific fields, including geophysics, ocean\nphysics, atmospheric sciences, etc. The GP is assumed to be a function of time\nand is described by a general Gaussian distribution, which is parameterized by\na mean function, a covariance function, and a finite number of parameters. We\nstudy the estimation of this GP in the presence of a known mean function. We\npropose to estimate the mean function and covariance function by fitting a\ngeneral GP to a given set of observed data. The fitting process consists of\nestimating the mean function and covariance function by minimizing a\nclassification criterion, which is based on a given criterion of the GP\npredictive distribution. We further propose to use the obtained mean function",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12213740458015267,
          "p": 0.2318840579710145,
          "f": 0.15999999548050012
        },
        "rouge-2": {
          "r": 0.010752688172043012,
          "p": 0.016666666666666666,
          "f": 0.013071890657441184
        },
        "rouge-l": {
          "r": 0.11450381679389313,
          "p": 0.21739130434782608,
          "f": 0.14999999548050014
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2502.09860v1",
      "true_abstract": "Molecular discovery has brought great benefits to the chemical industry.\nVarious molecule design techniques are developed to identify molecules with\ndesirable properties. Traditional optimization methods, such as genetic\nalgorithms, continue to achieve state-of-the-art results across multiple\nmolecular design benchmarks. However, these techniques rely solely on random\nwalk exploration, which hinders both the quality of the final solution and the\nconvergence speed. To address this limitation, we propose a novel approach\ncalled Gradient Genetic Algorithm (Gradient GA), which incorporates gradient\ninformation from the objective function into genetic algorithms. Instead of\nrandom exploration, each proposed sample iteratively progresses toward an\noptimal solution by following the gradient direction. We achieve this by\ndesigning a differentiable objective function parameterized by a neural network\nand utilizing the Discrete Langevin Proposal to enable gradient guidance in\ndiscrete molecular spaces. Experimental results demonstrate that our method\nsignificantly improves both convergence speed and solution quality,\noutperforming cutting-edge techniques. For example, it achieves up to a 25%\nimprovement in the top-10 score over the vanilla genetic algorithm. The code is\npublicly available at https://github.com/debadyuti23/GradientGA.",
      "generated_abstract": "of the functional consequences of altered gene expression in\ncells, especially when it is not due to a genetic defect, has become a key\narea of research in modern biology. A common approach to understanding these\nprocesses is to investigate the influence of different experimental conditions\non gene expression, but this approach suffers from two major limitations. First,\nit is not always possible to control the experimental conditions, and second,\nthe experimental conditions may not be ideal for the study of gene\nexpression. In this work, we introduce a novel approach that enables the\nstudy of the functional consequences of gene expression in cells without\nrelying on experimental conditions. This approach relies on the idea of\nreconstructing the functional consequences of gene expression from the\nobservation of gene expression itself. We introduce a mathematical model for\nthis reconstruction that is based on the observation of the expression of a\ngene in response to a set of conditions. Our",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1417910447761194,
          "p": 0.23170731707317074,
          "f": 0.17592592121570658
        },
        "rouge-2": {
          "r": 0.028901734104046242,
          "p": 0.03875968992248062,
          "f": 0.03311257788759338
        },
        "rouge-l": {
          "r": 0.13432835820895522,
          "p": 0.21951219512195122,
          "f": 0.1666666619564473
        }
      }
    },
    {
      "paper_id": "physics.gen-ph.physics/gen-ph/2503.09615v1",
      "true_abstract": "We proceed to the canonical quantization of the complex scalar field without\nmaking use of its real and imaginary parts. Our motivation is to formally\nconnect, as tightly as possible, the quantum-field notions of particle and\nantiparticle - most prominently represented, formally, by the creation and\nannihilation operators - to the initial classical field theory - whose main\nformal object is the field amplitude at a given spacetime point. Our point of\nview is that doing this via the use of the real and imaginary parts of the\nfield is not satisfying. The derivation demands to consider, just before\nquantization, the field and its complex conjugate as independent fields, which\nyields a system of two copies of independent complex scalar fields. One then\nproceeds to quantization with these two copies, which leads to the introduction\nof two families of creation and annihilation operators, corresponding to\nparticles on the one hand, and antiparticles on the other hand. One realizes\nthat having two such families is the only hope for being able to \"invert\" the\ndefinitions of the creation and annihilation in terms of the Fourier quantized\nfields, so as to obtain an expression of the direct-space fields in terms of\nthese creation and annihilation operators, because the real-field condition\nused in the case of a real scalar field does not hold for a complex scalar\nfield. This hope is then met by introducing the complex-conjugate constraint at\nthe quantum level, that is, that the second independent field copy is actually\nthe complex conjugate of the first. All standard results are then recovered in\na rigorous and purely deductive way. While we reckon our derivation exists in\nthe literature, we have not found it.",
      "generated_abstract": "r introduces a novel formulation of the Boltzmann-Ulam-Stanley\n(BUS) model, which is a generalization of the well-known Boltzmann-Gibbs (BG)\nmodel. The BUS model has been studied extensively in the literature, and its\nvariants have been used for applications in fluid mechanics, statistical\nmechanics, and many other fields. However, most existing BUS models are\ndesigned for a single initial condition. In this paper, we propose a new BUS\nmodel that incorporates multiple initial conditions and enables the model to\nsimulate multiple fluid configurations with a single run. This new BUS model\nprovides a flexible framework for analyzing complex fluid dynamics problems.\nFurthermore, it offers a novel approach for modeling fluid flow and turbulence\nin multi-dimensional geometries, which has not been explored in the literature\nto date",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18791946308724833,
          "p": 0.3333333333333333,
          "f": 0.24034334302860624
        },
        "rouge-2": {
          "r": 0.020242914979757085,
          "p": 0.042735042735042736,
          "f": 0.02747252311028327
        },
        "rouge-l": {
          "r": 0.1610738255033557,
          "p": 0.2857142857142857,
          "f": 0.2060085790801084
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2502.20852v1",
      "true_abstract": "Magnetic Resonance Imaging (MRI) Super-Resolution (SR) addresses the\nchallenges such as long scan times and expensive equipment by enhancing image\nresolution from low-quality inputs acquired in shorter scan times in clinical\nsettings. However, current SR techniques still have problems such as limited\nability to capture both local and global static patterns effectively and\nefficiently. To address these limitations, we propose Delta-WKV, a novel MRI\nsuper-resolution model that combines Meta-in-Context Learning (MiCL) with the\nDelta rule to better recognize both local and global patterns in MRI images.\nThis approach allows Delta-WKV to adjust weights dynamically during inference,\nimproving pattern recognition with fewer parameters and less computational\neffort, without using state-space modeling. Additionally, inspired by\nReceptance Weighted Key Value (RWKV), Delta-WKV uses a quad-directional\nscanning mechanism with time-mixing and channel-mixing structures to capture\nlong-range dependencies while maintaining high-frequency details. Tests on the\nIXI and fastMRI datasets show that Delta-WKV outperforms existing methods,\nimproving PSNR by 0.06 dB and SSIM by 0.001, while reducing training and\ninference times by over 15\\%. These results demonstrate its efficiency and\npotential for clinical use with large datasets and high-resolution imaging.",
      "generated_abstract": "r presents a novel method to enhance the spatial resolution of\nenhanced computed tomography (eCT) images by using a low-rank denoising\napproach. The proposed method consists of two main stages: an initial denoising\nstep and a subsequent deblurring stage. The initial denoising step employs a\nrandomized eigenvalue-based denoising technique, which is applied to the\nreconstructed eCT image and the noisy reconstructed eCT image to remove the\nnoise. The deblurring stage uses a deblurring model that predicts the blurred\nimage of the noisy eCT image based on the reconstructed eCT image. The\ndeblurring model is trained using a triplet loss, which encourages the model to\npredict the blurred image as close as possible to the ground truth. The\ntriplet loss is computed using the re",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11267605633802817,
          "p": 0.24242424242424243,
          "f": 0.15384614951368356
        },
        "rouge-2": {
          "r": 0.02247191011235955,
          "p": 0.037383177570093455,
          "f": 0.02807017074890814
        },
        "rouge-l": {
          "r": 0.1056338028169014,
          "p": 0.22727272727272727,
          "f": 0.14423076489829895
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2408.06683v1",
      "true_abstract": "The mechanical properties within living cells play a critical role in the\nadaptive regulation of their biological functions upon environmental and\ninternal stimuli. While these properties exhibit nonequilibrium dynamics due to\nthe thermal and nonthermal forces that universally coexist in\nactin-myosin-active proliferative cells, quantifying them within such complex\nsystems remains challenging. Here, we develop a nonequilibrium framework that\ncombines fluorescence correlation spectroscopy (FCS) measurements of\nintracellular diffusion with nonequilibrium theory to quantitatively analyze\ncell-specific nonthermal driving forces and cellular adaptability. Our results\nreveal that intracellular particle diffusion is influenced not only by common\nthermal forces but also by nonthermal forces generated by approximately 10-100\nmotor proteins. Furthermore, we derive a physical parameter that quantitatively\nassesses the sensitivity of intracellular particle responses to these\nnonthermal forces, showing that systems with more active diffusion exhibit\nhigher response sensitivity. Our work highlights the biological fluctuations\narising from multiple interacting elements, advancing the understanding of the\ncomplex mechanical properties within living cells.",
      "generated_abstract": "of complex biological systems is challenging due to the\ncomplexity of the physical and chemical interactions involved, as well as\ntheir interdependence. The combination of advanced mathematical techniques\nwith computational methods is essential for a better understanding of these\nsystems. In this work, we present a novel framework for studying the\nstochastic thermodynamic behavior of complex biological systems. We define a\ngeneral model of a stochastic thermodynamic system in which a stochastic\nvariable is coupled to a system of coupled thermodynamic variables. We\ninvestigate the behavior of this system through the study of its\nstochastic thermodynamic behavior, i.e., the distribution of thermodynamic\nvariables and the corresponding thermodynamic entropy. We also investigate the\nbehavior of the system when the system is coupled to a stochastic variable. We\nshow that the system behaves as a stochastic thermodynamic system when the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20754716981132076,
          "p": 0.3142857142857143,
          "f": 0.24999999520919433
        },
        "rouge-2": {
          "r": 0.02666666666666667,
          "p": 0.034782608695652174,
          "f": 0.03018867433250347
        },
        "rouge-l": {
          "r": 0.1792452830188679,
          "p": 0.2714285714285714,
          "f": 0.21590908611828522
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.03860v1",
      "true_abstract": "Covariate balancing is a popular technique for controlling confounding in\nobservational studies. It finds weights for the treatment group which are close\nto uniform, but make the group's covariate means (approximately) equal to those\nof the entire sample. A crucial question is: how approximate should the\nbalancing be, in order to minimize the error of the final estimate? Current\nguidance is derived from heuristic or asymptotic analyses, which are\nuninformative when the size of the sample is small compared to the number of\ncovariates. This paper presents the first rigorous, nonasymptotic analysis of\ncovariate balancing; specifically, we use PAC-Bayesian techniques to derive\nvalid, finite-sample confidence intervals for the treatment effect. More\ngenerally, we prove these guarantees for a flexible form of covariate balancing\nwhere the regularization parameters weighting the tradeoff between bias\n(imbalance) and variance (divergence from uniform) are optimized, not fixed.\nThis gives rise to a new balancing algorithm which empirically delivers\nsuperior adaptivity. Our overall contribution is to make covariate balancing a\nmore reliable method for causal inference.",
      "generated_abstract": "aper, we propose a novel framework for modeling non-Gaussian\nnon-linear time series. Our approach is based on the so-called SDE-based\nstatistical models and the so-called Gaussian process-based\nrepresentation-based models. The former is an extension of the\nCauchy-type models, and the latter is a generalization of the Gaussian\nprocess models. We show that the SDE-based models are a special case of the\nGaussian process-based models, and therefore, the SDE-based models can be\nrepresented by the Gaussian process-based models through a suitable\nrepresentation. The Gaussian process-based models are also called Gaussian\nprocess-based statistical models. As a consequence, we propose a unified\nframework for modeling non-Gaussian non-linear time series, and it is\nconcise and simple to use. We provide examples and illustrate the advantages\nof",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09243697478991597,
          "p": 0.16923076923076924,
          "f": 0.11956521282195197
        },
        "rouge-2": {
          "r": 0.012345679012345678,
          "p": 0.02040816326530612,
          "f": 0.0153846106875754
        },
        "rouge-l": {
          "r": 0.08403361344537816,
          "p": 0.15384615384615385,
          "f": 0.10869564760456069
        }
      }
    },
    {
      "paper_id": "cs.CY.q-fin/EC/2503.00632v1",
      "true_abstract": "Improving social welfare is a complex challenge requiring policymakers to\noptimize objectives across multiple time horizons. Evaluating the impact of\nsuch policies presents a fundamental challenge, as those that appear suboptimal\nin the short run may yield significant long-term benefits. We tackle this\nchallenge by analyzing the long-term dynamics of two prominent policy\nframeworks: Rawlsian policies, which prioritize those with the greatest need,\nand utilitarian policies, which maximize immediate welfare gains. Conventional\nwisdom suggests these policies are at odds, as Rawlsian policies are assumed to\ncome at the cost of reducing the average social welfare, which their\nutilitarian counterparts directly optimize. We challenge this assumption by\nanalyzing these policies in a sequential decision-making framework where\nindividuals' welfare levels stochastically decay over time, and policymakers\ncan intervene to prevent this decay. Under reasonable assumptions, we prove\nthat interventions following Rawlsian policies can outperform utilitarian\npolicies in the long run, even when the latter dominate in the short run. We\ncharacterize the exact conditions under which Rawlsian policies can outperform\nutilitarian policies. We further illustrate our theoretical findings using\nsimulations, which highlight the risks of evaluating policies based solely on\ntheir short-term effects. Our results underscore the necessity of considering\nlong-term horizons in designing and evaluating welfare policies; the true\nefficacy of even well-established policies may only emerge over time.",
      "generated_abstract": "t a framework for learning and modeling credit risk using deep\nlearning techniques. We propose a novel framework that integrates machine\nlearning models with stochastic differential equations (SDEs) to learn credit\nrisk from observed market data. The framework is built on the assumption that\nthe underlying stochastic dynamics are well described by an SDE. This framework\nis applied to the U.S. credit default market, where we use high-frequency\nmarket data to construct a Markov chain of SDEs for the credit default\nprobability. This allows us to build a credit default model that is trained\nusing both historical market data and a generative model that simulates\nhigh-frequency credit market data. Our results show that the generative model\ncan outperform a traditional autoregressive model in predicting credit default\nprobabilities, and that the generative model can also be used to simulate\nhigh-frequency credit market data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17037037037037037,
          "p": 0.27710843373493976,
          "f": 0.21100916959641453
        },
        "rouge-2": {
          "r": 0.009900990099009901,
          "p": 0.016129032258064516,
          "f": 0.012269933936544396
        },
        "rouge-l": {
          "r": 0.15555555555555556,
          "p": 0.25301204819277107,
          "f": 0.19266054574320354
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.04908v1",
      "true_abstract": "This paper presents a novel dissipativity-based distributed droop-free\ncontrol approach for voltage regulation and current sharing in DC microgrids\n(MGs) comprised of an interconnected set of distributed generators (DGs),\nloads, and power lines. First, we describe the closed-loop DC MG as a networked\nsystem where the DGs and lines (i.e., subsystems) are interconnected via a\nstatic interconnection matrix. This interconnection matrix demonstrates how the\ninputs, outputs, and disturbances of DGs and lines are connected in a DC MG.\nEach DG has a local controller and a distributed global controller. To design\nthe controller, we use the dissipativity properties of the subsystems and\nformulate a linear matrix inequality (LMI) problem. To support the feasibility\nof this problem, we identify a set of necessary local and global conditions\nthat we then enforce in a specifically developed LMI-based controller design\nprocess. In contrast to existing DC MG control solutions, our approach proposes\na unified framework for co-designing the distributed controller and\ncommunication topology. As the co-design process is LMI-based, it can be\nefficiently implemented and evaluated. The effectiveness of the proposed\nsolution can be verified by simulating an islanded DC MG in a MATLAB/Simulink\nenvironment under different scenarios, such as load changes and topological\nconstraint changes, and then comparing the performance with a recent droop\ncontrol algorithm.",
      "generated_abstract": "aper, we present a novel method for control of a two-degree-of-\nfreedom (2-Dof) manipulator using a single-quadrotor drone. The drone is\nsupplied with a laser rangefinder (LRF) to provide information about the\nposition and orientation of the manipulator, and a state estimator to provide\nthe state of the manipulator. The drone is equipped with a low-cost laser\nrangefinder (LRF) for position and orientation information. The drone is\nequipped with a low-cost camera to provide an additional state of the manipulator\nand the drone. The drone is equipped with a laser rangefinder (LRF) to provide\ninformation about the position and orientation of the manipulator. The drone is\nequipped with a low-cost camera to provide an additional state of the manipulator\nand the dr",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0962962962962963,
          "p": 0.325,
          "f": 0.14857142504489804
        },
        "rouge-2": {
          "r": 0.019704433497536946,
          "p": 0.0625,
          "f": 0.029962543171597744
        },
        "rouge-l": {
          "r": 0.08148148148148149,
          "p": 0.275,
          "f": 0.1257142821877552
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.01080v1",
      "true_abstract": "We introduce a new dynamic factor correlation model with a novel\nvariation-free parametrization of factor loadings. The model is applicable to\nhigh dimensions and can accommodate time-varying correlations, heterogeneous\nheavy-tailed distributions, and dependent idiosyncratic shocks, such as those\nobserved in returns on stocks in the same subindustry. We apply the model to a\n\"small universe\" with 12 asset returns and to a \"large universe\" with 323 asset\nreturns. The former facilitates a comprehensive empirical analysis and\ncomparisons and the latter demonstrates the flexibility and scalability of the\nmodel.",
      "generated_abstract": "uce a novel approach for estimating a stochastic volatility model\nwith a multivariate Ornstein-Uhlenbeck process (OU) of unknown mean. This\napproach is based on the idea of using a non-linear regression model to estimate\nthe Ornstein-Uhlenbeck process and the resulting stochastic volatility. We\nderive the closed-form expressions for the mean and covariance of the\nestimated Ornstein-Uhlenbeck process. Furthermore, we derive the asymptotic\nnormality of the estimated mean and covariance of the Ornstein-Uhlenbeck\nprocess. We apply our method to the estimation of a stochastic volatility model\nwith a multivariate OU of unknown mean. Our results indicate that the\nestimated Ornstein-Uhlenbeck process is asymptotically normally distributed and\nthe resulting stochastic volatility is consistent with the Orn",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2033898305084746,
          "p": 0.2222222222222222,
          "f": 0.2123893755407629
        },
        "rouge-2": {
          "r": 0.08433734939759036,
          "p": 0.08139534883720931,
          "f": 0.08284023168796642
        },
        "rouge-l": {
          "r": 0.1694915254237288,
          "p": 0.18518518518518517,
          "f": 0.17699114545226732
        }
      }
    },
    {
      "paper_id": "hep-th.math/QA/2503.10469v1",
      "true_abstract": "We introduce a novel machine learning based framework for discovering\nintegrable models. Our approach first employs a synchronized ensemble of neural\nnetworks to find high-precision numerical solution to the Yang-Baxter equation\nwithin a specified class. Then, using an auxiliary system of algebraic\nequations, [Q_2, Q_3] = 0, and the numerical value of the Hamiltonian obtained\nvia deep learning as a seed, we reconstruct the entire Hamiltonian family,\nforming an algebraic variety. We illustrate our presentation with three- and\nfour-dimensional spin chains of difference form with local interactions.\nRemarkably, all discovered Hamiltonian families form rational varieties.",
      "generated_abstract": "aper, we construct the first explicit quantum field theory (QFT)\nfor a class of theories defined on the noncommutative space $A_\\infty$-\nalgebras. This is a $2d$ QFT defined on a noncommutative space $A_\\infty$\nendowed with a noncommutative Poisson structure and equipped with a $2d$\nHamiltonian. The algebra $A_\\infty$ is generated by a family of $2d$\nnoncommutative coordinates, each endowed with a noncommutative Poisson bracket.\nWe then prove that the noncommutative coordinates are precisely the coordinates\nof the noncommutative space, while the noncommutative Poisson bracket is the\ncommutative one, hence it is compatible with the $2d$ Poisson structure.\nMoreover, we prove that the Hamiltonian is given by a $2d$ Hamiltonian in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14666666666666667,
          "p": 0.1896551724137931,
          "f": 0.16541352891627578
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.02247191011235955,
          "f": 0.02185791850100147
        },
        "rouge-l": {
          "r": 0.12,
          "p": 0.15517241379310345,
          "f": 0.13533834094635103
        }
      }
    },
    {
      "paper_id": "physics.geo-ph.physics/ao-ph/2503.04227v1",
      "true_abstract": "We present the first extensive analysis of K/Ka-band ranging post-fit\nresiduals of an official Level-2 product, characterised as Line-of-Sight\nGravity Differences (LGD), which exhibit and showcase interesting sub-monthly\ngeophysical signals. These residuals, provided by CSR, were derived from the\ndifference between spherical harmonic coefficient least-squares fits and\nreduced Level-1B range-rate observations. We classified the geophysical signals\ninto four distinct categories: oceanic, meteorological, hydrological, and solid\nEarth, focusing primarily on the first three categories in this study. In our\nexamination of oceanic processes, we identified notable mass anomalies in the\nArgentine basin, specifically within the Zapiola Rise, where persistent\nremnants of the rotating dipole-like modes are evident in the LGD post-fit\nresiduals. Our analysis extended to the Gulf of Carpentaria and Australia\nduring the 2013 Oswald cyclone, revealing significant LGD residual anomalies\nthat correlate with cyclone tracking and precipitation data. Additionally, we\ninvestigated the monsoon seasons in Bangladesh, particularly from June to\nSeptember 2007, where we observed peaks in sub-monthly variability. These\nfindings were further validated by demonstrating high spatial and temporal\ncorrelations between gridded LGD residuals and ITSG-Grace2018 daily solutions.\nGiven that these anomalies are associated with significant mass change\nphenomena, it is essential to integrate the post-fit residuals into a\nhigh-frequency mass change framework, with the purpose of providing enhanced\nspatial resolution compared to conventional Kalman-filtered methods.",
      "generated_abstract": "tudy, a novel type of aerosol particles that exhibit a strong\neffect on the scattering of solar radiation and hence on the Earth's climate is\npresented. It is based on a special type of clays, called \"Sun Clay\". The\nclay is a highly porous material that can absorb large amounts of moisture\nduring the dry season, and release it during the wet season. This phenomenon\nis termed as the \"wet season effect\". The main aim of this study is to\ninvestigate the relationship between the westerly winds and the aerosol\nparticles, which have a strong effect on the Earth's climate. The results\nindicate that the westerly winds have a strong impact on the amount of aerosol\nparticles that are present in the atmosphere. The study also shows that the\nwet season effect can be used to predict the amount of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12258064516129032,
          "p": 0.24358974358974358,
          "f": 0.16309012430142397
        },
        "rouge-2": {
          "r": 0.018957345971563982,
          "p": 0.03418803418803419,
          "f": 0.02439023931309575
        },
        "rouge-l": {
          "r": 0.09032258064516129,
          "p": 0.1794871794871795,
          "f": 0.1201716693658017
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.10009v1",
      "true_abstract": "We introduce a double/debiased machine learning (DML) estimator for the\nimpulse response function (IRF) in settings where a time series of interest is\nsubjected to multiple discrete treatments, assigned over time, which can have a\ncausal effect on future outcomes. The proposed estimator can rely on fully\nnonparametric relations between treatment and outcome variables, opening up the\npossibility to use flexible machine learning approaches to estimate IRFs. To\nthis end, we extend the theory of DML from an i.i.d. to a time series setting\nand show that the proposed DML estimator for the IRF is consistent and\nasymptotically normally distributed at the parametric rate, allowing for\nsemiparametric inference for dynamic effects in a time series setting. The\nproperties of the estimator are validated numerically in finite samples by\napplying it to learn the IRF in the presence of serial dependence in both the\nconfounder and observation innovation processes. We also illustrate the\nmethodology empirically by applying it to the estimation of the effects of\nmacroeconomic shocks.",
      "generated_abstract": "This paper studies a model in which the heterogeneous agents' data is\nrepeatedly observed under a common design, and the design is used to infer\nimportant parameters in the model. In particular, we consider a repeated\ndesign model in which each agent observes an unobserved heterogeneity of\ninterest, and each agent observes a common design that is repeated multiple\ntimes. We show that the model is identifiable if and only if the common design\nhas a sufficient statistic, and if the common design is sufficiently\nrepresentative of the population. We further show that the design is\nsufficiently representative if and only if the common design is\nsufficiently smooth. We show that the design is sufficiently smooth if and only\nif the common design is sufficiently smooth and sufficiently smoothly\ncontinuous.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12844036697247707,
          "p": 0.2545454545454545,
          "f": 0.17073170285916134
        },
        "rouge-2": {
          "r": 0.025974025974025976,
          "p": 0.045454545454545456,
          "f": 0.03305784661157091
        },
        "rouge-l": {
          "r": 0.11926605504587157,
          "p": 0.23636363636363636,
          "f": 0.15853658090794182
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/MF/2411.12375v3",
      "true_abstract": "In this paper, we introduce a novel pricing model for Uniswap V3, built upon\nstochastic processes and the Martingale Stopping Theorem. This model\ninnovatively frames the valuation of positions within Uniswap V3. We further\nconduct a numerical analysis and examine the sensitivities through Greek risk\nmeasures to elucidate the model's implications. The results underscore the\nmodel's significant academic contribution and its practical applicability for\nUniswap liquidity providers, particularly in assessing risk exposure and\nguiding hedging strategies.",
      "generated_abstract": "We propose a novel method for pricing American options on a portfolio of\ncompanies. We model the portfolio as a collection of independent Markov\nprocesses, and derive a pricing formula for American options on the\nunderlying portfolio. We derive the solution to the HJB equation, and then\nprove that the optimal solution to this problem is a solution to the\ncharacteristic problem of the Markov process modeling the portfolio. This\nresult extends the existing results on the pricing of American options on\nportfolios of individual stocks, and provides a new tool for pricing American\noptions on portfolios of companies.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20967741935483872,
          "p": 0.2653061224489796,
          "f": 0.23423422930281648
        },
        "rouge-2": {
          "r": 0.0136986301369863,
          "p": 0.012195121951219513,
          "f": 0.012903220823310977
        },
        "rouge-l": {
          "r": 0.20967741935483872,
          "p": 0.2653061224489796,
          "f": 0.23423422930281648
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/RO/2503.10484v1",
      "true_abstract": "Existing quadrupedal locomotion learning paradigms usually rely on extensive\ndomain randomization to alleviate the sim2real gap and enhance robustness. It\ntrains policies with a wide range of environment parameters and sensor noises\nto perform reliably under uncertainty. However, since optimal performance under\nideal conditions often conflicts with the need to handle worst-case scenarios,\nthere is a trade-off between optimality and robustness. This trade-off forces\nthe learned policy to prioritize stability in diverse and challenging\nconditions over efficiency and accuracy in ideal ones, leading to overly\nconservative behaviors that sacrifice peak performance. In this paper, we\npropose a two-stage framework that mitigates this trade-off by integrating\npolicy learning with imagined transitions. This framework enhances the\nconventional reinforcement learning (RL) approach by incorporating imagined\ntransitions as demonstrative inputs. These imagined transitions are derived\nfrom an optimal policy and a dynamics model operating within an idealized\nsetting. Our findings indicate that this approach significantly mitigates the\ndomain randomization-induced negative impact of existing RL algorithms. It\nleads to accelerated training, reduced tracking errors within the distribution,\nand enhanced robustness outside the distribution.",
      "generated_abstract": "asing prevalence of Autonomous Vehicles (AVs) presents challenges in\nadhering to the increasingly complex and diverse requirements of the European\nRoad Traffic Regulation (ETR). Traditional approaches to regulation development\nhave focused on the definition of technical requirements and have relied on\nstandardised documentation, making it difficult to adapt to the rapid\nevolution of AV technology. This paper presents a novel framework for the\ndevelopment of AV regulation based on the concept of \"Regulatory Fitness\",\nwhich addresses the complexities of the evolving requirements of AVs in a\npractical and efficient manner. The framework consists of four phases:\nDefinitions, Regulatory Fitness Assessment, Regulatory Fitness Evaluation, and\nRegulatory Fitness Implementation. The framework is illustrated through the\ndevelopment of the European Road Traffic Regulation (E-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08870967741935484,
          "p": 0.14864864864864866,
          "f": 0.11111110642995632
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08064516129032258,
          "p": 0.13513513513513514,
          "f": 0.10101009632894625
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2411.07986v2",
      "true_abstract": "A fundamental question in the field of molecular computation is what\ncomputational tasks a biochemical system can carry out. In this work, we focus\non the problem of finding the maximum likelihood estimate (MLE) for log-affine\nmodels. We revisit a construction due to Gopalkrishnan of a mass-action system\nwith the MLE as its unique positive steady state, which is based on choosing a\nbasis for the kernel of the design matrix of the model. We extend this\nconstruction to allow for any finite spanning set of the kernel, and explore\nhow the choice of spanning set influences the dynamics of the resulting\nnetwork, including the existence of boundary steady states, the deficiency of\nthe network, and the rate of convergence. In particular, we prove that using a\nMarkov basis as the spanning set guarantees global stability of the MLE steady\nstate.",
      "generated_abstract": "criptomic responses to viral infections are largely driven by\nthe host's response to the viral infection. Therefore, the transcriptomic\nresponses to viral infections can be used as a proxy for the host response to\nthe virus. This work introduces a novel framework to predict the transcriptomic\nresponses to viral infections using only the host response to the virus. The\nframework employs a novel methodology to model the host response to the virus\nby learning the interaction between viral gene expression and host response. The\nframework is tested on data collected from the AstraZeneca COVID-19 vaccine\ntrials. The results of the framework show that it can successfully predict\ntranscriptomic responses to viral infections with an accuracy of 80.4% and\noutperform other approaches in the literature, including a convolutional neural\nnetwork, a support vector",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19318181818181818,
          "p": 0.22972972972972974,
          "f": 0.20987653824721852
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.02857142857142857,
          "f": 0.025316450761096936
        },
        "rouge-l": {
          "r": 0.14772727272727273,
          "p": 0.17567567567567569,
          "f": 0.16049382219783587
        }
      }
    },
    {
      "paper_id": "physics.app-ph.physics/app-ph/2503.10139v1",
      "true_abstract": "We present an optomechanical device platform for characterization of optical,\nthermal, and rheological properties of fluids on the micron scale. A suspended\nsilicon microdisk resonator with a vibrating mass of 100 fg and an effective\nmeasurement volume of less than a pL is used to monitor properties of different\nfluids at rest. By employing analytical models for thermo-optical effects,\nthermal diffusion and fluid-structure interactions, our platform determines the\nrefractive index, thermal conductivity, viscosity, density and compressibility\nof the fluid, in a compact measurement setup. A single measurement takes as\nshort as 70 microseconds, and the employed power can be less than 100\nmicrowatts, guaranteeing measurement at rest and in thermal equilibrium.",
      "generated_abstract": "of the influence of the magnetic field on the motion of charged\nobjects has a long history. The initial work by Muller and Winkler on the\nmotion of charged particles in a uniform magnetic field was published in 1929,\nand the first detailed analysis of the interaction between the field and a\ncharged particle was performed in 1941 by Segr\u00e8 and Weiner. In 1947, the\ntheory of charged particles was further developed by Landau and Lifshitz, who\nintroduced the concept of the electric field gradient, which is the first\nprediction of the existence of the so-called electric field singularities. In\nthe 1970s, the theory of the electric field singularities was further developed\nby the authors of the present work, who introduced the concept of the\nelectric field singularity cone, and derived a set of differential equations\nfor the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11538461538461539,
          "p": 0.13846153846153847,
          "f": 0.12587412091544842
        },
        "rouge-2": {
          "r": 0.037383177570093455,
          "p": 0.03773584905660377,
          "f": 0.03755868044612026
        },
        "rouge-l": {
          "r": 0.10256410256410256,
          "p": 0.12307692307692308,
          "f": 0.11188810692943442
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.04265v1",
      "true_abstract": "Regression discontinuity (RD) designs typically identify the treatment effect\nat a single cutoff point. But when and how can we learn about treatment effects\naway from the cutoff? This paper addresses this question within a\nmultiple-cutoff RD framework. We begin by examining the plausibility of the\nconstant bias assumption proposed by Cattaneo, Keele, Titiunik, and\nVazquez-Bare (2021) through the lens of rational decision-making behavior,\nwhich suggests that a kind of similarity between groups and whether individuals\ncan influence the running variable are important factors. We then introduce an\nalternative set of assumptions and propose a broadly applicable partial\nidentification strategy. The potential applicability and usefulness of the\nproposed bounds are illustrated through two empirical examples.",
      "generated_abstract": "We introduce a novel framework for inferring treatment effects in\npolynomial-degree randomization experiments. Our approach allows for\ncomputationally efficient inference even when the number of treatments is\nsufficiently large. The method relies on the use of a linearized estimator of\nthe original estimator to construct a new estimator, which we call the\nreduced-form estimator. The key feature of our approach is that the reduced-form\nestimator is derived as a function of the original estimator, allowing for\ncomputationally efficient estimation of the treatment effects. We illustrate\nour method with a case study on a binary treatment experiment with two treatments\nand three covariates.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16483516483516483,
          "p": 0.23809523809523808,
          "f": 0.19480518997048418
        },
        "rouge-2": {
          "r": 0.02654867256637168,
          "p": 0.03225806451612903,
          "f": 0.029126208639363646
        },
        "rouge-l": {
          "r": 0.12087912087912088,
          "p": 0.1746031746031746,
          "f": 0.14285713802243227
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.03114v1",
      "true_abstract": "This study constructs a novel analytical general equilibrium model to compare\nenvironmental policies in a setting where oligopolistic energy firms engage in\nthird-degree price discrimination across residential consumers and industrial\nfirms. Closed-form solutions demonstrate the impact on prices and quantities.\nThe resulting welfare change is decomposed across three distortions: output,\nprice discrimination, and externality. This study finds that the output\ndistortion and price discrimination welfare effects generally move in opposite\ndirections under policies such as an emission tax or a two-part instrument.\nNumerical analysis compares policies and finds scenarios where the output\ndistortion and price discrimination welfare changes fully offset and thus\nleaves the net welfare gain of the externality correction. In this way,\nenvironmental policy can be designed to mitigate output distortion welfare\nconcerns when firms have market power.",
      "generated_abstract": "In the current era, the use of digital technology has become a common practice\nin the field of education. This paper aims to investigate the impact of\ndigital technology on education in Saudi Arabia. The study adopted descriptive\nresearch method and the data were collected using questionnaire. The\nsampled population was 2796 teachers and the results showed that the use of\ndigital technology has a positive impact on teachers' performance. The findings\nof this study have several implications for the education sector, especially\nin the field of digital technologies.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16483516483516483,
          "p": 0.2631578947368421,
          "f": 0.20270269796658152
        },
        "rouge-2": {
          "r": 0.025,
          "p": 0.039473684210526314,
          "f": 0.030612240149938262
        },
        "rouge-l": {
          "r": 0.16483516483516483,
          "p": 0.2631578947368421,
          "f": 0.20270269796658152
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2503.02642v1",
      "true_abstract": "In both machine learning and in computational neuroscience, plasticity in\nfunctional neural networks is frequently expressed as gradient descent on a\ncost. Often, this imposes symmetry constraints that are difficult to reconcile\nwith local computation, as is required for biological networks or neuromorphic\nhardware. For example, wake-sleep learning in networks characterized by\nBoltzmann distributions builds on the assumption of symmetric connectivity.\nSimilarly, the error backpropagation algorithm is notoriously plagued by the\nweight transport problem between the representation and the error stream.\nExisting solutions such as feedback alignment tend to circumvent the problem by\ndeferring to the robustness of these algorithms to weight asymmetry. However,\nthey are known to scale poorly with network size and depth. We introduce\nspike-based alignment learning (SAL), a complementary learning rule for spiking\nneural networks, which uses spike timing statistics to extract and correct the\nasymmetry between effective reciprocal connections. Apart from being\nspike-based and fully local, our proposed mechanism takes advantage of noise.\nBased on an interplay between Hebbian and anti-Hebbian plasticity, synapses can\nthereby recover the true local gradient. This also alleviates discrepancies\nthat arise from neuron and synapse variability -- an omnipresent property of\nphysical neuronal networks. We demonstrate the efficacy of our mechanism using\ndifferent spiking network models. First, we show how SAL can significantly\nimprove convergence to the target distribution in probabilistic spiking\nnetworks as compared to Hebbian plasticity alone. Second, in neuronal\nhierarchies based on cortical microcircuits, we show how our proposed mechanism\neffectively enables the alignment of feedback weights to the forward pathway,\nthus allowing the backpropagation of correct feedback errors.",
      "generated_abstract": "ence of the COVID-19 pandemic has prompted rapid advancements in\ncovid-19 RNA sequencing (RNA-seq) technology, which has facilitated the\ndetection of the SARS-CoV-2 virus. However, the vast amount of RNA-seq data\npresented by the pandemic poses significant challenges for researchers,\nincluding data management, annotation, and data integration. To address these\nchallenges, we propose the Covid-19 RNA-seq data integration platform (Covid-19\nRNA-seq DIP) as a cloud-based platform that integrates various data\ncollections, enabling researchers to conduct data integration, annotation,\nand data visualization tasks. Cutting-edge software, including Snakemake\npipeline, PipelineD, and GlycanD, are integrated into the CDIP to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08875739644970414,
          "p": 0.2112676056338028,
          "f": 0.1249999958336807
        },
        "rouge-2": {
          "r": 0.003937007874015748,
          "p": 0.011111111111111112,
          "f": 0.005813949624799756
        },
        "rouge-l": {
          "r": 0.08875739644970414,
          "p": 0.2112676056338028,
          "f": 0.1249999958336807
        }
      }
    },
    {
      "paper_id": "physics.soc-ph.physics/soc-ph/2503.08418v1",
      "true_abstract": "Human mobility, a pivotal aspect of urban dynamics, displays a profound and\nmultifaceted relationship with urban sustainability. Despite considerable\nefforts analyzing mobility patterns over decades, the ranking dynamics of urban\nmobility has received limited attention. This study aims to contribute to the\nfield by investigating changes in rank and size of hourly inflows to various\nlocations across 60 Chinese cities throughout the day. We find that the\nrank-size distribution of hourly inflows over the course of the day is stable\nacross cities. To uncover the microdynamics beneath the stable aggregate\ndistribution amidst shifting location inflows, we analyzed consecutive-hour\ninflow size and ranking variations. Our findings reveal a dichotomy: locations\nwith higher daily average inflow display a clear monotonic trend, with more\npronounced increases or decreases in consecutive-hour inflow. In contrast,\nranking variations exhibit a non-monotonic pattern, distinguished by the\nstability of not only the top and bottom rankings but also those in\nmoderately-inflowed locations. Finally, we compare ranking dynamics across\ncities using a ranking metric, the rank turnover. The results advance our\nunderstanding of urban mobility dynamics, providing a basis for applications in\nurban planning and traffic engineering.",
      "generated_abstract": "r presents the first detailed study of the evolution of the\ndynamics of the human population in response to a variety of social and\nenvironmental factors. We focus on the impact of global climate change, which\ncauses rapid changes in both the environment and human behavior. We find that\nclimate change causes a rapid and widespread decline in population, with\nsignificant demographic and economic consequences. The impact of climate\nchange on human population dynamics is complex and depends on a range of\ninterconnected factors, including the rate of change in the environment, the\ndegree of social integration, and the level of economic development. The\nimpact of climate change on population dynamics is not linear, with a\nsignificant population decline occurring relatively early in the climate\nchange scenario. This decline is driven by a combination of reduced food\nproduction and increased mortality due to heat stress. The impact of climate\nchange on population dynamics",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14634146341463414,
          "p": 0.23376623376623376,
          "f": 0.1799999952645001
        },
        "rouge-2": {
          "r": 0.02824858757062147,
          "p": 0.038461538461538464,
          "f": 0.03257328501947044
        },
        "rouge-l": {
          "r": 0.14634146341463414,
          "p": 0.23376623376623376,
          "f": 0.1799999952645001
        }
      }
    },
    {
      "paper_id": "math.AP.math/AP/2503.09924v1",
      "true_abstract": "This paper discusses the possibility of applying the velocity averaging\ntheorems in [F. Golse, P.-L. Lions, B. Perthame, R. Sentis: J. Funct. Anal.\n76(1):110--125, 1988] to the Wigner equation governing the quantum evolution of\nthe Wigner transform of quantum density operators. Our first main results\naddress the case of the Wigner function of a special class of density operators\nassociated to mixed states, whose Hilbert-Schmidt norm is of order\n$\\hbar^{d/2}$, where $d$ is the space dimension and $\\hbar$ the reduced Planck\nconstant. In space dimension $d=1$, we prove that the density function belongs\nto the Sobolev space $H^s(\\mathbb R)$ for some $s>0$. In the case of pure\nstates, we first obtain a characterization of the Wigner transform of rank-one\nquantum density operators, and apply this characterization (1) to analyze a\nrather general setting in which velocity averaging cannot apply to the Wigner\nfunctions of a family of rank-one density operators whose evolution is governed\nby the von Neumann equation, and (2) to obtain a quick derivation of Madelung's\nsystem of quantum hydrodynamic equations. This derivation provides a physical\nexplanation of one key assumption used in the proof of the negative result (1)\ndescribed above.",
      "generated_abstract": "We establish a connection between the Birkhoff-Kac formula and the\ngeneralized eigenvalue problem. In particular, we obtain a generalization of\nthe spectral theorem for finite-dimensional, locally compact groups. We also\nprove a version of the Birkhoff-Kac formula for a class of discrete groups. Our\nresults provide a unified approach to the study of these two topics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11403508771929824,
          "p": 0.325,
          "f": 0.16883116498566378
        },
        "rouge-2": {
          "r": 0.023255813953488372,
          "p": 0.07692307692307693,
          "f": 0.03571428214923505
        },
        "rouge-l": {
          "r": 0.11403508771929824,
          "p": 0.325,
          "f": 0.16883116498566378
        }
      }
    },
    {
      "paper_id": "math.NA.cs/NA/2503.10194v1",
      "true_abstract": "This paper describes novel algorithms for the identification of\n(almost-)resonant behavior in scattering problems. Our methods, relying on\nrational approximation, aim at building surrogate models of what we call \"field\namplification\", defined as the norm of the solution operator of the scattering\nproblem, which we express through boundary-integral equations. To provide our\ntechniques with theoretical foundations, we first derive results linking the\nfield amplification to the spectral properties of the operator that defines the\nscattering problem. Such results are then used to justify the use of rational\napproximation in the surrogate-modeling task. Some of our proposed methods\napply rational approximation in a \"standard\" way, building a rational\napproximant for either the solution operator directly or, in the interest of\ncomputational efficiency, for a randomly \"sketched\" version of it. Our other\n\"hybrid\" approaches are more innovative, combining\nrational-approximation-assisted root-finding with approximation using radial\nbasis functions. Three key features of our methods are that (i) they are\nagnostic of the strategy used to discretize the scattering problem, (ii) they\ndo not require any computations involving non-real wavenumbers, and (iii) they\ncan adjust to different settings through the use of adaptive sampling\nstrategies. We carry out some numerical experiments involving 2D scatterers to\ncompare our approaches. In our tests, two of our approaches (one standard, one\nhybrid) emerge as the best performers, with one or the other being preferable,\ndepending on whether emphasis is placed on accuracy or efficiency.",
      "generated_abstract": "em of solving the Maxwell's equations with unknown boundary\ninterfaces is a classical one and a well-established topic of research in\ncomputational electromagnetics. In this work, we propose an efficient\ndiscretization scheme for the Maxwell's equations with unknown boundary\ninterfaces in three dimensions, based on the Neumann-to-Dirichlet\ntransformation. This scheme is based on a finite element representation of\nelectromagnetic fields and utilizes a Neumann-to-Dirichlet transformation to\nsolve the resulting boundary value problem. We show that our scheme is\nwell-posed, converges to the true solution, and has an accuracy guarantee that\ndepends on the inverse of the total number of the mesh points. We also show that\nour scheme is stable with respect to the mesh size, and that the error can be\nbounded by a constant depending on the mesh size and the inverse of the total\nnumber of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14935064935064934,
          "p": 0.2948717948717949,
          "f": 0.19827585760552924
        },
        "rouge-2": {
          "r": 0.018433179723502304,
          "p": 0.035398230088495575,
          "f": 0.024242419739027465
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.2692307692307692,
          "f": 0.18103447829518443
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.08026v1",
      "true_abstract": "A principal uses payments conditioned on stochastic outcomes of a team\nproject to elicit costly effort from the team members. We develop a multi-agent\ngeneralization of a classic first-order approach to contract optimization by\nleveraging methods from network games. The main results characterize the\noptimal allocation of incentive pay across agents and outcomes. Incentive\noptimality requires equalizing, across agents, a product of (i) individual\nproductivity (ii) organizational centrality and (iii) responsiveness to\nmonetary incentives.",
      "generated_abstract": "We consider a two-period, two-player matching game with a fixed-price auction\n(seller's utility maximization). We show that under a certain standard, the\nseller can always obtain the expected outcome of a price of $P=0.23$ in the\nfirst period, i.e., the price is lower than the equilibrium price of $P=0.25$\nbut higher than the equilibrium price of $P=0.22$. The standard is that a\ngiven price must be lower than the equilibrium price of $P=0.25$ in the first\nperiod. Our result implies that the seller can always obtain the expected outcome\nof a price of $P=0.23$ in the first period, which is the equilibrium price of\n$P=0.25$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08333333333333333,
          "p": 0.09433962264150944,
          "f": 0.08849557024042631
        },
        "rouge-2": {
          "r": 0.013888888888888888,
          "p": 0.013513513513513514,
          "f": 0.013698625137926386
        },
        "rouge-l": {
          "r": 0.08333333333333333,
          "p": 0.09433962264150944,
          "f": 0.08849557024042631
        }
      }
    },
    {
      "paper_id": "math.RA.math/RA/2503.08288v1",
      "true_abstract": "We study numerical regularities for complexes over noncommutative noetherian\nlocally finite $\\mathbb{N}$-graded algebras $A$ such as CM (cm)-regularity, Tor\n(tor)-regularity (Ext (ext)-regularity) and Ex (ex)-regularity, which are the\nsupremum or infimum degrees of some associated canonical complexes. We show\nthat for any right bounded complex $X$ with finitely generated cohomologies,\nthe supremum degree of $R\\underline{\\text{Hom}}_A(X, A_0)$ coincides with the\nopposite of the infimum degree of $X$ if $A_0$ is semisimple. If $A$ has a\nbalanced dualizing complex and $A_0$ is semisimple, we prove that the\nCM-regularity of $X$ coincides with the supremum degree of\n$R\\underline{\\text{Hom}}_A(A_0,X)$ for any left bounded complex $X$ with\nfinitely generated cohomologies.\n  Several inequalities concerning the numerical regularities and the supremum\nor infimum degree of derived Hom or derived tensor complexes are given for\nnoncommutative noetherian locally finite $\\mathbb{N}$-graded algebras. Some of\nthese are generalizations of J\\o rgensen's results on the inequalities between\nthe CM-regularity and Tor-regularity, some are new even in the connected graded\ncase. Conditions are given under which the inequalities become equalities by\nestablishing two technical lemmas.\n  Following Kirkman, Won and Zhang, we also use the numerical AS-regularity\n(resp. little AS-regularity) to study Artin-Schelter regular property\n(finite-dimensional property) for noetherian $\\mathbb{N}$-graded algebras. We\nprove that the numerical AS-regularity of $A$ is zero if and only if that $A$\nis an $\\mathbb{N}$-graded AS-regular algebra under some mild conditions, which\ngeneralizes a result of Dong-Wu and a result of Kirkman-Won-Zhang. If $A$ has a\nbalanced dualizing complex and $A_0$ is semisimple, we prove that the little\nAS-regularity of $A$ is zero if and only if $A$ is finite-dimensional.",
      "generated_abstract": "new proof of the P\\'olya--Szeg\\\"o lemma for the random walk on\n$\\mathbb{Z}^d$ in the case of a random positive integer $S$, where the\nmeasure-theoretic probability is taken to be the Lebesgue measure. This is a\nsimpler proof than the one due to P\\'olya and Szeg\\\"o in 1959, which used the\nCantor measure. We also provide a short proof of the P\\'olya--Szeg\\\"o\nlemma for the random walk on $\\mathbb{Z}^d$ with the law of the absolute\nvalue of a random variable $S$, where the measure-theoretic probability is\ntaken to be the Lebesgue measure. This follows from a proof of the P\\'olya--Szeg\\\"o\nlemma for the random walk on $\\mathbb{Z}^d$ in the case of a random positive",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11811023622047244,
          "p": 0.30612244897959184,
          "f": 0.17045454143659616
        },
        "rouge-2": {
          "r": 0.015,
          "p": 0.04285714285714286,
          "f": 0.022222218381344968
        },
        "rouge-l": {
          "r": 0.11023622047244094,
          "p": 0.2857142857142857,
          "f": 0.1590909050729598
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.09647v1",
      "true_abstract": "This paper introduces a methodology leveraging Large Language Models (LLMs)\nfor sector-level portfolio allocation through systematic analysis of\nmacroeconomic conditions and market sentiment. Our framework emphasizes\ntop-down sector allocation by processing multiple data streams simultaneously,\nincluding policy documents, economic indicators, and sentiment patterns.\nEmpirical results demonstrate superior risk-adjusted returns compared to\ntraditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and\nportfolio return of 8.79% versus -0.61 and -1.39% respectively. These results\nsuggest that LLM-based systematic macro analysis presents a viable approach for\nenhancing automated portfolio allocation decisions at the sector level.",
      "generated_abstract": "r introduces a novel framework to support the development of\nthe next generation of AI and ML experts. The framework is based on the\nlearning-based approach, and it integrates the concept of coaching to support\nthe development of the AI and ML experts. We propose a coaching framework that\nis based on the knowledge representation and learning paradigm, which is\nintegrated with the expert development process. The framework is based on the\nlearning-based approach, which is based on the concept of coaching. The\nframework is applied to the development of the AI and ML experts, and the\nexperiments demonstrate that the framework is effective in supporting the\ndevelopment of the AI and ML experts. The results demonstrate that the framework\nis effective in supporting the development of the AI and ML experts. The\nframework integrates the concept of coaching, which is based on the knowledge\nrepresentation and learning parad",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.20833333333333334,
          "f": 0.15624999531250014
        },
        "rouge-2": {
          "r": 0.021052631578947368,
          "p": 0.02564102564102564,
          "f": 0.023121382331518982
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.20833333333333334,
          "f": 0.15624999531250014
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.10447v1",
      "true_abstract": "Audio-visual speech recognition (AVSR) has become critical for enhancing\nspeech recognition in noisy environments by integrating both auditory and\nvisual modalities. However, existing AVSR systems struggle to scale up without\ncompromising computational efficiency. In this study, we introduce MoHAVE\n(Mixture of Hierarchical Audio-Visual Experts), a novel robust AVSR framework\ndesigned to address these scalability constraints. By leveraging a\nMixture-of-Experts (MoE) architecture, MoHAVE activates modality-specific\nexpert groups, ensuring dynamic adaptation to various audio-visual inputs with\nminimal computational overhead. Key contributions of MoHAVE include: (1) a\nsparse MoE framework that efficiently scales AVSR model capacity, (2) a\nhierarchical gating mechanism that dynamically utilizes the expert groups based\non input context, enhancing adaptability and robustness, and (3) remarkable\nperformance across robust AVSR benchmarks, including LRS3 and MuAViC\ntranscription and translation tasks, setting a new standard for scalable speech\nrecognition systems.",
      "generated_abstract": "e a new end-to-end transformer-based speaker embedding model\nfor multi-speaker audio source separation, named MultiSpeakerV2.\nMultiSpeakerV2 improves the performance of the transformer-based encoder by\nadding an attention-based encoder-decoder architecture and a multi-head\nattention. The multi-head attention allows the model to attend to multiple\nspeakers simultaneously, improving the speaker separation performance.\nMultiSpeakerV2 also introduces a novel multi-speaker alignment model, which\naligns the time alignments between the audio and speaker embeddings.\nMultiSpeakerV2 achieves state-of-the-art performance in both speech enhancement\n(SE) and source separation (SS) tasks, surpassing previous state-of-the-art\nmodels by a large margin. The code will be released at:\nhttps://github.com/Hansh",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13761467889908258,
          "p": 0.21428571428571427,
          "f": 0.1675977606004808
        },
        "rouge-2": {
          "r": 0.015037593984962405,
          "p": 0.020618556701030927,
          "f": 0.01739129947032273
        },
        "rouge-l": {
          "r": 0.12844036697247707,
          "p": 0.2,
          "f": 0.15642457624293887
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2502.01992v1",
      "true_abstract": "In response to Task II of the FinRL Challenge at ACM ICAIF 2024, this study\nproposes a novel prompt framework for fine-tuning large language models (LLM)\nwith Reinforcement Learning from Market Feedback (RLMF). Our framework\nincorporates market-specific features and short-term price dynamics to generate\nmore precise trading signals. Traditional LLMs, while competent in sentiment\nanalysis, lack contextual alignment for financial market applications. To\nbridge this gap, we fine-tune the LLaMA-3.2-3B-Instruct model using a custom\nRLMF prompt design that integrates historical market data and reward-based\nfeedback. Our evaluation shows that this RLMF-tuned framework outperforms\nbaseline methods in signal consistency and achieving tighter trading outcomes;\nawarded as winner of Task II. You can find the code for this project on GitHub.",
      "generated_abstract": "This study presents a detailed analysis of the impact of interest rate\ncurve dynamics on the price of an inflation-indexed bond, which is commonly\nused as a proxy for inflation risk. We demonstrate that the expected value of\nthe bond's price is not solely determined by the expected inflation rate, but\nalso by the spread between the bond's coupon rate and the inflation rate.\nAdditionally, we show that the impact of interest rate movements on bond prices\ndepends on the relative strength of the inflation and interest rate dynamics.\nOur analysis provides valuable insights into the impact of interest rate\nmovements on inflation-indexed bond prices, and how these dynamics interact\nwith each other, highlighting the importance of modeling these dynamics in\nfinancial models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17346938775510204,
          "p": 0.23943661971830985,
          "f": 0.20118342708028442
        },
        "rouge-2": {
          "r": 0.00847457627118644,
          "p": 0.00980392156862745,
          "f": 0.009090904117358092
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.19718309859154928,
          "f": 0.16568046850040277
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.09177v1",
      "true_abstract": "We generalize the notions of composition series and composition factors for\nprofinite groups, and prove a profinite version of the Jordan-Holder Theorem.\nWe apply this to prove a Galois Theorem for infinite prosolvable extensions. In\naddition, we investigate the connection between the abstract and topological\ncomposition factors of a nonstrongly complete profinite group.",
      "generated_abstract": "We prove that a complete separable metric space is $\\delta$-amenable if and\nonly if it is $\\alpha$-amenable for some $\\alpha < \\delta$. We show that the\n$K_n$s are $\\delta$-amenable for $\\delta = 2$, and that the complete\nnon-separable metric spaces are $\\delta$-amenable for some $\\delta > 2$.\nMoreover, we show that a complete separable metric space is\n$\\delta$-amenable if and only if it is $\\alpha$-amenable for some $\\alpha <\n\\delta$. Finally, we show that a complete separable metric space is\n$\\delta$-amenable if and only if it is $\\alpha$-amenable for some $\\alpha <\n\\delta$ where $\\delta \\geq 3$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22857142857142856,
          "p": 0.21621621621621623,
          "f": 0.22222221722608038
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.22857142857142856,
          "p": 0.21621621621621623,
          "f": 0.22222221722608038
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.12638v2",
      "true_abstract": "3D molecule generation is crucial for drug discovery and material design.\nWhile prior efforts focus on 3D diffusion models for their benefits in modeling\ncontinuous 3D conformers, they overlook the advantages of 1D SELFIES-based\nLanguage Models (LMs), which can generate 100% valid molecules and leverage the\nbillion-scale 1D molecule datasets. To combine these advantages for 3D molecule\ngeneration, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D\nLanguage Modeling for 3D Molecule Generation. NExT-Mol uses an extensively\npretrained molecule LM for 1D molecule generation, and subsequently predicts\nthe generated molecule's 3D conformers with a 3D diffusion model. We enhance\nNExT-Mol's performance by scaling up the LM's model size, refining the\ndiffusion neural architecture, and applying 1D to 3D transfer learning.\nNotably, our 1D molecule LM significantly outperforms baselines in\ndistributional similarity while ensuring validity, and our 3D diffusion model\nachieves leading performances in conformer prediction. Given these improvements\nin 1D and 3D modeling, NExT-Mol achieves a 26% relative improvement in 3D FCD\nfor de novo 3D generation on GEOM-DRUGS, and a 13% average relative gain for\nconditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are\navailable at https://github.com/acharkq/NExT-Mol.",
      "generated_abstract": "ive analysis of single-cell RNA sequencing (scRNA-seq) data\nis crucial for understanding cellular biology. However, this task is\nchallenging due to the high variability in scRNA-seq data, the complexity of\nthe data structure, and the lack of standardized and reproducible methods.\nMethods that leverage machine learning can help overcome these limitations,\nbut current approaches are often limited by the lack of a robust and\ngeneralizable training dataset. Here, we present a novel method for the\ncomprehensive analysis of scRNA-seq data using a deep learning approach.\nSpecifically, we propose a novel framework that consists of a multimodal\nembedding layer, a multi-layer perceptron (MLP) layer, and a classifier layer\nto integrate diverse scRNA-seq data modalities. We validate our method on\nscRNA-seq data from",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16,
          "p": 0.23809523809523808,
          "f": 0.19138755500103033
        },
        "rouge-2": {
          "r": 0.027624309392265192,
          "p": 0.04424778761061947,
          "f": 0.03401360070965867
        },
        "rouge-l": {
          "r": 0.144,
          "p": 0.21428571428571427,
          "f": 0.17224879902016907
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.02383v1",
      "true_abstract": "This paper investigates strategic investments needed to mitigate transition\nrisks, particularly focusing on sectors significantly impacted by the shift to\na low-carbon economy. It emphasizes the importance of tailored sector-specific\nstrategies and the role of government interventions, such as carbon taxes and\nsubsidies, in shaping corporate behavior. In providing a multi-period\nframework, this paper evaluates the economic and operational trade-offs\ncompanies face under four various decarbonization scenarios: immediate, quick,\nslow, and no transitions. The analysis provides practical insights for both\npolicymakers and business leaders, demonstrating how regulatory frameworks and\nstrategic investments can be aligned to manage transition risks while\noptimizing long-term sustainability effectively. The findings contribute to a\ndeeper understanding of the economic impacts of regulatory policies and offer a\ncomprehensive framework to navigate the complexities of transitioning to a\nlow-carbon economy.",
      "generated_abstract": "e a simple, yet general, framework for identifying the effects of\nchanging the level of government spending on the output of a single firm. Our\napproach uses the framework of endogenous government spending to identify the\nimpact of government spending on the output of a firm as a function of the\nlevel of government spending. We find that increasing government spending leads\nto an increase in firm output and a reduction in the output elasticity of\ngovernment spending with respect to output. These results hold even when the\nlevel of government spending is a dynamic, endogenous variable. In the\nsimplified setting of a single firm and a single level of government spending,\nwe also find that the firm's output elasticity of government spending with\nrespect to output increases as the level of government spending increases.\nThese findings provide valuable insights into the impact of government spending",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1414141414141414,
          "p": 0.22580645161290322,
          "f": 0.17391303874233258
        },
        "rouge-2": {
          "r": 0.016,
          "p": 0.019801980198019802,
          "f": 0.017699110100635726
        },
        "rouge-l": {
          "r": 0.12121212121212122,
          "p": 0.1935483870967742,
          "f": 0.1490683182454382
        }
      }
    },
    {
      "paper_id": "physics.ins-det.physics/ins-det/2503.10383v1",
      "true_abstract": "In the DEAP-3600 dark matter search experiment, precise reconstruction of the\npositions of scattering events in liquid argon is key for background rejection\nand defining a fiducial volume that enhances dark matter candidate events\nidentification. This paper describes three distinct position reconstruction\nalgorithms employed by DEAP-3600, leveraging the spatial and temporal\ninformation provided by photomultipliers surrounding a spherical liquid argon\nvessel. Two of these methods are maximum-likelihood algorithms: the first uses\nthe spatial distribution of detected photoelectrons, while the second\nincorporates timing information from the detected scintillation light.\nAdditionally, a machine learning approach based on the pattern of photoelectron\ncounts across the photomultipliers is explored.",
      "generated_abstract": "r introduces a novel method for the reconstruction of nuclear charge\nrates using the time-of-flight (ToF) technique. By employing a novel\npseudo-interference-based algorithm, we achieve an improved measurement accuracy\nand robustness. The method is based on the principle of the pseudo-interference\nof a single-photon ionization channel with a background, which is considered as\na non-interferometric channel. The pseudo-interference effect is utilized to\ncalculate the charge rate of the background, which is then subtracted from the\ncharge rate of the target. The pseudo-interference effect also enhances the\nreconstruction accuracy by reducing the noise level of the ToF measurement.\nAdditionally, the method is robust against the non-idealities of the ToF\ncrystal, such as the drift rate and the drift time of the ionization channels.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.19117647058823528,
          "f": 0.17808218680427862
        },
        "rouge-2": {
          "r": 0.04950495049504951,
          "p": 0.04807692307692308,
          "f": 0.048780482805949356
        },
        "rouge-l": {
          "r": 0.14102564102564102,
          "p": 0.16176470588235295,
          "f": 0.15068492653030605
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.03312v1",
      "true_abstract": "In this paper, we conduct a large-scale field experiment to investigate the\nmanipulability of prediction markets. The main experiment involves randomly\nshocking prices across 817 separate markets; we then collect hourly price data\nto examine whether the effects of these shocks persist over time. We find that\nprediction markets can be manipulated: the effects of our trades are visible\neven 60 days after they have occurred. However, as predicted by our model, the\neffects of the manipulations somewhat fade over time. Markets with more\ntraders, greater trading volume, and an external source of probability\nestimates are harder to manipulate.",
      "generated_abstract": "We investigate the relationship between firm-level innovation and employment\nnovelty, exploring how firms innovate and the impact of innovation on\nemployment dynamics. Our analysis combines employment and firm-level\ninnovation data, employing a novel method to aggregate firm-level innovation to\nthe level of the firm. We find that innovation is a key driver of employment\nnovelty, with higher levels of innovation leading to higher levels of employment\nnovelty. Innovation levels also have a significant impact on firm-level\nemployment dynamics, with firms adopting innovation quickly experiencing\nemployment declines. These findings highlight the critical role of innovation\nin driving employment dynamics, emphasizing the need for policymakers to\nimplement innovation policies that foster firm-level innovation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13924050632911392,
          "p": 0.1746031746031746,
          "f": 0.15492957252826833
        },
        "rouge-2": {
          "r": 0.043478260869565216,
          "p": 0.04,
          "f": 0.04166666167534782
        },
        "rouge-l": {
          "r": 0.13924050632911392,
          "p": 0.1746031746031746,
          "f": 0.15492957252826833
        }
      }
    },
    {
      "paper_id": "math.RT.math/QA/2503.10394v1",
      "true_abstract": "This article investigates the two-parameter quantum matrix algebra at roots\nof unity. In the roots of unity setting, this algebra becomes a Polynomial\nIdentity (PI) algebra and it is known that simple modules over such algebra are\nfinite-dimensional with dimension at most the PI degree. We determine the\ncenter, compute the PI degree, and classify simple modules for two-parameter\nquantum matrix algebra, up to isomorphism, over an algebraically closed field\nof arbitrary characteristics.",
      "generated_abstract": "se of this paper is to prove the validity of the Weil conjecture\nfor the classical modular curves over the finite fields $\\mathbb{F}_q$\n($q\\in\\{2,\\dots,11\\}$). This conjecture is known to be true for the elliptic\ncurves $y^2 = x^3 + 2x$ over $\\mathbb{F}_q$ ($\\mathbb{F}_q$ is an arbitrary\nfinite field) and for the Jacobians of the curves $y^2 = x^3 + x$ over\n$\\mathbb{F}_q$ ($\\mathbb{F}_q$ is an arbitrary finite field). We use the\nGalois-theory based approach, which is a generalization of the work of\nKoblitz, Sparks, and Zannier. We show that the Weil conjecture is true for the\nclassical modular curves $y^2 = x",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2830188679245283,
          "p": 0.28846153846153844,
          "f": 0.28571428071473925
        },
        "rouge-2": {
          "r": 0.015151515151515152,
          "p": 0.013157894736842105,
          "f": 0.014084502067051946
        },
        "rouge-l": {
          "r": 0.22641509433962265,
          "p": 0.23076923076923078,
          "f": 0.2285714235718822
        }
      }
    },
    {
      "paper_id": "math.RA.math/RA/2503.05337v2",
      "true_abstract": "We classify all two-dimensional simple algebras over an algebraically closed\nfield. For each two-dimensional algebra $\\mathcal{A}$ with an infinite group of\nautomorphisms we describe a minimal (with respect to inclusion) generating set\nfor the algebra of invariants of the $m$-tuples of $\\mathcal{A}$ in\ncharacteristic zero case. As a consequence, we show that in characteristic zero\ncase Artin-Procesi-Iltyakov Equality holds for all two-dimensional simple\nalgebras with an infinite group of automorphisms. We also consider\nnondegenerate invariant bilinear forms over two-dimensional algebras.",
      "generated_abstract": "We consider the problem of constructing a universal approximation for\nthe function space $L_p(0,1)$ in terms of a universal function space $F_p(0,1)$\nfor $1\\le p<\\infty$. We show that for $p\\in(1,\\infty)$ the universal\napproximation space $F_p(0,1)$ is in fact the universal space $F_\\infty(0,1)$.\nMoreover, we prove that the universal approximation problem for $L_p(0,1)$ is\nNP-complete for $p\\in(1,\\infty)$ and NP-hard for $p=\\infty$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19607843137254902,
          "p": 0.3225806451612903,
          "f": 0.24390243432183228
        },
        "rouge-2": {
          "r": 0.029411764705882353,
          "p": 0.0392156862745098,
          "f": 0.03361344048019279
        },
        "rouge-l": {
          "r": 0.19607843137254902,
          "p": 0.3225806451612903,
          "f": 0.24390243432183228
        }
      }
    },
    {
      "paper_id": "econ.EM.q-fin/ST/2502.15458v1",
      "true_abstract": "Network connections, both across and within markets, are central in countless\neconomic contexts. In recent decades, a large literature has developed and\napplied flexible methods for measuring network connectedness and its evolution,\nbased on variance decompositions from vector autoregressions (VARs), as in\nDiebold and Yilmaz (2014). Those VARs are, however, typically identified using\nfull orthogonalization (Sims, 1980), or no orthogonalization (Koop, Pesaran,\nand Potter, 1996; Pesaran and Shin, 1998), which, although useful, are special\nand extreme cases of a more general framework that we develop in this paper. In\nparticular, we allow network nodes to be connected in \"clusters\", such as asset\nclasses, industries, regions, etc., where shocks are orthogonal across clusters\n(Sims style orthogonalized identification) but correlated within clusters\n(Koop-Pesaran-Potter-Shin style generalized identification), so that the\nordering of network nodes is relevant across clusters but irrelevant within\nclusters. After developing the clustered connectedness framework, we apply it\nin a detailed empirical exploration of sixteen country equity markets spanning\nthree global regions.",
      "generated_abstract": "In this paper, we investigate the relationship between financial market\nparameters and the subsequent development of stock prices. To address the\nlimitation of traditional regression-based approaches, we propose a novel\nframework that combines the Kalman filter with the dynamic factor model and the\nBayesian inference. The proposed model is capable of capturing the interplay\nbetween market parameters and stock prices. Furthermore, we examine the\neffects of market conditions on the performance of the stock market and\nidentify the underlying factors. Our findings suggest that the stock market\nexhibits high volatility and strong correlation with the macroeconomic and\nfinancial factors, while the market is less sensitive to market conditions.\nFinally, we investigate the potential of the proposed model for financial\ninstitutions, offering insights into risk management and investment strategies.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10236220472440945,
          "p": 0.16883116883116883,
          "f": 0.1274509756925223
        },
        "rouge-2": {
          "r": 0.012578616352201259,
          "p": 0.017391304347826087,
          "f": 0.014598535274923038
        },
        "rouge-l": {
          "r": 0.09448818897637795,
          "p": 0.15584415584415584,
          "f": 0.11764705412389484
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.03026v2",
      "true_abstract": "When can interventions in markets be designed to increase surplus robustly --\ni.e., with high probability -- accounting for uncertainty due to imprecise\ninformation about economic primitives? In a setting with many strategic firms,\neach possessing some market power, we present conditions for such interventions\nto exist. The key condition, recoverable structure, requires large-scale\ncomplementarities among families of products. The analysis works by decomposing\nthe incidence of interventions in terms of principal components of a Slutsky\nmatrix. Under recoverable structure, a noisy signal of this matrix reveals\nenough about these principal components to design robust interventions. Our\nresults demonstrate the usefulness of spectral methods for analyzing\nimperfectly observed strategic interactions with many agents.",
      "generated_abstract": "We consider the problem of estimating the elasticity of demand with respect to\na quantity of interest (QoI) by fitting a linear regression model to\ninformation on the QoI and a related quantity of interest (RQoI). We derive a\nnon-asymptotic bounds on the relative error in the elasticity of demand\nestimated by the linear regression model. We show that these bounds are\noptimal in the sense that they are the smallest possible errors attainable by\nany linear regression model. Our results apply to a wide range of estimation\nmethods, including the maximum likelihood estimator and the least squares\nestimator.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12790697674418605,
          "p": 0.1896551724137931,
          "f": 0.15277777296682113
        },
        "rouge-2": {
          "r": 0.00909090909090909,
          "p": 0.011627906976744186,
          "f": 0.0102040767076242
        },
        "rouge-l": {
          "r": 0.10465116279069768,
          "p": 0.15517241379310345,
          "f": 0.1249999951890434
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.05601v2",
      "true_abstract": "This paper proposes a Matrix Error Correction Model to identify cointegration\nrelations in matrix-valued time series. We hereby allow separate cointegrating\nrelations along the rows and columns of the matrix-valued time series and use\ninformation criteria to select the cointegration ranks. Through Monte Carlo\nsimulations and a macroeconomic application, we demonstrate that our approach\nprovides a reliable estimation of the number of cointegrating relationships.",
      "generated_abstract": "r develops a new method for identifying the causal effect of a\nvariable on another variable using a latent variable. We use the latent variable\nas a surrogate for the unobserved variable, and we use a novel method to\nestimate the causal effect of the latent variable on the unobserved variable\nusing the latent variable as a surrogate. This method is referred to as the\n\"latent variable method\" or the \"latent variable method with the\nLatent-Variable-as-Surrogate-for-the-Unobserved-Variable Method.\" This method\nis motivated by the assumption that the causal effect of the latent variable on\nthe unobserved variable is non-zero. We show that this assumption is consistent\nwith data, and we show that the resulting estimator is consistent and asymptotically\nnormal. We also show that this estimator is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20833333333333334,
          "p": 0.19607843137254902,
          "f": 0.2020201970247935
        },
        "rouge-2": {
          "r": 0.016666666666666666,
          "p": 0.01098901098901099,
          "f": 0.013245028323320885
        },
        "rouge-l": {
          "r": 0.20833333333333334,
          "p": 0.19607843137254902,
          "f": 0.2020201970247935
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.10431v1",
      "true_abstract": "Deep learning methods for point tracking are applicable in 2D\nechocardiography, but do not yet take advantage of domain specifics that enable\nextremely fast and efficient configurations. We developed MyoTracker, a\nlow-complexity architecture (0.3M parameters) for point tracking in\nechocardiography. It builds on the CoTracker2 architecture by simplifying its\ncomponents and extending the temporal context to provide point predictions for\nthe entire sequence in a single step. We applied MyoTracker to the right\nventricular (RV) myocardium in RV-focused recordings and compared the results\nwith those of CoTracker2 and EchoTracker, another specialized point tracking\narchitecture for echocardiography. MyoTracker achieved the lowest average point\ntrajectory error at 2.00 $\\pm$ 0.53 mm. Calculating RV Free Wall Strain (RV\nFWS) using MyoTracker's point predictions resulted in a -0.3$\\%$ bias with\n95$\\%$ limits of agreement from -6.1$\\%$ to 5.4$\\%$ compared to reference\nvalues from commercial software. This range falls within the interobserver\nvariability reported in previous studies. The limits of agreement were wider\nfor both CoTracker2 and EchoTracker, worse than the interobserver variability.\nAt inference, MyoTracker used 67$\\%$ less GPU memory than CoTracker2 and 84$\\%$\nless than EchoTracker on large sequences (100 frames). MyoTracker was 74 times\nfaster during inference than CoTracker2 and 11 times faster than EchoTracker\nwith our setup. Maintaining the entire sequence in the temporal context was the\ngreatest contributor to MyoTracker's accuracy. Slight additional gains can be\nmade by re-enabling iterative refinement, at the cost of longer processing\ntime.",
      "generated_abstract": "In this paper, we propose a novel two-stage method for multi-class\nexpert-guided image segmentation. First, we pre-train a 3D U-Net architecture\nwith the expert segmentation results on 15k images. Then, we fine-tune the\npre-trained model using the image-level annotations of 61k images. In the\nsecond stage, we employ a self-supervised learning (SSL) method to further\nimprove the segmentation performance. We validate our method on the MS-COCO and\nADE20K datasets and obtain state-of-the-art performance. Our code will be\navailable at https://github.com/shihangzhao/ExpertGuided_Segmentation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09876543209876543,
          "p": 0.25806451612903225,
          "f": 0.14285713885363532
        },
        "rouge-2": {
          "r": 0.0045045045045045045,
          "p": 0.0125,
          "f": 0.006622512661727652
        },
        "rouge-l": {
          "r": 0.09876543209876543,
          "p": 0.25806451612903225,
          "f": 0.14285713885363532
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-lat/2503.08847v1",
      "true_abstract": "The concept of nucleon radii plays a central role in our understanding of the\ninternal structure of protons and neutrons, providing critical insights into\nthe non-perturbative regime of quantum chromodynamics (QCD). While the charge\nradius is often interpreted as the ``size\" of the nucleon, this interpretation\nis an oversimplification that overlooks the multifaceted nature of nucleon\nstructure. This paper provides a comprehensive overview of the different\nnucleon radii, including the charge and magnetic radii, the axial radius, and\nthe emerging concepts of mechanical and mass radii. We discuss the definitions\nas well as the experimental, theoretical and phenomenological determinations of\nthese radii, highlighting their distinct physical origins and implications. By\nsynthesizing recent experimental results and theoretical advancements, we\nemphasize that each radius reflects a specific aspect of the nucleon's internal\nstructure, such as its electric charge distribution, magnetic properties, weak\ninteractions, or internal mechanical stress. In particular, we address the\ncommon but misleading interpretation of the proton radius as a simple measure\nof its size, underscoring the nuanced and context-dependent nature of nucleon\nradii. Through this exploration, we aim to clarify the roles of these radii in\ncharacterizing nucleon structure and to identify open questions that remain to\nbe addressed. This work contributes to a deeper understanding of the nucleon\nand its significance in the broader context of particle and nuclear physics.",
      "generated_abstract": "t a simple model for the dynamical generation of $B$ mesons in\ncontemporary QCD sum rules. The model is based on the recently proposed\n$B$-meson interpolating operator which is derived from a scalar-pseudoscalar\nmeson interpolating operator. The $B$-meson interpolating operator is constructed\nfrom a set of four operators which are obtained by contracting the operator\ndescribing the $B$ meson with a set of four operators which generate the\nmeson-meson correlation functions. In the model, the operator describing the\n$B$ meson is constructed from the scalar-pseudoscalar interpolating operator\nand the other operators are constructed from the operators describing the $B$\nmeson and the meson-meson correlation functions. The operator describing the\n$B$ meson is a generalization of the operator describing the $B$ meson in the\nmodel",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06818181818181818,
          "p": 0.1836734693877551,
          "f": 0.09944750986355742
        },
        "rouge-2": {
          "r": 0.01932367149758454,
          "p": 0.04597701149425287,
          "f": 0.02721088018672839
        },
        "rouge-l": {
          "r": 0.06818181818181818,
          "p": 0.1836734693877551,
          "f": 0.09944750986355742
        }
      }
    },
    {
      "paper_id": "math.RT.math/RT/2503.06607v1",
      "true_abstract": "We prove that any complex local representation of the flat virtual braid\ngroup, $FVB_2$, into $GL_2(\\mathbb{C})$, for $n\\geq 2$, has one of the types\n$\\lambda_i: FVB_2 \\rightarrow GL_2(\\mathbb{C})$, $1\\leq i\\leq 12$. We find\nnecessary and sufficient conditions that guarantee the irreducibility of\nrepresentations of type $\\lambda_i$, $1\\leq i\\leq 5$, and we prove that\nrepresentations of type $\\lambda_i$, $6\\leq i\\leq 12$, are reducible. Regarding\nfaithfulness, we find necessary and sufficient conditions for representations\nof type $\\lambda_6$ or $\\lambda_7$ to be faithful. Moreover, we give sufficient\nconditions for representations of type $\\lambda_1$, $\\lambda_2$, or $\\lambda_4$\nto be unfaithful, and we show that representations of type $\\lambda_i$, $i=3,\n5, 8, 9, 10, 11, 12$ are unfaithful. We prove that any complex homogeneous\nlocal representations of the flat virtual braid group, $FVB_n$, into\n$GL_{n}(\\mathbb{C})$, for $n\\geq 2$, has one of the types $\\gamma_i: FVB_n\n\\rightarrow GL_n(\\mathbb{C})$, $i=1, 2$. We then prove that representations of\ntype $\\gamma_1: FVB_n \\rightarrow GL_n(\\mathbb{C})$ are reducible for $n\\geq\n6$, while representations of type $\\gamma_2: FVB_n \\rightarrow\nGL_n(\\mathbb{C})$ are reducible for $n\\geq 3$. Then, we show that\nrepresentations of type $\\gamma_1$ are unfaithful for $n\\geq 3$ and that\nrepresentations of type $\\gamma_2$ are unfaithful if $y=b$. Furthermore, we\nprove that any complex homogeneous local representation of the flat virtual\nbraid group, $FVB_n$, into $GL_{n+1}(\\mathbb{C})$, for all $n\\geq 4$, has one\nof the types $\\delta_i: FVB_n \\rightarrow GL_{n+1}(\\mathbb{C})$, $1\\leq i\\leq\n8$. We prove that these representations are reducible for $n\\geq 10$. Then, we\nshow that representations of types $\\delta_i$, $i\\neq 5, 6$, are unfaithful,\nwhile representations of types $\\delta_5$ or $\\delta_6$ are unfaithful if\n$x=y$.",
      "generated_abstract": "We provide a general framework for the study of topological $K$-theory of\norganized Lie groups. This is done by constructing the derived category of\nvarieties of schemes over the ground field, using the theory of compact\nobjects. We then introduce the theory of topological $K$-theory of groups and\nstudy the case of topological $K$-theory of locally compact groups. The main\nresult is that for a locally compact group $G$ and a subgroup $H \\subset G$,\nthe pair $(G/H, \\pi_1(G/H))$ is a topological $K$-space. In particular, if\n$G$ is a compact group, then every compact topological $K$-space is a\ntopological $K$-space of compact groups. Moreover, we prove that the\ntopological $K$-theory of the group $G$ is concentrated in degrees $0$ and\n$\\dim G - 1$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1188118811881188,
          "p": 0.18461538461538463,
          "f": 0.1445783084881697
        },
        "rouge-2": {
          "r": 0.03125,
          "p": 0.04854368932038835,
          "f": 0.03802280892307307
        },
        "rouge-l": {
          "r": 0.1188118811881188,
          "p": 0.18461538461538463,
          "f": 0.1445783084881697
        }
      }
    },
    {
      "paper_id": "math.ST.math/MP/2503.08808v1",
      "true_abstract": "We consider two random variables $X$ and $Y$ following correlated Gamma\ndistributions, characterized by identical scale and shape parameters and a\nlinear correlation coefficient $\\rho$. Our focus is on the parameter: \\[\n  D(X,Y) = \\frac{|X - Y|}{X + Y}, \\] which appears in applied contexts such as\ndynamic speckle imaging, where it is known as the \\textit{Fujii index}. In this\nwork, we derive a closed-form expression for the probability density function\nof $D(X,Y)$ as well as analytical formulas for its moments of order $k$. Our\nderivation starts by representing $X$ and $Y$ as two correlated exponential\nrandom variables, obtained from the squared magnitudes of circular complex\nGaussian variables. By considering the sum of $k$ independent exponential\nvariables, we then derive the joint density of $(X,Y)$ when $X$ and $Y$ are two\ncorrelated Gamma variables. Through appropriate varable transformations, we\nobtain the theoretical distribution of $D(X,Y)$ and evaluate its moments\nanalytically. These theoretical findings are validated through numerical\nsimulations, with particular attention to two specific cases: zero correlation\nand unit shape parameter.",
      "generated_abstract": "The aim of this article is to develop a new approach for computing the\ndistribution of the largest eigenvalue of an $n \\times n$ symmetric matrix.\nWe prove that the distribution of the largest eigenvalue can be expressed as\nthe solution of a second-order linear differential equation, with explicit\nsolutions for the case where the matrix is diagonalizable. We then use the\nconcept of singular value decomposition to derive closed-form expressions for\nthe distribution of the largest and second largest eigenvalues, and show that\nthe distribution of the largest eigenvalue converges to a normal distribution\nwhen the number of eigenvalues tends to infinity. The methodology we propose\nrepresents a significant step towards the development of efficient methods for\ncomputing the distribution of eigenvalues in large matrices.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1694915254237288,
          "p": 0.2702702702702703,
          "f": 0.20833332859592024
        },
        "rouge-2": {
          "r": 0.018518518518518517,
          "p": 0.028846153846153848,
          "f": 0.022556386215162972
        },
        "rouge-l": {
          "r": 0.1440677966101695,
          "p": 0.22972972972972974,
          "f": 0.17708332859592024
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2411.16373v1",
      "true_abstract": "Slow-fast dynamics are intrinsically related to complex phenomena, and are\nresponsible for many of the homeostatic dynamics that keep biological systems\nhealthfully functioning. We study a discrete-time membrane potential model that\ncan generate a diverse set of spiking behavior depending on the choice of\nslow-fast time scales, from fast spiking to bursting, or plateau action\npotentials -- also known as cardiac spikes, since they are characteristic in\nheart myocytes. The plateau of cardiac spikes may lose stability, generating\nearly or delayed afterdepolarizations (EAD and DAD, respectively), both of\nwhich are related to cardiac arrhythmia. We show the periodicity changes along\nthe transition from the healthy action potentials to these impaired spikes. We\nshow that while EADs are mainly periodic attractors, DAD usually comes with\nchaos. EADs are found inside shrimps -- isoperiodic structures of the parameter\nspace. However, in our system, the shrimps have an internal structure made of\nmultiple periodicities, revealing a complete devil's staircase. Understanding\nthe periodicity of plateau attractors in slow-fast systems could come in handy\nto unveil the features of heart myocytes behavior that are linked to cardiac\narrhythmias.",
      "generated_abstract": "ic interplay between cellular processes and the extracellular matrix\n(ECM) is critical for cell function, growth, and survival. However, the\nunderstanding of the dynamic interactions between cells and ECM remains\nlimited. Recent advances in 3D cell culture and biomaterials engineering have\nprovided new opportunities for studying cell-ECM interactions. However, these\nmethods face challenges, including the limited resolution and diversity of the\n3D culture platforms and the lack of biomaterials with appropriate mechanical\nproperties. Here, we introduce a novel methodology that combines the advantages\nof both approaches to address these challenges. We developed a novel 3D cell\nculture system that combines the benefits of 3D cell culture and biomaterials\nengineering. The system features a robust mechanical system with a controlled\ncell proliferation environment, providing high-resolution 3D",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13114754098360656,
          "p": 0.20512820512820512,
          "f": 0.15999999524200015
        },
        "rouge-2": {
          "r": 0.005747126436781609,
          "p": 0.008928571428571428,
          "f": 0.006993002227985055
        },
        "rouge-l": {
          "r": 0.13114754098360656,
          "p": 0.20512820512820512,
          "f": 0.15999999524200015
        }
      }
    },
    {
      "paper_id": "math.OA.math/OA/2503.10505v1",
      "true_abstract": "We compute the $K_1$-group of ultraproducts of unital, simple $C^*$-algebras\nwith unique trace and strict comparison. As an application, we prove that the\nreduced free group $C^*$-algebras $C^*_r(F_m)$ and $C^*_r(F_n)$ are\nelementarily equivalent (i.e., have isomorphic ultrapowers) if and only if $m =\nn$. This settles in the negative the $C^*$-algebraic analogue of Tarski's 1945\nproblem for groups.",
      "generated_abstract": "aper, we study a class of infinite dimensional Banach spaces\n$\\mathcal{X}$ satisfying a type of local convexity property. For this purpose,\nwe construct an arbitrary linear operator $T: \\mathcal{X} \\to \\mathcal{X}$\nsatisfying the following properties:\n  - $T$ is continuous in the operator norm topology,\n  - $T$ has a continuous extension to a continuous linear operator $T:\n  \\mathcal{X} \\to \\mathcal{X}$,\n  - the extended operator $T$ is a contraction with respect to some\n  non-negative sequences of norms,\n  - the extended operator $T$ is continuous in the operator norm topology,\n  - $T$ is a contraction with respect to some non-negative sequences of norms.\n  - The extended operator $T$ is a contraction with respect to some non-negative\n  sequences of norm",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11764705882352941,
          "p": 0.11538461538461539,
          "f": 0.11650484936940356
        },
        "rouge-2": {
          "r": 0.01694915254237288,
          "p": 0.0136986301369863,
          "f": 0.015151510207761024
        },
        "rouge-l": {
          "r": 0.09803921568627451,
          "p": 0.09615384615384616,
          "f": 0.09708737364124827
        }
      }
    },
    {
      "paper_id": "cs.CE.econ/GN/2503.02692v1",
      "true_abstract": "To improve stock trend predictions and support personalized investment\ndecisions, this paper proposes FinArena, a novel Human-Agent collaboration\nframework. Inspired by the mixture of experts (MoE) approach, FinArena combines\nmultimodal financial data analysis with user interaction. The human module\nfeatures an interactive interface that captures individual risk preferences,\nallowing personalized investment strategies. The machine module utilizes a\nLarge Language Model-based (LLM-based) multi-agent system to integrate diverse\ndata sources, such as stock prices, news articles, and financial statements. To\naddress hallucinations in LLMs, FinArena employs the adaptive\nRetrieval-Augmented Generative (RAG) method for processing unstructured news\ndata. Finally, a universal expert agent makes investment decisions based on the\nfeatures extracted from multimodal data and investors' individual risk\npreferences. Extensive experiments show that FinArena surpasses both\ntraditional and state-of-the-art benchmarks in stock trend prediction and\nyields promising results in trading simulations across various risk profiles.\nThese findings highlight FinArena's potential to enhance investment outcomes by\naligning strategic insights with personalized risk considerations.",
      "generated_abstract": "r investigates the effects of a social network on the spread of\ninformation in a dynamic, imperfectly competitive environment. We consider\nnetworks with two types of members: (i) decision makers, who can adopt an\ninformation-sharing strategy, and (ii) consumers, who can adopt a\nselfish-information-sharing strategy. In the former case, the information is\nshared to the consumers' advantage. In the latter case, the information is\nshared to the consumers' disadvantage. We assume that the information is\navailable at the consumers' disposal. We show that, in the absence of an\ninformation-sharing incentive, the equilibrium behavior is a mixed strategy\nequilibrium where consumers and decision makers coexist. When information\nsharing is introduced, we find a unique mixed strategy equilibrium where both\ndecision makers and consumers adopt a selfish-information-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09917355371900827,
          "p": 0.16901408450704225,
          "f": 0.12499999533908437
        },
        "rouge-2": {
          "r": 0.0064516129032258064,
          "p": 0.009523809523809525,
          "f": 0.0076923028772219485
        },
        "rouge-l": {
          "r": 0.09917355371900827,
          "p": 0.16901408450704225,
          "f": 0.12499999533908437
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.astro-ph/CO/2503.10361v1",
      "true_abstract": "We study the stochastic gravitational wave background sourced by a network of\ncosmic superstrings and demonstrate that incorporating higher-mass string\nspecies, beyond the fundamental string, is crucial for accurately modeling the\nresulting gravitational wave spectrum across frequencies ranging from nanohertz\nto kilohertz. Using the multi-tension velocity-dependent one-scale model to\nevolve the cosmic superstring network, we perform several fits to the NANOGrav\n15-year dataset and obtain expectation values for the fundamental string\ntension, string coupling and effective size of compact extra dimensions. We\nfind that the cosmic superstring best-fits are comparable in likelihood to\nSupermassive Black Hole models, thought by many to be the leading candidate\nexplanation of the signal. The implications of the best-fit spectra are\ndiscussed within the context of future gravitational wave experiments. We\nobtain expectation values for the fundamental string tension of\n$\\log_{10}(G\\mu_1)=-11.5^{+0.3}_{-0.3}$($-11.6^{+0.2}_{-0.3}$) for\ngravitational waves originating from large cuspy (kinky) cosmic superstring\nloops and $\\log_{10}(G\\mu_1)=-9.7^{+0.7}_{-0.7}$($-9.9^{+1.0}_{-0.5}$) for\nsmall cuspy (kinky) loops. We also place $2\\sigma$ upper bounds on the string\ncoupling, finding $g_s<0.65$ in all cases, and comment on the implication of\nour results for the effective size of the compact extra dimensions.",
      "generated_abstract": "t a complete, high-resolution, and high-signal-to-noise\nradiation-transfer-modeling study of the interaction between a supermassive\nblack hole binary and an external disk. This work represents the first attempt\nto model the interaction between a supermassive black hole and a disk with a\ncombination of high-resolution hydrodynamic simulations and numerical-relativity\nsimulations. Our study is motivated by the ongoing merger of the M87 black hole\nand the MW disk, which is expected to occur within the next decade. In this\nwork, we focus on the inner region of the disk, which we model as a\nnon-relativistic, axisymmetric, polytropic disk with a mass of $M_{\\rm disk} =\n10^7 M_{\\odot}$. We perform 40,000 hydrodynamic simulations with a high-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11023622047244094,
          "p": 0.2028985507246377,
          "f": 0.1428571382949814
        },
        "rouge-2": {
          "r": 0.01694915254237288,
          "p": 0.030927835051546393,
          "f": 0.02189780564521381
        },
        "rouge-l": {
          "r": 0.10236220472440945,
          "p": 0.18840579710144928,
          "f": 0.13265305666232835
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/SE/2503.10407v1",
      "true_abstract": "The cloud computing model enables the on-demand provisioning of computing\nresources, reducing manual management, increasing efficiency, and improving\nenvironmental impact. Software architects now play a strategic role in\ndesigning and deploying elasticity policies for automated resource management.\nHowever, creating policies that meet performance and cost objectives is\ncomplex. Existing approaches, often relying on formal models like Queueing\nTheory, require advanced skills and lack specific methods for representing\nelasticity within architectural models. This paper introduces an architectural\nview type for modeling and simulating elasticity, supported by the Scaling\nPolicy Definition (SPD) modeling language, a visual notation, and precise\nsimulation semantics. The view type is integrated into the Palladio ecosystem,\nproviding both conceptual and tool-based support. We evaluate the approach\nthrough two single-case experiments and a user study. In the first experiment,\nsimulations of elasticity policies demonstrate sufficient accuracy when\ncompared to load tests, showing the utility of simulations for evaluating\nelasticity. The second experiment confirms feasibility for larger applications,\nthough with increased simulation times. The user study shows that participants\ncompleted 90% of tasks, rated the usability at 71%, and achieved an average\nscore of 76% in nearly half the allocated time. However, the empirical evidence\nsuggests that modeling with this architectural view requires more time than\nmodeling control flow, resource environments, or usage profiles, despite its\nbenefits for elasticity policy design and evaluation.",
      "generated_abstract": "This paper introduces and evaluates a novel method for building large\ndeeplearning models that have the ability to learn complex dependencies and\nextract useful insights from large datasets. Our approach combines a\ndeeplearning model with a graph neural network (GNN), which can be used to\nextract structural insights from complex datasets. By training the GNN on\nrelational data, we can generate a directed graph that represents the\nrelationships between entities in the dataset, allowing the deeplearning model to\nlearn to extract the relationships between entities. This approach enables the\ndeeplearning model to understand the context of the dataset and extract\nsemantic information from it, which can be used to improve its ability to\nmake predictions. The method is evaluated using a range of datasets, including\ntext, images, and audio, and shows promising results in terms of accuracy and\nperformance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.147239263803681,
          "p": 0.2857142857142857,
          "f": 0.19433197931715002
        },
        "rouge-2": {
          "r": 0.013761467889908258,
          "p": 0.02459016393442623,
          "f": 0.01764705422214653
        },
        "rouge-l": {
          "r": 0.12269938650306748,
          "p": 0.23809523809523808,
          "f": 0.1619433153495387
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SY/2503.03151v1",
      "true_abstract": "Subset selection is central to many wireless communication problems,\nincluding link scheduling, power allocation, and spectrum management. However,\nthese problems are often NP-complete, because of which heuristic algorithms\napplied to solve these problems struggle with scalability in large-scale\nsettings. To address this, we propose a determinantal point process-based\nlearning (DPPL) framework for efficiently solving general subset selection\nproblems in massive networks. The key idea is to model the optimal subset as a\nrealization of a determinantal point process (DPP), which balances the\ntrade-off between quality (signal strength) and similarity (mutual\ninterference) by enforcing negative correlation in the selection of {\\em\nsimilar} links (those that create significant mutual interference). However,\nconventional methods for constructing similarity matrices in DPP impose\ndecomposability and symmetry constraints that often do not hold in practice. To\novercome this, we introduce a new method based on the Gershgorin Circle Theorem\nfor constructing valid similarity matrices. The effectiveness of the proposed\napproach is demonstrated by applying it to two canonical wireless network\nsettings: an ad hoc network in 2D and a cellular network serving drones in 3D.\nSimulation results show that DPPL selects near-optimal subsets that maximize\nnetwork sum-rate while significantly reducing computational complexity compared\nto traditional optimization methods, demonstrating its scalability for\nlarge-scale networks.",
      "generated_abstract": "aper, we present a novel approach to designing a distributed\nchannel estimator for a multi-user massive MIMO system with imperfect CSI\nincorporating both non-orthogonal multiple access (NOMA) and frequency\nmultiplexing (FM). We propose a novel linear channel estimation methodology\nbased on the iterative least squares (ILS) algorithm that includes the\nestimation of the channel covariance matrix. To overcome the computational\ncomplexity of the ILS algorithm, we also propose a distributed iterative\nestimation algorithm (DIA), which reduces the complexity of the ILS algorithm\nby utilizing the channel covariance matrix estimates from the previous\niterations. Additionally, we extend the ILS algorithm to support multiple\nfrequency-division multiplexing (FDM) sub-carriers. We demonstrate that\ndistributed channel estimation can significantly reduce the computational\ncomplexity of the system and improve the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.28378378378378377,
          "f": 0.19004524441432413
        },
        "rouge-2": {
          "r": 0.03,
          "p": 0.05714285714285714,
          "f": 0.03934425778016715
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.28378378378378377,
          "f": 0.19004524441432413
        }
      }
    },
    {
      "paper_id": "math.CO.cs/CR/2503.10320v1",
      "true_abstract": "Cellular Automata (CA) are commonly investigated as a particular type of\ndynamical systems, defined by shift-invariant local rules. In this paper, we\nconsider instead CA as algebraic systems, focusing on the combinatorial designs\ninduced by their short-term behavior. Specifically, we review the main results\npublished in the literature concerning the construction of mutually orthogonal\nLatin squares via bipermutive CA, considering both the linear and nonlinear\ncases. We then survey some significant applications of these results to\ncryptography, and conclude with a discussion of open problems to be addressed\nin future research on CA-based combinatorial designs.",
      "generated_abstract": "We prove a new result about the asymptotic stability of a Markov chain\nwith mean zero transition matrix. The result extends a result of\nLiu-Murugan-Mohanty-Nguyen-Nguyen-Vu-Wang (2017) on the stability of a\nBrownian motion with a constant drift and a drift-free Markov chain with\nmean zero transition matrix. Our main result applies to a general class of\nMarkov chains with a drift-free transition matrix. The proof is based on a\nresult about the asymptotic stability of the mean zero Markov chain with\nmean zero transition matrix.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11842105263157894,
          "p": 0.23684210526315788,
          "f": 0.15789473239766091
        },
        "rouge-2": {
          "r": 0.021505376344086023,
          "p": 0.03389830508474576,
          "f": 0.026315784723858197
        },
        "rouge-l": {
          "r": 0.10526315789473684,
          "p": 0.21052631578947367,
          "f": 0.14035087274853814
        }
      }
    },
    {
      "paper_id": "cs.CE.econ/TH/2503.00201v1",
      "true_abstract": "This paper demonstrates that Automated Market Maker (AMM) based markets, such\nas those using constant product formulas (e.g., Uniswap), are inherently\npath-dependent. We prove mathematically that the sequence of operations in AMMs\ndetermines the final state, challenging the notion that market prices solely\nreflect information. This property has profound implications for decentralized\nprediction markets that rely on AMMs for price discovery, as it demonstrates\nthey cannot function as pure \"truth machines.\" Using both mathematical proofs\nand empirical evidence from ETH/USDC pools, we show that AMM-based markets\nincorporate historical path information beyond the current market beliefs. Our\nfindings contribute to the understanding of market efficiency, mechanism\ndesign, and the interpretation of prices in decentralized finance systems.",
      "generated_abstract": "aper, we propose a novel approach for training a Large Language\nModel (LLM) to generate a human-like narrative based on a large amount of\nexisting data. Our approach leverages the LLM's ability to capture the\nsemantic structure of the data and the knowledge it has acquired to generate a\nhuman-like narrative. To train the LLM, we collect a large amount of data from\ndiverse sources, including books, audio, and video, and use a pre-trained\ntransformer model to generate a narrative based on this data. The generated\nnarrative is then evaluated by human reviewers to determine if it captures the\nsemantic structure of the data and is human-like in its content. Our results\nshow that the LLM can generate human-like narratives using existing data,\ndemonstrating the potential of the LLM to generate human-like narratives based\non",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17204301075268819,
          "p": 0.21052631578947367,
          "f": 0.1893491074766291
        },
        "rouge-2": {
          "r": 0.02564102564102564,
          "p": 0.02702702702702703,
          "f": 0.02631578447714776
        },
        "rouge-l": {
          "r": 0.15053763440860216,
          "p": 0.18421052631578946,
          "f": 0.16568046842337467
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/OT/2405.07102v3",
      "true_abstract": "Instrumental variables (IV) are a commonly used tool to estimate causal\neffects from non-randomized data. An archetype of an IV is a randomized trial\nwith non-compliance where the randomized treatment assignment serves as an IV\nfor the non-ignorable treatment received. Under a monotonicity assumption, a\nvalid IV non-parametrically identifies the average treatment effect among a\nnon-identified, latent complier subgroup, whose generalizability is often under\ndebate. In many studies, there could exist multiple versions of an IV, for\ninstance, different nudges to take the same treatment in different study sites\nin a multicentre clinical trial. These different versions of an IV may result\nin different compliance rates and offer a unique opportunity to study IV\nestimates' generalizability. In this article, we introduce a novel nested IV\nassumption and study identification of the average treatment effect among two\nlatent subgroups: always-compliers and switchers, who are defined based on the\njoint potential treatment received under two versions of a binary IV. We derive\nthe efficient influence function for the SWitcher Average Treatment Effect\n(SWATE) under a non-parametric model and propose efficient estimators. We then\npropose formal statistical tests of the generalizability of IV estimates under\nthe nested IV framework. We apply the proposed method to the Prostate, Lung,\nColorectal and Ovarian (PLCO) Cancer Screening Trial and study the causal\neffect of colorectal cancer screening and its generalizability.",
      "generated_abstract": "r presents a novel approach to the problem of predicting future\npredictive performance using data collected over time. We introduce a novel\ndata-driven method for the prediction of the future performance of a predictor\nunder uncertainty, which we call the data-driven predictive performance\n(DPP) estimator. The DPP estimator is based on the prediction error, and\nprovides a robust estimate of the predictive performance of a predictor under\nuncertainty, which is independent of the predictor and the underlying data\ndistribution. The DPP estimator is also easily computable and provides a\nrobust estimate of the predictive performance of a predictor under uncertainty,\neven in the presence of outliers and/or missing data. We illustrate the\neffectiveness of the DPP estimator by applying it to the problem of predicting\nthe future performance of a predictor under uncertainty, where the predictor is\na time series forec",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14074074074074075,
          "p": 0.3064516129032258,
          "f": 0.19289339670179606
        },
        "rouge-2": {
          "r": 0.04830917874396135,
          "p": 0.10204081632653061,
          "f": 0.06557376613039535
        },
        "rouge-l": {
          "r": 0.1259259259259259,
          "p": 0.27419354838709675,
          "f": 0.1725888281738773
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10060v1",
      "true_abstract": "This paper investigates the resource allocation design for a pinching antenna\n(PA)-assisted multiuser multiple-input single-output (MISO) non-orthogonal\nmultiple access (NOMA) system featuring multiple dielectric waveguides. To\nenhance model accuracy, we propose a novel frequency-dependent power\nattenuation model for dielectric waveguides in PA-assisted systems. By jointly\noptimizing the precoder vector and the PA placement, we aim to maximize the\nsystem's sum-rate while accounting for the power attenuation across dielectric\nwaveguides. The design is formulated as a non-convex optimization problem. To\neffectively address the problem at hand, we introduce an alternating\noptimization-based algorithm to obtain a suboptimal solution in polynomial\ntime. Our results demonstrate that the proposed PA-assisted system not only\nsignificantly outperforms the conventional system but also surpasses a naive\nPA-assisted system that disregards power attenuation. The performance gain\ncompared to the naive PA-assisted system becomes more pronounced at high\ncarrier frequencies, emphasizing the importance of considering power\nattenuation in system design.",
      "generated_abstract": "opment of 5G networks is expected to provide a comprehensive and\nextensive suite of new technologies, including ultra-reliable low-latency\ncommunications (URLLC) and secure communication. While 5G networks are\nexpected to provide a large amount of network capacity, the increase in data\nvolume and the need for higher data rates are resulting in increasing energy\nconsumption. In order to mitigate these challenges, this paper presents a novel\nmethod for reducing energy consumption in URLLC networks. The proposed method\nfocuses on optimizing the time-frequency resources in a time-frequency\npartitioning scheme. The proposed method reduces the required number of\ntransmitting antennas while maintaining the required number of transmitting\nantennas. The performance of the proposed method is evaluated through a\nsimulation study using a 5G-like network with low latency requirements. The\nsim",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14705882352941177,
          "p": 0.18518518518518517,
          "f": 0.16393442129535088
        },
        "rouge-2": {
          "r": 0.035211267605633804,
          "p": 0.043859649122807015,
          "f": 0.03906249505981508
        },
        "rouge-l": {
          "r": 0.14705882352941177,
          "p": 0.18518518518518517,
          "f": 0.16393442129535088
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/CR/2503.09712v1",
      "true_abstract": "Time series classification (TSC) is a cornerstone of modern web applications,\npowering tasks such as financial data analysis, network traffic monitoring, and\nuser behavior analysis. In recent years, deep neural networks (DNNs) have\ngreatly enhanced the performance of TSC models in these critical domains.\nHowever, DNNs are vulnerable to backdoor attacks, where attackers can covertly\nimplant triggers into models to induce malicious outcomes. Existing backdoor\nattacks targeting DNN-based TSC models remain elementary. In particular, early\nmethods borrow trigger designs from computer vision, which are ineffective for\ntime series data. More recent approaches utilize generative models for trigger\ngeneration, but at the cost of significant computational complexity. In this\nwork, we analyze the limitations of existing attacks and introduce an enhanced\nmethod, FreqBack. Drawing inspiration from the fact that DNN models inherently\ncapture frequency domain features in time series data, we identify that\nimproper perturbations in the frequency domain are the root cause of\nineffective attacks. To address this, we propose to generate triggers both\neffectively and efficiently, guided by frequency analysis. FreqBack exhibits\nsubstantial performance across five models and eight datasets, achieving an\nimpressive attack success rate of over 90%, while maintaining less than a 3%\ndrop in model accuracy on clean data.",
      "generated_abstract": "tion of medical and biological research is driven by the rapid\ndevelopment of new technologies, including AI, data science, and bioinformatics,\nwhich are increasingly integrated into clinical settings. This paper introduces\nthe concept of \"bio-AI\" - an emerging paradigm that combines the capabilities of\nbiological systems with the advantages of AI. Bio-AI is a novel approach for\naddressing the challenges of large-scale genomic data analysis, leveraging the\nadvantages of machine learning while taking into account the inherent\nbiological complexity of genomic data. Bio-AI addresses these challenges by\nutilizing biological data to enhance the accuracy and efficiency of machine\nlearning models. This approach is particularly effective in complex\nbiological data, such as genomic sequences, gene expression levels, and protein\nstructures, and it can be applied to a wide range of applications, from\ndiagn",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18120805369127516,
          "p": 0.3,
          "f": 0.2259414178988464
        },
        "rouge-2": {
          "r": 0.020100502512562814,
          "p": 0.031746031746031744,
          "f": 0.024615379867645882
        },
        "rouge-l": {
          "r": 0.1610738255033557,
          "p": 0.26666666666666666,
          "f": 0.20083681538838616
        }
      }
    },
    {
      "paper_id": "math.NT.math/NT/2503.08800v1",
      "true_abstract": "We prove the Fontaine-Plamondon conjecture and show that there are precisely\n$4400$ and $26952$ positive integral $E_7$-friezes and $E_8$-friezes\nrespectively, completing the enumerative classification of all positive\nintegral friezes of Dynkin type. In general, we count positive integral friezes\nof rank $n$ by determining the positive integral points on a $n$-dimensional\nsingular affine variety. This gives new Diophantine proofs of the enumeration\ntheorems for friezes of the other Dynkin types, which were previously proved\nusing discrete geometry, algebraic combinatorics, and the theory of cluster\nalgebras.",
      "generated_abstract": "aper, we introduce a new way to define the $k$-th exterior power of\na smooth, compact, connected, oriented $n$-manifold $M$ using a finite set of\nrepresentatives. We show that this definition agrees with the usual definition\nfor $k=1$, and gives rise to an equivalence of categories between the category\nof $k$-th exterior powers of smooth, compact, connected, oriented $n$-manifolds\nwith a finite set of representatives and the category of cohomology classes of\nthe $(k+1)$-sphere $S^{n+1}$ with coefficients in $\\mathbb{R}$. We then prove\nthat for $k\\geq 2$, the canonical map\n$\\mathrm{H}^{k}(M)\\to\\mathrm{H}^{k-1}(M\\#S^{n+1})$ is an isomorphism,\nproviding a new proof of the Poinc",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20634920634920634,
          "p": 0.20967741935483872,
          "f": 0.20799999500032013
        },
        "rouge-2": {
          "r": 0.03896103896103896,
          "p": 0.034482758620689655,
          "f": 0.03658536087224934
        },
        "rouge-l": {
          "r": 0.19047619047619047,
          "p": 0.1935483870967742,
          "f": 0.19199999500032014
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.19869v1",
      "true_abstract": "This study investigates event-related desynchronization (ERD) phenomena\nduring motor imagery and actual movement. Using sLORETA software, we analyzed\nthe cortical current source density distributions in Mu and Beta frequency\nbands for 33 subjects during rest, motor imagery, and actual movement\nconditions. The results were normalized for analysis. Using sLORETA's\nstatistical tools, paired t-tests were conducted to compare the normalized\ncurrent source density results between rest and motor imagery, rest and actual\nmovement, and motor imagery and actual movement conditions in both frequency\nbands. The findings revealed: In both Mu and Beta frequency bands, during motor\nimagery, significant ERD (P<0.01) was observed in the salience network,\nsupplementary motor area, primary motor area, premotor cortex, primary\nsomatosensory cortex, and parietofrontal mirror neuron system. During actual\nmovement, significant ERD (P<0.05) was observed in the primary somatosensory\ncortex, primary motor area, and parietofrontal mirror neuron system in both\nfrequency bands. Comparing motor imagery to actual movement, the current source\ndensity in the primary somatosensory cortex and parietofrontal mirror neuron\nsystem was higher during motor imagery, though this difference was not\nstatistically significant (P>0.05). This paper analyzes the factors\ncontributing to these statistical results and proposes preliminary solutions.",
      "generated_abstract": "We develop a framework for modeling the evolution of gene regulatory networks\n(GRNs) in a Bayesian framework. We first develop a model for the posterior\ndistribution of a GRN given its current state and a set of experimental data.\nThe model is based on the assumption that the GRN is a Markov chain, with\nstates corresponding to the nodes in the graph. We then develop a Bayesian\ninference algorithm for the posterior distribution of the GRN. The algorithm\nuses a Metropolis-Hastings sampler to update the posterior distribution. The\nalgorithm is implemented in Python using the SciPy library. We test the\nperformance of the algorithm on simulated data and on a GRN for the growth of\nfruit fly wings.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07526881720430108,
          "p": 0.11475409836065574,
          "f": 0.09090908612497918
        },
        "rouge-2": {
          "r": 0.006944444444444444,
          "p": 0.009708737864077669,
          "f": 0.008097161129672311
        },
        "rouge-l": {
          "r": 0.07526881720430108,
          "p": 0.11475409836065574,
          "f": 0.09090908612497918
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/GN/2412.05430v1",
      "true_abstract": "Recent advances in self-supervised models for natural language, vision, and\nprotein sequences have inspired the development of large genomic DNA language\nmodels (DNALMs). These models aim to learn generalizable representations of\ndiverse DNA elements, potentially enabling various genomic prediction,\ninterpretation and design tasks. Despite their potential, existing benchmarks\ndo not adequately assess the capabilities of DNALMs on key downstream\napplications involving an important class of non-coding DNA elements critical\nfor regulating gene activity. In this study, we introduce DART-Eval, a suite of\nrepresentative benchmarks specifically focused on regulatory DNA to evaluate\nmodel performance across zero-shot, probed, and fine-tuned scenarios against\ncontemporary ab initio models as baselines. Our benchmarks target biologically\nmeaningful downstream tasks such as functional sequence feature discovery,\npredicting cell-type specific regulatory activity, and counterfactual\nprediction of the impacts of genetic variants. We find that current DNALMs\nexhibit inconsistent performance and do not offer compelling gains over\nalternative baseline models for most tasks, while requiring significantly more\ncomputational resources. We discuss potentially promising modeling, data\ncuration, and evaluation strategies for the next generation of DNALMs. Our code\nis available at https://github.com/kundajelab/DART-Eval.",
      "generated_abstract": "years, the study of neural oscillations has significantly advanced\nthe understanding of the brain. However, despite their prominence, it remains\nunclear how neural oscillations emerge in the brain. To address this question,\nwe propose a novel computational framework, the Neural Oscillation Generative\nModel (NOGM), which simulates the dynamics of neural oscillations and\ngenerates synthetic oscillations. This framework employs a novel neural\nnetwork architecture, a novel loss function, and a novel optimization\nalgorithm. The framework is trained on simulated data, and we validate the\nframework's performance on synthetic data generated by the framework. Our\nresults show that the framework can generate synthetic neural oscillations\nthat closely resemble real neural oscillations. Furthermore, the framework\ngenerates synthetic neural oscillations that closely resemble real neural\noscillations under conditions where the real neural oscillations exhibit\nsignific",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12056737588652482,
          "p": 0.21794871794871795,
          "f": 0.15525113696628523
        },
        "rouge-2": {
          "r": 0.00558659217877095,
          "p": 0.009259259259259259,
          "f": 0.006968636420986784
        },
        "rouge-l": {
          "r": 0.10638297872340426,
          "p": 0.19230769230769232,
          "f": 0.13698629678363686
        }
      }
    },
    {
      "paper_id": "math.PR.stat/TH/2502.17412v1",
      "true_abstract": "Gaussian multiplicative chaos (GMC) is a canonical random fractal measure\nobtained by exponentiating log-correlated Gaussian processes, first constructed\nin the seminal work of Kahane (1985). Since then it has served as an important\nbuilding block in constructions of quantum field theories and Liouville quantum\ngravity. However, in many natural settings, non-Gaussian log-correlated\nprocesses arise. In this paper, we investigate the universality of GMC through\nan invariance principle. We consider the model of a random Fourier series, a\nprocess known to be log-correlated. While the Gaussian Fourier series has been\na classical object of study, recently, the non-Gaussian counterpart was\ninvestigated and the associated multiplicative chaos constructed by Junnila in\n2016. We show that the Gaussian and non-Gaussian variables can be coupled so\nthat the associated chaos measures are almost surely mutually absolutely\ncontinuous throughout the entire sub-critical regime. This solves the main open\nproblem from Kim and Kriechbaum (2024) who had earlier established such a\nresult for a part of the regime. The main ingredient is a new high dimensional\nCLT for a sum of independent (but not i.i.d.) random vectors belonging to rank\none subspaces with error bounds involving the isotropic properties of the\ncovariance matrix of the sum, which we expect will find other applications. The\nproof relies on a path-wise analysis of Skorokhod embeddings as well as a\nperturbative result about square roots of positive semi-definite matrices\nwhich, surprisingly, appears to be new.",
      "generated_abstract": "We show that the law of large numbers for the empirical process of empirical\ndistributions of random variables in the unit ball of a normed space is the\nlaw of large numbers for the empirical process of empirical measures of random\nvariables in the unit ball. This result generalizes a result of Gnedenko and\nLawrence and a result of Weaver. We also prove the law of large numbers for the\nempirical process of empirical measures of random variables in the unit ball of\na normed space under the condition that the metric on the normed space is\ncompact and convex.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10303030303030303,
          "p": 0.4594594594594595,
          "f": 0.16831682869081468
        },
        "rouge-2": {
          "r": 0.026200873362445413,
          "p": 0.10714285714285714,
          "f": 0.04210526000024647
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.40540540540540543,
          "f": 0.14851484849279492
        }
      }
    },
    {
      "paper_id": "math-ph.math-ph/2503.09558v1",
      "true_abstract": "For a given graph $G$, Budzik, Gaiotto, Kulp, Wang, Williams, Wu, Yu, and the\nfirst author studied a ''topological'' differential form $\\alpha_G$, which\nexpresses violations of BRST-closedness of a quantum field theory along a\nsingle topological direction. In a seemingly unrelated context, Brown, Panzer,\nand the second author studied a ''Pfaffian'' differential form $\\phi_G$, which\nis used to construct cohomology classes of the odd commutative graph complex.\nWe give an explicit combinatorial proof that $\\alpha_G$ coincides with\n$\\phi_G$. We also discuss the equivalence of several properties of these forms,\nwhich had been established independently for both contexts in previous work.",
      "generated_abstract": "In this work, we study the spectral properties of the discrete Laplacian on\na graph with a large number of vertices, where the graph is an infinite\ntree. We prove that the spectrum of the discrete Laplacian is either discrete\nor continuous, and we also prove that the number of eigenvalues of the\ndiscrete Laplacian is upper bounded by the product of the largest and smallest\neigenvalues of the graph Laplacian. Moreover, we construct an explicit\nfunctional equation for the discrete Laplacian that determines the spectrum of\nthe discrete Laplacian, and we give a closed-form expression for the discrete\nLaplacian in terms of the continuous Laplacian. Finally, we give an\nalternative proof of the result in \\cite{kot11} that the number of eigenvalues\nof the discrete Laplacian is upper bounded by the number of eigenvalues of the\ncontinuous Laplacian.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24050632911392406,
          "p": 0.3275862068965517,
          "f": 0.2773722578912036
        },
        "rouge-2": {
          "r": 0.042105263157894736,
          "p": 0.044444444444444446,
          "f": 0.043243238246896125
        },
        "rouge-l": {
          "r": 0.22784810126582278,
          "p": 0.3103448275862069,
          "f": 0.2627737177452182
        }
      }
    },
    {
      "paper_id": "math.CO.math/IT/2503.08948v1",
      "true_abstract": "Let $C$ be a binary code of length $n$ with distances $0<d_1<\\cdots<d_s\\le\nn$. In this note we prove a general upper bound on the size of $C$ without any\nrestriction on the distances $d_i$. The bound is asymptotically optimal.",
      "generated_abstract": "the existence of local-in-time solutions for the 3D compressible\nEinstein-matter system in the presence of a non-magnetic source field and an\nanisotropic boundary condition. More precisely, we prove the existence of\nlocal-in-time solutions for the system in the Euclidean case, and the\nexistence of global-in-time solutions in the non-Euclidean ones when the\nsource is located on a compact hyper-surface of the manifold. The theory is\nextended to the case of general non-vanishing bending energy. The analysis is\nbased on the Lagrangian geometry and the non-vanishing bending energy induces\nnon-trivial constraints on the Lagrangian section. These constraints are\nsatisfied by a specific choice of the Lagrangian section, which is called the\ncavity. The existence of global-in-time",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.28125,
          "p": 0.140625,
          "f": 0.18749999555555566
        },
        "rouge-2": {
          "r": 0.05405405405405406,
          "p": 0.02127659574468085,
          "f": 0.030534347091661862
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.125,
          "f": 0.16666666222222234
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.18868v1",
      "true_abstract": "This study examines the relationship between income inequality, gender, and\nschool completion rates in Malaysia using machine learning techniques. The\ndataset utilized is from the Malaysia's Public Sector Open Data Portal,\ncovering the period 2016-2022. The analysis employs various machine learning\ntechniques, including K-means clustering, ARIMA modeling, Random Forest\nregression, and Prophet for time series forecasting. These models are used to\nidentify patterns, trends, and anomalies in the data, and to predict future\nschool completion rates. Key findings reveal significant disparities in school\ncompletion rates across states, genders, and income levels. The analysis also\nidentifies clusters of states with similar completion rates, suggesting\npotential regional factors influencing educational outcomes. Furthermore, time\nseries forecasting models accurately predict future completion rates,\nhighlighting the importance of ongoing monitoring and intervention strategies.\nThe study concludes with recommendations for policymakers and educators to\naddress the observed disparities and improve school completion rates in\nMalaysia. These recommendations include targeted interventions for specific\nstates and demographic groups, investment in early childhood education, and\naddressing the impact of income inequality on educational opportunities. The\nfindings of this study contribute to the understanding of the factors\ninfluencing school completion in Malaysia and provide valuable insights for\npolicymakers and educators to develop effective strategies to improve\neducational outcomes.",
      "generated_abstract": "r examines how the adoption of the global trading system and\nthe emergence of the World Trade Organization (WTO) influence the development\nof trade policy and trade-related policies in developing countries. We use\nempirical data from 1980 to 2020 to analyze the impact of the WTO on trade\npolicy and trade-related policies in 177 developing countries. Our findings\nsuggest that the WTO has significantly influenced the development of trade\npolicy and trade-related policies in developing countries. However, the\nimpact of the WTO on trade policy and trade-related policies varies significantly\nby country, with some countries experiencing a more positive effect than\nothers. Additionally, the impact of the WTO on trade policy and trade-related\npolicies varies across sectors, with some sectors experiencing a more positive\neffect than others. These findings highlight",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10483870967741936,
          "p": 0.20634920634920634,
          "f": 0.13903742868712302
        },
        "rouge-2": {
          "r": 0.016216216216216217,
          "p": 0.036585365853658534,
          "f": 0.022471905856444298
        },
        "rouge-l": {
          "r": 0.0967741935483871,
          "p": 0.19047619047619047,
          "f": 0.12834224152134763
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2502.02619v1",
      "true_abstract": "This paper introduces a novel agent-based approach for enhancing existing\nportfolio strategies using Proximal Policy Optimization (PPO). Rather than\nfocusing solely on traditional portfolio construction, our approach aims to\nimprove an already high-performing strategy through dynamic rebalancing driven\nby PPO and Oracle agents. Our target is to enhance the traditional 60/40\nbenchmark (60% stocks, 40% bonds) by employing the Regret-based Sharpe reward\nfunction. To address the impact of transaction fee frictions and prevent signal\nloss, we develop a transaction cost scheduler. We introduce a future-looking\nreward function and employ synthetic data training through a circular block\nbootstrap method to facilitate the learning of generalizable allocation\nstrategies. We focus on two key evaluation measures: return and maximum\ndrawdown. Given the high stochasticity of financial markets, we train 20\nindependent agents each period and evaluate their average performance against\nthe benchmark. Our method not only enhances the performance of the existing\nportfolio strategy through strategic rebalancing but also demonstrates strong\nresults compared to other baselines.",
      "generated_abstract": "This paper investigates the possibility of constructing a model for\nreinforcement learning that can handle a wide range of problems, including\nmultiple-armed bandits, multi-armed bandits, and sequential decision-making. We\nemploy a model-free reinforcement learning framework based on the\nprojection-augmented Lagrangian method and propose a novel algorithm for\nreinforcement learning with continuous-time dynamics. We establish the\nconvergence rate of the proposed algorithm, and derive a theoretical bound on\nthe performance gap between the proposed algorithm and the optimal algorithm.\nWe demonstrate the effectiveness of the proposed method in simulation and\napplication to real-world problems. The code for reproducing the results is\navailable at:\n  https://github.com/HongyuWu/Reinforcement_Learning_with_Multi-Armed_Bandits",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14049586776859505,
          "p": 0.2463768115942029,
          "f": 0.178947363795568
        },
        "rouge-2": {
          "r": 0.025157232704402517,
          "p": 0.041666666666666664,
          "f": 0.031372544324798864
        },
        "rouge-l": {
          "r": 0.12396694214876033,
          "p": 0.21739130434782608,
          "f": 0.15789473221662062
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2503.04246v1",
      "true_abstract": "Bayesian inference has many advantages for complex models. However, standard\nMonte Carlo methods for summarizing the posterior can be computationally\ndemanding, and it is attractive to consider optimization-based variational\napproximations. Our work considers Gaussian approximations with sparse\nprecision matrices which are tractable to optimize in high-dimensional\nproblems. Although the optimal Gaussian approximation is usually defined as the\none closest to the target posterior in Kullback-Leibler divergence, it is\nuseful to consider other divergences when the Gaussian assumption is crude, in\norder to capture important features of the posterior for a given application.\nOur work studies the weighted Fisher divergence, which focuses on gradient\ndifferences between the target posterior and its approximation, with the Fisher\nand score-based divergences being special cases. We make three main\ncontributions. First, we compare approximations for weighted Fisher divergences\nunder mean-field assumptions for both Gaussian and non-Gaussian targets with\nKullback-Leibler approximations. Second, we go beyond mean-field and consider\napproximations with sparse precision matrices reflecting posterior conditional\nindependence structure for hierarchical models. Using stochastic gradient\ndescent to enforce sparsity, we develop two approaches to minimize the weighted\nFisher divergence, based on the reparametrization trick and a batch\napproximation of the objective. Finally, we examine the performance of our\nmethods for examples involving logistic regression, generalized linear mixed\nmodels and stochastic volatility models.",
      "generated_abstract": "The problem of estimating the probability distribution of the sample\ndistances in nearest-neighbor search problems is of considerable importance.\nIn this paper, we study the problem of estimating the probability distribution\nof the sample distances in $k$-nearest neighbor search problems when the\nnumber of samples $n$ is fixed and the number of neighbors $k$ is\ndiminishing. We propose two estimators for the probability distribution of the\nsample distances. The first estimator is based on the expectation-maximization\nalgorithm and the second is a novel estimator based on the method of moments.\nThese two estimators are compared with each other through Monte Carlo\nsimulations. Our results show that the estimator based on the expectation-\nmaximization algorithm outperforms the estimator based on the method of moments\nfor small values of $k$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15267175572519084,
          "p": 0.29850746268656714,
          "f": 0.20202019754259778
        },
        "rouge-2": {
          "r": 0.025252525252525252,
          "p": 0.05319148936170213,
          "f": 0.03424657097673165
        },
        "rouge-l": {
          "r": 0.15267175572519084,
          "p": 0.29850746268656714,
          "f": 0.20202019754259778
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/SY/2503.09865v1",
      "true_abstract": "Remote driving of vehicles is gaining in importance in the transportation\nsector, especially when Automated Driving Systems (ADSs) reach the limits of\ntheir system boundaries. This study investigates the challenges faced by human\nRemote Drivers (RDs) during remote driving, particularly focusing on the\nidentification and classification of human performance-related challenges\nthrough a comprehensive analysis of real-world remote driving data Las Vegas.\nFor this purpose, a total of 183 RD performance-related Safety Driver (SD)\ninterventions were analyzed and classified using an introduced severity\nclassification. As it is essential to prevent the need for SD interventions,\nthis study identified and analyzed harsh driving events to detect an increased\nlikelihood of interventions by the SD. In addition, the results of the\nsubjective RD questionnaire are used to evaluate whether the objective metrics\nfrom SD interventions and harsh driving events can also be confirmed by the RDs\nand whether additional challenges can be uncovered. The analysis reveals\nlearning curves, showing a significant decrease in SD interventions as RD\nexperience increases. Early phases of remote driving experience, especially\nbelow 200 km of experience, showed the highest frequency of safety-related\nevents, including braking too late for traffic signs and responding impatiently\nto other traffic participants. Over time, RDs follow defined rules for\nimproving their control, with experience leading to less harsh braking,\nacceleration, and steering maneuvers. The study contributes to understanding\nthe requirements of RDS, emphasizing the importance of targeted training to\naddress human performance limitations. It further highlights the need for\nsystem improvements to address challenges like latency and the limited haptic\nfeedback replaced by visual feedback, which affect the RDs' perception and\nvehicle control.",
      "generated_abstract": "This paper presents a novel method for the estimation of the state of the\nsystem from the observation of a linear time-varying nonlinear system\ncoupled with a large-scale nonlinear process. The proposed method consists of\ntwo key elements: the first one is the identification of the state of the\nnonlinear system, which is done by a Kalman filter; the second one is the\nestimation of the nonlinear process state, which is done by a Kalman filter\ncoupled with a nonlinear model predictive control (MPC) algorithm. The\nperformance of the proposed method is evaluated through simulations and\nexperimental results in a large-scale nonlinear process system. The results\ndemonstrate that the proposed method outperforms the state-of-the-art methods\nin terms of both estimation accuracy and computational efficiency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10982658959537572,
          "p": 0.30158730158730157,
          "f": 0.16101694523879642
        },
        "rouge-2": {
          "r": 0.007662835249042145,
          "p": 0.020618556701030927,
          "f": 0.011173180406823906
        },
        "rouge-l": {
          "r": 0.10404624277456648,
          "p": 0.2857142857142857,
          "f": 0.15254236896760998
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.07461v1",
      "true_abstract": "We study the optimal management of a photovoltaic system's battery owned by a\nself-consumption group that aims to minimize energy consumption costs. We\nassume that the photovoltaic system is composed of a photovoltaic panel and a\nbattery, where the photovoltaic panel produces energy according to a certain\nstochastic process. The management of the battery is the responsibility of a\ngroup administrator, who makes the joint decision to either store part of the\nphotovoltaic energy production and sell the remaining energy at the electricity\nspot price, or discharge part of the energy stored in the battery and sell it\nin the electricity market. Inspired by European Union and Italian legislation,\nwhich promote incentives for energy transition and renewable energy production,\nwe assume that the group receives a monetary incentive for the virtual\nself-consumed energy, defined as the minimum between the power bought from the\ngrid to satisfy the group's power demand and the energy sold to the market. In\nthis case, the energy sold by the group is a mix of part of the photovoltaic\nproduction that is not stored and part of the energy discharged from the\nbattery. We model the problem as a stochastic optimal control problem, where\nthe optimal strategy is the joint charge-discharge decision that minimizes the\ngroup's energy consumption costs. We find the solution numerically by applying\na finite difference scheme to solve the Hamilton-Jacobi-Bellman equation\nassociated with the value function of the optimal control problem.",
      "generated_abstract": "r examines the effect of the introduction of the minimum wage in\n(M)and (M+1)countries on the wage distribution, the employment distribution,\nand the employment-to-population ratio. We compare the results of the\nestimates for M and M+1 with the results of the M+1-year simulations. We find\nthat the minimum wage has a significant impact on the wage distribution,\nespecially for those with low initial wages. The impact of the minimum wage on\nthe employment distribution is more pronounced for the M+1-year simulations\nthan for the M-year simulations. The employment-to-population ratio is stable\nfor the M+1-year simulations, but falls in the M-year simulations. The\nemployment-to-population ratio is higher in the M-year simulations than in the\nM+1-year simulations for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11304347826086956,
          "p": 0.26,
          "f": 0.1575757533516989
        },
        "rouge-2": {
          "r": 0.04,
          "p": 0.09876543209876543,
          "f": 0.05693949767606823
        },
        "rouge-l": {
          "r": 0.11304347826086956,
          "p": 0.26,
          "f": 0.1575757533516989
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/EM/2501.07615v1",
      "true_abstract": "Climate change is increasing the frequency and severity of natural disasters\nworldwide. Media coverage of these events may be vital to generate empathy and\nmobilize global populations to address the common threat posed by climate\nchange. Using a dataset of 466 news sources from 123 countries, covering 135\nmillion news articles since 2016, we apply an event study framework to measure\ncross-border media activity following natural disasters. Our results shows that\nwhile media attention rises after disasters, it is heavily skewed towards\ncertain events, notably earthquakes, accidents, and wildfires. In contrast,\nclimatologically salient events such as floods, droughts, or extreme\ntemperatures receive less coverage. This cross-border disaster reporting is\nstrongly related to the number of deaths associated with the event, especially\nwhen the affected populations share strong social ties or genetic similarities\nwith those in the reporting country. Achieving more balanced media coverage\nacross different types of natural disasters may be essential to counteract\nskewed perceptions. Further, fostering closer social connections between\ncountries may enhance empathy and mobilize the resources necessary to confront\nthe global threat of climate change.",
      "generated_abstract": "r presents a comprehensive analysis of the impact of the COVID-19\npulse on the financial markets in the U.S. using the dynamic panel data model\nwith event history. Our analysis focuses on the financial markets and the\neconomy during the first wave of the pandemic and the economic recovery from\nthe second wave. Our results show that the financial market's behavior was\nvery sensitive to the pandemic and the economic recession. The financial\nmarkets' volatility increased significantly, and the probability of stock\nmarket crashes increased. The probability of stock market crashes was 60% higher\nin the first quarter of 2020 compared to the first quarter of 2019. The\nprobability of stock market crashes also increased in the second quarter of\n2020 compared to the first quarter of 2020. The probability of stock market\ncrashes in the second",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09230769230769231,
          "p": 0.1935483870967742,
          "f": 0.12499999562717029
        },
        "rouge-2": {
          "r": 0.017543859649122806,
          "p": 0.03260869565217391,
          "f": 0.02281368366407007
        },
        "rouge-l": {
          "r": 0.09230769230769231,
          "p": 0.1935483870967742,
          "f": 0.12499999562717029
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2412.03883v1",
      "true_abstract": "To advance the understanding of cellular metabolisms and control\nbatch-to-batch variations in cell culture processes, a multi-scale mechanistic\nmodel with a bottom-up and top-down structure was developed to simulate the\ndynamics of cell culture process undergoing metabolic state transitions. This\nmodel integrates interactions at the molecular, cellular, and macro-kinetic\nlevels, accounting for inherent variations in metabolic state transitions of\nindividual cells. By incorporating both online (e.g., oxygen uptake, pH) and\noffline measurements (e.g., viable cell density, metabolite concentrations),\nthe proposed mechanistic model enables accurate long-term prediction of cell\nculture trajectories and provides reliable prediction intervals quantifying\nbatch-to-batch variations. This work can guide optimal design of experiments\nand robust process control to improve yield and production stability.\nAdditionally, the proposed multi-scale model has a modular design enables\nflexible in silico simulations and extrapolation across diverse conditions,\nproviding a robust prediction framework for scalable and flexible\nbiomanufacturing applications.",
      "generated_abstract": "f computational models to study protein-ligand interactions has\nimpacted drug discovery and the development of biotherapeutics, but current\napproaches are limited by the availability of large-scale datasets that are\nchallenging to process. In this study, we introduce the Protein-Ligand-Drug\n(PLD) dataset, a large-scale benchmark dataset for studying protein-ligand\ninteractions. The dataset consists of 2,855 protein-ligand pairs from 215\nproteins, each of which is paired with 113 ligands, including small molecules\nand peptides. The PLD dataset is constructed by leveraging the publicly\navailable PDB data, which contains 18,321 protein-ligand pairs. The dataset\nenables the evaluation of current computational models for protein-ligand\ninteractions, including the Gromacs-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11224489795918367,
          "p": 0.1527777777777778,
          "f": 0.12941175982283754
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11224489795918367,
          "p": 0.1527777777777778,
          "f": 0.12941175982283754
        }
      }
    },
    {
      "paper_id": "physics.plasm-ph.physics/space-ph/2503.00620v1",
      "true_abstract": "This study explores the generation of Electrostatic (ES) Electron\nKelvin-Helmholtz instability (EKHI) in collisionless plasma with a\nstep-function electron velocity shear akin to that developed in the electron\ndiffusion region in magnetic reconnection. In incompressible plasma, ES EKHI\ndoesn't arise in any velocity shear profile due to the decoupling of the\nelectric potential from the electron momentum equation. Instead a fluid-like\nKelvin-Helmholtz instability (KHI) can arise. However, in compressible plasma,\nthe compressibility couples the electric potential with the electron dynamics,\nleading to the emergence of a new ES mode EKHI on Debye length $\\lambda_{De}$,\naccompanied by the co-generation of an electron acoustic-like wave. The minimum\nthreshold of ES EKHI is $\\Delta \\mathbf{U}> 2c_{se}$, i.e., the electron\nvelocity shear larger than twice the electron acoustic speed $c_{se}$. The\ncorresponding growth rate is $Im(\\omega) = ((\\Delta \\mathbf{U}/c_{se})^2 -\n4)^{1/2} \\omega_{pe}$, where $\\omega_{pe}$ is the electron plasma frequency.",
      "generated_abstract": "term monitoring of the Earth's magnetic field is a key aspect in\nthe understanding of its geomagnetic and geophysical processes. In the past,\nmeasurements of the magnetic field were performed by observing the Earth's\nmagnetic dipole moment, but this method is limited by the low sensitivity and\nresolution of the magnetic field sensors and the high noise level of the\ninstruments. Recent advances in magnetometers have made it possible to measure\nthe magnetic field in the low-frequency regime, which is a promising alternative\nto the dipole moment. This work describes a new technique to measure the\nmagnetic field in the low-frequency regime by using the so-called interferometric\nmagnetometer (IM). The IM is based on the interferometric principle, which\nallows to measure the magnetic field by measuring the displacement of the\nrefracted",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13402061855670103,
          "p": 0.17567567567567569,
          "f": 0.15204677871618633
        },
        "rouge-2": {
          "r": 0.03759398496240601,
          "p": 0.04716981132075472,
          "f": 0.04184099924791291
        },
        "rouge-l": {
          "r": 0.12371134020618557,
          "p": 0.16216216216216217,
          "f": 0.14035087228343782
        }
      }
    },
    {
      "paper_id": "physics.comp-ph.physics/comp-ph/2503.10263v1",
      "true_abstract": "A new parallelized simulation code is presented, which uses a Monte Carlo\nmethod to determine particle spectra in the KATRIN source. Reaction chains are\ngenerated from the decay of tritium within the source. The code includes all\nrelevant processes: elastic scattering, ionization, excitation (electric,\nvibrational, rotational), recombination and various clustering processes. The\nmain emphasis of the code is the calculation of particle spectra and particle\ndensities and currents at specific points within the source. It features a new\ntechnique to determine these quantities. It also calculates target fields for\nthe interaction of particles with each other as it is needed for recombination\nprocesses. The code has been designed for the KATRIN experiment but is easily\nadapt-able for other tritium based experiments like Project 8. Geometry and\nbackground tritium gas flow can be given as user input. The code is\nparallelized using MPI and writes output using HDF5. Input to the simulation is\nread from a JSON description.",
      "generated_abstract": "e a new concept for a multi-functional electronic circuit that can\nbe fabricated by a single-step lithography process, where the circuit is\nformed in a silicon substrate. The circuit consists of a capacitor, a\nresonator, and a transmission line. The circuit is connected to a digital\ntransceiver that transmits data over the transmission line. The circuit is\ndesigned to operate in the mid-infrared range, with a maximum frequency of 5\nGHz, which is well above the 100 MHz bandwidth of the transmission line. The\nresonator is a single-frequency resonant circuit, and the transmission line is\na double-frequency resonant circuit. A 5 GHz mid-infrared transceiver is\nestablished between the resonator and the transmission line, and the\ntransmission line is connected to a digital transceiver. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1553398058252427,
          "p": 0.25396825396825395,
          "f": 0.1927710796276674
        },
        "rouge-2": {
          "r": 0.027586206896551724,
          "p": 0.04,
          "f": 0.032653056393170225
        },
        "rouge-l": {
          "r": 0.14563106796116504,
          "p": 0.23809523809523808,
          "f": 0.18072288685658308
        }
      }
    },
    {
      "paper_id": "cs.IR.cs/IR/2503.08452v1",
      "true_abstract": "We propose Knowledge-Aware Preprocessing (KAP), a two-stage preprocessing\nframework tailored for Traditional Chinese non-narrative documents, designed to\nenhance retrieval accuracy in Hybrid Retrieval systems. Hybrid Retrieval, which\nintegrates Sparse Retrieval (e.g., BM25) and Dense Retrieval (e.g., vector\nembeddings), has become a widely adopted approach for improving search\neffectiveness. However, its performance heavily depends on the quality of input\ntext, which is often degraded when dealing with non-narrative documents such as\nPDFs containing financial statements, contractual clauses, and tables. KAP\naddresses these challenges by integrating Multimodal Large Language Models\n(MLLMs) with LLM-driven post-OCR processing, refining extracted text to reduce\nOCR noise, restore table structures, and optimize text format. By ensuring\nbetter compatibility with Hybrid Retrieval, KAP improves the accuracy of both\nSparse and Dense Retrieval methods without modifying the retrieval architecture\nitself.",
      "generated_abstract": "r presents the first comprehensive survey of the field of\nHuman-Computer Interaction (HCI) in Libraries and Archives, exploring its\nhistory, current trends, and future directions. The survey draws on a wide\nrange of literature from academic journals and conferences, and includes\nover 200 survey respondents from libraries and archives around the world. It\nprovides a comprehensive overview of the field's current state-of-the-art and\ncurrent challenges, as well as future research directions. The survey results\nhighlight the importance of incorporating user research into library and\narchives design and operations, as well as the need for increased focus on\nuser-centric design and user experience. The survey findings provide valuable\ninsights into the current state of the field and highlight key areas for\nfurther research and innovation. The survey is available online at\nhttps://survey",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09345794392523364,
          "p": 0.12658227848101267,
          "f": 0.10752687683373822
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08411214953271028,
          "p": 0.11392405063291139,
          "f": 0.09677418866169524
        }
      }
    },
    {
      "paper_id": "math.SP.math/SP/2503.03248v1",
      "true_abstract": "We introduce and study a new theoretical concept of $\\textit{spectral pair}$\nfor a Schr\\\"{o}dinger operator $H$ in $L^2(\\mathbb{R}_{+})$ with a bounded\n$\\textit{complex-valued}$ potential. The spectral pair consists of a measure\nand a complex-valued function, both of which are defined on $\\mathbb{R}_{+}$.\nWe show that in many ways, the spectral pair generalises the classical spectral\nmeasure to the non-self-adjoint case. First, extending the classical\nBorg-Marchenko theorem, we prove a uniqueness result: the spectral pair\nuniquely determines the potential. Second, we derive asymptotic formulas for\nthe spectral pair in the spirit of the classical result of Marchenko. In the\ncase of real-valued potentials, we relate the spectral pair to the spectral\nmeasure of $H$. Lastly, we provide formulas for the spectral pair at a~simple\neigenvalue of $|H|$.",
      "generated_abstract": "We prove the existence of a global solution to the nonlinear hyperbolic\nsystem arising in the theory of fluid-structure interaction with a nonlinear\nelasticity, for the case where the elasticity is linear. We also prove the\nexistence of a global weak solution to the nonlinear hyperbolic system arising\nin the theory of fluid-structure interaction with a nonlinear elasticity.\nThese results generalize the existence of a weak solution to the nonlinear\nhyperbolic system arising in the theory of fluid-structure interaction with a\nlinear elasticity.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14084507042253522,
          "p": 0.3333333333333333,
          "f": 0.19801979780413695
        },
        "rouge-2": {
          "r": 0.05555555555555555,
          "p": 0.13636363636363635,
          "f": 0.07894736430747944
        },
        "rouge-l": {
          "r": 0.1267605633802817,
          "p": 0.3,
          "f": 0.17821781760611716
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.04851v2",
      "true_abstract": "In protein engineering, while computational models are increasingly used to\npredict mutation effects, their evaluations primarily rely on high-throughput\ndeep mutational scanning (DMS) experiments that use surrogate readouts, which\nmay not adequately capture the complex biochemical properties of interest. Many\nproteins and their functions cannot be assessed through high-throughput methods\ndue to technical limitations or the nature of the desired properties, and this\nis particularly true for the real industrial application scenario. Therefore,\nthe desired testing datasets, will be small-size (~10-100) experimental data\nfor each protein, and involve as many proteins as possible and as many\nproperties as possible, which is, however, lacking. Here, we present\nVenusMutHub, a comprehensive benchmark study using 905 small-scale experimental\ndatasets curated from published literature and public databases, spanning 527\nproteins across diverse functional properties including stability, activity,\nbinding affinity, and selectivity. These datasets feature direct biochemical\nmeasurements rather than surrogate readouts, providing a more rigorous\nassessment of model performance in predicting mutations that affect specific\nmolecular functions. We evaluate 23 computational models across various\nmethodological paradigms, such as sequence-based, structure-informed and\nevolutionary approaches. This benchmark provides practical guidance for\nselecting appropriate prediction methods in protein engineering applications\nwhere accurate prediction of specific functional properties is crucial.",
      "generated_abstract": "tudy, we introduce a novel framework for studying the emergence of\nsymmetry-breaking in large-scale neural networks. By using a novel data\nprocessing method that incorporates a pairwise correlation matrix, we construct\na network that simulates the activity of a population of neurons. We then\nperform a large-scale analysis of the resulting network to determine how\nsymmetry-breaking behaviors manifest in the network. We find that symmetry-breaking\nbehaviors are robust, persisting in the face of perturbations to the\nactivation functions of the neurons. Furthermore, we show that symmetry-breaking\nis sensitive to the degree of correlation between the activation functions of\nneurons, with the presence of stronger correlations leading to more pronounced\nsymmetry-breaking behaviors. We also find that symmetry-breaking can be\ninduced by perturbations to the activity of individual neurons, demonstr",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10526315789473684,
          "p": 0.22535211267605634,
          "f": 0.1434977535072092
        },
        "rouge-2": {
          "r": 0.00510204081632653,
          "p": 0.009009009009009009,
          "f": 0.006514653363752489
        },
        "rouge-l": {
          "r": 0.09210526315789473,
          "p": 0.19718309859154928,
          "f": 0.1255605337762675
        }
      }
    },
    {
      "paper_id": "cs.SI.cs/SI/2503.09725v1",
      "true_abstract": "Avian Influenza Virus (AIV) poses significant threats to the poultry\nindustry, humans, domestic animals, and wildlife health worldwide. Monitoring\nthis infectious disease is important for rapid and effective response to\npotential outbreaks. Conventional avian influenza surveillance systems have\nexhibited limitations in providing timely alerts for potential outbreaks. This\nstudy aimed to examine the idea of using online activity on social media, and\nGoogle searches to improve the identification of AIV in the early stage of an\noutbreak in a region. To this end, to evaluate the feasibility of this\napproach, we collected historical data on online user activities from X\n(formerly known as Twitter) and Google Trends and assessed the statistical\ncorrelation of activities in a region with the AIV outbreak officially reported\ncase numbers. In order to mitigate the effect of the noisy content on the\noutbreak identification process, large language models were utilized to filter\nout the relevant online activity on X that could be indicative of an outbreak.\nAdditionally, we conducted trend analysis on the selected internet-based data\nsources in terms of their timeliness and statistical significance in\nidentifying AIV outbreaks. Moreover, we performed an ablation study using\nautoregressive forecasting models to identify the contribution of X and Google\nTrends in predicting AIV outbreaks. The experimental findings illustrate that\nonline activity on social media and search engine trends can detect avian\ninfluenza outbreaks, providing alerts earlier compared to official reports.\nThis study suggests that real-time analysis of social media outlets and Google\nsearch trends can be used in avian influenza outbreak early warning systems,\nsupporting epidemiologists and animal health professionals in informed\ndecision-making.",
      "generated_abstract": "st few years, the emergence of quantum computers has sparked\ninterest in quantum mechanics as a formalism to describe systems with\nnon-classical properties. However, in quantum physics, the notion of a\nfundamental unit of a system (such as a qubit) is not well-defined. One way to\ndefine a fundamental unit of a system is by defining the physical size of the\nsystem, which is in turn a function of the system's Hamiltonian and the\ngeometrical configuration of the system. In this paper, we use the framework of\nquantum computers to model the interaction between a quantum system and a\nclassical system using a Hamiltonian that describes the interaction between\nquantum systems. We consider the quantum system to be a qubit, and the\nclassical system to be the environment. We show that this interaction can be\ndescribed by the Schr\\\"odinger equation with a Hamiltonian that is a sum of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10828025477707007,
          "p": 0.2328767123287671,
          "f": 0.14782608262344057
        },
        "rouge-2": {
          "r": 0.00819672131147541,
          "p": 0.015873015873015872,
          "f": 0.01081080631935906
        },
        "rouge-l": {
          "r": 0.09554140127388536,
          "p": 0.2054794520547945,
          "f": 0.1304347782756145
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.14154v1",
      "true_abstract": "In allocating objects via lotteries, it is common to consider ordinal rules\nthat rely solely on how agents rank degenerate lotteries. While ordinality is\noften imposed due to cognitive or informational constraints, we provide another\njustification from an axiomatic perspective: for three-agent problems, the\ncombination of efficiency, strategy-proofness, non-bossiness, and a weak form\nof continuity collectively implies ordinality.",
      "generated_abstract": "In this paper, we study a class of sequential decision problems where agents\nstrategically choose their actions from a finite set of actions, and the\noutcomes depend on their past actions. The agents' actions are observed by a\nsingle decision maker who has access to all past actions and can observe the\noutcomes of the agents' actions. We consider a two-agent problem where the\ndecision maker has two actions to choose between. In this problem, the agents\nare not perfectly rational and make mistakes when making their actions. We show\nthat the decision maker can always make the best possible choice if the agents\nare imperfectly rational. This result is new even in the case where the agents\nare perfectly rational.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24074074074074073,
          "p": 0.18571428571428572,
          "f": 0.20967741443808544
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.2037037037037037,
          "p": 0.15714285714285714,
          "f": 0.17741934992195643
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.12731v1",
      "true_abstract": "We address counterfactual analysis in empirical models of games with\npartially identified parameters, and multiple equilibria and/or randomized\nstrategies, by constructing and analyzing the counterfactual predictive\ndistribution set (CPDS). This framework accommodates various outcomes of\ninterest, including behavioral and welfare outcomes. It allows a variety of\nchanges to the environment to generate the counterfactual, including\nmodifications of the utility functions, the distribution of utility\ndeterminants, the number of decision makers, and the solution concept. We use a\nBayesian approach to summarize statistical uncertainty. We establish conditions\nunder which the population CPDS is sharp from the point of view of\nidentification. We also establish conditions under which the posterior CPDS is\nconsistent if the posterior distribution for the underlying model parameter is\nconsistent. Consequently, our results can be employed to conduct counterfactual\nanalysis after a preliminary step of identifying and estimating the underlying\nmodel parameter based on the existing literature. Our consistency results\ninvolve the development of a new general theory for Bayesian consistency of\nposterior distributions for mappings of sets. Although we primarily focus on a\nmodel of a strategic game, our approach is applicable to other structural\nmodels with similar features.",
      "generated_abstract": "This paper examines the interplay between the estimation of non-linear\ndifferential equations (NLEs) and non-linear regression models (NLRMs) by\ndeveloping an efficient estimator for the linear-quadratic-Gaussian (LQG)\nprocess. We first propose a linear-quadratic-Gaussian estimator for the NLE\nparameters, and then combine the estimator with a linear-quadratic-Gaussian\nestimator for the NLRM parameters. The proposed estimators are efficient,\nstable, and computationally simple. Furthermore, we show that the proposed\nestimators are consistent for the NLE parameters and NLRM parameters. The\ntheoretical properties of the proposed estimators are also established. Finally,\nwe apply the proposed estimators to the case of the linear-quadratic Gaussian\n(LQG) model in the financial context.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1452991452991453,
          "p": 0.2698412698412698,
          "f": 0.188888884338889
        },
        "rouge-2": {
          "r": 0.0223463687150838,
          "p": 0.046511627906976744,
          "f": 0.030188674861089993
        },
        "rouge-l": {
          "r": 0.1282051282051282,
          "p": 0.23809523809523808,
          "f": 0.16666666211666678
        }
      }
    },
    {
      "paper_id": "math.AP.physics/class-ph/2503.10300v1",
      "true_abstract": "We derive a new approach to analyze the coupling of linear Boussinesq and\nSaint-Venant shallow water wave equations in the case where the interface\nremains at a constant position in space. We propose a one-way coupling model as\na reference, which allows us to obtain an analytical solution, prove the\nwell-posedness of the original coupled model and compute what we call the\ncoupling error-a quantity that depends solely on the choice of transmission\nconditions at the interface. We prove that this coupling error is\nasymptotically small for a certain class of data and discuss its role as a\nproxy for the full error with respect to the 3D water wave problem.\nAdditionally, we highlight that this error can be easily computed in other\nscenarios. We show that the coupling error consists of reflected waves and\nargue that this explains some previously unexplained spurious oscillations\nreported in the literature. Finally, we prove the well-posedness of the\nhalf-line linear Boussinesq problem.",
      "generated_abstract": "er a single-species, non-reversible system in a stochastic\nenvironment where the system's probability distribution is described by a\nnon-Markovian, non-stationary, non-linear, self-consistent equation. We show\nthat this equation may be solved exactly for large enough system size and\nparameter values. This allows us to derive an exact expression for the\nstationary probability distribution function, and to calculate the long-time\naverage of the system's total energy and entropy. Our results demonstrate that\nthis equation possesses both stationary and dynamical stationarity, and\ncorresponds to a generalized Gibbs-Bogoliubov equation. We then consider a\ngeneralized Gibbs-Bogoliubov equation for a general class of non-Markovian,\nnon-stationary, non-linear, self-consistent equations, and show that it may be\nsolved exactly for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1958762886597938,
          "p": 0.2753623188405797,
          "f": 0.2289156577928583
        },
        "rouge-2": {
          "r": 0.06944444444444445,
          "p": 0.10526315789473684,
          "f": 0.083682003578369
        },
        "rouge-l": {
          "r": 0.17525773195876287,
          "p": 0.2463768115942029,
          "f": 0.2048192722506896
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/ST/2503.00603v1",
      "true_abstract": "Signature methods have been widely and effectively used as a tool for feature\nextraction in statistical learning methods, notably in mathematical finance.\nThey lack, however, interpretability: in the general case, it is unclear why\nsignatures actually work. The present article aims to address this issue\ndirectly, by introducing and developing the concept of signature perturbations.\nIn particular, we construct a regular perturbation of the signature of the term\nstructure of log prices for various commodities, in terms of the convenience\nyield. Our perturbation expansion and rigorous convergence estimates help\nexplain the success of signature-based classification of commodities markets\naccording to their term structure, with the volatility of the convenience yield\nas the major discriminant.",
      "generated_abstract": "We consider the problem of optimal control of a finite-dimensional\ndifferential-difference system, where the state is a vector-valued process and\nthe control is a vector-valued process. In the case of the linear diffusion\nprocess, the optimal control problem is well-known. However, in the case of\nnonlinear diffusion processes, the optimal control problem has received little\nattention. In this paper, we consider the problem of optimal control of a\nnonlinear diffusion process. To solve this problem, we develop a novel\nsemi-Lagrangian method for nonlinear optimal control. By applying this method,\nwe derive the optimal control solutions of the nonlinear diffusion process.\nOur results demonstrate that the optimal control problems of nonlinear\ndiffusion processes are different from those of linear diffusion processes.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12941176470588237,
          "p": 0.19298245614035087,
          "f": 0.15492957265919474
        },
        "rouge-2": {
          "r": 0.018518518518518517,
          "p": 0.022222222222222223,
          "f": 0.02020201524334373
        },
        "rouge-l": {
          "r": 0.12941176470588237,
          "p": 0.19298245614035087,
          "f": 0.15492957265919474
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2502.02397v1",
      "true_abstract": "Many data problems contain some reference or normal conditions, upon which to\ncompare newly collected data. This scenario occurs in data collected as part of\nclinical trials to detect adverse events, or for measuring climate change\nagainst historical norms. The data is typically multivariate, and often the\nnormal ranges are specified by a multivariate normal distribution. The work\npresented in this paper develops methods to compare the new sample against the\nreference distribution with high-dimensional visualisation. It uses a\nprojection pursuit guided tour to produce a sequence of low-dimensional\nprojections steered towards those where the new sample is most different from\nthe reference. A new projection pursuit index is defined for this purpose. The\ntour visualisation also includes drawing of the projected ellipse, which is\ncomputed analytically, corresponding to the reference distribution. The methods\nare implemented in the R package, tourr.",
      "generated_abstract": "nt COVID-19 pandemic has led to a significant increase in the\ncovid-19 exposure rate in the United States. This paper presents a novel\napproach to modeling covid-19 exposure rates using a generalized additive\nmodel (GAM). The covid-19 exposure rate is modeled as a time-varying function\nof covid-19 infections, covid-19 deaths, and covid-19 hospitalizations. The\ncovid-19 deaths and hospitalizations are modeled as a time-varying function of\ncovid-19 infections and covid-19 deaths. The covid-19 infections are modeled\nas a time-varying function of covid-19 cases and covid-19 cases growth rate.\nThe covid-19 deaths and hospitalizations",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.2727272727272727,
          "f": 0.17647058385813158
        },
        "rouge-2": {
          "r": 0.007575757575757576,
          "p": 0.015873015873015872,
          "f": 0.010256405882447623
        },
        "rouge-l": {
          "r": 0.13043478260869565,
          "p": 0.2727272727272727,
          "f": 0.17647058385813158
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.00945v1",
      "true_abstract": "Deep learning-based computer-aided diagnosis (CAD) of medical images requires\nlarge datasets. However, the lack of large publicly available labeled datasets\nlimits the development of deep learning-based CAD systems. Generative\nAdversarial Networks (GANs), in particular, CycleGAN, can be used to generate\nnew cross-domain images without paired training data. However, most\nCycleGAN-based synthesis methods lack the potential to overcome alignment and\nasymmetry between the input and generated data. We propose a two-stage\ntechnique for the synthesis of abdominal MRI using cross-modality translation\nof abdominal CT. We show that the synthetic data can help improve the\nperformance of the liver segmentation network. We increase the number of\nabdominal MRI images through cross-modality image transformation of unpaired CT\nimages using a CycleGAN inspired deformation invariant network called EssNet.\nSubsequently, we combine the synthetic MRI images with the original MRI images\nand use them to improve the accuracy of the U-Net on a liver segmentation task.\nWe train the U-Net on real MRI images and then on real and synthetic MRI\nimages. Consequently, by comparing both scenarios, we achieve an improvement in\nthe performance of U-Net. In summary, the improvement achieved in the\nIntersection over Union (IoU) is 1.17%. The results show potential to address\nthe data scarcity challenge in medical imaging.",
      "generated_abstract": "tworks have been used to predict the response of anatomical\nsections to magnetic resonance imaging (MRI) sequences. These models often\nrequire manual annotation of the anatomical structures of interest, limiting\ntheir applicability to clinical settings. We propose a deep learning approach\nthat leverages a large pre-trained model, DINO-ViT-B, to predict the\nanatomical structures from a MRI sequence. By training the model on a\ncomprehensive dataset of MRI sequences, we demonstrate that our approach can\nbe used to predict the anatomical structures of interest with higher accuracy\nthan conventional deep learning models. Additionally, the model's ability to\npredict the anatomical structures of interest has been validated on a\nsecond-party dataset. We evaluate the model's performance using various metrics\nincluding accuracy, sensitivity, specificity, and F1-score. Our results",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18699186991869918,
          "p": 0.2839506172839506,
          "f": 0.22549019129036918
        },
        "rouge-2": {
          "r": 0.0374331550802139,
          "p": 0.06666666666666667,
          "f": 0.04794520087375725
        },
        "rouge-l": {
          "r": 0.16260162601626016,
          "p": 0.24691358024691357,
          "f": 0.19607842658448685
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.14934v1",
      "true_abstract": "Molecular docking that predicts the bound structures of small molecules\n(ligands) to their protein targets, plays a vital role in drug discovery.\nHowever, existing docking methods often face limitations: they either overlook\ncrucial structural changes by assuming protein rigidity or suffer from low\ncomputational efficiency due to their reliance on generative models for\nstructure sampling. To address these challenges, we propose FABFlex, a fast and\naccurate regression-based multi-task learning model designed for realistic\nblind flexible docking scenarios, where proteins exhibit flexibility and\nbinding pocket sites are unknown (blind). Specifically, FABFlex's architecture\ncomprises three specialized modules working in concert: (1) A pocket prediction\nmodule that identifies potential binding sites, addressing the challenges\ninherent in blind docking scenarios. (2) A ligand docking module that predicts\nthe bound (holo) structures of ligands from their unbound (apo) states. (3) A\npocket docking module that forecasts the holo structures of protein pockets\nfrom their apo conformations. Notably, FABFlex incorporates an iterative update\nmechanism that serves as a conduit between the ligand and pocket docking\nmodules, enabling continuous structural refinements. This approach effectively\nintegrates the three subtasks of blind flexible docking-pocket identification,\nligand conformation prediction, and protein flexibility modeling-into a\nunified, coherent framework. Extensive experiments on public benchmark datasets\ndemonstrate that FABFlex not only achieves superior effectiveness in predicting\naccurate binding modes but also exhibits a significant speed advantage (208\n$\\times$) compared to existing state-of-the-art methods. Our code is released\nat https://github.com/tmlr-group/FABFlex.",
      "generated_abstract": "We present a computational approach for estimating the mean and variance of\nreaction rates in stochastic chemical reaction networks. Using the\nMarkov-chain Monte Carlo method, we simulate the time evolution of the\nstochastic reaction network and estimate the mean and variance of reaction\nrates. We apply our method to a simple model of chemical reactions in a cell,\nand we compare our results with those obtained by other numerical methods. We\nalso provide a detailed explanation of the implementation and computational\ndetails of our approach.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08187134502923976,
          "p": 0.2692307692307692,
          "f": 0.1255605345404091
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07602339181286549,
          "p": 0.25,
          "f": 0.11659192467493827
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/optics/2503.09339v1",
      "true_abstract": "We report a single-frequency, narrow-linewidth semiconductor pulsed laser\nbased on pump current modulation and optical injection locking technique. A\nmonolithic non-planar ring oscillator laser is employed as the seed source to\nguarantee the single-frequency narrow-linewidth performance. Simultaneously,\npulse operation is achieved by directly modulating the pump current of the\nsemiconductor laser. The single-frequency pulsed laser (SFPL) has achieved a\npulse repetition rate of 50 kHz-1 MHz, a pulse duration ranging from 120 ns to\na quasi-continuous state, and a peak power of 160 mW. Moreover, the SFPL has\nreached a pulsed laser linewidth as narrow as 905 Hz, optical spectrum\nsignal-to-noise ratio of better than 65 dB at a center wavelength of 1064.45\nnm. Such extremely narrow-linewidth, repetition-rate and pulse-width tunable\nSFPL has great potential for applications in coherent LIDAR, metrology, remote\nsensing, and nonlinear frequency conversion.",
      "generated_abstract": "of single-photon detection in the quantum regime has evolved\ninto a highly competitive field of research. While the field of photon counting\nand single-photon counting methods has developed substantially in recent\nyears, it remains a challenging task to implement quantum sensing methods. One\nof the major challenges in quantum sensing is the low sensitivity of current\nquantum sensing techniques. Therefore, the development of novel quantum\nsensing methods is highly desirable. In this study, we introduce a new\nquantum sensing method based on the quantum state diffusion (QSD) technique.\nThe QSD technique has been proven to be a robust method for quantum state\nmeasurement, which can work under the presence of noise and decoherence. The\nQSD technique has been used for the detection of a single photon in the\nquantum regime. The QSD technique is based on the quantum state diffusion",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1414141414141414,
          "p": 0.1917808219178082,
          "f": 0.16279069278866967
        },
        "rouge-2": {
          "r": 0.015151515151515152,
          "p": 0.017094017094017096,
          "f": 0.016064252046258862
        },
        "rouge-l": {
          "r": 0.1414141414141414,
          "p": 0.1917808219178082,
          "f": 0.16279069278866967
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.20745v1",
      "true_abstract": "Exposure to high ambient temperatures is a significant driver of preventable\nmortality, with non-linear health effects and elevated risks in specific\nregions. To capture this complexity and account for spatial dependencies across\nsmall areas, we propose a Bayesian framework that integrates non-linear\nfunctions with the Besag, York, and Mollie (BYM2) model. Applying this\nframework to all-cause mortality data in Switzerland, we quantified spatial\ninequalities in heat-related mortality. We retrieved daily all-cause mortality\nat small areas (2,145 municipalities) for people older than 65 years from the\nSwiss Federal Office of Public Health and daily mean temperature at\n1km$\\times$1km grid from the Swiss Federal Office of Meteorology. By fully\npropagating uncertainties, we derived key epidemiological metrics, including\nheat-related excess mortality and minimum mortality temperature (MMT).\nHeat-related excess mortality rates were higher in northern Switzerland, while\nlower MMTs were observed in mountainous regions. Further, we explored the role\nof the proportion of individuals older than 85 years, green space, average\ntemperature, deprivation, urbanicity, and language regions in explaining these\ndiscrepancies. We found that spatial disparities in heat-related excess\nmortality were primarily driven by population age distribution, green space,\nand vulnerabilities associated with elevated temperature exposure.",
      "generated_abstract": "nt study investigates the performance of an Adaptive Adaptive\nadaptive Particle Swarm Optimization (Adaptive PSO) algorithm with adaptive\nparameters for the selection of the number of particles (Np), the number of\niterations (Ni), and the parameter of the Euclidean norm (Nm) to solve a\nmulti-objective optimization problem. Adaptive PSO is an adaptive particle\nswarm optimization (PSO) algorithm that adapts the number of particles, the\nnumber of iterations, and the Euclidean norm parameter to achieve a\ncombination of the objective function values that minimizes the objective\nfunction. A modified PSO algorithm is developed for multi-objective\noptimization. The algorithm is designed to balance the convergence speed and\nthe computational complexity of the algorithm. A simulation study was\nconducted to evaluate the performance of the proposed algorithm. The results\nshow that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07874015748031496,
          "p": 0.14925373134328357,
          "f": 0.10309277898342031
        },
        "rouge-2": {
          "r": 0.00558659217877095,
          "p": 0.009345794392523364,
          "f": 0.006993002309896041
        },
        "rouge-l": {
          "r": 0.07086614173228346,
          "p": 0.13432835820895522,
          "f": 0.0927835006329049
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2501.17463v1",
      "true_abstract": "We discuss generalized linear models for directional data where the\nconditional distribution of the response is a von Mises-Fisher distribution in\narbitrary dimension or a Bingham distribution on the unit circle. To do this\nproperly, we parametrize von Mises-Fisher distributions by Euclidean parameters\nand investigate computational aspects of this parametrization. Then we modify\nthis approach for local polynomial regression as a means of nonparametric\nsmoothing of distributional data. The methods are illustrated with simulated\ndata and a data set from planetary sciences involving covariate vectors on a\nsphere with axial response.",
      "generated_abstract": "We propose a novel, robust approach for model selection in longitudinal\ndata analysis. This approach builds upon a generalization of the classical\nBayesian information criterion (BIC) by incorporating a novel term, the\n$\\phi$-BIC, which accounts for the non-normality of the longitudinal data.\nFurthermore, we propose an algorithm to estimate the $\\phi$-BIC and use this\ninformation to improve the selection of a particular model. We apply the\nproposed approach to a real data example and demonstrate its effectiveness in\nselecting between competing linear models for repeated blood pressure measurements\nfrom a single individual.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22388059701492538,
          "p": 0.23809523809523808,
          "f": 0.23076922577396458
        },
        "rouge-2": {
          "r": 0.0449438202247191,
          "p": 0.0449438202247191,
          "f": 0.04494381522471966
        },
        "rouge-l": {
          "r": 0.19402985074626866,
          "p": 0.20634920634920634,
          "f": 0.19999999500473387
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.03515v1",
      "true_abstract": "We consider Inverse Optimal Stopping (IOS) problem where, based on stopped\nexpert trajectories, one aims to recover the optimal stopping region through\ncontinuation and stopping gain functions approximation. The uniqueness of the\nstopping region allows the use of IOS in real-world applications with safety\nconcerns. While current state-of-the-art inverse reinforcement learning methods\nrecover both a Q-function and the corresponding optimal policy, they fail to\naccount for specific challenges posed by optimal stopping problems. These\ninclude data sparsity near the stopping region, non-Markovian nature of the\ncontinuation gain, a proper treatment of boundary conditions, the need for a\nstable offline approach for risk-sensitive applications, and a lack of a\nquality evaluation metric. These challenges are addressed with the proposed\nDynamics-Aware Offline Inverse Q-Learning for Optimal Stopping (DO-IQS), which\nincorporates temporal information by approximating the cumulative continuation\ngain together with the world dynamics and the Q-function without querying to\nthe environment. Moreover, a confidence-based oversampling approach is proposed\nto treat the data sparsity problem. We demonstrate the performance of our\nmodels on real and artificial data including an optimal intervention for\ncritical events problem.",
      "generated_abstract": "In this paper, we investigate the practical challenges of learning\ncomplex, high-dimensional models from low-dimensional data. We focus on\nconditional generative models, particularly the variational inference\napproach, and investigate the computational and statistical challenges of\nlearning from data that is limited in both dimensions and diversity. We\ndemonstrate that a lack of data diversity or limited data volume can lead to\nhigh computational and statistical costs, resulting in suboptimal learning\nperformance. To address these challenges, we introduce a novel framework for\nlearning from limited data that combines the power of variational inference\nwith the benefits of stochastic optimization. We demonstrate that our method\ncan effectively learn complex conditional generative models from limited data,\nachieving superior performance compared to existing methods. Our findings\nhighlight the importance of considering data diversity and data volume when\nlearning from limited data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17796610169491525,
          "p": 0.2625,
          "f": 0.2121212073053771
        },
        "rouge-2": {
          "r": 0.023121387283236993,
          "p": 0.03418803418803419,
          "f": 0.027586202082997273
        },
        "rouge-l": {
          "r": 0.13559322033898305,
          "p": 0.2,
          "f": 0.16161615680032665
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.18875v1",
      "true_abstract": "We study competitive equilibria in exchange economies when a continuum of\ngoods is conflated into a finite set of commodities. The design of conflation\nchoices affects the allocation of scarce resources among agents, by\nconstraining trading opportunities and shifting competitive pressures. We\ndemonstrate the consequences on relative prices, trading positions, and\nwelfare.",
      "generated_abstract": "f financial incentives to reduce incentive-driven behavior is\ntraditionally associated with the field of game theory. However, in the field\nof industrial organization, the use of financial incentives to induce behavior\nthat is costly for the agent to change has been observed for over 100 years.\nThis paper introduces a formal model for financial incentives to induce\ncostly behavior. This model provides a theoretical foundation for understanding\nthe role of financial incentives in a wide range of contexts, including\nenvironmental policy, labor market interventions, and public policy that\nencourages consumer behavior. The paper shows that the optimal incentives for\ninducing costly behavior are determined by the value of the cost of the costly\nbehavior. The paper also discusses the implications of the optimal incentives\nfor the use of financial incentives to induce costly behavior",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18604651162790697,
          "p": 0.11428571428571428,
          "f": 0.14159291563943943
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.16279069767441862,
          "p": 0.1,
          "f": 0.12389380059519166
        }
      }
    },
    {
      "paper_id": "quant-ph.physics/chem-ph/2503.09670v1",
      "true_abstract": "Solving challenging problems in quantum chemistry is one of the leading\npromised applications of quantum computers. Within the quantum algorithms\nproposed for problems in excited state quantum chemistry, subspace-based\nquantum algorithms, including quantum subspace expansion (QSE), quantum\nequation of motion (qEOM) and quantum self-consistent equation-of-motion\n(q-sc-EOM), are promising for pre-fault-tolerant quantum devices. The working\nequation of QSE and qEOM requires solving a generalized eigenvalue equation\nwith associated matrix elements measured on a quantum computer. Our\nquantitative analysis of the QSE method shows that the errors in eigenvalues\nincrease drastically with an increase in the condition number of the overlap\nmatrix when a generalized eigenvalue equation is solved in the presence of\nstatistical sampling errors. This makes such methods unstable to errors that\nare unavoidable when using quantum computers. Further, at very high condition\nnumbers of overlap matrix, the QSE's working equation could not be solved\nwithout any additional steps in the presence of sampling errors as it becomes\nill-conditioned. It was possible to use the thresholding technique in this case\nto solve the equation, but the solutions achieved had missing excited states,\nwhich may be a problem for future chemical studies. We also show that\nexcited-state methods that have an eigenvalue equation as the working equation,\nsuch as q-sc-EOM, do not have such problems and could be suitable candidates\nfor excited-state quantum chemistry calculations using quantum computers.",
      "generated_abstract": "t a complete theoretical framework for the study of the quantum\ncoherence and entanglement of the vibrational and rotational states of a\nmolecular system, focusing on the vibrational ground-state manifold. We\nconsider the non-Hermitian Hamiltonian\n$\\hat{H}=\\hat{H}_{\\mathrm{kin}}+\\hat{H}_{\\mathrm{int}}$, where\n$\\hat{H}_{\\mathrm{kin}}$ and $\\hat{H}_{\\mathrm{int}}$ describe the kinetic\nenergy and the interaction energy, respectively. The latter is represented by\nthe non-Hermitian Hamiltonian operator $\\hat{H}_{\\mathrm{int}}=\\hat{H}_{a}+\\hat{H}_{b}$,\nwhere $\\hat{H}_{a}$ and $\\hat{H}_{b}$ are the atomic Hamiltonian and the\nelectro-magnetic field Hamiltonian, respectively. We derive the\nHamiltonian-operator-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08396946564885496,
          "p": 0.21153846153846154,
          "f": 0.12021857516677131
        },
        "rouge-2": {
          "r": 0.009615384615384616,
          "p": 0.028169014084507043,
          "f": 0.014336913768323613
        },
        "rouge-l": {
          "r": 0.08396946564885496,
          "p": 0.21153846153846154,
          "f": 0.12021857516677131
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.16355v1",
      "true_abstract": "We study monotonicity testing of high-dimensional distributions on\n$\\{-1,1\\}^n$ in the model of subcube conditioning, suggested and studied by\nCanonne, Ron, and Servedio~\\cite{CRS15} and Bhattacharyya and\nChakraborty~\\cite{BC18}. Previous work shows that the \\emph{sample complexity}\nof monotonicity testing must be exponential in $n$ (Rubinfeld,\nVasilian~\\cite{RV20}, and Aliakbarpour, Gouleakis, Peebles, Rubinfeld,\nYodpinyanee~\\cite{AGPRY19}). We show that the subcube \\emph{query complexity}\nis $\\tilde{\\Theta}(n/\\varepsilon^2)$, by proving nearly matching upper and\nlower bounds. Our work is the first to use directed isoperimetric inequalities\n(developed for function monotonicity testing) for analyzing a distribution\ntesting algorithm. Along the way, we generalize an inequality of Khot, Minzer,\nand Safra~\\cite{KMS18} to real-valued functions on $\\{-1,1\\}^n$.\n  We also study uniformity testing of distributions that are promised to be\nmonotone, a problem introduced by Rubinfeld, Servedio~\\cite{RS09} , using\nsubcube conditioning. We show that the query complexity is\n$\\tilde{\\Theta}(\\sqrt{n}/\\varepsilon^2)$. Our work proves the lower bound,\nwhich matches (up to poly-logarithmic factors) the uniformity testing upper\nbound for general distributions (Canonne, Chen, Kamath, Levi,\nWaingarten~\\cite{CCKLW21}). Hence, we show that monotonicity does not help,\nbeyond logarithmic factors, in testing uniformity of distributions with subcube\nconditional queries.",
      "generated_abstract": "aper, we propose a novel method for constructing robust confidence\nintervals for the average over the random variables in a non-central F-distribution\nwith non-centrality parameter $\\beta$ under the null hypothesis. Our method\ninvolves two steps. First, we construct a confidence set for the random variable\n$X$ by applying a non-central t-distribution with parameters $\\beta$ and\n$\\beta(1-\\beta)$, where the non-centrality parameter $\\beta$ is\nindependently chosen from a known distribution. Then, for each such random\nvariable, we construct an optimal confidence set for the random variable $X$ by\nselecting the largest such confidence set. We establish a strong law of large\nnumbers for the random variables under the null hypothesis, and derive a\ngeneralized central limit theorem for the random variables under the null\nhypothesis. Numerical simulations demonstrate that our method yields more",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13157894736842105,
          "p": 0.19736842105263158,
          "f": 0.15789473204210538
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13157894736842105,
          "p": 0.19736842105263158,
          "f": 0.15789473204210538
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.09174v1",
      "true_abstract": "This paper examines the number of communication modes, that is, the degrees\nof freedom (DoF), in a wireless setup comprising a small continuous linear\nintelligent antenna array in the near field of a large one. The framework\nallows for any orientations between the arrays and any positions in a\ntwo-dimensional space assuming that the transmitting array is placed at the\norigin. Therefore, apart from the length of the two continuous arrays, four key\nparameters determine the DoF and are hence considered in the analysis: the\nCartesian coordinates of the center of the receiving array and two angles that\nmodel the rotation of each array around its center. The paper starts with the\ncalculation of the deterministic DoF for a generic geometric setting, which\nextends beyond the widely studied paraxial case. Subsequently, a stochastic\ngeometry framework is proposed to study the statistical DoF, as a first step\ntowards the investigation of the system-level performance in near field\nnetworks. Numerical results applied to millimeter wave networks reveal the\nlarge number of DoF provided by near-field communications and unveiled key\nsystem-level insights.",
      "generated_abstract": "This paper presents a low-complexity algorithm for the joint estimation of\nthe channel and the precoder in the presence of multipath interference in\nmulti-user, multi-antenna systems. The proposed algorithm uses an iterative\nmethod that exploits the non-convexity of the joint channel-precoder\noptimization problem. The proposed algorithm is shown to converge to the\noptimal solution, while achieving high computational efficiency. The numerical\nresults confirm that the proposed algorithm converges to the optimal solution\nand achieves a computational complexity of $O(M^2N)$ with $M$ number of\nantennas and $N$ number of users. Moreover, the effectiveness of the proposed\nalgorithm is demonstrated through simulation studies.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1415929203539823,
          "p": 0.25,
          "f": 0.1807909558351688
        },
        "rouge-2": {
          "r": 0.029411764705882353,
          "p": 0.056818181818181816,
          "f": 0.03875968542755896
        },
        "rouge-l": {
          "r": 0.13274336283185842,
          "p": 0.234375,
          "f": 0.16949152080692026
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.10065v1",
      "true_abstract": "This paper is the first to propose valid inference tools, based on\nself-normalization, in time series expected shortfall regressions. In doing so,\nwe propose a novel two-step estimator for expected shortfall regressions which\nis based on convex optimization in both steps (rendering computation easy) and\nit only requires minimization of quantile losses and squared error losses\n(methods for both of which are implemented in every standard statistical\ncomputing package). As a corollary, we also derive self-normalized inference\ntools in time series quantile regressions. Extant methods, based on a bootstrap\nor direct estimation of the long-run variance, are computationally more\ninvolved, require the choice of tuning parameters and have serious size\ndistortions when the regression errors are strongly serially dependent. In\ncontrast, our inference tools only require estimates of the quantile regression\nparameters that are computed on an expanding window and are correctly sized.\nSimulations show the advantageous finite-sample properties of our methods.\nFinally, two applications to stock return predictability and to Growth-at-Risk\ndemonstrate the practical usefulness of the developed inference tools.",
      "generated_abstract": "examines the impact of financial markets on the growth of the\nfinancial sector in Egypt, focusing on the role of the Cairo Stock Exchange,\nthe largest stock exchange in the Arab world. Using the time-varying\ninteraction model, the results indicate that the financial sector in Egypt\nexperienced a significant decline in its economic growth from 2010 to 2021,\nwith the Cairo Stock Exchange accounting for a significant share of this\ndecline. This decline was due to the impact of the COVID-19 pandemic and the\neconomic recession, which negatively affected the economy and financial\ninstitutions, leading to a significant decline in the economic growth of the\nfinancial sector in Egypt. The results of the study also show that the\nfinancial sector in Egypt experienced a significant decline in its economic\ngrowth from",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.22580645161290322,
          "f": 0.16091953564275346
        },
        "rouge-2": {
          "r": 0.00625,
          "p": 0.010869565217391304,
          "f": 0.007936503300582199
        },
        "rouge-l": {
          "r": 0.11607142857142858,
          "p": 0.20967741935483872,
          "f": 0.14942528276919026
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.08451v1",
      "true_abstract": "Early neural channel coding approaches leveraged dense neural networks with\none-hot encodings to design adaptive encoder-decoder pairs, improving block\nerror rate (BLER) and automating the design process. However, these methods\nstruggled with scalability as the size of message sets and block lengths\nincreased. TurboAE addressed this challenge by focusing on bit-sequence inputs\nrather than symbol-level representations, transforming the scalability issue\nassociated with large message sets into a sequence modeling problem. While\nrecurrent neural networks (RNNs) were a natural fit for sequence processing,\ntheir reliance on sequential computations made them computationally expensive\nand inefficient for long sequences. As a result, TurboAE adopted convolutional\nnetwork blocks, which were faster to train and more scalable, but lacked the\nsequential modeling advantages of RNNs. Recent advances in efficient RNN\narchitectures, such as minGRU and minLSTM, and structured state space models\n(SSMs) like S4 and S6, overcome these limitations by significantly reducing\nmemory and computational overhead. These models enable scalable sequence\nprocessing, making RNNs competitive for long-sequence tasks. In this work, we\nrevisit RNNs for Turbo autoencoders by integrating the lightweight minGRU model\nwith a Mamba block from SSMs into a parallel Turbo autoencoder framework. Our\nresults demonstrate that this hybrid design matches the performance of\nconvolutional network-based Turbo autoencoder approaches for short sequences\nwhile significantly improving scalability and training efficiency for long\nblock lengths. This highlights the potential of efficient RNNs in advancing\nneural channel coding for long-sequence scenarios.",
      "generated_abstract": "ration of Artificial Intelligence (AI) into the wireless network\n(Wi-Fi) offers an unprecedented capability to handle dynamic and\nnon-stationary user traffic. In this context, we introduce the concept of\ndynamic user groups (DUGs), which group user sessions based on their dynamic\nand non-stationary characteristics. The DUGs are constructed by a set of\nrelevant parameters, and each user is assigned to a DUG based on their\nrelevant parameters. The proposed DUGs can be used to enhance the performance\nof the Wi-Fi network by dynamically adjusting the parameters of the DUGs and\ntheir associated user sessions. Furthermore, we propose a distributed and\nenergy-efficient algorithm to construct the DUGs. The proposed algorithm\ndistributes the parameters of the DUGs and the user sessions and uses a\ncommunication-based method to exchange the parameters of the DUG",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10256410256410256,
          "p": 0.22535211267605634,
          "f": 0.14096915869665638
        },
        "rouge-2": {
          "r": 0.013392857142857142,
          "p": 0.02654867256637168,
          "f": 0.017804149845117297
        },
        "rouge-l": {
          "r": 0.08974358974358974,
          "p": 0.19718309859154928,
          "f": 0.12334801332220707
        }
      }
    },
    {
      "paper_id": "math-ph.math/SP/2502.17290v1",
      "true_abstract": "The two-dimensional magnetic Laplacian is considered. We calculate the\nleading term of the splitting between the first two eigenvalues of the operator\nin the semiclassical limit under the assumption that the magnetic field does\nnot vanish and has two symmetric magnetic wells with respect to the coordinate\naxes. This is the first result of quantum tunneling between purely magnetic\nwells under generic assumptions. The proof, which strongly relies on microlocal\nanalysis, reveals a purely magnetic Agmon distance between the wells.\nSurprisingly, it is discovered that the exponential decay of the eigenfunctions\naway from the magnetic wells is not crucial to derive the tunneling formula.\nThe key is a microlocal exponential decay inside the characteristic manifold,\nwith respect to the variable quantizing the classical center guide motion.",
      "generated_abstract": "aper, we consider the problem of constructing a localized solution to\nthe quantum Schr\\\"odinger equation on a torus. The key feature of our approach\nis the introduction of a new functional space, which we call a torus\nSchr\\\"odinger space. We prove that any localized solution of the quantum\nSchr\\\"odinger equation on a torus is a classical solution to the Schr\\\"odinger\nequation on the torus. Our approach is based on a functional calculus for the\nquantum Laplacian on the torus, which we extend to the quantum Schr\\\"odinger\nequation. We use the functional calculus to define a torus Schr\\\"odinger space\nand to prove that any localized solution of the quantum Schr\\\"odinger equation\non a torus is a classical solution to the Schr\\\"odinger equation on the torus.\nThis allows us to construct a localized solution",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21052631578947367,
          "p": 0.3333333333333333,
          "f": 0.2580645113839751
        },
        "rouge-2": {
          "r": 0.044642857142857144,
          "p": 0.06172839506172839,
          "f": 0.05181346663158788
        },
        "rouge-l": {
          "r": 0.17105263157894737,
          "p": 0.2708333333333333,
          "f": 0.20967741460978156
        }
      }
    },
    {
      "paper_id": "eess.SP.cs/SD/2503.09349v1",
      "true_abstract": "Correlation-based auditory attention decoding (AAD) algorithms exploit neural\ntracking mechanisms to determine listener attention among competing speech\nsources via, e.g., electroencephalography signals. The correlation coefficients\nbetween the decoded neural responses and encoded speech stimuli of the\ndifferent speakers then serve as AAD decision variables. A critical trade-off\nexists between the temporal resolution (the decision window length used to\ncompute these correlations) and the AAD accuracy. This trade-off is typically\ncharacterized by evaluating AAD accuracy across multiple window lengths,\nleading to the performance curve. We propose a novel method to model this\ntrade-off curve using labeled correlations from only a single decision window\nlength. Our approach models the (un)attended correlations with a normal\ndistribution after applying the Fisher transformation, enabling accurate AAD\naccuracy prediction across different window lengths. We validate the method on\ntwo distinct AAD implementations: a linear decoder and the non-linear VLAAI\ndeep neural network, evaluated on separate datasets. Results show consistently\nlow modeling errors of approximately 2 percent points, with 94% of true\naccuracies falling within estimated 95%-confidence intervals. The proposed\nmethod enables efficient performance curve modeling without extensive\nmulti-window length evaluation, facilitating practical applications in, e.g.,\nperformance tracking in neuro-steered hearing devices to continuously adapt the\nsystem parameters over time.",
      "generated_abstract": "r presents a novel method for generating synthetic magnetic resonance\nenergy (MRE) data using a non-stationary Gaussian process (GPyTorch) model.\nThis method allows for the generation of synthetic MRE data at various spatial\nand temporal resolutions, making it suitable for various research applications\nin biomedical imaging. By integrating a non-stationary Gaussian process\nmodel into the MRE generation process, we ensure that the generated data are\naccurate and realistic. The GPyTorch model was trained using an MRE dataset\nobtained from a healthy volunteer, and it was found to be able to generate\naccurate synthetic MRE data with a resolution of 128x128x16. The performance of\nthe model was evaluated using two different metrics: the mean absolute error\n(MAE) and the root mean squared error (RMSE). The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1476510067114094,
          "p": 0.275,
          "f": 0.1921397334452052
        },
        "rouge-2": {
          "r": 0.025380710659898477,
          "p": 0.042735042735042736,
          "f": 0.03184712908251925
        },
        "rouge-l": {
          "r": 0.12080536912751678,
          "p": 0.225,
          "f": 0.15720523562861133
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.01376v1",
      "true_abstract": "Structure-Based Drug Design (SBDD) has revolutionized drug discovery by\nenabling the rational design of molecules for specific protein targets. Despite\nsignificant advancements in improving docking scores, advanced 3D-SBDD\ngenerative models still face challenges in producing drug-like candidates that\nmeet medicinal chemistry standards and pharmacokinetic requirements. These\nlimitations arise from their inherent focus on molecular interactions, often\nneglecting critical aspects of drug-likeness. To address these shortcomings, we\nintroduce the Collaborative Intelligence Drug Design (CIDD) framework, which\ncombines the structural precision of 3D-SBDD models with the chemical reasoning\ncapabilities of large language models (LLMs). CIDD begins by generating\nsupporting molecules with 3D-SBDD models and then refines these molecules\nthrough LLM-supported modules to enhance drug-likeness and structural\nreasonability. When evaluated on the CrossDocked2020 dataset, CIDD achieved a\nremarkable success ratio of 37.94%, significantly outperforming the previous\nstate-of-the-art benchmark of 15.72%. Although improving molecular interactions\nand drug-likeness is often seen as a trade-off, CIDD uniquely achieves a\nbalanced improvement in both by leveraging the complementary strengths of\ndifferent models, offering a robust and innovative pathway for designing\ntherapeutically promising drug candidates.",
      "generated_abstract": "f large language models (LLMs) to analyze gene expression data\nprovides valuable insights, but the reliance on these models raises concerns\nabout the potential for misinterpretation. Here, we introduce a novel\nframework for analyzing gene expression data using a neural network that\ncomprises two key components: a gene-level transformer and a global gene\nfeature-level transformer. The gene-level transformer is a variant of the\nTransformer architecture that captures the complex interactions between\ngene-specific features and the gene expression data. The global gene feature\ntransformer utilizes these gene-level features to generate a single feature\nvector that captures the global interactions between genes. We demonstrate the\neffectiveness of this framework by analyzing gene expression data from a\nlung-cancer patient sample and a normal lung-cancer patient sample. We find\nthat the gene-level transformer outperforms the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.25675675675675674,
          "f": 0.1835748746332471
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.03508771929824561,
          "f": 0.027586202125090005
        },
        "rouge-l": {
          "r": 0.12781954887218044,
          "p": 0.22972972972972974,
          "f": 0.16425120313566255
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2501.13143v1",
      "true_abstract": "We introduce a model-free preference under ambiguity, as a primitive trait of\nbehavior, which we apply once as well as repeatedly. Its single and double\napplication yield simple, easily interpretable definitions of ambiguity\naversion and ambiguity prudence. We derive their implications within canonical\nmodels for decision under risk and ambiguity. We establish in particular that\nour new definition of ambiguity prudence is equivalent to a positive third\nderivative of: (i) the capacity in the Choquet expected utility model, (ii) the\ndual conjugate of the divergence function under variational divergence\npreferences and (iii) the ambiguity attitude function in the smooth ambiguity\nmodel. We show that our definition of ambiguity prudent behavior may be\nnaturally linked to an optimal insurance problem under ambiguity.",
      "generated_abstract": "We investigate the performance of a generalised additive maximum likelihood\n(GAML) estimator for the quantile regression model when the dependent variable\nhas a non-linear link function. We show that the proposed estimator is\nconsistent and asymptotically normal, and establish the convergence rate of its\nvariance when the link function is non-linear. Furthermore, we derive a\nconsistent estimator for the quantile of the dependent variable, which is\nconsistent when the link function is linear. The proposed estimator is\nillustrated through numerical experiments and the results are consistent with\nthe theoretical analysis.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18072289156626506,
          "p": 0.28846153846153844,
          "f": 0.22222221748587115
        },
        "rouge-2": {
          "r": 0.02654867256637168,
          "p": 0.039473684210526314,
          "f": 0.03174602693765645
        },
        "rouge-l": {
          "r": 0.14457831325301204,
          "p": 0.23076923076923078,
          "f": 0.17777777304142675
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.01296v1",
      "true_abstract": "The separating Noether number $\\beta_{\\mathrm{sep}}(G)$ of a finite group $G$\nis the minimal positive integer $d$ such that for every $G$-module $V$ there is\na separating set of degree $\\leq d$. In this manuscript, we investigate the\nseparating Noether number $\\beta_{\\mathrm{sep}}(G)$. Among others, we obtain\nthe exact value of $\\beta_{\\mathrm{sep}}(G)$ for finite abelian groups $G$,\nwhen either $G$ is a $p$-group or $\\mathsf r(G)\\in \\{3,5\\}$.",
      "generated_abstract": "We consider the problem of computing the order of the multiplicative group\n${\\mathbb{Z}}[\\sqrt{5}",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06382978723404255,
          "p": 0.3,
          "f": 0.105263155001539
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0425531914893617,
          "p": 0.2,
          "f": 0.07017543570329343
        }
      }
    },
    {
      "paper_id": "physics.chem-ph.physics/chem-ph/2503.10038v1",
      "true_abstract": "We synthesize MoS$_{2}$ atomic layer flakes at different growth conditions to\ntailor S-terminated and Mo-terminated edge defect states that are investigated\nfor their ferromagnetic response. We leverage quantum weak measurement\nprinciples to construct a spin Hall effect of light-based magneto-optic Kerr\neffect (SHEL-MOKE) setup to sense the ultra-small magnetic response from the\nsynthesized atomic layers. Our findings demonstrate that Mo-terminated edge\nstates are the primary source of ferromagnetic response from MoS$_{2}$ flakes,\nwhich is consistent with X-ray photoelectron, Raman and photoluminescence\nspectroscopic results. In the process, we demonstrate SHEL-MOKE to be a robust\ntechnique to investigate ultra weak properties in novel atomic-scale materials.",
      "generated_abstract": "A method is presented for estimating the concentration of a solute in a\nsolution using the relationship between the concentration and the molar heat\ncapacity of the solute. This is based on the assumption that the concentration\nof a solute in a solution can be calculated by integrating the molar heat\ncapacity as a function of temperature over the entire solution, which is the\ncase when the solute is a liquid. The method is validated by comparison with\nexperimental data. The method is applied to the concentration of the solute in\na solution of a mixture of salts, where the solute concentration is of\ninterest.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.23076923076923078,
          "f": 0.18461537981538473
        },
        "rouge-2": {
          "r": 0.010101010101010102,
          "p": 0.0125,
          "f": 0.011173179413878161
        },
        "rouge-l": {
          "r": 0.14102564102564102,
          "p": 0.21153846153846154,
          "f": 0.16923076443076937
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2502.14228v1",
      "true_abstract": "The umbilical cord plays a critical role in delivering nutrients and oxygen\nfrom the placenta to the fetus through the umbilical vein, while the two\numbilical arteries carry deoxygenated blood with waste products back to the\nplacenta. Although solute exchange in the placenta has been extensively\nstudied, exchange within the cord tissue has not been investigated. Here, we\nexplore the hypothesis that the coiled structure of the umbilical cord could\nstrengthen diffusive coupling between the arteries and the vein, resulting in a\nfunctional shunt. We calculate the diffusion of solutes, such as oxygen, and\nheat in the umbilical cord to quantify how this shunt is affected by vascular\nconfiguration within the cord. We demonstrate that the shunt is enhanced by\ncoiling and vessel proximity. Furthermore, our model predicts that typical\nvascular configurations of the human cord tend to minimise shunting, which\ncould otherwise disrupt thermal regulation of the fetus. We also show that the\nexchange, amplified by coiling, can provide additional oxygen supply to the\ncord tissue surrounding the umbilical vessels.",
      "generated_abstract": "t of large language models (LLMs) has enabled the development of\nhigh-performance speech translation models that can translate between\nsignificantly different languages. While these models have demonstrated\nsignificant advancements, there is still a gap in their ability to translate\nbetween human languages, such as English and Chinese. In this study, we propose\na novel LLM-based model that leverages both the source and target languages\ninto a unified framework to achieve both translational and compositional\ncapabilities. This model is named VISION-LSTM-Transformer (VIST), which\nintegrates visual and language modality, enabling it to effectively handle both\nvisual and linguistic information, which is crucial for effective\ntranslation. Our experiments demonstrate that VIST significantly outperforms\nstate-of-the-art models across various translation tasks, demonstrating its\npotential for advancing language model-based translation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18446601941747573,
          "p": 0.20652173913043478,
          "f": 0.19487178988770557
        },
        "rouge-2": {
          "r": 0.013333333333333334,
          "p": 0.01652892561983471,
          "f": 0.014760142658734514
        },
        "rouge-l": {
          "r": 0.1650485436893204,
          "p": 0.18478260869565216,
          "f": 0.1743589693748851
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2405.20522v1",
      "true_abstract": "This project is a collaboration between industry and academia to delve into\nFinance Social Networks, specifically the Board of Directors of public\ncompanies. Knowing the connections between Directors and Executives in\ndifferent companies can generate powerful stories and meaningful insights on\ninvestments. A proof of concept in the form of a Data Visualization tool\nreveals its strength in investigating corporate governance and sustainability,\nas well as in the partnership between industry and academic institutions.",
      "generated_abstract": "This paper studies the optimal taxation of a family of firms that face\ncompetitive and non-competitive taxes. In the former case, the firm faces a\nnegative tax rate and a positive tax rate. In the latter case, the firm faces a\npositive tax rate and a negative tax rate. The family of firms is heterogeneous\nin terms of the tax rates faced by its members. We show that there is no\nsatisfactory family of firms whose tax rate is equal to the optimal tax rate.\nHowever, in the presence of a positive tax rate, the optimal family of firms is\nthe family of firms with a negative tax rate.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.1875,
          "f": 0.17647058325259532
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.02666666666666667,
          "f": 0.02758620190249793
        },
        "rouge-l": {
          "r": 0.14814814814814814,
          "p": 0.16666666666666666,
          "f": 0.15686274011534043
        }
      }
    },
    {
      "paper_id": "cs.CL.econ/GN/2412.20438v1",
      "true_abstract": "The financial sector, a pivotal force in economic development, increasingly\nuses the intelligent technologies such as natural language processing to\nenhance data processing and insight extraction. This research paper through a\nreview process of the time span of 2018-2023 explores the use of text mining as\nnatural language processing techniques in various components of the financial\nsystem including asset pricing, corporate finance, derivatives, risk\nmanagement, and public finance and highlights the need to address the specific\nproblems in the discussion section. We notice that most of the research\nmaterials combined probabilistic with vector-space models, and text-data with\nnumerical ones. The most used technique regarding information processing is the\ninformation classification technique and the most used algorithms include the\nlong-short term memory and bidirectional encoder models. The research noticed\nthat new specific algorithms are developed and the focus of the financial\nsystem is mainly on asset pricing component. The research also proposes a path\nfrom engineering perspective for researchers who need to analyze financial\ntext. The challenges regarding text mining perspective such as data quality,\ncontext-adaption and model interpretability need to be solved so to integrate\nadvanced natural language processing models and techniques in enhancing\nfinancial analysis and prediction. Keywords: Financial System (FS), Natural\nLanguage Processing (NLP), Software and Text Engineering, Probabilistic,\nVector-Space, Models, Techniques, TextData, Financial Analysis.",
      "generated_abstract": "r studies the design of large language models (LLMs) for\ncognitive science research, particularly focusing on the challenges of\nunderstanding the social implications of LLMs and developing methodologies for\nresearchers to assess their impact. We introduce a framework for assessing\nLLMs' social impact, including their ability to foster inclusive, equitable,\nand sustainable communities. We develop a theoretical framework for assessing\nsocial impact and propose a methodology for evaluating the impact of LLMs in\nresearch. Through a series of case studies, we demonstrate how the framework\ncan be applied to assess the social impact of LLMs, including evaluating\npotential harms and identifying opportunities for positive impact. The\nframework and methodology presented in this paper provide a framework for\nevaluating the social impact of LLMs, and offer insights into how researchers\ncan use LLM",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13970588235294118,
          "p": 0.25,
          "f": 0.17924527841936644
        },
        "rouge-2": {
          "r": 0.005050505050505051,
          "p": 0.008620689655172414,
          "f": 0.006369422092583228
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.2236842105263158,
          "f": 0.16037735389106456
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.02349v1",
      "true_abstract": "Sequential change-point detection for time series is widely used in data\nmonitoring in practice. In this work, we focus on sequential change-point\ndetection on high-order compositional time series models. Under the regularity\nconditions, we prove that a process following the generalized Beta AR(p) model\nwith exogenous variables is stationary and ergodic. We develop a nonparametric\nsequential change-point detection method for the generalized Beta AR(p) model,\nwhich does not rely on any strong assumptions about the sources of the change\npoints. We show that the power of the test converges to one given that the\namount of initial observations is large enough. We apply the nonparametric\nmethod to a rate of automobile crashes with alcohol involved, which is recorded\nmonthly from January 2010 to December 2020; the exogenous variable is the price\nlevel of alcoholic beverages, which has a change point around August 2019. We\nfit a generalized Beta AR(p) model to the crash rate sequence, and we use the\nnonparametric sequential change-point detection method to successfully detect\nthe change point.",
      "generated_abstract": "tudy, we propose a novel approach for the estimation of\nparameter and density functions of a general functional distribution that\nincludes functions of arbitrary order. The proposed method relies on the\ncomputation of an expression for the expectation of the square-root of the\nlog-likelihood of the functional distribution. As a result, the proposed method\ncan be used for the estimation of a functional distribution with any\narbitrary functional shape, such as the Beta distribution, the Gamma\ndistribution, and the Weibull distribution. In addition, the proposed method\nrequires no parameter tuning and can be used for high-dimensional datasets.\nSimulation studies were conducted to evaluate the performance of the proposed\nmethod and the existing methods. The simulation studies were conducted on 500\nsimulated datasets and 100 real datasets, and the results indicated that the\nproposed method can provide more accurate estimation than existing methods,\nparticularly",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15,
          "p": 0.19736842105263158,
          "f": 0.17045454054752082
        },
        "rouge-2": {
          "r": 0.020134228187919462,
          "p": 0.02586206896551724,
          "f": 0.022641504511499825
        },
        "rouge-l": {
          "r": 0.15,
          "p": 0.19736842105263158,
          "f": 0.17045454054752082
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/QM/2503.02685v1",
      "true_abstract": "Precise parcellation of functional networks (FNs) of early developing human\nbrain is the fundamental basis for identifying biomarker of developmental\ndisorders and understanding functional development. Resting-state fMRI\n(rs-fMRI) enables in vivo exploration of functional changes, but adult FN\nparcellations cannot be directly applied to the neonates due to incomplete\nnetwork maturation. No standardized neonatal functional atlas is currently\navailable. To solve this fundamental issue, we propose TReND, a novel and fully\nautomated self-supervised transformer-autoencoder framework that integrates\nregularized nonnegative matrix factorization (RNMF) to unveil the FNs in\nneonates. TReND effectively disentangles spatiotemporal features in voxel-wise\nrs-fMRI data. The framework integrates confidence-adaptive masks into\ntransformer self-attention layers to mitigate noise influence. A self\nsupervised decoder acts as a regulator to refine the encoder's latent\nembeddings, which serve as reliable temporal features. For spatial coherence,\nwe incorporate brain surface-based geodesic distances as spatial encodings\nalong with functional connectivity from temporal features. The TReND clustering\napproach processes these features under sparsity and smoothness constraints,\nproducing robust and biologically plausible parcellations. We extensively\nvalidated our TReND framework on three different rs-fMRI datasets: simulated,\ndHCP and HCP-YA against comparable traditional feature extraction and\nclustering techniques. Our results demonstrated the superiority of the TReND\nframework in the delineation of neonate FNs with significantly better spatial\ncontiguity and functional homogeneity. Collectively, we established TReND, a\nnovel and robust framework, for neonatal FN delineation. TReND-derived neonatal\nFNs could serve as a neonatal functional atlas for perinatal populations in\nhealth and disease.",
      "generated_abstract": "We investigate the dynamics of the Gillespie algorithm in a two-species\nmodel with a non-monotonic competition parameter. We show that the dynamics\nchange significantly if the competition parameter is not monotonic. For\nmonotonic cases, we find that the Gillespie algorithm can be simulated with\nmuch higher accuracy than standard Monte Carlo simulations, but the accuracy\ndegrades sharply for non-monotonic cases. We also consider the problem of\npredicting the dynamics of the Gillespie algorithm in a non-monotonic model,\nand show that the Gillespie algorithm can be used to predict the dynamics of\nthe standard Monte Carlo simulation for non-monotonic cases, but the accuracy\ndegrades sharply for monotonic cases. We also show that the Gillespie algorithm\nis not suitable for predicting the dynamics of the standard Monte Carlo\nsimulation for non-monotonic cases.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0963855421686747,
          "p": 0.2962962962962963,
          "f": 0.14545454175041334
        },
        "rouge-2": {
          "r": 0.004310344827586207,
          "p": 0.012345679012345678,
          "f": 0.006389772521514209
        },
        "rouge-l": {
          "r": 0.09036144578313253,
          "p": 0.2777777777777778,
          "f": 0.1363636326595042
        }
      }
    },
    {
      "paper_id": "math.ST.econ/EM/2502.20917v1",
      "true_abstract": "We examine the location characteristics of a conditional selective confidence\ninterval based on the polyhedral method. This interval is constructed from the\ndistribution of a test statistic conditional upon the event of statistical\nsignificance. In the case of a one-sided test, the behavior of the interval\nvaries depending on whether the parameter is highly significant or only\nmarginally significant. When the parameter is highly significant, the interval\nis similar to the usual confidence interval derived without considering\nselection. However, when the parameter is only marginally significant, the\ninterval falls into an extreme range and deviates greatly from the estimated\nvalue of the parameter. In contrast, an interval conditional on two-sided\nsignificance does not yield extreme results, although it may exclude the\nestimated parameter value.",
      "generated_abstract": "We consider a linear regression model with a linear instrumental variable and\ndifferential privacy. We study the privacy-utility tradeoff for differential\nprivacy and provide a new bound for the difference between the differentially\nprivate estimator and the true value of the true causal effect. We show that if\nthe privacy budget is sufficiently large, then the differentially private\nestimator satisfies the local differential privacy constraint.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09859154929577464,
          "p": 0.17073170731707318,
          "f": 0.12499999535873742
        },
        "rouge-2": {
          "r": 0.018867924528301886,
          "p": 0.03508771929824561,
          "f": 0.024539872752456712
        },
        "rouge-l": {
          "r": 0.09859154929577464,
          "p": 0.17073170731707318,
          "f": 0.12499999535873742
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/PM/2412.11019v1",
      "true_abstract": "The domain of hedge fund investments is undergoing significant\ntransformation, influenced by the rapid expansion of data availability and the\nadvancement of analytical technologies. This study explores the enhancement of\nhedge fund investment performance through the integration of machine learning\ntechniques, the application of PolyModel feature selection, and the analysis of\nfund size. We address three critical questions: (1) the effect of machine\nlearning on trading performance, (2) the role of PolyModel feature selection in\nfund selection and performance, and (3) the comparative reliability of larger\nversus smaller funds.\n  Our findings offer compelling insights. We observe that while machine\nlearning techniques enhance cumulative returns, they also increase annual\nvolatility, indicating variability in performance. PolyModel feature selection\nproves to be a robust strategy, with approaches that utilize a comprehensive\nset of features for fund selection outperforming more selective methodologies.\nNotably, Long-Term Stability (LTS) effectively manages portfolio volatility\nwhile delivering favorable returns. Contrary to popular belief, our results\nsuggest that larger funds do not consistently yield better investment outcomes,\nchallenging the assumption of their inherent reliability.\n  This research highlights the transformative impact of data-driven approaches\nin the hedge fund investment arena and provides valuable implications for\ninvestors and asset managers. By leveraging machine learning and PolyModel\nfeature selection, investors can enhance portfolio optimization and reassess\nthe dependability of larger funds, leading to more informed investment\nstrategies.",
      "generated_abstract": "e a novel approach to the problem of forecasting volatility,\ninvolving the joint use of a time series model and a Gaussian process (GP)\nrepresentation of volatility. This approach is based on the use of GPs in the\nrepresentation of volatility and on the use of a time series model in the\nforecasting process. We first present the classical method of using the\nconditional expectation of the squared logarithm of volatility as a surrogate\nfor the conditional expectation of the squared logarithm of return. We then\npropose a more efficient method based on a GP representation of the\nconditional expectation of the squared logarithm of return. Finally, we\nintroduce a novel method for forecasting the conditional expectation of the\nsquared logarithm of return that combines the GP representation of the\nconditional expectation of the squared logarithm of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10416666666666667,
          "p": 0.28846153846153844,
          "f": 0.15306122059142033
        },
        "rouge-2": {
          "r": 0.0048543689320388345,
          "p": 0.011494252873563218,
          "f": 0.006825934391317537
        },
        "rouge-l": {
          "r": 0.09722222222222222,
          "p": 0.2692307692307692,
          "f": 0.1428571389587673
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.07327v1",
      "true_abstract": "Multilinear Principal Component Analysis (MPCA) is an important tool for\nanalyzing tensor data. It performs dimension reduction similar to PCA for\nmultivariate data. However, standard MPCA is sensitive to outliers. It is\nhighly influenced by observations deviating from the bulk of the data, called\ncasewise outliers, as well as by individual outlying cells in the tensors,\nso-called cellwise outliers. This latter type of outlier is highly likely to\noccur in tensor data, as tensors typically consist of many cells. This paper\nintroduces a novel robust MPCA method that can handle both types of outliers\nsimultaneously, and can cope with missing values as well. This method uses a\nsingle loss function to reduce the influence of both casewise and cellwise\noutliers. The solution that minimizes this loss function is computed using an\niteratively reweighted least squares algorithm with a robust initialization.\nGraphical diagnostic tools are also proposed to identify the different types of\noutliers that have been found by the new robust MPCA method. The performance of\nthe method and associated graphical displays is assessed through simulations\nand illustrated on two real datasets.",
      "generated_abstract": "This paper introduces a novel method for the analysis of multivariate\ndata that is based on the concept of data-driven decision trees. Our method\ncombines a novel data-driven decision tree with a multivariate Gaussian\nprocess. We demonstrate the effectiveness of the proposed method through\nsimulations, and we illustrate its application to a real-world dataset. The\nproposed method offers several advantages over existing methods in terms of\ncomputational efficiency, data-driven decision tree construction, and\nflexibility in data modeling. Additionally, it offers the potential for\nimproved modeling accuracy and robustness compared to existing methods. The\nproposed method is particularly useful for analyzing high-dimensional\nmultivariate data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18803418803418803,
          "p": 0.3384615384615385,
          "f": 0.2417582371664051
        },
        "rouge-2": {
          "r": 0.046511627906976744,
          "p": 0.08602150537634409,
          "f": 0.06037735393492381
        },
        "rouge-l": {
          "r": 0.18803418803418803,
          "p": 0.3384615384615385,
          "f": 0.2417582371664051
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/SC/2307.10289v1",
      "true_abstract": "This is the second part of the previous review. In the previous review we\nsuspected that Orai3 channels were involved in lung cancer and more precisely\nin several cancers. Here we confirm that calcium dysregulation is important for\ncancer development. in this paper we show that Orai3 is an upstream activator\nof AKT and we prove that AKT is involved in chemoresistance in NSCLC.",
      "generated_abstract": "of protein-protein interactions (PPIs) is a key tool for\nstudying cellular processes, from drug design to protein folding and\ndisease diagnosis. Traditionally, PPIs have been modeled as a network of\ninteraction partners, often with a single protein central to the network.\nHowever, recent advancements in machine learning have enabled the development of\nmodels that capture the complex relationships within PPIs. These models\ngenerally incorporate two key components: a graph structure model that\ncaptures the underlying relationships between proteins and a feature model that\ncaptures the biological meaning of these relationships. In this paper, we\nintroduce a novel approach to PPI modeling that combines these two components\ninto a single model. Our model, referred to as PPI-GRM, utilizes a graph\nstructure model to capture the underlying relationships between proteins and a\nfeature model to capture",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2682926829268293,
          "p": 0.13414634146341464,
          "f": 0.17886178417344187
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.2682926829268293,
          "p": 0.13414634146341464,
          "f": 0.17886178417344187
        }
      }
    },
    {
      "paper_id": "physics.ed-ph.physics/ed-ph/2502.15781v1",
      "true_abstract": "Digital innovation in education has revolutionized teaching and learning\nprocesses, demanding a rethink of pedagogical competence among educators. This\nstudy evaluates the preparation of instructors to use digital technologies into\ntheir educational practices. The study used a mixed-methods approach,\nintegrating both qualitative interviews and quantitative surveys to evaluate\nteachers' institutional support systems, beliefs, and technical proficiency.\nThe results show that even while a large number of educators acknowledge the\nbenefits of digital tools, problems including poor professional development and\nchange aversion still exist. In order to improve digital pedagogical\npreparation, the study emphasizes the necessity of focused training initiatives\nand encouraging institutional regulations. There is discussion on the\nimplications for educational institutions and policymakers.",
      "generated_abstract": "aper, we introduce a novel way to teach the concept of thermal\nenergy through a series of experiments. We first introduce the idea of\nthermal energy by defining it as the change in kinetic energy of particles as a\nresult of their interaction with a potential. The kinetic energy is then\nrepresented as the energy of the particles at rest, and we then show that the\npotential energy is the energy of the particles in motion. The concept is then\napplied to a simple experiment in which two particles are moved by an external\nforce. We then show how the work done by the force can be expressed as the\ndifference in kinetic energy between the initial and final kinetic energy. We\nthen move the experiment to a more advanced level and show how the concept of\nthermal energy can be extended to a system with more than two particles. This\ncan be done by first introducing the concept of internal energy, which is\ndefined",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14772727272727273,
          "p": 0.17567567567567569,
          "f": 0.16049382219783587
        },
        "rouge-2": {
          "r": 0.008849557522123894,
          "p": 0.007518796992481203,
          "f": 0.008130076333865153
        },
        "rouge-l": {
          "r": 0.14772727272727273,
          "p": 0.17567567567567569,
          "f": 0.16049382219783587
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.astro-ph/CO/2503.09769v1",
      "true_abstract": "The Standard Model of cosmology, $\\Lambda$CDM, while enormously successful,\nis currently unable to account for several cosmological anomalies the most\nprominent of which are in the measurements of the Hubble parameter and $S_8$.\nAdditionally, the inclusion of the cosmological constant is theoretically\nunappealing. This has lead to extensions of the model such as the use of fluid\nequations for interacting dark matter and dark energy which, however, are ad\nhoc since they do not appear to arise from a Lagrangian. Recently, we have\nproposed $\\mathcal{Q}_{\\rm CDM}$ as an alternative to $\\Lambda$CDM which is a\ndynamical model of a quintessence field interacting with dark matter within a\nfield theoretic approach. In this approach, we analyze the effect of the dark\nmatter mass, the dark matter-dark energy interaction strength and the dark\nmatter self-interaction on the cosmological parameters. Further, within\n$\\mathcal{Q}_{\\rm CDM}$ we investigate the possible alleviation of the Hubble\ntension and the $S_8$ anomaly and the nature of dark energy.",
      "generated_abstract": "Synoptic Survey Telescope (LSST) is a 40-m telescope designed to\ncapture the first light of over 300 million galaxies and quasars over its\nlifespan. LSST is uniquely positioned to detect and characterize the first\ngalaxies and quasars that have not been imaged before, enabling a unique\nexploration of the formation and evolution of the early Universe. The LSST\ncatalogue will be a comprehensive catalogue of over 100 million galaxies and\nquasars, representing a substantial increase in the volume of astronomical\ndata. We present a catalogue of galaxies and quasars at the redshift $z\\sim\n0.7$ that have been imaged by the Dark Energy Camera Legacy Survey (DECaLS),\nthe Large Apache Point Observatory Galaxy Survey (LAPO) and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10416666666666667,
          "p": 0.13513513513513514,
          "f": 0.11764705390726665
        },
        "rouge-2": {
          "r": 0.02097902097902098,
          "p": 0.02912621359223301,
          "f": 0.024390239034636444
        },
        "rouge-l": {
          "r": 0.10416666666666667,
          "p": 0.13513513513513514,
          "f": 0.11764705390726665
        }
      }
    },
    {
      "paper_id": "nlin.AO.nlin/AO/2503.07585v1",
      "true_abstract": "In this paper we explore the effects of instantaneous stochastic resetting on\na planar slow-fast dynamical system of the form $\\dot{x}=f(x)-y$ and\n$\\dot{y}=\\epsilon (x-y)$ with $0<\\epsilon \\ll 1$. We assume that only the fast\nvariable $x(t)$ resets to its initial state $x_0$ at a random sequence of times\ngenerated from a Poisson process of rate $r$. Fixing the slow variable, we\ndetermine the parameterized probability density $p(x,t|y)$, which is the\nsolution to a modified Liouville equation. We then show how for $r\\gg \\epsilon$\nthe slow dynamics can be approximated by the averaged equation\n$dy/d\\tau=\\E[x|y]-y$ where $\\tau=\\epsilon t$, $\\E[x|y]=\\int x p^*(x|y)dx$ and\n$p^*(x|y)=\\lim_{t\\rightarrow \\infty}p(x,t|y)$. We illustrate the theory for\n$f(x)$ given by the cubic function of the FitzHugh-Nagumo equation. We find\nthat the slow variable typically converges to an $r$-dependent fixed point\n$y^*$ that is a solution of the equation $y^*=\\E[x|y^*]$. Finally, we\nnumerically explore deviations from averaging theory when $r=O(\\epsilon)$.",
      "generated_abstract": "We present a new analytic approximation for the time evolution of the\nnonlinear wave-particle interactions. Our formalism is based on the\nconventional nonlinear Schrodinger equation and we extend it to a nonlinear\nHamiltonian system with linearized dynamics. This approach is particularly\nuseful for studying the nonlinear interaction of wave and particle in\nnon-equilibrium situations. We apply our formalism to the motion of\nparticles in a non-equilibrium quantum gas in an external potential. In this\ncase, the linear dynamics of the particles is a good approximation for the\nnonlinear dynamics. Our results show that the dynamics of the particles can be\ndescribed by a time-dependent version of the classical hydrodynamic equations\nfor a nonlinear Schrodinger equation. The nonlinear interaction of the\nparticles with the quantum gas can be studied by means of the nonlinear\nHamiltonian system.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20754716981132076,
          "p": 0.3055555555555556,
          "f": 0.24719100641838163
        },
        "rouge-2": {
          "r": 0.04195804195804196,
          "p": 0.05357142857142857,
          "f": 0.047058818603306934
        },
        "rouge-l": {
          "r": 0.18867924528301888,
          "p": 0.2777777777777778,
          "f": 0.22471909630602208
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/GL/2211.09554v1",
      "true_abstract": "It is essential to discuss the role, difficulties, and opportunities\nconcerning people of different gender in the field of software engineering\nresearch, education, and industry. Although some literature reviews address\nsoftware engineering and gender, it is still unclear how research and practices\nin Asia exist for handling gender aspects in software development and\nengineering. We conducted a systematic literature review to grasp the\ncomprehensive view of gender research and practices in Asia. We analyzed the 32\nidentified papers concerning countries and publication years among 463\npublications. Researchers and practitioners from various organizations actively\nwork on gender research and practices in some countries, including China,\nIndia, and Turkey. We identified topics and classified them into seven\ncategories varying from personal mental health and team building to\norganization. Future research directions include investigating the synergy\nbetween (regional) gender aspects and cultural concerns and considering\npossible contributions and dependency among different topics to have a solid\nfoundation for accelerating further research and getting actionable practices.",
      "generated_abstract": "er the problem of learning the underlying graph structure from\nmonotone functions of the input. This is a classical problem in theoretical\ncomputer science, which is believed to be undecidable in general. However, it\nhas a number of applications, including graph isomorphism, graph clustering, and\ngraph matching. In this work, we consider the problem of learning the underlying\ngraph structure from monotone functions of the input, with a specific focus on\ngraph isomorphism. We introduce a novel graph isomorphism learning algorithm\ncalled ISIS, which achieves a $1/2$-approximation to the optimal $\\ell_1$\nregularized isomerism score. Our algorithm is based on an iterative process\nwherein we repeatedly optimize the regularized isomerism score while\nincrementally building a graph isomorphism learning model. We prove that the\nalgorithm runs in polynomial time and produces an $\\ell_1$ regularized\ngraph isomorphism model that matches the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.16455696202531644,
          "f": 0.14207649782555481
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.16455696202531644,
          "f": 0.14207649782555481
        }
      }
    },
    {
      "paper_id": "physics.hist-ph.physics/hist-ph/2502.18231v1",
      "true_abstract": "1. Strong and weak notions of erasure are distinguished according to whether\nthe single erasure procedure does or does not leave the environment in the same\nstate independently of the pre-erasure state.\n  2. Purely thermodynamic considerations show that strong erasure cannot be\ndissipationless.\n  3. The main source of entropy creation in erasure processes at molecular\nscales is the entropy that must be created to suppress thermal fluctuations\n(\"noise\").\n  4. A phase space analysis recovers no minimum entropy cost for weak erasure\nand a positive minimum entropy cost for strong erasure.\n  5. An information entropy term has been attributed mistakenly to pre-erasure\nstates in the Gibbs formalism through the neglect of an additive constant in\nthe \"-k sum p log p\" Gibbs entropy formula.",
      "generated_abstract": "r provides a general framework for studying the evolution of the\nquantum theory of gravity. The framework is based on a quantum theory of\ninteraction between two particles, in which the interaction is mediated by a\nlocal Hamiltonian, and the two particles are described by quantum mechanics.\nThe Hamiltonian of the system is written as a sum of a classical Hamiltonian\nfor the classical dynamics of the two particles, and a quantum Hamiltonian for\nthe quantum dynamics of the interaction. The quantum Hamiltonian is derived\nfrom the classical Hamiltonian, by considering the evolution of the two\nparticles under the interaction, in the classical limit of vanishing interaction\nstrength. We then show that the quantum Hamiltonian of the system can be\nrepresented as a sum of terms that are either the standard quantization of the\nclassical Hamiltonian, or the standard quantization of the interaction\nHamiltonian. We demonstrate that the standard quantization of the classical\nHamiltonian is equivalent to the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1744186046511628,
          "p": 0.24193548387096775,
          "f": 0.20270269783418568
        },
        "rouge-2": {
          "r": 0.034482758620689655,
          "p": 0.03571428571428571,
          "f": 0.035087714299785265
        },
        "rouge-l": {
          "r": 0.16279069767441862,
          "p": 0.22580645161290322,
          "f": 0.18918918432067217
        }
      }
    },
    {
      "paper_id": "physics.ed-ph.physics/ed-ph/2503.00336v1",
      "true_abstract": "Continuing professional development for teachers in the physical sciences is\ncrucial to maintaining high-quality instruction, especially when addressing\nmodern physics. Nevertheless, the teaching of these topics often relies on\ntheoretical models that may seem abstract and removed from practical\napplications. In this context, research in astrophysics provides many valuable\ninsights into the nature of light and its fundamental properties, such as\ncontinuous and discrete spectra, blackbody radiation, and atomic orbitals. This\npaper, aimed at both high school and university-level physics teachers,\nexamines the peculiarities of the emission and absorption spectra of various\ntypes of astronomical objects and demonstrates how spectroscopy is applied in\nastrophysics research. From this perspective, the study conceptually\nillustrates how astrophysicists, by measuring light spectra, determine the\ncomposition, physical properties, origin, and evolution of celestial bodies\nand, by extension, of the universe as a whole. By understanding not only the\ntheory but also the direct applications of astronomical spectroscopy, teachers\nwill be better prepared to guide their students, thereby showcasing the true\nvalue of modern physics in the real world.",
      "generated_abstract": "cle introduces the concept of a 'coding-free' or 'zero-text'\ntextbook, a new approach to teaching physics that does not use textbooks or\nassessments, but rather relies on students to engage in the process of\nunderstanding the physical world through the use of code. By removing the\ntextbook as an essential part of the learning process, students are freed to\nexplore physics through their own creative means, and are able to work\nunconstrained by traditional methods of assessment. This approach has the\npotential to create a more interactive and engaging learning environment,\nleveraging students' own creativity and interests in a way that traditional\nmethods of teaching cannot. In this article, we will explore the concept of\ncoding-free textbooks and how they can be implemented in the classroom. We will\nalso discuss the benefits of this approach, as well as some of the challeng",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21428571428571427,
          "p": 0.29347826086956524,
          "f": 0.2477064171399715
        },
        "rouge-2": {
          "r": 0.029940119760479042,
          "p": 0.037037037037037035,
          "f": 0.03311257783759558
        },
        "rouge-l": {
          "r": 0.15873015873015872,
          "p": 0.21739130434782608,
          "f": 0.183486233653733
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.07126v1",
      "true_abstract": "We propose to relax traditional axioms in decision theory by incorporating a\nmeasurement, or degree, of satisfaction. For example, if the independence axiom\nof expected utility theory is violated, we can measure the size of the\nviolation. This measure allows us to derive an approximation guarantee for a\nutility representation that aligns with the unmodified version of the axiom.\nAlmost satisfying the axiom implies, then, a utility that is near a utility\nrepresentation. We develop specific examples drawn from expected utility theory\nunder risk and uncertainty.",
      "generated_abstract": "This paper considers the problem of distributing a resource among a\ndistributionally\nLIKELY to be the case, it is often assumed that the resource has a\ndistributionally likely value. This paper extends this concept to\ndistributionally likely outcomes, such as those resulting from a fair lottery,\nwhere the outcome is more likely to be a specific outcome than not. It\nintroduces a notion of distributionally likely outcomes and a matching\nalgorithm to allocate resources among these outcomes. The matching algorithm\nis designed to ensure that the total expected payoff of the agents is equal\nto the expected payoff of the resource.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18333333333333332,
          "p": 0.1896551724137931,
          "f": 0.18644067296753822
        },
        "rouge-2": {
          "r": 0.01282051282051282,
          "p": 0.011235955056179775,
          "f": 0.011976042925886827
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.1724137931034483,
          "f": 0.16949152042516535
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.07843v1",
      "true_abstract": "We show that compact locally symmetric Lorentz manifolds are geodesically\ncomplete.",
      "generated_abstract": "We show that the $k$-th cohomology of the enveloping algebra $U(\\widehat{GL_n})$\nis isomorphic to the $k$-th cohomology of the universal enveloping algebra\n$U(\\widehat{GL(n,k)})$. We also show that the $k$-th cohomology of the enveloping\nalgebra $U(\\widehat{SL_n})$ is isomorphic to the $k$-th cohomology of the\nuniversal enveloping algebra $U(\\widehat{SL(n,k)})$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2727272727272727,
          "p": 0.16666666666666666,
          "f": 0.2068965470154579
        },
        "rouge-2": {
          "r": 0.2,
          "p": 0.08695652173913043,
          "f": 0.12121211698806258
        },
        "rouge-l": {
          "r": 0.2727272727272727,
          "p": 0.16666666666666666,
          "f": 0.2068965470154579
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.q-bio/MN/2501.00983v1",
      "true_abstract": "We study Hopfield networks with non-reciprocal coupling inducing switches\nbetween memory patterns. Dynamical phase transitions occur between phases of no\nmemory retrieval, retrieval of multiple point-attractors, and limit-cycle\nattractors. The limit cycle phase is bounded by two critical regions: a Hopf\nbifurcation line and a fold bifurcation line, each with unique dynamical\ncritical exponents and sensitivity to perturbations. A Master Equation approach\nnumerically verifies the critical behavior predicted analytically. We discuss\nhow these networks could model biological processes near a critical threshold\nof cyclic instability evolving through multi-step transitions.",
      "generated_abstract": "t an artificial neural network (ANN) model that uses an\nmodified version of the classical Widrow-Sklar theorem to represent the\ndynamics of a population of neurons. Our model incorporates an unbiased\nmechanism for the production of action potentials, which results in a more\nrealistic representation of the spiking dynamics of neurons. We also propose a\nmethod for training our ANN model, which allows us to generate accurate\nrepresentations of the population dynamics. Our model exhibits several\nsignificant advantages over existing models. First, we show that the\nrepresentations produced by our model are highly interpretable. Second, we\ndemonstrate that our model can be trained to generate spike trains that are\nclose to the corresponding population dynamics. Finally, we show that our\nmodel can be used for tasks such as predicting the firing rate of a single\nneuron in a population",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09722222222222222,
          "p": 0.08333333333333333,
          "f": 0.08974358477317582
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09722222222222222,
          "p": 0.08333333333333333,
          "f": 0.08974358477317582
        }
      }
    },
    {
      "paper_id": "cs.LG.econ/TH/2502.06387v1",
      "true_abstract": "Human-annotated preference data play an important role in aligning large\nlanguage models (LLMs). In this paper, we investigate the questions of\nassessing the performance of human annotators and incentivizing them to provide\nhigh-quality annotations. The quality assessment of language/text annotation\nfaces two challenges: (i) the intrinsic heterogeneity among annotators, which\nprevents the classic methods that assume the underlying existence of a true\nlabel; and (ii) the unclear relationship between the annotation quality and the\nperformance of downstream tasks, which excludes the possibility of inferring\nthe annotators' behavior based on the model performance trained from the\nannotation data. Then we formulate a principal-agent model to characterize the\nbehaviors of and the interactions between the company and the human annotators.\nThe model rationalizes a practical mechanism of a bonus scheme to incentivize\nannotators which benefits both parties and it underscores the importance of the\njoint presence of an assessment system and a proper contract scheme. From a\ntechnical perspective, our analysis extends the existing literature on the\nprincipal-agent model by considering a continuous action space for the agent.\nWe show the gap between the first-best and the second-best solutions (under the\ncontinuous action space) is of $\\Theta(1/\\sqrt{n \\log n})$ for the binary\ncontracts and $\\Theta(1/n)$ for the linear contracts, where $n$ is the number\nof samples used for performance assessment; this contrasts with the known\nresult of $\\exp(-\\Theta(n))$ for the binary contracts when the action space is\ndiscrete. Throughout the paper, we use real preference annotation data to\naccompany our discussions.",
      "generated_abstract": "the problem of predicting the number of votes cast for each\nballot in a multi-ballot election. In our setting, voters can choose to vote\nindividually or collectively, and a single ballot can be cast by multiple\nindividuals or by a single collective. This is a special case of a multi-ballot\nelection with binary votes, and we focus on the binary case, where a single\nballot can be cast by a single voter. In this setting, we consider the problem\nof learning the number of votes cast for each ballot. We provide the first\nexistence results for the expected number of votes cast in this setting, and\nestablish a sharp lower bound for the expected number of votes cast. We also\nprovide a polynomial-time algorithm that learns the number of votes cast for\neach ballot with probability one, and a sample-efficient algorithm that\nlearns the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1506849315068493,
          "p": 0.3333333333333333,
          "f": 0.207547165523318
        },
        "rouge-2": {
          "r": 0.03524229074889868,
          "p": 0.0784313725490196,
          "f": 0.0486322145667542
        },
        "rouge-l": {
          "r": 0.136986301369863,
          "p": 0.30303030303030304,
          "f": 0.1886792409950161
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/AP/2502.16988v1",
      "true_abstract": "A dynamic treatment regime is a sequence of treatment decision rules tailored\nto an individual's evolving status over time. In precision medicine, much focus\nhas been placed on finding an optimal dynamic treatment regime which, if\nfollowed by everyone in the population, would yield the best outcome on\naverage; and extensive investigation has been conducted from both\nmethodological and applications standpoints. The aim of this tutorial is to\nprovide readers who are interested in optimal dynamic treatment regimes with a\nsystematic, detailed but accessible introduction, including the formal\ndefinition and formulation of this topic within the framework of causal\ninference, identification assumptions required to link the causal quantity of\ninterest to the observed data, existing statistical models and estimation\nmethods to learn the optimal regime from data, and application of these methods\nto both simulated and real data.",
      "generated_abstract": "In this paper, we propose a novel method for the estimation of the\ndistribution of a random variable conditional on a set of conditionally\nindependent random variables. The method, called the conditional mean estimator,\nis based on a combination of two previously introduced methods: the\nconditional mean estimator, which is based on the maximum likelihood estimator\n(MLE) for the conditional distribution of the random variable, and the\nconditional estimator, which is based on the conditional mean estimator. We\nprovide theoretical guarantees for the performance of the proposed method in\nterms of the consistency and asymptotic normality of the estimator. Numerical\nsimulations and real data examples are presented to illustrate the proposed\nmethod.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16842105263157894,
          "p": 0.26666666666666666,
          "f": 0.20645160815816865
        },
        "rouge-2": {
          "r": 0.015384615384615385,
          "p": 0.022988505747126436,
          "f": 0.018433174919833907
        },
        "rouge-l": {
          "r": 0.1368421052631579,
          "p": 0.21666666666666667,
          "f": 0.16774193073881388
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.10304v2",
      "true_abstract": "A popular approach to perform inference on a target parameter in the presence\nof nuisance parameters is to construct estimating equations that are orthogonal\nto the nuisance parameters, in the sense that their expected first derivative\nis zero. Such first-order orthogonalization may, however, not suffice when the\nnuisance parameters are very imprecisely estimated. Leading examples where this\nis the case are models for panel and network data that feature fixed effects.\nIn this paper, we show how, in the conditional-likelihood setting, estimating\nequations can be constructed that are orthogonal to any chosen order. Combining\nthese equations with sample splitting yields higher-order bias-corrected\nestimators of target parameters. In an empirical application we apply our\nmethod to a fixed-effect model of team production and obtain estimates of\ncomplementarity in production and impacts of counterfactual re-allocations.",
      "generated_abstract": "We provide an analytical characterization of the asymptotic covariance matrix\nof a sample of random variables generated by a stochastic differential\nequation (SDE) with Lipschitz drift and square-integrable noise. This result\nprovides a rigorous foundation for the central limit theorem and other\nasymptotic results in stochastic differential equations. We apply our results\nto the simulation of stochastic differential equations, including the\nwell-known Brownian motion and the well-known fractional Brownian motion.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13978494623655913,
          "p": 0.2653061224489796,
          "f": 0.18309858702935938
        },
        "rouge-2": {
          "r": 0.016260162601626018,
          "p": 0.03076923076923077,
          "f": 0.021276591220575885
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.24489795918367346,
          "f": 0.16901407998710583
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.02178v1",
      "true_abstract": "This paper proposes an online inference method of the stochastic gradient\ndescent (SGD) with a constant learning rate for quantile loss functions with\ntheoretical guarantees. Since the quantile loss function is neither smooth nor\nstrongly convex, we view such SGD iterates as an irreducible and positive\nrecurrent Markov chain. By leveraging this interpretation, we show the\nexistence of a unique asymptotic stationary distribution, regardless of the\narbitrarily fixed initialization. To characterize the exact form of this\nlimiting distribution, we derive bounds for its moment generating function and\ntail probabilities, controlling over the first and second moments of SGD\niterates. By these techniques, we prove that the stationary distribution\nconverges to a Gaussian distribution as the constant learning rate\n$\\eta\\rightarrow0$. Our findings provide the first central limit theorem\n(CLT)-type theoretical guarantees for the last iterate of constant\nlearning-rate SGD in non-smooth and non-strongly convex settings. We further\npropose a recursive algorithm to construct confidence intervals of SGD iterates\nin an online manner. Numerical studies demonstrate strong finite-sample\nperformance of our proposed quantile estimator and inference method. The\ntheoretical tools in this study are of independent interest to investigate\ngeneral transition kernels in Markov chains.",
      "generated_abstract": "eld of Machine Learning, data is often described as a matrix of\nvalues. However, in many applications, the data are actually stored in\nother ways. One of the key challenges in these applications is the ability to\nperform matrix computations in a way that is compatible with the data format.\nThis problem is known as data format conversion, and it has received significant\nattention in recent years.\n  In this paper, we introduce the concept of data format conversion as a\nproblem in matrix computation. We also propose a solution to this problem,\nnamely, a set of rules for data format conversion. These rules are based on\nthe notion of matrix-form-preserving operations, which is a generalization of\nthe notion of matrix multiplication.\n  We show that the data format conversion problem is NP-complete and that\nthere are no polynomial-time algorithms for the data format conversion problem.\nMoreover, we show that the problem",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1484375,
          "p": 0.2345679012345679,
          "f": 0.18181817707103787
        },
        "rouge-2": {
          "r": 0.027624309392265192,
          "p": 0.038461538461538464,
          "f": 0.03215433597047251
        },
        "rouge-l": {
          "r": 0.1484375,
          "p": 0.2345679012345679,
          "f": 0.18181817707103787
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/CP/2501.15106v1",
      "true_abstract": "We study operator learning in the context of linear propagator models for\noptimal order execution problems with transient price impact \\`a la Bouchaud et\nal. (2004) and Gatheral (2010). Transient price impact persists and decays over\ntime according to some propagator kernel. Specifically, we propose to use\nIn-Context Operator Networks (ICON), a novel transformer-based neural network\narchitecture introduced by Yang et al. (2023), which facilitates data-driven\nlearning of operators by merging offline pre-training with an online few-shot\nprompting inference. First, we train ICON to learn the operator from various\npropagator models that maps the trading rate to the induced transient price\nimpact. The inference step is then based on in-context prediction, where ICON\nis presented only with a few examples. We illustrate that ICON is capable of\naccurately inferring the underlying price impact model from the data prompts,\neven with propagator kernels not seen in the training data. In a second step,\nwe employ the pre-trained ICON model provided with context as a surrogate\noperator in solving an optimal order execution problem via a neural network\ncontrol policy, and demonstrate that the exact optimal execution strategies\nfrom Abi Jaber and Neuman (2022) for the models generating the context are\ncorrectly retrieved. Our introduced methodology is very general, offering a new\napproach to solving optimal stochastic control problems with unknown state\ndynamics, inferred data-efficiently from a limited number of examples by\nleveraging the few-shot and transfer learning capabilities of transformer\nnetworks.",
      "generated_abstract": "ence of large language models (LLMs) has significantly enhanced\nthe efficiency of financial decision-making. However, the privacy and\ncompetitive risks of using LLMs in financial applications are still unclear. In\nthis paper, we analyze the trade-off between privacy and accuracy in LLM-based\nfinancial applications. We propose a novel framework that quantifies the\nprivacy-accuracy trade-off in LLM-based decision-making, and we demonstrate\nthat LLMs can improve the accuracy of financial decision-making but can\nincrease the privacy risks of using LLMs. We also propose an LLM-based\nframework for financial risk prediction. Our findings provide insights into the\nprivacy-accuracy trade-off in LLM-based financial applications and highlight the\npotential risks of using LLMs in financial decision-making. Our findings",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11409395973154363,
          "p": 0.2833333333333333,
          "f": 0.1626794217440078
        },
        "rouge-2": {
          "r": 0.008849557522123894,
          "p": 0.022222222222222223,
          "f": 0.01265822377423621
        },
        "rouge-l": {
          "r": 0.11409395973154363,
          "p": 0.2833333333333333,
          "f": 0.1626794217440078
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2408.15310v1",
      "true_abstract": "Recent studies suggest that drug-drug interaction (DDI) prediction via\ncomputational approaches has significant importance for understanding the\nfunctions and co-prescriptions of multiple drugs. However, the existing silico\nDDI prediction methods either ignore the potential interactions among drug-drug\npairs (DDPs), or fail to explicitly model and fuse the multi-scale drug feature\nrepresentations for better prediction. In this study, we propose RGDA-DDI, a\nresidual graph attention network (residual-GAT) and dual-attention based\nframework for drug-drug interaction prediction. A residual-GAT module is\nintroduced to simultaneously learn multi-scale feature representations from\ndrugs and DDPs. In addition, a dual-attention based feature fusion block is\nconstructed to learn local joint interaction representations. A series of\nevaluation metrics demonstrate that the RGDA-DDI significantly improved DDI\nprediction performance on two public benchmark datasets, which provides a new\ninsight into drug development.",
      "generated_abstract": "The emergence of biomolecular complexes is a crucial step in the evolution of\ncomplex life forms. We present a novel approach that enables the analysis of\ncomplex structures in a single-cell microscopy image. The method, based on\nsupervised machine learning, combines structural and functional features of the\nimage to identify cellular structures, such as chromatin, DNA, and membranes.\nUsing this approach, we identified and classified 216 different cellular\nstructures in a single-cell microscopy image of a human fibroblast. The\nstructures were identified as chromatin, DNA, and membranes, and classified as\nnon-chromatin, chromatin, and membrane structures, respectively. The method\nprovides a framework for the analysis of structural information in single-cell\nimages, offering a powerful tool for understanding the origins of complex\ncellular structures.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15789473684210525,
          "p": 0.20833333333333334,
          "f": 0.17964071365771464
        },
        "rouge-2": {
          "r": 0.031496062992125984,
          "p": 0.037383177570093455,
          "f": 0.03418802922456059
        },
        "rouge-l": {
          "r": 0.14736842105263157,
          "p": 0.19444444444444445,
          "f": 0.16766466575352304
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2502.21311v1",
      "true_abstract": "Comb Sign is an important imaging biomarker to detect multiple\ngastrointestinal diseases. It shows up as increased blood flow along the\nintestinal wall indicating potential abnormality, which helps doctors diagnose\ninflammatory conditions. Despite its clinical significance, current detection\nmethods are manual, time-intensive, and prone to subjective interpretation due\nto the need for multi-planar image-orientation. To the best of our knowledge,\nwe are the first to propose a fully automated technique for the detection of\nComb Sign from CTE scans. Our novel approach is based on developing a\nprobabilistic map that shows areas of pathological hypervascularity by\nidentifying fine vascular bifurcations and wall enhancement via processing\nthrough stepwise algorithmic modules. These modules include utilising deep\nlearning segmentation model, a Gaussian Mixture Model (GMM), vessel extraction\nusing vesselness filter, iterative probabilistic enhancement of vesselness via\nneighborhood maximization and a distance-based weighting scheme over the\nvessels. Experimental results demonstrate that our pipeline effectively\nidentifies Comb Sign, offering an objective, accurate, and reliable tool to\nenhance diagnostic accuracy in Crohn's disease and related hypervascular\nconditions where Comb Sign is considered as one of the important biomarkers.",
      "generated_abstract": "t advancement of deep learning techniques has enabled the\ndevelopment of efficient and accurate segmentation methods for medical images,\nparticularly for brain tumors. Despite the success of these methods, the\npresence of noise, inaccuracies, and variations in patient characteristics\nstill pose challenges in the automatic segmentation of tumors. In this work, we\nintroduce a novel framework for tumor segmentation, based on self-attention\nmechanisms. Our approach leverages the power of transformer architectures to\ncapture spatial information while eliminating the need for extensive training\ndata. Additionally, we propose a novel approach for tumor segmentation based on\nthe diffusion model. Our approach is based on the diffusion model, which has\nbeen shown to be a powerful tool for generating high-quality images. By\nintegrating this model with the transformer-based framework, we achieve a\nsignificant improvement in tumor",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.2696629213483146,
          "f": 0.21145373972636777
        },
        "rouge-2": {
          "r": 0.0449438202247191,
          "p": 0.064,
          "f": 0.052805275681033895
        },
        "rouge-l": {
          "r": 0.15217391304347827,
          "p": 0.23595505617977527,
          "f": 0.18502202166469378
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.08747v1",
      "true_abstract": "Cognitive delegation to artificial intelligence (AI) systems is transforming\nscientific research by enabling the automation of analytical processes and the\ndiscovery of new patterns in large datasets. This study examines the ability of\nAI to complement and expand knowledge in the analysis of breast cancer using\ndynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Building on a\nprevious study, we assess the extent to which AI can generate novel approaches\nand successfully solve them. For this purpose, AI models, specifically\nChatGPT-4o, were used for data preprocessing, hypothesis generation, and the\napplication of clustering techniques, predictive modeling, and correlation\nnetwork analysis. The results obtained were compared with manually computed\noutcomes, revealing limitations in process transparency and the accuracy of\ncertain calculations. However, as AI reduces errors and improves reasoning\ncapabilities, an important question arises regarding the future of scientific\nresearch: could automation replace the human role in science? This study seeks\nto open the debate on the methodological and ethical implications of a science\ndominated by artificial intelligence.",
      "generated_abstract": "We investigate the influence of dietary omega-3 fatty acids on immune\nchallenges and subsequent viral infections. A novel model incorporates\nimmune-related processes in a compartmental model framework with an\nindividual-based approach. We investigate the influence of dietary omega-3\nfatty acids on viral infection in a compartmental model framework. A novel\nmodel incorporates immune-related processes in a compartmental model framework\nwith an individual-based approach. We study the influence of dietary omega-3\nfatty acids on viral infection, with the goal of understanding the role of\nomega-3 fatty acids in viral infection.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09917355371900827,
          "p": 0.3333333333333333,
          "f": 0.15286623850379336
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08264462809917356,
          "p": 0.2777777777777778,
          "f": 0.12738853149742393
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/SR/2503.09744v1",
      "true_abstract": "The Sun's open-closed flux boundary (OCB) separates closed and open magnetic\nfield lines, and is the site for interchange magnetic reconnection processes\nthought to be linked to the origin of the slow solar wind (SSW). We analyse the\nglobal magnetic field structure and OCB from December 2010 to December 2019\nusing three coronal magnetic field models: a potential field source surface\n(PFSS) model, a static equilibrium magnetofrictional model, and a\ntime-dependent magnetofrictional model. We analyse the model and cycle\ndependence of the OCB length on the photosphere, as well as the magnetic flux\nin the vicinity of the OCB. Near solar maximum, the coronal magnetic field for\neach model consists predominantly of long, narrow coronal holes, and nearly all\nthe open flux lies within one supergranule-diameter (25 Mm) of the OCB. By\ncomparing to interplanetary scintillation measurements of SSW speeds, we argue\nthat the fraction of open flux within this 25 Mm band is a good predictor of\nthe amount of SSW in the heliosphere. Importantly, despite its simplicity, we\nshow that the PFSS model estimates this fraction as well as the time-dependent\nmodel. We discuss the implications of our results for understanding SSW origins\nand interchange reconnection at the OCB.",
      "generated_abstract": "t a new and efficient method to obtain the radial velocity (RV)\nspread of a planetary system from the radial velocity of a single star.\n  Using a non-linear least-squares method, we determine the RV spread of a\nsingle star using the measured RV of the target star. This method is applicable\nto systems where there are multiple stars, and the individual RVs are\nunavailable. The method is illustrated with the case of a solar-type star and\nits seven known planets. The RV spread of the star is found using the RV of a\nsingle star. Using this, we obtain the RV spread of the planets, and then\ndetermine the mass of the star using the RV spread of the star. The method\nreduces the need for observational data, and it is therefore suitable for\nsystems where observational data are scarce. The method is also applicable to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10714285714285714,
          "p": 0.18181818181818182,
          "f": 0.13483145600807994
        },
        "rouge-2": {
          "r": 0.00558659217877095,
          "p": 0.009433962264150943,
          "f": 0.00701753918769163
        },
        "rouge-l": {
          "r": 0.08928571428571429,
          "p": 0.15151515151515152,
          "f": 0.11235954589572043
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SY/2503.08920v1",
      "true_abstract": "A distributed integrated sensing and communication (D-ISAC) system offers\nsignificant cooperative gains for both sensing and communication performance.\nThese gains, however, can only be fully realized when the distributed nodes are\nperfectly synchronized, which is a challenge that remains largely unaddressed\nin current ISAC research. In this paper, we propose an over-the-air\ntime-frequency synchronization framework for the D-ISAC system, leveraging the\nreciprocity of bistatic sensing channels. This approach overcomes the\nimpractical dependency of traditional methods on a direct line-of-sight (LoS)\nlink, enabling the estimation of time offset (TO) and carrier frequency offset\n(CFO) between two ISAC nodes even in non-LoS (NLOS) scenarios. To achieve this,\nwe introduce a bistatic signal matching (BSM) technique with delay-Doppler\ndecoupling, which exploits offset reciprocity (OR) in bistatic observations.\nThis method compresses multiple sensing links into a single offset for\nestimation. We further present off-grid super-resolution estimators for TO and\nCFO, including the maximum likelihood estimator (MLE) and the matrix pencil\n(MP) method, combined with BSM processing. These estimators provide accurate\noffset estimation compared to spectral cross-correlation techniques. Also, we\nextend the pairwise synchronization leveraging OR between two nodes to the\nsynchronization of $N$ multiple distributed nodes, referred to as centralized\npairwise synchronization. We analyze the Cramer-Rao bounds (CRBs) for TO and\nCFO estimates and evaluate the impact of D-ISAC synchronization on the\nbottom-line target localization performance. Simulation results validate the\neffectiveness of the proposed algorithm, confirm the theoretical analysis, and\ndemonstrate that the proposed synchronization approach can recover up to 96% of\nthe bottom-line target localization performance of the fully-synchronous\nD-ISAC.",
      "generated_abstract": "r introduces a novel framework for controllability analysis of\ndynamic multi-agent systems (DMASs) using graph theory. The proposed approach\nconsiders a DMAS consisting of a graph-based network of agents with a\nsupervisory controller. The control policy of the supervisory controller\ninvolves a set of control inputs for each agent. The supervisory controller\nallocates these control inputs among the agents in the network, and each agent\noptimally chooses a control input that maximizes its expected utility. The\nproblem of finding the optimal control input allocation is formulated as a\nlinear programming problem, which can be solved efficiently using standard\nlinear programming techniques. The proposed approach enables the analysis of\ncontrol policies for DMASs based on a set of control inputs, which can be\ncomputed in polynomial time. The performance of the proposed approach is\nvalidated through simulations of a DMAS consisting of four agents",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12121212121212122,
          "p": 0.24691358024691357,
          "f": 0.1626016215992466
        },
        "rouge-2": {
          "r": 0.0205761316872428,
          "p": 0.04132231404958678,
          "f": 0.027472523034205523
        },
        "rouge-l": {
          "r": 0.10909090909090909,
          "p": 0.2222222222222222,
          "f": 0.1463414589976206
        }
      }
    },
    {
      "paper_id": "cs.LO.cs/LO/2503.08530v1",
      "true_abstract": "We present a choreographic framework for modelling and\n  analysing concurrent probabilistic systems based on the PRISM\n  model-checker. This is achieved through the development of a\n  choreography language, which is a specification language that allows\n  to describe the desired interactions within a concurrent system from\n  a global viewpoint. Using choreographies gives a clear and complete\n  view of system interactions, making it easier to understand the\n  process flow and identify potential errors, which helps ensure\n  correct execution and improves system reliability. We equip our\n  language with a probabilistic semantics and then define a formal\n  encoding into the PRISM language and discuss its\n  correctness. Properties of programs written in our choreographic\n  language can be model-checked by the PRISM model-checker via their\n  translation into the PRISM language. Finally, we implement a\n  compiler for our language and demonstrate its practical\n  applicability via examples drawn from the use cases featured in the\n  PRISM website.",
      "generated_abstract": "We present the first complete proof of the well-known fact that the\n$(1+\\varepsilon)$-reduction is not $\\varepsilon$-hard, for any $\\varepsilon > 0$.\nOur proof uses a $\\varepsilon$-reduction from the $\\varepsilon$-bounded-model problem\nto the $\\varepsilon$-bounded-model problem with bounded size, and thus it\nimproves on a $\\varepsilon$-reduction from the bounded-model problem to the\nbounded-model problem with bounded size. We also prove that this $\\varepsilon$ is\noptimal. Our proof combines a $\\varepsilon$-reduction from the $(1+\\varepsilon)$-bounded-model\nproblem with bounded size to the $\\varepsilon$-bounded-model problem with bounded\nsize, and a $\\varepsilon$-reduction from the $\\varepsilon$-bounded-model problem with\nbounded size to the $(1+\\varepsilon)$-bounded-model problem. We also prove that\nthis $\\varepsilon$ is optimal.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16494845360824742,
          "p": 0.36363636363636365,
          "f": 0.22695035031638255
        },
        "rouge-2": {
          "r": 0.014492753623188406,
          "p": 0.03389830508474576,
          "f": 0.020304564331985714
        },
        "rouge-l": {
          "r": 0.12371134020618557,
          "p": 0.2727272727272727,
          "f": 0.1702127616639003
        }
      }
    },
    {
      "paper_id": "math.RA.math/KT/2502.16257v1",
      "true_abstract": "The aim of this paper is twofold. In the first part, we define the cohomology\nof a Nijenhuis Lie algebra with coefficients in a suitable representation. Our\ncohomology of a Nijenhuis Lie algebra governs the simultaneous deformations of\nthe underlying Lie algebra and the Nijenhuis operator. Subsequently, we define\nhomotopy Nijenhuis operators on $2$-term $L_\\infty$-algebras and show that in\nsome cases they are related to third cocycles of Nijenhuis Lie algebras. In\nanother part of this paper, we extend our study to (generic) Nijenhuis Lie\nbialgebras where the Nijenhuis operators on the underlying Lie algebras and Lie\ncoalgebras need not be the same. In due course, we introduce matched pairs and\nManin triples of Nijenhuis Lie algebras and show that they are equivalent to\nNijenhuis Lie bialgebras. Finally, we consider the admissible classical\nYang-Baxter equation whose antisymmetric solutions yield Nijenhuis Lie\nbialgebras.",
      "generated_abstract": "In this article, we introduce a general framework for studying the\nnonlinear dynamics of the one-dimensional diffusion process with a\nnonlinear drift and a linear diffusion term, where the drift and the\ndiffusion coefficients are allowed to depend on the state variable. The\ndependence of the coefficients on the state variable is assumed to be\nnonlocal. We show that the one-dimensional diffusion process with a\nnonlinear drift and a linear diffusion term can be reduced to the\nnonlinear stochastic differential equation with a nonlinear drift and a\nlinear diffusion term. We also present the conditions under which the\nreduced nonlinear stochastic differential equation with a nonlinear drift\nand a linear diffusion term can be solved explicitly. Numerical simulations\nare also performed to illustrate the effectiveness of our proposed method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26582278481012656,
          "p": 0.3442622950819672,
          "f": 0.29999999508265307
        },
        "rouge-2": {
          "r": 0.05263157894736842,
          "p": 0.06741573033707865,
          "f": 0.0591132955684442
        },
        "rouge-l": {
          "r": 0.24050632911392406,
          "p": 0.3114754098360656,
          "f": 0.27142856651122454
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/optics/2503.10553v1",
      "true_abstract": "Between the absorption and the emission spectral lineshapes of dense atomic\nand molecular media, such as dye solutions and alkali-noble buffer gas mixtures\nat high pressure, in many cases there exists a universal scaling, the\nKennard-Stepanov relation, which is a manifestation of detailed balance. This\nrelation plays a crucial role in recent Bose-Einstein condensation experiments\nof visible-spectral-photons in e.g. dye-solution-filled optical microcavities.\nIt has recently been proposed to use high-pressure xenon-noble gas mixtures as\na thermalization medium for vacuum-ultraviolet regime photons, so as to extend\nthe achievable wavelength range of such Bose-Einstein-condensed optical sources\nfrom the visible to the vacuum-ultraviolet regime. In this work, we report\ntwo-photon excitation spectroscopy measurements of ground state ($5p^6$) xenon\natoms subject to up to 80bar of helium or krypton buffer gas pressure,\nrespectively, in the 220nm - 260nm wavelength range. The study of such\ntwo-photon spectra is of interest e.g. for the exploration of possible pumping\nschemes of a future vacuum-ultraviolet photon Bose-Einstein condensate. We have\nalso recorded absorption and emission spectra of the $5p^6 \\leftrightarrow\n5p^56s$ single-photon transition near 147nm wavelength of xenon atoms subject\nto 80bar of krypton buffer gas pressure. We find that the ratio of absorption\nand emission follows a Kennard-Stepanov scaling, which suggests that such gas\nmixtures are promising candidates as a thermalization medium for a\nBose-Einstein condensate of vacuum-ultraviolet photons.",
      "generated_abstract": "We present a theoretical study of the propagation of light through an\ng-quartz-glass lens, focusing on the influence of the birefringence of the\nlens on the phase and amplitude of the light beam. The results show that the\nbirefringence of the lens influences the propagation of the light beam in\nterms of both the phase and amplitude of the beam. We analyze the effect of\nbirefringence on the amplitude of the beam and the phase of the beam, as well\nas the influence of the birefringence on the intensity of the light beam. The\nresults indicate that the birefringence of the lens has a significant impact\non the propagation of the light beam, which can be used to improve the\nperformance of lenses with birefringence.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09774436090225563,
          "p": 0.2653061224489796,
          "f": 0.14285713892223173
        },
        "rouge-2": {
          "r": 0.01990049751243781,
          "p": 0.05063291139240506,
          "f": 0.028571424520663843
        },
        "rouge-l": {
          "r": 0.09774436090225563,
          "p": 0.2653061224489796,
          "f": 0.14285713892223173
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.gr-qc/2503.10423v1",
      "true_abstract": "Sterile neutrinos can influence the evolution of the universe, and thus\ncosmological observations can be used to search for sterile neutrinos. In this\nstudy, we utilized the latest baryon acoustic oscillations data from DESI,\ncombined with the cosmic microwave background data from Planck and the\nfive-year supernova data from DES, to constrain the interacting dark energy\n(IDE) models involving both cases of massless and massive sterile neutrinos. We\nconsider four typical forms of the interaction term $Q=\\beta H \\rho_{\\rm de}$,\n$Q=\\beta H \\rho_{\\rm c}$, $Q=\\beta H_{0} \\rho_{\\rm de}$, and $Q=\\beta H_{0}\n\\rho_{\\rm c}$, respectively. Our analysis indicates that the current data\nprovide only a hint of the existence of massless sterile neutrinos (as dark\nradiation) at about the $1\\sigma$ level. In contrast, no evidence supports the\nexistence of massive sterile neutrinos. Furthermore, in IDE models, the\ninclusion of (massless/massive) sterile neutrinos has a negligible impact on\nthe constraint of the coupling parameter $\\beta$. The IDE model of $Q=\\beta H\n\\rho_{\\rm c}$ with sterile neutrinos does not favor an interaction. However,\nthe other three IDE models with sterile neutrinos support an interaction in\nwhich dark energy decays into dark matter.",
      "generated_abstract": "t the first cosmological constraints on the non-standard model of\nnon-gravitational-wave gravitational waves from gravitational-wave observations\nand the Cosmic Microwave Background (CMB). We use the first year of the\nBeyond Gravitational-Wave Observatory (BGO) data to constrain the parameters of\nthe so-called non-standard model of non-gravitational-wave gravitational waves.\nThe non-standard model is the extension of general relativity in which the\ngravitational field is described by a non-linear function of the metric and\ntensor, and the gravitational waves are generated by this non-linear field. We\nshow that the non-standard model of non-gravitational-wave gravitational waves\ncan explain the observations and can also fit the CMB temperature and polarization\nobservations. We find that the non-standard model of non",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.168141592920354,
          "p": 0.3114754098360656,
          "f": 0.21839080004425956
        },
        "rouge-2": {
          "r": 0.04938271604938271,
          "p": 0.08888888888888889,
          "f": 0.06349205890022709
        },
        "rouge-l": {
          "r": 0.1504424778761062,
          "p": 0.2786885245901639,
          "f": 0.19540229429713316
        }
      }
    },
    {
      "paper_id": "physics.ins-det.hep-ex/2503.09303v1",
      "true_abstract": "This contribution introduces a novel test system developed to evaluate the\nsignal transmission quality in high-speed data links for the 2026 Inner Tracker\n(ITk) upgrade of the ATLAS experiment. Using an FPGA-based data acquisition\n(DAQ) framework, the setup can run simultaneous Bit Error Rate (BER) tests for\nup to 64 channels and generate virtual eye diagrams, for qualifying the\n$\\sim$26k electrical links at the ATLAS ITk data rate of 1.28Gb/s. The paper\nincludes results from system calibration, yielding its contribution to the\nmeasured losses, and preliminary results from tests of prototype and\npre-production assemblies of on-detector links of the three ATLAS ITk Pixel\nsubsystems.",
      "generated_abstract": "C will provide unprecedented luminosities to probe the dynamics of\nthe most extreme particle physics and astronomical phenomena. This paper\nintroduces a novel framework to explore the full phase space of the HL-LHC\nusing a novel event generator, which is able to model the detector response\nand the interactions of the particles with the detector. This new framework\nenables a more detailed investigation of the HL-LHC physics, which will not be\npossible using the standard Monte-Carlo event generator. We demonstrate the\npotential of this event generator to provide insights into the HL-LHC physics\nby exploring the high-energy physics of the Higgs boson, the search for new\nparticles, the search for dark matter, and the search for physics beyond the\nStandard Model. The event generator also allows us to explore the physics of\nother high-energy experiments,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14473684210526316,
          "p": 0.14285714285714285,
          "f": 0.14379084467341638
        },
        "rouge-2": {
          "r": 0.03,
          "p": 0.02586206896551724,
          "f": 0.02777777280521351
        },
        "rouge-l": {
          "r": 0.13157894736842105,
          "p": 0.12987012987012986,
          "f": 0.13071894924857977
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.10004v1",
      "true_abstract": "In this paper, we present a hierarchical framework that integrates\nupper-level routing with low-level optimal trajectory planning for connected\nand automated vehicles (CAVs) traveling in an urban network. The upper-level\ncontroller efficiently distributes traffic flows by utilizing a dynamic\nre-routing algorithm that leverages real-time density information and the\nfundamental diagrams of each network edge. This re-routing approach predicts\nwhen each edge will reach critical density and proactively adjusts the routing\nalgorithm's weights to prevent congestion before it occurs. The low-level\ncontroller coordinates CAVs as they cross signal-free intersections, generating\noptimal, fuel-efficient trajectories while ensuring safe passage by satisfying\nall relevant constraints. We formulate the problem as an optimal control\nproblem and derive an analytical solution. Using the SUMO micro-simulation\nplatform, we conduct simulation experiments on a realistic network. The results\nshow that our hierarchical framework significantly enhances network performance\ncompared to a baseline static routing approach. By dynamically re-routing\nvehicles, our approach successfully reduces total travel time and mitigates\ncongestion before it develops.",
      "generated_abstract": "In this paper, we propose a novel framework for a single-cell multi-objective\noptimization (SCMO) problem. SCMO is a multi-objective optimization problem\nwhere each objective function is a function of the state variables of a\nparticular system. We propose a hybrid approach for solving SCMO by employing\nthe extended Kalman filter (EKF) and the particle filter (PF). The EKF is\napplied to estimate the system's state variables while the PF is used to\nestimate the optimal control. The proposed hybrid approach is validated with\nnumerical simulations. It is shown that the proposed hybrid approach\nsignificantly outperforms the traditional single-objective optimization\nframework, particularly in terms of computational efficiency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.35294117647058826,
          "f": 0.25531914431869634
        },
        "rouge-2": {
          "r": 0.03164556962025317,
          "p": 0.052083333333333336,
          "f": 0.03937007403806864
        },
        "rouge-l": {
          "r": 0.19166666666666668,
          "p": 0.3382352941176471,
          "f": 0.2446808464463559
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2502.12479v2",
      "true_abstract": "The motif-scaffolding problem is a central task in computational protein\ndesign: Given the coordinates of atoms in a geometry chosen to confer a desired\nbiochemical function (a motif), the task is to identify diverse protein\nstructures (scaffolds) that include the motif and maintain its geometry.\nSignificant recent progress on motif-scaffolding has been made due to\ncomputational evaluation with reliable protein structure prediction and\nfixed-backbone sequence design methods. However, significant variability in\nevaluation strategies across publications has hindered comparability of\nresults, challenged reproducibility, and impeded robust progress. In response\nwe introduce MotifBench, comprising (1) a precisely specified pipeline and\nevaluation metrics, (2) a collection of 30 benchmark problems, and (3) an\nimplementation of this benchmark and leaderboard at\ngithub.com/blt2114/MotifBench. The MotifBench test cases are more difficult\ncompared to earlier benchmarks, and include protein design problems for which\nsolutions are known but on which, to the best of our knowledge,\nstate-of-the-art methods fail to identify any solution.",
      "generated_abstract": "nge of biological systems, including neurons, have been shown to\nbe governed by the same set of conserved principles, such as the activity\nspectrum and spike-timing-dependent plasticity. This has led to the emergence of\na unifying framework, the activity spectrum model, which provides a coherent\nexplanation of a diverse range of biological phenomena. This model holds that\nneurons respond to stimuli by varying the strength of their excitatory postsynaptic\npotential (EPSP) and, as a consequence, by varying the time of their\nspikes. In this work, we present a novel framework for understanding the\nspike-timing-dependent plasticity of single-cell recording data. By exploiting\nthe spike-timing-dependent plasticity of the neurons, we can construct a\nspike-timing-dependent dynamics model that can",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.1891891891891892,
          "f": 0.15053762961729694
        },
        "rouge-2": {
          "r": 0.006493506493506494,
          "p": 0.009615384615384616,
          "f": 0.007751933172288306
        },
        "rouge-l": {
          "r": 0.10714285714285714,
          "p": 0.16216216216216217,
          "f": 0.12903225327321094
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/CV/2503.10633v1",
      "true_abstract": "As there are now millions of publicly available neural networks, searching\nand analyzing large model repositories becomes increasingly important.\nNavigating so many models requires an atlas, but as most models are poorly\ndocumented charting such an atlas is challenging. To explore the hidden\npotential of model repositories, we chart a preliminary atlas representing the\ndocumented fraction of Hugging Face. It provides stunning visualizations of the\nmodel landscape and evolution. We demonstrate several applications of this\natlas including predicting model attributes (e.g., accuracy), and analyzing\ntrends in computer vision models. However, as the current atlas remains\nincomplete, we propose a method for charting undocumented regions.\nSpecifically, we identify high-confidence structural priors based on dominant\nreal-world model training practices. Leveraging these priors, our approach\nenables accurate mapping of previously undocumented areas of the atlas. We\npublicly release our datasets, code, and interactive atlas.",
      "generated_abstract": "This paper studies the problem of online learning in a stochastic\ngraph with $n$ nodes and edge-dependent weights. The goal is to learn a\nrepresentation of the graph, which we call a \\emph{spectral embedding}, that\ncaptures the structural properties of the graph. In particular, the goal is to\nlearn a spectral embedding that is a $k$-hop projection of the graph, for\n$k\\leq n/2$, and an $n$-hop projection of the graph. We derive a\n$1-\\frac{O(1)}{n}$-approximation for the problem of learning a spectral\nembedding with $n$ nodes and edge-dependent weights.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10377358490566038,
          "p": 0.22916666666666666,
          "f": 0.14285713856636884
        },
        "rouge-2": {
          "r": 0.007142857142857143,
          "p": 0.014705882352941176,
          "f": 0.009615380214499055
        },
        "rouge-l": {
          "r": 0.09433962264150944,
          "p": 0.20833333333333334,
          "f": 0.12987012557935587
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/SY/2503.09904v1",
      "true_abstract": "In studies on complex network systems using graph theory, eigen-analysis is\ntypically performed on an undirected graph model of the network. However, when\nanalyzing cascading failures in a power system, the interactions among failures\nsuggest the need for a directed graph beyond the topology of the power system\nto model directions of failure propagation. To accurately quantify failure\ninteractions for effective mitigation strategies, this paper proposes a\nstochastic interaction graph model and associated eigen-analysis. Different\ntypes of modes on failure propagations are defined and characterized by the\neigenvalues of a stochastic interaction matrix, whose absolute values are\nunity, zero, or in between. Finding and interpreting these modes helps identify\nthe probable patterns of failure propagation, either local or widespread, and\nthe participating components based on eigenvectors. Then, by lowering the\nfailure probabilities of critical components highly participating in a mode of\nwidespread failures, cascading can be mitigated. The validity of the proposed\nstochastic interaction graph model, eigen-analysis and the resulting mitigation\nstrategies is demonstrated using simulated cascading failure data on an NPCC\n140-bus system.",
      "generated_abstract": "r proposes a novel distributed multi-agent reinforcement learning\n(MARL) framework for decentralized control of robotic systems. The proposed\nframework integrates a cooperative coordination strategy and a decentralized\ncontroller design with a centralized model predictive controller (MPC). The\ncooperative coordination strategy enables the decentralized controllers to\nobserve the centralized MPC, providing them with an estimate of the centralized\ncontroller's state. The decentralized controllers then adjust their own\ncontrol strategies to align with the centralized MPC's state estimate. The\ncentralized MPC uses the centralized controller's estimate to design an\noptimized control law for the centralized controller. The decentralized\ncontroller's estimate of the centralized MPC's state is obtained by\ncooperatively observing the centralized MPC's state estimate, which is\nobtained from",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14018691588785046,
          "p": 0.2459016393442623,
          "f": 0.17857142394628697
        },
        "rouge-2": {
          "r": 0.012345679012345678,
          "p": 0.020618556701030927,
          "f": 0.015444010758934652
        },
        "rouge-l": {
          "r": 0.1308411214953271,
          "p": 0.22950819672131148,
          "f": 0.16666666204152505
        }
      }
    },
    {
      "paper_id": "quant-ph.physics/atom-ph/2503.09946v1",
      "true_abstract": "The radiative properties of atoms are inherently linked to their surrounding\nenvironment. Placing an electromagnetic resonator around atoms can enhance\nspontaneous emission, as shown by Purcell in the 1940s. This approach is now\nroutinely used in quantum computing and communication to channel photons\nemitted by atoms into well-defined modes and control atom-photon interactions.\nFor solid-state artificial atoms, such as color-centers, the host lattice\nintroduces an acoustic environment, allowing excited atoms to relax by emitting\nphonons. Here we observe the acoustic Purcell effect by constructing a\nspecially engineered, microwave-frequency nanomechanical resonator around a\ncolor-center spin qubit in diamond. Using a co-localized optical mode of the\nstructure that strongly couples to the color-center's excited state, we perform\nsingle-photon-level laser spectroscopy at milliKelvin temperatures and observe\nten-fold faster spin relaxation when the spin qubit is tuned into resonance\nwith a 12 GHz acoustic mode. Additionally, we use the color-center as an\natomic-scale probe to measure the broadband phonon spectrum of the\nnanostructure up to a frequency of 28 GHz. Our work establishes a new regime of\ncontrol for quantum defects in solids and paves the way for interconnects\nbetween atomic-scale quantum memories and qubits encoded in acoustic and\nsuperconducting devices.",
      "generated_abstract": "igate the quantum properties of a one-dimensional (1D) quantum dot\nin the presence of a local magnetic field. The dot is composed of an electron\nand a hole, and the 1D geometry allows for a direct coupling between the dot's\nenergy eigenstates. The dot's energy spectrum is split into two degenerate\nsubspaces by the magnetic field, which can be characterized by a Zeeman\nsplitting. We calculate the electron and hole's energy eigenvalues and\neigenstates, and investigate their effects on the dot's quantum properties. We\ninvestigate the effects of the Zeeman splitting on the dot's energy spectrum,\nand show that the Zeeman splitting can be used to tune the dot's energy\neigenstates. Additionally, we examine the effects of the Zeeman splitting on\nthe dot's quantum properties. We show that the Zeeman splitting can be used to\ntune the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.3492063492063492,
          "f": 0.22564102126706123
        },
        "rouge-2": {
          "r": 0.02072538860103627,
          "p": 0.04,
          "f": 0.027303749769945644
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.3492063492063492,
          "f": 0.22564102126706123
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.18710v1",
      "true_abstract": "Understanding convergent learning -- the extent to which artificial and\nbiological neural networks develop similar representations -- is crucial for\nneuroscience and AI, as it reveals shared learning principles and guides\nbrain-like model design. While several studies have noted convergence in early\nand late layers of vision networks, key gaps remain. First, much existing work\nrelies on a limited set of metrics, overlooking transformation invariances\nrequired for proper alignment. We compare three metrics that ignore specific\nirrelevant transformations: linear regression (ignoring affine\ntransformations), Procrustes (ignoring rotations and reflections), and\npermutation/soft-matching (ignoring unit order). Notably, orthogonal\ntransformations align representations nearly as effectively as more flexible\nlinear ones, and although permutation scores are lower, they significantly\nexceed chance, indicating a robust representational basis. A second critical\ngap lies in understanding when alignment emerges during training. Contrary to\nexpectations that convergence builds gradually with task-specific learning, our\nfindings reveal that nearly all convergence occurs within the first epoch --\nlong before networks achieve optimal performance. This suggests that shared\ninput statistics, architectural biases, or early training dynamics drive\nconvergence rather than the final task solution. Finally, prior studies have\nnot systematically examined how changes in input statistics affect alignment.\nOur work shows that out-of-distribution (OOD) inputs consistently amplify\ndifferences in later layers, while early layers remain aligned for both\nin-distribution and OOD inputs, suggesting that this alignment is driven by\ngeneralizable features stable across distribution shifts. These findings fill\ncritical gaps in our understanding of representational convergence, with\nimplications for neuroscience and AI.",
      "generated_abstract": "cle provides a concise review of the biological bases of\nthe neural correlates of attention, focusing on how attention changes the\ndensity of the synaptic connections between neurons. Attention is defined as a\nchange in the relative strength of the synaptic connections between neurons,\nwhich can be measured in terms of the change in the spike rate of the neurons\nundergoing attention. This review is organized into two parts. The first part\nreviews the biological bases of attention, focusing on how attention changes\nthe density of the synaptic connections between neurons. The second part\ndiscusses the neural mechanisms underlying attention, focusing on how the\nneural mechanisms underlying attention change. We first discuss the biological\nbasis of attention, including the concept of attention, the neural mechanisms\nof attention, and the neural correlates of attention. We then review the\nbiological bases of the neural",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09574468085106383,
          "p": 0.3050847457627119,
          "f": 0.14574898421806629
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09042553191489362,
          "p": 0.288135593220339,
          "f": 0.13765181822616346
        }
      }
    },
    {
      "paper_id": "cs.DB.stat/OT/2403.08127v2",
      "true_abstract": "Globally, there is an increased need for guidelines to produce high-quality\ndata outputs for analysis. No framework currently exists that provides\nguidelines for a comprehensive approach to producing analysis ready data (ARD).\nThrough critically reviewing and summarising current literature, this paper\nproposes such guidelines for the creation of ARD. The guidelines proposed in\nthis paper inform ten steps in the generation of ARD: ethics, project\ndocumentation, data governance, data management, data storage, data discovery\nand collection, data cleaning, quality assurance, metadata, and data\ndictionary. These steps are illustrated through a substantive case study that\naimed to create ARD for a digital spatial platform: the Australian Child and\nYouth Wellbeing Atlas (ACYWA).",
      "generated_abstract": "This paper introduces a novel framework for the development of efficient\nreliable, scalable and fault-tolerant distributed algorithms. The framework\nutilizes the notion of multi-objective functional, which is a function that\nquantifies the quality of a solution across multiple objectives. We demonstrate\nthat a significant amount of effort can be saved in the design of multi-objective\nalgorithms by using a single objective. The proposed framework is validated by\ncomparing the performance of a multi-objective algorithm with a single-objective\nalgorithm that has a significant overhead in terms of computational time and\nmemory requirements.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1625,
          "p": 0.21666666666666667,
          "f": 0.18571428081632668
        },
        "rouge-2": {
          "r": 0.018691588785046728,
          "p": 0.022988505747126436,
          "f": 0.02061855175417272
        },
        "rouge-l": {
          "r": 0.1625,
          "p": 0.21666666666666667,
          "f": 0.18571428081632668
        }
      }
    },
    {
      "paper_id": "gr-qc.math/MP/2503.09222v1",
      "true_abstract": "In this paper, we study the energy conditions of charged traversable\nwormholes in the framework of $f(R, \\mathscr{L}_m)$\n  modified gravity. In the first case, we derive the shape functions (SFs) for\ntwo different choices of the charge function $\\mathcal{E}^2$ by considering the\nExponential Spheroid (ES) model and analyze the null energy condition (NEC). In\nthe second case, we consider a particular shape function and study its\nimplications for the energy conditions. In both cases, we obtain expressions\nfor energy density and pressure in radial and tangential directions. Our\nfindings show that the radial NEC remains satisfied across a wide range of\ncharge parameters $\\mathcal{E}$ consistent with established physical laws.\nHowever, the tangential NEC is only sustained in the range $0.1 \\leq\n\\mathcal{E} \\leq 0.6$; for higher charge values, violations occur, indicating\nthe formation of a throat-like structure necessary for wormhole stability.\nAdditionally, we compare the pressure-density profiles of these charged\nwormholes with those of compact objects such as neutron stars, revealing\ndistinct variations in matter distribution. This analysis highlights the\ncrucial role of charge and modified gravity in determining the stability and\nphysical characteristics of wormhole structures.",
      "generated_abstract": "aper, we study the dynamics of the Hamiltonian system\n$\\dot{z} = \\mathcal{H}(z)$ in a domain $\\mathcal{D} \\subset \\mathbb{C}^n$,\nwhere $\\mathcal{H}(z)$ is a polynomial of degree $n$ in $z$. We assume that\n$\\mathcal{H}(z)$ is a rational function, and we study the dynamics of $\\mathcal{H}$\non the subspace $\\mathcal{F}$ of rational functions. We prove that for any\n$z\\in \\mathcal{D}$ there exists a unique rational function $\\mathcal{H}_{\\mathcal{D}}(z)$\nsuch that $\\mathcal{H}(z) = \\mathcal{H}_{\\mathcal{D}}(z)$ and\n$\\mathcal{H}_{\\mathcal{D}}(z) \\in \\mathcal{F}$. We show that $\\mathcal{H}_{\\mathcal{D}}(z)$\nis a polynomial of degree $",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11666666666666667,
          "p": 0.2641509433962264,
          "f": 0.1618497067326006
        },
        "rouge-2": {
          "r": 0.022099447513812154,
          "p": 0.05555555555555555,
          "f": 0.031620549287756934
        },
        "rouge-l": {
          "r": 0.10833333333333334,
          "p": 0.24528301886792453,
          "f": 0.1502890130909821
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2502.16378v1",
      "true_abstract": "Machine learning (ML) has been playing important roles in drug discovery in\nthe past years by providing (pre-)screening tools for prioritising chemical\ncompounds to pass through wet lab experiments. One of the main ML tasks in drug\ndiscovery is to build quantitative structure-activity relationship (QSAR)\nmodels, associating the molecular structure of chemical compounds with an\nactivity or property. These properties -- including absorption, distribution,\nmetabolism, excretion and toxicity (ADMET) -- are essential to model compound\nbehaviour, activity and interactions in the organism. Although several methods\nexist, the majority of them do not provide an appropriate model's\npersonalisation, yielding to bias and lack of generalisation to new data since\nthe chemical space usually shifts from application to application. This fact\nleads to low predictive performance when completely new data is being tested by\nthe model. The area of Automated Machine Learning (AutoML) emerged aiming to\nsolve this issue, outputting tailored ML algorithms to the data at hand.\nAlthough an important task, AutoML has not been practically used to assist\ncheminformatics and computational chemistry researchers often, with just a few\nworks related to the field. To address these challenges, this work introduces\nAuto-ADMET, an interpretable evolutionary-based AutoML method for chemical\nADMET property prediction. Auto-ADMET employs a Grammar-based Genetic\nProgramming (GGP) method with a Bayesian Network Model to achieve comparable or\nbetter predictive performance against three alternative methods -- standard GGP\nmethod, pkCSM and XGBOOST model -- on 12 benchmark chemical ADMET property\nprediction datasets. The use of a Bayesian Network model on Auto-ADMET's\nevolutionary process assisted in both shaping the search procedure and\ninterpreting the causes of its AutoML performance.",
      "generated_abstract": "st decade, deep learning-based methods have revolutionized\nchallenging tasks in the field of molecular protein structure prediction.\nHowever, these methods still face limitations in predicting secondary structures\nof biomolecules, due to the limited number of training examples and the\ncomplexities of structure prediction. In this study, we propose a novel\nframework for the generation of large-scale training data of biomolecular\nsecondary structures. Our approach is based on a novel dataset generation\ntechnique that combines the features of molecular dynamics simulations with\nneural networks. By combining the molecular dynamics simulations and\nneural network prediction results, the proposed framework generates a large\ndataset of biomolecular secondary structures. We evaluate our framework on the\nHUGO Gene Nomenclature Committee (HGNC) database, where we achieve an F1 score\nof 0.971. Additionally, we apply our framework",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11731843575418995,
          "p": 0.2413793103448276,
          "f": 0.1578947324402172
        },
        "rouge-2": {
          "r": 0.0196078431372549,
          "p": 0.041666666666666664,
          "f": 0.026666662314667376
        },
        "rouge-l": {
          "r": 0.11173184357541899,
          "p": 0.22988505747126436,
          "f": 0.150375935447736
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ph/2503.10397v1",
      "true_abstract": "We give a pedagogical introduction to hadron spectroscopy and structure\nstudies using functional methods. We explain the basic features of\nDyson-Schwinger, Bethe-Salpeter and Faddeev equations, which are employed to\ncalculate the spectra of mesons, baryons and four-quark states. We discuss\ndynamical mass generation as a consequence of the spontaneous breaking of\nchiral symmetry, which is intertwined with the emergence of the light pions as\nGoldstone bosons of QCD. We highlight the importance of diquark correlations in\nthe baryon sector, while for four-quark states such as the light scalar mesons\nand heavy exotics the dominant two-body clusters are typically mesons. We\nconclude with a brief discussion of hadron matrix elements like electromagnetic\nform factors and how vector-meson dominance is an automatic outcome of\nfunctional equations.",
      "generated_abstract": "the hadronic spectrum of the $Z\\to 3\\ell$ decay in the Standard\nmodel and in a $Z\\gamma$ mediated $SU(2)$ gauge-mediated supersymmetric\nextension (SUSY-GMS). We find that the $Z\\to 3\\ell$ decay is suppressed by\nfraction of the $Z\\to \\ell\\ell$ decay rate, and therefore the $Z\\to 3\\ell$\ncross section is suppressed by the same factor. This results in a suppression of\nthe $Z\\to 3\\ell$ cross section in the SUSY-GMS model. We further calculate the\n$Z\\to 3\\ell$ cross section in the minimal supersymmetric standard model (MSSM)\nin the presence of a Higgs doublet. We find that the $Z\\to 3\\ell$",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09411764705882353,
          "p": 0.17777777777777778,
          "f": 0.12307691855029602
        },
        "rouge-2": {
          "r": 0.025,
          "p": 0.045454545454545456,
          "f": 0.03225805993756569
        },
        "rouge-l": {
          "r": 0.09411764705882353,
          "p": 0.17777777777777778,
          "f": 0.12307691855029602
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.08262v1",
      "true_abstract": "The search for the optimal pair of active and protection paths in a network\nwith Shared Risk Link Groups (SRLG) is a challenging but high-value problem in\nthe industry that is inevitable in ensuring reliable connections on the modern\nInternet. We propose a new approach to solving this problem, with a novel use\nof statistical analysis of the distribution of paths with respect to their\ncost, which is an integral part of our innovation. The key idea in our\nalgorithm is to employ iterative updates of cost bounds, allowing efficient\npruning of suboptimal paths. This idea drives an efficacious exploration of the\nsearch space. We benchmark our algorithms against the state-of-the-art\nalgorithms that exploit the alternative strategy of conflicting links\nexclusion, showing that our approach has the advantage of finding more feasible\nconnections within a set time limit.",
      "generated_abstract": "r presents a novel framework for solving the bipartite graph\nproblem of finding a minimum-weight perfect matching using dynamic programming.\nThe algorithm is based on a modified dynamic programming approach. It\ndemonstrates that the optimal solution for the bipartite graph problem can be\nobtained in polynomial time in the number of edges of the bipartite graph and\nin the number of iterations of the dynamic programming process. Moreover, the\nalgorithm achieves an optimal running time in the worst case of linear time\nin the number of edges of the bipartite graph. The algorithm is also\ngeneralizable to the undirected graph problem, where it achieves a running time\nof $O(n^2)$, where $n$ is the number of edges of the graph. Additionally, the\nalgorithm is also applied to the weighted bipartite graph problem and the\nunweighted bipartite graph problem. The running time of the algorithm for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22580645161290322,
          "p": 0.3387096774193548,
          "f": 0.27096773713548394
        },
        "rouge-2": {
          "r": 0.051470588235294115,
          "p": 0.06930693069306931,
          "f": 0.05907172506685223
        },
        "rouge-l": {
          "r": 0.1935483870967742,
          "p": 0.2903225806451613,
          "f": 0.23225805971612912
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2503.07357v1",
      "true_abstract": "In this work, we investigate the generalization of a multi-channel\nlearning-based replay speech detector, which employs adaptive beamforming and\ndetection, across different microphone arrays. In general, deep neural\nnetwork-based microphone array processing techniques generalize poorly to\nunseen array types, i.e., showing a significant training-test mismatch of\nperformance. We employ the ReMASC dataset to analyze performance degradation\ndue to inter- and intra-device mismatches, assessing both single- and\nmulti-channel configurations. Furthermore, we explore fine-tuning to mitigate\nthe performance loss when transitioning to unseen microphone arrays. Our\nfindings reveal that array mismatches significantly decrease detection\naccuracy, with intra-device generalization being more robust than inter-device.\nHowever, fine-tuning with as little as ten minutes of target data can\neffectively recover performance, providing insights for practical deployment of\nreplay detection systems in heterogeneous automatic speaker verification\nenvironments.",
      "generated_abstract": "cements of neural networks have significantly enhanced speech\nprocessing, especially in low-resource languages. However, their performance\nis often hindered by the lack of adequate data for training. In this paper, we\nintroduce a novel data augmentation method for low-resource languages,\nwhich aims to improve the generalization capability of neural networks. The\nproposed method is based on data augmentation, which transforms the original\ndataset into multiple synthetic datasets by generating synthetic data. Our\nproposed method is inspired by the recent success of the zero-shot learning\nmethod, which takes the original dataset and a few synthetic datasets as the\ntraining data. However, there is no established method for synthesizing\nsynthetic data, which makes it challenging to design a general and effective\nproposed method. To address this issue, we propose a novel method for\nsynthesizing synthetic data, which can",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21359223300970873,
          "p": 0.2682926829268293,
          "f": 0.2378378329022645
        },
        "rouge-2": {
          "r": 0.015384615384615385,
          "p": 0.01680672268907563,
          "f": 0.016064252037871908
        },
        "rouge-l": {
          "r": 0.1941747572815534,
          "p": 0.24390243902439024,
          "f": 0.21621621128064292
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.02342v1",
      "true_abstract": "This paper proposes an original methodology based on Named Entity Recognition\n(NER) to identify companies involved in downstream space activities, i.e.,\ncompanies that provide services or products exploiting data and technology from\nspace. Using a rule-based approach, the method leverages a corpus of texts from\ndigitized French press articles to extract company names related to the\ndownstream space segment. This approach allowed the detection of 88 new\ndownstream space companies, enriching the existing database of the sector by\n33\\%. The paper details the identification process and provides guidelines for\nfuture replications, applying the method to other geographic areas, or adapting\nit to other industries where new entrants are challenging to identify using\ntraditional activity classifications.",
      "generated_abstract": "r introduces a new methodology to assess the impact of the COVID-19\noutbreak on the global economy. We develop a novel framework to assess the\nimpact of the pandemic on the global economy and its sectors, focusing on\ninterdependencies between countries. Our methodology combines a dynamic\nsimulation model with a comprehensive dataset to estimate the impact of the\npandemic on the global economy and its sectors. The simulation model employs a\nmulti-sectoral approach to capture the impact of the pandemic on various\nsectors, including manufacturing, transportation, services, and energy. The\nmodel simulates the impact of the pandemic on economic output, employment, and\nincome using a dynamic, stochastic, and structural framework. The model also\nencompasses interdependencies between countries, including supply chains,\ntrade, and financial links. The model captures the impact",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.16666666666666666,
          "f": 0.14285713795918387
        },
        "rouge-2": {
          "r": 0.009009009009009009,
          "p": 0.010526315789473684,
          "f": 0.009708732894243283
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.16666666666666666,
          "f": 0.14285713795918387
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2503.02697v1",
      "true_abstract": "This paper investigates an infinite horizon, discounted,\nconsumption-portfolio problem in a market with one bond, one liquid risky\nasset, and one illiquid risky asset with proportional transaction costs. We\nconsider an agent with liquidity preference, modeled by a Cobb-Douglas utility\nfunction that includes the liquid wealth. We analyze the properties of the\nvalue function and divide the solvency region into three regions: the buying\nregion, the no-trading region, and the selling region, and prove that all three\nregions are non-empty. We mathematically characterize and numerically solve the\noptimal policy and prove its optimality. Our numerical analysis sheds light on\nthe impact of various parameters on the optimal policy, and some intuition and\neconomic insights behind it are also analyzed. We find that liquidity\npreference encourages agents to retain more liquid wealth and inhibits\nconsumption, and may even result in a negative allocation to the illiquid\nasset. The liquid risky asset not only affects the location of the three\nregions but also has an impact on consumption. However, whether this impact on\nconsumption is promoted or inhibited depends on the degree of risk aversion of\nagents.",
      "generated_abstract": "aper, we propose a new approach to model the price dynamics of\ninterest rate derivatives based on a modified stochastic differential equation\n(SDE). In the proposed model, we introduce a new parameter, which is\nintroduced as a function of the logarithm of the underlying asset price. We\nshow that the parameter is a Gaussian process and can be integrated using the\nstochastic differential equation. We show that the price dynamics of the new\nmodel can be expressed as a mean-field Gaussian process. We prove that the\nprice dynamics of the new model converges to the price dynamics of the\noriginal model as the number of assets in the model grows. We prove that the\nprice dynamics of the new model converges to the price dynamics of the original\nmodel in the case of the existence of the parameters of the new model. We also\nshow that the price dynamics of the new model can be approximated by the price\ndynamics of the original",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13793103448275862,
          "p": 0.26229508196721313,
          "f": 0.18079095593475705
        },
        "rouge-2": {
          "r": 0.017543859649122806,
          "p": 0.030612244897959183,
          "f": 0.022304828081978822
        },
        "rouge-l": {
          "r": 0.1206896551724138,
          "p": 0.22950819672131148,
          "f": 0.1581920858782599
        }
      }
    },
    {
      "paper_id": "cs.AI.q-fin/CP/2501.05278v1",
      "true_abstract": "Counterfactual estimators are critical for learning and refining policies\nusing logged data, a process known as Off-Policy Evaluation (OPE). OPE allows\nresearchers to assess new policies without costly experiments, speeding up the\nevaluation process. Online experimental methods, such as A/B tests, are\neffective but often slow, thus delaying the policy selection and optimization\nprocess.\n  In this work, we explore the application of OPE methods in the context of\nresource allocation in dynamic auction environments. Given the competitive\nnature of environments where rapid decision-making is crucial for gaining a\ncompetitive edge, the ability to quickly and accurately assess algorithmic\nperformance is essential. By utilizing counterfactual estimators as a\npreliminary step before conducting A/B tests, we aim to streamline the\nevaluation process, reduce the time and resources required for experimentation,\nand enhance confidence in the chosen policies. Our investigation focuses on the\nfeasibility and effectiveness of using these estimators to predict the outcomes\nof potential resource allocation strategies, evaluate their performance, and\nfacilitate more informed decision-making in policy selection. Motivated by the\noutcomes of our initial study, we envision an advanced analytics system\ndesigned to seamlessly and dynamically assess new resource allocation\nstrategies and policies.",
      "generated_abstract": "y introduces a framework for multi-agent reinforcement learning\n(MARL) with embedded agents in an evolutionary game, which incorporates\nreinforcement learning agents, evolutionary strategies, and multi-agent\nreinforcement learning (MARL). The proposed framework is based on a\ngeneral-purpose evolutionary game, which is defined by an evolutionary\nstrategy and an evolutionary game. The evolutionary game includes\nevolutionary strategies that are used by evolutionary strategies to solve\nevolutionary games. The evolutionary strategy is defined by a population of\nreinforcement learning agents, which solve the evolutionary game, thereby\nproducing a population of evolutionary strategies. The evolutionary game and\nthe evolutionary strategy are modeled using the general-purpose evolutionary\ngame. The evolutionary strategy and the population of reinforcement learning\nagents are modeled using an evolutionary",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12598425196850394,
          "p": 0.34782608695652173,
          "f": 0.18497109436199013
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12598425196850394,
          "p": 0.34782608695652173,
          "f": 0.18497109436199013
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/GN/2412.11084v1",
      "true_abstract": "DNA barcodes are crucial in biodiversity analysis for building automatic\nidentification systems that recognize known species and discover unseen\nspecies. Unlike human genome modeling, barcode-based invertebrate\nidentification poses challenges in the vast diversity of species and taxonomic\ncomplexity. Among Transformer-based foundation models, BarcodeBERT excelled in\nspecies-level identification of invertebrates, highlighting the effectiveness\nof self-supervised pretraining on barcode-specific datasets. Recently,\nstructured state space models (SSMs) have emerged, with a time complexity that\nscales sub-quadratically with the context length. SSMs provide an efficient\nparameterization of sequence modeling relative to attention-based\narchitectures. Given the success of Mamba and Mamba-2 in natural language, we\ndesigned BarcodeMamba, a performant and efficient foundation model for DNA\nbarcodes in biodiversity analysis. We conducted a comprehensive ablation study\non the impacts of self-supervised training and tokenization methods, and\ncompared both versions of Mamba layers in terms of expressiveness and their\ncapacity to identify \"unseen\" species held back from training. Our study shows\nthat BarcodeMamba has better performance than BarcodeBERT even when using only\n8.3% as many parameters, and improves accuracy to 99.2% on species-level\naccuracy in linear probing without fine-tuning for \"seen\" species. In our\nscaling study, BarcodeMamba with 63.6% of BarcodeBERT's parameters achieved\n70.2% genus-level accuracy in 1-nearest neighbor (1-NN) probing for unseen\nspecies. The code repository to reproduce our experiments is available at\nhttps://github.com/bioscan-ml/BarcodeMamba.",
      "generated_abstract": "ence of AI-driven technologies has significantly transformed how\nwe work, play, and interact. This shift has led to a significant increase in\nthe amount of data generated, requiring the development of new tools to\neffectively manage and exploit it. One critical component of these tools is\nthe ability to model complex systems through the use of graph neural networks\n(GNNs). These networks have been successfully applied to a variety of\ndomains, including biology, finance, and healthcare. However, the application\nof GNNs to social science data has been limited due to the complex and\nmulti-scale nature of these datasets. In this paper, we propose a novel\nframework that integrates graph neural networks with the concept of\nco-occurrence matrices to analyze social science data. Our framework enables\nthe modeling of complex social networks using GNNs, which are known for their\nability to capture complex relationships. By integr",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14743589743589744,
          "p": 0.23958333333333334,
          "f": 0.18253967782312935
        },
        "rouge-2": {
          "r": 0.009345794392523364,
          "p": 0.014598540145985401,
          "f": 0.01139600663663643
        },
        "rouge-l": {
          "r": 0.14743589743589744,
          "p": 0.23958333333333334,
          "f": 0.18253967782312935
        }
      }
    },
    {
      "paper_id": "cs.LG.physics/ao-ph/2503.03038v1",
      "true_abstract": "Machine learning models have shown great success in predicting weather up to\ntwo weeks ahead, outperforming process-based benchmarks. However, existing\napproaches mostly focus on the prediction task, and do not incorporate the\nnecessary data assimilation. Moreover, these models suffer from error\naccumulation in long roll-outs, limiting their applicability to seasonal\npredictions or climate projections. Here, we introduce Generative Assimilation\nand Prediction (GAP), a unified deep generative framework for assimilation and\nprediction of both weather and climate. By learning to quantify the\nprobabilistic distribution of atmospheric states under observational,\npredictive, and external forcing constraints, GAP excels in a broad range of\nweather-climate related tasks, including data assimilation, seamless\nprediction, and climate simulation. In particular, GAP is competitive with\nstate-of-the-art ensemble assimilation, probabilistic weather forecast and\nseasonal prediction, yields stable millennial simulations, and reproduces\nclimate variability from daily to decadal time scales.",
      "generated_abstract": "ence of large language models (LLMs) has revolutionized text\ninference, providing unprecedented access to knowledge in large domains.\nHowever, LLMs are limited by the availability of high-quality training data,\nwhich can be expensive to collect. In this paper, we propose a novel data\ncollection strategy that addresses this limitation by leveraging the\ncomprehensive knowledge base of a large language model (LLM). Our approach\nemploys a large language model to generate questions about the model's\nknowledge, which are then answered by the LLM. We demonstrate that this method\ncan significantly improve the quality of textual data, particularly for\nunseen entities. The results indicate that our approach significantly improves\ntextual data collection efficiency while maintaining high-quality annotations,\nenabling the generation of large-scale, high-quality textual data for LLMs.\nThis work paves the way for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.102803738317757,
          "p": 0.125,
          "f": 0.1128205078679818
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.102803738317757,
          "p": 0.125,
          "f": 0.1128205078679818
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PR/2411.05425v1",
      "true_abstract": "This article presents a generic hybrid numerical method to price a wide range\nof options on one or several assets, as well as assets with stochastic drift or\nvolatility. In particular for equity and interest rate hybrid with local\nvolatility.",
      "generated_abstract": "r presents an approach to the problem of pricing and hedging options\nin an environment with a continuous market with multiple investors and multiple\nassets. The paper introduces a new class of options, called hybrid options,\nwhich are a combination of a call and a put option. The hybrid option allows\nfor the possibility of simultaneous exercise of both options. The hybrid\noptions are priced using the method of mean-variance optimization. The\nassumptions of the model are based on the model of a continuous market with\nmultiple investors and multiple assets. The paper is structured as follows. The\nhybrid options are introduced in Section 2. The model is described in Section\n3. The method of mean-variance optimization is described in Section 4. The\nresults of the model are presented in Section 5. The results of the model are\npresented in Section 6. The paper is concluded in Section",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.38235294117647056,
          "p": 0.2,
          "f": 0.26262625811651885
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.3235294117647059,
          "p": 0.16923076923076924,
          "f": 0.22222221771247844
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ph/2503.10343v1",
      "true_abstract": "We present a comprehensive reappraisal of the in-medium properties of the rho\nmeson using the inverse QCD sum rules (QCDSR) formalism, offering a novel,\nmodel-independent approach to studying hadronic modifications in nuclear\nmatter. Unlike conventional QCDSR, which rely on a predefined pole+continuum\nstructure, the inverse method reconstructs the spectral function directly from\nthe operator product expansion (OPE), eliminating assumptions about the\nspectral ansatz. To the best of our knowledge, this is the first application of\nthe inverse QCDSR method to the rho meson in nuclear matter. Our analysis\nreveals a significant reduction in the rho meson mass, consistent with previous\ntheoretical predictions, and highlights the crucial role of medium-induced\nmodifications, including condensate suppression and factorization-breaking\neffects. Furthermore, we assess the sensitivity of our results to the\nfactorization assumption and higher-dimensional condensates, demonstrating the\nnecessity of refining nonperturbative contributions for an accurate description\nof in-medium hadron properties. Our findings establish the inverse QCDSR method\nas a robust alternative to conventional spectral analysis techniques, providing\na systematically controlled framework for exploring strongly interacting matter\nunder extreme conditions. These results offer important theoretical benchmarks\nfor lattice QCD simulations and heavy-ion collision experiments, shedding light\non the restoration of chiral symmetry and the evolution of hadronic matter in\ndense environments.",
      "generated_abstract": "igate the phenomenological implications of a Higgs sector with a\nstrictly chiral $U(1)$ symmetry. This symmetry, which is spontaneously\nbroken by the Higgs vev, leads to the appearance of a chiral anomaly,\ngenerating a mass for the Goldstone bosons. We study the effects of this\nanomaly on the Higgs sector. We find that, in the presence of a large mass for\nthe Goldstone bosons, a heavy Higgs boson can be produced in the first-diboson\nprocess at the LHC. We also examine the effect of the chiral anomaly on the\ndecay width of the heavy Higgs boson, as well as on the Higgs boson\nself-couplings to other particles. The decay width is suppressed by a\nfactor of $\\sqrt{m_{\\rm Higgs}^2",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11851851851851852,
          "p": 0.23880597014925373,
          "f": 0.15841583715076968
        },
        "rouge-2": {
          "r": 0.021164021164021163,
          "p": 0.039603960396039604,
          "f": 0.02758620235695675
        },
        "rouge-l": {
          "r": 0.1037037037037037,
          "p": 0.208955223880597,
          "f": 0.1386138569527499
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.16352v1",
      "true_abstract": "We present a design-based model of a randomized experiment in which the\nobserved outcomes are informative about the joint distribution of potential\noutcomes within the experimental sample. We derive a likelihood function that\nmaintains curvature with respect to the joint distribution of potential\noutcomes, even when holding the marginal distributions of potential outcomes\nconstant -- curvature that is not maintained in a sampling-based likelihood\nthat imposes a large sample assumption. Our proposed decision rule guesses the\njoint distribution of potential outcomes in the sample as the distribution that\nmaximizes the likelihood. We show that this decision rule is Bayes optimal\nunder a uniform prior. Our optimal decision rule differs from and significantly\noutperforms a ``monotonicity'' decision rule that assumes no defiers or no\ncompliers. In sample sizes ranging from 2 to 40, we show that the Bayes\nexpected utility of the optimal rule increases relative to the monotonicity\nrule as the sample size increases. In two experiments in health care, we show\nthat the joint distribution of potential outcomes that maximizes the likelihood\nneed not include compliers even when the average outcome in the intervention\ngroup exceeds the average outcome in the control group, and that the maximizer\nof the likelihood may include both compliers and defiers, even when the average\nintervention effect is large and statistically significant.",
      "generated_abstract": "f statistical models in economic policy making is increasingly\nunder scrutiny due to their potential for distorting policy outcomes. This\npaper provides a general framework for evaluating the performance of\nstatistical models in policy making. We consider a model with two components,\nthe error term and the regression coefficients, and show that a model that\nrepresents the error term as a vector autoregression of the regression\ncoefficients and a constant term is consistent under a wide class of\nconditions. We develop a practical method for evaluating the performance of\nthe model using a sample of cross-section data and a test statistic based on\nthe empirical correlation between the model's residuals and the sample\ncoefficients. The proposed approach is applied to a number of recent and\nclassical policy issues, including the effect of taxes on consumption, the\neffect of monetary policy on output, and the effects of fis",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20388349514563106,
          "p": 0.25301204819277107,
          "f": 0.22580644667071348
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.031007751937984496,
          "f": 0.026229503315453744
        },
        "rouge-l": {
          "r": 0.1941747572815534,
          "p": 0.24096385542168675,
          "f": 0.21505375849867048
        }
      }
    },
    {
      "paper_id": "q-bio.SC.q-bio/SC/2402.10638v2",
      "true_abstract": "During cell division, the mitotic spindle moves dynamically through the cell\nto position the chromosomes and determine the ultimate spatial position of the\ntwo daughter cells. These movements have been attributed to the action of\ncortical force generators which pull on the astral microtubules to position the\nspindle, as well as pushing events by these same microtubules against the cell\ncortex and plasma membrane. Attachment and detachment of cortical force\ngenerators working antagonistically against centring forces of microtubules\nhave been modelled previously (Grill et al. 2005, Phys. Rev. Lett. 94:108104)\nvia stochastic simulations and mean-field Fokker-Planck equations (describing\nrandom motion of force generators) to predict oscillations of a spindle pole in\none spatial dimension. Using systematic asymptotic methods, we reduce the\nFokker-Planck system to a set of ordinary differential equations (ODEs),\nconsistent with a set proposed by Grill et al., which can provide accurate\npredictions of the conditions for the Fokker-Planck system to exhibit\noscillations. In the limit of small restoring forces, we derive an algebraic\nprediction of the amplitude of spindle-pole oscillations and demonstrate the\nrelaxation structure of nonlinear oscillations. We also show how noise-induced\noscillations can arise in stochastic simulations for conditions in which the\nmean-field Fokker-Planck system predicts stability, but for which the period\ncan be estimated directly by the ODE model and the amplitude by a related\nstochastic differential equation that incorporates random binding kinetics.",
      "generated_abstract": "ork, we present a novel approach to the study of microbial\nsequencing data, using a novel formulation of the inverse problem in the\ncontext of sequencing-based metagenomic studies. We describe a novel\nmethodology that integrates information from the original sequencing data\nwith information from previously published metagenomic datasets, allowing us to\nreconstruct the sequence space of the original samples. Our approach allows us\nto reconstruct the sequence space of the original samples, which we term the\n\"original sequence space\", and it provides an avenue for studying the\nsequencing data by reconstructing the sequence space of the sequencing data.\nUsing this approach, we reconstruct the sequence space of the original\nsequencing data, which allows us to explore the evolutionary relationships\nbetween the original sequences. We then use this sequence space to explore the\nevolutionary relationships between the original sequences. We demonstrate the\npower of this approach in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12318840579710146,
          "p": 0.2463768115942029,
          "f": 0.16425120328502427
        },
        "rouge-2": {
          "r": 0.014285714285714285,
          "p": 0.028846153846153848,
          "f": 0.019108275824578087
        },
        "rouge-l": {
          "r": 0.10144927536231885,
          "p": 0.2028985507246377,
          "f": 0.1352656960386475
        }
      }
    },
    {
      "paper_id": "q-fin.PM.econ/EM/2502.13461v1",
      "true_abstract": "Style investing creates asset classes (or the so-called \"styles\") with low\ncorrelations, aligning well with the principle of \"Holy Grail of investing\" in\nterms of portfolio selection. The returns of styles naturally form a\ntensor-valued time series, which requires new tools for studying the dynamics\nof the conditional correlation matrix to facilitate the aforementioned\nprinciple. Towards this goal, we introduce a new tensor dynamic conditional\ncorrelation (TDCC) model, which is based on two novel treatments:\ntrace-normalization and dimension-normalization. These two normalizations adapt\nto the tensor nature of the data, and they are necessary except when the tensor\ndata reduce to vector data. Moreover, we provide an easy-to-implement\nestimation procedure for the TDCC model, and examine its finite sample\nperformance by simulations. Finally, we assess the usefulness of the TDCC model\nin international portfolio selection across ten global markets and in large\nportfolio selection for 1800 stocks from the Chinese stock market.",
      "generated_abstract": "We propose a novel and simple approach to portfolio optimization that\nassesses risk-adjusted returns across multiple assets. In particular, we\ndemonstrate how to efficiently implement an optimal investment strategy\nbased on an empirical risk measure, where the risk measure is determined by\nempirical returns of each asset. We introduce a novel approach to constructing\nthe empirical risk measure that reduces the computational complexity of the\noptimization problem. Our empirical risk measure is based on the Sharpe ratio\nand the excess return of the portfolio. We show how to compute the Sharpe ratio\nand the excess return of the portfolio in a computationally efficient way. We\nalso show how to implement the optimal investment strategy using a simple\nnumerical optimization algorithm. Our empirical results demonstrate that our\napproach can produce optimal portfolio strategies that outperform those based\non traditional risk measures.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16981132075471697,
          "p": 0.25,
          "f": 0.20224718619366255
        },
        "rouge-2": {
          "r": 0.03496503496503497,
          "p": 0.043859649122807015,
          "f": 0.03891050090024136
        },
        "rouge-l": {
          "r": 0.14150943396226415,
          "p": 0.20833333333333334,
          "f": 0.16853932102512323
        }
      }
    },
    {
      "paper_id": "physics.plasm-ph.physics/plasm-ph/2503.06176v1",
      "true_abstract": "In strictly axisymmetric configurations of tokamaks, field-line tracing\nreduces from a three-dimensional ODE system to a two-dimensional one, where\nPoincar\\'e-Bendixson theorem applies and guarantees the nonexistence of chaos.\nThe formulae of functional perturbation theory (FPT) mostly simplify to compact\nclosed-form expressions to allow the computation to finish instantly, which\ncould improve and accelerate the existing plasma control systems by detangling\nthe plasma dynamics from the magnetic topology change. FPT can conveniently\ncalculate how the key geometric objects of magnetic topology:\n  1. the divertor X-point(s) and the magnetic axis,\n  2. the last closed flux surface (LCFS)\n  3. flux surfaces\n  change under perturbation. For example, when the divertor X-point shifts\noutwards, the LCFS there must expand accordingly, but not necessarily for other\nplaces of the LCFS, which could also contract, depending on the perturbation.\nFPT can not only facilitate adaptive control of plasma, but also enable\nutilizing as much as possible space in the vacuum vessel by weakening the\nplasma-wall interaction (PWI) via tuning the eigenvalues of $\\mathcal{DP}^m$ of\nthe divertor X-point(s), such that the field line connection lengths in the\nscrape-off layer (SOL) are long enough to achieve detachment. Increasing flux\nexpansion $f_x$ is another option for detachment and can also be facilitated by\nFPT.\n  Apart from the edge, FPT can also benefit the understanding of the plasma\ncore. Since the magnetic axis O-point would also shift under perturbation and\nthe shift is known by FPT, the O-point can be controlled without full knowledge\nof the plasma response, which shall not significantly change the tendency.",
      "generated_abstract": "We present a new approach to investigate the dynamics of a charged\ncoherent plasma under the influence of magnetic fields in a periodic\nlaboratory setting. The plasma is subject to a constant magnetic field and\ninteractions with a plasma-sidewall electrode, which can either be in a\nperpendicular or parallel configuration to the magnetic field. A new technique\nis introduced to observe the dynamics of the plasma, and the experimental\nresults are compared to the predictions of a theoretical model based on\nclassical electrostatics. The findings show that the theory is in excellent\nagreement with the observations, and it is shown that the observed effects can\nbe explained by the existence of a resonance between the magnetic field and the\ncharge distribution in the plasma.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13496932515337423,
          "p": 0.3013698630136986,
          "f": 0.1864406736932635
        },
        "rouge-2": {
          "r": 0.038135593220338986,
          "p": 0.08108108108108109,
          "f": 0.05187319449609285
        },
        "rouge-l": {
          "r": 0.1165644171779141,
          "p": 0.2602739726027397,
          "f": 0.1610169448797042
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.16694v1",
      "true_abstract": "When is it beneficial to constrain creativity? Creativity thrives with\nfreedom, but when people collaborate to create artifacts, there is tension\nbetween giving individuals freedom to revise, and protecting prior\nachievements. To test how imposing constraints may affect collective\ncreativity, we performed cultural evolution experiments where participants\ncollaborated to create melodies and images in chains. With melodies, we found\nthat limiting step size (number of musical notes that can be changed) improved\npleasantness ratings for created tunes. Similar results were observed in\ncohorts of musicians, and with different selection regimes. In contrast,\nlimiting step size in creating images consistently reduced pleasantness. These\nconflicting findings suggest that in domains such as music, where artifacts can\nbe easily damaged, and where evolutionary outcomes are hard to foresee,\ncollective creativity may benefit from imposing small step sizes. We discuss\nparallels with search algorithms and the evolution of conservative birdsong\ncultures.",
      "generated_abstract": "the COVID-19 pandemic caused a massive global health crisis. Although\nmany advancements in vaccine development have been made, there is still a\nsignificant lack of access to vaccines for the most vulnerable. This lack of\nvaccine access has been attributed to the high cost of vaccine procurement and\ndelivery. Additionally, there is a high demand for vaccine doses, which can\nbecome a bottleneck for supply. In this paper, we propose a novel mechanism,\nthe Vaccine Equity Mechanism (VEM), which aims to reduce the cost of vaccine\nprocurement and delivery while also ensuring that the most vulnerable groups\nhave access to vaccines. We introduce a novel approach to VEM that balances\nequity and efficiency by using a multi-period, multi-stage, and multi-object",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11504424778761062,
          "p": 0.17105263157894737,
          "f": 0.1375661327577617
        },
        "rouge-2": {
          "r": 0.007042253521126761,
          "p": 0.009433962264150943,
          "f": 0.00806451123439423
        },
        "rouge-l": {
          "r": 0.11504424778761062,
          "p": 0.17105263157894737,
          "f": 0.1375661327577617
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2503.00455v1",
      "true_abstract": "Existing Existing automatic audio generation methods struggle to generate\npodcast-like audio programs effectively. The key challenges lie in in-depth\ncontent generation, appropriate and expressive voice production. This paper\nproposed PodAgent, a comprehensive framework for creating audio programs.\nPodAgent 1) generates informative topic-discussion content by designing a\nHost-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for\nsuitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis\nmethod to generate expressive conversational speech. Given the absence of\nstandardized evaluation criteria for podcast-like audio generation, we\ndeveloped comprehensive assessment guidelines to effectively evaluate the\nmodel's performance. Experimental results demonstrate PodAgent's effectiveness,\nsignificantly surpassing direct GPT-4 generation in topic-discussion dialogue\ncontent, achieving an 87.4% voice-matching accuracy, and producing more\nexpressive speech through LLM-guided synthesis. Demo page:\nhttps://podcast-agent.github.io/demo/. Source code:\nhttps://github.com/yujxx/PodAgent.",
      "generated_abstract": "ning-based methods have been widely used for audio event detection\nin speech signals, but they often require extensive labeled data, which is\ndifficult to obtain in real-world applications due to the high costs of\ncollecting. To address this problem, we propose a novel approach for audio\nevent detection that utilizes the temporal dynamics of audio signals. Specifically,\nwe introduce a novel multi-scale event detection model, called the Audio Event\nDetection Network (AEDNet), which leverages the temporal dynamics of audio\nsignals. By integrating the multi-scale information of audio signals, AEDNet\neffectively captures the complex temporal dynamics of audio signals, enabling\nthe network to detect audio events more accurately. Additionally, AEDNet\nintegrates an attention mechanism into the multi-scale event detection model,\nwhich enhances the model's ability to understand the temporal dynamics of\naudio signals. Experiments on both synthetic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1485148514851485,
          "p": 0.17647058823529413,
          "f": 0.1612903176176438
        },
        "rouge-2": {
          "r": 0.007936507936507936,
          "p": 0.009009009009009009,
          "f": 0.008438813585432624
        },
        "rouge-l": {
          "r": 0.13861386138613863,
          "p": 0.16470588235294117,
          "f": 0.15053762944560084
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10316v1",
      "true_abstract": "In this paper, we consider a tunable liquid convex lens-assisted imaging\nreceiver for indoor multiple-input multiple-output (MIMO) visible light\ncommunication (VLC) systems. In contrast to existing MIMO VLC receivers that\nrely on fixed optical lenses, the proposed receiver leverages the additional\ndegrees of freedom offered by liquid lenses via adjusting both focal length and\norientation angles of the lens. This capability facilitates the mitigation of\nspatial correlation between the channel gains, thereby enhancing the overall\nsignal quality and leading to improved bit-error rate (BER) performance. We\npresent an accurate channel model for the liquid lens-assisted VLC system by\nusing three-dimensional geometry and geometric optics. To achieve optimal\nperformance under practical conditions such as random receiver orientation and\nuser mobility, optimization of both focal length and orientation angles of the\nlens are required. To this end, driven by the fact that channel models are\nmathematically complex, we present two optimization schemes including a\nblockwise machine learning (ML) architecture that includes convolution layers\nto extract spatial features from the received signal, long-short term memory\nlayers to predict the user position and orientation, and fully connected layers\nto estimate the optimal lens parameters. Numerical results are presented to\ncompare the performance of each scheme with conventional receivers. Results\nshow that a significant BER improvement is achieved when liquid lenses and\npresented ML-based optimization approaches are used. Specifically, the BER can\nbe improved from $6\\times 10^{-2}$ to $1.4\\times 10^{-3}$ at an average\nsignal-to-noise ratio of $30$ dB.",
      "generated_abstract": "r presents a novel hybrid approach to enhancing the performance of\nmixed-signal (MS) digital-to-analog (DAC) converters for high-speed digital\nsignals (HSDS) in wireless communications. Traditional DAC converters in\nHSDS systems have limited dynamic range and bandwidth, resulting in a\nreduction in the bit resolution and signal quality. The proposed methodology\naddresses these limitations by integrating an analog-to-digital (A/D) converter\nwith a digital-to-analog converter (DAC) hybrid architecture. By incorporating\nthe A/D converter, the converter performance is enhanced by improving the\nbandwidth and dynamic range of the DAC converter. Furthermore, the proposed\nmethodology enhances the bit resolution of the DAC converter by combining it\nwith a digital signal processor (DSP) to perform additional processing operations\nsuch as",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12269938650306748,
          "p": 0.2702702702702703,
          "f": 0.1687763670131212
        },
        "rouge-2": {
          "r": 0.03017241379310345,
          "p": 0.0660377358490566,
          "f": 0.04142011403802433
        },
        "rouge-l": {
          "r": 0.10429447852760736,
          "p": 0.22972972972972974,
          "f": 0.1434599113169187
        }
      }
    },
    {
      "paper_id": "math.ST.stat/CO/2502.03849v1",
      "true_abstract": "This paper presents a new algorithm (and an additional trick) that allows to\ncompute fastly an entire curve of post hoc bounds for the False Discovery\nProportion when the underlying bound $V^*_{\\mathfrak{R}}$ construction is based\non a reference family $\\mathfrak{R}$ with a forest structure {\\`a} la Durand et\nal. (2020). By an entire curve, we mean the values\n$V^*_{\\mathfrak{R}}(S_1),\\dotsc,V^*_{\\mathfrak{R}}(S_m)$ computed on a path of\nincreasing selection sets $S_1\\subsetneq\\dotsb\\subsetneq S_m$, $|S_t|=t$. The\nnew algorithm leverages the fact that going from $S_t$ to $S_{t+1}$ is done by\nadding only one hypothesis.",
      "generated_abstract": "the statistical problem of estimating the conditional expectation\n(mean) of a function of two random variables given a third random variable. In\nthis setting, we propose a novel estimator of the conditional expectation\n(mean) that has the advantage of being both consistent and asymptotically\nnormal. This estimator has an explicit expression and is based on a novel\nrepresentation of the conditional expectation. We show that this estimator\nconsistently and asymptotically approximates the conditional expectation of the\nfunction of the random variables given the third random variable. Furthermore,\nwe prove that this estimator is asymptotically normal and that its asymptotic\nvariance is equal to the variance of the estimator of the conditional\nexpectation. Finally, we derive an approximation of the conditional expectation\nof the function of the random variables given the third random variable by a\nweighted sum of the estimator of the conditional expectation and a linear\ncombination of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1643835616438356,
          "p": 0.2033898305084746,
          "f": 0.18181817687442622
        },
        "rouge-2": {
          "r": 0.03529411764705882,
          "p": 0.029411764705882353,
          "f": 0.03208555653864929
        },
        "rouge-l": {
          "r": 0.1643835616438356,
          "p": 0.2033898305084746,
          "f": 0.18181817687442622
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.09848v1",
      "true_abstract": "In this work, we present a second-order numerical scheme to address the\nsolution of optimal control problems constrained by the evolution of nonlinear\nFokker-Planck equations arising from socio-economic dynamics. In order to\ndesign an appropriate numerical scheme for control realization, a coupled\nforward-backward system is derived based on the associated optimality\nconditions. The forward equation, corresponding to the Fokker-Planck dynamics,\nis discretized using a structure preserving scheme able to capture steady\nstates. On the other hand, the backward equation, modeled as a\nHamilton-Jacobi-Bellman problem, is solved via a semi-Lagrangian scheme that\nsupports large time steps while preserving stability. Coupling between the\nforward and backward problems is achieved through a gradient descent\noptimization strategy, ensuring convergence to the optimal control. Numerical\nexperiments demonstrate second-order accuracy, computational efficiency, and\neffectiveness in controlling different examples across various scenarios in\nsocial dynamics. This approach provides a reliable computational tool for the\nstudy of opinion manipulation and consensus formation in socially structured\nsystems.",
      "generated_abstract": "the problem of finding the maximum of a function over a domain\nwith two boundary conditions. The problem is formulated in terms of a\ngeneralized Nevanlinna-Pick problem, which is a generalized version of the\nproblem of finding the maximum of a function over a domain with two boundary\nconditions. The solution of the problem is expressed as a solution of a\nspecialized Nevanlinna-Pick problem. We prove that the solutions of the\nspecialized Nevanlinna-Pick problem are determined by the solutions of the\ngeneralized Nevanlinna-Pick problem. We also show that the solutions of the\nspecialized Nevanlinna-Pick problem are determined by the solutions of the\ngeneralized Nevanlinna-Pick problem with a fixed value of the parameter in the\nNevanlinna-Pick problem. The solutions of the specialized Nevanlinna-Pick\nproblem are",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10619469026548672,
          "p": 0.3076923076923077,
          "f": 0.15789473302718152
        },
        "rouge-2": {
          "r": 0.025974025974025976,
          "p": 0.06060606060606061,
          "f": 0.03636363216363685
        },
        "rouge-l": {
          "r": 0.09734513274336283,
          "p": 0.28205128205128205,
          "f": 0.14473683829033945
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.12309v1",
      "true_abstract": "Square matrices often arise in microeconomics, particularly in network models\naddressing applications from opinion dynamics to platform regulation. Spectral\ntheory provides powerful tools for analyzing their properties. We present an\naccessible overview of several fundamental applications of spectral methods in\nmicroeconomics, focusing especially on the Perron-Frobenius Theorem's role and\nits connection to centrality measures. Applications include social learning,\nnetwork games, public goods provision, and market intervention under\nuncertainty. The exposition assumes minimal social science background, using\nspectral theory as a unifying mathematical thread to introduce interested\nreaders to some exciting current topics in microeconomic theory.",
      "generated_abstract": "e a game-theoretic approach to identify the optimal design of\ntreatments in randomized controlled trials (RCTs). We show that the optimal\ntreatment design is the median of the optimal treatment design across all\ncombinations of treatment and covariate assignment. This is the first time that\nthe median of the optimal treatment design has been identified as the optimal\ndesign in RCTs. We formalize this notion of the median of the optimal\ntreatment design as the median of the optimal treatment design across all\ncombinations of treatment and covariate assignment. We also propose an\nalgorithm for identifying the median of the optimal treatment design for RCTs.\nWe demonstrate the practical feasibility of our approach by applying it to a\ndata set on the effects of a treatment on the incidence of heart attacks in\nthe United States. Our analysis suggests that a median of the optimal\ntreatment design is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1375,
          "p": 0.171875,
          "f": 0.15277777283950636
        },
        "rouge-2": {
          "r": 0.010752688172043012,
          "p": 0.010309278350515464,
          "f": 0.010526310791692122
        },
        "rouge-l": {
          "r": 0.1375,
          "p": 0.171875,
          "f": 0.15277777283950636
        }
      }
    },
    {
      "paper_id": "math.GM.math/GM/2503.07406v1",
      "true_abstract": "This paper presents a distinctive prime detection approach. This method use\nGM-(n+1) sequences to effectively eliminate complex numbers. The sequences,\nwhich consist of odd a number of (n+1), exclude all components except for the\ninitial prime integer. Only the first prime number is presented. This research\nproposes an approach using this model to identify exceptional candidates and\nexamine their distribution. This study examines the interconnections among the\nlaws of division, basic gaps, and their applications in analytical procedures.\nComputer studies may provide a novel perspective on the theory of prime\nnumbers, demonstrating the effectiveness of this approach in refining the\nsearch space for primes.",
      "generated_abstract": "that the number of points in the curve class of a general quartic\nsans-serif curve is at most $\\frac{2}{3}\\pi^2$ times the number of points in the\ncurve class of a general cubic surface. This number is an explicit upper\nbound for the number of points in the curve class of a general quartic\nsans-serif curve. Our proof is based on the existence of a special genus-1\ncurve class that contains many points. We prove that this special genus-1 curve\nclass has a very special form. We use this form to show that the number of\npoints in the curve class of a general quartic sans-serif curve is at most\n$\\frac{2}{3}\\pi^2$ times the number of points in the curve class of a general\ncubic surface. This number is an explicit upper bound for the number of points\nin the curve class",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16883116883116883,
          "p": 0.3023255813953488,
          "f": 0.21666666206805565
        },
        "rouge-2": {
          "r": 0.038834951456310676,
          "p": 0.06349206349206349,
          "f": 0.04819276637465571
        },
        "rouge-l": {
          "r": 0.16883116883116883,
          "p": 0.3023255813953488,
          "f": 0.21666666206805565
        }
      }
    },
    {
      "paper_id": "physics.class-ph.physics/class-ph/2503.04206v1",
      "true_abstract": "The refraction of light by dispersion-free dielectric media can be modeled\nusing well-localized macroscopic wave packets, enabling a description in terms\nof pseudo-particles. This approach is often used in thought experiments to\nillustrate aspects of the Abraham-Minkowski debate. This work uses the particle\npicture to show at an elementary level how different types of momenta come into\nplay, and how light refraction can be explained at the level of particles. A\nspecial exactly solvable microscopic model is used to illustrate the interplay\nand tension between microscopic physics and the conventional effective-medium\nMaxwell equations.",
      "generated_abstract": "We present a novel method for quantifying the phase and amplitude of\na single-mode, time-varying optical field in a time-periodic medium. This\nmethod is based on the analysis of the spectral decomposition of the\ntime-periodic Fourier transform of the field, which is obtained by combining\nthe Fourier transform of the field and the spectral decomposition of the\nperiodic medium. We show that the method is applicable to a wide range of\ntime-periodic media, including those with non-trivial dispersion, such as\ncrystals, fibers, and microstructures. This method is particularly useful for\nstudying the phase and amplitude modulation of light in various time-periodic\nmedia, as well as for characterizing the non-trivial dispersion in these\nmedia.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17647058823529413,
          "p": 0.1935483870967742,
          "f": 0.18461537962603566
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.03125,
          "f": 0.032258059521332726
        },
        "rouge-l": {
          "r": 0.14705882352941177,
          "p": 0.16129032258064516,
          "f": 0.1538461488568049
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/CV/2503.10629v1",
      "true_abstract": "Adversarial attacks pose significant challenges for vision models in critical\nfields like healthcare, where reliability is essential. Although adversarial\ntraining has been well studied in natural images, its application to biomedical\nand microscopy data remains limited. Existing self-supervised adversarial\ntraining methods overlook the hierarchical structure of histopathology images,\nwhere patient-slide-patch relationships provide valuable discriminative\nsignals. To address this, we propose Hierarchical Self-Supervised Adversarial\nTraining (HSAT), which exploits these properties to craft adversarial examples\nusing multi-level contrastive learning and integrate it into adversarial\ntraining for enhanced robustness. We evaluate HSAT on multiclass histopathology\ndataset OpenSRH and the results show that HSAT outperforms existing methods\nfrom both biomedical and natural image domains. HSAT enhances robustness,\nachieving an average gain of 54.31% in the white-box setting and reducing\nperformance drops to 3-4% in the black-box setting, compared to 25-30% for the\nbaseline. These results set a new benchmark for adversarial training in this\ndomain, paving the way for more robust models. Our Code for training and\nevaluation is available at https://github.com/HashmatShadab/HSAT.",
      "generated_abstract": "guage models (LLMs) have recently demonstrated remarkable\naccuracy in video generation, often producing realistic videos that are\nconsistent with human perception. However, the LLMs' video generation still\nlacks interpretability, as they are based on a black-box model that produces\nvideo clips without any human-designed grounding information. In this paper,\nwe propose a new model, VideoGPT-4, which integrates a video-grounding network\n(VGNet) into the VideoGPT-4 model to enhance video generation interpretability.\nVGNet is designed to generate a grounding image for each video clip, which\nallows the VideoGPT-4 model to generate video clips with grounded information.\nExperiments on both synthetic and real video datasets demonstrate that\nVideoGPT-4 outperforms existing LLM-based video generators. Notably, the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16535433070866143,
          "p": 0.25609756097560976,
          "f": 0.2009569330308373
        },
        "rouge-2": {
          "r": 0.018292682926829267,
          "p": 0.02857142857142857,
          "f": 0.02230482795428578
        },
        "rouge-l": {
          "r": 0.14960629921259844,
          "p": 0.23170731707317074,
          "f": 0.1818181770499761
        }
      }
    },
    {
      "paper_id": "gr-qc.gr-qc/2503.10323v1",
      "true_abstract": "Einstein equations can be written in the so-called Fully Constrained\nFormulation (FCF). This formulation has two different sectors: the elliptic\nsector, formed by the Hamiltonian and Momentum constraints together with the\nequations derived from the gauge choice; and the hyperbolic sector, formed by\nthe evolution of the rest of the spacetime metric variables, which encodes the\ngravitational radiation. In this work, we present a modification of both\nsectors that keeps local uniqueness properties of the elliptic system of\nequations and includes a hierarchical post-Newtonian structure of all the\nelliptic and hyperbolic equations. This reformulation can have potential\napplications in cosmology and relativistic astrophysics. Moreover, we show how\ninitial stationary data can be computed numerically using this formulation\nwithout assuming a conformally flat spatial metric, with the illustrative\nexample of a rotating neutron star.",
      "generated_abstract": "of the dynamics of a non-relativistic particle subject to a\nnon-linear force in a curved background spacetime is a very active area of\nresearch. This study concerns a particle of mass $m$ and charge $q$ moving in a\nspacetime with an anisotropic non-linear gravitational potential $V(r)$, which\nis assumed to be an arbitrary function of the radial coordinate $r$. We\nconsider the case of a non-rotating particle in an asymptotically flat space\nwith a constant curvature $K$, with the assumption that $V(r)$ is\nquasi-isotropic. We derive the exact solution of the equations of motion for\nthe particle position and momentum, and we investigate the energy spectrum and\nthe energy-momentum tensor of the system. In addition, we compute the\ncharacteristic frequencies of the system and the corresponding eigenvectors,\nwhich are",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.22784810126582278,
          "f": 0.21301774650047278
        },
        "rouge-2": {
          "r": 0.04065040650406504,
          "p": 0.041666666666666664,
          "f": 0.04115225837524829
        },
        "rouge-l": {
          "r": 0.15555555555555556,
          "p": 0.17721518987341772,
          "f": 0.16568046839396394
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/NE/2503.05573v1",
      "true_abstract": "Model-based Reinforcement Learning (MBRL) has emerged as a promising paradigm\nfor autonomous driving, where data efficiency and robustness are critical. Yet,\nexisting solutions often rely on carefully crafted, task specific extrinsic\nrewards, limiting generalization to new tasks or environments. In this paper,\nwe propose InDRiVE (Intrinsic Disagreement based Reinforcement for Vehicle\nExploration), a method that leverages purely intrinsic, disagreement based\nrewards within a Dreamer based MBRL framework. By training an ensemble of world\nmodels, the agent actively explores high uncertainty regions of environments\nwithout any task specific feedback. This approach yields a task agnostic latent\nrepresentation, allowing for rapid zero shot or few shot fine tuning on\ndownstream driving tasks such as lane following and collision avoidance.\nExperimental results in both seen and unseen environments demonstrate that\nInDRiVE achieves higher success rates and fewer infractions compared to\nDreamerV2 and DreamerV3 baselines despite using significantly fewer training\nsteps. Our findings highlight the effectiveness of purely intrinsic exploration\nfor learning robust vehicle control behaviors, paving the way for more scalable\nand adaptable autonomous driving systems.",
      "generated_abstract": "r proposes a novel method for training an inverse kinematics (IK)\nmodel for a multi-finger robotic hand. By employing a deep learning-based\napproach, we introduce a novel IK learning method that allows for the\ngeneration of high-quality, real-time 3D-printed models of the hand for\nmanipulation. This approach enhances the accuracy and realism of the generated\nmodels, which are then used to train the IK model. By leveraging the\nrepresentative shapes of the 3D-printed models, we can reduce the complexity\nand computational overhead associated with training the IK model, enabling\nmore efficient training. We validate the effectiveness of our approach by\ncomparing the performance of the trained IK model with existing IK models\navailable in the literature. Our results show that the proposed approach\nenhances the accuracy and realism of generated",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16911764705882354,
          "p": 0.2948717948717949,
          "f": 0.21495326639531848
        },
        "rouge-2": {
          "r": 0.029239766081871343,
          "p": 0.042735042735042736,
          "f": 0.03472221739800414
        },
        "rouge-l": {
          "r": 0.15441176470588236,
          "p": 0.2692307692307692,
          "f": 0.19626167761027175
        }
      }
    },
    {
      "paper_id": "math.ST.stat/ML/2503.03356v1",
      "true_abstract": "We use tools from random matrix theory to study the multi-spiked tensor\nmodel, i.e., a rank-$r$ deformation of a symmetric random Gaussian tensor. In\nparticular, thanks to the nature of local optimization methods used to find the\nmaximum likelihood estimator of this model, we propose to study the phase\ntransition phenomenon for finding critical points of the corresponding\noptimization problem, i.e., those points defined by the Karush-Kuhn-Tucker\n(KKT) conditions. Moreover, we characterize the limiting alignments between the\nestimated signals corresponding to a critical point of the likelihood and the\nground truth signals. With the help of these results, we propose a new\nestimator of the rank-$r$ tensor weights by solving a system of polynomial\nequations, which is asymptotically unbiased contrary the maximum likelihood\nestimator.",
      "generated_abstract": "the problem of estimating a function from a sequence of samples\n$\\{x_n\\}_{n\\ge 1}$, where the function is defined on a set $X$ and we are\ninterested in estimating the support of $X$. We focus on the case when $X$ is a\nprobability space and $x_n$ are i.i.d. samples from a probability measure\n$\\mu$ on $X$. The problem is of independent interest, as it has applications in\nstatistical inference and machine learning. A crucial ingredient in our analysis\nis a certain concentration inequality for the empirical measure of the\nsample. We prove that, under mild assumptions on the function $f$ and the\nmeasure $\\mu$, the sample estimator $\\widehat f_n$ satisfies the\n$L^p$-concentration inequality with a constant that depends only on $f$ and\n$\\mu$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.1643835616438356,
          "f": 0.15894039235647575
        },
        "rouge-2": {
          "r": 0.017241379310344827,
          "p": 0.017241379310344827,
          "f": 0.017241374310346277
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.1643835616438356,
          "f": 0.15894039235647575
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/SE/2503.10099v1",
      "true_abstract": "While the trend of decentralized governance is obvious (cryptocurrencies and\nblockchains are widely adopted by multiple sovereign countries), initiating\ngovernance proposals within Decentralized Autonomous Organizations (DAOs) is\nstill challenging, i.e., it requires providing a low-level transaction payload,\ntherefore posing significant barriers to broad community participation. To\naddress these challenges, we propose a multi-agent system powered by Large\nLanguage Models with a novel Label-Centric Retrieval algorithm to automate the\ntranslation from natural language inputs into executable proposal transactions.\nThe system incorporates DAOLang, a Domain-Specific Language to simplify the\nspecification of various governance proposals. The key optimization achieved by\nDAOLang is a semantic-aware abstraction of user input that reliably secures\nproposal generation with a low level of token demand. A preliminary evaluation\non real-world applications reflects the potential of DAOLang in terms of\ngenerating complicated types of proposals with existing foundation models, e.g.\nGPT-4o.",
      "generated_abstract": "The ability to write and code is essential for our daily lives, but the\nlearning and development of these skills is often overlooked. This article\nexplores the intersection of coding and learning, focusing on the development\nof critical thinking and problem-solving skills. Through a combination of\ninterviews, surveys, and case studies, we examine the challenges and\nopportunities for teaching coding in higher education. We conclude with a set\nof recommendations for future research and practice.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10619469026548672,
          "p": 0.21818181818181817,
          "f": 0.14285713845308967
        },
        "rouge-2": {
          "r": 0.007042253521126761,
          "p": 0.0136986301369863,
          "f": 0.009302321096378581
        },
        "rouge-l": {
          "r": 0.07964601769911504,
          "p": 0.16363636363636364,
          "f": 0.10714285273880403
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.10578v1",
      "true_abstract": "In this paper we investigate the generalization error of gradient descent\n(GD) applied to an $\\ell_2$-regularized OLS objective function in the linear\nmodel. Based on our analysis we develop new methodology for computationally\ntractable and statistically efficient linear prediction in a high-dimensional\nand massive data scenario (large-$n$, large-$p$). Our results are based on the\nsurprising observation that the generalization error of optimally tuned\nregularized gradient descent approaches that of an optimal benchmark procedure\n$monotonically$ in the iteration number $m$. On the other hand standard GD for\nOLS (without explicit regularization) can achieve the benchmark only in\ndegenerate cases. This shows that (optimal) explicit regularization can be\nnearly statistically efficient (for large $m$) whereas implicit regularization\nby (optimal) early stopping can not.\n  To complete our methodology, we provide a fully data driven and\ncomputationally tractable choice of $\\ell_2$ regularization parameter $\\lambda$\nthat is computationally cheaper than cross-validation. On this way, we follow\nand extend ideas of Dicker (2014) to the non-gaussian case, which requires new\nresults on high-dimensional sample covariance matrices that might be of\nindependent interest.",
      "generated_abstract": "The spectral method is a powerful tool for studying the dynamics of random\nnetworks. In this paper, we present a novel spectral method for computing the\nspectral radius of a graph Laplacian by applying the Fourier transform to the\ngraph Laplacian. We show that the method is equivalent to the well-known\nBloch-Bloch method for computing the eigenvalues of a matrix. We further show\nthat the spectral radius of the graph Laplacian is the same as the spectral\nradius of the Bloch-Bloch matrix. This result provides a unified viewpoint for\ncomputing the spectral radius of a graph Laplacian and the spectral radius of\na Bloch-Bloch matrix.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1092436974789916,
          "p": 0.2826086956521739,
          "f": 0.15757575355445375
        },
        "rouge-2": {
          "r": 0.017857142857142856,
          "p": 0.041666666666666664,
          "f": 0.024999995800000705
        },
        "rouge-l": {
          "r": 0.1092436974789916,
          "p": 0.2826086956521739,
          "f": 0.15757575355445375
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2502.11954v1",
      "true_abstract": "This paper presents a novel approach to stochastic volatility (SV) modeling\nby utilizing nonparametric techniques that enhance our ability to capture the\nvolatility of financial time series data, with a particular emphasis on the\nnon-Gaussian behavior of asset return distributions. Although traditional\nparametric SV models can be useful, they often suffer from restrictive\nassumptions regarding errors, which may inadequately represent extreme values\nand tail behavior in financial returns. To address these limitations, we\npropose two semiparametric SV models that use data to better approximate error\ndistributions. To facilitate the computation of model parameters, we developed\na Markov Chain Monte Carlo (MCMC) method for estimating model parameters and\nvolatility dynamics. Simulations and empirical tests on S&P 500 data indicate\nthat nonparametric models can minimize bias and variance in volatility\nestimation, providing a more accurate reflection of market expectations about\nvolatility. This methodology serves as a promising alternative to conventional\nparametric models, improving precision in financial risk assessment and\ndeepening our understanding of the volatility dynamics of financial returns.",
      "generated_abstract": "We introduce the use of randomized designs to address the challenge of\ndetecting the presence of stochastic error in the predictions of a statistical\nmodel. We present a new framework for model selection in stochastic regression\nthat incorporates the concept of randomization, enabling the use of\nrandomized designs to address the stochasticity of the predictions of the\nmodel. This framework allows us to incorporate the notion of randomization in\nthe analysis of the model, as well as the use of randomization in the\nselection of the model. We also present the use of the proposed framework in\nthe analysis of the impact of climate change on crop yields.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12605042016806722,
          "p": 0.30612244897959184,
          "f": 0.17857142443948423
        },
        "rouge-2": {
          "r": 0.006289308176100629,
          "p": 0.0125,
          "f": 0.008368196383118911
        },
        "rouge-l": {
          "r": 0.10084033613445378,
          "p": 0.24489795918367346,
          "f": 0.1428571387251985
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2502.15867v1",
      "true_abstract": "Artificial intelligence (AI) is transforming scientific research, including\nproteomics. Advances in mass spectrometry (MS)-based proteomics data quality,\ndiversity, and scale, combined with groundbreaking AI techniques, are unlocking\nnew challenges and opportunities in biological discovery. Here, we highlight\nkey areas where AI is driving innovation, from data analysis to new biological\ninsights. These include developing an AI-friendly ecosystem for proteomics data\ngeneration, sharing, and analysis; improving peptide and protein identification\nand quantification; characterizing protein-protein interactions and protein\ncomplexes; advancing spatial and perturbation proteomics; integrating\nmulti-omics data; and ultimately enabling AI-empowered virtual cells.",
      "generated_abstract": "activity is a fundamental part of neural computation and is\nneeded to generate a variety of complex behaviors. The generation of complex\nbehaviors, however, requires a robust system of signal processing that allows\nfor rapid response to environmental cues. In the brain, this is achieved by\nreleasing large amounts of the neurotransmitter glutamate, which is then\nconverted into a number of different chemicals through the action of enzymes.\nOne of these enzymes, known as the glutamate decarboxylase (GAD), is important\nfor controlling the release of glutamate in response to environmental cues. In\nthis study, we focus on the role of GAD in the generation of the firing patterns\nof neurons. We show that the amount of GAD available to the neuron is regulated\nby the amount of the neurotransmitter glutam",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08108108108108109,
          "p": 0.075,
          "f": 0.07792207292966806
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08108108108108109,
          "p": 0.075,
          "f": 0.07792207292966806
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2502.00024v1",
      "true_abstract": "This project focuses on analyzing retail market trends using historical sales\ndata, search trends, and customer reviews. By identifying the patterns and\ntrending products, the analysis provides actionable insights for retailers to\noptimize inventory management and marketing strategies, ultimately enhancing\ncustomer satisfaction and maximizing revenue.",
      "generated_abstract": "e a novel framework to forecast stock market returns by leveraging\ndata from the past to predict the future. Our approach combines historical\ndata with predictive models to generate high-fidelity forecasts, which are\nthen used to generate trading signals. The framework consists of two main\ncomponents: (i) a forecasting model that generates future returns using historical\ndata, and (ii) a trading signal generation model that uses the forecast to\ngenerate trading signals. The forecasting model is trained on historical data\nfrom the past and used to generate future returns. The trading signal generation\nmodel generates trading signals based on the forecast, which are then used to\ngenerate trading signals. The framework is implemented through a Python\nframework, and is implemented using a series of Python scripts. The framework\nis tested on a dataset of 1,614 stocks from the S&P 500 index",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.11594202898550725,
          "f": 0.14678898617961464
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.009174311926605505,
          "f": 0.013071891327268576
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.11594202898550725,
          "f": 0.14678898617961464
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.02713v1",
      "true_abstract": "This paper presents new empirical evidence from four emerging economies on\nthe relationship between educational assortative mating and household income\ninequality. Using a methodological approach that allows for studying marital\nsorting patterns without imposing restrictive assumptions about search\nfrictions, the study finds that people in Brazil, Indonesia, Mexico, and South\nAfrica tend to sort into internally homogeneous marriages based on education\nlevel. While educational sorting has a noticeable impact on household income\ninequality in any given year, changes in the degree of sorting over time barely\nhave any impact on inequality. Further analysis reveals that this\ncounterintuitive result is due to different dynamics within educational groups.\nThe inequality-decreasing impact from reduced sorting among the highly educated\nis almost entirely offset by the inequality-increasing impact from increased\nsorting among the least educated. While it is certainly reassuring that\nconcerns about educational assortative mating having a potentially large effect\non income disparities between households appear to be unwarranted, these\nfindings suggest another concerning narrative. Marginalization processes are\noccurring at low levels of the educational distribution. The least educated are\nbeing left behind, facing limited labor market opportunities and diminished\nchances of achieving upward socioeconomic mobility through marriage to more\neducated partners.",
      "generated_abstract": "We investigate the impact of austerity policies on unemployment, wage\ndispersion, and inequality in Spain. Using a large panel of firms, we find that\nausterity policies increase the share of low-wage workers, decrease the\nshare of high-wage workers, and increase inequality. These effects are\nstronger for the lowest quintile and for workers with lower education. The\nresults hold across all unemployment rates and across firms of different\nsize. We also find that austerity policies increase the wage gap between\nlow-wage and high-wage workers and decrease the wage gap between those with\nhigher and lower education. Finally, we estimate a dynamic version of the\ngeneralized method of moments that accounts for the dynamic interaction between\nausterity policies and unemployment.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11971830985915492,
          "p": 0.265625,
          "f": 0.16504853940616468
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.029411764705882353,
          "f": 0.02061855214794446
        },
        "rouge-l": {
          "r": 0.11267605633802817,
          "p": 0.25,
          "f": 0.155339801542087
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.cond-mat/mes-hall/2503.09689v1",
      "true_abstract": "Using a self-consistent Hartree-Fock theory, we show that the recently\nobserved ferromagnetism in twisted bilayer WSe$_2$ [Nat. Commun. 16, 1959\n(2025)] can be understood as a Stoner-like instability of\ninteraction-renormalized moir\\'e bands. We quantitatively reproduce the\nobserved Lifshitz transition as function of hole filling and applied electric\nfield that marks the boundary between layer-hybridized and layer-polarized\nregimes. The former supports a ferromagnetic valley-polarized ground state\nbelow half-filling, developing a topological charge gap at half-filling for\nsmall twists. At larger twist angles there is a transition to a gapped\ntriangular N\\'eel antiferromagnet. The layer-polarized regime supports a stripe\nantiferromagnet below half-filling and a wing-shaped multiferroic ground state\nabove half-filling. We map the evolution of these states as a function of\nfilling factor, electric field, twist angle, and interaction strength. Beyond\nproviding an understanding of recent experiments, our methodology is applicable\nto a broad class of moir\\'e systems.",
      "generated_abstract": "We study the interaction between a quantum dot and a two-dimensional electron\ng gas in a strong magnetic field. We find that the interaction changes the\nbehavior of the electron distribution function and the spin polarization of\nthe two-dimensional electron gas, depending on the density and the magnetic\nfield. The electron distribution function shows a singularity at a finite\nmagnetic field, which can be understood as a topological phase transition.\n  We also find that the electron spin polarization can be controlled by\nmagnetic field, which may be useful for realizing quantum spintronic\ndevices. Furthermore, we propose a strategy for generating the spin polarization\nof the two-dimensional electron gas in a strong magnetic field. This strategy\nmay be applicable to the spintronic devices.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23076923076923078,
          "p": 0.39344262295081966,
          "f": 0.29090908624866857
        },
        "rouge-2": {
          "r": 0.05714285714285714,
          "p": 0.0851063829787234,
          "f": 0.06837606356928955
        },
        "rouge-l": {
          "r": 0.19230769230769232,
          "p": 0.32786885245901637,
          "f": 0.24242423776382008
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.hep-th/2503.10423v1",
      "true_abstract": "Sterile neutrinos can influence the evolution of the universe, and thus\ncosmological observations can be used to search for sterile neutrinos. In this\nstudy, we utilized the latest baryon acoustic oscillations data from DESI,\ncombined with the cosmic microwave background data from Planck and the\nfive-year supernova data from DES, to constrain the interacting dark energy\n(IDE) models involving both cases of massless and massive sterile neutrinos. We\nconsider four typical forms of the interaction term $Q=\\beta H \\rho_{\\rm de}$,\n$Q=\\beta H \\rho_{\\rm c}$, $Q=\\beta H_{0} \\rho_{\\rm de}$, and $Q=\\beta H_{0}\n\\rho_{\\rm c}$, respectively. Our analysis indicates that the current data\nprovide only a hint of the existence of massless sterile neutrinos (as dark\nradiation) at about the $1\\sigma$ level. In contrast, no evidence supports the\nexistence of massive sterile neutrinos. Furthermore, in IDE models, the\ninclusion of (massless/massive) sterile neutrinos has a negligible impact on\nthe constraint of the coupling parameter $\\beta$. The IDE model of $Q=\\beta H\n\\rho_{\\rm c}$ with sterile neutrinos does not favor an interaction. However,\nthe other three IDE models with sterile neutrinos support an interaction in\nwhich dark energy decays into dark matter.",
      "generated_abstract": "igate the evolution of the gravitational wave (GW) signal in\nstellar-mass binaries with tidal disruption and mergers in a general\nnon-rotating, non-magnetic, general relativistic spacetime. The tidal disruption\nefficiency (TDE) is assumed to be a monotonically decreasing function of the\nmass ratio, which is the ratio of the orbital mass to the stellar mass. The\nbinary's mass is assumed to be fixed, and the binary's eccentricity,\ninclination, and angular momentum are allowed to evolve. We consider three\nspecific TDE scenarios: a point-like source of TDEs, a continuous source, and a\nsource that can be observed at different times. We then examine the impact of\nthe TDE rate on the binary evolution and the GW signal. We find that the\nbinary's eccentricity",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.168141592920354,
          "p": 0.2602739726027397,
          "f": 0.20430107050005794
        },
        "rouge-2": {
          "r": 0.04938271604938271,
          "p": 0.07207207207207207,
          "f": 0.05860805378255484
        },
        "rouge-l": {
          "r": 0.168141592920354,
          "p": 0.2602739726027397,
          "f": 0.20430107050005794
        }
      }
    },
    {
      "paper_id": "math.SG.math/GT/2503.10283v1",
      "true_abstract": "Given a closed connected symplectic manifold $(M,\\omega)$, we construct an\nalternating $\\mathbb{R}$-bilinear form\n$\\mathfrak{b}=\\mathfrak{b}_{\\mu_{\\mathrm{Sh}}}$ on the real first cohomology of\n$M$ from Shelukhin's quasimorphism $\\mu_{\\mathrm{Sh}}$. Here\n$\\mu_{\\mathrm{Sh}}$ is defined on the universal cover of the group of\nHamiltonian diffeomorphisms on $(M,\\omega)$. This bilinear form is invariant\nunder the symplectic mapping class group action, and $\\mathfrak{b}$ yields a\nconstraint on the fluxes of commuting two elements in the group of\nsymplectomorphisms on $(M,\\omega)$. These results might be seen as an analog of\nRousseau's result for an open connected symplectic manifold, where he recovered\nthe symplectic pairing from the Calabi homomorphism. Furthermore,\n$\\mathfrak{b}$ controls the extendability of Shelukhin's quasimorphisms, as\nwell as the triviality of a characteristic class of Reznikov. To construct\n$\\mathfrak{b}$, we build general machinery for a group $G$ of producing a\nreal-valued $\\mathbb{Z}$-bilinear form $\\mathfrak{b}_{\\mu}$ from a\n$G$-invariant quasimorphism $\\mu$ on the commutator subgroup of $G$.",
      "generated_abstract": "In this paper, we give a characterisation of the set of all smooth\ncompact K\\\"ahler manifolds. More precisely, we prove that a smooth compact\nK\\\"ahler manifold is a quotient of a K\\\"ahler manifold of constant sectional\ncurvature. This characterisation is based on the comparison between the\nK\\\"ahler-Ricci flow and the K\\\"ahler-Einstein flow on K\\\"ahler manifolds. We\nalso prove that a compact K\\\"ahler manifold of constant sectional curvature is\na K\\\"ahler-Ricci soliton. These results extend several known results for\ncompact K\\\"ahler manifolds and their quotients.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13186813186813187,
          "p": 0.26666666666666666,
          "f": 0.1764705838073098
        },
        "rouge-2": {
          "r": 0.028985507246376812,
          "p": 0.058823529411764705,
          "f": 0.03883494703365118
        },
        "rouge-l": {
          "r": 0.13186813186813187,
          "p": 0.26666666666666666,
          "f": 0.1764705838073098
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.03356v1",
      "true_abstract": "We use tools from random matrix theory to study the multi-spiked tensor\nmodel, i.e., a rank-$r$ deformation of a symmetric random Gaussian tensor. In\nparticular, thanks to the nature of local optimization methods used to find the\nmaximum likelihood estimator of this model, we propose to study the phase\ntransition phenomenon for finding critical points of the corresponding\noptimization problem, i.e., those points defined by the Karush-Kuhn-Tucker\n(KKT) conditions. Moreover, we characterize the limiting alignments between the\nestimated signals corresponding to a critical point of the likelihood and the\nground truth signals. With the help of these results, we propose a new\nestimator of the rank-$r$ tensor weights by solving a system of polynomial\nequations, which is asymptotically unbiased contrary the maximum likelihood\nestimator.",
      "generated_abstract": "r proposes a novel method for modeling and predicting the dynamics of\nsignals and their associated noise. The approach is based on a novel\nmultiscale framework that integrates high-dimensional signal and noise\nrepresentations. This framework is designed to model both the short- and\nlong-term dynamics of the signal and its associated noise, while simultaneously\naccounting for their interplay. Furthermore, it enables the decomposition of\nthe multiscale dynamics into a low-dimensional representation of the signal\nand its associated noise, which can be then modeled using linear or nonlinear\nmodels. This framework is particularly useful in scenarios where the signal and\nits associated noise exhibit complex dynamics, such as in biological\nsignals, where the signal can be represented as a time-dependent gene expression\nlevel and its associated noise as a stochastic noise process. This approach is\nalso applicable to a wide range of other multiscale systems, including those",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1282051282051282,
          "p": 0.11764705882352941,
          "f": 0.12269938151228896
        },
        "rouge-2": {
          "r": 0.017241379310344827,
          "p": 0.016666666666666666,
          "f": 0.016949147543810723
        },
        "rouge-l": {
          "r": 0.11538461538461539,
          "p": 0.10588235294117647,
          "f": 0.11042944286198224
        }
      }
    },
    {
      "paper_id": "cs.AR.cs/AR/2503.05290v1",
      "true_abstract": "Transformers are central to advances in artificial intelligence (AI),\nexcelling in fields ranging from computer vision to natural language\nprocessing. Despite their success, their large parameter count and\ncomputational demands challenge efficient acceleration. To address these\nlimitations, this paper proposes MatrixFlow, a novel co-designed\nsystem-accelerator architecture based on a loosely coupled systolic array\nincluding a new software mapping approach for efficient transformer code\nexecution. MatrixFlow is co-optimized via a novel dataflow-based matrix\nmultiplication technique that reduces memory overhead. These innovations\nsignificantly improve data throughput, which is critical for handling the\nextensive computations required by transformers. We validate our approach\nthrough full system simulation using gem5 across various BERT and ViT\nTransformer models featuring different data types, demonstrating significant\napplication-wide speed-ups. Our method achieves up to a 22x improvement\ncompared to a many-core CPU system, and outperforms the closest\nstate-of-the-art loosely-coupled and tightly-coupled accelerators by over 5x\nand 8x, respectively.",
      "generated_abstract": "guage models (LLMs) are increasingly used for complex tasks such\nas healthcare diagnosis, yet the quality of their predictions remains\ncontroversial. This paper investigates the effects of different pretraining\nstrategies and LLMs on diagnostic prediction performance, focusing on the\nFine-Tuning (FT) and Inference-Pretraining (IP) methods. We evaluate two\narchitectures: the Transformer-Based model and the Longformer-Based model,\nwhich are both LLMs with 12 transformer layers and 1024 hidden units. We\ncomparatively analyze the performance of these models on four different\ndatasets: a publicly available dataset with 4000 diagnostic test results, a\nsmaller dataset with 400 diagnostic test results, and two clinical data\ndatasets with 500 and 10",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11811023622047244,
          "p": 0.19480519480519481,
          "f": 0.14705881882977714
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11023622047244094,
          "p": 0.18181818181818182,
          "f": 0.13725489726114973
        }
      }
    },
    {
      "paper_id": "math.MG.math/GN/2502.11615v1",
      "true_abstract": "The Gromov-Hausdorff distance is a dissimilarity metric capturing how far two\nspaces are from being isometric. The Gromov-Prokhorov distance is a similar\nnotion for metric measure spaces. In this paper, we study the topological\ndimension of the Gromov-Hausdorff and Gromov-Prokhorov spaces. We show that the\ndimension of the space of isometry classes of metric spaces with at most $n$\npoints endowed with the Gromov-Hausdorff distance is $\\frac{n(n-1)}{2}$, and\nthat of mm-isomorphism classes of metric measure spaces whose support consists\nof $n$ points is $\\frac{(n+2)(n-1)}{2}$. Hence, the spaces of all isometry\nclasses of finite metric spaces and of all mm-isomorphism classes of finite\nmetric measure spaces are strongly countable dimensional. If, instead, the\ncardinalities are not limited, the spaces are strongly infinite-dimensional.",
      "generated_abstract": "We give a complete classification of all compact Lie groups which admit a\ngauge action with finite-dimensional Lie algebra of automorphisms. Our result\nis a generalization of a result of L. Barrett and J. E. Gomis. We also give an\nexplicit list of compact Lie groups admitting a gauge action with finite-dimensional\nLie algebra of automorphisms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11475409836065574,
          "p": 0.21212121212121213,
          "f": 0.14893616565640577
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.044444444444444446,
          "f": 0.02877697403861151
        },
        "rouge-l": {
          "r": 0.11475409836065574,
          "p": 0.21212121212121213,
          "f": 0.14893616565640577
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.04883v1",
      "true_abstract": "Automatic Speech Recognition (ASR) performance for low-resource languages is\nstill far behind that of higher-resource languages such as English, due to a\nlack of sufficient labeled data. State-of-the-art methods deploy\nself-supervised transfer learning where a model pre-trained on large amounts of\ndata is fine-tuned using little labeled data in a target low-resource language.\nIn this paper, we present and examine a method for fine-tuning an SSL-based\nmodel in order to improve the performance for Frisian and its regional dialects\n(Clay Frisian, Wood Frisian, and South Frisian). We show that Frisian ASR\nperformance can be improved by using multilingual (Frisian, Dutch, English and\nGerman) fine-tuning data and an auxiliary language identification task. In\naddition, our findings show that performance on dialectal speech suffers\nsubstantially, and, importantly, that this effect is moderated by the\nelicitation approach used to collect the dialectal data. Our findings also\nparticularly suggest that relying solely on standard language data for ASR\nevaluation may underestimate real-world performance, particularly in languages\nwith substantial dialectal variation.",
      "generated_abstract": "opment of large language models (LLMs) has enabled the creation of\nsophisticated conversational agents that can engage in dialogue with users.\nHowever, these models face challenges in effectively understanding and\nresponding to multi-turn conversations, as their limited contextual awareness\nleads to inaccurate responses and can be inconsistent with the user's intent.\nExisting methods primarily focus on improving LLMs' ability to understand\ncontextual cues, but have limited success in addressing the more challenging\nproblem of understanding and responding to multiple-turn conversations. In this\npaper, we propose Conversation-Aware Transformer (CAT), a novel framework that\nintegrates contextual information from both the past and future turns into\nattention mechanisms, enabling the LLM to better understand and respond to\nmulti-turn conversations. Our experiments demonstrate that CAT significantly",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18018018018018017,
          "p": 0.21739130434782608,
          "f": 0.19704433001917068
        },
        "rouge-2": {
          "r": 0.024691358024691357,
          "p": 0.03418803418803419,
          "f": 0.028673830255521695
        },
        "rouge-l": {
          "r": 0.17117117117117117,
          "p": 0.20652173913043478,
          "f": 0.18719211327040222
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2412.13311v1",
      "true_abstract": "This paper investigates cash productivity as a signal for future stock\nperformance, building on the cash-return framework of Faulkender and Wang\n(2006). Using financial and market data from WRDS, we calculate cash returns as\na proxy for operational efficiency and evaluate a long-only strategy applied to\nNasdaq-listed non-financial firms. Results show limited predictive power across\nthe broader Nasdaq universe but strong performance in a handpicked portfolio,\nwhich achieves significant positive alpha after controlling for the Fama-French\nthree factors. These findings underscore the importance of refined universe\nselection. While promising, the strategy requires further validation, including\nthe incorporation of transaction costs and performance testing across economic\ncycles. Our results suggest that cash productivity, when combined with other\ncomplementary signals and careful universe selection, can be a valuable tool\nfor generating excess returns.",
      "generated_abstract": "We introduce a novel framework for pricing financial derivatives using\nprincipal component analysis (PCA) and latent variable models. The framework\nenables pricing with a single equation, without the need for numerical\nsolutions to large systems of equations. The framework is applicable to both\nforward and inverse problems, and can be used for pricing both standard\nderivatives and new derivatives. We demonstrate the framework's effectiveness\nby applying it to the pricing of options on interest rate swaps and credit\ndefault swaps.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11538461538461539,
          "p": 0.21052631578947367,
          "f": 0.14906831840746898
        },
        "rouge-2": {
          "r": 0.007751937984496124,
          "p": 0.012987012987012988,
          "f": 0.009708733182677344
        },
        "rouge-l": {
          "r": 0.09615384615384616,
          "p": 0.17543859649122806,
          "f": 0.12422359791057462
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.20838v1",
      "true_abstract": "Marine ecosystem monitoring via Passive Acoustic Monitoring (PAM) generates\nvast data, but deep learning often requires precise annotations and short\nsegments. We introduce DSMIL-LocNet, a Multiple Instance Learning framework for\nwhale call detection and localization using only bag-level labels. Our\ndual-stream model processes 2-30 minute audio segments, leveraging spectral and\ntemporal features with attention-based instance selection. Tests on Antarctic\nwhale data show longer contexts improve classification (F1: 0.8-0.9) while\nmedium instances ensure localization precision (0.65-0.70). This suggests MIL\ncan enhance scalable marine monitoring. Code:\nhttps://github.com/Ragib-Amin-Nihal/DSMIL-Loc",
      "generated_abstract": "aper, we propose a novel audio-to-speech synthesis method, which\nintegrates the audio-to-speech speech generation model with a large language\nmodel (LLM). The proposed method can generate speech with the same quality as\nhuman speech, which is crucial for various speech applications. In our method,\nwe introduce an audio-to-speech speech generation model as a sub-module. This\nmodel generates the speech using the audio input. The speech is then\ncompressed using an LLM, and the compressed speech is used as input for the\nmain speech generation model. The proposed method can generate speech with the\nsame quality as human speech. This method can be used in various speech\napplications, such as virtual assistants, voice assistants, and conversational\nai systems. The main contributions of this paper are as follows: (1) We propose\na novel audio-to-speech speech generation method",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1411764705882353,
          "p": 0.1791044776119403,
          "f": 0.15789473191222314
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1411764705882353,
          "p": 0.1791044776119403,
          "f": 0.15789473191222314
        }
      }
    },
    {
      "paper_id": "cs.CL.cs/CY/2503.08588v1",
      "true_abstract": "Previous studies have established that language models manifest stereotyped\nbiases. Existing debiasing strategies, such as retraining a model with\ncounterfactual data, representation projection, and prompting often fail to\nefficiently eliminate bias or directly alter the models' biased internal\nrepresentations. To address these issues, we propose BiasEdit, an efficient\nmodel editing method to remove stereotypical bias from language models through\nlightweight networks that act as editors to generate parameter updates.\nBiasEdit employs a debiasing loss guiding editor networks to conduct local\nedits on partial parameters of a language model for debiasing while preserving\nthe language modeling abilities during editing through a retention loss.\nExperiments on StereoSet and Crows-Pairs demonstrate the effectiveness,\nefficiency, and robustness of BiasEdit in eliminating bias compared to\ntangental debiasing baselines and little to no impact on the language models'\ngeneral capabilities. In addition, we conduct bias tracing to probe bias in\nvarious modules and explore bias editing impacts on different components of\nlanguage models.",
      "generated_abstract": "Neural machine translation (NMT) models often struggle with translation\ntransparency, making them difficult to understand. In this work, we propose\nTransparencyFix, a method to improve NMT models' transparency by enhancing the\ninterpretability of their internal representations. Specifically, we design an\ninterpretation-guided loss that directly encourages the model to make more\nprecise decisions during inference. This approach enhances the interpretability\nof the model's internal representations, facilitating more detailed\ninspection of its processing steps. Extensive experiments on four NMT\nbenchmarks demonstrate that TransparencyFix significantly improves the\ntransparency of the models, offering valuable insights into the internal\nworking of NMT models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.205607476635514,
          "p": 0.29333333333333333,
          "f": 0.24175823691281254
        },
        "rouge-2": {
          "r": 0.0196078431372549,
          "p": 0.031914893617021274,
          "f": 0.02429149326099519
        },
        "rouge-l": {
          "r": 0.18691588785046728,
          "p": 0.26666666666666666,
          "f": 0.2197802149347906
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/CV/2503.10624v1",
      "true_abstract": "Fitting a body to a 3D clothed human point cloud is a common yet challenging\ntask. Traditional optimization-based approaches use multi-stage pipelines that\nare sensitive to pose initialization, while recent learning-based methods often\nstruggle with generalization across diverse poses and garment types. We propose\nEquivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline\nthat estimates cloth-to-body surface mapping through locally approximate SE(3)\nequivariance, encoding tightness as displacement vectors from the cloth surface\nto the underlying body. Following this mapping, pose-invariant body features\nregress sparse body markers, simplifying clothed human fitting into an\ninner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show\nthat ETCH significantly outperforms state-of-the-art methods -- both\ntightness-agnostic and tightness-aware -- in body fitting accuracy on loose\nclothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant\ntightness design can even reduce directional errors by (67.2% ~ 89.8%) in\none-shot (or out-of-distribution) settings. Qualitative results demonstrate\nstrong generalization of ETCH, regardless of challenging poses, unseen shapes,\nloose clothing, and non-rigid dynamics. We will release the code and models\nsoon for research purposes at https://boqian-li.github.io/ETCH/.",
      "generated_abstract": "We present a novel 3D optical flow estimation framework for the challenging\ntask of reconstructing 3D motion fields from RGB images. Unlike traditional\nmethods, our method does not require any motion estimation prior, and does not\nmake any assumptions about the motion fields. Instead, we propose a simple\nlightweight optimization algorithm that learns the motion fields from the\nexisting RGB images. We show that our approach can handle complex occlusions\nand multiple occluders in the scene, even without any explicit occlusion\nguidance. Our experiments show that our method outperforms traditional methods\nin terms of both reconstructed motion fields and motion field quality metrics,\nincluding the mean absolute error, root mean squared error, and correlation\ncoefficient. Furthermore, we demonstrate that our method can be easily integrated\ninto existing motion estimation systems, making it a flexible alternative for\nreal-time 3D motion estimation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1610738255033557,
          "p": 0.2696629213483146,
          "f": 0.20168066758668182
        },
        "rouge-2": {
          "r": 0.021505376344086023,
          "p": 0.03225806451612903,
          "f": 0.025806446812904116
        },
        "rouge-l": {
          "r": 0.15436241610738255,
          "p": 0.25842696629213485,
          "f": 0.19327730624214404
        }
      }
    },
    {
      "paper_id": "cs.CL.q-fin/TR/2412.10823v1",
      "true_abstract": "Financial sentiment analysis is crucial for understanding the influence of\nnews on stock prices. Recently, large language models (LLMs) have been widely\nadopted for this purpose due to their advanced text analysis capabilities.\nHowever, these models often only consider the news content itself, ignoring its\ndissemination, which hampers accurate prediction of short-term stock movements.\nAdditionally, current methods often lack sufficient contextual data and\nexplicit instructions in their prompts, limiting LLMs' ability to interpret\nnews. In this paper, we propose a data-driven approach that enhances\nLLM-powered sentiment-based stock movement predictions by incorporating news\ndissemination breadth, contextual data, and explicit instructions. We cluster\nrecent company-related news to assess its reach and influence, enriching\nprompts with more specific data and precise instructions. This data is used to\nconstruct an instruction tuning dataset to fine-tune an LLM for predicting\nshort-term stock price movements. Our experimental results show that our\napproach improves prediction accuracy by 8\\% compared to existing methods.",
      "generated_abstract": "We present the first end-to-end method for real-time generation of personalized\nfinancial news stories. Our approach, the Personalized Finance News (PFN)\ngenerator, learns to generate personalized financial news stories from a\nfinancial news dataset. The PFN generator combines a news generation model\nwith a financial news dataset, and leverages a news-specific personalization\nmodel to personalize the financial news. Our experiments show that the PFN\ngenerator outperforms existing news generation methods, particularly on\nfinancial news personalization, while also demonstrating significant\ncomputational efficiency. The PFN generator can be used to generate financial\nnews stories for any news source in any language, without requiring\ninterpretation or language modeling. Our code and models are available at\nhttps://github.com/hari-r/pfn.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16521739130434782,
          "p": 0.25675675675675674,
          "f": 0.20105819629349692
        },
        "rouge-2": {
          "r": 0.013333333333333334,
          "p": 0.02040816326530612,
          "f": 0.016129027477889035
        },
        "rouge-l": {
          "r": 0.14782608695652175,
          "p": 0.22972972972972974,
          "f": 0.1798941751294758
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.05514v1",
      "true_abstract": "Securing Internet of Things (IoT) devices presents increasing challenges due\nto their limited computational and energy resources. Radio Frequency\nFingerprint Identification (RFFI) emerges as a promising authentication\ntechnique to identify wireless devices through hardware impairments. RFFI\nperformance under low signal-to-noise ratio (SNR) scenarios is significantly\ndegraded because the minute hardware features can be easily swamped in noise.\nIn this paper, we leveraged the diffusion model to effectively restore the RFF\nunder low SNR scenarios. Specifically, we trained a powerful noise predictor\nand tailored a noise removal algorithm to effectively reduce the noise level in\nthe received signal and restore the device fingerprints. We used Wi-Fi as a\ncase study and created a testbed involving 6 commercial off-the-shelf Wi-Fi\ndongles and a USRP N210 software-defined radio (SDR) platform. We conducted\nexperimental evaluations on various SNR scenarios. The experimental results\nshow that the proposed algorithm can improve the classification accuracy by up\nto 34.9%.",
      "generated_abstract": "a of cognitive radio networks, the effective resource allocation is\nrequired to balance the utilization of licensed and unlicensed spectrum\nresources. In this paper, we propose a multi-user distributed cognitive radio\nnetwork (MU-DRCN) that uses a centralized centralized cooperative (C-C)\nframework. The C-C network architecture provides a flexible and efficient\nplatform for cognitive radio networks to efficiently utilize spectrum resources\nin the presence of dynamic channel states. Furthermore, the MU-DRCN adopts a\ndistributed resource allocation strategy that can effectively balance the\nutilization of licensed and unlicensed spectrum resources. The proposed MU-DRCN\nis evaluated through extensive simulations. The results show that the MU-DRCN\nachieves higher throughput, throughput improvement, and average utilization\nlevel compared to the conventional centralized C-C network. The MU-DRCN also",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20353982300884957,
          "p": 0.3150684931506849,
          "f": 0.2473118231882299
        },
        "rouge-2": {
          "r": 0.047619047619047616,
          "p": 0.0660377358490566,
          "f": 0.05533596351075673
        },
        "rouge-l": {
          "r": 0.19469026548672566,
          "p": 0.3013698630136986,
          "f": 0.23655913501618694
        }
      }
    },
    {
      "paper_id": "math.GN.math/GN/2502.20164v1",
      "true_abstract": "This paper concerns various models of ``at-most-$n$-valued maps''. That is,\nmultivalued maps $f:X\\multimap Y$ for which $f(x)$ has cardinality at most $n$\nfor each $x$. We consider 4 classes of such maps which have appeared in the\nliterature: $\\mathcal U$, the set of exactly $n$-valued maps, or unions of\nsuch; $\\mathcal F$, the set of $n$-fold maps defined by Crabb; $\\mathcal S$,\nthe set of symmetric product maps; and $\\mathcal W$, the set of weighted maps\nwith weights in $\\mathbb N$. Our main result is roughly that these classes\nsatisfy the following containments: \\[ \\mathcal U \\subsetneq \\mathcal F\n\\subsetneq \\mathcal S = \\mathcal W \\]\n  Furthermore we define the general class $\\mathcal C$ of all\nat-most-$n$-valued maps, and show that there are maps in $\\mathcal C$ which are\noutside of any of the other classes above. We also describe a\nconfiguration-space point of view for the class $\\mathcal C$, defining a\nconfiguration space $C_n(Y)$ such that any at-most-$n$-valued map $f:X\\multimap\nY$ corresponds naturally to a single-valued map $f:X\\to C_n(Y)$. We give a full\ncalculation of the fundamental group and homology groups of $C_n(S^1)$.",
      "generated_abstract": "the stability of the boundary of a compact convex set of a general\nrelatively compact convex set with a specific convex body $K$. We show that the\nboundary is stable under the following conditions:\n  (i) if $K$ is a convex body, the boundary is stable under $x\\mapsto\n\\argmin_{y\\in K} d(x,y)$;\n  (ii) if $K$ is a compact convex set, the boundary is stable under\n$\\argmin_{y\\in K} d(x,y)$.\n  In the case $K$ is a convex body, we show that the boundary is stable under\nthe following conditions:\n  (i) if $x\\mapsto d(x,K)$ is Lipschitz continuous, the boundary is stable\nunder $x\\mapsto \\argmin_{y\\in K} d(x,y)$.\n  (ii) if $",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10084033613445378,
          "p": 0.3,
          "f": 0.15094339246074134
        },
        "rouge-2": {
          "r": 0.023255813953488372,
          "p": 0.06451612903225806,
          "f": 0.03418803029293638
        },
        "rouge-l": {
          "r": 0.08403361344537816,
          "p": 0.25,
          "f": 0.12578615975633886
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/RO/2503.10341v1",
      "true_abstract": "The field of high-speed autonomous racing has seen significant advances in\nrecent years, with the rise of competitions such as RoboRace and the Indy\nAutonomous Challenge providing a platform for researchers to develop software\nstacks for autonomous race vehicles capable of reaching speeds in excess of 170\nmph. Ensuring the safety of these vehicles requires the software to\ncontinuously monitor for different faults and erroneous operating conditions\nduring high-speed operation, with the goal of mitigating any unreasonable risks\nposed by malfunctions in sub-systems and components. This paper presents a\ncomprehensive overview of the HALO safety architecture, which has been\nimplemented on a full-scale autonomous racing vehicle as part of the Indy\nAutonomous Challenge. The paper begins with a failure mode and criticality\nanalysis of the perception, planning, control, and communication modules of the\nsoftware stack. Specifically, we examine three different types of faults - node\nhealth, data health, and behavioral-safety faults. To mitigate these faults,\nthe paper then outlines HALO safety archetypes and runtime monitoring methods.\nFinally, the paper demonstrates the effectiveness of the HALO safety\narchitecture for each of the faults, through real-world data gathered from\nautonomous racing vehicle trials during multi-agent scenarios.",
      "generated_abstract": "r introduces a novel approach to robot-aided surgery that leverages\nRobotics and Autonomy (R&A) technologies to enhance the surgical experience.\nWe propose a novel framework that integrates an in-vivo surgical simulation\nenvironment with robotic surgical systems, enabling surgeons to practice their\nskills in a safe and realistic environment. By simulating surgical scenarios\nand providing real-time feedback, the framework helps surgeons improve their\nskills and optimize their workflows. Additionally, we incorporate a\ncommunication protocol that enables seamless communication between the\nsimulation and real-world robotic systems, ensuring that surgeons receive\nreal-time feedback and maintain a safe surgical environment. The framework\nprovides a flexible and scalable solution for training and evaluating\nrobotic-assisted surgical skills, enabling surgeons to practice their",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10833333333333334,
          "p": 0.18309859154929578,
          "f": 0.13612564977933736
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10833333333333334,
          "p": 0.18309859154929578,
          "f": 0.13612564977933736
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2501.11189v1",
      "true_abstract": "This paper is a sequel of the 2019 paper [5]. It demonstrates the following:\na) the Poisson multi-Bernoulli mixture (PMBM) approach to detected vs.\nundetected (U/D) targets cannot be rigorously formulated using either the\ntwo-step or single-step multitarget recursive Bayes filter (MRBF); b) it can,\nhowever, be partially salvaged using a novel single-step MRBF; c) probability\nhypothesis density (PHD) filters can be derived for both the original \"S-U/D\"\napproach in [5] and the novel \"D-U/D\" approach; d) important U/D formulas in\n[5] can be verified using purely algebraic methods rather than the intricate\nstatistical analysis employed in that paper; and e) the claim, that PMBM\nfilters can propagate detected and undetected targets separately in parallel,\nis doubtful.",
      "generated_abstract": "er the problem of estimating a mean function from a set of\ndistinct measurements. The mean function is assumed to have a compact support\nand is described by a Gaussian process, which can be modeled by a\nMultivariate Gaussian. We develop a stochastic gradient descent algorithm that\ntakes advantage of the compactness of the mean function and its Gaussian\napproximation. This allows us to use a low-dimensional parameterization of the\nmean function, and to use only a few observations. Our method is\ncomputationally efficient and is designed to work with data sets that are\nlarge in size and have high dimensionality. We also consider the problem of\ninference, where we estimate the mean function from the observations. We show\nthat the method we propose can be used to estimate the mean function in a\nnumber of different ways. Finally, we provide a theoretical justification of\nthe method, and we use it to estimate the mean function for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15294117647058825,
          "p": 0.15294117647058825,
          "f": 0.15294117147058842
        },
        "rouge-2": {
          "r": 0.017699115044247787,
          "p": 0.014925373134328358,
          "f": 0.016194327019949394
        },
        "rouge-l": {
          "r": 0.12941176470588237,
          "p": 0.12941176470588237,
          "f": 0.12941175970588256
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.07088v1",
      "true_abstract": "We construct a family of estimators for a regression function based on a\nsample following a qdistribution. Our approach is nonparametric, using kernel\nmethods built from operations that leverage the properties of q-calculus.\nFurthermore, under appropriate assumptions, we establish the weak convergence\nand strong consistency of this family of estimators.",
      "generated_abstract": "This paper considers the problem of estimating a deterministic function of a\ndistributed variable. We derive the asymptotic distribution of the estimator of\nthe function in the worst case, where the random variables are dependent. We\nalso provide an asymptotic analysis of the performance of the estimator in the\naverage case. Our analysis is based on a novel methodology for analyzing\nasymptotic distributions of a random vector of variables, which allows us to\nobtain the asymptotic distribution of the estimator in the average case even\nwhen the random variables are dependent. We apply this methodology to derive\nasymptotic results for the estimation of the average of a deterministic\nfunction of a random variable and for the estimation of the average of a\nrandom function of a random variable.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2926829268292683,
          "p": 0.21818181818181817,
          "f": 0.2499999951063369
        },
        "rouge-2": {
          "r": 0.0425531914893617,
          "p": 0.022988505747126436,
          "f": 0.02985074171419094
        },
        "rouge-l": {
          "r": 0.2682926829268293,
          "p": 0.2,
          "f": 0.2291666617730036
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.10101v1",
      "true_abstract": "Financial bubbles and crashes have repeatedly caused economic turmoil notably\nbut not only during the 2008 financial crisis. However, both in the popular\npress as well as scientific publications, the meaning of bubble is sometimes\nunspecified. Due to the multitude of bubble definitions, we conduct a\nsystematic review with the following questions: What definitions of asset price\nbubbles exist in the literature? Which definitions are used in which\ndisciplines and how frequently? We develop a system of definition categories\nand categorize a total of 122 papers from eleven research areas. Our results\nshow that although one definition is indeed prevalent in the literature, the\noverall definition landscape is not uniform. Next to the mostly used definition\nas deviation from a present value of expected future cash flows, we identify\nseveral other definitions, which rely on price properties or other\nspecifications of a fundamental value. This research contributes by shedding\nlight on the possible variations in which bubbles are defined and\noperationalized.",
      "generated_abstract": "r introduces a new dataset of over 1,000 interviews with the\n(a) top 1000 most senior executives in the U.S. technology industry and (b) the\ntop 1000 most senior executives in the U.S. consumer electronics industry. The\nstudy is unique in its focus on the top executives in these industries,\nhighlighting their key roles in shaping the technology and consumer electronics\nindustries. The data provides a unique window into the strategic thinking of\nthese executives and their role in shaping their respective industries.\n  The data is available in a web-based tool at\nhttps://technology.dynata.com/datasets/executive-interviews-2024.\n  The dataset includes 1,044 interviews with the top 1000 executives in the U.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07272727272727272,
          "p": 0.13793103448275862,
          "f": 0.09523809071712039
        },
        "rouge-2": {
          "r": 0.01935483870967742,
          "p": 0.03409090909090909,
          "f": 0.02469135340480027
        },
        "rouge-l": {
          "r": 0.07272727272727272,
          "p": 0.13793103448275862,
          "f": 0.09523809071712039
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.08001v1",
      "true_abstract": "Mobile edge computing (MEC) enables the provision of high-reliability and\nlow-latency applications by offering computation and storage resources in close\nproximity to end-users. Different from traditional computation task offloading\nin MEC systems, the large data volume and complex task computation of\nartificial intelligence involved intelligent computation task offloading have\nincreased greatly. To address this challenge, we propose a MEC system for\nmultiple base stations and multiple terminals, which exploits semantic\ntransmission and early exit of inference. Based on this, we investigate a joint\nsemantic transmission and resource allocation problem for maximizing system\nreward combined with analysis of semantic transmission and intelligent\ncomputation process. To solve the formulated problem, we decompose it into\ncommunication resource allocation subproblem, semantic transmission subproblem,\nand computation capacity allocation subproblem. Then, we use 3D matching and\nconvex optimization method to solve subproblems based on the block coordinate\ndescent (BCD) framework. The optimized feasible solutions are derived from an\nefficient BCD based joint semantic transmission and resource allocation\nalgorithm in MEC systems. Our simulation demonstrates that: 1) The proposed\nalgorithm significantly improves the delay performance for MEC systems compared\nwith benchmarks; 2) The design of transmission mode and early exit of inference\ngreatly increases system reward during offloading; and 3) Our proposed system\nachieves efficient utilization of resources from the perspective of system\nreward in the intelligent scenario.",
      "generated_abstract": "r proposes a distributed energy management system (DEMS) for\nunder-powered vehicle charging stations (VCSs) in electric vehicle (EV)\ncharging networks. The system design incorporates an energy storage system\n(ESS) and a power electronics controller (PEC) to provide energy storage and\npower conversion capabilities, respectively. The ESS and PEC are connected to\nthe network through a communication network, enabling seamless communication\nbetween the controller and the network. To ensure the efficient operation of\nthe system, a control algorithm is proposed to ensure system stability. The\nalgorithm is based on the backstepping method of the mean, which is suitable\nfor the VCS, and the proposed control algorithm is verified through simulations\nand real-world experiments. The simulation results show that the proposed\ncontrol algorithm can effectively control the system's state, thereby ensuring\nits system stability and ensuring the smooth operation of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.2962962962962963,
          "f": 0.2253521079627059
        },
        "rouge-2": {
          "r": 0.010101010101010102,
          "p": 0.016,
          "f": 0.01238389618418838
        },
        "rouge-l": {
          "r": 0.1590909090909091,
          "p": 0.25925925925925924,
          "f": 0.19718309387819888
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.cs/CE/2503.07684v1",
      "true_abstract": "Lithium-sulfur (Li-S) batteries offer a promising alternative to current\nlithium-ion (Li-ion) batteries, with a high theoretical energy density,\nimproved safety and high abundance, low cost of materials. For Li-S to reach\ncommercial application, it is essential to understand how the behaviour scales\nbetween cell formats; new material development is predominately completed at\ncoin-cell level, whilst pouch-cells will be used for commercial applications.\nDifferences such as reduced electrolyte-to-sulfur (E/S) ratios and increased\ngeometric size at larger cell formats contribute to the behavioural\ndifferences, in terms of achievable capacity, cyclability and potential\ndegradation mechanisms.\n  This work focuses on the steps required to capture and test coin-cell\nbehaviour, building upon the existing models within the literature, which\npredominately focus on pouch-cells. The areas investigated throughout this\nstudy, to improve the capability of the model in terms of scaling ability and\ncausality of predictions, include the cathode surface area, precipitation\ndynamics and C-rate dependence.",
      "generated_abstract": "ew explores the impact of quantum mechanics on the structure,\nenergy, and thermodynamics of materials. We start by defining quantum\nmechanics, including the wave-particle duality, the Pauli exclusion principle,\nand the many-body nature of matter. We then delve into the concept of\nquantum entanglement, which is essential to the description of quantum\nphenomena in materials. We examine the role of quantum effects in superconductors,\nsuperfluids, and superalgebras, as well as in topological materials. We also\ndiscuss the quantum properties of matter in quantum materials, including\nspintronics, superconductivity, and topological insulators. We then explore\nthe role of entanglement in quantum materials, including entanglement effects in\ntransport, spintronics, and topological insulators. Finally, we discuss the\nimplications of quantum entanglement",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09401709401709402,
          "p": 0.18032786885245902,
          "f": 0.12359550111286471
        },
        "rouge-2": {
          "r": 0.034013605442176874,
          "p": 0.050505050505050504,
          "f": 0.0406504016944285
        },
        "rouge-l": {
          "r": 0.07692307692307693,
          "p": 0.14754098360655737,
          "f": 0.10112359100050519
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2410.22030v1",
      "true_abstract": "Metabolite biosynthesis is regulated via metabolic pathways, which can be\nactivated and deactivated within organisms. Understanding and identifying an\norganism's metabolic pathway network is a crucial aspect for various research\nfields, including crop and life stock breeding, pharmacology, and medicine. The\nproblem of identifying whether a pathway is part of a studied metabolic system\nis commonly framed as a hyperlink prediction problem. The most important\nchallenge in prediction of metabolic pathways is the sparsity of the labeled\ndata. This challenge can partially be mitigated using metabolite correlation\nnetworks which are affected by all active pathways including those that were\nnot confirmed yet in laboratory experiments. Unfortunately, extracting\nproperties that can confirm or refute existence of a metabolic pathway in a\nparticular organism is not a trivial task. In this research, we introduce the\nNetwork Auralization Hyperlink Prediction (NetAurHPD) which is a framework that\nrelies on (1) graph auralization to extract and aggregate representations of\nnodes in metabolite correlation networks and (2) data augmentation method that\ngenerates metabolite correlation networks given a subset of chemical reactions\ndefined as hyperlinks. Experiments with metabolites correlation-based networks\nof tomato pericarp demonstrate promising results for NetAurHPD, compared to\nalternative methods. Furthermore, the application of data augmentation improved\nNetAurHPD's learning capabilities and overall performance. Additionally,\nNetAurHPD outperformed state-of-the-art method in experiments under challenging\nconditions, and has the potential to be a valuable tool for exploring organisms\nwith limited existing knowledge.",
      "generated_abstract": "aper, we propose a novel approach to the analysis of protein\nstructure, namely the use of an iterative algorithm based on the\nHamilton-Jacobi-Bellman equation. Our method, called PEASE (Protein\nEvolutionary Structure Analysis and Simulation using Evolutionary Search\nEquations), is based on the use of an evolutionary search algorithm, which\nallows us to explore the parameter space of a protein and obtain its\noptimized structure, as well as its evolutionary history. The proposed\napproach is based on a combination of two types of evolutionary search\nalgorithms: a genetic algorithm and a simulated annealing algorithm. The\nproposed algorithm is capable of analyzing proteins with different lengths and\ndifferent number of residues. Additionally, the algorithm can be used for\nanalyzing proteins with a high degree of structure flexibility and for\nsimulating evolutionary processes",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11688311688311688,
          "p": 0.23076923076923078,
          "f": 0.15517240932966722
        },
        "rouge-2": {
          "r": 0.008888888888888889,
          "p": 0.01694915254237288,
          "f": 0.011661803066751145
        },
        "rouge-l": {
          "r": 0.11038961038961038,
          "p": 0.21794871794871795,
          "f": 0.1465517196744948
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/MF/2502.00740v1",
      "true_abstract": "This paper examines a semi-analytical approach for pricing American options\nin time-inhomogeneous models characterized by negative interest rates (for\nequity, FX) or negative convenience yields (for commodities, cryptocurrencies).\nUnder such conditions, exercise boundaries may exhibit a \"floating\" structure -\ndynamically appearing and disappearing. For example, a second exercise boundary\ncould emerge within the computational domain and subsequently both could\ncollapse, demanding specialized pricing methodologies.",
      "generated_abstract": "We study the long-short trading problem with a continuous-time Markov chain\nand a continuous-time stochastic control process. The stochastic control\nprocess is modelled as a linear stochastic differential equation with\nintermediate-type jump diffusion structure. We consider a wide range of\nstochastic control processes, including non-interacting and interacting\ncontrols. We also consider the case where the market state is an observable\ncontinuous-time Markov chain. We derive the optimal stopping problem and\nderive the optimal stopping rule under the policy that chooses the stopping time\nat the first jump. We also derive the optimal trading rule and derive the\noptimal trading rule under the policy that chooses the stopping time at the\nfirst jump.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07142857142857142,
          "p": 0.07547169811320754,
          "f": 0.07339449041663193
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07142857142857142,
          "p": 0.07547169811320754,
          "f": 0.07339449041663193
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2501.05232v1",
      "true_abstract": "Tether Limited has the sole authority to create (mint) and destroy (burn)\nTether stablecoins (USDT). This paper investigates Bitcoin's response to USDT\nsupply change events between 2014 and 2021 and identifies an interesting\nasymmetry between Bitcoin's responses to USDT minting and burning events.\nBitcoin responds positively to USDT minting events over 5- to 30-minute event\nwindows, but this response begins declining after 60 minutes. State-dependence\nis also demonstrated, with Bitcoin prices exhibiting a greater increase when\nthe corresponding USDT minting event coincides with positive investor sentiment\nand is announced to the public by data service provider, Whale Alert, on\nTwitter.",
      "generated_abstract": "In this paper, we propose a novel method for pricing Asian options with\nprincipal-agent and multiple-stage risk transfer, in which the agent is the\nprincipal and the first stage is the purchase stage. The pricing method is based\non the semigroup theory of the Black-Scholes model with the extended\nGaussian-Nash-Williams (EGNW) model. We derive the pricing formula of Asian\noptions with risk transfer, and the numerical results verify the correctness\nof our pricing formula.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0945945945945946,
          "p": 0.15217391304347827,
          "f": 0.11666666193888908
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08108108108108109,
          "p": 0.13043478260869565,
          "f": 0.09999999527222246
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.00648v1",
      "true_abstract": "T-cells play a key role in adaptive immunity by mounting specific responses\nagainst diverse pathogens. An effective binding between T-cell receptors (TCRs)\nand pathogen-derived peptides presented on Major Histocompatibility Complexes\n(MHCs) mediate an immune response. However, predicting these interactions\nremains challenging due to limited functional data on T-cell reactivities.\nHere, we introduce a computational approach to predict TCR interactions with\npeptides presented on MHC class I alleles, and to design novel immunogenic\npeptides for specified TCR-MHC complexes. Our method leverages HERMES, a\nstructure-based, physics-guided machine learning model trained on the protein\nuniverse to predict amino acid preferences based on local structural\nenvironments. Despite no direct training on TCR-pMHC data, the implicit\nphysical reasoning in HERMES enables us to make accurate predictions of both\nTCR-pMHC binding affinities and T-cell activities across diverse viral epitopes\nand cancer neoantigens, achieving up to 72% correlation with experimental data.\nLeveraging our TCR recognition model, we develop a computational protocol for\nde novo design of immunogenic peptides. Through experimental validation in\nthree TCR-MHC systems targeting viral and cancer peptides, we demonstrate that\nour designs--with up to five substitutions from the native sequence--activate\nT-cells at success rates of up to 50%. Lastly, we use our generative framework\nto quantify the diversity of the peptide recognition landscape for various\nTCR-MHC complexes, offering key insights into T-cell specificity in both humans\nand mice. Our approach provides a platform for immunogenic peptide and\nneoantigen design, opening new computational paths for T-cell vaccine\ndevelopment against viruses and cancer.",
      "generated_abstract": "he advancement of deep learning (DL) methods for molecular\nautomated functional genomics (MAFG) analysis, the need for interpretable and\nreproducible models has been highlighted. This paper introduces an\ninterpretable, reproducible, and scalable framework for MAFG that combines\nDL-based models with machine learning (ML) techniques. The framework incorporates\nthe DL-based models to extract molecular features and generates predictions\nusing the ML-based models. The framework provides an efficient, scalable, and\ninterpretable method for MAFG. The framework is applicable to any biological\nsystem and can be easily extended to multiple molecular types, enabling\ntransfer learning. The framework was validated through several case studies in\ndifferent biological systems. The framework achieved superior accuracy,\nrobustness, and scalability compared to state-of-the-art MAFG methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10179640718562874,
          "p": 0.20987654320987653,
          "f": 0.13709676979481025
        },
        "rouge-2": {
          "r": 0.004219409282700422,
          "p": 0.00909090909090909,
          "f": 0.005763684430569074
        },
        "rouge-l": {
          "r": 0.09580838323353294,
          "p": 0.19753086419753085,
          "f": 0.129032253665778
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.09411v1",
      "true_abstract": "The learning rate in stochastic gradient methods is a critical hyperparameter\nthat is notoriously costly to tune via standard grid search, especially for\ntraining modern large-scale models with billions of parameters. We identify a\ntheoretical advantage of learning rate annealing schemes that decay the\nlearning rate to zero at a polynomial rate, such as the widely-used cosine\nschedule, by demonstrating their increased robustness to initial parameter\nmisspecification due to a coarse grid search. We present an analysis in a\nstochastic convex optimization setup demonstrating that the convergence rate of\nstochastic gradient descent with annealed schedules depends sublinearly on the\nmultiplicative misspecification factor $\\rho$ (i.e., the grid resolution),\nachieving a rate of $O(\\rho^{1/(2p+1)}/\\sqrt{T})$ where $p$ is the degree of\npolynomial decay and $T$ is the number of steps, in contrast to the\n$O(\\rho/\\sqrt{T})$ rate that arises with fixed stepsizes and exhibits a linear\ndependence on $\\rho$. Experiments confirm the increased robustness compared to\ntuning with a fixed stepsize, that has significant implications for the\ncomputational overhead of hyperparameter search in practical training\nscenarios.",
      "generated_abstract": "r explores the potential of deep generative models for\napplications in genomic data analysis. We first provide a survey of\nexisting methods for generating synthetic datasets from genomic data,\nhighlighting their strengths and limitations. We then introduce a novel\napproach based on a generative adversarial network (GAN) designed to generate\nsynthetic data that closely resembles the observed data distribution. This\nmethod is based on the idea that a GAN can learn to imitate the observed data\ndistribution, and it can be trained in an end-to-end fashion using a single\ntraining set. We evaluate this approach using synthetic data generated from the\nNHLB dataset, which includes data from 100 different individuals with 10\ndifferent types of cancer. We find that the generated data can be used to\nperform tasks such as identifying which type of cancer each individual has,\npredicting which cancer each",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18691588785046728,
          "p": 0.21505376344086022,
          "f": 0.1999999950245001
        },
        "rouge-2": {
          "r": 0.017964071856287425,
          "p": 0.022388059701492536,
          "f": 0.019933549877375643
        },
        "rouge-l": {
          "r": 0.14018691588785046,
          "p": 0.16129032258064516,
          "f": 0.14999999502450015
        }
      }
    },
    {
      "paper_id": "math.OC.econ/GN/2412.05234v1",
      "true_abstract": "Risk measures, which typically evaluate the impact of extreme losses, are\nhighly sensitive to misspecification in the tails. This paper studies a robust\noptimization approach to combat tail uncertainty by proposing a unifying\nframework to construct uncertainty sets for a broad class of risk measures,\ngiven a specified nominal model. Our framework is based on a parametrization of\nrobust risk measures using two (or multiple) $\\phi$-divergence functions, which\nenables us to provide uncertainty sets that are tailored to both the\nsensitivity of each risk measure to tail losses and the tail behavior of the\nnominal distribution. In addition, our formulation allows for a tractable\ncomputation of robust risk measures, and elicitation of $\\phi$-divergences that\ndescribe a decision maker's risk and ambiguity preferences.",
      "generated_abstract": "We consider the problem of designing a single-agent learning algorithm that\nis capable of learning to maximize a utility function under a monotone\nconstraint on the learning rate. We show that this problem can be reduced to\nthe problem of designing a learning algorithm that is capable of learning to\nmaximize a monotone utility function under a constraint on the learning rate.\nWe then use a strategy based on the approach in \\cite{MV16} to show that this\nproblem can be solved in polynomial time. This result extends a result in\n\\cite{BZ17} which shows that the problem is NP-hard. We provide a full\nanalysis of the approximation guarantee for the algorithm that is designed in\nthe paper and show that it is asymptotically optimal.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.2857142857142857,
          "f": 0.23529411280276827
        },
        "rouge-2": {
          "r": 0.02586206896551724,
          "p": 0.03333333333333333,
          "f": 0.029126208671883192
        },
        "rouge-l": {
          "r": 0.15,
          "p": 0.21428571428571427,
          "f": 0.17647058339100358
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.08902v1",
      "true_abstract": "Mutual Information (MI) is a crucial measure for capturing dependencies\nbetween variables, but exact computation is challenging in high dimensions with\nintractable likelihoods, impacting accuracy and robustness. One idea is to use\nan auxiliary neural network to train an MI estimator; however, methods based on\nthe empirical distribution function (EDF) can introduce sharp fluctuations in\nthe MI loss due to poor out-of-sample performance, destabilizing convergence.\nWe present a Bayesian nonparametric (BNP) solution for training an MI estimator\nby constructing the MI loss with a finite representation of the Dirichlet\nprocess posterior to incorporate regularization in the training process. With\nthis regularization, the MI loss integrates both prior knowledge and empirical\ndata to reduce the loss sensitivity to fluctuations and outliers in the sample\ndata, especially in small sample settings like mini-batches. This approach\naddresses the challenge of balancing accuracy and low variance by effectively\nreducing variance, leading to stabilized and robust MI loss gradients during\ntraining and enhancing the convergence of the MI approximation while offering\nstronger theoretical guarantees for convergence. We explore the application of\nour estimator in maximizing MI between the data space and the latent space of a\nvariational autoencoder. Experimental results demonstrate significant\nimprovements in convergence over EDF-based methods, with applications across\nsynthetic and real datasets, notably in 3D CT image generation, yielding\nenhanced structure discovery and reduced overfitting in data synthesis. While\nthis paper focuses on generative models in application, the proposed estimator\nis not restricted to this setting and can be applied more broadly in various\nBNP learning procedures.",
      "generated_abstract": "In the context of active learning, we consider a set of $n$ instances, where\neach instance is associated with a target variable. We wish to select a\nsubset of instances that maximizes the expected gain in classification\nperformance. We show that this problem can be reformulated as an optimization\nproblem with a non-convex objective function. To address this non-convexity, we\npropose a novel method based on the Generalized Lagrangian Duality Theorem.\nWe derive a closed-form solution for the dual variables and derive an\napproximate solution by using an iterative procedure. We validate our approach\non synthetic data and real data sets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14457831325301204,
          "p": 0.3157894736842105,
          "f": 0.19834710312956771
        },
        "rouge-2": {
          "r": 0.024793388429752067,
          "p": 0.06060606060606061,
          "f": 0.035190611715070014
        },
        "rouge-l": {
          "r": 0.14457831325301204,
          "p": 0.3157894736842105,
          "f": 0.19834710312956771
        }
      }
    },
    {
      "paper_id": "cs.DC.cs/DC/2503.09318v1",
      "true_abstract": "Modern data analytics requires a huge amount of computing power and processes\na massive amount of data. At the same time, the underlying computing platform\nis becoming much more heterogeneous on both hardware and software. Even though\nspecialized hardware, e.g., FPGA- or GPU- or TPU-based systems, often achieves\nbetter performance than a CPU-only system due to the slowing of Moore's law,\nsuch systems are limited in what they can do. For example, GPU-only approaches\nsuffer from severe IO limitations. To truly exploit the potential of hardware\nheterogeneity, we present FpgaHub, an FPGA-centric hyper-heterogeneous\ncomputing platform for big data analytics. The key idea of FpgaHub is to use\nreconfigurable computing to implement a versatile hub complementing other\nprocessors (CPUs, GPUs, DPUs, programmable switches, computational storage,\netc.). Using an FPGA as the basis, we can take advantage of its highly\nreconfigurable nature and rich IO interfaces such as PCIe, networking, and\non-board memory, to place it at the center of the architecture and use it as a\ndata and control plane for data movement, scheduling, pre-processing, etc.\nFpgaHub enables architectural flexibility to allow exploring the rich design\nspace of heterogeneous computing platforms.",
      "generated_abstract": "The growing number of autonomous vehicles (AVs) in urban areas poses new\nchallenges to data privacy. While traditional data privacy mechanisms, such as\nleakage-based anonymization, can effectively protect personal information,\nthey often introduce significant privacy costs. This paper proposes a new\napproach for data privacy in AVs based on differentially private data\naggregation. By leveraging the differential privacy framework, the paper\nintroduces a novel privacy-preserving approach for data aggregation in AVs,\nenabling data aggregation while preserving privacy. Our approach uses the\ndifferential privacy principle to ensure that no individual information is\nleaked. We evaluate our approach through extensive simulations and find that\nour approach significantly reduces privacy leakage while preserving privacy\nconsistency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11678832116788321,
          "p": 0.20253164556962025,
          "f": 0.14814814350865926
        },
        "rouge-2": {
          "r": 0.010582010582010581,
          "p": 0.02,
          "f": 0.013840825924020587
        },
        "rouge-l": {
          "r": 0.10218978102189781,
          "p": 0.17721518987341772,
          "f": 0.12962962499014077
        }
      }
    },
    {
      "paper_id": "math.OC.math/OC/2503.10164v1",
      "true_abstract": "This paper addresses the safety challenges in impulsive systems, where abrupt\nstate jumps introduce significant complexities into system dynamics. A unified\nframework is proposed by integrating Quadratic Programming (QP), Control\nBarrier Functions (CBFs), and adaptive gain mechanisms to ensure system safety\nduring impulsive events. The CBFs are constructed to enforce safety constraints\nby capturing the system's continuous dynamics and the effects of impulsive\nstate transitions. An adaptive gain mechanism dynamically adjusts control\ninputs based on the magnitudes of the impulses and the system's proximity to\nsafety boundaries, maintaining safety during instantaneous state jumps. A\ntailored QP formulation incorporates CBFs constraints and adaptive gain\nadjustments, optimizing control inputs while ensuring compliance with\nsafety-critical requirements. Theoretical analysis establishes the boundedness,\ncontinuity, and feasibility of the adaptive gain and the overall framework. The\neffectiveness of the method is demonstrated through simulations on a robotic\nmanipulator, showcasing its practical applicability to impulsive systems with\nstate jumps.",
      "generated_abstract": "uce a new notion of equimultiplicity of a complex linear space\n$V$ with a finite dimensional complex vector space $V_0$, generalizing the\nwell-known notion of equimultiplicity of a complex linear space $V$ with a\nfinite dimensional complex vector space $V_0$ as a subspace of the tensor\nproduct $V\\otimes V_0$. We prove that the equimultiplicity of $V$ with $V_0$\nis a natural generalization of the equimultiplicity of $V$ with $V_0$ as a\nsubspace of the tensor product. Moreover, we show that equimultiplicity of $V$\nwith $V_0$ is preserved under the action of the group of invertible endomorphisms\nof $V$ and in particular the group of invertible endomorphisms of $V_0$. We\nalso show that if $",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0707070707070707,
          "p": 0.15217391304347827,
          "f": 0.0965517198059455
        },
        "rouge-2": {
          "r": 0.007246376811594203,
          "p": 0.014285714285714285,
          "f": 0.00961538014978018
        },
        "rouge-l": {
          "r": 0.0707070707070707,
          "p": 0.15217391304347827,
          "f": 0.0965517198059455
        }
      }
    },
    {
      "paper_id": "math.AG.math/DG/2503.09195v1",
      "true_abstract": "Refined algebraic domains are regions in the plane surrounded by finitely\nmany non-singular real algebraic curves which may intersect with normal\ncrossing. We are interested in shapes of such regions with surrounding real\nalgebraic curves. Poincar'e-Reeb Graphs of them are graphs the regions\nnaturally collapse to respecting the projection to a straight line. Such graphs\nwere first formulated by Sorea, for example, around 2020, and regions\nsurrounded by mutually disjoint non-singular real algebraic curves were mainly\nconsidered. The author has generalized the studies to several general\nsituations.\n  We find classes of such objects defined inductively by adding curves. We\nrespect characteristic finite sets in the curves. We consider regions\nsurrounded by the curves and of a new type. We investigate geometric properties\nand combinatorial ones of them and discuss important examples. We also\npreviously studied explicit classes defined inductively in this way and review\nthem.",
      "generated_abstract": "We introduce the notion of a localization of a category with respect to a\nseparable subset of objects and study its structure. We show that a category\nwithin a localized category is localized and that the localizations of a\ncategory are the localizations of its localized category. Furthermore, we\nintroduce the notion of a localization of a functor and show that it is\nisomorphic to the localization of the corresponding functor. We also show that\nthe localizations of a functor within a localized category are the localizations\nof its localized category. We apply the localization theory to the category\n$\\mathrm{Perf}(\\mathbb{Z})$ and the category $\\mathrm{Mod}(\\mathbb{Z}_2)$ of\nperfect complexes and complexes of odd-dimensional $\\mathbb{Z}_2$-modules,\nrespectively.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12359550561797752,
          "p": 0.2682926829268293,
          "f": 0.16923076491242617
        },
        "rouge-2": {
          "r": 0.023076923076923078,
          "p": 0.03896103896103896,
          "f": 0.028985502574156507
        },
        "rouge-l": {
          "r": 0.11235955056179775,
          "p": 0.24390243902439024,
          "f": 0.15384614952781075
        }
      }
    },
    {
      "paper_id": "math.NA.cs/NA/2503.10402v1",
      "true_abstract": "We explore a family of numerical methods, based on the Steffensen divided\ndifference iterative algorithm, that do not evaluate the derivative of the\nobjective functions. The family of methods achieves second-order convergence\nwith two function evaluations per iteration with marginal additional\ncomputational cost. An important side benefit of the method is the improvement\nin stability for different initial conditions compared to the vanilla\nSteffensen method. We present numerical results for scalar functions, fields,\nand scalar fields. This family of methods outperforms the Steffensen method\nwith respect to standard quantitative metrics in most cases.",
      "generated_abstract": "t development of multiscale models, which combine spatial and\ntime dimensions into a unified framework, has revolutionized the study of\ndiffusion processes in different settings. These models are particularly\nappropriate for the study of the growth of large cells, and their study\nrequires the development of numerical algorithms to solve the resulting\ndiffusion equations. In this paper, we propose a new iterative algorithm for\nsolving the equation for the growth of a cell in a 3D domain. Our approach\nrelies on the use of a stencil operator, which is obtained by applying a\ndiscretization of the Laplace operator to a multiscale model, which is a\nmultiscale version of the diffusion equation. The algorithm is based on the\niterative construction of a new stencil operator, which is then used to solve\nthe equation. The proposed algorithm is tested on various examples, including\na three-dimensional model for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20588235294117646,
          "p": 0.17073170731707318,
          "f": 0.18666666171022236
        },
        "rouge-2": {
          "r": 0.046511627906976744,
          "p": 0.03278688524590164,
          "f": 0.03846153361131718
        },
        "rouge-l": {
          "r": 0.19117647058823528,
          "p": 0.15853658536585366,
          "f": 0.17333332837688903
        }
      }
    },
    {
      "paper_id": "math.OA.math/FA/2503.09548v1",
      "true_abstract": "Let $\\Gamma$ be a countable discrete group. We say that $\\Gamma$ has\n$C^*$-invariant subalgebra rigidity (ISR) property if every $\\Gamma$-invariant\n$C^*$-subalgebra $\\mathcal{A}\\le C_r^*(\\Gamma)$ is of the form $C_r^*(N)$ for\nsome normal subgroup $N\\triangleleft\\Gamma$. We show that all torsion-free,\nnon-amenable (cylindrically) hyperbolic groups with property-AP and a finite\ndirect product of such groups have this property. We also prove that an\ninfinite group $\\Gamma$ has the C$^*$-ISR property only if $\\Gamma$ is simple\namenable or $C^*$-simple.",
      "generated_abstract": "We prove that the set of all $\\omega$-algebras is a compact Hausdorff space\nunder the topology of the weak-$^*$ convergence on the space of all $\\omega\n$-algebras. As a consequence, we obtain a compactification of the space of\nall $\\omega$-algebras. We apply our result to the study of the topology of\n$\\omega$-algebras generated by a certain class of finite subsets of $\\omega$\nand show that, in the case of a countable set $\\omega$, the topology of the\nspace of all $\\omega$-algebras generated by the countable set of all finite\nsubsets of $\\omega$ is a subspace of the topology of the space of all $\\omega\n$-algebras.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20689655172413793,
          "p": 0.26666666666666666,
          "f": 0.2330097038175135
        },
        "rouge-2": {
          "r": 0.0410958904109589,
          "p": 0.04285714285714286,
          "f": 0.04195803696024315
        },
        "rouge-l": {
          "r": 0.1896551724137931,
          "p": 0.24444444444444444,
          "f": 0.2135922280893582
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2501.15828v4",
      "true_abstract": "Recovery rate prediction plays a pivotal role in bond investment strategies,\nenhancing risk assessment, optimizing portfolio allocation, improving pricing\naccuracy, and supporting effective credit risk management. However, forecasting\nfaces challenges like high-dimensional features, small sample sizes, and\noverfitting. We propose a hybrid Quantum Machine Learning model incorporating\nParameterized Quantum Circuits (PQC) within a neural network framework. PQCs\ninherently preserve unitarity, avoiding computationally costly orthogonality\nconstraints, while amplitude encoding enables exponential data compression,\nreducing qubit requirements logarithmically. Applied to a global dataset of\n1,725 observations (1996-2023), our method achieved superior accuracy (RMSE\n0.228) compared to classical neural networks (0.246) and quantum models with\nangle encoding (0.242), with efficient computation times. This work highlights\nthe potential of hybrid quantum-classical architectures in advancing recovery\nrate forecasting.",
      "generated_abstract": "the optimal distribution of risks in the presence of information\nrisk, i.e., the risk of incorrect information. In this setting, we assume that\nthe risk of incorrect information can be modeled by a risk distribution and the\nrisk of not knowing the correct information is modeled by a risk distribution\nthat is the complement of the risk distribution of incorrect information. We\nshow that the optimal distribution of risks depends on the distribution of\ninformation risk. We provide examples showing that the optimal distribution of\nrisks depends on the distribution of risk that is modeled by the optimal\ndistribution of risks. We further show that the optimal distribution of risks\ndepends on the distribution of risk that is modeled by the optimal distribution\nof risks. We further show that the optimal distribution of risks depends on the\ndistribution of risk that is modeled by the optimal distribution of risks. We\nalso show that the optimal distribution of risks",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06422018348623854,
          "p": 0.175,
          "f": 0.09395972761587333
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.06422018348623854,
          "p": 0.175,
          "f": 0.09395972761587333
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.00732v1",
      "true_abstract": "In conventional approaches for multiobject tracking (MOT), raw sensor data\nundergoes several preprocessing stages to reduce data rate and computational\ncomplexity. This typically includes coherent processing that aims at maximizing\nthe signal-to-noise ratio (SNR), followed by a detector that extracts \"point\"\nmeasurements, e.g., the range and bearing of objects, which serve as inputs for\nsequential Bayesian MOT. While using point measurements significantly\nsimplifies the statistical model, the reduced data rate can lead to a loss of\ncritical, object-related information and, thus, potentially to reduced tracking\nperformance. In this paper, we propose a direct tracking approach that avoids a\ndetector and most preprocessing stages. For direct tracking, we introduce a\nmeasurement model for the data-generating process of the sensor data, along\nwith state-transition and birth models for the dynamics and the appearance and\ndisappearance of objects. Based on the new statistical model, we develop a\nfactor graph and particle-based belief propagation (BP) method for efficient\nsequential Bayesian estimation. Contrary to the track-before-detect (TBD)\nparadigm which also avoids a detector, direct tracking integrates coherent\nprocessing within the Bayesian MOT framework. Numerical experiments based on a\npassive acoustic dataset demonstrate that the proposed direct approach\noutperforms state-of-the-art conventional methods that rely on multiple\npreprocessing stages.",
      "generated_abstract": "This paper addresses the challenges in wireless multiuser detection (MUD)\nfor multipath channels in a cognitive radio network (CRN) with non-orthogonal\nmultiplexing (NOMA). The MUD problem is formulated as a constrained\noptimization problem with the constraints of the total number of detected\nusers and the maximum number of interference sources per user. To address the\nnon-orthogonality of the received signals, we propose a novel framework that\ncombines the non-orthogonality and the interference sources constraints in a\nunified manner. Specifically, we first formulate the problem as a linear\nprogramming (LP) problem, and then apply the convex transformation to relax\nthe non-orthogonality constraints and the interference sources constraints.\nThe optimal solution is then obtained by solving the relaxed LP problem. The\nproposed framework is validated through simulation results.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11278195488721804,
          "p": 0.1875,
          "f": 0.14084506573210798
        },
        "rouge-2": {
          "r": 0.020833333333333332,
          "p": 0.035398230088495575,
          "f": 0.0262295035321696
        },
        "rouge-l": {
          "r": 0.10526315789473684,
          "p": 0.175,
          "f": 0.13145539437060563
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.17600v1",
      "true_abstract": "Providing wellbeing for all while safeguarding planetary boundaries may\nrequire governments to pursue post-growth policies. Previous empirical studies\nof sustainable wellbeing initiatives investigating enablers of and barriers to\npost-growth policymaking are either based on a small number of empirical cases\nor lack an explicit analytical framework. To better understand how post-growth\npolicymaking could be fostered, we investigate 29 initiatives across governance\nscales in Europe, New Zealand, and Canada. We apply a framework that\ndistinguishes polity, politics, and policy to analyze the data. We find that\nthe main enablers and barriers relate to the economic growth paradigm, the\norganization of government, attitudes towards policymaking, political\nstrategies, and policy tools and outcomes. Engaging in positive framings of\npost-growth visions to change narratives and building broad-based alliances\ncould act as drivers. However, initiatives face a tension between the need to\nconnect to broad audiences and a risk of co-optation by depolitization.",
      "generated_abstract": "y examines the impact of the COVID-19 pandemic on the labor\nmarket and examines the effects of the COVID-19 pandemic on the employment\nprospects of different groups of workers, including women, youth, and\ndisadvantaged groups. We use a sample of 10,400 adults in 41 countries to\ninvestigate the impact of COVID-19 on employment prospects of these groups.\nOur findings show that the COVID-19 pandemic has a negative impact on\nemployment prospects for women and youth, as well as on employment prospects\nfor disadvantaged groups. However, the impact of COVID-19 on employment\nprospects of workers with a higher education degree is more positive. We also\nfind that the COVID-19 pandemic has a positive impact on employment prospects\nfor those who were previously employed before",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14018691588785046,
          "p": 0.25,
          "f": 0.17964071395890863
        },
        "rouge-2": {
          "r": 0.013888888888888888,
          "p": 0.022727272727272728,
          "f": 0.017241374601665967
        },
        "rouge-l": {
          "r": 0.1308411214953271,
          "p": 0.23333333333333334,
          "f": 0.16766466605471705
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.18714v1",
      "true_abstract": "The relationship of policy choice by majority voting and by maximization of\nutilitarian welfare has long been discussed. I consider choice between a status\nquo and a proposed policy when persons have interpersonally comparable cardinal\nutilities taking values in a bounded interval, voting is compulsory, and each\nperson votes for a policy that maximizes utility. I show that knowledge of the\nattained status quo welfare and the voting outcome yields an informative bound\non welfare with the proposed policy. The bound contains the value of status quo\nwelfare, so the better utilitarian policy is not known. The minimax-regret\ndecision and certain Bayes decisions choose the proposed policy if its vote\nshare exceeds the known value of status quo welfare. This procedure differs\nfrom majority rule, which chooses the proposed policy if its vote share exceeds\n1/2.",
      "generated_abstract": "We provide a simple and intuitive argument for the well-known fact that\nthe Bayesian update rule is not equivalent to the classical update rule.\nBased on this observation, we introduce a novel update rule, the\n{\\em conditional-expectation update rule}, which we show is equivalent to the\nclassical update rule. We further provide a simple and intuitive argument for\nthe equivalence between the Bayesian update rule and the conditional-expectation\nupdate rule. We show that, under certain conditions, the Bayesian update rule\nimplies the conditional-expectation update rule, and vice versa. Furthermore,\nwe show that the Bayesian update rule is equivalent to the update rule under\nwhich the conditional-expectation update rule is maximized. This provides a\nsimple and intuitive derivation of the equivalence between the Bayesian update\nrule and the update rule under which the conditional-expectation update rule\nis maximized.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18518518518518517,
          "p": 0.30612244897959184,
          "f": 0.23076922607218944
        },
        "rouge-2": {
          "r": 0.03361344537815126,
          "p": 0.05263157894736842,
          "f": 0.0410256362687711
        },
        "rouge-l": {
          "r": 0.1728395061728395,
          "p": 0.2857142857142857,
          "f": 0.21538461068757406
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.08337v1",
      "true_abstract": "This paper provides a discretization-free solution to the synthesis of\napprox-imation-free closed-form controllers for unknown nonlinear systems to\nenforce complex properties expressed by $\\omega$-regular languages, as\nrecognized by Non-deterministic B\\\"uchi Automata (NBA). In order to solve this\nproblem, we first decompose NBA into a sequence of reach-avoid problems, which\nare solved using the Spatiotemporal Tubes (STT) approach. Controllers for each\nreach-avoid task are then integrated into a hybrid policy that ensures the\nfulfillment of the desired $\\omega$-regular properties. We validate our method\nthrough omnidirectional robot navigation and manipulator control case studies.",
      "generated_abstract": "r presents a novel method for optimal control of a multi-agent\nsystem consisting of both a target and a target-following agent, where the\ntarget-following agent's goal is to minimize a cost function while ensuring the\ntarget's safety. The multi-agent system is modeled as a stochastic linear\nquadratic Gaussian (LQG) control problem, and its optimal control problem is\nformulated as an infinite-horizon optimal control problem with a linear\ndiscrete-time model. To overcome the challenges associated with the infinite\nhorizon, we develop a time-shifting method that first constructs a finite-horizon\ncontrol problem for the target-following agent, and then formulates the\ninfinite-horizon problem as a linear discrete-time model with a Lyapunov\nfunction. The optimal control problem is then formulated as a quadratic\noptimization problem, which can be",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.22388059701492538,
          "f": 0.2112676006496728
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.22388059701492538,
          "f": 0.2112676006496728
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2502.13325v1",
      "true_abstract": "In this paper, we consider catastrophe stop-loss reinsurance valuation for a\nreinsurance company with dynamic contagion claims. To deal with conventional\nand emerging catastrophic events, we propose the use of a compound dynamic\ncontagion process for the catastrophic component of the liability. Under the\npremise that there is an absence of arbitrage opportunity in the market, we\nobtain arbitrage-free premiums for these contacts. To this end, the Esscher\ntransform is adopted to specify an equivalent martingale probability measure.\nWe show that reinsurers have various ways of levying the security loading on\nthe net premiums to quantify the catastrophic liability in light of the growing\nchallenges posed by emerging risks arising from climate change, cyberattacks,\nand pandemics. We numerically compare arbitrage-free catastrophe stop-loss\nreinsurance premiums via the Monte Carlo simulation method. Sensitivity\nanalyzes are performed by changing the Esscher parameters and the retention\nlevel.",
      "generated_abstract": "r introduces a new approach to quantitative finance, based on\nthe notion of stochastic volatility as a nonlinear function of the price\nprocess and the option price. This approach is based on the idea that the\nprice process and the option price are closely related, and the former is a\nstochastic version of the latter. The main focus of this paper is to explore\nthe properties of the stochastic volatility function and its applications to\nthe pricing of financial derivatives. We first review the theory of\nstochastic volatility and its connection to option pricing. We then present\nthe main results of our analysis, including the existence of the solution to\nthe linear stochastic volatility equation, the existence of the solution to\nthe nonlinear stochastic volatility equation, and the properties of the\nsolutions. We also derive the solutions of the linear and nonlinear stochastic\nvolatility equations using the HJB",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.18461538461538463,
          "f": 0.14906831816673755
        },
        "rouge-2": {
          "r": 0.022058823529411766,
          "p": 0.026785714285714284,
          "f": 0.02419354343392401
        },
        "rouge-l": {
          "r": 0.10416666666666667,
          "p": 0.15384615384615385,
          "f": 0.12422359766984317
        }
      }
    },
    {
      "paper_id": "math.AC.math/HO/2502.13273v1",
      "true_abstract": "We give a new proof of the fundamental theorem of algebra. It is entirely\nelementary, focused on using long division to its fullest extent. Further, the\nmethod quickly recovers a more general version of the theorem recently obtained\nby Joseph Shipman.",
      "generated_abstract": "In this paper, we construct an action of a non-amenable group $G$ on a\nfinite CW-complex $X$ such that the induced action of $G$ on $X$ is not\namenable. In particular, we show that this cannot be achieved by a finite\nCW-decomposition of $X$ and a proper action of $G$. This is a generalization\nof a result of Fay and Rigal in which they show that the action of the\nhyperbolic group $H$ on the annulus $S^1\\times S^1$ cannot be obtained by a\nfinite CW-decomposition of $S^1\\times S^1$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.15217391304347827,
          "f": 0.1728395012650512
        },
        "rouge-2": {
          "r": 0.05128205128205128,
          "p": 0.027777777777777776,
          "f": 0.036036031477965025
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.15217391304347827,
          "f": 0.1728395012650512
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.08755v1",
      "true_abstract": "Combining the technique of employing coset codes for communicating over a\nquantum broadcast channel and the recent discovery of \\textit{tilting,\nsmoothing and augmentation} by Sen to perform simultaneous decoding over\nnetwork quantum channels, we derive new inner bounds to the capacity region of\na $3-$user classical quantum broadcast channel that subsumes all known.",
      "generated_abstract": "a of 5G and beyond, there is a growing demand for low-power and\nelectronic devices with high-speed communication capabilities, such as wireless\ntransceiver modules. In this context, the challenge of energy efficiency has\nbecome a significant concern. In this paper, we propose a novel technique for\nenergy-efficient communication systems using an integrated approach. First, we\nintroduce a novel technique that allows for a highly energy-efficient\ncommunication system by optimizing the modulation and coding schemes to\nmaximize the signal-to-noise ratio. Then, we propose a novel technique that\ncombines a low-complexity power control algorithm with a power-aware\ntransmission scheme to further reduce the power consumption of the system.\nFinally, we provide an energy-aware channel estimation algorithm that\noptimizes the channel estimation process based on the energy efficiency\nmeasures. Our simulation results show that the proposed techniques reduce",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2682926829268293,
          "p": 0.12643678160919541,
          "f": 0.17187499564575207
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.24390243902439024,
          "p": 0.11494252873563218,
          "f": 0.1562499956457521
        }
      }
    },
    {
      "paper_id": "cs.DC.cs/PF/2503.03274v1",
      "true_abstract": "Ensuring Service Level Objectives (SLOs) in large-scale architectures, such\nas Distributed Computing Continuum Systems (DCCS), is challenging due to their\nheterogeneous nature and varying service requirements across different devices\nand applications. Additionally, unpredictable workloads and resource\nlimitations lead to fluctuating performance and violated SLOs. To improve SLO\ncompliance in DCCS, one possibility is to apply machine learning; however, the\ndesign choices are often left to the developer. To that extent, we provide a\nbenchmark of Active Inference -- an emerging method from neuroscience --\nagainst three established reinforcement learning algorithms (Deep Q-Network,\nAdvantage Actor-Critic, and Proximal Policy Optimization). We consider a\nrealistic DCCS use case: an edge device running a video conferencing\napplication alongside a WebSocket server streaming videos. Using one of the\nrespective algorithms, we continuously monitor key performance metrics, such as\nlatency and bandwidth usage, to dynamically adjust parameters -- including the\nnumber of streams, frame rate, and resolution -- to optimize service quality\nand user experience. To test algorithms' adaptability to constant system\nchanges, we simulate dynamically changing SLOs and both instant and gradual\ndata-shift scenarios, such as network bandwidth limitations and fluctuating\ndevice thermal states. Although the evaluated algorithms all showed advantages\nand limitations, our findings demonstrate that Active Inference is a promising\napproach for ensuring SLO compliance in DCCS, offering lower memory usage,\nstable CPU utilization, and fast convergence.",
      "generated_abstract": "We present a novel framework for learning and reasoning about complex\ndomains from limited data. The framework is based on a novel learning algorithm\ncalled Uncertainty-Aware Context-Aware Adaptive Network (U-CAN). Uncertainty\nis a fundamental aspect of complex systems, and it can be expressed by\ncontext-aware adaptation of network architectures. In U-CAN, context-aware\nadaptation of a network architecture is performed by learning a weighted\nrepresentation of the context, which is called the uncertainty representation.\nThe network architecture is then learned by optimizing a loss function that\npromotes the learning of the uncertainty representation. The framework allows\nfor learning complex domains with limited data, and its effectiveness has been\ndemonstrated by benchmarks on different complex domains. The framework is\navailable at https://github.com/Sarah-Fitzgerald/U-CAN.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0736196319018405,
          "p": 0.16901408450704225,
          "f": 0.10256409833698608
        },
        "rouge-2": {
          "r": 0.009216589861751152,
          "p": 0.01904761904761905,
          "f": 0.012422355853363692
        },
        "rouge-l": {
          "r": 0.06134969325153374,
          "p": 0.14084507042253522,
          "f": 0.08547008124296901
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2503.08272v1",
      "true_abstract": "Monotone mean-variance (MMV) utility is the minimal modification of the\nclassical Markowitz utility that respects rational ordering of investment\nopportunities. This paper provides, for the first time, a complete\ncharacterization of optimal dynamic portfolio choice for the MMV utility in\nasset price models with independent returns. The task is performed under\nminimal assumptions, weaker than the existence of an equivalent martingale\nmeasure and with no restrictions on the moments of asset returns. We interpret\nthe maximal MMV utility in terms of the monotone Sharpe ratio (MSR) and show\nthat the global squared MSR arises as the nominal yield from continuously\ncompounding at the rate equal to the maximal local squared MSR. The paper gives\nsimple necessary and sufficient conditions for mean-variance (MV) efficient\nportfolios to be MMV efficient. Several illustrative examples contrasting the\nMV and MMV criteria are provided.",
      "generated_abstract": "y provides a theoretical framework for understanding the\ndistribution of returns of equity funds in India. It considers a\ndynamic-market-neutral strategy that aims to capture the entire distribution\nof returns across different equity funds. The framework is based on the\nmean-variance optimization framework, which seeks to balance the risk and return\ngoals of the strategy. We find that the returns of the equity funds are\ndistributed in two distinct regimes: a low-volatility regime and a high-volatility\nregime. The low-volatility regime has a positive mean and a negative variance,\nwhile the high-volatility regime has a negative mean and a positive variance.\nThe returns of the equity funds are also heterogeneous across different funds.\nOur findings suggest that the strategy can capture the entire distribution of\nreturns across different equity funds, while also mitigating excessive risk.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15625,
          "p": 0.22727272727272727,
          "f": 0.18518518035665305
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.029411764705882353,
          "f": 0.025641020723209357
        },
        "rouge-l": {
          "r": 0.15625,
          "p": 0.22727272727272727,
          "f": 0.18518518035665305
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/SI/2503.07961v1",
      "true_abstract": "Although hypergraph neural networks (HGNNs) have emerged as a powerful\nframework for analyzing complex datasets, their practical performance often\nremains limited. On one hand, existing networks typically employ a single type\nof attention mechanism, focusing on either structural or feature similarities\nduring message passing. On the other hand, assuming that all nodes in current\nhypergraph models have the same level of overlap may lead to suboptimal\ngeneralization. To overcome these limitations, we propose a novel framework,\noverlap-aware meta-learning attention for hypergraph neural networks\n(OMA-HGNN). First, we introduce a hypergraph attention mechanism that\nintegrates both structural and feature similarities. Specifically, we linearly\ncombine their respective losses with weighted factors for the HGNN model.\nSecond, we partition nodes into different tasks based on their diverse overlap\nlevels and develop a multi-task Meta-Weight-Net (MWN) to determine the\ncorresponding weighted factors. Third, we jointly train the internal MWN model\nwith the losses from the external HGNN model and train the external model with\nthe weighted factors from the internal model. To evaluate the effectiveness of\nOMA-HGNN, we conducted experiments on six real-world datasets and benchmarked\nits perfor-mance against nine state-of-the-art methods for node classification.\nThe results demonstrate that OMA-HGNN excels in learning superior node\nrepresentations and outperforms these baselines.",
      "generated_abstract": "ss of machine learning (ML) in many fields relies on data. Data\nscalability is a critical concern as ML models are increasingly used for\nlarge-scale tasks, such as speech recognition. Traditional data storage\nformats, such as the HDF5 format, are not scalable for large-scale data\nstores. In this paper, we propose a new data storage format, named HDFS-ML,\nwhich can store large-scale data with minimal overhead. The key idea is to\ndistribute the data into small pieces, compress them, and store them in a\ndistributed file system (DFS). We evaluate HDFS-ML on three datasets: a\ncomprehensive comparison of HDF5 and HDFS-ML on various tasks demonstrates\nthat HDFS-ML achieves similar or better performance. Additionally, we propose a\nmethod for training a large-scale speech recognition model using HDF",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16417910447761194,
          "p": 0.24444444444444444,
          "f": 0.19642856662149247
        },
        "rouge-2": {
          "r": 0.010416666666666666,
          "p": 0.017094017094017096,
          "f": 0.01294497911333316
        },
        "rouge-l": {
          "r": 0.1417910447761194,
          "p": 0.2111111111111111,
          "f": 0.1696428523357782
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/OT/2405.01342v1",
      "true_abstract": "Economic policy sciences are constantly investigating the quality of\nwell-being of broad sections of the population in order to describe the current\ninterdependence between unequal living conditions, low levels of education and\na lack of integration into society. Such studies are often carried out in the\nform of surveys, e.g. as part of the EU-SILC program. If the survey is designed\nat national or international level, the results of the study are often used as\na reference by a broad range of public institutions. However, the sampling\nstrategy per se may not capture enough information to provide an accurate\nrepresentation of all population strata. Problems might arise from rare, or\nhard-to-sample, populations and the conclusion of the study may be compromised\nor unrealistic. We propose here a two-phase methodology to identify rare,\npoorly sampled populations and then resample the hard-to-sample strata. We\nfocused our attention on the 2019 EU-SILC section concerning the Italian region\nof Liguria. Methods based on dispersion indices or deep learning were used to\ndetect rare populations. A multi-frame survey was proposed as the sampling\ndesign. The results showed that factors such as citizenship, material\ndeprivation and large families are still fundamental characteristics that are\ndifficult to capture.",
      "generated_abstract": "y examines the influence of age on the likelihood of a patient\nbeing referred to a hospital for a procedure, and whether the age of the\npatient influences the likelihood of the procedure being performed. We also\nexamine whether there is a difference in the likelihood of the procedure being\nperformed between patients aged 18 years and above and those aged 17 years\nand below. The data used in this study were from the National Healthcare\nSurveys. The analysis involved a multivariate logistic regression model.\nResults indicate that the likelihood of a patient being referred to a\nhospital for a procedure is significantly higher for patients aged 18 years and\nabove than for patients aged 17 years and below. A higher likelihood of\nperformance of a procedure is also found among patients aged 18 years and above\ncompared to those aged 17 years and below. The likelihood",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12318840579710146,
          "p": 0.265625,
          "f": 0.16831682735418108
        },
        "rouge-2": {
          "r": 0.015463917525773196,
          "p": 0.030303030303030304,
          "f": 0.020477811225291895
        },
        "rouge-l": {
          "r": 0.10869565217391304,
          "p": 0.234375,
          "f": 0.14851484715616128
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.02073v1",
      "true_abstract": "Analyzing time-series cross-sectional (also known as longitudinal or panel)\ndata is an important process across a number of fields, including the social\nsciences, economics, finance, and medicine. PanelMatch is an R package that\nimplements a set of tools enabling researchers to apply matching methods for\ncausal inference with time-series cross-sectional data. Relative to other\ncommonly used methods for longitudinal analyses, like regression with fixed\neffects, the matching-based approach implemented in PanelMatch makes fewer\nparametric assumptions and offers more diagnostics. In this paper, we discuss\nthe PanelMatch package, showing users a recommended pipeline for doing causal\ninference analysis with it and highlighting useful diagnostic and visualization\ntools.",
      "generated_abstract": "The use of multiple linear regression (MLR) for complex multi-variate\ntransfer function (MTF) models is a well-established technique. However, the\nuse of a single MTL regression model for all variables is not recommended due\nto the possible limitations of the model fit. In this paper, we propose a\nnovel approach to the MTF modeling using a mixture of MLRs for the MTF\nvariables. The proposed method can be used to model a multi-variate MTF\nmodel that is composed of a mixture of MTL regression models. We provide a\nclosed-form solution to the MLR regression model in the case of a single MTL\nMLR model. Furthermore, the proposed approach is compared to the MLR method\nin the case of a mixture of MTL MLR models. The results show that the\nproposed approach performs well in terms of prediction accuracy and model\nfitting.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20987654320987653,
          "p": 0.2463768115942029,
          "f": 0.22666666169866678
        },
        "rouge-2": {
          "r": 0.0297029702970297,
          "p": 0.02586206896551724,
          "f": 0.02764976460914528
        },
        "rouge-l": {
          "r": 0.19753086419753085,
          "p": 0.2318840579710145,
          "f": 0.21333332836533342
        }
      }
    },
    {
      "paper_id": "astro-ph.IM.eess/IV/2503.00156v1",
      "true_abstract": "Neural posterior estimation (NPE), a type of amortized variational inference,\nis a computationally efficient means of constructing probabilistic catalogs of\nlight sources from astronomical images. To date, NPE has not been used to\nperform inference in models with spatially varying covariates. However,\nground-based astronomical images have spatially varying sky backgrounds and\npoint spread functions (PSFs), and accounting for this variation is essential\nfor constructing accurate catalogs of imaged light sources. In this work, we\nintroduce a method of performing NPE with spatially varying backgrounds and\nPSFs. In this method, we generate synthetic catalogs and semi-synthetic images\nfor these catalogs using randomly sampled PSF and background estimates from\nexisting surveys. Using this data, we train a neural network, which takes an\nastronomical image and representations of its background and PSF as input, to\noutput a probabilistic catalog. Our experiments with Sloan Digital Sky Survey\ndata demonstrate the effectiveness of NPE in the presence of spatially varying\nbackgrounds and PSFs for light source detection, star/galaxy separation, and\nflux measurement.",
      "generated_abstract": "t of high-bandwidth communications has enabled the development of\nlarge-scale distributed wireless networks, which offer a promising means of\novercoming the limitations of conventional distributed antenna systems (DAS) in\nterms of cost, reliability, and scalability. The emergence of the Internet of\nThings (IoT) has further motivated the development of distributed wireless\nnetworks, which offer the potential for energy-efficient communication in\nsensitive environments. This paper introduces a novel distributed wireless\nnetwork architecture that combines the advantages of DAS and IoT. This\narchitecture is based on a multi-antenna system with a distributed antenna\narray (DAA) at the base station (BS) and distributed antenna elements (DAEs) at\nthe users, offering a scalable solution for low-power IoT devices. We\ncharacterize the performance of the proposed architecture under various\nconditions, including the effects",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.102803738317757,
          "p": 0.13414634146341464,
          "f": 0.11640211148960017
        },
        "rouge-2": {
          "r": 0.006493506493506494,
          "p": 0.008620689655172414,
          "f": 0.0074074025064504315
        },
        "rouge-l": {
          "r": 0.102803738317757,
          "p": 0.13414634146341464,
          "f": 0.11640211148960017
        }
      }
    },
    {
      "paper_id": "cs.AI.q-fin/ST/2411.04788v1",
      "true_abstract": "In recent years, the application of generative artificial intelligence\n(GenAI) in financial analysis and investment decision-making has gained\nsignificant attention. However, most existing approaches rely on single-agent\nsystems, which fail to fully utilize the collaborative potential of multiple AI\nagents. In this paper, we propose a novel multi-agent collaboration system\ndesigned to enhance decision-making in financial investment research. The\nsystem incorporates agent groups with both configurable group sizes and\ncollaboration structures to leverage the strengths of each agent group type. By\nutilizing a sub-optimal combination strategy, the system dynamically adapts to\nvarying market conditions and investment scenarios, optimizing performance\nacross different tasks. We focus on three sub-tasks: fundamentals, market\nsentiment, and risk analysis, by analyzing the 2023 SEC 10-K forms of 30\ncompanies listed on the Dow Jones Index. Our findings reveal significant\nperformance variations based on the configurations of AI agents for different\ntasks. The results demonstrate that our multi-agent collaboration system\noutperforms traditional single-agent models, offering improved accuracy,\nefficiency, and adaptability in complex financial environments. This study\nhighlights the potential of multi-agent systems in transforming financial\nanalysis and investment decision-making by integrating diverse analytical\nperspectives.",
      "generated_abstract": "uce a novel approach to model the uncertainty associated with\nmarket data in deep learning for financial forecasting. Inspired by the\napproach in the recent literature, we focus on the use of deep neural networks\nto model the time-series data that characterize the market's evolution. We\ndemonstrate the efficacy of our approach through two case studies, where we\napply it to predict the daily return of the S&P 500 index for the period from\n2016 to 2023. The first case study explores the potential of our approach to\nmodel the uncertainty associated with the market data and, in the second\nstudy, we introduce a forecasting model that combines our approach with\ntraditional forecasting methods. Our results demonstrate that the use of our\napproach can improve the predictive performance of the traditional forecasting\nmethods. Furthermore, our approach allows for the prediction",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2076923076923077,
          "p": 0.35526315789473684,
          "f": 0.26213591767367334
        },
        "rouge-2": {
          "r": 0.040229885057471264,
          "p": 0.05982905982905983,
          "f": 0.04810996082757692
        },
        "rouge-l": {
          "r": 0.18461538461538463,
          "p": 0.3157894736842105,
          "f": 0.23300970408144037
        }
      }
    },
    {
      "paper_id": "cs.AR.cs/OS/2502.02349v1",
      "true_abstract": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times.",
      "generated_abstract": "of AI-driven applications in cloud environments has presented\nnew challenges to traditional operating systems (OSes). In this paper, we\npresent Dracula, an AI-powered OS designed to address these challenges. Dracula\ncombines AI with a customized, hardware-aware kernel, enabling it to adapt to\ndiverse workloads and resource demands. The AI component, known as Dracula\nAI, uses advanced machine learning techniques to automatically detect and\nclassify different workloads. The kernel, known as Dracula-OS, is customized\nto optimize performance for different workloads. Dracula-OS includes\nadaptive scheduling and resource management mechanisms, such as dynamic\nscheduling, dynamic resource allocation, and priority-based scheduling. It also\nprovides support for virtualization and containerization, enabling seamless\nintegration with existing virtual machine (VM",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.16470588235294117,
          "f": 0.15909090409672022
        },
        "rouge-2": {
          "r": 0.007874015748031496,
          "p": 0.009174311926605505,
          "f": 0.008474571300275825
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.15294117647058825,
          "f": 0.14772726773308387
        }
      }
    },
    {
      "paper_id": "math.ST.math/ST/2503.07022v1",
      "true_abstract": "For some discretely observed path of oscillating Brownian motion with level\nof self-organized criticality $\\rho_0$, we prove in the infill asymptotics that\nthe MLE is $n$-consistent, where $n$ denotes the sample size, and derive its\nlimit distribution with respect to stable convergence. As the transition\ndensity of this homogeneous Markov process is not even continuous in $\\rho_0$,\nthe analysis is highly non-standard. Therefore, interesting and somewhat\nunexpected phenomena occur: The likelihood function splits into several\ncomponents, each of them contributing very differently depending on how close\nthe argument $\\rho$ is to $\\rho_0$. Correspondingly, the MLE is successively\nexcluded to lay outside a compact set, a $1/\\sqrt{n}$-neighborhood and finally\na $1/n$-neigborhood of $\\rho_0$ asymptotically. The crucial argument to derive\nthe stable convergence is to exploit the semimartingale structure of the\nsequential suitably rescaled local log-likelihood function (as a process in\ntime). Both sequentially and as a process in $\\rho$, it exhibits a bivariate\nPoissonian behavior in the stable limit with its intensity being a multiple of\nthe local time at $\\rho_0$.",
      "generated_abstract": "In this paper, we study the asymptotic behavior of the first eigenvalue of\nthe Laplacian on the complete graph. We establish the explicit asymptotic\nbehavior of the first eigenvalue in terms of the second eigenvalue and the\nnumber of vertices of the graph. Our results extend the work of\nLiu-Wang-Zhao-Zhou, which deals with the case where the graph has a fixed\nnumber of vertices.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09734513274336283,
          "p": 0.2972972972972973,
          "f": 0.14666666295022232
        },
        "rouge-2": {
          "r": 0.00625,
          "p": 0.019230769230769232,
          "f": 0.009433958561767207
        },
        "rouge-l": {
          "r": 0.08849557522123894,
          "p": 0.2702702702702703,
          "f": 0.13333332961688898
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/FL/2502.19603v1",
      "true_abstract": "This work studies the planning problem for robotic systems under both\nquantifiable and unquantifiable uncertainty. The objective is to enable the\nrobotic systems to optimally fulfill high-level tasks specified by Linear\nTemporal Logic (LTL) formulas. To capture both types of uncertainty in a\nunified modelling framework, we utilise Markov Decision Processes with\nSet-valued Transitions (MDPSTs). We introduce a novel solution technique for\nthe optimal robust strategy synthesis of MDPSTs with LTL specifications. To\nimprove efficiency, our work leverages limit-deterministic B\\\"uchi automata\n(LDBAs) as the automaton representation for LTL to take advantage of their\nefficient constructions. To tackle the inherent nondeterminism in MDPSTs, which\npresents a significant challenge for reducing the LTL planning problem to a\nreachability problem, we introduce the concept of a Winning Region (WR) for\nMDPSTs. Additionally, we propose an algorithm for computing the WR over the\nproduct of the MDPST and the LDBA. Finally, a robust value iteration algorithm\nis invoked to solve the reachability problem. We validate the effectiveness of\nour approach through a case study involving a mobile robot operating in the\nhexagonal world, demonstrating promising efficiency gains.",
      "generated_abstract": "This paper addresses the problem of robot navigation in unknown environments\nwithout prior knowledge of the topology of the environment. The problem\nconsists of identifying the shortest path between two points in the environment,\nand avoiding obstacles. The problem is formulated as a graph problem and\nsolved through the use of a neural network, trained to solve the problem. The\nmodel is trained on a dataset of trajectories of a mobile robot, generated by\nsimulating the robot's navigation in an environment with obstacles. The model is\nevaluated on the dataset of trajectories, and the results demonstrate that the\nmodel performs well and can successfully navigate the robot through the unknown\nenvironment.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14634146341463414,
          "p": 0.2857142857142857,
          "f": 0.19354838261706564
        },
        "rouge-2": {
          "r": 0.03888888888888889,
          "p": 0.07,
          "f": 0.049999995408163696
        },
        "rouge-l": {
          "r": 0.13008130081300814,
          "p": 0.25396825396825395,
          "f": 0.17204300627297966
        }
      }
    },
    {
      "paper_id": "cs.CC.cs/CC/2503.05548v1",
      "true_abstract": "We study integer linear programs (ILP) of the form $\\min\\{c^\\top x\\ \\vert\\\nAx=b,l\\le x\\le u,x\\in\\mathbb Z^n\\}$ and analyze their parameterized complexity\nwith respect to their distance to the generalized matching problem--following\nthe well-established approach of capturing the hardness of a problem by the\ndistance to triviality. The generalized matching problem is an ILP where each\ncolumn of the constraint matrix has $1$-norm of at most $2$. It captures\nseveral well-known polynomial time solvable problems such as matching and flow\nproblems. We parameterize by the size of variable and constraint backdoors,\nwhich measure the least number of columns or rows that must be deleted to\nobtain a generalized matching ILP.\n  We present the following results: (i) a fixed-parameter tractable (FPT)\nalgorithm for ILPs parameterized by the size $p$ of a minimum variable backdoor\nto generalized matching; (ii) a randomized slice-wise polynomial (XP) time\nalgorithm for ILPs parameterized by the size $h$ of a minimum constraint\nbackdoor to generalized matching as long as $c$ and $A$ are encoded in unary;\n(iii) we complement (ii) by proving that solving an ILP is W[1]-hard when\nparameterized by $h$ even when $c,A,l,u$ have coefficients of constant size. To\nobtain (i), we prove a variant of lattice-convexity of the degree sequences of\nweighted $b$-matchings, which we study in the light of SBO jump M-convex\nfunctions. This allows us to model the matching part as a polyhedral constraint\non the integer backdoor variables. The resulting ILP is solved in FPT time\nusing an integer programming algorithm. For (ii), the randomized XP time\nalgorithm is obtained by pseudo-polynomially reducing the problem to the exact\nmatching problem. To prevent an exponential blowup in terms of the encoding\nlength of $b$, we bound the Graver complexity of the constraint matrix and\nemploy a Graver augmentation local search framework.",
      "generated_abstract": "We propose a new approach to building and analyzing data-driven models of\ncompetitive multi-agent systems. This approach relies on the development of a\nrepresentation that allows to encode a multi-agent system as a graph,\nenabling the efficient identification of its behavioral rules. We demonstrate\nthe effectiveness of our approach by building a model of the 2D robot navigation\nproblem in the context of multi-agent systems, and by analyzing the\nstatistical properties of the resulting graph. This analysis reveals a\nsignificant reduction in the number of states required to represent the\nsystem, leading to a significant reduction in the computational complexity of\nthe model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1165644171779141,
          "p": 0.3114754098360656,
          "f": 0.16964285317960787
        },
        "rouge-2": {
          "r": 0.026217228464419477,
          "p": 0.07368421052631578,
          "f": 0.03867402927795284
        },
        "rouge-l": {
          "r": 0.09815950920245399,
          "p": 0.26229508196721313,
          "f": 0.14285713889389362
        }
      }
    },
    {
      "paper_id": "cs.CC.cs/CC/2503.01180v1",
      "true_abstract": "It is well known that the graph isomorphism problem is polynomial-time\nreducible to the graph automorphism problem (in fact these two problems are\npolynomial-time equivalent). We show that, analogously, the group isomorphism\nproblem is polynomial-time reducible to the group automorphism problem.\nReductions to other relevant problems like automorphism counting are also\ngiven.",
      "generated_abstract": "aper, we propose a novel framework that enables the detection of\nsignificant outliers in the output of a neural network (NN) by leveraging\noutlier detection techniques. Specifically, we develop a novel framework that\ncombines the performance of a state-of-the-art outlier detection technique,\nGaussian Processes for Machine Learning (GP-ML), with a neural network (NN) to\ndetect outliers. The proposed framework incorporates GP-ML with a simple\nneural network to detect outliers. We validate our approach by applying it to\na real-world dataset and compared with state-of-the-art methods such as\nOutlierR and OutlierNet. The results demonstrate that our proposed framework\nachieves superior outlier detection performance, particularly in detecting\nsignificant outliers. Furthermore, we discuss the practical and theoretical\nadvantages of our proposed framework and identify future research directions\nthat could further",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.05333333333333334,
          "f": 0.07476635094768126
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.05333333333333334,
          "f": 0.07476635094768126
        }
      }
    },
    {
      "paper_id": "math.SP.math/SP/2503.06634v1",
      "true_abstract": "In our recent papers, we studied semiclassical spectral problems for the\nBochner-Schr\\\"odinger operator on a manifold of bounded geometry. We survey\nsome results of these papers in the setting of the magnetic Schr\\\"odinger\noperator in the Euclidean space and describe some ideas of the proofs.",
      "generated_abstract": "We prove a sharp version of the Pohozaev identity for the entropy stable\nmanifold flow, in the case when the initial metric is a product metric. The\nPohozaev identity is obtained by integrating the entropy equation with respect\nto the metric at the time of the initial time. The key point is to establish a\nconsequence of the linear Stokes system for the entropy stable flow. Our proof\nis based on a perturbation argument. This result extends the Pohozaev identity\nfor the linear Stokes system to the case of the entropy stable flow. The\nresults extend the Pohozaev identity for the linear Stokes system to the case\nof the entropy stable flow.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2571428571428571,
          "p": 0.18,
          "f": 0.2117647010380624
        },
        "rouge-2": {
          "r": 0.09523809523809523,
          "p": 0.05333333333333334,
          "f": 0.06837606377383332
        },
        "rouge-l": {
          "r": 0.2571428571428571,
          "p": 0.18,
          "f": 0.2117647010380624
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.03781v1",
      "true_abstract": "This technical report presents a novel DMD-based characterization method for\nvision sensors, particularly neuromorphic sensors such as event-based vision\nsensors (EVS) and Tianmouc, a complementary vision sensor. Traditional image\nsensor characterization standards, such as EMVA1288, are unsuitable for BVS due\nto their dynamic response characteristics. To address this, we propose a\nhigh-speed, high-precision testing system using a Digital Micromirror Device\n(DMD) to modulate spatial and temporal light intensity. This approach enables\nquantitative analysis of key parameters such as event latency, signal-to-noise\nratio (SNR), and dynamic range (DR) under controlled conditions. Our method\nprovides a standardized and reproducible testing framework, overcoming the\nlimitations of existing evaluation techniques for neuromorphic sensors.\nFurthermore, we discuss the potential of this method for large-scale BVS\ndataset generation and conversion, paving the way for more consistent\nbenchmarking of bio-inspired vision technologies.",
      "generated_abstract": "puter interfaces (BCIs) are crucial for enhancing the quality of life\nfor patients suffering from neurological disorders such as stroke, Parkinson's\ndisease, and epilepsy. In this work, we present an end-to-end BCI system that\nuses the TensorFlow Audio Modeling Framework (TFAMF) and DeepSpeech (DS) to\ngenerate auditory feedback. The proposed approach enables the system to\nsynthesize auditory signals that are optimized for different tasks, such as\nreading, writing, and typing. Additionally, it can be used to produce auditory\nsignals that are generic and can be used for various applications. To demonstrate\nthe capabilities of the proposed system, we present results on the Brain-Computer\nInterface for Speech Recognition (BCISR) task, a challenging scenario in which\na patient must perform speech recognition tasks",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15306122448979592,
          "p": 0.16853932584269662,
          "f": 0.16042780249821284
        },
        "rouge-2": {
          "r": 0.015384615384615385,
          "p": 0.017699115044247787,
          "f": 0.016460900374266944
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.15730337078651685,
          "f": 0.14973261533243748
        }
      }
    },
    {
      "paper_id": "physics.gen-ph.physics/gen-ph/2502.20425v1",
      "true_abstract": "The measurements of cluster abundances, gravitational lensings, redshift\nspace distortions and peculiar velocities at lower redshifts point out to much\nsmaller sigma_8 than its value deduced from the measurements of the CMB\nfluctuations assuming the standard LCDM cosmology. We examine and compare the\nsigma_8 redshift dependence calculated within the gauge invariant formalism in\nthe LCDM and the Einstein-Cartan cosmology. It appears that the Einstein-Cartan\ncosmology provides systematically larger sigma_8(z) for higher redshifts\ncompared to the LCDM. Because the CMB fluctuations comprise a cosmological data\nfrom the recombination era to the present, the S8 problem of the LCDM cosmology\nis not a surprise from the standpoint of the Einstein-Cartan cosmology.",
      "generated_abstract": "We propose a simple, yet powerful, approach to understand the behavior of\nrandomly generated random variables. This approach is based on the idea that\neach randomly generated variable is a probability distribution, and that the\nprobability of a randomly generated variable is a function of the random\nvariables that have contributed to its value. We show how this approach can be\nextended to more general distributions, and we demonstrate its application to\nthe problem of finding the mean and variance of a random variable.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15942028985507245,
          "p": 0.22916666666666666,
          "f": 0.18803418319526638
        },
        "rouge-2": {
          "r": 0.053763440860215055,
          "p": 0.06578947368421052,
          "f": 0.059171592683729984
        },
        "rouge-l": {
          "r": 0.14492753623188406,
          "p": 0.20833333333333334,
          "f": 0.17094016610124932
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.00863v1",
      "true_abstract": "This paper studies the relationship between soft and hard paternalism by\nexamining two kinds of restriction: a waiting period and a hard limit (cap) on\nrisk-seeking behavior. Mandatory waiting periods have been instituted for\nmedical procedures, gun purchases and other high-stakes decisions. Are these\npolicies substitutes for hard restrictions, and are delayed decisions more\nrespected? In an experiment, decision-makers are informed about an impending\nhigh-stakes decision. Treatments define when the decision is made: on the spot\nor after one day, and whether the initial decision can be revised. In a general\npopulation survey experiment, another class of subjects (Choice Architects) is\ngranted the opportunity to make rules for decision-makers. Given a decision's\ntemporal structure, Choice Architects can decide on a cap to the\ndecision-maker's risk taking. In another treatment, Choice Architects can\nimplement a mandatory waiting period in addition to the cap. This allows us to\nstudy the substitutional relationship between waiting periods and paternalistic\naction and the effect of deliberation on the autonomy afforded to the\ndecision-maker. Our highly powered experiment reveals that exogenous\ndeliberation has no effect on the cap. Moreover, endogenously prescribed\nwaiting periods represent add-on restrictions that do not substitute for the\ncap. Choice Architects believe that, with time, the average decision-maker will\ntake less risk and -- because of the distribution of Choice Architects' bliss\npoints -- come closer to Choice Architects' subjective ideal choice. These\nfindings highlight the complementarity of policy tools in targeting various\nparts of a distribution of decision-makers.",
      "generated_abstract": "nt paper, I explored the effectiveness of different types of\ninterventions to reduce food waste in the United States. The results of my\nanalysis indicated that food insecurity and food access issues were the most\nsignificant factors influencing the decision to consume food waste. In this\npaper, I further investigate the potential of using data science to develop\ninterventions that can reduce food waste and improve food access. In particular,\nI explore the potential of using big data to identify food waste hotspots and\ndevelop interventions to reduce food waste in these areas. The results of this\nanalysis indicate that food waste hotspots can be identified using big data\nand that interventions to reduce food waste in these areas can be developed.\nFurthermore, I explore the potential of using big data to identify food\naccess gaps and develop interventions to address these gaps. The results of this\nanalysis indicate that food",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07096774193548387,
          "p": 0.1864406779661017,
          "f": 0.10280373432395858
        },
        "rouge-2": {
          "r": 0.008658008658008658,
          "p": 0.020202020202020204,
          "f": 0.012121207921213576
        },
        "rouge-l": {
          "r": 0.06451612903225806,
          "p": 0.1694915254237288,
          "f": 0.09345793993143522
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/NE/2503.06484v1",
      "true_abstract": "Accurate sign language understanding serves as a crucial communication\nchannel for individuals with disabilities. Current sign language translation\nalgorithms predominantly rely on RGB frames, which may be limited by fixed\nframe rates, variable lighting conditions, and motion blur caused by rapid hand\nmovements. Inspired by the recent successful application of event cameras in\nother fields, we propose to leverage event streams to assist RGB cameras in\ncapturing gesture data, addressing the various challenges mentioned above.\nSpecifically, we first collect a large-scale RGB-Event sign language\ntranslation dataset using the DVS346 camera, termed VECSL, which contains\n15,676 RGB-Event samples, 15,191 glosses, and covers 2,568 Chinese characters.\nThese samples were gathered across a diverse range of indoor and outdoor\nenvironments, capturing multiple viewing angles, varying light intensities, and\ndifferent camera motions. Due to the absence of benchmark algorithms for\ncomparison in this new task, we retrained and evaluated multiple\nstate-of-the-art SLT algorithms, and believe that this benchmark can\neffectively support subsequent related research. Additionally, we propose a\nnovel RGB-Event sign language translation framework (i.e., M$^2$-SLT) that\nincorporates fine-grained micro-sign and coarse-grained macro-sign retrieval,\nachieving state-of-the-art results on the proposed dataset. Both the source\ncode and dataset will be released on https://github.com/Event-AHU/OpenESL.",
      "generated_abstract": "The growing popularity of mobile robots has led to the development of\nmobile robot perception systems. However, current approaches often rely on\nexisting off-the-shelf sensors, which can limit their adaptability and\nreliability. To address this, we propose a novel approach for mobile robot\nperception based on the integration of a multi-modal deep learning model with\nan onboard camera and a laser scanner. Our system incorporates a\ncomprehensive fusion mechanism that optimally integrates the disparity and\ndepth images from the camera and the laser scanner to enhance robustness and\nreliability. We evaluate our method on a real robot with a multi-modal\ncombination of depth and disparity images, as well as on simulated data. The\nresults show that our approach significantly outperforms existing methods,\nenhancing the robot's ability to navigate and recognize objects in the environment.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1360544217687075,
          "p": 0.2222222222222222,
          "f": 0.16877636659723347
        },
        "rouge-2": {
          "r": 0.03125,
          "p": 0.047619047619047616,
          "f": 0.037735844271983524
        },
        "rouge-l": {
          "r": 0.12244897959183673,
          "p": 0.2,
          "f": 0.15189872946643182
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.11448v1",
      "true_abstract": "Gaussian processes (GPs) are flexible, probabilistic, non-parametric models\nwidely employed in various fields such as spatial statistics, time series\nanalysis, and machine learning. A drawback of Gaussian processes is their\ncomputational cost having $\\mathcal{O}(N^3)$ time and $\\mathcal{O}(N^2)$ memory\ncomplexity which makes them prohibitive for large datasets. Numerous\napproximation techniques have been proposed to address this limitation. In this\nwork, we systematically compare the accuracy of different Gaussian process\napproximations concerning marginal likelihood evaluation, parameter estimation,\nand prediction taking into account the time required to achieve a certain\naccuracy. We analyze this trade-off between accuracy and runtime on multiple\nsimulated and large-scale real-world datasets and find that Vecchia\napproximations consistently emerge as the most accurate in almost all\nexperiments. However, for certain real-world data sets, low-rank inducing\npoint-based methods, i.e., full-scale and modified predictive process\napproximations, can provide more accurate predictive distributions for\nextrapolation.",
      "generated_abstract": "In this paper, we present a framework for constructing a class of\nstatistical models, called structured time-varying models, that are capable of\nrepresenting time-varying and stochastic processes. These models are built by\nusing the structural equations modeling approach to construct a class of\nstructured stochastic processes. We present a detailed mathematical derivation\nof the structural equations modeling approach and discuss its properties and\napplications. We present the results of our numerical experiments and show\nthat our model is able to capture the stochastic nature of the process. We\nalso show that the model can be used for forecasting, monitoring, and\ntrending purposes.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1565217391304348,
          "p": 0.2903225806451613,
          "f": 0.20338982595678137
        },
        "rouge-2": {
          "r": 0.006993006993006993,
          "p": 0.011111111111111112,
          "f": 0.008583686245835106
        },
        "rouge-l": {
          "r": 0.1391304347826087,
          "p": 0.25806451612903225,
          "f": 0.1807909559002842
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.21306v1",
      "true_abstract": "Solar prosumers, residential electricity consumers equipped with photovoltaic\n(PV) systems and battery storage, are transforming electricity markets. Their\ninteractions with the transmission grid under varying tariff designs are not\nyet fully understood. We explore the influence of different pricing regimes on\nprosumer investment and dispatch decisions and their subsequent impact on the\ntransmission grid. Using an integrated modeling approach that combines two\nopen-source dispatch, investment and grid models, we simulate prosumage\nbehavior in Germany's electricity market under real-time pricing or\ntime-invariant pricing, as well as under zonal or nodal pricing. Our findings\nshow that zonal pricing favors prosumer investments, while time-invariant\npricing rather hinders it. In comparison, regional solar availability emerges\nas a larger driver for rooftop PV investments. The impact of prosumer\nstrategies on grid congestion remains limited within the scope of our\nmodel-setup, in which home batteries cannot be used for energy arbitrage.",
      "generated_abstract": "nt global financial system is vulnerable to the risks of systemic\ncrashes, including panics, contagion, and systemic risk. These risks are\nexacerbated by the interconnected nature of the financial system, the presence\nof unregulated, shadow banks, and the fragmentation of supervision and\nregulation. In response, the European Commission has proposed the Financial\nStability Board (FSB) to coordinate regulatory action and strengthen financial\nsystem resilience. This paper examines the current financial system's\nvulnerabilities and highlights potential solutions to mitigate systemic risks.\nFirst, we analyze the systemic risks facing the financial system and the\npotential solutions proposed by the FSB. Next, we examine the role of\ncentral-bank balance sheets and the impact of the pandemic on the systemic\nrisks of the financial system. Finally, we",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07339449541284404,
          "p": 0.11267605633802817,
          "f": 0.08888888411172866
        },
        "rouge-2": {
          "r": 0.014184397163120567,
          "p": 0.018691588785046728,
          "f": 0.016129027352043634
        },
        "rouge-l": {
          "r": 0.07339449541284404,
          "p": 0.11267605633802817,
          "f": 0.08888888411172866
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2412.03606v1",
      "true_abstract": "This paper aims to study the prediction of the bank stability index based on\nthe Time Series Transformer model. The bank stability index is an important\nindicator to measure the health status and risk resistance of financial\ninstitutions. Traditional prediction methods are difficult to adapt to complex\nmarket changes because they rely on single-dimensional macroeconomic data. This\npaper proposes a prediction framework based on the Time Series Transformer,\nwhich uses the self-attention mechanism of the model to capture the complex\ntemporal dependencies and nonlinear relationships in financial data. Through\nexperiments, we compare the model with LSTM, GRU, CNN, TCN and RNN-Transformer\nmodels. The experimental results show that the Time Series Transformer model\noutperforms other models in both mean square error (MSE) and mean absolute\nerror (MAE) evaluation indicators, showing strong prediction ability. This\nshows that the Time Series Transformer model can better handle multidimensional\ntime series data in bank stability prediction, providing new technical\napproaches and solutions for financial risk management.",
      "generated_abstract": "This paper studies a class of non-linear stochastic control problems that\nare non-convex and non-smooth with state-dependent delays. The optimal control\nproblems are constrained to the state-dependent delay constrained set. The\ncontrol objective is to minimize a cost functional that depends on the current\nstate of the system, the delay, and the control. We establish a duality\nrelation between the constrained optimal control problem and a non-constrained\noptimization problem. This duality relation leads to a novel optimization\nformulation that is computationally efficient. The new formulation is then\napplied to study the optimal control problem for non-linear stochastic\ndelays. We prove the existence of the optimal control and develop a numerical\nalgorithm to solve the problem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1568627450980392,
          "p": 0.26666666666666666,
          "f": 0.19753085953360777
        },
        "rouge-2": {
          "r": 0.03546099290780142,
          "p": 0.047619047619047616,
          "f": 0.040650401611144756
        },
        "rouge-l": {
          "r": 0.14705882352941177,
          "p": 0.25,
          "f": 0.18518518052126212
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06572v1",
      "true_abstract": "Recent research has paid little attention to complex driving behaviors,\nnamely merging car-following and lane-changing behavior, and how lane-changing\naffects algorithms designed to model and control a car-following vehicle.\nDuring the merging behavior, the Follower Vehicle (FV) might significantly\ndiverge from typical car-following models. Thus, this paper aims to control the\nFV witnessing lane-changing behavior based on anticipation, perception,\npreparation, and relaxation states defined by a novel measurable human\nperception index. Data from human drivers are utilized to create a\nperception-based fuzzy controller for the behavior vehicle's route guidance,\ntaking into account the opacity of human driving judgments. We illustrate the\nefficacy of the established technique using simulated trials and data from\nactual drivers, focusing on the benefits of the increased comfort, safety, and\nuniformity of traffic flow and the decreased of wait time and motion sickness\nthis brings about.",
      "generated_abstract": "This paper addresses the problem of adapting the state space representation\nof a stochastic dynamic system using only a single parameter. The problem is\nformulated as a constrained optimization problem, which is then solved by\nemploying an efficient iterative algorithm. The algorithm is based on the\nproposed parametric Kalman filter (PKF) and is optimized in a manner that\nmaximizes the approximation of the state transition probability matrix. The\nalgorithm is illustrated through a numerical example, and its performance is\nevaluated using a comprehensive suite of simulation experiments.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09,
          "p": 0.14516129032258066,
          "f": 0.11111110638622182
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.03614457831325301,
          "f": 0.02714931657664748
        },
        "rouge-l": {
          "r": 0.07,
          "p": 0.11290322580645161,
          "f": 0.08641974836153052
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2502.07806v1",
      "true_abstract": "The integration of Quantum Deep Learning (QDL) techniques into the landscape\nof financial risk analysis presents a promising avenue for innovation. This\nstudy introduces a framework for credit risk assessment in the banking sector,\ncombining quantum deep learning techniques with adaptive modeling for Row-Type\nDependent Predictive Analysis (RTDPA). By leveraging RTDPA, the proposed\napproach tailors predictive models to different loan categories, aiming to\nenhance the accuracy and efficiency of credit risk evaluation. While this work\nexplores the potential of integrating quantum methods with classical deep\nlearning for risk assessment, it focuses on the feasibility and performance of\nthis hybrid framework rather than claiming transformative industry-wide\nimpacts. The findings offer insights into how quantum techniques can complement\ntraditional financial analysis, paving the way for further advancements in\npredictive modeling for credit risk.",
      "generated_abstract": "This paper develops a novel framework to evaluate the performance of\nfinancial models in the presence of stochastic volatility. We introduce a\ntwo-dimensional Black-Scholes model with stochastic volatility that allows for\nthe estimation of volatility through the use of finite-dimensional\nrepresentations. We show that, under appropriate assumptions, this model can be\nused to calibrate the stochastic volatility term structure. We then apply this\nframework to the evaluation of the performance of a number of financial models\ninvolving stochastic volatility. Our findings suggest that the use of\nfinite-dimensional representations of volatility can significantly enhance the\ncalibration of stochastic volatility term structures, with implications for\nfinancial modeling and calibration.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20652173913043478,
          "p": 0.3275862068965517,
          "f": 0.2533333285902223
        },
        "rouge-2": {
          "r": 0.032,
          "p": 0.044444444444444446,
          "f": 0.037209297458086085
        },
        "rouge-l": {
          "r": 0.20652173913043478,
          "p": 0.3275862068965517,
          "f": 0.2533333285902223
        }
      }
    },
    {
      "paper_id": "cs.MA.cs/GT/2503.10186v1",
      "true_abstract": "Beyond specific settings, many multi-agent learning algorithms fail to\nconverge to an equilibrium solution, and instead display complex,\nnon-stationary behaviours such as recurrent or chaotic orbits. In fact, recent\nliterature suggests that such complex behaviours are likely to occur when the\nnumber of agents increases. In this paper, we study Q-learning dynamics in\nnetwork polymatrix games where the network structure is drawn from classical\nrandom graph models. In particular, we focus on the Erdos-Renyi model, a\nwell-studied model for social networks, and the Stochastic Block model, which\ngeneralizes the above by accounting for community structures within the\nnetwork. In each setting, we establish sufficient conditions under which the\nagents' joint strategies converge to a unique equilibrium. We investigate how\nthis condition depends on the exploration rates, payoff matrices and,\ncrucially, the sparsity of the network. Finally, we validate our theoretical\nfindings through numerical simulations and demonstrate that convergence can be\nreliably achieved in many-agent systems, provided network sparsity is\ncontrolled.",
      "generated_abstract": "In this paper, we investigate the problem of evaluating a class of nonlinear\nreinforcement learning (RL) algorithms. Our goal is to determine the\nperformance of the algorithms with respect to a predefined performance metric.\nWe propose a novel evaluation framework that leverages a combination of\nstatistical and symbolic approaches. Our framework combines a statistical\nanalysis with symbolic model checking to ensure that the evaluation procedure\nremains computationally tractable. We provide theoretical guarantees on the\nefficiency of our procedure for evaluating a class of algorithms. Additionally,\nwe conduct experiments to evaluate our framework on two real-world problems.\nThese experiments show that our framework is computationally efficient, and it\nyields accurate results for the evaluation of a broad class of algorithms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1652892561983471,
          "p": 0.2898550724637681,
          "f": 0.210526311163989
        },
        "rouge-2": {
          "r": 0.03870967741935484,
          "p": 0.05405405405405406,
          "f": 0.045112777091695935
        },
        "rouge-l": {
          "r": 0.15702479338842976,
          "p": 0.2753623188405797,
          "f": 0.19999999537451532
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.physics/optics/2503.10359v1",
      "true_abstract": "In this study, we systematically explore the non-Hermitian skin effect (NHSE)\nand its associated complex-frequency detection in the context of a\nfrequency-dependent non-Hermitian Hamiltonian. This Hamiltonian arises from the\nself-energy correction of the subsystem and can be calculated exactly within\nour theoretical model, without the need for non-Hermitian approximations.\nAdditionally, complex frequency detection, which encompasses complex frequency\nexcitation, synthesis, and fingerprint, enables us to detect the physical\nresponses driven by complex frequency excitations. Our calculations reveal that\nboth complex frequency excitation and synthesis are sensitive to the\nnon-Hermitian approximation and are unable to characterize the presence or\nabscence of the NHSE. In contrast, the complex-frequency fingerprint\nsuccessfully detects the novel responses induced by the NHSE through the\nintroduction of a double-frequency Green's function. Our work paves the way for\na rigorous understanding of non-Hermitian physics in quantum systems and their\nexperimental verification through complex frequency-domain techniques.",
      "generated_abstract": "igate the effect of the magnetic field on the spin-orbit coupling\n(SOC) in magnetic nanostructures such as magnetic tunnel junctions (MTJs) and\nmagnetic tunnel barriers (MTBs). We find that the SOC in MTBs is suppressed\nbelow a critical magnetic field strength, which depends on the type of\nmagnetic material and the direction of the magnetic field. We show that the\nsuppression of SOC in MTBs can be explained by the existence of a magnetic\nsinglet state in the $g=1$ manifold. The SOC in MTBs is also suppressed by the\nmagnetic field when it is parallel to the magnetization direction. We find that\nthe SOC in MTBs is suppressed by the magnetic field only when the magnetic\nfield is perpendicular to the magnetization direction. These results suggest\nthat the SOC in MTBs",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12244897959183673,
          "p": 0.2033898305084746,
          "f": 0.15286623734674848
        },
        "rouge-2": {
          "r": 0.043478260869565216,
          "p": 0.06382978723404255,
          "f": 0.05172413311088036
        },
        "rouge-l": {
          "r": 0.12244897959183673,
          "p": 0.2033898305084746,
          "f": 0.15286623734674848
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2409.08728v1",
      "true_abstract": "We use a methodology based on a machine learning algorithm to quantify firms'\ncyber risks based on their disclosures and a dedicated cyber corpus. The model\ncan identify paragraphs related to determined cyber-threat types and\naccordingly attribute several related cyber scores to the firm. The cyber\nscores are unrelated to other firms' characteristics. Stocks with high cyber\nscores significantly outperform other stocks. The long-short cyber risk factors\nhave positive risk premia, are robust to all factors' benchmarks, and help\nprice returns. Furthermore, we suggest the market does not distinguish between\ndifferent types of cyber risks but instead views them as a single, aggregate\ncyber risk.",
      "generated_abstract": "er the problem of optimal portfolio selection under incomplete\nmarket information, with an endowment problem to be solved in a market where\nthe riskless investor holds a fixed but unknown portfolio. We propose a\nnonparametric approach to estimate the risk-neutral distribution of the\nendowment. The estimation procedure is based on a kernel regression model and\nestimates the risk-neutral distribution using a kernel density estimator. The\nestimated risk-neutral distribution is then used to compute the optimal\nportfolio selection under a nonparametric risk measure. We show that the\nestimated risk-neutral distribution has an important structural property that\nallows us to compute the optimal portfolio selection in closed form, thus\nproviding a rigorous solution to the problem of optimal portfolio selection.\nWe also develop a semi-parametric approach to estimate the risk-neutral\ndistribution of the endow",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1891891891891892,
          "p": 0.19718309859154928,
          "f": 0.1931034432780025
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.028846153846153848,
          "f": 0.029556645249339583
        },
        "rouge-l": {
          "r": 0.16216216216216217,
          "p": 0.16901408450704225,
          "f": 0.1655172363814508
        }
      }
    },
    {
      "paper_id": "math.DG.math/SG/2503.06570v1",
      "true_abstract": "In this paper, we propose a condition on the coefficients of a\ncohomology-valued power series, which we call ``asymptotically\nMittag-Leffler''. We show that if the $J$-function of a Fano manifold is\nasymptotically Mittag-Leffler, then it has the exponential growth as $t\\to\n+\\infty$. This provides an alternative method to compute the principal\nasymptotic class of a Fano manifold using the coefficients of $J$-function. We\nalso verify that the $J$-function of the projective space is asymptotically\nMittag-Leffler, and the property of having an asymptotically Mittag-Leffler\n$J$-function is preserved when taking product and hypersurface.",
      "generated_abstract": "er the problem of determining the dimension of the space of\n$(m,n)$-multilinear symmetric forms on a finite-dimensional vector space. We\nintroduce a new definition of multilinear forms, which we call \"cohomological\nmultilinear forms\", and show that the dimension of the space of\n$m$-cohomological multilinear forms is equal to the Euler class of the\nmultiplicative group of the vector space. We also show that the dimension of\nthe space of $(m,n)$-multilinear forms is equal to the Euler class of the\nmultiplicative group of the vector space. In this way, we obtain a new proof\nof the classical result of Hodge that the dimension of the space of cohomological\nmultilinear forms is equal to the Euler class of the multiplicative group of\nthe vector space. We also show that the dimension",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3,
          "p": 0.391304347826087,
          "f": 0.33962263659665365
        },
        "rouge-2": {
          "r": 0.1,
          "p": 0.11764705882352941,
          "f": 0.10810810314097904
        },
        "rouge-l": {
          "r": 0.26666666666666666,
          "p": 0.34782608695652173,
          "f": 0.30188678754004994
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09972v1",
      "true_abstract": "It is known that, when $n$ is even, the number of permutations of\n$\\{1,2,\\dots,n\\}$ all of whose cycles have odd length equals the number of\nthose all of whose cycles have even length. Adin, Heged\\H{u}s and Roichman\nrecently found a surprising refinement of this identity. They showed that, for\nany fixed set $J$, the equality still holds when restricting to permutations\nwith descent set $J$ on one side, and permutations with ascent set $J$ on the\nother. Their proof uses generating functions for higher Lie characters. They\nalso deduce a version for odd $n$.\n  Here we give a bijective proof of their result. We first use known bijections\nto restate the identity in terms of multisets of necklaces, and then describe a\nnew weight-preserving bijection between words all of whose Lyndon factors have\nodd length and are distinct, and words all of whose Lyndon factors have even\nlength. We also show that the corresponding equality about Lyndon\nfactorizations has a short proof using generating functions.",
      "generated_abstract": "uct the first complete flag complex on the non-commutative\nscheme $A = \\mathrm{Spec}(R)$ over an algebraically closed base field $k$ with\nnon-commutative coordinates $(R, \\mathfrak{m} R, \\mathfrak{m} + \\mathfrak{m}^2)$\nby using a $k$-algebra $R$ and a collection of $k$-linear idempotents\n$\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n$ in $R$. The flag complex is\nconstructed as a chain complex of chain complexes of $k$-linear $R$-modules\n$\\mathcal{F}_n(\\cdots \\mathcal{F}_2(\\mathcal{F}_1(A))))$, where each\n$\\mathcal{F}_i(A))$ is defined as the completion of the localization\n$R_i = R[x_1, \\ldots, x",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10309278350515463,
          "p": 0.1694915254237288,
          "f": 0.1282051235018082
        },
        "rouge-2": {
          "r": 0.007042253521126761,
          "p": 0.013157894736842105,
          "f": 0.009174307384902509
        },
        "rouge-l": {
          "r": 0.08247422680412371,
          "p": 0.13559322033898305,
          "f": 0.10256409786078259
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2502.00877v1",
      "true_abstract": "This study investigates the inherently random structures of dry bulk shipping\nnetworks, often likened to a taxi service, and identifies the underlying trade\ndynamics that contribute to this randomness within individual cargo\nsub-networks. By analysing micro-level trade flow data from 2015 to 2023, we\nexplore the evolution of dry commodity networks, including grain, coal, and\niron ore, and suggest that the Giant Strongly Connected Components exhibit\nsmall-world phenomena, indicative of efficient bilateral trade. The significant\nheterogeneity of in-degree and out-degree within these sub-networks, primarily\ndriven by importing ports, underscores the complexity of their dynamics. Our\ntemporal analysis shows that while the Covid-19 pandemic profoundly impacted\nthe coal network, the Ukraine conflict significantly altered the grain network,\nresulting in changes in community structures. Notably, grain sub-networks\ndisplay periodic changes, suggesting distinct life cycles absent in coal and\niron ore networks. These findings illustrate that the randomness in dry bulk\nshipping networks is a reflection of real-world trade dynamics, providing\nvaluable insights for stakeholders in navigating and predicting network\nbehaviours.",
      "generated_abstract": "We consider a generalised linear model with a stochastic volatility\ncomponent. The volatility is modelled as a Gaussian process with a positive\nBrownian noise. We establish the existence and uniqueness of the solution to\nthe stochastic differential equation associated with this model. We then\nestablish the consistency of the Gaussian process component and the\nasymptotic normality of the solution. Finally, we derive the asymptotic\ndistribution of the solution to the stochastic differential equation. The\nmodel is applied to the pricing of European options and the risk\ncharacterisation of the option.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0743801652892562,
          "p": 0.1956521739130435,
          "f": 0.10778442714618683
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0743801652892562,
          "p": 0.1956521739130435,
          "f": 0.10778442714618683
        }
      }
    },
    {
      "paper_id": "math.GT.math/GT/2503.04607v1",
      "true_abstract": "Heegaard splittings stratify 3-manifolds by complexity; only $S^3$ admits a\ngenus-zero splitting, and only $S^3$, $S^1 \\times S^2$, and lens spaces\n$L(p,q)$ admit genus-one splittings. In dimension four, the second author and\nJeffrey Meier proved that only a handful of simply-connected 4-manifolds have\ntrisection genus two or less, while Meier conjectured that if $X$ admits a\ngenus-three trisection, then $X$ is diffeomorphic to a spun lens space $S_p$ or\nits sibling $S_p'$, $S^4$, or a connected sum of copies of $\\pm \\mathbb{CP}^2$,\n$S^1 \\times S^3$, and $S^2 \\times S^2$. We prove Meier's conjecture in the case\nthat $X$ admits a weakly reducible genus-three trisection, where weak\nreducibility is a new idea adapted from Heegaard theory and is defined in terms\nof disjoint curves bounding compressing disks in various handlebodies. The\ntools and techniques used to prove the main theorem borrow heavily from\n3-manifold topology. Of independent interest, we give a trisection-diagrammatic\ndescription of 4-manifolds obtained by surgery on loops and spheres in other\n4-manifolds.",
      "generated_abstract": "aper, we study the asymptotic behavior of the solution to the\ngeneralized $p$-Laplacian equation, which can be considered as the generalized\nwave equation with an additional nonlinearity term, and the nonlinear\nself-similarity is replaced by the $p$-Laplacian self-similarity. We first\nstudy the asymptotic behavior of the solutions to the generalized $p$-Laplacian\nequation with an oscillatory initial data. For the nonlinearity term, we show\nthat the oscillatory initial data has a unique local-in-time solution with\nsmall data if the nonlinearity is sufficiently smooth, which is in\ncontrast with the previous work in the literature. Next, we derive the\nasymptotic behavior of the solutions to the generalized $p$-Laplacian equation\nwith a general nonlinearity term. For the nonlinearity, we construct the\nlocal-in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10434782608695652,
          "p": 0.2033898305084746,
          "f": 0.13793103000066073
        },
        "rouge-2": {
          "r": 0.006329113924050633,
          "p": 0.01098901098901099,
          "f": 0.008032123876069193
        },
        "rouge-l": {
          "r": 0.09565217391304348,
          "p": 0.1864406779661017,
          "f": 0.12643677712709753
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SY/2503.07997v1",
      "true_abstract": "Autonomous stores leverage advanced sensing technologies to enable\ncashier-less shopping, real-time inventory tracking, and seamless customer\ninteractions. However, these systems face significant challenges, including\nocclusion in vision-based tracking, scalability of sensor deployment, theft\nprevention, and real-time data processing. To address these issues, researchers\nhave explored multi-modal sensing approaches, integrating computer vision,\nRFID, weight sensing, vibration-based detection, and LiDAR to enhance accuracy\nand efficiency. This survey provides a comprehensive review of sensing\ntechnologies used in autonomous retail environments, highlighting their\nstrengths, limitations, and integration strategies. We categorize existing\nsolutions across inventory tracking, environmental monitoring, people-tracking,\nand theft detection, discussing key challenges and emerging trends. Finally, we\noutline future directions for scalable, cost-efficient, and privacy-conscious\nautonomous store systems.",
      "generated_abstract": "In this paper, we consider a wireless power transfer system, in which a\ndevice requires recharge while being located in a non-line-of-sight (NLOS)\nenvironment. In this scenario, the system operates in a time-varying\nenvironment. We propose a distributed recharge control scheme, where the\nenergy is stored in a battery, and a distributed recharge controller is\nconstructed, which dynamically selects the optimal charging point and the\nmaximum recharge power. Furthermore, a novel energy allocation scheme is\nproposed to minimize the energy waste while maintaining the charging efficiency.\nThe effectiveness of the proposed algorithms is verified using numerical\nsimulations and a real-world experiment.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0851063829787234,
          "p": 0.12307692307692308,
          "f": 0.10062892598394074
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0851063829787234,
          "p": 0.12307692307692308,
          "f": 0.10062892598394074
        }
      }
    },
    {
      "paper_id": "math.QA.math/QA/2503.10327v1",
      "true_abstract": "Solutions to the quiver-theoretic quantum Yang-Baxter equation are associated\nwith structure categories and structure groupoids. We prove that the structure\ngroupoids of involutive non-degenerate solutions are Garside. This generalises\na well-known result about the structure groups of set-theoretic solutions, due\nto Chouraqui. We also construct involutive non-degenerate solutions from\nsuitable presented categories. We then investigate the case of solutions of\nprincipal homogeneous type. Finally, we present some examples of this new class\nof Garside groupoids.",
      "generated_abstract": "In this paper, we study the limit behavior of the KAM theory of a family of\nnon-linear Schr\\\"odinger equations with the general potential term. We prove\nthat the limiting equation is the non-linear Schr\\\"odinger equation with\nquadratic potential. We also study the limiting equation when the general\npotential term is replaced by the linear potential term. We prove that the\nlimiting equation is the non-linear Schr\\\"odinger equation with the quadratic\npotential. In addition, we study the limiting equation when the general\npotential term is replaced by the non-linear Schr\\\"odinger equation with the\ngeneral potential term.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21153846153846154,
          "p": 0.3333333333333333,
          "f": 0.25882352466159175
        },
        "rouge-2": {
          "r": 0.05714285714285714,
          "p": 0.0784313725490196,
          "f": 0.06611569760262313
        },
        "rouge-l": {
          "r": 0.17307692307692307,
          "p": 0.2727272727272727,
          "f": 0.21176470113218004
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.05022v2",
      "true_abstract": "Understanding firm conduct is crucial for industrial organization and\nantitrust policy. In this article, we develop a testing procedure based on the\nRivers and Vuong non-nested model selection framework. Unlike existing methods\nthat require estimating the demand and supply system, our approach compares the\nmodel fit of two first-stage price regressions. Through an extensive Monte\nCarlo study, we demonstrate that our test performs comparably to, or\noutperforms, existing methods in detecting collusion across various collusive\nscenarios. The results are robust to model misspecification, alternative\nfunctional forms for instruments, and data limitations. By simplifying the\ndiagnosis of firm behavior, our method provides an efficient tool for\nresearchers and regulators to assess industry conduct. Additionally, our\napproach offers a practical guideline for enhancing the strength of BLP-style\ninstruments in demand estimation: once collusion is detected, researchers are\nadvised to incorporate the product characteristics of colluding partners into\nown-firm instruments while excluding them from other-firm instruments.",
      "generated_abstract": "r examines the evolution of the European single market in the\ntime period 1995-2019, using the macro-fractional-difference method to\nquantify the effects of the EU's structural reforms on economic activity.\nResults indicate that the expansion of the single market was driven by the\nincrease in the share of exports in the EU economy, while the share of imports\ndeclined. The results also show that the implementation of structural reforms\nhad a negative impact on economic growth, with a significant decline in the\naggregate value added of firms. In addition, the study found that the\nimplementation of structural reforms had a negative impact on employment, with\na significant decline in the number of employees. These results indicate that\nstructural reforms have had negative effects on economic growth and employment\nin the EU, highlighting the need for further research to address the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12280701754385964,
          "p": 0.18666666666666668,
          "f": 0.14814814336104826
        },
        "rouge-2": {
          "r": 0.006666666666666667,
          "p": 0.009174311926605505,
          "f": 0.007722002847307082
        },
        "rouge-l": {
          "r": 0.12280701754385964,
          "p": 0.18666666666666668,
          "f": 0.14814814336104826
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2503.02611v1",
      "true_abstract": "Statistical integration of diverse data sources is an essential step in the\nbuilding of generalizable prediction tools, especially in precision health. The\ninvariant features model is a new paradigm for multi-source data integration\nwhich posits that a small number of covariates affect the outcome identically\nacross all possible environments. Existing methods for estimating invariant\neffects suffer from immense computational costs or only offer good statistical\nperformance under strict assumptions. In this work, we provide a general\nframework for estimation under the invariant features model that is\ncomputationally efficient and statistically flexible. We also provide a robust\nextension of our proposed method to protect against possibly corrupted or\nmisspecified data sources. We demonstrate the robust properties of our method\nvia simulations, and use it to build a transferable prediction model for end\nstage renal disease using electronic health records from the All of Us research\nprogram.",
      "generated_abstract": "e a Bayesian approach for modeling the distribution of the\ntime to first infection (TFI) in a population with multiple susceptible\nindividuals. The model is based on a latent Markov process and the\ndistribution of the TFI is estimated by a Markov chain Monte Carlo (MCMC)\nalgorithm. The key idea of the proposed approach is to consider the\ndistribution of the TFI as a function of the latent Markov process, which\ncaptures the time required to reach the infection state. We show that this\napproach leads to a simpler model than a simple linear regression model,\nwhich is commonly used in the literature. To assess the model fit, we\npropose a likelihood function based on the relative entropy. We show that the\nlikelihood function is convex in the latent Markov process, which is a\nsignificant property for MCMC algorithms. To obtain the posterior distribution\nof the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.20512820512820512,
          "f": 0.1758241709262168
        },
        "rouge-2": {
          "r": 0.02158273381294964,
          "p": 0.02459016393442623,
          "f": 0.02298850076833977
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.20512820512820512,
          "f": 0.1758241709262168
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2409.13957v1",
      "true_abstract": "One type of bond with the most implicit government guarantee is municipal\ninvestment bonds. In recent years, there have been an increasing number of\ndowngrades in the credit ratings of municipal bonds, which has led some people\nto question whether the implicit government guarantee may affect the\nobjectivity of the bond ratings? This paper uses text mining methods to mine\nrelevant policy documents related to municipal investment bond issuance, and\ncalculates the implicit guarantee strength of municipal investment bonds based\non the PMC index model. It further analyzes the impact of the implicit\nguarantee strength of municipal bonds on their credit evaluation. The study\nfound that the implicit government guarantee on municipal investment bonds does\nindeed help to raise the credit ratings assigned by credit rating agencies. The\nstudy found that, moreover, the government's implicit guarantee has a more\npronounced effect in boosting credit ratings in less developed western regions.",
      "generated_abstract": "r develops a model to estimate the expected time of default (ETD)\nof a firm. ETD is a key metric for assessing the financial health of a\nfirm and is considered one of the most important risk indicators for\ninvestors. It is typically estimated using data from a bankruptcy court case\nthat occurs at the end of the default period. However, such data are often\ndifficult to obtain. In this paper, we propose a method to estimate ETD from\ndata on the firm's cash flows and debt holdings. Our method is based on\ninformation theory and is applicable to a broad class of firm-level data. The\nmain contributions of this paper are twofold: (1) We develop a model to estimate\nETD from firm-level data, and (2) We apply our method to a dataset on 15,000\nfirms from the United States. Our find",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.1724137931034483,
          "f": 0.16949152042516535
        },
        "rouge-2": {
          "r": 0.032,
          "p": 0.031007751937984496,
          "f": 0.031496057993366786
        },
        "rouge-l": {
          "r": 0.15555555555555556,
          "p": 0.16091954022988506,
          "f": 0.15819208539691676
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2501.16391v1",
      "true_abstract": "Significant differences in protein structures hinder the generalization of\nexisting drug-target interaction (DTI) models, which often rely heavily on\npre-learned binding principles or detailed annotations. In contrast, BioBridge\ndesigns an Inductive-Associative pipeline inspired by the workflow of\nscientists who base their accumulated expertise on drawing insights into novel\ndrug-target pairs from weakly related references. BioBridge predicts novel\ndrug-target interactions using limited sequence data, incorporating multi-level\nencoders with adversarial training to accumulate transferable binding\nprinciples. On these principles basis, BioBridge employs a dynamic prototype\nmeta-learning framework to associate insights from weakly related annotations,\nenabling robust predictions for previously unseen drug-target pairs. Extensive\nexperiments demonstrate that BioBridge surpasses existing models, especially\nfor unseen proteins. Notably, when only homologous protein binding data is\navailable, BioBridge proves effective for virtual screening of the epidermal\ngrowth factor receptor and adenosine receptor, underscoring its potential in\ndrug discovery.",
      "generated_abstract": "ding the mechanisms underlying the evolution of large language\nmodel (LLM) models is crucial for developing more robust and effective\nmodels. However, existing methods are limited by the lack of systematic\nexperiments and insufficient exploration of the complex interplay between LLMs\nand the underlying data distribution. In this work, we propose a novel\nmulti-scale model architecture, Multi-Scale Multi-Agent Model (MSMAM), for\nstudying the evolution of LLMs. MSMAM is an LLM-agnostic multi-agent\narchitecture, which is designed to capture the complex interplay between LLMs\nand the underlying data distribution. To address the challenges of existing\nmethods, we introduce two innovative components: the hierarchical multi-agent\nmodel and the multi-scale learning. The hierarchical multi-agent model\nstructures the LLM model as a hierarchy of agents, each of which is trained",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16964285714285715,
          "p": 0.25,
          "f": 0.20212765475780908
        },
        "rouge-2": {
          "r": 0.022058823529411766,
          "p": 0.027522935779816515,
          "f": 0.02448979097909304
        },
        "rouge-l": {
          "r": 0.13392857142857142,
          "p": 0.19736842105263158,
          "f": 0.1595744632684474
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/AI/2503.10529v1",
      "true_abstract": "3D Multimodal Large Language Models (MLLMs) have recently made substantial\nadvancements. However, their potential remains untapped, primarily due to the\nlimited quantity and suboptimal quality of 3D datasets. Current approaches\nattempt to transfer knowledge from 2D MLLMs to expand 3D instruction data, but\nstill face modality and domain gaps. To this end, we introduce PiSA-Engine\n(Point-Self-Augmented-Engine), a new framework for generating instruction\npoint-language datasets enriched with 3D spatial semantics. We observe that\nexisting 3D MLLMs offer a comprehensive understanding of point clouds for\nannotation, while 2D MLLMs excel at cross-validation by providing complementary\ninformation. By integrating holistic 2D and 3D insights from off-the-shelf\nMLLMs, PiSA-Engine enables a continuous cycle of high-quality data generation.\nWe select PointLLM as the baseline and adopt this co-evolution training\nframework to develop an enhanced 3D MLLM, termed PointLLM-PiSA. Additionally,\nwe identify limitations in previous 3D benchmarks, which often feature coarse\nlanguage captions and insufficient category diversity, resulting in inaccurate\nevaluations. To address this gap, we further introduce PiSA-Bench, a\ncomprehensive 3D benchmark covering six key aspects with detailed and diverse\nlabels. Experimental results demonstrate PointLLM-PiSA's state-of-the-art\nperformance in zero-shot 3D object captioning and generative classification on\nour PiSA-Bench, achieving significant improvements of 46.45% (+8.33%) and\n63.75% (+16.25%), respectively. We will release the code, datasets, and\nbenchmark.",
      "generated_abstract": "years, deep learning has achieved significant success in object\nrecognition. However, traditional object recognition methods suffer from\ndifficulties in handling diverse visual scenes, such as different lighting\nconditions and occlusion, which hinder the recognition of objects in complex\nscenes. To address this problem, we propose an object detection method based on\nan attention-guided image feature extractor and a multi-scale convolutional\nnetwork. The proposed method integrates the attention mechanism and convolution\nlayer to extract multi-scale features, which enhance the discrimination of\nobjects in complex scenes. Additionally, a lightweight and efficient\nconvolutional network is designed to accurately extract features, which\nenhances the detection performance. Experimental results on the PASCAL VOC\n2007, PASCAL VOC 2012, and COCO datasets demonstrate that our method can\nachieve superior detection performance compared to state",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17177914110429449,
          "p": 0.3146067415730337,
          "f": 0.22222221765337627
        },
        "rouge-2": {
          "r": 0.014150943396226415,
          "p": 0.02564102564102564,
          "f": 0.018237077483764234
        },
        "rouge-l": {
          "r": 0.147239263803681,
          "p": 0.2696629213483146,
          "f": 0.19047618590734453
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2410.02846v1",
      "true_abstract": "We introduce a novel machine learning model for credit risk by combining\ntree-boosting with a latent spatio-temporal Gaussian process model accounting\nfor frailty correlation. This allows for modeling non-linearities and\ninteractions among predictor variables in a flexible data-driven manner and for\naccounting for spatio-temporal variation that is not explained by observable\npredictor variables. We also show how estimation and prediction can be done in\na computationally efficient manner. In an application to a large U.S. mortgage\ncredit risk data set, we find that both predictive default probabilities for\nindividual loans and predictive loan portfolio loss distributions obtained with\nour novel approach are more accurate compared to conventional independent\nlinear hazard models and also linear spatio-temporal models. Using\ninterpretability tools for machine learning models, we find that the likely\nreasons for this outperformance are strong interaction and non-linear effects\nin the predictor variables and the presence of large spatio-temporal frailty\neffects.",
      "generated_abstract": "The recent growth of financial markets has been driven by the development of\ncomputer technology, which has enabled the automation of trading operations.\nThis paper examines the potential of artificial intelligence (AI) in the\nfinancial industry, focusing on stock trading. We propose a framework for\nmodeling and predicting stock trends using machine learning and deep learning\ntechniques. This approach combines statistical analysis with artificial\nintelligence techniques to improve the accuracy and efficiency of stock\ntrading. Our framework includes a deep learning model for predicting stock\nreturns, as well as a neural network model for predicting stock volatility.\nThrough experimental studies, we demonstrate the effectiveness of these\nmodels in simulated and real-world data. Our results show that the AI-based\nmodels outperform traditional methods in terms of accuracy and efficiency,\ncontributing to more efficient and profitable trading strategies.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23232323232323232,
          "p": 0.26136363636363635,
          "f": 0.24598929983013534
        },
        "rouge-2": {
          "r": 0.04225352112676056,
          "p": 0.047244094488188976,
          "f": 0.044609660443056906
        },
        "rouge-l": {
          "r": 0.21212121212121213,
          "p": 0.23863636363636365,
          "f": 0.22459892549858457
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/AP/2502.16520v2",
      "true_abstract": "The increasing complexity of supply chains and the rising costs associated\nwith defective or substandard goods (bad goods) highlight the urgent need for\nadvanced predictive methodologies to mitigate risks and enhance operational\nefficiency. This research presents a novel framework that integrates Time\nSeries ARIMA (AutoRegressive Integrated Moving Average) models with a\nproprietary formula specifically designed to calculate bad goods after time\nseries forecasting. By leveraging historical data patterns, including sales,\nreturns, and capacity, the model forecasts potential quality failures, enabling\nproactive decision-making. ARIMA is employed to capture temporal trends in time\nseries data, while the newly developed formula quantifies the likelihood and\nimpact of defects with greater precision. Experimental results, validated on a\ndataset spanning 2022-2024 for Organic Beer-G 1 Liter, demonstrate that the\nproposed method outperforms traditional statistical models, such as Exponential\nSmoothing and Holt-Winters, in both prediction accuracy and risk evaluation.\nThis study advances the field of predictive analytics by bridging time series\nforecasting, ARIMA, and risk management in supply chain quality control,\noffering a scalable and practical solution for minimizing losses due to bad\ngoods.",
      "generated_abstract": "vances in large language models (LLMs) have shown promise in\ntask-oriented dialogue, but existing methods often focus on improving the\nperformance of a single task and neglect the communication of users and\nsystems. In this paper, we propose a novel dialogue modeling framework called\nDialogue-Based Task-Oriented Generation (DBTAG). The framework consists of a\ntask-oriented dialogue module, a communication-oriented generation module, and\nan evaluation module. The task-oriented dialogue module provides a\ncomprehensive understanding of the task context, enabling the generation of\neffective responses. The communication-oriented generation module generates\nresponses that are both task-oriented and user-friendly, ensuring that users\nunderstand the system's response. The evaluation module evaluates the\neffectiveness of the model by comparing generated responses with human-written\nresponses",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.22666666666666666,
          "f": 0.16113743617618664
        },
        "rouge-2": {
          "r": 0.011560693641618497,
          "p": 0.018018018018018018,
          "f": 0.014084502280551104
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.22666666666666666,
          "f": 0.16113743617618664
        }
      }
    },
    {
      "paper_id": "math.OC.math/OC/2503.10405v1",
      "true_abstract": "In this paper we aim to construct piecewise-linear (PWL) approximations for\nfunctions of multiple variables and to build compact mixed-integer linear\nprogramming (MILP) formulations to represent the resulting PWL function. On the\none hand, we describe a simple heuristic to iteratively construct a\ntriangulation with a small number of triangles, while decreasing the error of\nthe piecewise-linear approximation. On the other hand, we extend known\ntechniques for modeling PWLs in MILPs more efficiently than state-of-the-art\nmethods permit. The crux of our method is that the MILP model is a result of\nsolving some hard combinatorial optimization problems, for which we present\nheuristic algorithms. The effectiveness of our techniques is demonstrated by a\nseries of computational experiments including a short-term hydropower\nscheduling problem",
      "generated_abstract": "We prove that the number of edges in a 3-colourable graph is bounded from\nabove by a function of the maximum degree, the number of colours, and the\nnumber of vertices. In the case of 4 colours, we prove a more precise result\nthat the number of edges is bounded from above by a function of the maximum\ndegree, the number of colours, and the number of vertices. We also give a\ncombinatorial proof of this result.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.47058823529411764,
          "f": 0.2622950779467885
        },
        "rouge-2": {
          "r": 0.03389830508474576,
          "p": 0.08163265306122448,
          "f": 0.04790418747032916
        },
        "rouge-l": {
          "r": 0.14772727272727273,
          "p": 0.38235294117647056,
          "f": 0.2131147500779361
        }
      }
    },
    {
      "paper_id": "cs.CC.stat/TH/2502.15024v1",
      "true_abstract": "We investigate implications of the (extended) low-degree conjecture (recently\nformalized in [MW23]) in the context of the symmetric stochastic block model.\nAssuming the conjecture holds, we establish that no polynomial-time algorithm\ncan weakly recover community labels below the Kesten-Stigum (KS) threshold. In\nparticular, we rule out polynomial-time estimators that, with constant\nprobability, achieve correlation with the true communities that is\nsignificantly better than random. Whereas, above the KS threshold,\npolynomial-time algorithms are known to achieve constant correlation with the\ntrue communities with high probability[Mas14,AS15].\n  To our knowledge, we provide the first rigorous evidence for the sharp\ntransition in recovery rate for polynomial-time algorithms at the KS threshold.\nNotably, under a stronger version of the low-degree conjecture, our lower bound\nremains valid even when the number of blocks diverges. Furthermore, our results\nprovide evidence of a computational-to-statistical gap in learning the\nparameters of stochastic block models.\n  In contrast to prior work, which either (i) rules out polynomial-time\nalgorithms for hypothesis testing with 1-o(1) success probability [Hopkins18,\nBBK+21a] under the low-degree conjecture, or (ii) rules out low-degree\npolynomials for learning the edge connection probability matrix [LG23], our\napproach provides stronger lower bounds on the recovery and learning problem.\n  Our proof combines low-degree lower bounds from [Hopkins18, BBK+21a] with\ngraph splitting and cross-validation techniques. In order to rule out general\nrecovery algorithms, we employ the correlation preserving projection method\ndeveloped in [HS17].",
      "generated_abstract": "aper, we propose a novel framework for large-scale collaborative\nconversational modeling with attention. Our framework, called Large-Scale\nConversational Attention Modeling (LCA), leverages multi-task attention\nmechanism to jointly optimize the sequence-to-sequence modeling and\ndialogue-to-sequential modeling. Specifically, we introduce a sequence-to-sequence\nmodeling head that generates the next token sequence to maximize the\nreconstruction loss, and a dialogue-to-sequential modeling head that generates\nthe next token sequence to maximize the dialogue generation loss. To ensure\nrepresentation consistency, we propose a multi-task attention mechanism that\nincorporates the multi-task attention mechanism into the multi-head attention\nmechanism. Experiments on the Large Language Model (LLM)-based Conversation\nModeling Benchmark (LCMB)",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07801418439716312,
          "p": 0.18032786885245902,
          "f": 0.10891088687334594
        },
        "rouge-2": {
          "r": 0.004761904761904762,
          "p": 0.012345679012345678,
          "f": 0.006872848216249203
        },
        "rouge-l": {
          "r": 0.07801418439716312,
          "p": 0.18032786885245902,
          "f": 0.10891088687334594
        }
      }
    },
    {
      "paper_id": "math.GN.math/GN/2502.08506v1",
      "true_abstract": "We introduce a two-parameter modification of the cofinality invariant of\nideals. This allows us to include the interaction of a pair of ideals in the\nstudy of base-like structures. We find the values (cardinal numbers or\nwell-known cardinal invariants) of the invariant for pairs of some critical\nideals on $\\omega$. We also dichotomously divide pairs of known ideals on the\nreal line based on whether their relative cofinality is trivial or uncountable.\nFinally, we also study the relative cofinality of maximal ideals.",
      "generated_abstract": "We prove that every finitely generated group of finite type with infinite\ncofibres has a presentation of the form $G = \\langle X, t \\mid g_i x g_j = x\nt^k g_i x t^l g_j x \\rangle$, where $X$ is a finite set of generators, $t$ is a\ntransposition, $g_1, \\ldots, g_n$ are generators, $k, l, n \\in \\mathbb N$, and\n$G$ is normalised. We also provide a characterisation of groups $G$ in this\nsetting.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13725490196078433,
          "p": 0.12280701754385964,
          "f": 0.12962962464506192
        },
        "rouge-2": {
          "r": 0.02631578947368421,
          "p": 0.027777777777777776,
          "f": 0.02702702203068025
        },
        "rouge-l": {
          "r": 0.13725490196078433,
          "p": 0.12280701754385964,
          "f": 0.12962962464506192
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.03084v1",
      "true_abstract": "We present simple to implement Wald-type statistics that deliver a general\nnonparametric inference theory for linear restrictions on varying coefficients\nin a range of spatial autoregressive models. Our theory covers error dependence\nof a general form, allows for a degree of misspecification robustness via\nnonparametric spatial weights and permits inference on both varying regression\nand spatial coefficients. One application of our method finds evidence for\nconstant returns to scale in the production function of the Chinese nonmetal\nmineral industry, while another finds a nonlinear impact of the distance to the\nemployment center on housing prices in Boston. A simulation study confirms that\nour tests perform well in finite-samples.",
      "generated_abstract": "We propose a novel framework for estimation of dynamic stochastic generalised\nregressions (DSGRs) using a data-driven approach. Our framework includes a\nclass of models that are not well-established in the literature, and we propose\na novel algorithm for estimating DSGRs. We demonstrate the efficiency of our\nalgorithm through a simulation study, where we compare our algorithm with\nestimators based on various alternative approaches. The simulation study\nhighlights the performance of our proposed methodology, with an estimated DSGR\nmodel outperforming a number of alternative methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18666666666666668,
          "p": 0.23333333333333334,
          "f": 0.20740740246913592
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.0379746835443038,
          "f": 0.03260869075200924
        },
        "rouge-l": {
          "r": 0.18666666666666668,
          "p": 0.23333333333333334,
          "f": 0.20740740246913592
        }
      }
    },
    {
      "paper_id": "quant-ph.math/IT/2503.09012v1",
      "true_abstract": "The thought experiment of Maxwell's demon highlights the effect of side\ninformation in thermodynamics. In this paper, we present an axiomatic treatment\nof a quantum Maxwell's demon, by introducing a resource-theoretic framework of\nquantum thermodynamics in the presence of quantum side information. Under\nminimal operational assumptions that capture the demon's behaviour, we derive\nthe one-shot work costs of preparing, as well as erasing, a thermodynamic\nsystem whose coupling with the demon's mind is described by a bipartite quantum\nstate. With trivial Hamiltonians, these work costs are precisely captured by\nthe smoothed conditional min- and max-entropies, respectively, thus providing\noperational interpretations for these one-shot information-theoretic quantities\nin microscopic thermodynamics. An immediate, information-theoretic implication\nof our results is an affirmative proof of the conjectured maximality of the\nconditional max-entropy among all axiomatically plausible conditional\nentropies, complementing the recently established minimality of the conditional\nmin-entropy. We then generalize our main results to the setting with nontrivial\nHamiltonians, wherein the work costs of preparation and erasure are captured by\na generalized type of mutual information. Finally, we present a macroscopic\nsecond law of thermodynamics in the presence of quantum side information, in\nterms of a conditional version of the Helmholtz free energy. Our results extend\nthe conceptual connection between thermodynamics and quantum information theory\nby refining the axiomatic common ground between the two theories and revealing\nfundamental insights of each theory in light of the other.",
      "generated_abstract": "aper, we propose a novel framework for the design of quantum\nquantum-in-the-loop (QQ-ITL) architectures, which are capable of simultaneously\ngenerating quantum entanglement and performing quantum computation. The\narchitecture is composed of a quantum controller, an entanglement source, and a\nquantum computer. The controller is designed to control the entanglement\nsource to generate entanglement. The entanglement source is designed to\ngenerate entanglement according to a given protocol. The quantum computer is\ndesigned to perform a given quantum algorithm. We first show that for any\nQQ-ITL architecture, the controller, entanglement source, and quantum\ncomputer can be designed in a unified manner. We then show that, for any\nQQ-ITL architecture, the entanglement source and quantum computer can be\noptimized separately. Finally, we show that any QQ-IT",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13333333333333333,
          "p": 0.3,
          "f": 0.1846153803550297
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.061224489795918366,
          "f": 0.03896103462219647
        },
        "rouge-l": {
          "r": 0.11851851851851852,
          "p": 0.26666666666666666,
          "f": 0.1641025598422092
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.03333v1",
      "true_abstract": "Dynamic networks models describe temporal interactions between social actors,\nand as such have been used to describe financial fraudulent transactions,\ndispersion of destructive invasive species across the globe, and the spread of\nfake news. An important question in all of these examples is what are the\ncausal drivers underlying these processes. Current network models are\nexclusively descriptive and based on correlative structures.\n  In this paper we propose a causal extension of dynamic network modelling. In\nparticular, we prove that the causal model satisfies a set of population\nconditions that uniquely identifies the causal drivers. The empirical analogue\nof these conditions provide a consistent causal discovery algorithm, which\ndistinguishes it from other inferential approaches. Crucially, data from a\nsingle environment is sufficient. We apply the method in an analysis of bike\nsharing data in Washington D.C. in July 2023.",
      "generated_abstract": "r presents a novel Bayesian framework for the estimation of\nregression coefficients in nonlinear models. The framework is based on a\nBayesian hierarchical modeling approach for the joint estimation of the\nregression coefficient and the random effects, with a novel and flexible\nHamilton-Jacobi-like prior on the random effects. The proposed approach\nincorporates the nonlinearity of the regression model into the Bayesian\nframework, enabling a more flexible and realistic modeling of the relationship\nbetween the regressors and the response. By introducing a flexible prior on\nthe random effects, the model is able to accommodate more complex and\nnon-linear relationships between the regressors and the response, which is\nnecessary for modeling situations where the regressors are more complex and\nnon-linear. The proposed Bayesian framework is illustrated through an\napplication to the estimation of the regression coefficients of the linear\nregression",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1568627450980392,
          "p": 0.25,
          "f": 0.1927710795993614
        },
        "rouge-2": {
          "r": 0.014925373134328358,
          "p": 0.018518518518518517,
          "f": 0.016528920677550824
        },
        "rouge-l": {
          "r": 0.14705882352941177,
          "p": 0.234375,
          "f": 0.18072288682827709
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.09075v1",
      "true_abstract": "The pinching-antenna system (PASS) introduces new degrees of freedom (DoFs)\nfor physical layer security (PLS) through pinching beamforming. In this paper,\na couple of scenarios for secure beamforming for PASS are studied. 1) For the\ncase with a single legitimate user (Bob) and a single eavesdropper (Eve), a\nclosed-form expression for the optimal baseband beamformer is derived. On this\nbasis, a gradient-based method is proposed to optimize the activated positions\nof pinching antennas (PAs). 2) For the case with multiple Bobs and multiple\nEves, a fractional programming (FP)-based block coordinate descent (BCD)\nalgorithm, termed FP-BCD, is proposed for optimizing the weighted secrecy\nsum-rate (WSSR). Specifically, a closed-form baseband beamformer is obtained\nvia Lagrange multiplier method. Furthermore, owing to the non-convex objective\nfunction exhibiting numerous stationary points, a low-complexity\none-dimensional search is used to find a high-quality solution of the PAs'\nactivated locations. Numerical results are provided to demonstrate that: i) All\nproposed algorithms achieve stable convergence within a few iterations, ii)\nacross all considered power ranges, the FP-BCD algorithm outperforms baseline\nmethods using zero-forcing (ZF) and maximal-ratio transmission (MRT)\nbeamforming in terms of the WSSR, and iii) PASS achieves a significantly higher\nsecrecy rate than traditional fixed-antenna systems.",
      "generated_abstract": "r proposes a novel system architecture for cooperative MIMO-OFDM\nsystems based on a single-input single-output (SISO) receiver. This architecture\nenables the simultaneous processing of multiple simultaneous users (SUs) in\ndifferent frequency bands, while guaranteeing a common time-frequency\nrepresentation of the received signal. The proposed architecture utilizes\ncoherent detection and beamforming to jointly process the received signals of\nall SUs in the same frequency band. By exploiting the coherent nature of the\ncoherent detection, the proposed architecture achieves high spectral efficiency\nand low power consumption. Furthermore, the proposed architecture provides\nrobustness to additive white Gaussian noise (AWGN) and outage probability\n(OP) guarantees under a given number of users. The performance of the proposed\narchitecture is evaluated through simulations, and it is shown that the\nproposed architecture can",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13286713286713286,
          "p": 0.22093023255813954,
          "f": 0.1659388599385978
        },
        "rouge-2": {
          "r": 0.005319148936170213,
          "p": 0.008771929824561403,
          "f": 0.0066225118565008565
        },
        "rouge-l": {
          "r": 0.11888111888111888,
          "p": 0.19767441860465115,
          "f": 0.14847161103030088
        }
      }
    },
    {
      "paper_id": "eess.IV.cs/HC/2503.09885v1",
      "true_abstract": "Analyzing CT scans, MRIs and X-rays is pivotal in diagnosing and treating\ndiseases. However, detecting and identifying abnormalities from such medical\nimages is a time-intensive process that requires expert analysis and is prone\nto interobserver variability. To mitigate such issues, machine learning-based\nmodels have been introduced to automate and significantly reduce the cost of\nimage segmentation. Despite significant advances in medical image analysis in\nrecent years, many of the latest models are never applied in clinical settings\nbecause state-of-the-art models do not easily interface with existing medical\nimage viewers. To address these limitations, we propose QuickDraw, an\nopen-source framework for medical image visualization and analysis that allows\nusers to upload DICOM images and run off-the-shelf models to generate 3D\nsegmentation masks. In addition, our tool allows users to edit, export, and\nevaluate segmentation masks to iteratively improve state-of-the-art models\nthrough active learning. In this paper, we detail the design of our tool and\npresent survey results that highlight the usability of our software. Notably,\nwe find that QuickDraw reduces the time to manually segment a CT scan from four\nhours to six minutes and reduces machine learning-assisted segmentation time by\n10\\% compared to prior work. Our code and documentation are available at\nhttps://github.com/qd-seg/quickdraw",
      "generated_abstract": "years, deep learning has emerged as a powerful tool for\nestimating the posterior of an unknown distribution. However, the\ncomputational and memory requirements of the likelihood estimator can\nsignificantly limit its applicability. In this paper, we propose a\nreparameterization of the likelihood estimator that significantly reduces\nthe computational cost while maintaining the same accuracy. By incorporating a\nreparameterization, we effectively reduce the computational cost of the\nlikelihood estimator from $\\mathcal{O}(M^3N^2)$ to $\\mathcal{O}(MN)$ where\n$M$ is the number of observations and $N$ is the number of parameters in the\nmodel. Furthermore, we introduce a new approach to efficiently construct the\nreparameterization, which reduces the computational cost from $\\mathcal{O}(M^4N)$\nto $\\mathcal{O}(MN)$. In addition, we propose a novel parameterization",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18382352941176472,
          "p": 0.352112676056338,
          "f": 0.24154588921281717
        },
        "rouge-2": {
          "r": 0.046153846153846156,
          "p": 0.09278350515463918,
          "f": 0.06164383117963065
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.3380281690140845,
          "f": 0.2318840534640249
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2502.08144v2",
      "true_abstract": "In recent years, the dominance of machine learning in stock market\nforecasting has been evident. While these models have shown decreasing\nprediction errors, their robustness across different datasets has been a\nconcern. A successful stock market prediction model minimizes prediction errors\nand showcases robustness across various data sets, indicating superior\nforecasting performance. This study introduces a novel multiple lag order\nprobabilistic model based on trend encoding (TeMoP) that enhances stock market\npredictions through a probabilistic approach. Results across different stock\nindexes from nine countries demonstrate that the TeMoP outperforms the\nstate-of-the-art machine learning models in predicting accuracy and\nstabilization.",
      "generated_abstract": "ork, we propose a novel multi-agent reinforcement learning (MARL)\nframework to trade on the Binance exchange. The proposed system consists of\na centralized algorithm and a decentralized trading agent, which are implemented\nas two separate agents. The centralized algorithm is a two-layer neural network\nwith 128 hidden layers, 512 neurons in each layer, and a 2-layer fully\nconnected neural network for the final layer. The trading agent consists of a\ntwo-layer neural network with 128 neurons in each layer. We first propose an\niterative algorithm to train the centralized algorithm to maximize the expected\nreturn of the Binance exchange. The trading agent is trained using the\ncentralized algorithm. Then, we introduce a reinforcement learning algorithm to\ntrain the trading agent. We evaluate the performance of the proposed system\nusing the Mean Absolute Per",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1232876712328767,
          "p": 0.1267605633802817,
          "f": 0.1249999950009647
        },
        "rouge-2": {
          "r": 0.010869565217391304,
          "p": 0.009708737864077669,
          "f": 0.01025640527232326
        },
        "rouge-l": {
          "r": 0.1232876712328767,
          "p": 0.1267605633802817,
          "f": 0.1249999950009647
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.q-bio/CB/2408.07551v1",
      "true_abstract": "In the recently proposed Graph Vertex Model (GVM), cellular rearrangements\nare implemented as local graph transformations of the cell aggregate,\nrepresented by a knowledge graph [1]. This study extends GVM to incorporate\ncell division, a critical biological process involved in morphogenesis,\nhomeostasis, and disease progression. Like cellular rearrangements, cell\ndivision in GVM begins by identifying a subgraph of nodes and links, involved\nin the division, by matching suitable graph patterns or templates within the\nfull knowledge graph. The matched subgraph is then transformed to incorporate\ntopological changes within the knowledge graph, caused by the division event.\nImportantly, when this transformation is applied to a polygon in a 2D tiling,\nit performs the transformation, required to divide a polygon, indicating that\nthe 3D graph transformation is general and applicable also to 2D vertex models.\nOur extension of GVM enables the study of the dynamics of growing cell\naggregates in 3D to offer new insights into developmental processes and cancer\ngrowth.",
      "generated_abstract": "Inspired by the success of the quasiparticle model for the two-dimensional\ncrystalline liquid, we propose a model for a complex, three-dimensional\ncrystalline liquid, with a large number of atoms in each unit cell. The\nquasiparticle model is based on the concept of the so-called quasiparticle\ndensity and the quasiparticle radius of the system. Our model is based on the\ntwo-dimensional quasiparticle model, but it includes an additional parameter,\nthe concentration of the crystalline component. We show that the model can\nreproduce the main features of the experimental data. In particular, we show\nthat the model reproduces the formation of the hexagonal phase in the\ntwo-dimensional quasiparticle model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12745098039215685,
          "p": 0.22033898305084745,
          "f": 0.16149067858647442
        },
        "rouge-2": {
          "r": 0.02631578947368421,
          "p": 0.046511627906976744,
          "f": 0.03361344076265864
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.2033898305084746,
          "f": 0.14906831833802722
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/AP/2503.06837v1",
      "true_abstract": "This paper investigates a robust empirical Bayes correction for Bayesian\nmodeling. We show the application of the model on income distribution. Income\nshock includes temporal and permanent shocks. We aim to eliminate temporal\nshock and permanent shock using two-step local empirical correction method. Our\nresults show that only 6.7% of the observed income shocks were permanent shock,\nand the posterior (permanent) mean weekly income was reduced from the observed\nincome 415 pounds to 202 pounds for the United Kingdom using the Living Costs\nand Food Survey in 2021-2022. Keywords: Empirical Bayes correction; Outliers;\nBayesian modeling",
      "generated_abstract": "e a novel method for constructing confidence intervals for the\nprediction error of random variables. The confidence intervals are obtained by\nsolving a system of nonlinear equations that are constructed using\nprobability-based estimation. The proposed method is based on the Bayesian\nframework and can be applied to a wide range of settings where the data\ngenerating process is unknown and cannot be modeled using probability\ndistributions. The method is flexible in that it can handle both the case where\nthe random variables are correlated (i.e., they are dependent) and the case where\nthey are not (i.e., they are independent). In the case of independence, the\nconfidence intervals are constructed using the Kullback-Leibler divergence\nbetween the prediction distributions of the dependent random variables. For\ncases of correlation, the confidence intervals are constructed using the\nKullback-Leibler divergence between the prediction distributions of the\nindependent",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18461538461538463,
          "p": 0.1643835616438356,
          "f": 0.1739130384950642
        },
        "rouge-2": {
          "r": 0.044444444444444446,
          "p": 0.035398230088495575,
          "f": 0.03940886205925952
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.136986301369863,
          "f": 0.14492753124868743
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.10445v1",
      "true_abstract": "Reducing the spread of misinformation is challenging. AI-based fact\nverification systems offer a promising solution by addressing the high costs\nand slow pace of traditional fact-checking. However, the problem of how to\neffectively communicate the results to users remains unsolved. Warning labels\nmay seem an easy solution, but they fail to account for fuzzy misinformation\nthat is not entirely fake. Additionally, users' limited attention spans and\nsocial media information should be taken into account while designing the\npresentation. The online experiment (n = 537) investigates the impact of\nsources and granularity on users' perception of information veracity and the\nsystem's usefulness and trustworthiness. Findings show that fine-grained\nindicators enhance nuanced opinions, information awareness, and the intention\nto use fact-checking systems. Source differences had minimal impact on opinions\nand perceptions, except for informativeness. Qualitative findings suggest the\nproposed indicators promote critical thinking. We discuss implications for\ndesigning concise, user-friendly AI fact-checking feedback.",
      "generated_abstract": "of deep learning has transformed many domains, including image\nprocessing and natural language processing. However, these advances have\nsometimes come at the cost of increased computational complexity. To address\nthese concerns, this study proposes a novel approach to efficiently and\naccurately generate textual descriptions of images. Our approach utilizes\nmultimodal language models to generate textual descriptions of images based on\ntheir visual features. We then leverage large language models (LLMs) to\nautomatically convert these descriptions into textual summaries. The summaries\nare used to generate image descriptions, which are then used to create\nvisualization-based documents. We evaluate our approach on the Image-to-Text\n(I2T) and Text-to-Image (T2I) benchmarks, demonstrating that our approach\noutperforms existing approaches in both I2T and T2I. Our results highlight the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10526315789473684,
          "p": 0.1411764705882353,
          "f": 0.12060301018156128
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10526315789473684,
          "p": 0.1411764705882353,
          "f": 0.12060301018156128
        }
      }
    },
    {
      "paper_id": "eess.SY.stat/TH/2503.03328v1",
      "true_abstract": "We address the problem of safety verification for nonlinear stochastic\nsystems, specifically the task of certifying that system trajectories remain\nwithin a safe set with high probability. To tackle this challenge, we adopt a\nset-erosion strategy, which decouples the effects of stochastic disturbances\nfrom deterministic dynamics. This approach converts the stochastic safety\nverification problem on a safe set into a deterministic safety verification\nproblem on an eroded subset of the safe set. The success of this strategy\nhinges on the depth of erosion, which is determined by a probabilistic tube\nthat bounds the deviation of stochastic trajectories from their corresponding\ndeterministic trajectories. Our main contribution is the establishment of a\ntight bound for the probabilistic tube of nonlinear stochastic systems. To\nobtain a probabilistic bound for stochastic trajectories, we adopt a\nmartingale-based approach. The core innovation lies in the design of a novel\nenergy function associated with the averaged moment generating function, which\nforms an affine martingale, a generalization of the traditional c-martingale.\nUsing this energy function, we derive a precise bound for the probabilistic\ntube. Furthermore, we enhance this bound by incorporating the union-bound\ninequality for strictly contractive dynamics. By integrating the derived\nprobabilistic tubes into the set-erosion strategy, we demonstrate that the\nsafety verification problem for nonlinear stochastic systems can be reduced to\na deterministic safety verification problem. Our theoretical results are\nvalidated through applications in reachability-based safety verification and\nsafe controller synthesis, accompanied by several numerical examples that\nillustrate their effectiveness.",
      "generated_abstract": "In this paper, we propose a novel approach for robust learning of state-space\nmodels, based on the use of state-space models with hidden Markov components,\nwhich allow the modeling of complex time-varying dynamics. To this end, we\nintroduce a novel extension of the Bayesian state-space model, based on a\ndiscrete Markov chain that encodes the dynamics of the hidden Markov components\nas a mixture of discrete-time Markov chains, and we provide an algorithm to\nestimate the parameters of the model, based on the proposed Markov chain\nframework. We also discuss the properties of the proposed model, and we\ninvestigate the robustness of the proposed algorithm against state-space model\nfitting errors, as well as the impact of the number of hidden Markov components\non the estimation error. Finally, we present simulation results to illustrate\nthe robustness of the proposed algorithm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15748031496062992,
          "p": 0.273972602739726,
          "f": 0.1999999953645001
        },
        "rouge-2": {
          "r": 0.018867924528301886,
          "p": 0.03508771929824561,
          "f": 0.024539872752456712
        },
        "rouge-l": {
          "r": 0.15748031496062992,
          "p": 0.273972602739726,
          "f": 0.1999999953645001
        }
      }
    },
    {
      "paper_id": "cs.DM.cs/DM/2503.07208v1",
      "true_abstract": "In the Subset Feedback Arc Set in Tournaments, Subset-FAST problem we are\ngiven as input a tournament $T$ with a vertex set $V(T)$ and an arc set $A(T)$,\nalong with a terminal set $S \\subseteq V(T)$, and an integer $ k$. The\nobjective is to determine whether there exists a set $ F \\subseteq A(T) $ with\n$|F| \\leq k$ such that the resulting graph $T-F $ contains no cycle that\nincludes any vertex of $S$. When $S=V(T)$ this is the classic Feedback Arc Set\nin Tournaments (FAST) problem. We obtain the first polynomial kernel for this\nproblem parameterized by the solution size. More precisely, we obtain an\nalgorithm that, given an input instance $(T, S, k)$, produces an equivalent\ninstance $(T',S',k')$ with $k'\\leq k$ and $V(T')=O(k^2)$.\n  It was known that FAST admits a simple quadratic vertex kernel and a\nnon-trivial linear vertex kernel. However, no such kernel was previously known\nfor Subset-FAST. Our kernel employs variants of the most well-known reduction\nrules for FAST and introduces two new reduction rules to identify irrelevant\nvertices. As a result of our kernelization, we also obtain the first\nsub-exponential time FPT algorithm for Subset-FAST.",
      "generated_abstract": "We study the complexity of a problem where, given a graph $G$, a set of\nedge insertions\n$I\\subseteq E(G)$, and an edge-coloring of $G-I$, does the problem of\ndetermining whether there is an edge-coloring of $G$ that preserves the\ncoloring of $G-I$ and does not use more than $k$ colors? We show that this\nproblem is in $\\text{NP}$-hard even when the graph is the complete bipartite\ngraph $K_{2,t}$. We also show that this problem is in $\\text{NP}$-hard even when\nthe graph is the bipartite graph $K_{2,t}$ if $t\\geq 3$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.3469387755102041,
          "f": 0.202380948249008
        },
        "rouge-2": {
          "r": 0.027624309392265192,
          "p": 0.06944444444444445,
          "f": 0.03952568762767778
        },
        "rouge-l": {
          "r": 0.12605042016806722,
          "p": 0.30612244897959184,
          "f": 0.17857142443948423
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.04091v1",
      "true_abstract": "Federated Learning (FL) is a widely adopted privacy-preserving distributed\nlearning framework, yet its generalization performance remains less explored\ncompared to centralized learning. In FL, the generalization error consists of\ntwo components: the out-of-sample gap, which measures the gap between the\nempirical and true risk for participating clients, and the participation gap,\nwhich quantifies the risk difference between participating and\nnon-participating clients. In this work, we apply an information-theoretic\nanalysis via the conditional mutual information (CMI) framework to study FL's\ntwo-level generalization. Beyond the traditional supersample-based CMI\nframework, we introduce a superclient construction to accommodate the two-level\ngeneralization setting in FL. We derive multiple CMI-based bounds, including\nhypothesis-based CMI bounds, illustrating how privacy constraints in FL can\nimply generalization guarantees. Furthermore, we propose fast-rate evaluated\nCMI bounds that recover the best-known convergence rate for two-level FL\ngeneralization in the small empirical risk regime. For specific FL model\naggregation strategies and structured loss functions, we refine our bounds to\nachieve improved convergence rates with respect to the number of participating\nclients. Empirical evaluations confirm that our evaluated CMI bounds are\nnon-vacuous and accurately capture the generalization behavior of FL\nalgorithms.",
      "generated_abstract": "f multi-task learning for complex decision-making tasks is\naccelerating, but existing approaches often suffer from slow learning rates,\nhigh variance, and suboptimal generalization. To address these issues, we\npropose a novel method for multi-task learning, which we call MT-LSTM. Our\nmethod consists of a two-layer LSTM neural network that can learn both\npredictive and task-specific representations of the input, and a simple\nreinforcement learning agent that selects the tasks to learn. We evaluate MT-LSTM\non a variety of multi-task learning tasks, including decision-making,\nstatistical modeling, and reinforcement learning. The results demonstrate that\nMT-LSTM consistently outperforms baseline methods in terms of accuracy,\ngeneralization, and computational efficiency. Our code and dataset are available\nat https://github.com/shrey",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.152,
          "p": 0.2235294117647059,
          "f": 0.18095237613378698
        },
        "rouge-2": {
          "r": 0.011049723756906077,
          "p": 0.018018018018018018,
          "f": 0.013698625424330762
        },
        "rouge-l": {
          "r": 0.144,
          "p": 0.21176470588235294,
          "f": 0.17142856660997746
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.03629v3",
      "true_abstract": "Traffic simulation is essential for autonomous vehicle (AV) development,\nenabling comprehensive safety evaluation across diverse driving conditions.\nHowever, traditional rule-based simulators struggle to capture complex human\ninteractions, while data-driven approaches often fail to maintain long-term\nbehavioral realism or generate diverse safety-critical events. To address these\nchallenges, we propose TeraSim, an open-source, high-fidelity traffic\nsimulation platform designed to uncover unknown unsafe events and efficiently\nestimate AV statistical performance metrics, such as crash rates. TeraSim is\ndesigned for seamless integration with third-party physics simulators and\nstandalone AV stacks, to construct a complete AV simulation system.\nExperimental results demonstrate its effectiveness in generating diverse\nsafety-critical events involving both static and dynamic agents, identifying\nhidden deficiencies in AV systems, and enabling statistical performance\nevaluation. These findings highlight TeraSim's potential as a practical tool\nfor AV safety assessment, benefiting researchers, developers, and policymakers.\nThe code is available at https://github.com/mcity/TeraSim.",
      "generated_abstract": "r presents a framework for designing and integrating autonomous\ncar systems into complex environments, with a focus on human-robot interaction\n(HRI). The framework aims to enhance the safety of autonomous vehicles by\nincorporating human-centered design principles in the vehicle design process. To\nthis end, the framework includes three main components: (1) a human-centered\ndesign toolkit, (2) a human-in-the-loop simulation environment, and (3) a\nhuman-robot interaction (HRI) framework. The toolkit provides a framework for\ndesigning and evaluating human-centered design scenarios. The simulation\nenvironment facilitates human-robot interaction design by providing a\nrepresentative, realistic, and user-friendly environment for evaluating\nHRI. The framework is illustrated through a case study of a mobile robot that\nassists elderly individuals with daily activities",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12389380530973451,
          "p": 0.1891891891891892,
          "f": 0.1497326155383341
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11504424778761062,
          "p": 0.17567567567567569,
          "f": 0.13903742837255872
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.10611v1",
      "true_abstract": "We provide a full characterization of geodesic completeness for spaces of\nconfigurations of landmarks with smooth Riemannian metrics that satisfy a\nrotational and translation invariance and which are induced from metrics on\nsubgroups of the diffeomorphism group for the shape domain. These spaces are\nwidely used for applications in shape analysis, for example, for measuring\nshape changes in medical imaging and morphometrics in biology. For statistics\nof such data to be well-defined, it is imperative to know if geodesics exist\nfor all times. We extend previously known sufficient conditions for geodesic\ncompleteness based on the regularity of the metric to give a full\ncharacterization for smooth Riemannian metrics with a rotational and\ntranslation invariance by means of an integrability criterion that involves\nonly the behavior of the cometric kernel as landmarks approach collision. We\nfurther use the integrability criterion for geodesic completeness and previous\nwork on stochastic completeness to construct a family of Riemannian landmark\nmanifolds that are geodesically complete but stochastically incomplete.",
      "generated_abstract": "We study the asymptotic behavior of the entropy critical exponent of\na class of Markov chains on finite state spaces. Our main results show that\nthe critical exponent is the same as that of the associated diffusion,\nprovided the chain is ergodic and the state space has a finite $p$-th\ndimensional perimeter. We prove that for $p>2$, the critical exponent\ndiverges as $n \\to \\infty$ if the chain is ergodic and the state space has a\nfinite $p$-th dimensional perimeter. We also prove that the critical exponent\ndiverges as $n \\to \\infty$ if the chain is not ergodic and the state space\nhas a finite $p$-th dimensional perimeter.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1326530612244898,
          "p": 0.2826086956521739,
          "f": 0.18055555120756184
        },
        "rouge-2": {
          "r": 0.013513513513513514,
          "p": 0.030303030303030304,
          "f": 0.018691584519172952
        },
        "rouge-l": {
          "r": 0.10204081632653061,
          "p": 0.21739130434782608,
          "f": 0.1388888845408952
        }
      }
    },
    {
      "paper_id": "cs.CV.q-bio/CB/2502.05459v1",
      "true_abstract": "White blood cells (WBC) are important parts of our immune system, and they\nprotect our body against infections by eliminating viruses, bacteria, parasites\nand fungi. The number of WBC types and the total number of WBCs provide\nimportant information about our health status. A traditional method,\nconvolutional neural networks (CNN), a deep learning architecture, can classify\nthe blood cell from a part of an object and perform object recognition. Various\nCNN models exhibit potential; however, their development often involves ad-hoc\nprocesses that neglect unnecessary layers, leading to issues with unbalanced\ndatasets and insufficient data augmentation. To address these challenges, we\npropose a novel ensemble approach that integrates three CNN architectures, each\nuniquely configured with different dropout and max-pooling layer settings to\nenhance feature learning. This ensemble model, named DCENWCNet, effectively\nbalances the bias-variance trade-off. When evaluated on the widely recognized\nRabbin-WBC dataset, our model outperforms existing state-of-the-art networks,\nachieving highest mean accuracy. Additionally, it demonstrates superior\nperformance in precision, recall, F1-score, and Area Under the ROC Curve (AUC)\nacross all categories. To delve deeper into the interpretability of\nclassifiers, we employ reliable post-hoc explanation techniques, including\nLocal Interpretable Model-Agnostic Explanations (LIME). These methods\napproximate the behavior of a black-box model by elucidating the relationships\nbetween feature values and predictions. Interpretable results enable users to\ncomprehend and validate the model's predictions, thereby increasing their\nconfidence in the automated diagnosis.",
      "generated_abstract": "nt trend in computational biology is to develop large language\nmodel (LLM)-based methods for sequence-to-sequence learning, which are\ncapable of processing large textual data. However, the use of LLMs for\ncomputational biology is limited by their inability to process large images\nefficiently. This limitation poses a significant challenge for biomedical\napplications, particularly in the field of computational pathology. To address\nthis challenge, we propose a novel approach that utilizes an LLM to process\nimages, followed by a sequence-to-sequence model to generate high-quality\npathology annotations. Our method leverages a pretrained LLM to generate\nsemantic-level features for image analysis, which is then used to improve the\nperformance of a sequence-to-sequence model for pathology annotation.\nFurthermore, we propose a novel method for training a sequence-to-sequence\nmodel",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11602209944751381,
          "p": 0.2727272727272727,
          "f": 0.1627906934868699
        },
        "rouge-2": {
          "r": 0.030973451327433628,
          "p": 0.06363636363636363,
          "f": 0.04166666226261385
        },
        "rouge-l": {
          "r": 0.11049723756906077,
          "p": 0.2597402597402597,
          "f": 0.15503875550237375
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/OT/2501.10482v1",
      "true_abstract": "Random fuzzy variables join the modeling of the impreciseness (due to their\n``fuzzy part'') and randomness. Statistical samples of such objects are widely\nused, and their direct, numerically effective generation is therefore\nnecessary. Usually, these samples consist of triangular or trapezoidal fuzzy\nnumbers. In this paper, we describe theoretical results and simulation\nalgorithms for another family of fuzzy numbers -- LR fuzzy numbers with\ninterval-valued cores. Starting from a simulation perspective on the piecewise\nlinear LR fuzzy numbers with the interval-valued cores, their limiting behavior\nis then considered. This leads us to the numerically efficient algorithm for\nsimulating a sample consisting of such fuzzy values.",
      "generated_abstract": "aper, we propose an unsupervised machine learning approach for\nidentifying the most significant features of a multi-modal dataset. The\nmain contribution of this work is to propose a novel approach for feature\nselection and a novel method for feature importance evaluation in the\nmulti-modal domain. The proposed approach uses the Sparse Pixel-wise Similarity\n(SPS) matrix as a feature representation for the multi-modal data. The SPS\nmatrix is computed by performing a similarity analysis between a pair of\nimages of a multi-modal dataset. The proposed method has the following\nadvantages: (i) the feature selection is performed without any external\nknowledge, (ii) the feature selection process is performed in an unsupervised\nmanner, (iii) the evaluation of the feature importance is performed in a\ndifferent manner than the evaluation of the importance of the class labels, and\n(iv) the feature importance evaluation is performed in a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12162162162162163,
          "p": 0.125,
          "f": 0.12328766623381518
        },
        "rouge-2": {
          "r": 0.01020408163265306,
          "p": 0.008771929824561403,
          "f": 0.009433957292633448
        },
        "rouge-l": {
          "r": 0.12162162162162163,
          "p": 0.125,
          "f": 0.12328766623381518
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2501.07508v1",
      "true_abstract": "This paper applies deep reinforcement learning (DRL) to optimize liquidity\nprovisioning in Uniswap v3, a decentralized finance (DeFi) protocol\nimplementing an automated market maker (AMM) model with concentrated liquidity.\nWe model the liquidity provision task as a Markov Decision Process (MDP) and\ntrain an active liquidity provider (LP) agent using the Proximal Policy\nOptimization (PPO) algorithm. The agent dynamically adjusts liquidity positions\nby using information about price dynamics to balance fee maximization and\nimpermanent loss mitigation. We use a rolling window approach for training and\ntesting, reflecting realistic market conditions and regime shifts. This study\ncompares the data-driven performance of the DRL-based strategy against common\nheuristics adopted by small retail LP actors that do not systematically modify\ntheir liquidity positions. By promoting more efficient liquidity management,\nthis work aims to make DeFi markets more accessible and inclusive for a broader\nrange of participants. Through a data-driven approach to liquidity management,\nthis work seeks to contribute to the ongoing development of more efficient and\nuser-friendly DeFi markets.",
      "generated_abstract": "y presents a novel approach for the estimation of the probability\nof a corporate default under the joint impact of the COVID-19 pandemic and\ninflation. The model incorporates the impact of COVID-19 on corporate bond\nprices, inflation, and the impact of inflation on corporate bond prices. The\nmodel is calibrated using data from 2021, and the calibrated model is used to\npredict the probability of corporate defaults in the next year. The model\npredicts that the probability of default will increase by 25.46% in 2022, and\nby 26.83% in 2023. The model also predicts that the probability of default in\n2024 will increase by 17.67% in inflation-adjusted terms. The calibrated\nmodel provides valuable insights into the impact of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1092436974789916,
          "p": 0.21311475409836064,
          "f": 0.14444443996358042
        },
        "rouge-2": {
          "r": 0.012578616352201259,
          "p": 0.021052631578947368,
          "f": 0.01574802681350502
        },
        "rouge-l": {
          "r": 0.10084033613445378,
          "p": 0.19672131147540983,
          "f": 0.1333333288524693
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.06046v1",
      "true_abstract": "Televised debates between presidential candidates are often regarded as the\nexemplar of persuasive communication. Yet, recent evidence from Le Pennec and\nPons (2023) indicates that they may not sway voters as strongly as popular\nbelief suggests. We revisit their findings through the lens of the persuasion\nrate and introduce a robust framework that does not require exogenous\ntreatment, parallel trends, or credible instruments. Instead, we leverage\nplausible monotonicity assumptions to partially identify the persuasion rate\nand related parameters. Our results reaffirm that the sharp upper bounds on the\npersuasive effects of TV debates remain modest.",
      "generated_abstract": "The use of structural econometric models has grown significantly over the\ncurrent decade, with an emphasis on leveraging machine learning techniques.\nHowever, the use of these models has been hindered by the lack of a comprehensive\nframework to develop and evaluate them. This paper introduces the structural\nmodel framework, which is designed to address these issues. It provides a\ncomprehensive approach for developing and evaluating structural models,\nincluding a novel methodology for model selection that employs a novel\ninformation criterion. The framework is also used to develop a novel model for\nthe empirical analysis of wage dynamics, which is shown to produce highly\naccurate results. These results are validated using both simulated and real data\nsources. The paper concludes with a discussion of future research directions\nand potential applications of the framework.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12987012987012986,
          "p": 0.11494252873563218,
          "f": 0.12195121453078545
        },
        "rouge-2": {
          "r": 0.01098901098901099,
          "p": 0.008064516129032258,
          "f": 0.009302320699191312
        },
        "rouge-l": {
          "r": 0.12987012987012986,
          "p": 0.11494252873563218,
          "f": 0.12195121453078545
        }
      }
    },
    {
      "paper_id": "math.PR.q-bio/SC/2406.12493v1",
      "true_abstract": "We prove a Large Deviation Principle for Piecewise Deterministic Markov\nProcesses (PDMPs). This is an asymptotic estimate for the probability of a\ntrajectory in the large size limit. Explicit Euler-Lagrange equations are\ndetermined for computing optimal first-hitting-time trajectories. The results\nare applied to a model of stochastic calcium dynamics. It is widely conjectured\nthat the mechanism of calcium puff generation is a multiscale process: with\nmicroscopic stochastic fluctuations in the opening and closing of individual\nchannels generating cell-wide waves via the diffusion of calcium and other\nsignaling molecules. We model this system as a PDMP, with $N \\gg 1$ stochastic\ncalcium channels that are coupled via the ambient calcium concentration. We\nemploy the Large Deviations theory to estimate the probability of cell-wide\ncalcium waves being produced through microscopic stochasticity.",
      "generated_abstract": "f protein phosphatase inhibitors (PPIs) is a common therapeutic\nmethod in oncology, particularly in the treatment of solid tumors. The\nphosphatase activity of these inhibitors is essential for their function.\nHowever, the role of protein kinase C (PKC) inhibitors in cancer treatment is\nstill unclear. This study investigates the effects of the PKC inhibitor,\nMK-2206, on the phosphatase activity of the PKC inhibitor, MK-801, and the\nphosphatase activity of the PKC inhibitor, MK-801, on the phosphatase\nactivity of the PKC inhibitor, MK-2206. The results show that MK-2206 has a\nstrong inhibitory effect on the phosphat",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13095238095238096,
          "p": 0.21153846153846154,
          "f": 0.1617647011591697
        },
        "rouge-2": {
          "r": 0.02459016393442623,
          "p": 0.04054054054054054,
          "f": 0.03061224019783496
        },
        "rouge-l": {
          "r": 0.13095238095238096,
          "p": 0.21153846153846154,
          "f": 0.1617647011591697
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.01179v1",
      "true_abstract": "Chemical reaction network theory provides powerful tools for rigorously\nunderstanding chemical reactions and the dynamical systems and differential\nequations that represent them. A frequent issue with mathematical analyses of\nthese networks is the reliance on explicit parameter values which in many cases\ncannot be determined experimentally. This can make analyzing a dynamical system\ninfeasible, particularly when the size of the system is large. One approach is\nto analyze subnetworks of the full network and use the results for a full\nanalysis.\n  Our focus is on the equilibria of reaction networks. Gr\\\"obner basis\ncomputation is a useful approach for solving the polynomial equations which\ncorrespond to equilibria of a dynamical system. We identify a class of networks\nfor which Gr\\\"obner basis computations of subnetworks can be used to\nreconstruct the more expensive Gr\\\"obner basis computation of the whole\nnetwork. We compliment this result with tools to determine if a steady state\ncan exist, and if so, how many.",
      "generated_abstract": "of protein-protein interactions (PPIs) is crucial for understanding\nthe complex biological interactions between proteins and their partners. A\nrecent study on PPIs in the cytoplasmic region of the human immunodeficiency\nvirus (HIV) demonstrated the importance of this region in the regulation of\nviral replication and the activation of HIV-1 proteins. The cytoplasmic domain\nof HIV-1 gp120 (gp120) contains 1226 amino acids and is composed of 227\nresidues in the extracellular domain (ECD) and 95 residues in the transmembrane\ndomain (TMD). The extracellular domain (ECD) plays a crucial role in the\nregulation of HIV-1 replication and the activation of viral proteins, while",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11827956989247312,
          "p": 0.19298245614035087,
          "f": 0.14666666195466682
        },
        "rouge-2": {
          "r": 0.013513513513513514,
          "p": 0.023809523809523808,
          "f": 0.01724137469084547
        },
        "rouge-l": {
          "r": 0.11827956989247312,
          "p": 0.19298245614035087,
          "f": 0.14666666195466682
        }
      }
    },
    {
      "paper_id": "math.OC.math/OC/2503.10005v1",
      "true_abstract": "Training deep neural networks is challenging. To accelerate training and\nenhance performance, we propose PadamP, a novel optimization algorithm. PadamP\nis derived by applying the adaptive estimation of the p-th power of the\nsecond-order moments under scale invariance, enhancing projection adaptability\nby modifying the projection discrimination condition. It is integrated into\nAdam-type algorithms, accelerating training, boosting performance, and\nimproving generalization in deep learning. Combining projected gradient\nbenefits with adaptive moment estimation, PadamP tackles unconstrained\nnon-convex problems. Convergence for the non-convex case is analyzed, focusing\non the decoupling of first-order moment estimation coefficients and\nsecond-order moment estimation coefficients. Unlike prior work relying on , our\nproof generalizes the convergence theorem, enhancing practicality. Experiments\nusing VGG-16 and ResNet-18 on CIFAR-10 and CIFAR-100 show PadamP's\neffectiveness, with notable performance on CIFAR-10/100, especially for VGG-16.\nThe results demonstrate that PadamP outperforms existing algorithms in terms of\nconvergence speed and generalization ability, making it a valuable addition to\nthe field of deep learning optimization.",
      "generated_abstract": "We prove that the set of all nonnegative solutions of a given ordinary\ndifferential equation has a non-empty interior, with respect to the weak\ntopology. In particular, we prove that the set of all solutions of a given\nordinary differential equation has a non-empty interior, with respect to the\nweak topology. This result, combined with the results in [22",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08035714285714286,
          "p": 0.2903225806451613,
          "f": 0.1258741224783609
        },
        "rouge-2": {
          "r": 0.006493506493506494,
          "p": 0.02702702702702703,
          "f": 0.01047120106466473
        },
        "rouge-l": {
          "r": 0.08035714285714286,
          "p": 0.2903225806451613,
          "f": 0.1258741224783609
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/ST/2502.15726v1",
      "true_abstract": "The marketing departments of financial institutions strive to craft products\nand services that cater to the diverse needs of businesses of all sizes.\nHowever, it is evident upon analysis that larger corporations often receive a\nmore substantial portion of available funds. This disparity arises from the\nrelative ease of assessing the risk of default and bankruptcy in these more\nprominent companies. Historically, risk analysis studies have focused on data\nfrom publicly traded or stock exchange-listed companies, leaving a gap in\nknowledge about small and medium-sized enterprises (SMEs). Addressing this gap,\nthis study introduces a method for evaluating SMEs by generating images for\nprocessing via a convolutional neural network (CNN). To this end, more than\n10,000 images, one for each company in the sample, were created to identify\nscenarios in which the CNN can operate with higher assertiveness and reduced\ntraining error probability. The findings demonstrate a significant predictive\ncapacity, achieving 97.8% accuracy, when a substantial number of images are\nutilized. Moreover, the image creation method paves the way for potential\napplications of this technique in various sectors and for different analytical\npurposes.",
      "generated_abstract": "We propose a novel method for identifying and pricing financial options in\nthe presence of correlated volatility, focusing on the pricing of American\noptions on a European call. Our approach is based on the analysis of a\nstructural stochastic volatility model for the underlying asset, which\nintegrates the effect of correlated volatility with the intrinsic variance of\nthe asset. This structure enables us to derive closed-form expressions for the\nvolatility and the corresponding option pricing formulas. We apply our\nframework to a two-asset model, where we consider both the European and the\nAmerican call option on the European asset. Our results show that the\nstructural volatility model can accurately capture the volatility of the\nunderlying asset and the correlations between the underlying and the call\nprice, which allows us to derive closed-form formulas for the European call\nprice and the American call price.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12318840579710146,
          "p": 0.23943661971830985,
          "f": 0.16267942135115968
        },
        "rouge-2": {
          "r": 0.01098901098901099,
          "p": 0.01680672268907563,
          "f": 0.013289031763889536
        },
        "rouge-l": {
          "r": 0.10869565217391304,
          "p": 0.2112676056338028,
          "f": 0.14354066537029844
        }
      }
    },
    {
      "paper_id": "physics.atom-ph.physics/atom-ph/2503.07161v1",
      "true_abstract": "We show that atomic antimatter spectroscopy can be used to search for new\nbosons that carry spin-dependent exotic forces between antifermions. A\ncomparison of a recent precise measurement of the hyperfine splitting of the\n$1$S and $2$S electronic levels of antihydrogen and bound-state quantum\nelectrodynamics theory yields the first tests of positron-antiproton exotic\ninteractions, constraining the dimensionless coupling strengths $g_pg_p$,\n$g_Vg_V$ and $g_Ag_A$, corresponding to the exchange of a pseudoscalar\n(axionlike), vector, or axial-vector boson, respectively. We also discuss new\ntests of CPT invariance with exotic spin-dependent and spin-independent\ninteractions involving antimatter.",
      "generated_abstract": "the first observation of the spin-orbit splitting of a doubly\nexcited state in the $^3E$ ground state of the $^1\\Sigma_g^+$ ground state of\nmethylideneiminoxylidene cyclopropen-1-one (MDI-CP1) by time-resolved\nphotoemission spectroscopy (TR-PES). The energy separation of the spin-orbit\nsplit $^3E$ and $^1E$ states is 1.33 eV, which is 1.56 times larger than the\nspin-orbit splitting between the $^3E$ and $^1E$ states in the ground state of\nthe $^1\\Sigma_u^+$ ground state of MDI-CP1. This enhancement is due to the\ndecrease of the rotational barrier and the increase of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.1836734693877551,
          "f": 0.152542368024993
        },
        "rouge-2": {
          "r": 0.056818181818181816,
          "p": 0.07352941176470588,
          "f": 0.06410255918474726
        },
        "rouge-l": {
          "r": 0.10144927536231885,
          "p": 0.14285714285714285,
          "f": 0.11864406294024726
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.08953v1",
      "true_abstract": "Digital twin (DT) has emerged as a powerful tool to facilitate monitoring,\ncontrol, and other decision-making tasks in real-world engineering systems.\nOnline update methods have been proposed to update DT models. Considering the\ndegradation behavior in the system lifecycle, these methods fail to enable DT\nmodels to predict the system responses affected by the system degradation over\ntime. To alleviate this problem, degradation models of measurable parameters\nhave been integrated into DT construction. However, identifying the degradation\nparameters relies on prior knowledge of the system and expensive experiments.\nTo mitigate those limitations, this paper proposes a lifelong update method for\nDT models to capture the effects of system degradation on system responses\nwithout any prior knowledge and expensive offline experiments on the system.\nThe core idea in the work is to represent the system degradation during the\nlifecycle as the dynamic changes of DT configurations (i.e., model parameters\nwith a fixed model structure) at all degradation stages. During the lifelong\nupdate process, an Autoencoder is adopted to reconstruct the model parameters\nof all hidden layers simultaneously, so that the latent features taking into\naccount the dependencies among hidden layers are obtained for each degradation\nstage. The dynamic behavior of latent features among successive degradation\nstages is then captured by a long short-term memory model, which enables\nprediction of the latent feature at any unseen stage. Based on the predicted\nlatent features, the model configuration at future degradation stage is\nreconstructed to determine the new DT model, which predicts the system\nresponses affected by the degradation at the same stage. The test results on\ntwo engineering datasets demonstrate that the proposed update method could\ncapture effects of system degradation on system responses during the lifecycle.",
      "generated_abstract": "r introduces the concept of \"multi-sensor fusion\", which is a\nmulti-sensor fusion paradigm for enhancing the performance of autonomous\nvehicles (AVs). The main contributions of this work are threefold. First, we\nintroduce a novel method for generating multi-sensor fusion signals by\ntranslating the existing methods for generating sensor fusion signals into\nmulti-sensor fusion signals. Second, we propose a novel method for generating\nmulti-sensor fusion signals that can enhance the performance of AVs. Third, we\nintroduce a multi-sensor fusion method for enhancing the performance of AVs.\nThis method is based on the concept of multi-sensor fusion and is a novel\napproach to generate multi-sensor fusion signals. This method is based on\ngenerating multi-sensor fusion signals by translating the existing methods for\ngenerating sensor fusion signals",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12413793103448276,
          "p": 0.33962264150943394,
          "f": 0.18181817789766358
        },
        "rouge-2": {
          "r": 0.008298755186721992,
          "p": 0.02666666666666667,
          "f": 0.012658224227889195
        },
        "rouge-l": {
          "r": 0.1103448275862069,
          "p": 0.3018867924528302,
          "f": 0.16161615769564341
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.00741v2",
      "true_abstract": "Fully-supervised lesion recognition methods in medical imaging face\nchallenges due to the reliance on large annotated datasets, which are expensive\nand difficult to collect. To address this, synthetic lesion generation has\nbecome a promising approach. However, existing models struggle with\nscalability, fine-grained control over lesion attributes, and the generation of\ncomplex structures. We propose LesionDiffusion, a text-controllable lesion\nsynthesis framework for 3D CT imaging that generates both lesions and\ncorresponding masks. By utilizing a structured lesion report template, our\nmodel provides greater control over lesion attributes and supports a wider\nvariety of lesion types. We introduce a dataset of 1,505 annotated CT scans\nwith paired lesion masks and structured reports, covering 14 lesion types\nacross 8 organs. LesionDiffusion consists of two components: a lesion mask\nsynthesis network (LMNet) and a lesion inpainting network (LINet), both guided\nby lesion attributes and image features. Extensive experiments demonstrate that\nLesionDiffusion significantly improves segmentation performance, with strong\ngeneralization to unseen lesion types and organs, outperforming current\nstate-of-the-art models. Code will be available at\nhttps://github.com/HengruiTianSJTU/LesionDiffusion.",
      "generated_abstract": "t a method for segmenting tumors from MRI scans. By combining\nrepresentation learning with a contrastive learning framework, we construct a\nlatent space for tumor segmentation, where tumor features are embedded and\nrepresented. This latent space is used for classification and semantic segmentation\nof tumors in MRI scans. We propose a novel contrastive loss, which is\noptimized using self-supervised learning. This contrastive loss encourages\nsimilarities between the tumor and healthy images, which leads to more accurate\nclassification of tumors. The proposed method is evaluated using a\npublicly available dataset of MRI scans of breast cancer. The results show that\nthe proposed method achieves a mean accuracy of 80% on the test set, with a\nprecision of 98.3% and a recall of 96.1%. The proposed method is publicly\navailable at",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.175,
          "p": 0.2625,
          "f": 0.2099999952000001
        },
        "rouge-2": {
          "r": 0.024539877300613498,
          "p": 0.03333333333333333,
          "f": 0.028268546352184037
        },
        "rouge-l": {
          "r": 0.15833333333333333,
          "p": 0.2375,
          "f": 0.1899999952000001
        }
      }
    },
    {
      "paper_id": "cs.MM.cs/MM/2503.07911v1",
      "true_abstract": "Pixel-level segmentation is essential in remote sensing, where foundational\nvision models like CLIP and Segment Anything Model(SAM) have demonstrated\nsignificant capabilities in zero-shot segmentation tasks. Despite their\nadvances, challenges specific to remote sensing remain substantial. Firstly,\nThe SAM without clear prompt constraints, often generates redundant masks, and\nmaking post-processing more complex. Secondly, the CLIP model, mainly designed\nfor global feature alignment in foundational models, often overlooks local\nobjects crucial to remote sensing. This oversight leads to inaccurate\nrecognition or misplaced focus in multi-target remote sensing imagery. Thirdly,\nboth models have not been pre-trained on multi-scale aerial views, increasing\nthe likelihood of detection failures. To tackle these challenges, we introduce\nthe innovative VTPSeg pipeline, utilizing the strengths of Grounding DINO,\nCLIP, and SAM for enhanced open-vocabulary image segmentation. The Grounding\nDINO+(GD+) module generates initial candidate bounding boxes, while the CLIP\nFilter++(CLIP++) module uses a combination of visual and textual prompts to\nrefine and filter out irrelevant object bounding boxes, ensuring that only\npertinent objects are considered. Subsequently, these refined bounding boxes\nserve as specific prompts for the FastSAM model, which executes precise\nsegmentation. Our VTPSeg is validated by experimental and ablation study\nresults on five popular remote sensing image segmentation datasets.",
      "generated_abstract": "r investigates the problem of optimizing the number of communication\nsteps required to achieve a certain degree of accuracy in the context of\nmulti-agent systems. Specifically, we focus on a communication-aware\nstochastic-reinforcement learning (C-SRL) problem where a multi-agent system\n(MAS) observes the environment and learns a policy to select and communicate\nwith a subset of the agents. The MAS then selects the best communication\nstrategy based on the selected agents' communication accuracy and communicates\nthe selected strategy to the rest of the agents. In this work, we focus on the\nproblem of optimizing the number of communication steps required to achieve a\ngiven degree of accuracy in the context of multi-agent systems. We formulate\nthe problem as a constrained integer linear programming (ILP) problem and\npresent a polynomial-time algorithm to solve it. We show that the problem can\nbe reduced to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08904109589041095,
          "p": 0.17567567567567569,
          "f": 0.11818181371735555
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07534246575342465,
          "p": 0.14864864864864866,
          "f": 0.0999999955355374
        }
      }
    },
    {
      "paper_id": "math.ST.math/ST/2503.08355v1",
      "true_abstract": "This work addresses the problem of estimating a vector field from a noisy\nOrdinary Differential Equation (ODE) in a non-parametric regression setting\nwith a random design for initial values. More specifically, given a vector\nfield $ f:\\mathbb{R}^{D}\\rightarrow \\mathbb{R}^{D}$ governing a dynamical\nsystem defined by the autonomous ODE: $y' = f(y)$, we assume that the\nobservations are $\\tilde{y}_{X_{i}}(t_{j}) = y_{X_{i}}(t_{j}) +\n\\varepsilon_{i,j}$ where $y_{X_{i}}(t_{j})$ is the solution of the ODE at time\n$t_{j}$ with initial condition $y(0) = X_{i}$, $X_{i}$ is sampled from a\nprobability distribution $\\mu$, and $\\varepsilon_{i,j}$ some noise. In this\ncontext, we investigate, from a minimax perspective, the pointwise\nreconstruction of $f$ within the envelope of trajectories originating from the\nsupport of $\\mu$. We propose an estimation strategy based on preliminary flow\nreconstruction and techniques from derivative estimation in non-parametric\nregression. Under mild assumptions on $f$, we establish convergence rates that\ndepend on the temporal resolution, the number of sampled initial values and the\nmass concentration of $\\mu$. Importantly, we show that these rates are minimax\noptimal. Furthermore, we discuss the implications of our results in a manifold\nlearning setting, providing insights into how our approach can mitigate the\ncurse of dimensionality.",
      "generated_abstract": "We prove the existence of a global minimizer for the problem\n$$\\begin{aligned} \\min_{\\omega \\in \\mathcal{W}} \\int_\\Omega \\nabla\nu\\cdot \\nabla \\omega \\,dx + \\int_\\Omega f\\cdot \\omega \\,dx + \\int_\\Omega\ng\\cdot \\omega \\,dx, \\end{aligned}$$ where $u\\in W^{1,2}(\\Omega,\\mathbb{R}^2)$ and\n$f,g\\in L^2(\\Omega,\\mathbb{R}^2)$, where $W^{1,2}$ is the Sobolev space of\nsquare integrable functions with square integrable derivatives in $L^2(\\Omega)$.\nThe minimizer is given by the weak solution of a system of wave equations. We\nalso provide a numerical scheme for the problem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12213740458015267,
          "p": 0.3018867924528302,
          "f": 0.17391303937677233
        },
        "rouge-2": {
          "r": 0.021621621621621623,
          "p": 0.057971014492753624,
          "f": 0.03149605903496857
        },
        "rouge-l": {
          "r": 0.10687022900763359,
          "p": 0.2641509433962264,
          "f": 0.1521739089419897
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2503.06707v1",
      "true_abstract": "We extend the scope of differential machine learning and introduce a new\nbreed of supervised principal component analysis to reduce dimensionality of\nDerivatives problems. Applications include the specification and calibration of\npricing models, the identification of regression features in least-square\nMonte-Carlo, and the pre-processing of simulated datasets for (differential)\nmachine learning.",
      "generated_abstract": "We propose a novel method for the estimation of the mean reversion of\nrecent price movements in financial markets using a Bayesian approach. The\nmethodology is based on a joint estimation of the mean reversion and the\nexponential moving average of the market price. In addition to providing\nmean-reversion estimates, the proposed approach is able to assess the\nprobability of the market moving in the direction of the mean reversion. The\nmethodology is applied to the CryptoCompare dataset to estimate the mean\nreversion of Bitcoin. The estimation results are in good agreement with the\nactual mean reversion. Moreover, we show that the estimated mean reversion\nfollows a logistic distribution, which is in good agreement with the\nlogistic distribution proposed by Schaal and Sperber (2004).",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20512820512820512,
          "p": 0.11940298507462686,
          "f": 0.15094339157529382
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.00980392156862745,
          "f": 0.013245028728565986
        },
        "rouge-l": {
          "r": 0.1794871794871795,
          "p": 0.1044776119402985,
          "f": 0.132075467046992
        }
      }
    },
    {
      "paper_id": "cs.CV.stat/OT/2501.12596v1",
      "true_abstract": "This expository paper introduces a simplified approach to image-based quality\ninspection in manufacturing using OpenAI's CLIP (Contrastive Language-Image\nPretraining) model adapted for few-shot learning. While CLIP has demonstrated\nimpressive capabilities in general computer vision tasks, its direct\napplication to manufacturing inspection presents challenges due to the domain\ngap between its training data and industrial applications. We evaluate CLIP's\neffectiveness through five case studies: metallic pan surface inspection, 3D\nprinting extrusion profile analysis, stochastic textured surface evaluation,\nautomotive assembly inspection, and microstructure image classification. Our\nresults show that CLIP can achieve high classification accuracy with relatively\nsmall learning sets (50-100 examples per class) for single-component and\ntexture-based applications. However, the performance degrades with complex\nmulti-component scenes. We provide a practical implementation framework that\nenables quality engineers to quickly assess CLIP's suitability for their\nspecific applications before pursuing more complex solutions. This work\nestablishes CLIP-based few-shot learning as an effective baseline approach that\nbalances implementation simplicity with robust performance, demonstrated in\nseveral manufacturing quality control applications.",
      "generated_abstract": "aper, we present a novel method for measuring the spatial correlation\nof a signal in a given scene. This is achieved by combining a local\nfeature-based approach with a global context-aware learning scheme, which\nenables the extraction of a global-local representation of the scene. Our\nmethodology is based on the assumption that the scene is composed of a\nhomogeneous set of objects, and that these objects exhibit a certain degree of\ncorrelation. This assumption is supported by the observation that objects that\nare closer to each other tend to share similar features. The proposed method\nrequires only a few examples of the scene, and is therefore amenable to\nreal-time processing. To the best of our knowledge, this is the first work that\nproposes a method for measuring spatial correlation in a scene composed of\nobjects that are distributed randomly in space. We validate the method through\ntwo experiments: the first focuses on the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.136,
          "p": 0.19101123595505617,
          "f": 0.15887849981439442
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.136,
          "p": 0.19101123595505617,
          "f": 0.15887849981439442
        }
      }
    },
    {
      "paper_id": "math.SG.math/AT/2503.09783v1",
      "true_abstract": "For a Weinstein manifold, we compare and contrast the properties of admitting\nan arboreal skeleton and admitting Maslov data. Both properties are implied by\nthe existence of a polarization, for which a basic obstruction is that the odd\nChern classes are 2-torsion. In a similar spirit we establish cohomological\nobstructions to the existence of arboreal skeleta and to the existence of\nMaslov data, exhibiting concrete examples to illustrate their failure. For\ninstance, we show that complements of smooth anti-canonical divisors in complex\nprojective space may fail to admit either arboreal skeleta or Maslov data. We\nalso exhibit an example of a Weinstein manifold which admits Maslov data but\ndoes not admit an arboreal skeleton.",
      "generated_abstract": "The aim of this paper is to study the connection between the dynamics of\nthe harmonic map on the sphere and the dynamics of the harmonic map on the\nhyperbolic plane. We consider the case of a harmonic map of a closed surface\n$M$ with an isolated singularity, which is called a boundary critical point. We\nshow that the critical point has to be a boundary critical point. Furthermore,\nwe prove that the critical point is a boundary critical point if and only if\nthe critical point is a boundary critical point. The case of a critical point\nof the harmonic map which is a boundary critical point is studied. We show that\nthe critical point is a boundary critical point if and only if the critical\npoint is a boundary critical point.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.26666666666666666,
          "f": 0.2051282003944774
        },
        "rouge-2": {
          "r": 0.0297029702970297,
          "p": 0.041666666666666664,
          "f": 0.03468207606535535
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.26666666666666666,
          "f": 0.2051282003944774
        }
      }
    },
    {
      "paper_id": "cs.CY.physics/ed-ph/2502.08705v2",
      "true_abstract": "Engaging the public with science is critical for a well-informed population.\nA popular method of scientific communication is documentaries. Once released,\nit can be difficult to assess the impact of such works on a large scale, due to\nthe overhead required for in-depth audience feedback studies. In what follows,\nwe overview our complementary approach to qualitative studies through\nquantitative impact and sentiment analysis of Amazon reviews for several\nscientific documentaries. In addition to developing a novel impact category\ntaxonomy for this analysis, we release a dataset containing 1296\nhuman-annotated sentences from 1043 Amazon reviews for six movies created in\nwhole or part by the Advanced Visualization Lab (AVL). This interdisciplinary\nteam is housed at the National Center for Supercomputing Applications and\nconsists of visualization designers who focus on cinematic presentations of\nscientific data. Using this data, we train and evaluate several machine\nlearning and large language models, discussing their effectiveness and possible\ngeneralizability for documentaries beyond those focused on for this work.\nThemes are also extracted from our annotated dataset which, along with our\nlarge language model analysis, demonstrate a measure of the ability of\nscientific documentaries to engage with the public.",
      "generated_abstract": "This study introduces the use of the neural network architecture, a Convolutional\nNeural Network (CNN), as a data collection tool for measuring the\nelectrokinetic properties of aqueous solutions. The proposed methodology\nenables the analysis of the electrophoretic mobility of different types of\npolar molecules (acidic and basic) and their complexes. The proposed methodology\nis applicable to a wide range of electrolytes and has the potential to be\nextended to other electrochemical applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09302325581395349,
          "p": 0.22641509433962265,
          "f": 0.1318681277400074
        },
        "rouge-2": {
          "r": 0.01092896174863388,
          "p": 0.029411764705882353,
          "f": 0.015936251029667173
        },
        "rouge-l": {
          "r": 0.08527131782945736,
          "p": 0.20754716981132076,
          "f": 0.12087911675099641
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2501.12233v1",
      "true_abstract": "We address the challenge of identifying all real positive steady states in\nchemical reaction networks (CRNs) governed by mass-action kinetics. Traditional\nnumerical methods often require specific initial guesses and may fail to find\nall the solutions in systems exhibiting multistability. Gr\\\"obner bases offer\nan algebraic framework that systematically transforms polynomial equations into\nsimpler forms, facilitating comprehensive solution enumeration. In this work,\nwe propose a conjecture that CRNs with at most pairwise interactions yield\nGr\\\"obner bases possessing a near-\"triangular\" structure, under appropriate\nassumptions. We illustrate this phenomenon using examples from a gene\nregulatory network and the Wnt signaling pathway, where the Gr\\\"obner basis\napproach reliably captures all real positive solutions. Our computational\nexperiments reveal the potential of Gr\\\"obner bases to overcome limitations of\nlocal numerical methods for finding the steady states of complex biological\nsystems, making them a powerful tool for understanding dynamical processes\nacross diverse biochemical models.",
      "generated_abstract": "tion of the genome-editing tools used in molecular biology is\nrevolutionizing the study of gene function, a crucial step in understanding\ngenetic diseases. Despite the advancements in genome editing, the analysis of\nthe genetic effects of these tools remains a challenge, as it is often\nhindered by the lack of high-quality data. In this study, we developed a\ncomprehensive database, Genetic Effects of CRISPR/Cas9-Guided Genome Editing\n(GEDB), which integrates more than 1,500 studies that have been published in\nthe past two years. This database offers a comprehensive and user-friendly\ninterface that allows users to search for genetic effects of CRISPR/Cas9-guided\ngenome editing tools, including Cas9, ZFN, TALEN, and CRISPR-associ",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1391304347826087,
          "p": 0.1951219512195122,
          "f": 0.1624365433636529
        },
        "rouge-2": {
          "r": 0.007142857142857143,
          "p": 0.009615384615384616,
          "f": 0.008196716420320037
        },
        "rouge-l": {
          "r": 0.12173913043478261,
          "p": 0.17073170731707318,
          "f": 0.14213197483573417
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.12669v1",
      "true_abstract": "This paper examines the optimal design of information sharing in\norganizations. Organizational performance depends on agents adapting to\nuncertain external environments while coordinating their actions, where\ncoordination incentives and synergies are modeled as graphs (networks). The\nequilibrium strategies and the principal's objective function are summarized\nusing Laplacian matrices of these graphs. I formulate a Bayesian persuasion\nproblem to determine the optimal public signal and show that it comprises a set\nof statistics on local states, necessarily including their average, which\nserves as the organizational goal. When the principal benefits equally from the\ncoordination of any two agents, the choice of disclosed statistics is based on\nthe Laplacian eigenvectors and eigenvalues of the incentive graph. The\nalgebraic connectivity (the second smallest Laplacian eigenvalue) determines\nthe condition for full revelation, while the Laplacian spectral radius (the\nlargest Laplacian eigenvalue) establishes the condition for minimum\ntransparency, where only the average state is disclosed.",
      "generated_abstract": "aper, we introduce a novel stochastic dynamic programming framework\nfor solving optimal policies in stochastic dynamic games. This framework\nrelies on a new notion of expectation, which is derived from a novel\ninterpretation of the value function in stochastic dynamic games. This new\nexpectation concept is particularly useful for analyzing optimal policies in\nstochastic dynamic games, where the value function is often intractable or\ncomputationally infeasible. Furthermore, we present a new approach for\nconstructing optimal policies in stochastic dynamic games, which combines\nstochastic dynamic programming with expectation theory. This approach provides\na more efficient solution for computing optimal policies in stochastic dynamic\ngames than previous approaches. The new approach is also able to handle\nsuboptimal policies, which is an important feature in real-world applications.\nTheoretically, we establish the convergence of the algorithm to the\napproximate solution of the value function. N",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14150943396226415,
          "p": 0.2,
          "f": 0.16574585150025958
        },
        "rouge-2": {
          "r": 0.006944444444444444,
          "p": 0.009174311926605505,
          "f": 0.00790513343561366
        },
        "rouge-l": {
          "r": 0.1320754716981132,
          "p": 0.18666666666666668,
          "f": 0.15469612774335353
        }
      }
    },
    {
      "paper_id": "eess.IV.q-bio/TO/2503.03780v1",
      "true_abstract": "Camera-based vital signs monitoring in recent years has attracted more and\nmore researchers and the results are promising. However, a few research works\nfocus on heart rate extraction under extremely low illumination environments.\nIn this paper, we propose a novel framework for remote heart rate estimation\nunder low-light conditions. This method uses singular spectrum analysis (SSA)\nto decompose the filtered signal into several reconstructed components. A\nspectral masking algorithm is utilized to refine the preliminary candidate\ncomponents on the basis of a reference heart rate. The contributive components\nare fused into the final pulse signal. To evaluate the performance of our\nframework in low-light conditions, the proposed approach is tested on a\nlarge-scale multi-illumination HR dataset (named MIHR). The test results verify\nthat the proposed method has stronger robustness to low illumination than\nstate-of-the-art methods, effectively improving the signal-to-noise ratio and\nheart rate estimation precision. We further perform experiments on the PUlse\nRatE detection (PURE) dataset which is recorded under normal light conditions\nto demonstrate the generalization of our method. The experiment results show\nthat our method can stably detect pulse rate and achieve comparative results.\nThe proposed method pioneers a new solution to the remote heart rate estimation\nin low-light conditions.",
      "generated_abstract": "brain tumor segmentation is crucial for the diagnosis of\nresectable malignancies. Current deep learning-based approaches for this task\noften rely on large-scale datasets, requiring significant computational\nexpense. Additionally, existing methods often rely on large pre-trained\nmodels, limiting their adaptability to new tumor types or tumor\nsub-structures. In this work, we introduce a new approach for segmenting\nresectable brain tumors based on single-image deep learning. Our method uses\nan encoder-decoder architecture, integrating a series of residual connections\nto capture both low-level and high-level features. We use a small encoder to\ncapture global image information, while a larger decoder extracts fine-grained\nfeatures for localization. The decoder is further enhanced through\nself-supervised learning to enhance feature extraction. The decoder",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15873015873015872,
          "p": 0.22727272727272727,
          "f": 0.1869158830081231
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.02702702702702703,
          "f": 0.0202020155208663
        },
        "rouge-l": {
          "r": 0.15079365079365079,
          "p": 0.2159090909090909,
          "f": 0.17757008861559975
        }
      }
    },
    {
      "paper_id": "cs.MA.econ/GN/2502.13267v1",
      "true_abstract": "BeforeIT is an open-source software for building and simulating\nstate-of-the-art macroeconomic agent-based models (macro ABMs) based on the\nrecently introduced macro ABM developed in [1] and here referred to as the base\nmodel. Written in Julia, it combines extraordinary computational efficiency\nwith user-friendliness and extensibility. We present the main structure of the\nsoftware, demonstrate its ease of use with illustrative examples, and benchmark\nits performance. Our benchmarks show that the base model built with BeforeIT is\norders of magnitude faster than a Matlab version, and significantly faster than\nMatlab-generated C code. BeforeIT is designed to facilitate reproducibility,\nextensibility, and experimentation. As the first open-source, industry-grade\nsoftware to build macro ABMs of the type of the base model, BeforeIT can\nsignificantly foster collaboration and innovation in the field of agent-based\nmacroeconomic modelling. The package, along with its documentation, is freely\navailable at https://github.com/bancaditalia/BeforeIT.jl under the AGPL-3.0.",
      "generated_abstract": "r examines the economic implications of the rise of the non-personal\neconomy, a growing sector of the economy that is driven by algorithms and\ntechnology, with a focus on the impact on labour markets. We provide a\ntheoretical framework for the analysis of the labour market impact of the\nnon-personal economy, and use this framework to estimate the impact of\nalgorithmic labour markets on wages and employment in the United States. Our\nanalysis finds that algorithmic labour markets increase labour supply,\nincrease wage inequality, and reduce employment. We then explore the\nimplications of algorithmic labour markets for labour market institutions,\nsuch as wage setting, through the use of the firm's wage setting function,\nwhich is estimated using a sample of 1,133 firms from the American Time Use\nSurvey. Our analysis finds that algorithmic labour markets are",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14563106796116504,
          "p": 0.20833333333333334,
          "f": 0.17142856658546954
        },
        "rouge-2": {
          "r": 0.02158273381294964,
          "p": 0.026785714285714284,
          "f": 0.023904377527976767
        },
        "rouge-l": {
          "r": 0.1262135922330097,
          "p": 0.18055555555555555,
          "f": 0.1485714237283267
        }
      }
    },
    {
      "paper_id": "math-ph.nlin/SI/2503.01578v1",
      "true_abstract": "We compute scalar products of off-shell Bethe vectors in models with\n$o_{2n+1}$ symmetry. The scalar products are expressed as a sum over partitions\nof the Bethe parameter sets, the building blocks being the so-called highest\ncoefficients. We prove some recurrence relations and a residue theorem for\nthese highest coefficients, and prove that they are consistent with the\nreduction to $gl_n$ invariant models. We also express the norm of on-shell\nBethe vectors as a Gaudin determinant.",
      "generated_abstract": "We study the dynamics of the fractional order $1/k$ of a system of\ndegenerate ordinary differential equations (ODEs) coupled via the linearized\nHamiltonian constraint, where $k$ is an integer. We show that, if the system\nis Hamiltonian with respect to a fixed Hamiltonian, the dynamics can be\ndescribed by a Hamiltonian system of fractional order $1/k$. We characterize\nthe evolution of the fractional order of the Hamiltonian constraint, and we\nderive a general formula for the evolution of the fractional order of the\nHamiltonian constraint in the case of a single-degree of freedom. We also\nderive an explicit formula for the evolution of the fractional order of the\nHamiltonian constraint for the case of two-degree of freedom system. The\nresults of this study provide a new perspective on the dynamics of fractional\norder systems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20754716981132076,
          "p": 0.18032786885245902,
          "f": 0.19298245116497395
        },
        "rouge-2": {
          "r": 0.028169014084507043,
          "p": 0.020833333333333332,
          "f": 0.023952090920435577
        },
        "rouge-l": {
          "r": 0.20754716981132076,
          "p": 0.18032786885245902,
          "f": 0.19298245116497395
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/TO/2503.03126v1",
      "true_abstract": "Groups of cells, including clusters of cancerous cells, multicellular\norganisms, and developing organs, may both grow and break apart. What physical\nfactors control these fractures? In these processes, what sets the eventual\nsize of clusters? We develop a framework for understanding cell clusters that\ncan fragment due to cell motility using an active particle model. We compute\nanalytically how the break rate of cell-cell junctions depends on cell speed,\ncell persistence, and cell-cell junction properties. Next, we find the cluster\nsize distributions, which differ depending on whether all cells can divide or\nonly the cells on the edge of the cluster divide. Cluster size distributions\ndepend solely on the ratio of the break rate to the growth rate - allowing us\nto predict how cluster size and variability depend on cell motility and\ncell-cell mechanics. Our results suggest that organisms can achieve better size\ncontrol when cell division is restricted to the cluster boundaries or when\nfracture can be localized to the cluster center. Our results link the general\nphysics problem of a collective active escape over a barrier to size control,\nproviding a quantitative measure of how motility can regulate organ or organism\nsize.",
      "generated_abstract": "very of the role of the enzyme phosphoglucomutase (PGM) in\nthe biosynthesis of the enzyme glucose-6-phosphate (G6P) and the subsequent\nconversion of G6P into glucose has led to a fundamental understanding of\nglycolysis. However, PGM does not appear in the enzyme complexes that catalyze\nthe other key reactions in the glycolytic pathway, namely the oxidative\nphosphorylation of adenosine triphosphate (ATP) and the transfer of phosphate\nto glucose-6-phosphate. The lack of a PGM-like enzyme has impeded our\nunderstanding of the glycolytic pathway and the generation of ATP, particularly\nin light of the increasing evidence that phosphoglucomutase is important",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06837606837606838,
          "p": 0.13559322033898305,
          "f": 0.09090908645209217
        },
        "rouge-2": {
          "r": 0.011111111111111112,
          "p": 0.023809523809523808,
          "f": 0.015151510812673418
        },
        "rouge-l": {
          "r": 0.05982905982905983,
          "p": 0.11864406779661017,
          "f": 0.07954545008845583
        }
      }
    },
    {
      "paper_id": "math.AP.math/SP/2503.01528v1",
      "true_abstract": "We examine semiclassical measures for Laplace eigenfunctions on compact\nhyperbolic $(n+1)$-manifolds. We prove their support must contain the cosphere\nbundle of a compact immersed totally geodesic submanifold. Our proof adapts the\nargument of Dyatlov and Jin to higher dimensions and classifies the closures of\nhorocyclic orbits using Ratner theory. An important step in the proof is a\ngeneralization of the higher-dimensional fractal uncertainty principle of Cohen\nto Fourier integral operators, which may be of independent interest.",
      "generated_abstract": "We prove a local-global principle for the KAM-Taylor-Whitham system on\nA-invariant manifolds. A-invariant means the action is invariant under a\n(possibly non-compact) subgroup of the Poincar\\'e group. The system is\ndescribed by the KAM-Taylor-Whitham system on A-invariant manifolds, which is\nthe analogue of the KAM-Taylor system on compact manifolds in the compact\nsetting. We prove the local-global principle for the system on A-invariant\nmanifolds using a generalization of the classical Heunf theorem. As an\napplication, we prove the local-global principle for the KAM-Taylor-Whitham\nsystem on compact manifolds.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22950819672131148,
          "p": 0.3333333333333333,
          "f": 0.2718446553643134
        },
        "rouge-2": {
          "r": 0.08,
          "p": 0.0967741935483871,
          "f": 0.08759123592093375
        },
        "rouge-l": {
          "r": 0.21311475409836064,
          "p": 0.30952380952380953,
          "f": 0.25242717963615807
        }
      }
    },
    {
      "paper_id": "math-ph.math/MP/2503.09558v1",
      "true_abstract": "For a given graph $G$, Budzik, Gaiotto, Kulp, Wang, Williams, Wu, Yu, and the\nfirst author studied a ''topological'' differential form $\\alpha_G$, which\nexpresses violations of BRST-closedness of a quantum field theory along a\nsingle topological direction. In a seemingly unrelated context, Brown, Panzer,\nand the second author studied a ''Pfaffian'' differential form $\\phi_G$, which\nis used to construct cohomology classes of the odd commutative graph complex.\nWe give an explicit combinatorial proof that $\\alpha_G$ coincides with\n$\\phi_G$. We also discuss the equivalence of several properties of these forms,\nwhich had been established independently for both contexts in previous work.",
      "generated_abstract": "We consider the problem of constructing an optimal control system for a\nsystem of nonlinear equations with a Hamiltonian function. We solve this\nproblem for the case when the Hamiltonian function is quadratic, and we use\nthe method of Lagrange multipliers. The solution of the problem is an optimal\ncontrol system, which we show to be a particular case of the optimal control\nsystem for the Hamiltonian system with a quadratic Hamiltonian. We also\nconsider a special case when the Hamiltonian function is cubic. Our results\nshow that in this case the optimal control system for the Hamiltonian system\nwith a cubic Hamiltonian is different from the optimal control system for the\nHamiltonian system with a quadratic Hamiltonian.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17721518987341772,
          "p": 0.2978723404255319,
          "f": 0.22222221754472168
        },
        "rouge-2": {
          "r": 0.021052631578947368,
          "p": 0.024691358024691357,
          "f": 0.02272726775891121
        },
        "rouge-l": {
          "r": 0.16455696202531644,
          "p": 0.2765957446808511,
          "f": 0.2063492016717058
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.07830v1",
      "true_abstract": "Let (K,v) be a valued field. Take an extension of v to a fixed algebraic\nclosure L of K. In this paper we show that an element a in L admits a complete\ndistinguished chain over K if and only if the extension (K(a)|K,v) is\ndefectless and unibranched. This characterization generalizes the known result\nin the henselian case. In particular, our result shows that if a admits a\ncomplete distinguished chain over K, then it also admits one over the\nhenselization; however, the converse may not be true. The main tool employed in\nour analysis is the stability of the j-invariant associated to a valuation\ntranscendental extension under passage to the henselization.",
      "generated_abstract": "In this paper, we introduce a new model for a class of Markov chains on\nthe finite state space that is not restricted to the case of finite-state\nMarkov chains. We show that this new model can be realized by a simple\nconfiguration space that is defined as the product of two other simple\nconfigurations spaces. As a consequence, we obtain a simple proof of a\ncharacterization result that has been known for finite-state Markov chains.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.32653061224489793,
          "f": 0.26446280509801245
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.04411764705882353,
          "f": 0.03468207615356409
        },
        "rouge-l": {
          "r": 0.20833333333333334,
          "p": 0.30612244897959184,
          "f": 0.24793387947817777
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.07139v1",
      "true_abstract": "In this letter, we investigate a coordinated multiple point (CoMP)-aided\nintegrated sensing and communication (ISAC) system that supports multiple users\nand targets. Multiple base stations (BSs) employ a coordinated power allocation\nstrategy to serve their associated single-antenna communication users (CUs)\nwhile utilizing the echo signals for joint radar target (RT) detection. The\nprobability of detection (PoD) of the CoMP-ISAC system is then proposed for\nassessing the sensing performance. To maximize the sum rate while ensuring the\nPoD for each RT and adhering to the total transmit power budget across all BSs,\nwe introduce an efficient power allocation strategy. Finally, simulation\nresults are provided to validate the analytical findings, demonstrating that\nthe proposed power allocation scheme effectively enhances the sum rate while\nsatisfying the sensing requirements.",
      "generated_abstract": "t a novel method for multi-user multiple-input multiple-output\n(MIMO) channel estimation, where the transmitter (TX) and receiver (RX)\nequipments share a common channel. Specifically, the RX estimates the TX\nchannel by exploiting the channel estimation from the RX. The TX estimates the\nchannel by using the RX channel estimation. This is a two-step approach for\nestimating the channel. In the first step, we estimate the channel using a\ncentralized approach, where the TX and RX share a common channel. In the second\nstep, the TX estimates the channel using the RX channel estimation. The\nproposed scheme is designed to improve the channel estimation performance in\nMIMO systems, particularly when the RX has a poor channel. Furthermore, the\nproposed scheme allows for efficient implementation, as the TX and RX can share\nthe same hardware. N",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14942528735632185,
          "p": 0.19696969696969696,
          "f": 0.16993463561707053
        },
        "rouge-2": {
          "r": 0.008620689655172414,
          "p": 0.00980392156862745,
          "f": 0.009174306947229366
        },
        "rouge-l": {
          "r": 0.13793103448275862,
          "p": 0.18181818181818182,
          "f": 0.1568627401922339
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2402.04765v2",
      "true_abstract": "Early-stage firms play a significant role in driving innovation and creating\nnew products and services, especially for cybersecurity. Therefore, evaluating\ntheir performance is crucial for investors and policymakers. This work presents\na financial evaluation of early-stage firms' performance in 19 cybersecurity\nsectors using a private-equity dataset from 2010 to 2022 retrieved from\nCrunchbase. We observe firms, their primary and secondary activities, funding\nrounds, and pre and post-money valuations. We compare cybersecurity sectors\nregarding the amount raised over funding rounds and post-money valuations while\ninferring missing observations. We observe significant investor interest\nvariations across categories, periods, and locations. In particular, we find\nthe average capital raised (valuations) to range from USD 7.24 mln (USD 32.39\nmln) for spam filtering to USD 45.46 mln (USD 447.22 mln) for the private cloud\nsector. Next, we assume a log process for returns computed from post-money\nvaluations and estimate the expected returns, systematic and specific risks,\nand risk-adjusted returns of investments in early-stage firms belonging to\ncybersecurity sectors. Again, we observe substantial performance variations\nwith annualized expected returns ranging from 9.72\\% for privacy to 177.27\\%\nfor the blockchain sector. Finally, we show that overall, the cybersecurity\nindustry performance is on par with previous results found in private equity.\nOur results shed light on the performance of cybersecurity investments and,\nthus, on investors' expectations about cybersecurity.",
      "generated_abstract": "ntext of global financial stability, it is essential to analyze\nthe interplay between financial institutions and their environment, which is\noften characterized by a complex network structure. In this paper, we propose\na network-based model for the analysis of systemic risk in the banking sector.\nThe model integrates the main features of the traditional Gale-Shapley\nsolution, such as a hierarchical organization of institutions, and extends it\nwith a network structure to capture the interactions among banks. The proposed\nmodel is then used to investigate the evolution of systemic risk over time\nunder different financial conditions. We focus on the banking system in Italy\nduring the period 2005-2021. Our analysis shows that the emergence of a\nsignificant systemic risk in the Italian banking sector occurs during the 2007\nfinancial crisis, when the number of systemically important banks",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13986013986013987,
          "p": 0.22727272727272727,
          "f": 0.17316016844362
        },
        "rouge-2": {
          "r": 0.013953488372093023,
          "p": 0.023809523809523808,
          "f": 0.01759530325848714
        },
        "rouge-l": {
          "r": 0.13286713286713286,
          "p": 0.2159090909090909,
          "f": 0.16450215978561136
        }
      }
    },
    {
      "paper_id": "math.CO.cs/CG/2503.02336v1",
      "true_abstract": "We present a program for enumerating all pseudoline arrangements with a small\nnumber of pseudolines and abstract order types of small point sets. This\nprogram supports computer experiments with these structures, and it complements\nthe order-type database of Aichholzer, Aurenhammer, and Krasser. This system\nmakes it practical to explore the abstract order types for 12 points, and the\npseudoline arrangements of 11 pseudolines.",
      "generated_abstract": "This paper introduces a new approach to the design of multivariate\ngeneralized orthogonal polynomials on a compact Riemannian manifold, by\nintroducing a new parameterization of the unit ball of the manifold. By\nexploiting the structure of the associated symmetric, hyperbolic, and\nquasi-Hermitian manifolds, we derive explicit formulas for the eigenfunctions\nand eigenvalues of the associated Laplace operator, as well as for the\neigenfunctions and eigenvalues of the associated Schr\\\"odinger operator. We\nfurthermore analyze the properties of the new parameterization, and provide\nexamples of manifolds which admit the new parameterization. As a\nconsequence, we obtain new formulas for the eigenfunctions and eigenvalues of\nthe associated Schr\\\"odinger operator on the unit ball of the manifold, which\ndo not depend on the choice of the parameterization.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19047619047619047,
          "p": 0.12698412698412698,
          "f": 0.15238094758095252
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.19047619047619047,
          "p": 0.12698412698412698,
          "f": 0.15238094758095252
        }
      }
    },
    {
      "paper_id": "eess.SY.econ/TH/2502.14150v1",
      "true_abstract": "We propose a risk-sensitive security-constrained economic dispatch (R-SCED)\nformulation capturing the tradeoff between dispatch cost and resilience against\npotential line failures, where risk is modeled via the conditional value at\nrisk (CVaR). In the context of our formulation, we analyze revenue adequacy and\nside payments of two pricing models, one based on nominal generation costs, and\nanother based on total marginal cost including contingencies. In particular, we\nprove that the system operator's (SO) merchandising surplus (MS) and total\nrevenue are nonnegative under the latter, while under the former the same does\nnot hold in general. We demonstrate that the proposed R-SCED formulation is\namenable to decomposition and describe a Benders' decomposition algorithm to\nsolve it. In numerical examples, we illustrate the differences in MS and total\nrevenue under the considered pricing schemes, and the computational efficiency\nof our decomposition approach.",
      "generated_abstract": "opment of intelligent transportation systems (ITS) is essential for\ntransportation efficiency and sustainability. In this paper, we propose a\nhybrid optimization framework to minimize the total cost of vehicle routing\nproblem (VRP) under traffic congestion. First, we formulate the VRP as a\nmulti-objective optimization problem. Then, we introduce a cost-based\nobjective function to minimize the travel time of vehicles and a\nconservation-based objective function to minimize the total cost of the\nsystem. Finally, we develop a two-stage stochastic optimization algorithm to\noptimize the proposed hybrid optimization framework. The proposed algorithm\nachieves good performance in terms of both computational efficiency and\nconservation. Extensive experiments on the New York City (NYC) VRP and the\nNew York City (NYC) Toll VRP demonstrate that the proposed algorithm\nach",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21875,
          "p": 0.28,
          "f": 0.24561403016312722
        },
        "rouge-2": {
          "r": 0.06060606060606061,
          "p": 0.0761904761904762,
          "f": 0.06751054358810057
        },
        "rouge-l": {
          "r": 0.20833333333333334,
          "p": 0.26666666666666666,
          "f": 0.2339181237303787
        }
      }
    },
    {
      "paper_id": "math.OC.econ/EM/2502.05212v1",
      "true_abstract": "In this paper, we provide analytic expressions for the first-order loss\nfunction, the complementary loss function and the second-order loss function\nfor several probability distributions. These loss functions are important\nfunctions in inventory optimization and other quantitative fields. For several\nreasons, which will become apparent throughout this paper, the implementation\nof these loss functions prefers the use of an analytic expression, only using\nstandard probability functions. However, complete and consistent references of\nanalytic expressions for these loss functions are lacking in literature. This\npaper aims to close this gap and can serve as a reference for researchers,\nsoftware engineers and practitioners that are concerned with the optimization\nof a quantitative system. This should lead directly to easily using different\nprobability distributions in quantitive models which is at the core of\noptimization. Also, this paper serves as a broad introduction to loss functions\nand their use in inventory control.",
      "generated_abstract": "This study investigates the long-term impact of the COVID-19 pandemic on\nthe tourism industry. It employs a panel data set comprising 148 countries\ncovering 2020 and 2021. The research employs a difference-in-difference\nframework to analyse the impact of the pandemic on tourism demand, focusing on\nthe tourism industry's capacity to recover. The results indicate that the\npandemic had a significant negative impact on tourism demand, with a negative\neffect on all countries. The pandemic also had a significant negative impact on\nthe industry's capacity to recover. The findings suggest that the tourism\nindustry's capacity to recover is positively correlated with economic development\nand the availability of social safety nets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10344827586206896,
          "p": 0.15789473684210525,
          "f": 0.12499999521701408
        },
        "rouge-2": {
          "r": 0.007407407407407408,
          "p": 0.012048192771084338,
          "f": 0.009174307211095765
        },
        "rouge-l": {
          "r": 0.10344827586206896,
          "p": 0.15789473684210525,
          "f": 0.12499999521701408
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.astro-ph/CO/2503.09599v1",
      "true_abstract": "Primordial Magnetic Fields (PMFs), long studied as potential relics of the\nearly Universe, accelerate the recombination process and have been proposed as\na possible way to relieve the Hubble tension. However, previous studies relied\non simplified toy models. In this study, for the first time, we use the recent\nhigh-precision evaluations of recombination with PMFs, incorporating full\nmagnetohydrodynamic (MHD) simulations and detailed Lyman-alpha radiative\ntransfer, to test PMF-enhanced recombination ($b\\Lambda$CDM) against\nobservational data from the cosmic microwave background (CMB), baryon acoustic\noscillations (BAO), and Type Ia supernovae (SN). Focusing on non-helical PMFs\nwith a Batchelor spectrum, we find a preference for present-day total field\nstrengths of approximately 5-10 pico-Gauss. Depending on the dataset\ncombination, this preference ranges from mild ($\\sim 1.8\\sigma$ with Planck +\nDESI) to moderate ($\\sim 3\\sigma$ with Planck + DESI + SH0ES-calibrated SN)\nsignificance. The $b\\Lambda$CDM has Planck + DESI $\\chi^2$ values equal or\nbetter than those of the $\\Lambda$CDM model while predicting a higher Hubble\nconstant. The favored field strengths align closely with those required for\ncluster magnetic fields to originate entirely from primordial sources, without\nthe need for additional dynamo amplification or stellar magnetic field\ncontamination. Future high-resolution CMB temperature and polarization\nmeasurements will be crucial for confirming or further constraining the\npresence of PMFs at recombination.",
      "generated_abstract": "of stellar populations in the Magellanic Clouds is important for\nstellar evolution and chemical abundance studies, as well as for the search for\nexotic planetary systems. In this paper, we present the results of a\ncomprehensive study of stellar populations in the Small Magellanic Cloud (SMC)\nand the Large Magellanic Cloud (LMC). Using archival data from the Sloan\nDigital Sky Survey (SDSS) and the Two-Micron All Sky Survey (2MASS), we\ninvestigate the stellar populations of the SMC and LMC, focusing on the\ndifferences between the two galaxies. We compare the kinematics and abundances\nof stars in the two galaxies, and also compare the results of stellar\npopulation studies in the two galaxies with each other. We find that the SMC\nstars have a higher metallicity and a higher",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11612903225806452,
          "p": 0.2465753424657534,
          "f": 0.15789473248884284
        },
        "rouge-2": {
          "r": 0.03398058252427184,
          "p": 0.06481481481481481,
          "f": 0.04458598274818497
        },
        "rouge-l": {
          "r": 0.10967741935483871,
          "p": 0.2328767123287671,
          "f": 0.14912280266428143
        }
      }
    },
    {
      "paper_id": "math.AG.math/AG/2503.08924v1",
      "true_abstract": "This article introduces efficient and user-friendly tools for analyzing the\nintersection curve between a ringed torus and an irreducible quadric surface.\nWithout loose of generality, it is assumed that the torus is centered at the\norigin, and its axis of revolution coincides with the $z$-axis. The paper\nprimarily focuses on examining the curve's projection onto the plane $z=0$,\nreferred to as the cutcurve, which is essential for ensuring accurate lifting\nprocedures. Additionally, we provide a detailed characterization of the\nsingularities in both the projection and the intersection curve, as well as the\nexistence of double tangents. A key tool for the analysis is the theory of\nresultant and subresultant polynomials.",
      "generated_abstract": "r explores a simple, yet surprising, feature of the $L^2$\nequivalence of two metric spaces. We show that for two metric spaces $(X,\\rho)$\nand $(Y,\\sigma)$, the metric spaces $(X,\\rho\\circ\\pi)$ and $(Y,\\sigma\\circ\n\\pi)$ are $L^2$-equivalent if and only if there exists an isometric embedding\n$i:X\\rightarrow Y$ such that the metric on $i(X)$ is $\\rho\\circ\\pi$ and the\nmetric on $i(Y)$ is $\\sigma\\circ\\pi$. We further explore the relationship\nbetween the $L^2$ equivalence of metric spaces and their relative\n$L^2$ equivalence. In particular, we show that if $(X,\\rho)$ and $(Y,\\sigma)$\nare $L^2$-equivalent metric spaces, then $(X,\\rho\\circ\\pi)$",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13924050632911392,
          "p": 0.2037037037037037,
          "f": 0.16541352901125006
        },
        "rouge-2": {
          "r": 0.028037383177570093,
          "p": 0.03896103896103896,
          "f": 0.03260869078509052
        },
        "rouge-l": {
          "r": 0.12658227848101267,
          "p": 0.18518518518518517,
          "f": 0.15037593502628768
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.02578v1",
      "true_abstract": "Bird's Eye View (BEV) perception technology is crucial for autonomous\ndriving, as it generates top-down 2D maps for environment perception,\nnavigation, and decision-making. Nevertheless, the majority of current BEV map\ngeneration studies focusing on visual map generation lack depth-aware reasoning\ncapabilities. They exhibit limited efficacy in managing occlusions and handling\ncomplex environments, with a notable decline in perceptual performance under\nadverse weather conditions or low-light scenarios. Therefore, this paper\nproposes TS-CGNet, which leverages Temporal-Spatial fusion with\nCenterline-Guided diffusion. This visual framework, grounded in prior\nknowledge, is designed for integration into any existing network for building\nBEV maps. Specifically, this framework is decoupled into three parts: Local\nmapping system involves the initial generation of semantic maps using purely\nvisual information; The Temporal-Spatial Aligner Module (TSAM) integrates\nhistorical information into mapping generation by applying transformation\nmatrices; The Centerline-Guided Diffusion Model (CGDM) is a prediction module\nbased on the diffusion model. CGDM incorporates centerline information through\nspatial-attention mechanisms to enhance semantic segmentation reconstruction.\nWe construct BEV semantic segmentation maps by our methods on the public\nnuScenes and the robustness benchmarks under various corruptions. Our method\nimproves 1.90%, 1.73%, and 2.87% for perceived ranges of 60x30m, 120x60m, and\n240x60m in the task of BEV HD mapping. TS-CGNet attains an improvement of 1.92%\nfor perceived ranges of 100x100m in the task of BEV semantic mapping. Moreover,\nTS-CGNet achieves an average improvement of 2.92% in detection accuracy under\nvarying weather conditions and sensor interferences in the perception range of\n240x60m. The source code will be publicly available at\nhttps://github.com/krabs-H/TS-CGNet.",
      "generated_abstract": "This paper introduces a novel method for generating 3D facial expressions\nin videos, where a single video is used to generate multiple facial expressions\nwith varying intensities. The method uses a neural network to predict the\nparameters of the 3D facial expression model, which is then used to generate\nvideos of facial expressions. The method is trained and tested on the\nMicrosoft COCO-style 3D facial expressions dataset. The results demonstrate\nthat the proposed method is capable of generating facial expressions with high\nfidelity and realism, and the method can be used for real-time facial\nexpression generation in video-based applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10857142857142857,
          "p": 0.3275862068965517,
          "f": 0.16309012501611753
        },
        "rouge-2": {
          "r": 0.00819672131147541,
          "p": 0.023255813953488372,
          "f": 0.01212120826740251
        },
        "rouge-l": {
          "r": 0.10857142857142857,
          "p": 0.3275862068965517,
          "f": 0.16309012501611753
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/AP/2503.02850v1",
      "true_abstract": "The comparison of different medical treatments from observational studies or\nacross different clinical studies is often biased by confounding factors such\nas systematic differences in patient demographics or in the inclusion criteria\nfor the trials. Propensity score matching is a popular method to adjust for\nsuch confounding. It compares weighted averages of patient responses. The\nweights are calculated from logistic regression models with the intention to\nreduce differences between the confounders in the treatment groups. However,\nthe groups are only \"roughly matched\" with no generally accepted principle to\ndetermine when a match is \"good enough\".\n  In this manuscript, we propose an alternative approach to the matching\nproblem by considering it as a constrained optimization problem. We investigate\nthe conditions for exact matching in the sense that the average values of\nconfounders are identical in the treatment groups after matching. Our approach\nis similar to the matching-adjusted indirect comparison approach by\nSignorovitch et al. (2010) but with two major differences: First, we do not\nimpose any specific functional form on the matching weights; second, the\nproposed approach can be applied to individual patient data from several\ntreatment groups as well as to a mix of individual patient and aggregated data.",
      "generated_abstract": "r introduces a novel approach for the simultaneous estimation of\nboth a latent parameter and a covariate in a dynamic model of the form\n$\\{y_{t}\\}_{t=1}^\\infty = \\theta_0 + \\theta_1 x_{t-1} + \\epsilon_{t}$, where\n$\\theta_0$ and $\\theta_1$ are unknown parameters and $\\epsilon_{t}$ is a\ntime-varying random process. We present a two-step method that allows for\nparameter estimation and prediction in a single step. To overcome the challenges\ninvolved in estimating both a latent parameter and a covariate simultaneously,\nwe propose a novel estimation approach that employs an alternative form of the\nmodel with a simpler likelihood. We demonstrate the efficacy of our method via\nsimulations and a real-world application. The proposed method is applicable to\na variety of dynamic models with unknown",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1746031746031746,
          "p": 0.3013698630136986,
          "f": 0.22110552299285383
        },
        "rouge-2": {
          "r": 0.031746031746031744,
          "p": 0.05454545454545454,
          "f": 0.0401337746132599
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.2876712328767123,
          "f": 0.2110552717365724
        }
      }
    },
    {
      "paper_id": "physics.soc-ph.physics/pop-ph/2502.03191v2",
      "true_abstract": "This study intends to test the hypothesis that, contrary to traditional\ninterpretation, the social structure of the polity of Aksum - especially in its\nearly stages - was not characterized by a vertical hierarchy with highly\ncentralized administrative power, and that the leaders mentioned in the few\navailable inscriptions were predominantly ritual leaders with religious rather\nthan coercive political authority. This hypothesis, suggested by the available\narchaeological evidence, is grounded in Charles Stanish's model, which posits\nthat pre-state societies could achieve cooperative behavior without the\npresence of coercive authority. Using agent-based modeling applied to data\ninspired by the Aksum civilization, we examine the dynamics of cooperation in\nthe presence and absence of a Public Goods Game. Results show that while\ncooperative behavior can emerge in the short term without coercive power, it\nmay not be sustainable over the long term, suggesting a need for centralized\nauthority to foster stable, complex societies. These findings provide insights\ninto the evolutionary pathways that lead to state formation and complex social\nstructures.",
      "generated_abstract": "sis of the data from the GW170817 event has uncovered the existence\nof a black hole with mass 4.36 $M_{\\odot}$, located at a distance of 45.5 Mpc,\nwith a spin parameter $a=0.25$. The measurement of the mass of the black hole\nwas based on the detection of gravitational waves from the merger of two black\nholes. The spin of the black hole, which is a measure of its angular momentum,\nis a measure of its spin-orbit angular momentum, and can be constrained by\nmeasuring the spin-orbit splitting of the gravitational wave signal. The\nspin-orbit splitting of the gravitational wave signal is a measure of the\nangular momentum of the binary system, which is a measure of the spin of the\nblack hole. This measurement of the spin of the black hole is the first",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12173913043478261,
          "p": 0.22950819672131148,
          "f": 0.15909090456159622
        },
        "rouge-2": {
          "r": 0.012345679012345678,
          "p": 0.020618556701030927,
          "f": 0.015444010758934652
        },
        "rouge-l": {
          "r": 0.09565217391304348,
          "p": 0.18032786885245902,
          "f": 0.12499999547068717
        }
      }
    },
    {
      "paper_id": "astro-ph.GA.astro-ph/GA/2503.10087v1",
      "true_abstract": "Galaxy formation models predict that local galaxies are surrounded by hot\nX-ray-emitting halos, which are technically difficult to detect due to their\nextended and low surface brightness nature. Previous X-ray studies have mostly\nfocused on disk galaxies more massive than the Milky Way, with essentially no\nconsensus on the halo X-ray properties at the lower mass end. We utilize the\nearly-released eROSITA and archival Chandra observations to analyze the diffuse\nX-ray emission of NGC7793, a nearby spiral galaxy with an estimated stellar\nmass of only $3.2\\times 10^9$ $M_{\\odot}$. We find evidence for extraplanar hot\ngas emission from both the radial and vertical soft X-ray intensity profiles,\nwhich spreads up to a galactocentric distance of $\\sim$ 6 kpc, nearly 30 $\\%$\nmore extended than its stellar disk. Analysis of the eROSITA spectra indicates\nthat the hot gas can be characterized by a temperature of\n$0.18^{+0.02}_{-0.03}$ keV, with 0.5--2 keV unabsorbed luminosity of $1.3\\times\n10^{38}$ erg $s^{-1}$. We compare our results with the IllustrisTNG simulations\nand find overall consistence on the disk scale, whereas excessive emission at\nlarge radii is predicted by TNG50. This work provides the latest detection of\nhot corona around a low-mass galaxy, putting new constrains on state-of-the-art\ncosmological simulations. We also verify the detectability of hot\ncircumgalactic medium around even low-mass spirals with future high-resolution\nX-ray spectrometer such as the Hot Universe Baryon Surveyor.",
      "generated_abstract": "t an in-depth analysis of the first two years of data from the\nGRAVITY\n  instrument on the VLT, covering 21,140 days of observations. This dataset\nincludes 1,335,200 unique pointings, and we focus on the analysis of the\nhigh-resolution spectra taken with the GRAVITY instrument. We present a\ncomprehensive catalog of the detected sources, using the GRAVITY data to\nmeasure their positions and redshifts, and compare the results with those from\nprevious observations. We also examine the sensitivity and angular resolution\nof GRAVITY, identifying key features of the data that affect the detection\nefficiency. Finally, we use the high-resolution data to measure the\nfluctuations in the density of stars and gas in the nearby Universe, using\nhigh-fidelity simulations to model the effects",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1402439024390244,
          "p": 0.30666666666666664,
          "f": 0.19246861494021472
        },
        "rouge-2": {
          "r": 0.021929824561403508,
          "p": 0.046296296296296294,
          "f": 0.029761900399660507
        },
        "rouge-l": {
          "r": 0.10365853658536585,
          "p": 0.22666666666666666,
          "f": 0.14225940991929426
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.07061v1",
      "true_abstract": "We consider a group of agents who can each take an irreversible costly action\nwhose payoff depends on an unknown state. Agents learn about the state from\nprivate signals, as well as from past actions of their social network\nneighbors, which creates an incentive to postpone taking the action. We show\nthat outcomes depend on network structure: on networks with a linear structure\npatient agents do not converge to the first-best action, while on regular\ndirected tree networks they do.",
      "generated_abstract": "We study the effect of a negative welfare outcome on individual decisions.\nUncertainty about the negative welfare outcome induces a loss aversion. The\nnegative welfare outcome can be caused by a policy change, a change in the\ngovernment's behavior, or a change in the underlying social structure. We\nformulate the problem as a stochastic optimization problem with state-dependent\nuncertainty and explore different ways to model the uncertainty. We show that\nthe uncertainty can be modeled as a non-linear stochastic differential\nequation with non-stationary volatility. We then analyze the resulting\nstochastic optimal control problem and the impact of the uncertainty on\ndecision making. Our analysis reveals that individuals are more risk averse\nwhen uncertainty is more pronounced.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22950819672131148,
          "p": 0.1917808219178082,
          "f": 0.20895521892069513
        },
        "rouge-2": {
          "r": 0.0379746835443038,
          "p": 0.028846153846153848,
          "f": 0.0327868803392166
        },
        "rouge-l": {
          "r": 0.18032786885245902,
          "p": 0.1506849315068493,
          "f": 0.1641790995177101
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.05674v3",
      "true_abstract": "Driven by advances in self-supervised learning for speech, state-of-the-art\nsynthetic speech detectors have achieved low error rates on popular benchmarks\nsuch as ASVspoof. However, prior benchmarks do not address the wide range of\nreal-world variability in speech. Are reported error rates realistic in\nreal-world conditions? To assess detector failure modes and robustness under\ncontrolled distribution shifts, we introduce ShiftySpeech, a benchmark with\nmore than 3000 hours of synthetic speech from 7 domains, 6 TTS systems, 12\nvocoders, and 3 languages. We found that all distribution shifts degraded model\nperformance, and contrary to prior findings, training on more vocoders,\nspeakers, or with data augmentation did not guarantee better generalization. In\nfact, we found that training on less diverse data resulted in better\ngeneralization, and that a detector fit using samples from a single carefully\nselected vocoder and a small number of speakers, without data augmentations,\nachieved state-of-the-art results on the challenging In-the-Wild benchmark.",
      "generated_abstract": "The increasing availability of large-scale speech data has prompted the\ncreation of large-scale speech translation models. However, the translation\nprocess can be highly expensive in terms of computational resources and\ntime. This paper proposes a novel approach that integrates a multi-agent\nframework with a large-scale speech translation model to achieve efficient\ntranslation tasks. Specifically, the proposed method first generates a\ntranslation model based on the large-scale speech translation model, and then\ncooperatively trains the generated translation model and the multi-agent\nframework with limited resources. The cooperation between the multi-agent\nframework and the translation model improves the efficiency of translation\ntasks. Experimental results show that the proposed method achieves\ncompetitive translation performance compared to state-of-the-art methods.\nAdditionally, it significantly reduces the translation time and computational\ncost.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13761467889908258,
          "p": 0.20833333333333334,
          "f": 0.16574585156252877
        },
        "rouge-2": {
          "r": 0.006802721088435374,
          "p": 0.009523809523809525,
          "f": 0.0079365030753998
        },
        "rouge-l": {
          "r": 0.13761467889908258,
          "p": 0.20833333333333334,
          "f": 0.16574585156252877
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.06651v1",
      "true_abstract": "This paper explores the emerging research direction of electromagnetic\ninformation theory (EIT), which aims to integrate traditional Shannon-based\nmethodologies with physical consistency, particularly the electromagnetic\nproperties of communication channels. We propose an EIT-based multiple-input\nmultiple-output (MIMO) paradigm that enhances conventional spatially-discrete\nMIMO models by incorporating the concepts of electromagnetic (EM) precoding and\nEM combining. This approach aims to improve the modeling of next-generation\nsystems while remaining consistent with Shannon's theoretical foundations. We\nexplore typical EIT applications, such as densely spaced MIMO, near-field\ncommunications, and tri-polarized antennas, and analyze their channel\ncharacteristics through theoretical simulations and measured datasets. The\npaper also discusses critical research challenges and opportunities for EIT\napplications from an industrial perspective, emphasizing the field's potential\nfor practical applications.",
      "generated_abstract": "aper, we present a novel approach to the task of object segmentation\nin the presence of object misalignment. This problem arises in many practical\napplications, such as autonomous driving, where the object to be detected\ncan be misaligned with respect to the camera, or even occluded. To address\nthese challenges, we propose a novel approach based on a mixture of experts\n(MoE) network, which can adaptively select different levels of object\ndetection (i.e., classifier) in the network, based on the similarity between\nthe input object and the detected object in the previous frame. This approach\nenables the network to adaptively balance the performance of different\nobject-related layers. We further propose a novel loss function to enhance\nthe performance of the MoE network, which is designed to promote the\nrepresentations learned by the MoE network to be consistent with the\ninter",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16842105263157894,
          "p": 0.1927710843373494,
          "f": 0.17977527592160092
        },
        "rouge-2": {
          "r": 0.03418803418803419,
          "p": 0.032,
          "f": 0.03305784624513429
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.18072289156626506,
          "f": 0.1685393208654212
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.12833v1",
      "true_abstract": "In areas infested with Aedes aegypti mosquitoes it may be possible to control\ndengue, and some other vector-borne diseases, by introducing Wolbachia-infected\nmosquitoes into the wildtype population. Thus far, empirical and theoretical\nstudies of Wolbachia release have tended to focus on the dynamics at the\ncommunity scale. However, Ae. aegypti mosquitoes typically dwell in and around\nthe same houses as the people they bite and it can be insightful to explore\nwhat happens at the household scale where small population sizes lead to\ninherently stochastic dynamics. Here we use a continuous-time Markov framework\nto develop a stochastic household model for small populations of wildtype and\nWolbachia-infected mosquitoes. We investigate the transient and long term\ndynamics of the system, in particular examining the impact of stochasticity on\nthe Wolbachia invasion threshold and bistability between the wildtype-only and\nWolbachia-only steady states previously observed in deterministic models. We\nfocus on the influence of key parameters which determine the fitness cost of\nWolbachia infection and the probability of Wolbachia vertical transmission.\nUsing Markov and matrix population theory, we derive salient characteristics of\nthe system including the probability of successful Wolbachia invasion, the\nexpected time until invasion and the probability that a Wolbachia-infected\npopulation reverts to a wildtype population. These attributes can inform\nstrategies for the release of Wolbachia-infected mosquitoes. In addition, we\nfind that releasing the minimum number of Wolbachia-infected mosquitoes\nrequired to displace a resident wildtype population according to the\ndeterministic model, only results in that outcome about 20% of the time in the\nstochastic model; a significantly larger release is required to reach a steady\nstate composed entirely of Wolbachia-infected mosquitoes 90% of the time.",
      "generated_abstract": "e an integrated framework for analyzing the effects of biochemical\nreaction dynamics on the evolution of a population. The framework is based on\nthe concept of the \"effective number of genotypes\" and the \"effective\neffectivity\" of a reaction. The effective number of genotypes is defined as\nthe number of genotypes of a population that would be expected to exist under\nthe reaction dynamics, taking into account the genotype-specific fitness\neffects of the reactions. The effective effectivity is defined as the ratio of\nthe actual number of genotypes of a population to the effective number of\ngenotypes. The framework allows for a wide range of reaction dynamics,\nincluding nonlinear, stochastic, and multiple-species reactions. We demonstrate\nthe utility of the framework by analyzing the effects of nonlinear and\nstochastic reactions on the evolution of a population of mutant yeast cells.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1437908496732026,
          "p": 0.3548387096774194,
          "f": 0.20465115868642517
        },
        "rouge-2": {
          "r": 0.024193548387096774,
          "p": 0.05825242718446602,
          "f": 0.0341880300413151
        },
        "rouge-l": {
          "r": 0.13071895424836602,
          "p": 0.3225806451612903,
          "f": 0.18604650752363447
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08094v1",
      "true_abstract": "Medical image denoising is essential for improving the reliability of\nclinical diagnosis and guiding subsequent image-based tasks. In this paper, we\npropose a multi-scale approach that integrates anisotropic Gaussian filtering\nwith progressive Bezier-path redrawing. Our method constructs a scale-space\npyramid to mitigate noise while preserving critical structural details.\nStarting at the coarsest scale, we segment partially denoised images into\ncoherent components and redraw each using a parametric Bezier path with\nrepresentative color. Through iterative refinements at finer scales, small and\nintricate structures are accurately reconstructed, while large homogeneous\nregions remain robustly smoothed. We employ both mean square error and\nself-intersection constraints to maintain shape coherence during path\noptimization. Empirical results on multiple MRI datasets demonstrate consistent\nimprovements in PSNR and SSIM over competing methods. This coarse-to-fine\nframework offers a robust, data-efficient solution for cross-domain denoising,\nreinforcing its potential clinical utility and versatility. Future work extends\nthis technique to three-dimensional data.",
      "generated_abstract": "This paper proposes a novel dual-band 3D-aware end-to-end model, named\nHeterogeneous 3D-aware Masked Attention Network (H3MAN), for 3D ultrasound\nimage segmentation. Specifically, we introduce a Heterogeneous 3D-aware\nMasked Attention module, which dynamically masks the attention weights of\neach feature map based on the semantic information of the feature map, thus\nenhancing the segmentation ability of the model. Furthermore, we propose a\n3D-aware masking strategy to enhance the robustness of the model to 3D\nstructural heterogeneity. Extensive experiments on the TUM 3DQA benchmark\nshow that our model achieves the best performance in terms of both accuracy and\nAUC. The codes will be released.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12213740458015267,
          "p": 0.21621621621621623,
          "f": 0.15609755636216552
        },
        "rouge-2": {
          "r": 0.013422818791946308,
          "p": 0.02127659574468085,
          "f": 0.016460900605938807
        },
        "rouge-l": {
          "r": 0.10687022900763359,
          "p": 0.1891891891891892,
          "f": 0.1365853612402143
        }
      }
    },
    {
      "paper_id": "physics.data-an.physics/data-an/2503.09415v1",
      "true_abstract": "SPARKX is an open-source Python package developed to analyze simulation data\nfrom heavy-ion collision experiments. By offering a comprehensive suite of\ntools, SPARKX simplifies data analysis workflows, supports multiple formats\nsuch as OSCAR2013, and integrates seamlessly with SMASH and JETSCAPE/X-SCAPE.\nThis paper describes SPARKX's architecture, features, and applications and\ndemonstrates its effectiveness through detailed examples and performance\nbenchmarks. SPARKX enhances productivity and precision in relativistic\nkinematics studies.",
      "generated_abstract": "f the neural network (NN) for the identification of the material\n(including the constituent elements) in neutron scattering data is a promising\napproach, especially in the case of the high-resolution studies. In this work,\nwe present a novel application of the NN-based identification method in the\nstudy of the material composition of fcc Fe by neutron scattering experiments.\nWe focus on the identification of the presence of Fe3O4 in the sample.\nFurthermore, we use the NN-based approach to determine the composition of the\nFe3O4 nanoparticles and to estimate their size distribution. The proposed\napproach is based on the analysis of the scattering intensity at the three\nmost relevant scattering angles (2.2, 5.8, and 10.5~{\\rm meV}) in the case of\nFe3O4 nanoparticles. We",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1694915254237288,
          "p": 0.136986301369863,
          "f": 0.15151514657139595
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1694915254237288,
          "p": 0.136986301369863,
          "f": 0.15151514657139595
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.18923v1",
      "true_abstract": "Slutsky symmetry and negative semidefiniteness are necessary and sufficient\nconditions for the rationality of demand functions. While the empirical\nimplications of Slutsky negative semidefiniteness in repeated cross-sectional\ndemand data are well understood, the empirical content of Slutsky symmetry\nremains largely unexplored. This paper takes an important first step toward\naddressing this gap. We demonstrate that the average Slutsky matrix is not\nidentified and that its identified set always contains a symmetric matrix. A\nkey implication of our findings is that the symmetry of the average Slutsky\nmatrix is untestable, and consequently, individual Slutsky symmetry cannot be\ntested using the average Slutsky matrix.",
      "generated_abstract": "r investigates the empirical distribution of a multivariate\ndistribution based on a set of random variables that are correlated with each\nother. We introduce a new distribution called the generalized multivariate\nGaussian family and show that the empirical distribution of a multivariate\ndistribution based on a set of random variables that are correlated with each\nother is a member of this family. We show that the empirical distribution of a\nmultivariate distribution based on a set of random variables that are\ncorrelated with each other is a member of this family. We provide a\ncomputational method to find the members of this family. We show that the\nempirical distribution of a multivariate distribution based on a set of\nrandom variables that are correlated with each other is a member of this family\nwhen the correlation coefficients are not all zero. We use the empirical\ndistribution of a multivariate distribution based on a set of random variables",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18461538461538463,
          "p": 0.2727272727272727,
          "f": 0.22018348142412264
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.03125,
          "f": 0.026315784598338852
        },
        "rouge-l": {
          "r": 0.16923076923076924,
          "p": 0.25,
          "f": 0.20183485757091166
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2502.11510v1",
      "true_abstract": "Longitudinal models with dynamics governed by differential equations may\nrequire numerical integration alongside parameter estimation. We have\nidentified a situation where the numerical integration introduces error in such\na way that it becomes a novel source of non-uniqueness in estimation. We obtain\ntwo very different sets of parameters, one of which is a good estimate of the\ntrue values and the other a very poor one. The two estimates have forward\nnumerical projections statistically indistinguishable from each other because\nof numerical error. In such cases, the posterior distribution for parameters is\nbimodal, with a dominant mode closer to the true parameter value, and a second\ncluster around the errant value. We demonstrate that bimodality exists both\ntheoretically and empirically for an affine first order differential equation,\nthat a simulation workflow can test for evidence of the issue more generally,\nand that Markov Chain Monte Carlo sampling with a suitable solution can avoid\nbimodality. The issue of bimodal posteriors arising from numerical error has\nconsequences for Bayesian inverse methods that rely on numerical integration\nmore broadly.",
      "generated_abstract": "We introduce a novel method to estimate a parameter of a non-linear\nestimator of the cumulative distribution function (CDF) from a sample of\nobservations. The method is based on the use of the maximum likelihood\nestimator (MLE) and the maximum a-posteriori (MAP) estimator. The proposed\nmethod provides a closed-form expression for the estimator and is applicable\nfor both discrete and continuous CDFs. Theoretical properties are studied and\nvalidated through simulations. The proposed method is applied to the\nestimation of the CDF of a random variable with a truncated Gaussian distribution\nand to the estimation of the CDF of the Laplace distribution. The proposed\nmethod is also used to compare the performance of several parametric\ndistributions in a simulation study.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17543859649122806,
          "p": 0.30303030303030304,
          "f": 0.22222221757777788
        },
        "rouge-2": {
          "r": 0.03592814371257485,
          "p": 0.0594059405940594,
          "f": 0.04477611470622682
        },
        "rouge-l": {
          "r": 0.14035087719298245,
          "p": 0.24242424242424243,
          "f": 0.17777777313333343
        }
      }
    },
    {
      "paper_id": "math.NA.stat/CO/2502.07918v2",
      "true_abstract": "Stochastic reaction networks (SRNs) model stochastic effects for various\napplications, including intracellular chemical or biological processes and\nepidemiology. A typical challenge in practical problems modeled by SRNs is that\nonly a few state variables can be dynamically observed. Given the measurement\ntrajectories, one can estimate the conditional probability distribution of\nunobserved (hidden) state variables by solving a stochastic filtering problem.\nIn this setting, the conditional distribution evolves over time according to an\nextensive or potentially infinite-dimensional system of coupled ordinary\ndifferential equations with jumps, known as the filtering equation. The current\nnumerical filtering techniques, such as the Filtered Finite State Projection\n(D'Ambrosio et al., 2022), are hindered by the curse of dimensionality,\nsignificantly affecting their computational performance. To address these\nlimitations, we propose to use a dimensionality reduction technique based on\nthe Markovian projection (MP), initially introduced for forward problems (Ben\nHammouda et al., 2024). In this work, we explore how to adapt the existing MP\napproach to the filtering problem and introduce a novel version of the MP, the\nFiltered MP, that guarantees the consistency of the resulting estimator. The\nnovel method employs a reduced-variance particle filter for estimating the jump\nintensities of the projected model and solves the filtering equations in a\nlow-dimensional space. The analysis and empirical results highlight the\nsuperior computational efficiency of projection methods compared to the\nexisting filtered finite state projection in the large dimensional setting.",
      "generated_abstract": "er a class of multi-task linear regression models with an\nrandom covariance structure and a structured bias term. The goal is to\nidentify the bias, which is commonly estimated by a least squares estimator.\nOur main focus is on the case where the bias is a function of the covariance\nmatrix. The task-specific bias can be estimated using the least squares\nestimator and the covariance matrix can be estimated by a generalised least\nsquares estimator. We provide a general framework for the bias estimator and\nestimate the covariance matrix using the estimator of the bias estimator. The\ncovariance matrix is estimated by a generalised least squares estimator.\n  We obtain consistent estimators for both the bias and the covariance\nmatrix. The consistency of the estimators is established under mild conditions\non the covariance matrix and the bias. We also provide a non",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1069182389937107,
          "p": 0.2786885245901639,
          "f": 0.1545454505376034
        },
        "rouge-2": {
          "r": 0.0273972602739726,
          "p": 0.058823529411764705,
          "f": 0.03738317323434411
        },
        "rouge-l": {
          "r": 0.09433962264150944,
          "p": 0.2459016393442623,
          "f": 0.13636363235578525
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.16525v1",
      "true_abstract": "Recurrent neural networks (RNNs) are central to sequence modeling tasks, yet\ntheir high computational complexity poses challenges for scalability and\nreal-time deployment. Traditional pruning techniques, predominantly based on\nweight magnitudes, often overlook the intrinsic structural properties of these\nnetworks. We introduce a novel framework that models RNNs as partially ordered\nsets (posets) and constructs corresponding dependency lattices. By identifying\nmeet irreducible neurons, our lattice-based pruning algorithm selectively\nretains critical connections while eliminating redundant ones. The method is\nimplemented using both binary and continuous-valued adjacency matrices to\ncapture different aspects of network connectivity. Evaluated on the MNIST\ndataset, our approach exhibits a clear trade-off between sparsity and\nclassification accuracy. Moderate pruning maintains accuracy above 98%, while\naggressive pruning achieves higher sparsity with only a modest performance\ndecline. Unlike conventional magnitude-based pruning, our method leverages the\nstructural organization of RNNs, resulting in more effective preservation of\nfunctional connectivity and improved efficiency in multilayer networks with\ntop-down feedback. The proposed lattice-based pruning framework offers a\nrigorous and scalable approach for reducing RNN complexity while sustaining\nrobust performance, paving the way for more efficient hierarchical models in\nboth machine learning and computational neuroscience.",
      "generated_abstract": "vances in machine learning have uncovered the intrinsic neural\nrecurrence of complex biological systems, revealing that neural circuits\nundergo regular recurrence in their dynamics. However, these findings have\ngenerally been limited to single-cell data, missing the bulk dynamics of\nmulti-cellular systems. Here, we develop a deep learning framework to uncover\nthe intrinsic recurrence of the neural dynamics of multi-cellular systems. By\nfitting a neural network to the dynamics of a multi-cellular system, we\nautomatically generate a recurrent network that faithfully captures the\nneural dynamics of the system. We demonstrate this approach on an autism\ngenetics dataset, showing that neural dynamics are recurrent across diverse\ncell types, supporting the idea that neural circuits exhibit intrinsic\nrecurrence. We also extend this framework to other multi-cellular systems,\nincluding neural circuits and neural",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14685314685314685,
          "p": 0.28378378378378377,
          "f": 0.19354838260230636
        },
        "rouge-2": {
          "r": 0.0106951871657754,
          "p": 0.018018018018018018,
          "f": 0.013422814117158512
        },
        "rouge-l": {
          "r": 0.13986013986013987,
          "p": 0.2702702702702703,
          "f": 0.18433179274055522
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.00920v2",
      "true_abstract": "This paper contributes to the literature on parametric demand estimation by\nusing deep learning to model consumer preferences. Traditional econometric\nmethods often struggle with limited within-product price variation, a challenge\naddressed by the proposed neural network approach. The proposed method\nestimates the functional form of the demand and demonstrates higher performance\nin both simulations and empirical applications. Notably, under low price\nvariation, the machine learning model outperforms econometric approaches,\nreducing the mean squared error of initial price parameter estimates by nearly\nthreefold. In empirical setting, the ML model consistently predicts a negative\nrelationship between demand and price in 100% of cases, whereas the econometric\napproach fails to do so in 20% of cases. The suggested model incorporates a\nwide range of product characteristics, as well as prices of other products and\ncompetitors.",
      "generated_abstract": "y examines the impact of the COVID-19 pandemic on the global\nmarket for electric vehicles (EVs). The findings reveal that the pandemic\ndisrupted supply chains and altered the market dynamics, resulting in a\nsignificant decline in EV sales. This study provides a comprehensive analysis\nof the impact of the pandemic on EV market dynamics, identifying key\nrelevant factors such as supply chain disruptions, economic uncertainty, and\nthe rise of electric vehicles in the auto industry. The research explores the\nimpact of COVID-19 on EV sales, market demand, and production, assessing the\nimpact of these factors on the global EV market. The findings reveal that the\npandemic had a significant impact on EV sales, with declining demand due to\neconomic uncertainty and the rise of EVs in the auto industry. This study\nprovides",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13186813186813187,
          "p": 0.1875,
          "f": 0.1548387048291365
        },
        "rouge-2": {
          "r": 0.007751937984496124,
          "p": 0.010101010101010102,
          "f": 0.008771924911129252
        },
        "rouge-l": {
          "r": 0.10989010989010989,
          "p": 0.15625,
          "f": 0.1290322532162333
        }
      }
    },
    {
      "paper_id": "math.AG.math/AC/2503.01752v1",
      "true_abstract": "Border basis schemes are open subschemes of the Hilbert scheme of $\\mu$\npoints in an affine space $\\mathbb{A}^n$. They have easily describable systems\nof generators of their vanishing ideals for a natural embedding into a large\naffine space $\\mathbb{A}^{\\mu\\nu}$. Here we bring together several techniques\nfor re-embedding affine schemes into lower dimensional spaces which we\ndeveloped in the last years. We study their efficacy for some special types of\nborder basis schemes such as MaxDeg border basis schemes, L-shape and\nsimplicial border basis schemes, as well as planar border basis schemes. A\nparticular care is taken to make these re-embeddings efficiently computable and\nto check when we actually get an isomorphism with $\\mathbb{A}^{n\\mu}$, i.e.,\nwhen the border basis scheme is an affine cell.",
      "generated_abstract": "igate the relationship between the Hodge-Tate spectrum and the\nalgebraic Hodge-Tate spectrum, and their intersection with the derived category\nof coherent sheaves on a smooth projective variety $X$. We prove that the\nintersection is a complete intersection with respect to the Hodge-Tate filtration\non the Hodge-Tate spectrum, and that the intersection of the algebraic Hodge-Tate\nspectrum with the derived category is a complete intersection with respect to\nthe algebraic Hodge-Tate filtration on the algebraic Hodge-Tate spectrum. In\nparticular, we show that the algebraic Hodge-Tate spectrum is the intersection of\nthe Hodge-Tate spectrum and the algebraic Hodge-Tate spectrum. We also prove\nthat the intersection of the algebraic Hodge-Tate spectrum with the derived\ncategory of coherent sheaves is a complete intersection with respect to the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11764705882352941,
          "p": 0.2777777777777778,
          "f": 0.16528925201830486
        },
        "rouge-2": {
          "r": 0.008695652173913044,
          "p": 0.017241379310344827,
          "f": 0.011560689184404805
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.2777777777777778,
          "f": 0.16528925201830486
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.nlin/CD/2503.08905v1",
      "true_abstract": "Sub-Neptunes have been found to be one of the most common types of\nexoplanets, yet their physical parameters and properties are poorly determined\nand in need of further investigation. In order to improve the mass measurement\nand parameter determination of two sub-Neptunes, K2-266 d and K2-266 e, we\npresent new transit observations obtained with CHaracterising ExOPlanets\nSatellite (CHEOPS) and Transiting Exoplanet Survey Satellite (TESS), increasing\nthe baseline of transit data from a few epochs to 165 epochs for K2-266 d, and\nto 121 epochs for K2-266 e. Through a two-stage fitting process, it is found\nthat the masses of K2-266 d and K2-266 e are 6.01$\\pm$0.43 $M_\\oplus$ and\n7.70$\\pm$0.58 $M_\\oplus$, respectively. With these updated values and one order\nof magnitude better precision, we confirm the planets to belong to the\npopulation of planets that has been determined to be volatile-rich. Finally, we\npresent the results of dynamical simulations, showing that the system is\nstable, the orbits are not chaotic, and that these two planets are close to but\nnot in 4:3 mean motion resonance.",
      "generated_abstract": "The presence of a black hole (BH) in the center of the galaxy M87 can be\ndetected only if the BH is gravitationally lensed. In this paper we study the\npossibility of lensing of the CMB in the framework of the Einstein-de Sitter\n(EDS) spacetime. We compute the lensing potentials of the CMB and the lens\npotential of the galaxy, and we perform a systematic analysis of the constraints\non the parameters of the model. We find that the CMB lensing potential\ndeviates significantly from the standard Einstein-de Sitter model, and we find\nthat the lensing of the galaxy by the BH can be observed with current and up\ncoming telescopes.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11711711711711711,
          "p": 0.20967741935483872,
          "f": 0.15028901274215659
        },
        "rouge-2": {
          "r": 0.011904761904761904,
          "p": 0.02197802197802198,
          "f": 0.015444010885945202
        },
        "rouge-l": {
          "r": 0.08108108108108109,
          "p": 0.14516129032258066,
          "f": 0.10404623817568266
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.08457v1",
      "true_abstract": "This paper explores foliated differential graded algebras (dga) and their\nrole in extending fundamental theorems of differential geometry to foliations.\nWe establish an $A_{\\infty}$ de Rham theorem for foliations, demonstrating that\nthe classical quasi-isomorphism between singular cochains and de Rham forms\nlifts to an $A_{\\infty}$ quasi-isomorphism in the foliated setting.\nFurthermore, we investigate the Riemann-Hilbert correspondence for foliations,\nbuilding upon the established higher Riemann-Hilbert correspondence for\nmanifolds. By constructing an integration functor, we prove a higher\nRiemann-Hilbert correspondence for foliations, revealing an equivalence between\n$\\infty$-representations of $L_{\\infty}$-algebroids and\n$\\infty$-representations of Lie $\\infty$-groupoids within the context of\nfoliations. This work generalizes the classical Riemann-Hilbert correspondence\nto foliations, providing a deeper understanding of the relationship between\nrepresentations of Lie algebroids and Lie groupoids in this framework.",
      "generated_abstract": "We study the (generalized) Riemann-Hilbert problem for the operator $L=\n(-\\Delta +1)^{1/2}$, where $-\\Delta$ is the Laplacian on the unit disk. We\nexamine the behavior of the spectral parameter $z\\in \\mathbb{C}$ as $L$\napproaches the unitary operator $U(z) = e^{z\\sqrt{1-z^2}}$. We obtain the\nspectral data for $U(z)$, including the spectrum and the associated resolvent,\nthe $L^2$-eigenfunctions, and the associated eigenvalues. We also investigate\nthe behavior of the $L^2$-eigenfunctions, especially their asymptotics as\n$|z|\\to \\infty$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10810810810810811,
          "p": 0.16326530612244897,
          "f": 0.13008129601956525
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.030303030303030304,
          "f": 0.02272726803977369
        },
        "rouge-l": {
          "r": 0.0945945945945946,
          "p": 0.14285714285714285,
          "f": 0.11382113341793927
        }
      }
    },
    {
      "paper_id": "math.QA.math/QA/2503.08839v1",
      "true_abstract": "We introduce a new topological coproduct $\\Delta^{\\psi}_{u}$ for quantum\ntoroidal algebras $U_{q}(\\mathfrak{g}_{\\mathrm{tor}})$ in untwisted types,\nleading to a well-defined tensor product on the category\n$\\widehat{\\mathcal{O}}_{\\mathrm{int}}$ of integrable representations. This is\ndefined by twisting the Drinfeld coproduct $\\Delta_{u}$ with an anti-involution\n$\\psi$ of $U_{q}(\\mathfrak{g}_{\\mathrm{tor}})$ that swaps its horizontal and\nvertical quantum affine subalgebras. Other applications of $\\psi$ include\ngeneralising the celebrated Miki automorphism from type $A$, and an action of\nthe universal cover of $SL_{2}(\\mathbb{Z})$.\n  Next, we investigate the ensuing tensor representations of\n$U_{q}(\\mathfrak{g}_{\\mathrm{tor}})$, and prove quantum toroidal analogues for\na series of influential results by Chari-Pressley on the affine level. In\nparticular, there is a compatibility with Drinfeld polynomials, and the product\nof irreducibles is generically irreducible. Furthermore, we obtain $R$-matrices\nwith spectral parameter which provide solutions to the (trigonometric, quantum)\nYang-Baxter equation, and endow $\\widehat{\\mathcal{O}}_{\\mathrm{int}}$ with a\nmeromorphic braiding. These moreover give rise to a commuting family of\ntransfer matrices for each module.",
      "generated_abstract": "the noncommutative cohomology of a noncommutative analytic space\n$X$ with a noncommutative analytic action of a Lie group $G$. The action is\ndefined by a family of germs of noncommutative analytic functions on $X$ and is\ntame if the family is tame. The noncommutative cohomology of $X$ is defined as\nthe cohomology of the noncommutative sheaf of germs of noncommutative analytic\nfunctions on $X$ with values in the algebra of germs of noncommutative\nanalytic functions on $X$. We prove that the noncommutative cohomology is\nisomorphic to the noncommutative cohomology of $X$ with coefficients in the\nalgebra of germs of noncommutative analytic functions on $X$ with values in\n$G$-invariant elements of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1523809523809524,
          "p": 0.43243243243243246,
          "f": 0.2253521088226543
        },
        "rouge-2": {
          "r": 0.04666666666666667,
          "p": 0.109375,
          "f": 0.06542055655515792
        },
        "rouge-l": {
          "r": 0.12380952380952381,
          "p": 0.35135135135135137,
          "f": 0.18309858769589377
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2502.14160v1",
      "true_abstract": "In this paper, we study inverse game theory (resp. inverse multiagent\nlearning) in which the goal is to find parameters of a game's payoff functions\nfor which the expected (resp. sampled) behavior is an equilibrium. We formulate\nthese problems as generative-adversarial (i.e., min-max) optimization problems,\nfor which we develop polynomial-time algorithms to solve, the former of which\nrelies on an exact first-order oracle, and the latter, a stochastic one. We\nextend our approach to solve inverse multiagent simulacral learning in\npolynomial time and number of samples. In these problems, we seek a simulacrum,\nmeaning parameters and an associated equilibrium that replicate the given\nobservations in expectation. We find that our approach outperforms the\nwidely-used ARIMA method in predicting prices in Spanish electricity markets\nbased on time-series data.",
      "generated_abstract": "uce a new class of game-theoretic settings where the underlying\ncharacteristic of the players is a random variable. This setting extends the\nclass of randomized game-theoretic settings considered in the literature to\ninclude the setting of random choice, where the players choose among multiple\noutcomes, and the setting of random payoffs, where the players choose among\nmultiple payoffs. We develop an extension of Nash equilibrium concept to the\nsetting of random choices and payoffs. In the special case of random choice,\nour notion of Nash equilibrium is equivalent to the standard Nash equilibrium\nin the corresponding randomized game-theoretic setting. We further propose a\nclass of randomized games that includes the standard randomized games and\nextend the notion of Nash equilibrium to this setting. We prove that the\nclass of randomized games admits an extension of Nash equilibrium to this\nsetting. Finally, we extend the notion of Nash equilibrium to the setting of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19540229885057472,
          "p": 0.288135593220339,
          "f": 0.23287670751266668
        },
        "rouge-2": {
          "r": 0.008064516129032258,
          "p": 0.01,
          "f": 0.008928566485972123
        },
        "rouge-l": {
          "r": 0.19540229885057472,
          "p": 0.288135593220339,
          "f": 0.23287670751266668
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/GN/2404.15226v3",
      "true_abstract": "We revisit granular models that represent the size of a firm as the sum of\nthe sizes of multiple constituents or sub-units. Originally developed to\naddress the unexpectedly slow reduction in volatility as firm size increases,\nthese models also explain the shape of the distribution of firm growth rates.\n  We introduce new theoretical insights regarding the relationship between firm\nsize and growth rate statistics within this framework, directly linking the\ngrowth statistics of a firm to how diversified it is. The non-intuitive nature\nof our results arises from the fat-tailed distributions of the size and the\nnumber of sub-units, which suggest the categorization of firms into three\ndistinct diversification types: well-diversified firms with sizes evenly\ndistributed across many sub-units, firms with many sub-units but concentrated\nsize in just a few, and poorly diversified firms consisting of only a small\nnumber of sub-units.\n  Inspired by our theoretical findings, we identify new empirical patterns in\nfirm growth. Our findings show that growth volatility, when adjusted by average\nsize-conditioned volatility, has a size-independent distribution, but with a\ntail that is much too thin to be in agreement with the predictions of granular\nmodels. Furthermore, the predicted Gaussian distribution of growth rates, even\nwhen rescaled for firm-specific volatility, remains fat-tailed across all\nsizes. Such discrepancies not only challenge the granularity hypothesis but\nalso underscore the need for deeper exploration into the mechanisms driving\nfirm growth.",
      "generated_abstract": "y examines the impact of the Covid-19 pandemic on the performance of\nthe European equity markets. We employ the GARCH model to model the volatility\nof the returns of the STOXX 600 and the Euro Stoxx 50 indices, as well as the\nVolatility (IVol) and Implied Volatility (IVol) indexes, in order to analyze\nthe market's response to the pandemic. We find that the pandemic had a\nsignificant impact on stock market volatility, with the IVol and IVol\nindices exhibiting higher volatility than the STOXX 600 and the Euro Stoxx 50\nindices. This volatility increased during the first wave of the pandemic,\nexhibiting a negative correlation with economic activity and a positive\ncorrelation with the STOXX 600 and Euro",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08888888888888889,
          "p": 0.1935483870967742,
          "f": 0.12182740685408039
        },
        "rouge-2": {
          "r": 0.018433179723502304,
          "p": 0.04040404040404041,
          "f": 0.02531645139340724
        },
        "rouge-l": {
          "r": 0.08148148148148149,
          "p": 0.1774193548387097,
          "f": 0.11167512259012104
        }
      }
    },
    {
      "paper_id": "quant-ph.math/IT/2503.08736v1",
      "true_abstract": "We provide a streamlined elaboration on existing ideas that link Ising anyon\n(or equivalently, Majorana) stabilizer codes to certain classes of binary\nclassical codes. The groundwork for such Majorana-based quantum codes can be\nfound in earlier works (including, for example, Bravyi (arXiv:1004.3791) and\nVijay et al. (arXiv:1703.00459)), where it was observed that commuting families\nof fermionic (Clifford) operators can can often be systematically lifted from\nweakly self-dual or self-orthogonal binary codes. Here, we recast and unify\nthese ideas into a classification theorem that explicitly shows how explicitly\nshows how q-isotropic subspaces in $\\mathbb{F}_2^{2n}$ yield commuting Clifford\noperators relevant to Ising anyons, and how these subspaces naturally\ncorrespond to punctured self-orthogonal codes in $\\mathbb{F}_2^{2n+1}$.",
      "generated_abstract": "of entanglement in systems with finite resources has been a\nsubject of intense research in the last few years. Recently, the study of\nquantum-to-classical-box (QCXB) entanglement has emerged as a promising avenue\nfor exploring quantum entanglement resources. The study of QCXB entanglement has\nbeen restricted to two-qubit systems, and the study of entanglement in higher\ndimensional systems is still in its infancy. In this work, we study the\nentanglement of a three-qubit system with a finite number of copies in the\nframework of the QCXB entanglement. We focus on the entanglement of a three-qubit\nsystem with a finite number of copies in the framework of the QCXB entanglement.\nUsing a series of operations, we show that the entanglement of the three",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1411764705882353,
          "p": 0.2033898305084746,
          "f": 0.16666666182966836
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1411764705882353,
          "p": 0.2033898305084746,
          "f": 0.16666666182966836
        }
      }
    },
    {
      "paper_id": "cs.CL.econ/GN/2502.14497v1",
      "true_abstract": "Macroeconomic fluctuations and the narratives that shape them form a mutually\nreinforcing cycle: public discourse can spur behavioural changes leading to\neconomic shifts, which then result in changes in the stories that propagate. We\nshow that shifts in semantic embedding space can be causally linked to\nfinancial market shocks -- deviations from the expected market behaviour.\nFurthermore, we show how partisanship can influence the predictive power of\ntext for market fluctuations and shape reactions to those same shocks. We also\nprovide some evidence that text-based signals are particularly salient during\nunexpected events such as COVID-19, highlighting the value of language data as\nan exogenous variable in economic forecasting. Our findings underscore the\nbidirectional relationship between news outlets and market shocks, offering a\nnovel empirical approach to studying their effect on each other.",
      "generated_abstract": "r studies the problem of designing an auction protocol for the\nbid-ask-spread of a non-monetary asset, i.e., a bid-ask spread auction. We\nintroduce a novel approach, the Dynamic Auction (DA) with Precise Information\n(DPI), which is a non-monetary auction protocol that combines the advantages of\nthe existing auction protocols for monetary assets, namely, the Dynamic\nAuction (DA) and the Precise Information (PI) auction. The DPI is a hybrid\nauction protocol that combines the strengths of the DA and the PI auction. The\nmain innovation of the DPI is the use of a weighted auction mechanism, where\nthe weights of the winning bids are determined based on the precision of the\ninformation provided by the bidders. In contrast to the existing au",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12871287128712872,
          "p": 0.18571428571428572,
          "f": 0.15204677879005524
        },
        "rouge-2": {
          "r": 0.015384615384615385,
          "p": 0.0196078431372549,
          "f": 0.0172413743831762
        },
        "rouge-l": {
          "r": 0.1188118811881188,
          "p": 0.17142857142857143,
          "f": 0.14035087235730667
        }
      }
    },
    {
      "paper_id": "math-ph.math/SP/2503.08595v1",
      "true_abstract": "In this expository note, we study several families of periodic graphs which\nsatisfy a sufficient condition for the ergodicity of the associated\ncontinuous-time quantum walk. For these graphs, we compute the limiting\ndistribution of the walk explicitly. We uncover interesting behavior where in\nsome families, the walk is ergodic in both horizontal and sectional directions,\nwhile in others, ergodicity only holds in the horizontal (large N) direction.\nWe compare this to the limiting distribution of classical random walks on the\nsame graphs.",
      "generated_abstract": "We give a proof of the $3$-dimensional Ising-type Ising-Hamiltonian\nproperty of the generalized Ising model, which generalizes the Ising\nHamiltonian property of the 3-dimensional Ising model. We give an explicit\nrepresentation of the Hamiltonian in terms of the generalized Ising model and\nstudy its properties. We show that the generalized Ising model is a\ntopological spin model and that its ground state is a spin-1/2 quantum spin\nchain with a given ground state energy. We show that the generalized Ising\nmodel has a simple Hamiltonian which is related to the original Ising\nHamiltonian by a simple gauge transformation. We give a combinatorial proof of\nthe Ising-Hamiltonian property for the generalized Ising model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2033898305084746,
          "p": 0.24,
          "f": 0.22018348127262027
        },
        "rouge-2": {
          "r": 0.039473684210526314,
          "p": 0.0375,
          "f": 0.038461533464826426
        },
        "rouge-l": {
          "r": 0.15254237288135594,
          "p": 0.18,
          "f": 0.1651376097129873
        }
      }
    },
    {
      "paper_id": "math.CA.math/FA/2503.10190v1",
      "true_abstract": "In a famous paper published in 1904, Helge von Koch introduced the curve that\nstill serves nowadays as an iconic representation of fractal shapes. In fact,\nvon Koch's main goal was the construction of a continuous but nowhere\ndifferentiable function, very similar to the snowflake, using elementary\ngeometric procedures, and not analytical formulae. We prove that a parametrized\nfamily of functions (including and) generalizing von Koch's example enjoys a\nrich multifractal behavior, thus enriching the class of historical mathematical\nobjects having surprising regularity properties. The analysis relies on the\nstudy of the orbits of an underlying dynamical system and on the introduction\nof self-similar measures and non-trivial iterated functions systems adapted to\nthe problem.",
      "generated_abstract": "We give a proof of the conjecture of Fuchs that the real-analytic extension\nof a meromorphic function on a complex surface is meromorphic on the\nassociated Riemann surface. We give a proof of the conjecture of Kawai that\nthe real-analytic extension of a meromorphic function on a complex surface is\nmeromorphic on the Riemann surface of the associated meromorphic function. We\nalso prove that the real-analytic extension of a meromorphic function on a\ncomplex surface is real-analytic on the Riemann surface of the associated\nmeromorphic function.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08045977011494253,
          "p": 0.3181818181818182,
          "f": 0.12844036375052614
        },
        "rouge-2": {
          "r": 0.03636363636363636,
          "p": 0.10256410256410256,
          "f": 0.053691271303094736
        },
        "rouge-l": {
          "r": 0.08045977011494253,
          "p": 0.3181818181818182,
          "f": 0.12844036375052614
        }
      }
    },
    {
      "paper_id": "math.PR.stat/TH/2502.15116v1",
      "true_abstract": "We introduce an empirical functional $\\Psi$ that is an optimal uniform mean\nestimator: Let $F\\subset L_2(\\mu)$ be a class of mean zero functions, $u$ is a\nreal valued function, and $X_1,\\dots,X_N$ are independent, distributed\naccording to $\\mu$. We show that under minimal assumptions, with $\\mu^{\\otimes\nN}$ exponentially high probability, \\[ \\sup_{f\\in F} |\\Psi(X_1,\\dots,X_N,f) -\n\\mathbb{E} u(f(X))| \\leq c R(F) \\frac{ \\mathbb{E} \\sup_{f\\in F } |G_f| }{\\sqrt\nN}, \\] where $(G_f)_{f\\in F}$ is the gaussian processes indexed by $F$ and\n$R(F)$ is an appropriate notion of `diameter' of the class $\\{u(f(X)) : f\\in\nF\\}$.\n  The fact that such a bound is possible is surprising, and it leads to the\nsolution of various key problems in high dimensional probability and high\ndimensional statistics. The construction is based on combining Talagrand's\ngeneric chaining mechanism with optimal mean estimation procedures for a single\nreal-valued random variable.",
      "generated_abstract": "We introduce a novel generalization of the Laplace approximation, which is\nestimated by the method of moments using the multivariate Gaussian distribution.\nThis approach is particularly suited to the estimation of high-dimensional\nestimators, such as the mean and covariance of a multivariate Gaussian\ndistribution. Furthermore, the method of moments can be efficiently implemented\nin large-scale applications, which is particularly important in the context of\nlarge-scale Bayesian modeling and large-scale data analysis. In particular, we\ndemonstrate that the proposed method of moments is computationally more\nefficient than the standard Laplace approximation and the Gaussian-Markov\nchain Monte Carlo method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1388888888888889,
          "p": 0.24193548387096775,
          "f": 0.17647058360138423
        },
        "rouge-2": {
          "r": 0.02158273381294964,
          "p": 0.034482758620689655,
          "f": 0.026548667831076102
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.1935483870967742,
          "f": 0.1411764659543254
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/MN/2408.08503v1",
      "true_abstract": "Research organisms provide invaluable insights into human biology and\ndiseases, serving as essential tools for functional experiments, disease\nmodeling, and drug testing. However, evolutionary divergence between humans and\nresearch organisms hinders effective knowledge transfer across species. Here,\nwe review state-of-the-art methods for computationally transferring knowledge\nacross species, primarily focusing on methods that utilize transcriptome data\nand/or molecular networks. We introduce the term \"agnology\" to describe the\nfunctional equivalence of molecular components regardless of evolutionary\norigin, as this concept is becoming pervasive in integrative data-driven models\nwhere the role of evolutionary origin can become unclear. Our review addresses\nfour key areas of information and knowledge transfer across species: (1)\ntransferring disease and gene annotation knowledge, (2) identifying agnologous\nmolecular components, (3) inferring equivalent perturbed genes or gene sets,\nand (4) identifying agnologous cell types. We conclude with an outlook on\nfuture directions and several key challenges that remain in cross-species\nknowledge transfer.",
      "generated_abstract": "aper, we introduce a novel methodology to quantify the degree of\nconformity of protein-protein interactions (PPIs) to their respective\nmetabolic pathways. We show that the degree of conformity, defined as the\npercentage of PPIs that are predicted to be functional, can be used to\nquantitatively assess the similarity between metabolic pathways and their\nprotein-protein interactions. We validate our approach by analyzing the\nconformity of PPIs to 14 different metabolic pathways, which are representative\nof the largest and most diverse set of pathways in the DrugBank database. Our\nresults show that the degree of conformity correlates well with the similarity\nbetween the PPIs and their respective metabolic pathways. For example, we find\nthat 82% of the PPIs predicted to be functional are also functional in the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13274336283185842,
          "p": 0.22727272727272727,
          "f": 0.1675977607078432
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13274336283185842,
          "p": 0.22727272727272727,
          "f": 0.1675977607078432
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/RM/2412.09662v1",
      "true_abstract": "This article analytically characterizes the impermanent loss for automatic\nmarket makers in decentralized exchanges such as Uniswap or Balancer (CPMM). We\npresent a theoretical static replication formula for the pool value using a\ncombination of European calls and puts. We will formulate a result to guarantee\ncoverage for any final price that falls within a predefined range.",
      "generated_abstract": "This paper investigates the impact of liquidity on asset prices in the\nmarket with a long-short equity strategy. We formulate a dynamic model that\nrepresents the market dynamics with the stochastic volatility model. Our model\nallows the incorporation of the liquidity, which is an important feature in\nthe market. The liquidity plays a crucial role in the market, as it affects\nthe price of a security. Our analysis focuses on the effects of liquidity on\nthe asset prices. The paper is organized as follows. In Section 2, we present\nthe model and the assumptions. In Section 3, we discuss the numerical\nsimulations. Finally, in Section 4, we conclude our findings.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26,
          "p": 0.19696969696969696,
          "f": 0.2241379261296077
        },
        "rouge-2": {
          "r": 0.017857142857142856,
          "p": 0.01,
          "f": 0.0128205082182791
        },
        "rouge-l": {
          "r": 0.26,
          "p": 0.19696969696969696,
          "f": 0.2241379261296077
        }
      }
    },
    {
      "paper_id": "nucl-th.nucl-ex/2503.09045v1",
      "true_abstract": "Electromagnetic (EM) probes, including photons and dileptons, do not interact\nstrongly after their production in heavy-ion collisions, allowing them to carry\nundistorted information from their points of origin. This makes them powerful\ntools for studying early-stage equilibration and the thermodynamic properties\nof the quark-gluon plasma (QGP). In these proceedings, we highlight recent\ntheoretical advancements in EM probes, focusing on their role in probing\nearly-stage dynamics and extracting medium properties. We also discuss the\nemerging multimessenger approach, which combines hadronic and electromagnetic\nprobes to achieve a more comprehensive understanding of the QGP.",
      "generated_abstract": "of the spin-orbit interaction is a crucial aspect in nuclear\nstructure calculations, yet its role in nuclei beyond the proton-neutron\ndichotomy has not been fully explored. Here, we present a detailed\ncalculation of the spin-orbit interaction in nuclei up to $^{208}$Pb, focusing\non the nuclear symmetry energy and the chiral EFT effects. Our analysis\nincorporates the effects of the spin-orbit interaction by means of a\nself-consistent procedure based on the Gogny-D1S force, where the Gogny\nparameters are optimized to reproduce the experimental binding energies and\nisospin asymmetry parameters of the nuclei considered. Our results indicate\nthat the spin-orbit interaction can significantly modify the binding energy of\nnuclei beyond the proton-neutron dichotomy. This effect is particularly\npr",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16216216216216217,
          "p": 0.16666666666666666,
          "f": 0.16438355664477405
        },
        "rouge-2": {
          "r": 0.0449438202247191,
          "p": 0.04,
          "f": 0.04232803734497973
        },
        "rouge-l": {
          "r": 0.14864864864864866,
          "p": 0.1527777777777778,
          "f": 0.15068492650778773
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.04407v1",
      "true_abstract": "In this paper, we propose a movable antenna (MA)-enabled frequency-hopping\n(FH) multiple-input multiple-output (MIMO) radar system and investigate its\nsensing resolution. Specifically, we derive the expression of the ambiguity\nfunction and analyze the relationship between its main lobe width and the\ntransmit antenna positions. In particular, the optimal antenna distribution to\nachieve the minimum main lobe width in the angular domain is characterized. We\ndiscover that this minimum width is related to the antenna size, the antenna\nnumber, and the target angle. Meanwhile, we present lower bounds of the\nambiguity function in the Doppler and delay domains, and show that the impact\nof the antenna size on the radar performance in these two domains is very\ndifferent from that in the angular domain. Moreover, the performance\nenhancement brought by MAs exhibits a certain trade-off between the main lobe\nwidth and the side lobe peak levels. Therefore, we propose to balance between\nminimizing the side lobe levels and narrowing the main lobe of the ambiguity\nfunction by optimizing the antenna positions. To achieve this goal, we propose\na low-complexity algorithm based on the Rosen's gradient projection method, and\nshow that its performance is very close to the baseline. Simulation results are\npresented to validate the theoretical analysis on the properties of the\nambiguity function, and demonstrate that MAs can reduce the main lobe width and\nsuppress the side lobe levels of the ambiguity function, thereby enhancing\nradar performance.",
      "generated_abstract": "aper, we propose a novel adaptive multi-target tracking (AMT)\nsystem for multi-beam radar systems that can handle multiple targets. The\nproposed AMT system is composed of a radar and a tracking controller that\nintegrates an adaptive radar phase shifter (RPS) and a multi-target\ntracking-by-detection (MT-TBDD) detector. The proposed AMT system achieves\nhigh-performance tracking under different radar configurations. The RPS is\nadaptively adjusted to optimize the radar phase, thereby enhancing\ntracking-by-detection performance. The MT-TBDD detector is designed to\naccurately track multiple targets in the presence of interference. The AMT\nsystem is implemented on a dual-beam radar, which consists of two radar\nchannels and two tracking stages. Experimental results",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.29411764705882354,
          "f": 0.2185792303024875
        },
        "rouge-2": {
          "r": 0.026595744680851064,
          "p": 0.050505050505050504,
          "f": 0.03484320105573758
        },
        "rouge-l": {
          "r": 0.17391304347826086,
          "p": 0.29411764705882354,
          "f": 0.2185792303024875
        }
      }
    },
    {
      "paper_id": "math.FA.math/CA/2503.04285v1",
      "true_abstract": "We introduce the space SBV$_X$ of special functions with bounded\n$X$-variation in Carnot-Carath\\'eodory spaces and study its main properties.\nOur main outcome is an approximation result, with respect to the BV$_X$\ntopology, for SBV$_X$ functions.",
      "generated_abstract": "aper we consider the problem of constructing a random walk on the\nfinite simple graph $G$ with a non-constant rate of transition and with an\narbitrary probability distribution on the state space of the walk. We\nintroduce a new notion of mixing of the random walk and show that it is\nequivalent to the existence of a Markov chain on the state space of the walk\nwith a non-constant transition rate and with a non-constant probability\ndistribution on the state space. We show that in the case of a simple graph\nthere is no such Markov chain, and in the case of a finite graph there exists a\nMarkov chain with a non-constant probability distribution on the state space\nof the walk. Finally, we show that the mixing of the random walk on the graph\n$G$ is equivalent to the existence of a Markov chain on the state space of the\nwalk with a non-constant transition rate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.36666666666666664,
          "p": 0.22916666666666666,
          "f": 0.28205127731755425
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.022727272727272728,
          "f": 0.032786881225477515
        },
        "rouge-l": {
          "r": 0.3,
          "p": 0.1875,
          "f": 0.23076922603550304
        }
      }
    },
    {
      "paper_id": "math.RT.math/RT/2503.08020v2",
      "true_abstract": "We introduce weighted cycles on weaves of general Dynkin types and define a\nskew-symmetrizable intersection pairing between weighted cycles. We prove that\nweighted cycles on a weave form a Laurent polynomial algebra and construct a\nquantization for this algebra using the skew-symmetric intersection pairing in\nthe simply-laced case. We define merodromies along weighted cycles as functions\non the decorated flag moduli space of the weave. We relate weighted cycles with\ncluster variables in a cluster algebra and prove that mutations of weighted\ncycles are compatible with mutations of cluster variables.",
      "generated_abstract": "the dynamics of a class of non-autonomous ordinary differential\nequations (ODEs) which arise from the study of the stability of a state\nvariable in a non-autonomous system. The dynamics of these equations are\ndescribed by the so-called $p$-Laplacian on a Riemannian manifold. In this paper,\nwe study the dynamics of these ODEs in two different ways. The first one is by\nusing the $p$-Laplacian on the Riemannian manifold and the second one is by\nusing the corresponding Hamiltonian system. We obtain a result that the\ndynamics of these ODEs are similar to the dynamics of the corresponding ODEs\non the corresponding Hamiltonian system. Furthermore, we study the dynamics of\nthese ODEs in a Hilbert space and we obtain a result that the dynamics of these\nODEs are similar to the dynamics of the corresponding ODEs",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24489795918367346,
          "p": 0.2222222222222222,
          "f": 0.23300970374964664
        },
        "rouge-2": {
          "r": 0.0641025641025641,
          "p": 0.05813953488372093,
          "f": 0.06097560476799565
        },
        "rouge-l": {
          "r": 0.22448979591836735,
          "p": 0.2037037037037037,
          "f": 0.21359222802149128
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/OS/2503.05117v1",
      "true_abstract": "This paper presents HyperGraph ROS, an open-source robot operating system\nthat unifies intra-process, inter-process, and cross-device computation into a\ncomputational hypergraph for efficient message passing and parallel execution.\nIn order to optimize communication, HyperGraph ROS dynamically selects the\noptimal communication mechanism while maintaining a consistent API. For\nintra-process messages, Intel-TBB Flow Graph is used with C++ pointer passing,\nwhich ensures zero memory copying and instant delivery. Meanwhile,\ninter-process and cross-device communication seamlessly switch to ZeroMQ. When\na node receives a message from any source, it is immediately activated and\nscheduled for parallel execution by Intel-TBB. The computational hypergraph\nconsists of nodes represented by TBB flow graph nodes and edges formed by TBB\npointer-based connections for intra-process communication, as well as ZeroMQ\nlinks for inter-process and cross-device communication. This structure enables\nseamless distributed parallelism. Additionally, HyperGraph ROS provides\nROS-like utilities such as a parameter server, a coordinate transformation\ntree, and visualization tools. Evaluation in diverse robotic scenarios\ndemonstrates significantly higher transmission and throughput efficiency\ncompared to ROS 2. Our work is available at\nhttps://github.com/wujiazheng2020a/hyper_graph_ros.",
      "generated_abstract": "development of AI technology has transformed the automotive\nindustry, enabling advanced driver assistance systems (ADAS) and fully\nautonomous driving (FAAD). However, these systems face significant challenges\nin real-world driving environments, including the challenges of\nautonomous-driving system (ADS) collision avoidance and human-ADS interaction.\nTo address these challenges, this paper presents the ROS-based ADS-Vision\nSystem, which provides real-time visual feedback to the ADS. The system\nintegrates a lightweight ROS platform and a convolutional neural network (CNN)\nto generate collision avoidance visual feedback for the ADS. It also\nintegrates an image recognition algorithm to extract facial expressions of the\nhuman-ADS interaction. The system is designed to work seamlessly with the\nROS-based RGB-D camera and the ROS-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14728682170542637,
          "p": 0.2345679012345679,
          "f": 0.1809523762136056
        },
        "rouge-2": {
          "r": 0.006060606060606061,
          "p": 0.009433962264150943,
          "f": 0.007380069037734042
        },
        "rouge-l": {
          "r": 0.13178294573643412,
          "p": 0.20987654320987653,
          "f": 0.16190475716598654
        }
      }
    },
    {
      "paper_id": "math.ST.q-bio/PE/2501.17622v1",
      "true_abstract": "We study the optimization landscape of maximum likelihood estimation for a\nbinary latent tree model with hidden variables at internal nodes and observed\nvariables at the leaves. This model, known as the Cavender-Farris-Neyman (CFN)\nmodel in statistical phylogenetics, is also a special case of the ferromagnetic\nIsing model. While the likelihood function is known to be non-concave with\nmultiple critical points in general, gradient descent-type optimization methods\nhave proven surprisingly effective in practice. We provide theoretical insights\ninto this phenomenon by analyzing the population likelihood landscape in a\nneighborhood of the true parameter vector. Under some conditions on the edge\nparameters, we show that the expected log-likelihood is strongly concave and\nsmooth in a box around the true parameter whose size is independent of both the\ntree topology and number of leaves. The key technical contribution is a careful\nanalysis of the expected Hessian, showing that its diagonal entries are large\nwhile its off-diagonal entries decay exponentially in the graph distance\nbetween the corresponding edges. These results provide the first rigorous\ntheoretical evidence for the effectiveness of optimization methods in this\nsetting and may suggest broader principles for understanding optimization in\nlatent variable models on trees.",
      "generated_abstract": "We present a theoretical analysis of the performance of the Dempster-Shafer\ntheory (DST) for the inference of the likelihood ratio test. This approach\nprovides a rigorous framework for establishing the consistency and asymptotic\nnormality of DST-based methods. In particular, we establish the consistency\nof the DST likelihood ratio test under weak regularity conditions. We then\nintroduce a novel approximation procedure that is shown to lead to asymptotically\nnormal estimates. This procedure is applied to the case of a binary classification\nproblem and we show that the resulting estimator is asymptotically normal,\nproviding a rigorous justification for the use of DST in such scenarios. We\nalso demonstrate the effectiveness of the proposed approach through numerical\nexperiments.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1796875,
          "p": 0.3194444444444444,
          "f": 0.22999999539200008
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.10476190476190476,
          "f": 0.07534246114772969
        },
        "rouge-l": {
          "r": 0.171875,
          "p": 0.3055555555555556,
          "f": 0.21999999539200013
        }
      }
    },
    {
      "paper_id": "math.HO.math/HO/2502.11145v1",
      "true_abstract": "The studies of Bonaventura Cavalieri's indivisibles by Giusti, Andersen,\nMancosu and others provide a comprehensive picture of Cavalieri's mathematics,\nas well as of the mathematical objections to it as formulated by Paul Guldin\nand other critics. An issue that has been studied in less detail concerns the\ntheological underpinnings of the contemporary debate over indivisibles, its\nhistorical roots, the geopolitical situation at the time, and its relation to\nthe ultimate suppression of Cavalieri's religious order. We analyze sources\nfrom the 17th through 21st centuries to investigate such a relation.",
      "generated_abstract": "In this paper we study the geometry of the set of isoperimetric curves in the\nplane. We give a complete classification of such curves for any $n\\geq 3$, and\nprove that every such curve is either a line or a circle. For $n=2$ we obtain a\ncomplete classification of all curves in the plane, and for $n=3$ we give a\ncomplete classification of all curves in the plane and a complete description\nof all curves in the plane up to isoperimetric ratio $1$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13432835820895522,
          "p": 0.20454545454545456,
          "f": 0.16216215737683645
        },
        "rouge-2": {
          "r": 0.011627906976744186,
          "p": 0.016129032258064516,
          "f": 0.013513508644998099
        },
        "rouge-l": {
          "r": 0.11940298507462686,
          "p": 0.18181818181818182,
          "f": 0.14414413935881845
        }
      }
    },
    {
      "paper_id": "cs.HC.stat/CO/2502.08114v2",
      "true_abstract": "The rapid proliferation of data science forced different groups of\nindividuals with different backgrounds to adapt to statistical analysis. We\nhypothesize that conversational agents are better suited for statistical\nanalysis than traditional graphical user interfaces (GUI). In this work, we\npropose a novel conversational agent, StatZ, for statistical analysis. We\nevaluate the efficacy of StatZ relative to established statistical\nsoftware:SPSS, SAS, Stata, and JMP in terms of accuracy, task completion time,\nuser experience, and user satisfaction. We combined the proposed analysis\nquestion from state-of-the-art language models with suggestions from\nstatistical analysis experts and tested with 51 participants from diverse\nbackgrounds. Our experimental design assessed each participant's ability to\nperform statistical analysis tasks using traditional statistical analysis tools\nwith GUI and our conversational agent. Results indicate that the proposed\nconversational agents significantly outperform GUI statistical software in all\nassessed metrics, including quantitative (task completion time, accuracy, and\nuser experience), and qualitative (user satisfaction) metrics. Our findings\nunderscore the potential of using conversational agents to enhance statistical\nanalysis processes, reducing cognitive load and learning curves and thereby\nproliferating data analysis capabilities, to individuals with limited knowledge\nof statistics.",
      "generated_abstract": "aper, we propose a new algorithm to obtain the maximum likelihood\neigenvalues (MLEs) of the covariance matrix of a Gaussian random variable with\na given mean and covariance matrix. The proposed algorithm is based on the\nreduced-rank regression technique and uses the MLEs of the columns of a given\nrank-$r$ matrix to estimate the MLEs of the columns of the covariance matrix.\nThe proposed algorithm is shown to achieve asymptotic normality in the\ndimension $d$ of the covariance matrix and a $d^r$-th power complexity in the\ndimension $d$ of the covariance matrix, where $r$ is the rank of the\ncovariance matrix. Moreover, we show that the proposed algorithm is optimal\nwith respect to the asymptotic normality and the complexity in the dimension\n$d$ of the covariance matrix. Finally, we apply",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10256410256410256,
          "p": 0.21052631578947367,
          "f": 0.1379310300772891
        },
        "rouge-2": {
          "r": 0.023391812865497075,
          "p": 0.044444444444444446,
          "f": 0.030651336477738805
        },
        "rouge-l": {
          "r": 0.10256410256410256,
          "p": 0.21052631578947367,
          "f": 0.1379310300772891
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.physics/bio-ph/2503.08818v1",
      "true_abstract": "Molecular motors are in charge of almost every process in the life cycle of\ncells, such as protein synthesis, DNA replication, and cell locomotion, hence\nbeing of crucial importance for understanding the cellular dynamics. However,\ngiven their size scales on the order of nanometers, direct measurements are\nrather challenging, and the information that can be extracted from them is\nlimited. In this work, we propose strategies based on martingale theory in\nstochastic thermodynamics to infer thermodynamic properties of molecular motors\nusing a limited amount of available information. In particular, we use two\nrecent theoretical results valid for systems arbitrary far of equilibrium: the\nintegral fluctuation theorem (IFT) at stopping times, and a family of bounds to\nthe maximal excursions of entropy production. The potential of these strategies\nis illustrated with a simple model for the F1-ATPase rotary molecular motor,\nwhere our approach is able to estimate several quantities determining the\nthermodynamics of the motor, such as the rotational work of the motor performed\nagainst an externally applied force, or the effective environmental\ntemperature.",
      "generated_abstract": "pt of non-equilibrium phase transitions (NEPTs) has gained wide\napplicability in a wide range of problems. It has been shown that NEPTs are\ngenerally of second-order, and that NEPTs are generally characterized by a\ncomplex set of critical exponents. While these properties have been studied in\nmany different systems, the complexity of these critical exponents makes it\ndifficult to construct a general theory of NEPTs. Here, we present a\ntheoretical framework to study NEPTs that is based on the concept of\ntopological phase transitions. This approach provides a unified framework that\nexplains the behavior of NEPTs in both equilibrium and nonequilibrium systems,\nand is applicable to both thermodynamic and nonequilibrium systems. We show\nthat, in equilibrium systems, NEPTs can be described by a single critical\nexponent, and that in none",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.2692307692307692,
          "f": 0.20588234821799314
        },
        "rouge-2": {
          "r": 0.023529411764705882,
          "p": 0.03333333333333333,
          "f": 0.027586202045185155
        },
        "rouge-l": {
          "r": 0.1349206349206349,
          "p": 0.21794871794871795,
          "f": 0.16666666194348342
        }
      }
    },
    {
      "paper_id": "cs.CC.cs/CC/2503.07285v1",
      "true_abstract": "The complexity of solving equations over finite groups has been an active\narea of research over the last two decades, starting with Goldmann and Russell,\n\\emph{The complexity of solving equations over finite groups} from 1999. One\nimportant case of a group with unknown complexity is the symmetric group $S_4.$\nIn 2023, Idziak, Kawa{\\l}ek, and Krzaczkowski published $\\exp(\\Omega(\\log^2\nn))$ lower bounds for the satisfiability and equivalence problems over $S_4$\nunder the Exponential Time Hypothesis. In the present note, we prove that the\nsatisfiability problem $\\textsc{PolSat}(S_4)$ can be reduced to the equivalence\nproblem $\\textsc{PolEqv}(S_4)$ and thus, the two problems have the same\ncomplexity. We provide several equivalent formulations of the problem. In\nparticular, we prove that $\\textsc{PolEqv}(S_4)$ is equivalent to the circuit\nequivalence problem for $\\operatorname{CC}[2,3,2]$-circuits, which were\nintroduced by Idziak, Kawe{\\l}ek and Krzaczkowski. Under their strong\nexponential size hypothesis, such circuits cannot compute\n$\\operatorname{AND}_n$ in size $\\exp(o(\\sqrt{n})).$ Our results provide an\nupper bound on the complexity of $\\textsc{PolEqv}(S_4)$ that is based on the\nminimal size of $\\operatorname{AND}_n$ over\n$\\operatorname{CC}[2,3,2]$-circuits.",
      "generated_abstract": "velopment of big data has become more prevalent in our daily lives,\nit has also become crucial for the security of data to be protected. There\nare two main ways to protect data: encryption and hashing. Encryption is a\nprocess of encoding data into a form that is difficult to decipher. Hashing is\na process of transforming data into a form that is easy to compare with other\ndata. When it comes to encrypting data, there are many choices to consider,\nsuch as using a symmetric key or a public key. When it comes to hashing data,\nthere are many choices to consider, such as using a hashing function or\ndigest function. There are also many choices to consider when encrypting and\nhashing data. In this paper, we will discuss the encryption and hashing process\nof data and their pros and cons. We will also discuss the various types of\nhash functions and how they can",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19047619047619047,
          "p": 0.24096385542168675,
          "f": 0.21276595251527852
        },
        "rouge-2": {
          "r": 0.01282051282051282,
          "p": 0.016,
          "f": 0.014234870505694396
        },
        "rouge-l": {
          "r": 0.17142857142857143,
          "p": 0.21686746987951808,
          "f": 0.19148935677059772
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.14731v1",
      "true_abstract": "Motor skill acquisition in fields like surgery, robotics, and sports involves\nlearning complex task sequences through extensive training. Traditional\nperformance metrics, like execution time and error rates, offer limited insight\nas they fail to capture the neural mechanisms underlying skill learning and\nretention. This study introduces directed functional connectivity (dFC),\nderived from electroencephalography (EEG), as a novel brain-based biomarker for\nassessing motor skill learning and retention. For the first time, dFC is\napplied as a biomarker to map the stages of the Fitts and Posner motor learning\nmodel, offering new insights into the neural mechanisms underlying skill\nacquisition and retention. Unlike traditional measures, it captures both the\nstrength and direction of neural information flow, providing a comprehensive\nunderstanding of neural adaptations across different learning stages. The\nanalysis demonstrates that dFC can effectively identify and track the\nprogression through various stages of the Fitts and Posner model. Furthermore,\nits stability over a six-week washout period highlights its utility in\nmonitoring long-term retention. No significant changes in dFC were observed in\na control group, confirming that the observed neural adaptations were specific\nto training and not due to external factors. By offering a granular view of the\nlearning process at the group and individual levels, dFC facilitates the\ndevelopment of personalized, targeted training protocols aimed at enhancing\noutcomes in fields where precision and long-term retention are critical, such\nas surgical education. These findings underscore the value of dFC as a robust\nbiomarker that complements traditional performance metrics, providing a deeper\nunderstanding of motor skill learning and retention.",
      "generated_abstract": "aper, we propose a novel approach to learning and evaluating\nmodels for the task of protein sequence prediction. Our approach is based on\nthe combination of the autoregressive language model and the transformer, a\ndeep learning architecture that has proven to be effective in a variety of\ntasks, including sequence prediction. In our experiments, we compare our\nmodel with various baselines, including the autoregressive language model, the\ntransformer, and a model that combines both. We find that our model performs\nsignificantly better than the baseline models, especially in the case of the\ntransformer model, which is an architecture that has been gaining traction in\nthe field of language modeling. In addition, we evaluate the model's ability to\ngeneralize to unseen sequences, and we find that it performs well even when\ntask-specific constraints are imposed. Our findings suggest that the combination\nof the autoreg",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12418300653594772,
          "p": 0.2261904761904762,
          "f": 0.16033754816642645
        },
        "rouge-2": {
          "r": 0.022123893805309734,
          "p": 0.03875968992248062,
          "f": 0.02816900945780675
        },
        "rouge-l": {
          "r": 0.10457516339869281,
          "p": 0.19047619047619047,
          "f": 0.13502109247022395
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/NI/2503.08262v1",
      "true_abstract": "The search for the optimal pair of active and protection paths in a network\nwith Shared Risk Link Groups (SRLG) is a challenging but high-value problem in\nthe industry that is inevitable in ensuring reliable connections on the modern\nInternet. We propose a new approach to solving this problem, with a novel use\nof statistical analysis of the distribution of paths with respect to their\ncost, which is an integral part of our innovation. The key idea in our\nalgorithm is to employ iterative updates of cost bounds, allowing efficient\npruning of suboptimal paths. This idea drives an efficacious exploration of the\nsearch space. We benchmark our algorithms against the state-of-the-art\nalgorithms that exploit the alternative strategy of conflicting links\nexclusion, showing that our approach has the advantage of finding more feasible\nconnections within a set time limit.",
      "generated_abstract": "In this paper, we propose a novel approach to solve the Traveling Salesman\nproblem (TSP) with a focus on the challenges of dynamic routing and\nreconfiguration. The proposed approach utilizes a hierarchical approach to\nsolve the TSP with dynamic routing and reconfiguration. The main contributions\nof the proposed approach include: (1) a hierarchical approach to solve the TSP\nwith dynamic routing and reconfiguration, (2) a novel dynamic routing\nstrategy that uses a dynamic heuristic search to solve the TSP with dynamic\nrouting and reconfiguration efficiently, and (3) a reconfiguration mechanism\nthat allows the agent to reconfigure its route and replan its route when the\ncurrent route is not optimal. The experiments conducted on two real-world\ndatasets demonstrate that the proposed approach significantly outperforms the\nexisting approaches in terms of both route planning and reconfiguration\nefficiency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20430107526881722,
          "p": 0.2753623188405797,
          "f": 0.23456789634430736
        },
        "rouge-2": {
          "r": 0.04411764705882353,
          "p": 0.058823529411764705,
          "f": 0.050420163169268194
        },
        "rouge-l": {
          "r": 0.16129032258064516,
          "p": 0.21739130434782608,
          "f": 0.18518518029492467
        }
      }
    },
    {
      "paper_id": "cs.DC.cs/DC/2503.10292v1",
      "true_abstract": "Synchronous consensus protocols offer a significant advantage over their\nasynchronous and partially synchronous counterparts by providing higher fault\ntolerance -- an essential benefit in distributed systems, like blockchains,\nwhere participants may have incentives to act maliciously. However, despite\nthis advantage, synchronous protocols are often met with skepticism due to\nconcerns about their performance, as the latency of synchronous protocols is\ntightly linked to a conservative time bound for message delivery.\n  This paper introduces AlterBFT, a new Byzantine fault-tolerant consensus\nprotocol. The key idea behind AlterBFT lies in the new model we propose, called\nhybrid synchronous system model. The new model is inspired by empirical\nobservations about network behavior in the public cloud environment and\ncombines elements from the synchronous and partially synchronous models.\nNamely, it distinguishes between small messages that respect time bounds and\nlarge messages that may violate bounds but are eventually timely. Leveraging\nthis observation, AlterBFT achieves up to 15$\\times$ lower latency than\nstate-of-the-art synchronous protocols while maintaining similar throughput and\nthe same fault tolerance. Compared to partially synchronous protocols, AlterBFT\nprovides higher fault tolerance, higher throughput, and comparable latency.",
      "generated_abstract": "r presents a novel approach to identify and detect anomalous\nactivity in large-scale cyber-physical systems (CPS) using deep learning\ntechniques. We propose a multi-task deep learning model for the detection of\nanomalous activity in the system. The proposed approach combines two types of\ndata: historical data from sensors and real-time data from the CPS. The\nhistorical data is used for feature extraction and anomaly detection, while the\nreal-time data is used to train the model. We evaluated the model's performance\non two different CPS: a hybrid electric vehicle (HEV) and a battery electric\nvehicle (BEV). The results show that the model's performance is significantly\nimproved by using the real-time data in the training phase. Moreover, the\nmodel is able to detect anomalous activity, even in cases where the system is\nundergoing normal",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14173228346456693,
          "p": 0.23376623376623376,
          "f": 0.1764705835356595
        },
        "rouge-2": {
          "r": 0.017543859649122806,
          "p": 0.02631578947368421,
          "f": 0.02105262677894846
        },
        "rouge-l": {
          "r": 0.13385826771653545,
          "p": 0.22077922077922077,
          "f": 0.16666666196703206
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ph/2503.10366v1",
      "true_abstract": "It is a common lore that in the thermal leptogenesis in the type-1 seesaw\nscenario with the conventional hierarchy of heavy right-handed neutrinos\n(RHNs), the CP violating, out-of-equilibrium decay of the lightest RHN ($N_1$)\nis the only relevant source of $B-L$ asymmetry. Any asymmetry produced by the\nheavier RHNs ($N_2$ and $N_3$) gets washed out by the lepton number violating\nprocesses mediated by $N_1$. In this paper, we examine this assumption\ncomprehensively, considering decay and inverse decay processes as well as the\ninclusion of scatterings. We find that the above said assumption is true only\nif all the RHNs ($N_1, N_2$ and $N_3$) are in strong washout regime. However,\nwe saw that, to satisfy the neutrino masses and mixing given by the low energy\nneutrino oscillation data, at most one of the RHNs can be in the weak washout\nregime. This leads to, if $N_1$ is in the weak washout regime, then the washout\nparameters of $N_2$ and $N_3$ can be chosen in such a way that the impact of\n$N_2$ and $N_3$ on the final $B-L$ asymmetry is relatively small. On the other\nhand, if $N_2$ or $N_3$ is in weak washout regime, then the asymmetry produced\nby them can affect the final $B-L$ asymmetry even if $N_1$ is in the strong\nwashout regime, which we call the memory effect. We delineated the parameter\nspace where the memory effect is significant.",
      "generated_abstract": "We present a novel and simple technique to estimate the mass of a hypothetical\nmassive particle (a ghost) by measuring the mass of a non-ghost boson. The\ntechnique is based on the observation that the ghost can be identified with a\nmassive particle by measuring the mass of a non-ghost boson, which is\nmeasured in a measurement of the ghost. We show that this method can be used to\ndetermine the mass of the ghost, and we discuss the implications of this\ntechnique for the Standard Model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13385826771653545,
          "p": 0.3469387755102041,
          "f": 0.19318181416386887
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.056338028169014086,
          "f": 0.02996254291237128
        },
        "rouge-l": {
          "r": 0.11811023622047244,
          "p": 0.30612244897959184,
          "f": 0.17045454143659616
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/CV/2503.10632v1",
      "true_abstract": "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of\nlearnable activation functions with the potential to capture more complex\nrelationships from data. Although KANs are useful in finding symbolic\nrepresentations and continual learning of one-dimensional functions, their\neffectiveness in diverse machine learning (ML) tasks, such as vision, remains\nquestionable. Presently, KANs are deployed by replacing multilayer perceptrons\n(MLPs) in deep network architectures, including advanced architectures such as\nvision Transformers (ViTs). In this paper, we are the first to design a general\nlearnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate\non any choice of basis. However, the computing and memory costs of training\nthem motivated us to propose a more modular version, and we designed particular\nlearnable attention, called Fourier-KArAt. Fourier-KArAt and its variants\neither outperform their ViT counterparts or show comparable performance on\nCIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures'\nperformance and generalization capacity by analyzing their loss landscapes,\nweight distributions, optimizer path, attention visualization, and spectral\nbehavior, and contrast them with vanilla ViTs. The goal of this paper is not to\nproduce parameter- and compute-efficient attention, but to encourage the\ncommunity to explore KANs in conjunction with more advanced architectures that\nrequire a careful understanding of learnable activations. Our open-source code\nand implementation details are available on: https://subhajitmaity.me/KArAt",
      "generated_abstract": "aper, we propose a novel framework named Deep-Q-Pose that leverages\ndeep learning to generate posed-image samples from 3D point clouds. This\napproach introduces a novel Q-Pose module that generates pose-aware sample\ninstances, enhancing the realism of generated posed images. The Q-Pose module\nuses a Q-function to map 3D points to pose-aware sample instances,\noptimizing the pose distribution and the pose consistency of sample instances.\nWe evaluate our method on the DP-3T dataset, demonstrating its ability to\ngenerate high-quality posed samples in diverse poses, while maintaining\nrealistic appearance and texture details. Our experiments show that our\nframework outperforms the state-of-the-art methods in pose quality, realism,\nand diversity. We also demonstrate that our approach can be applied to other\n3D pose",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13636363636363635,
          "p": 0.25609756097560976,
          "f": 0.17796609716029888
        },
        "rouge-2": {
          "r": 0.009615384615384616,
          "p": 0.018018018018018018,
          "f": 0.012539180415288441
        },
        "rouge-l": {
          "r": 0.12987012987012986,
          "p": 0.24390243902439024,
          "f": 0.16949152088911243
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2502.15800v1",
      "true_abstract": "This paper explores how Large Language Models (LLMs) behave in a classic\nexperimental finance paradigm widely known for eliciting bubbles and crashes in\nhuman participants. We adapt an established trading design, where traders buy\nand sell a risky asset with a known fundamental value, and introduce several\nLLM-based agents, both in single-model markets (all traders are instances of\nthe same LLM) and in mixed-model \"battle royale\" settings (multiple LLMs\ncompeting in the same market). Our findings reveal that LLMs generally exhibit\na \"textbook-rational\" approach, pricing the asset near its fundamental value,\nand show only a muted tendency toward bubble formation. Further analyses\nindicate that LLM-based agents display less trading strategy variance in\ncontrast to humans. Taken together, these results highlight the risk of relying\non LLM-only data to replicate human-driven market phenomena, as key behavioral\nfeatures, such as large emergent bubbles, were not robustly reproduced. While\nLLMs clearly possess the capacity for strategic decision-making, their relative\nconsistency and rationality suggest that they do not accurately mimic human\nmarket dynamics.",
      "generated_abstract": "r addresses the challenges of financial risk management with\na novel framework combining the principles of game theory with the framework of\nstochastic control. The proposed framework is designed to address the complexity\nof financial risk management and the limitations of existing approaches. The\nproposed framework integrates game theory with stochastic control to address\nthe challenges of financial risk management. The framework is designed to\naddress the complexity of financial risk management and the limitations of\nexisting approaches. The framework is based on the concept of cooperative game,\nwhere a firm and a regulatory authority collaborate to mitigate the risk of\nfinancial crises. The framework combines game theory and stochastic control\nto address the challenges of financial risk management. The framework is\ndesigned to address the complexity of financial risk management and the\nlimitations of existing approaches. The framework is based on the concept of\ncooperative game, where a firm and a regulatory",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06818181818181818,
          "p": 0.20930232558139536,
          "f": 0.1028571391503675
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.041666666666666664,
          "f": 0.025316451466111903
        },
        "rouge-l": {
          "r": 0.06060606060606061,
          "p": 0.18604651162790697,
          "f": 0.09142856772179607
        }
      }
    },
    {
      "paper_id": "cs.CL.cs/AI/2503.10533v1",
      "true_abstract": "High-quality test items are essential for educational assessments,\nparticularly within Item Response Theory (IRT). Traditional validation methods\nrely on resource-intensive pilot testing to estimate item difficulty and\ndiscrimination. More recently, Item-Writing Flaw (IWF) rubrics emerged as a\ndomain-general approach for evaluating test items based on textual features.\nHowever, their relationship to IRT parameters remains underexplored. To address\nthis gap, we conducted a study involving over 7,000 multiple-choice questions\nacross various STEM subjects (e.g., math and biology). Using an automated\napproach, we annotated each question with a 19-criteria IWF rubric and studied\nrelationships to data-driven IRT parameters. Our analysis revealed\nstatistically significant links between the number of IWFs and IRT difficulty\nand discrimination parameters, particularly in life and physical science\ndomains. We further observed how specific IWF criteria can impact item quality\nmore and less severely (e.g., negative wording vs. implausible distractors).\nOverall, while IWFs are useful for predicting IRT parameters--particularly for\nscreening low-difficulty MCQs--they cannot replace traditional data-driven\nvalidation methods. Our findings highlight the need for further research on\ndomain-general evaluation rubrics and algorithms that understand\ndomain-specific content for robust item validation.",
      "generated_abstract": "nt state of the art in natural language generation (NLG) is\noften based on a combination of large language models (LLMs) and text-to-text\ntransformer (T2T) models. This approach offers great potential for\naccelerating NLG production, but it comes with significant limitations.\nFirstly, T2T models often struggle with complex, multi-turn tasks, leading to\nsuboptimal performance. Secondly, LLMs are often trained on a single domain\n(e.g., news articles), which limits their ability to generalize to new\ndomains. To address these challenges, we propose the T2T+LLM approach, which\ncombines T2T with large language models (LLMs). This approach enhances the\ncapabilities of LLMs, enabling them to better handle complex tasks and\ngeneralize to new domains. By leveraging the strengths of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15602836879432624,
          "p": 0.25287356321839083,
          "f": 0.1929824514208219
        },
        "rouge-2": {
          "r": 0.02247191011235955,
          "p": 0.03669724770642202,
          "f": 0.027874559748935466
        },
        "rouge-l": {
          "r": 0.14893617021276595,
          "p": 0.2413793103448276,
          "f": 0.1842105215962605
        }
      }
    },
    {
      "paper_id": "econ.TH.q-fin/PM/2410.18432v1",
      "true_abstract": "This paper develops a dynamic model to analyze the general equilibrium of the\ninsurance market, focusing on the interaction between insurers' underwriting\nand investment strategies. Three possible equilibrium outcomes are identified:\na positive insurance market, a zero insurance market, and market failure. Our\nfindings reveal why insurers may rationally accept underwriting losses by\nsetting a negative safety loading while relying on investment profits,\nparticularly when there is a negative correlation between insurance gains and\nfinancial returns. Additionally, we explore the impact of regulatory frictions,\nshowing that while imposing a cost on investment can enhance social welfare\nunder certain conditions, it may not always be necessary. Therefore, we\nemphasize the importance of tailoring regulatory interventions to specific\nmarket conditions.",
      "generated_abstract": "r introduces the notion of a \\emph{non-negative integral partition}\nof a finite set $\\mathcal{A}$ of non-negative integers, and establishes the\ninclusion-exclusion principle for these partitions. We demonstrate the\nimportance of this concept through the study of the \\emph{partitions of the\nsums of the first $n$ natural numbers}, which we show are equal to the\n\\emph{non-negative integral partitions} of $\\mathbb{Z}_n$. We also show that\nthese partitions can be used to prove a non-trivial result about the\n\\emph{sameness} of sums of the first $n$ natural numbers. This work is\nmotivated by the study of the \\emph{permutation sums}, which are defined as\nthe sums of the first $n$ natural numbers, and we show that the \\emph{partitions\nof the sums of the first $",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16279069767441862,
          "p": 0.21875,
          "f": 0.18666666177422236
        },
        "rouge-2": {
          "r": 0.02654867256637168,
          "p": 0.03296703296703297,
          "f": 0.029411759764033894
        },
        "rouge-l": {
          "r": 0.1511627906976744,
          "p": 0.203125,
          "f": 0.17333332844088903
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.cond-mat/other/2503.05400v1",
      "true_abstract": "We investigate site-controlled In$_{0.25}$Ga$_{0.75}$As quantum dots in\n(111)B GaAs pyramidal recesses as spin qubits. Combining scanning confocal\ncryomicroscopy, magneto-photoluminescence studies and resonant excitation, we\nidentify and isolate a positively charged exciton with a hole-spin in its\nground state. Application of a strong 5 T magnetic field parallel to the growth\naxis, induces a fourfold splitting of the energy levels of the positively\ncharged exciton creating an optically addressable double-lambda system. We\ncombine weak above-band and resonant excitation to demonstrate spin pumping and\nhigh-fidelity spin initialization through all four optical transitions and\nstudy the system behavior as a function of the resonant driving strength\nshowing the existence of a robust spin that can be optically pumped and\ninitialized. These results demonstrate the potential of these quantum dots for\nprecise spin manipulation and their relevance for future quantum hardware.",
      "generated_abstract": "igate the influence of the magnetic field on the transport properties of\nsublattices of a two-dimensional (2D) electron gas in a magnetic field. The\nmagnetic field is applied along the $y$-direction, and the sublattice is\nrepresented by a stack of 1D (1D) quantum wires. We employ a dynamical mean\nfield theory (DMFT) approach to treat the electron-electron interactions,\nwhich is valid in the strong coupling regime. We find that the density of\nstates (DOS) exhibits a broad peak at the Fermi level, and the spin polarization\nof the sublattice is affected by the magnetic field. In particular, the\nmagnetic field induces a shift in the Fermi level, resulting in a shift of the\npeak position and a broadening of the peak width. Furthermore, we find that the\nmagnetic field",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15463917525773196,
          "p": 0.21428571428571427,
          "f": 0.17964071369357107
        },
        "rouge-2": {
          "r": 0.030534351145038167,
          "p": 0.03773584905660377,
          "f": 0.033755269317239774
        },
        "rouge-l": {
          "r": 0.14432989690721648,
          "p": 0.2,
          "f": 0.16766466578937947
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2502.20749v1",
      "true_abstract": "Deep learning-based medical image segmentation typically requires large\namount of labeled data for training, making it less applicable in clinical\nsettings due to high annotation cost. Semi-supervised learning (SSL) has\nemerged as an appealing strategy due to its less dependence on acquiring\nabundant annotations from experts compared to fully supervised methods. Beyond\nexisting model-centric advancements of SSL by designing novel regularization\nstrategies, we anticipate a paradigmatic shift due to the emergence of\npromptable segmentation foundation models with universal segmentation\ncapabilities using positional prompts represented by Segment Anything Model\n(SAM). In this paper, we present SemiSAM+, a foundation model-driven SSL\nframework to efficiently learn from limited labeled data for medical image\nsegmentation. SemiSAM+ consists of one or multiple promptable foundation models\nas generalist models, and a trainable task-specific segmentation model as\nspecialist model. For a given new segmentation task, the training is based on\nthe specialist-generalist collaborative learning procedure, where the trainable\nspecialist model delivers positional prompts to interact with the frozen\ngeneralist models to acquire pseudo-labels, and then the generalist model\noutput provides the specialist model with informative and efficient supervision\nwhich benefits the automatic segmentation and prompt generation in turn.\nExtensive experiments on two public datasets and one in-house clinical dataset\ndemonstrate that SemiSAM+ achieves significant performance improvement,\nespecially under extremely limited annotation scenarios, and shows strong\nefficiency as a plug-and-play strategy that can be easily adapted to different\nspecialist and generalist models.",
      "generated_abstract": "development of digital technologies has enabled the development of\na new generation of digital medical images. These digital images are\ncomposed of multiple medical images, each with a different acquisition\ntechnology, which may lead to inconsistencies in the data acquisition\nprotocols, as well as in the image processing techniques. To address these\nissues, we propose a novel method for the creation of multi-institutional\nmedical images in a fully automatic manner. Our methodology involves\nidentifying the necessary acquisition protocols, based on the clinical\ninstitutions involved, and creating a data set comprising multiple images\nacquired from multiple institutions. To validate the methodology, we have\nperformed an experiment on 100 images from the TIMI study, which includes\nclinical and functional data from 22 patients. The methodology was validated\nthrough comparison with an existing method, and we also compared the performance",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19480519480519481,
          "p": 0.3409090909090909,
          "f": 0.24793387966942154
        },
        "rouge-2": {
          "r": 0.009009009009009009,
          "p": 0.015267175572519083,
          "f": 0.011331440091488091
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.3181818181818182,
          "f": 0.23140495404958686
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2412.00658v1",
      "true_abstract": "A new modular approximate Bayesian inferential framework is proposed that\nenables fast calculation of probabilistic predictions of future option prices.\nWe exploit multiple information sources, including daily spot returns,\nhigh-frequency spot data and option prices. A benefit of this modular Bayesian\napproach is that it allows us to work with the theoretical option pricing\nmodel, without needing to specify an arbitrary statistical model that links the\ntheoretical prices to their observed counterparts. We show that our approach\nproduces accurate probabilistic predictions of option prices in realistic\nscenarios and, despite not explicitly modelling pricing errors, the method is\nshown to be robust to their presence. Predictive accuracy based on the Heston\nstochastic volatility model, with predictions produced via rapid real-time\nupdates, is illustrated empirically for short-maturity options.",
      "generated_abstract": "r presents a novel methodology to predict the impact of a\ntransfer of ownership in a real estate portfolio. The proposed methodology\nintroduces an improved version of the conventional model that combines both the\ndynamic and the static characteristics of the market. The proposed model\nenables the assessment of the market trends, as well as the estimation of the\nimpact of the transfer of ownership on the real estate portfolio. Additionally,\nthe proposed model highlights the correlation between the market trends and the\ntransfer of ownership. The results of the proposed model demonstrate that the\ndynamic characteristics of the market significantly affect the transfer of\nownership. The proposed model also demonstrates that the market dynamics have\na significant impact on the transfer of ownership. Moreover, the proposed model\ndemonstrates that the market dynamics significantly affect the transfer of\nownership. The results of the proposed model suggest that the market dynamics\nhave a significant impact on",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12222222222222222,
          "p": 0.2,
          "f": 0.15172413322235448
        },
        "rouge-2": {
          "r": 0.01680672268907563,
          "p": 0.021739130434782608,
          "f": 0.018957341053436733
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.18181818181818182,
          "f": 0.13793102977407862
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2412.00471v1",
      "true_abstract": "Building a general-purpose task model similar to ChatGPT has been an\nimportant research direction for gene large language models. Instruction\nfine-tuning is a key component in building ChatGPT, but existing instructions\nare primarily based on natural language. Natural language and gene sequences\nhave significant differences in tokenization and encoding. Therefore,\nconstructing a multilingual model that can handle both natural language and\ngene sequences is crucial for solving this problem.In this paper, we expand the\ncapabilities of the LLaMA large language model to include gene language. This\ninvolves expanding the vocabulary using the Byte Pair Encoding (BPE) method,\nspecifically tailored for DNA and protein sequences, and conducting further\npre-training on these sequences. We then convert various downstream gene task\ndata into a unified format for instruction fine-tuning and further fine-tune\nthe model on this data.Our study demonstrates that a mixed model of gene and\nnatural language, fine-tuned with instructions, achieves results comparable to\nthe current state-of-the-art (SOTA) in tasks such as gene classification and\ngene sequence interaction. This provides a promising direction for building a\nunified large language model for gene tasks.",
      "generated_abstract": "This study investigated the biomarkers for the diagnosis of Parkinson's disease\n(PD), the most common neurodegenerative disorder, through the analysis of\ngenomic, proteomic, and transcriptomic data from peripheral blood samples. The\nresults showed that the gene expression of BDNF (brain-derived neurotrophic\nfactor) and the protein expression of PD-related proteins, such as Parkin and\nTau, were significantly increased in PD patients compared to healthy controls.\nThe findings indicate that BDNF and PD-related proteins are potential\nbiomarkers for the diagnosis of PD. This study provides valuable insights into\nthe pathogenesis of PD and offers new strategies for the early detection and\nprognosis of the disease.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1565217391304348,
          "p": 0.2608695652173913,
          "f": 0.19565216922554357
        },
        "rouge-2": {
          "r": 0.011695906432748537,
          "p": 0.02127659574468085,
          "f": 0.015094335044786005
        },
        "rouge-l": {
          "r": 0.14782608695652175,
          "p": 0.2463768115942029,
          "f": 0.1847826040081523
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.00352v1",
      "true_abstract": "This paper reviews statistical methods for hypothesis testing and clustering\nin network models. We analyze the method by Bickel et al. (2016) for deriving\nthe asymptotic null distribution of the largest eigenvalue, noting its slow\nconvergence and the need for bootstrap corrections. The SCORE method by Jin et\nal. (2015) and the NCV method by Chen et al. (2018) are evaluated for their\nefficacy in clustering within Degree-Corrected Block Models, with NCV facing\nchallenges due to its time-intensive nature. We suggest exploring eigenvector\nentry distributions as a potential efficiency improvement.",
      "generated_abstract": "We study the design of the least squares estimator in the context of\nthe so-called non-convex non-smooth functionals, i.e., when the functionals\nare not convex and not smooth. We provide a novel approach based on a\nnon-convex optimization framework that allows us to define an unbiased estimator\nfor the least squares. We derive the asymptotic distribution of the estimator\nunder a particular class of assumptions on the functionals. We further\ninvestigate the asymptotic behavior of the estimator in the case when the\nfunctionals are strongly convex and smooth. Furthermore, we propose a novel\nprocedure to obtain the asymptotic distribution of the estimator under the\nconverse assumption that the functionals are strongly convex and smooth. We\nprovide a simulation study and numerical examples to demonstrate the performance\nof our estimator.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16176470588235295,
          "p": 0.16923076923076924,
          "f": 0.16541352883713056
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.03,
          "f": 0.03260869068998185
        },
        "rouge-l": {
          "r": 0.16176470588235295,
          "p": 0.16923076923076924,
          "f": 0.16541352883713056
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09525v1",
      "true_abstract": "The complexity of continuous piecewise affine (CPA) functions can be measured\nby the number of pieces $p$ or the number of distinct affine functions $n$. For\nCPA functions on $\\mathbb{R}^d$, this paper shows an upper bound of\n$p=O(n^{d+1})$ and constructs a family of functions achieving a lower bound of\n$p=\\Omega(n^{d+1-\\frac{c}{\\sqrt{\\log_2(n)}}})$.",
      "generated_abstract": "f this article is to present a novel approach to the study of\nquantum systems with a single qubit, namely the concept of quantum gates\nrepresenting unitary operations. We show that the classical analogue of a\nquantum gate, the Hadamard gate, can be understood as a certain type of\nunitary operator. We also present a method for constructing a quantum gate\nrepresenting a unitary operator from a classical gate. Furthermore, we show\nthat the number of classical gates needed to implement a unitary operation is\nexactly the same as the number of quantum gates required to implement the\nunitary operation. This result is proven using the universal set of quantum\ngates, which is the set of all unitary operations. We also show that the\nnumber of classical gates needed to implement a non-unitary operation is\nexactly the same as the number of quantum gates required to implement the\nnon-unitary operation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1891891891891892,
          "p": 0.11290322580645161,
          "f": 0.14141413673298658
        },
        "rouge-2": {
          "r": 0.06521739130434782,
          "p": 0.02830188679245283,
          "f": 0.03947367998961264
        },
        "rouge-l": {
          "r": 0.13513513513513514,
          "p": 0.08064516129032258,
          "f": 0.10101009632894625
        }
      }
    },
    {
      "paper_id": "math.ST.math/ST/2503.07918v1",
      "true_abstract": "COVID-19 has had a large scale negative impact on the health of opioid users\nexacerbating the health of an already vulnerable population. Critical\ninformation on the total impact of COVID-19 on opioid users is unknown due to a\nlack of comprehensive data on COVID-19 cases, inaccurate diagnostic coding, and\nlack of data coverage. To assess the impact of COVID-19 on small-area opioid\nmortality, we developed a Bayesian hierarchical excess opioid mortality\nmodeling approach. We incorporate spatio-temporal autocorrelation structures to\nallow for sharing of information across small areas and time to reduce\nuncertainty in small area estimates. Excess mortality is defined as the\ndifference between observed trends after a crisis and expected trends based on\nobserved historical trends, which captures the total increase in observed\nmortality rates compared to what was expected prior to the crisis. We\nillustrate the application of our approach to assess excess opioid mortality\nrisk estimates for 159 counties in GA. Using our proposed approach will help\ninform interventions in opioid-related public health responses, policies, and\nresource allocation. The application of this work also provides a general\nframework for improving the estimation and mapping of health indicators during\ncrisis periods for the opioid user population.",
      "generated_abstract": "In this paper, we give a new proof of the existence of a compact metric\nspace in which the distance between two points is an increasing function of the\ndistance to the point. We also prove that this is an open problem to find the\nsmallest possible number of points required to achieve the existence of a\ncompact metric space in which the distance between two points is a\n$k$-increasing function of the distance to the point for any $k \\in \\mathbb\nN$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11864406779661017,
          "p": 0.3111111111111111,
          "f": 0.17177913710715503
        },
        "rouge-2": {
          "r": 0.005405405405405406,
          "p": 0.01694915254237288,
          "f": 0.008196717644787988
        },
        "rouge-l": {
          "r": 0.09322033898305085,
          "p": 0.24444444444444444,
          "f": 0.13496932115623483
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.03323v1",
      "true_abstract": "This study as part of an ongoing research effort, empirically examines the\nrelationship between foreign trade in the Istanbul Ataturk Airport Free Zone\nand exchange rate movements. Monthly data from 2003 to 2016 were analyzed\nthrough stationarity tests (Unit Root), followed by the Vector Autoregressive\n(VAR) model, Cointegration Analysis, and the Toda-Yamamoto Causality Test. The\nfindings indicate that the exchange rate does not significantly affect imports\nand exports in the free zone. This result suggests that free zones, due to\ntheir structural characteristics and operational framework, may be relatively\ninsulated from exchange rate fluctuations. The study contributes to the\nliterature by providing a focused analysis of a specific free zone in Turkiye,\nhighlighting the potential independence of free zone trade from exchange rate\nvolatility.",
      "generated_abstract": "r introduces a novel framework for analyzing the effects of\ncompetition in the supply chain of the Chinese economy. We develop a\nmultinomial logit model to examine the impact of competitive conditions on\nfirms' production decisions and output prices. Our results show that firms in\nthe export-oriented industries are more likely to operate in a competitive\nenvironment. Specifically, the export-oriented firms are 1.40 times more likely\nto be competitive. The competitive behavior of Chinese firms is also influenced\nby their production costs, which are significantly lower than their\nexport-oriented competitors. Firms that operate in a competitive environment\nalso tend to increase their production capacity and increase their employment\nlevels. Furthermore, we find that the competitive behavior of Chinese firms is\nstrongly associated with their export-oriented competitors. The competitive\nbehavior of Chinese fir",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13636363636363635,
          "p": 0.15789473684210525,
          "f": 0.1463414584414041
        },
        "rouge-2": {
          "r": 0.017241379310344827,
          "p": 0.018018018018018018,
          "f": 0.017621140376876572
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.15789473684210525,
          "f": 0.1463414584414041
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.10206v1",
      "true_abstract": "A graph $G$ is perfectly divisible if, for every induced subgraph $H$ of $G$,\neither $V(H)$ is a stable set or admits a partition into two sets $X_1$ and\n$X_2$ such that $\\omega(H[X_1]) < \\omega(H)$ and $H[X_2]$ is a perfect graph.\nIn this article, we propose the following generalisation of perfectly divisible\ngraphs. A graph $G$ is perfectly $1$-divisible if $G$ is perfect and perfectly\n$k$-divisible if, for every induced subgraph $H$ of $G$, either $V(H)$ is a\nstable set or admits a partition into two sets $X_1$ and $X_2$ such that\n$\\omega(H[X_1]) < \\omega(H)$ and $H[X_2]$ is perfectly $(k-1)$-divisible, $k\n\\in \\mathbb{N}_{> 1}$. Our main result establishes that every perfectly\n$k$-divisible graph $G$ satisfies $\\chi(G) \\leq \\binom{\\omega(G)+k-1}{k}$ which\ngeneralises the known bound for perfectly divisible graphs.",
      "generated_abstract": "In this paper, we study the class of linear operators $\\mathcal{L}(V)$ on\nHilbert space $V$ which are isometries of $V^{\\otimes n}$ for some $n\\geq\n1$. We prove that the class of $\\mathcal{L}(V)$ on $V^{\\otimes n}$ is\ncontained in the class of $\\mathcal{L}(V)$ on $V^{\\otimes m}$ for some $m\\geq\nn$. In addition, we provide a necessary and sufficient condition for $\\mathcal{L}(V)$ to be\nin the class of $\\mathcal{L}(V)$ on $V^{\\otimes n}$ for some $n\\geq 1$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.171875,
          "p": 0.2619047619047619,
          "f": 0.2075471650267
        },
        "rouge-2": {
          "r": 0.012048192771084338,
          "p": 0.019230769230769232,
          "f": 0.014814810078465164
        },
        "rouge-l": {
          "r": 0.15625,
          "p": 0.23809523809523808,
          "f": 0.18867924049839813
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2412.11019v1",
      "true_abstract": "The domain of hedge fund investments is undergoing significant\ntransformation, influenced by the rapid expansion of data availability and the\nadvancement of analytical technologies. This study explores the enhancement of\nhedge fund investment performance through the integration of machine learning\ntechniques, the application of PolyModel feature selection, and the analysis of\nfund size. We address three critical questions: (1) the effect of machine\nlearning on trading performance, (2) the role of PolyModel feature selection in\nfund selection and performance, and (3) the comparative reliability of larger\nversus smaller funds.\n  Our findings offer compelling insights. We observe that while machine\nlearning techniques enhance cumulative returns, they also increase annual\nvolatility, indicating variability in performance. PolyModel feature selection\nproves to be a robust strategy, with approaches that utilize a comprehensive\nset of features for fund selection outperforming more selective methodologies.\nNotably, Long-Term Stability (LTS) effectively manages portfolio volatility\nwhile delivering favorable returns. Contrary to popular belief, our results\nsuggest that larger funds do not consistently yield better investment outcomes,\nchallenging the assumption of their inherent reliability.\n  This research highlights the transformative impact of data-driven approaches\nin the hedge fund investment arena and provides valuable implications for\ninvestors and asset managers. By leveraging machine learning and PolyModel\nfeature selection, investors can enhance portfolio optimization and reassess\nthe dependability of larger funds, leading to more informed investment\nstrategies.",
      "generated_abstract": "This paper develops a framework for learning financial time series with\nintricate nonlinear dynamics. We introduce a new framework that incorporates\nboth the time-invariant and time-varying components of the time series. We\npropose a novel model that consists of two layers of autoregressive (AR)\nprocesses, followed by an AR process, and a linear process. This model is\napplicable to both stationary and nonstationary time series. We evaluate the\nperformance of our model using simulated data, and then apply it to real-world\ndata from the U.S. equity markets. The results demonstrate that our model\noutperforms existing models in terms of root mean squared prediction error\n(RMSE) and mean absolute prediction error (MAPE). Furthermore, we find that our\nmodel is able to capture the intricate nonlinear dynamics of financial time\nseries.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13194444444444445,
          "p": 0.2261904761904762,
          "f": 0.16666666201292718
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10416666666666667,
          "p": 0.17857142857142858,
          "f": 0.1315789427146816
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2502.07625v1",
      "true_abstract": "An empirical stochastic analysis of high-frequency, tick-by-tick order data\nof NASDAQ100 listed stocks is conducted using a first-order discrete-time\nMarkov chain model to explore intraday order transition dynamics. This analysis\nfocuses on three market cap categories: High, Medium, and Low. Time-homogeneous\ntransition probability matrices are estimated and compared across time-zones\nand market cap categories, and we found that limit orders exhibit higher degree\nof inertia (DoI), i.e., the probability of placing consecutive limit order is\nhigher, during the opening hour. However, in the subsequent hour, the DoI of\nlimit order decreases, while that of market order increases. Limit order\nadjustments via additions and deletions of limit orders increases significantly\nafter the opening hour. All the order transitions then stabilize during\nmid-hours. As the closing hour approaches, consecutive order executions surge,\nwith decreased placement of buy and sell limit orders following sell and buy\nexecutions, respectively. In terms of the differences in order transitions\nbetween stocks of different market cap, DoI of orders is stronger in high and\nmedium market cap stocks. On the other hand, lower market cap stocks show a\nhigher probability of limit order modifications and greater likelihood of\nsubmitting sell/buy limit orders after buy/sell executions. Further, order\ntransitions are clustered across all stocks, except during opening and closing\nhours. The findings of this study may be useful in understanding intraday order\nplacement dynamics across stocks of varying market cap, thus aiding market\nparticipants in making informed order placements at different times of trading\nhour.",
      "generated_abstract": "the optimal allocation of assets in a regime switching\nfinancial market with a liquidity-constrained liquidity provider. The\nliquidity provider can issue new assets or liquidate existing assets, which\ntrigger regime switching. We formulate this problem as a reinforcement learning\nproblem with two players: the liquidity provider, who seeks to maximize her\nutility, and a regulator, who is incentivized to maximize her own utility. We\nshow that the regulator can learn to trade between two regimes, maximizing her\nutility in each regime, and we present a simple Nash equilibrium for the\nliquidity provider. We also show that the regulator can learn to trade between\ntwo regimes, maximizing her utility in each regime, and we present a simple\nNash equilibrium for the liquidity provider. We also demonstrate that the\nliquidity provider's best response to the regulator'",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1048951048951049,
          "p": 0.21739130434782608,
          "f": 0.14150942957146687
        },
        "rouge-2": {
          "r": 0.0043859649122807015,
          "p": 0.010416666666666666,
          "f": 0.006172835336079634
        },
        "rouge-l": {
          "r": 0.1048951048951049,
          "p": 0.21739130434782608,
          "f": 0.14150942957146687
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2503.05340v1",
      "true_abstract": "Matrix-valued time series analysis has gained prominence in econometrics and\nfinance due to the increasing availability of high-dimensional data with\ninherent matrix structures. Traditional approaches, such as Matrix\nAutoregressive (MAR) models and Dynamic Matrix Factor (DMF) models, often\nimpose restrictive assumptions that may not align with real-world data\ncomplexities. To address this gap, we propose a novel Matrix Autoregressive\nwith Common Factors (MARCF) model, which bridges the gap between MAR and DMF\nframeworks by introducing common bases between predictor and response\nsubspaces. The MARCF model achieves significant dimension reduction and enables\na more flexible and interpretable factor representation of dynamic\nrelationships. We develop a computationally efficient estimator and a gradient\ndescent algorithm. Theoretical guarantees for computational and statistical\nconvergence are provided, and extensive simulations demonstrate the robustness\nand accuracy of the model. Applied to a multinational macroeconomic dataset,\nthe MARCF model outperforms existing methods in forecasting and provides\nmeaningful insights into the interplay between countries and economic factors.",
      "generated_abstract": "p a Bayesian framework for modeling the time-varying effects of\ntime-varying treatment effects on outcomes, with a focus on the application to\ntime-to-event data. We model the time-varying treatment effect as a time-varying\nlinear function of the current treatment status, and the time-varying\noutcome as a time-varying linear function of the current treatment status and\nthe time-varying treatment effect. We assume the treatment effect is\ntime-varying across the treatment status domain, and we assume the outcome is\ntime-varying across the outcome status domain. We propose a Bayesian\nsmoothing model for the treatment effect and a Bayesian smoothing model for the\noutcome. We derive the posterior distributions of the treatment effect and\noutcome for a given set of treatment and outcome parameters. We also derive\nthe posterior distributions of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10569105691056911,
          "p": 0.28888888888888886,
          "f": 0.15476190083971098
        },
        "rouge-2": {
          "r": 0.01935483870967742,
          "p": 0.03488372093023256,
          "f": 0.024896260970025466
        },
        "rouge-l": {
          "r": 0.08943089430894309,
          "p": 0.24444444444444444,
          "f": 0.13095237703018717
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/DS/2503.09802v1",
      "true_abstract": "We study the task of list-decodable linear regression using batches. A batch\nis called clean if it consists of i.i.d. samples from an unknown linear\nregression distribution. For a parameter $\\alpha \\in (0, 1/2)$, an unknown\n$\\alpha$-fraction of the batches are clean and no assumptions are made on the\nremaining ones. The goal is to output a small list of vectors at least one of\nwhich is close to the true regressor vector in $\\ell_2$-norm. [DJKS23] gave an\nefficient algorithm, under natural distributional assumptions, with the\nfollowing guarantee. Assuming that the batch size $n$ satisfies $n \\geq\n\\tilde{\\Omega}(\\alpha^{-1})$ and the number of batches is $m = \\mathrm{poly}(d,\nn, 1/\\alpha)$, their algorithm runs in polynomial time and outputs a list of\n$O(1/\\alpha^2)$ vectors at least one of which is\n$\\tilde{O}(\\alpha^{-1/2}/\\sqrt{n})$ close to the target regressor. Here we\ndesign a new polynomial time algorithm with significantly stronger guarantees\nunder the assumption that the low-degree moments of the covariates distribution\nare Sum-of-Squares (SoS) certifiably bounded. Specifically, for any constant\n$\\delta>0$, as long as the batch size is $n \\geq\n\\Omega_{\\delta}(\\alpha^{-\\delta})$ and the degree-$\\Theta(1/\\delta)$ moments of\nthe covariates are SoS certifiably bounded, our algorithm uses $m =\n\\mathrm{poly}((dn)^{1/\\delta}, 1/\\alpha)$ batches, runs in polynomial-time, and\noutputs an $O(1/\\alpha)$-sized list of vectors one of which is\n$O(\\alpha^{-\\delta/2}/\\sqrt{n})$ close to the target. That is, our algorithm\nachieves substantially smaller minimum batch size and final error, while\nachieving the optimal list size. Our approach uses higher-order moment\ninformation by carefully combining the SoS paradigm interleaved with an\niterative method and a novel list pruning procedure. In the process, we give an\nSoS proof of the Marcinkiewicz-Zygmund inequality that may be of broader\napplicability.",
      "generated_abstract": "er the problem of designing a fair and efficient mechanism that\nrespects privacy constraints. We introduce the notion of \\emph{fairness\nconstrained} privacy-utility trade-off (FCPUT), which captures a\npareto-feasible trade-off between privacy and utility. This trade-off is\ninherently non-monotone and cannot be obtained by a monotone mechanism. We\npropose two novel algorithms that solve the FCPUT problem, both of which\nachieve a polylogarithmic approximation guarantee. The first algorithm,\ncalled \\textsc{FCPUT-Efficient}, is a greedy algorithm that runs in $O(n\n\\log^2 n)$ time. The second algorithm, called \\textsc{FCPUT-Polylog}, is a\npolynomial-time algorithm that runs in $O(n \\log n \\log \\log n)$",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12345679012345678,
          "p": 0.30303030303030304,
          "f": 0.17543859237765475
        },
        "rouge-2": {
          "r": 0.008298755186721992,
          "p": 0.02197802197802198,
          "f": 0.012048188791734511
        },
        "rouge-l": {
          "r": 0.10493827160493827,
          "p": 0.25757575757575757,
          "f": 0.14912280290397056
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2410.01864v1",
      "true_abstract": "This paper introduces a novel approach to optimizing portfolio rebalancing by\nintegrating Graph Neural Networks (GNNs) for predicting transaction costs and\nDijkstra's algorithm for identifying cost-efficient rebalancing paths. Using\nhistorical stock data from prominent technology firms, the GNN is trained to\nforecast future transaction costs, which are then applied as edge weights in a\nfinancial asset graph. Dijkstra's algorithm is used to find the least costly\npath for reallocating capital between assets. Empirical results show that this\nhybrid approach significantly reduces transaction costs, offering a powerful\ntool for portfolio managers, especially in high-frequency trading environments.\nThis methodology demonstrates the potential of combining advanced machine\nlearning techniques with classical optimization algorithms to improve financial\ndecision-making processes. Future research will explore expanding the asset\nuniverse and incorporating reinforcement learning for continuous portfolio\noptimization.",
      "generated_abstract": "In this paper, we propose a new approach to investment portfolio selection,\nusing an information-theoretic approach based on the entropy of the portfolio\nreturns. This method allows for a more robust selection of the portfolio\ncomponents and an improved risk analysis of the portfolio. We show that the\ninformation entropy of the portfolio returns can be used as a surrogate for the\nlogarithm of the Sharpe ratio for portfolio selection. We develop a\ncomputational algorithm to obtain the optimal portfolio based on the information\nentropy. We show that the optimal portfolio is a mixture of the assets with\nthe highest information entropy. We compare our approach with other approaches\nsuch as the expected return and the Sharpe ratio, and demonstrate its\nefficiency and robustness.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17647058823529413,
          "p": 0.26865671641791045,
          "f": 0.21301774669374332
        },
        "rouge-2": {
          "r": 0.0234375,
          "p": 0.0297029702970297,
          "f": 0.026200868431953024
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.26865671641791045,
          "f": 0.21301774669374332
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.00308v1",
      "true_abstract": "We propose a solution to the problem of bargaining with transfers, along with\nan axiomatisation of the solution. Inefficient allocations in the bargaining\nset can influence the solution, but are discounted relative to more efficient\nones. The key axioms are additivity and a property we call \\emph{inverse\nmonotonicity}, which states that adding an allocation to the bargaining set\nthat is worse for a given player than the initial solution cannot benefit that\nplayer.",
      "generated_abstract": "r considers the problem of assigning prices to a family of goods,\nin which the goods are sold to the agent. We assume that the goods are\nidentically and independently distributed according to a known distribution, and\nthe agent can acquire information about the distribution only through his\nobservation of the realized price of the goods. We derive a closed-form solution\nfor the optimal price, which we show is the same as the solution to the\nMarkov-Perron problem for the same family of goods. We also present an\nalternative derivation of the optimal price, which is based on a modified\nversion of the Perron-Frobenius theorem, and which yields a closed-form solution\nthat differs from the solution to the Markov-Perron problem. We discuss the\nimplications of our findings for the optimal taxation of sales taxes. Finally,\nwe illustrate the potential usefulness of our findings by applying them",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3269230769230769,
          "p": 0.21794871794871795,
          "f": 0.2615384567384616
        },
        "rouge-2": {
          "r": 0.08695652173913043,
          "p": 0.05084745762711865,
          "f": 0.06417111833795681
        },
        "rouge-l": {
          "r": 0.28846153846153844,
          "p": 0.19230769230769232,
          "f": 0.23076922596923088
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.09345v1",
      "true_abstract": "This report documents the development, test, and application of Large\nLanguage Models (LLMs) for automated text analysis, with a specific focus on\ngambling-like elements in digital games, such as lootboxes. The project aimed\nnot only to analyse user opinions and attitudes towards these mechanics, but\nalso to advance methodological research in text analysis. By employing\nprompting techniques and iterative prompt refinement processes, the study\nsought to test and improve the accuracy of LLM-based text analysis. The\nfindings indicate that while LLMs can effectively identify relevant patterns\nand themes on par with human coders, there are still challenges in handling\nmore complex tasks, underscoring the need for ongoing refinement in\nmethodologies. The methodological advancements achieved through this study\nsignificantly enhance the application of LLMs in real-world text analysis. The\nresearch provides valuable insights into how these models can be better\nutilized to analyze complex, user-generated content.",
      "generated_abstract": "This paper examines the effects of the 2008 financial crisis on the\nestablishment of new firms in the United States. We identify the impact of\nfinancial stress on the timing of firm establishment by leveraging the\ntiming of bankruptcy filings, which are a common indicator of financial stress.\nWe find that financial stress has a positive and significant impact on the\ntiming of firm establishment, particularly during the recessionary period. The\nfindings suggest that financial stress has a significant impact on the pace of\nfirms' growth, and the impact may be larger in smaller cities and states.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11926605504587157,
          "p": 0.24074074074074073,
          "f": 0.1595091980232603
        },
        "rouge-2": {
          "r": 0.007142857142857143,
          "p": 0.01282051282051282,
          "f": 0.009174307331035043
        },
        "rouge-l": {
          "r": 0.11009174311926606,
          "p": 0.2222222222222222,
          "f": 0.14723925937295357
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-th/2503.10595v1",
      "true_abstract": "We present compact two-loop QCD corrections in the leading-color\napproximation for the production of an electroweak vector boson, $V =\n\\{W^{\\pm}, Z,\\gamma^\\star\\}$, in association with two light jets ($Vjj$) at\nhadron colliders. Leptonic decays of the electroweak boson are included at the\namplitude level. Working in the analytic-reconstruction approach, we develop\ntwo techniques to build compact partial-fraction forms for individual rational\nfunctions. One approach exploits their analytic structure. In the other, we\niteratively construct subtraction terms that match the rational functions in\nsingular limits. Moreover, we show how the singular behavior of the rational\nfunctions can be systematically used to find a more compact basis of the space\nthat they span. We apply our techniques to the $Vjj$ amplitudes, yielding a\nrepresentation that is three orders of magnitude smaller than previous results.\nWe then use these compact expressions to provide an efficient and stable C++\nnumerical implementation suitable for phenomenological applications.",
      "generated_abstract": "aper, we present a novel nonperturbative method for computing the\nenergy spectra of heavy particles in the presence of a strong magnetic field,\nin the framework of the Standard Model and of extended versions of the Standard\nModel, including the Higgs sector. Our approach, inspired by the method developed\nin the literature for computing the energy spectra of heavy particles in the\npresence of a magnetic field in the absence of an external field, provides a\nnew framework for studying the evolution of the energy spectra of heavy\nparticles in the Standard Model, as well as in various extended models of\nparticle physics. Our method provides a new tool for investigating the\nevolution of heavy particles in the Standard Model and beyond. It is a\nnonperturbative technique for computing the energy spectra of heavy particles\nin the presence of a magnetic field, and in the framework of extended\ntheories. This approach is particularly useful in the context",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11009174311926606,
          "p": 0.1935483870967742,
          "f": 0.14035087257070566
        },
        "rouge-2": {
          "r": 0.02097902097902098,
          "p": 0.030303030303030304,
          "f": 0.024793383595042264
        },
        "rouge-l": {
          "r": 0.09174311926605505,
          "p": 0.16129032258064516,
          "f": 0.11695905970520865
        }
      }
    },
    {
      "paper_id": "physics.soc-ph.econ/GN/2412.10421v1",
      "true_abstract": "The global food system provides the energy that supports human metabolism,\nwith complex spatial interdependencies between food production, transformation,\nand consumption. Empirical food system data for these global processes are\noften fragmented and inconsistent, with only certain components captured in\nspatially resolved formats. Here we propose a flexible approach to allocating\ncountry-level food system data subnationally. We estimate the spatial patterns\nof food energy production and supply, which we compare to estimates of human\nmetabolism based on average body size. We downscale these rates onto a\none-degree resolution grid with 95 corresponding food types to derive an\ninternally consistent, energy-conserving, and spatially resolved dataset. We\nshow that national food supply varies linearly with metabolism per capita, with\nabout half the variation in food supply explained by metabolic rates. Our data\nprocessing pipeline is openly available and can readily incorporate new inputs\nin order to advance trans-disciplinary food system modeling efforts.",
      "generated_abstract": "-19 pandemic has caused unprecedented disruption to the global\nemerging markets (EM) economy. This paper investigates the impact of COVID-19\non EM countries using a simultaneous equation (SE) model, which accounts for\nboth the effects of the pandemic on the global economy and the impact on the\nEM countries. The results indicate that the pandemic has caused significant\ndeterioration in the economic outlook and growth prospects of EM countries.\nThe COVID-19 pandemic has caused significant disruptions to the global\neconomy, with EM countries facing distinct challenges. The COVID-19 pandemic\nhas also had a significant impact on EM countries' economies, with disruptions\nto supply chains and international trade, and negative impacts on tourism and\ntrade. The impact of COVID-19 on EM countries is likely to persist for some\ntime,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14150943396226415,
          "p": 0.22058823529411764,
          "f": 0.17241378834192114
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14150943396226415,
          "p": 0.22058823529411764,
          "f": 0.17241378834192114
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.09898v1",
      "true_abstract": "Dynamic simulation plays a crucial role in power system transient stability\nanalysis, but traditional numerical integration-based methods are\ntime-consuming due to the small time step sizes. Other semi-analytical solution\nmethods, such as the Differential Transformation method, often struggle to\nselect proper orders and steps, leading to slow performance and numerical\ninstability. To address these challenges, this paper proposes a novel adaptive\ndynamic simulation approach for power system transient\n  stability analysis. The approach adds feedback control and optimization to\nselecting the step and order, utilizing the Differential Transformation method\nand a proportional-integral control strategy to control truncation errors.\nOrder selection is formulated as an optimization problem resulting in a\nvariable-step-optimal-order method that achieves significantly larger time step\nsizes without violating numerical stability. It is applied to three systems:\nthe IEEE 9-bus, 3-generator system, IEEE 39-bus, 10-generator system, and a\nPolish 2383-bus, 327-generator system, promising computational efficiency and\nnumerical robustness for large-scale power system is demonstrated in\ncomprehensive case studies.",
      "generated_abstract": "ration of renewable energy sources (RES) into the power grid is\nsignificantly contributing to decarbonization, but the impact of RES on the\noperational reliability of power systems remains underexplored. This paper\npresents a comprehensive analysis of the operational reliability of power\nsystems with and without RES, focusing on the grid stability and frequency\nstability. To address this challenge, the reliability of the power system is\nassessed using the multi-criteria decision-making (MCDM) approach. A\nmulti-objective optimization model is developed to evaluate the reliability\nperformance of the power system under various operating conditions. The results\nshow that the reliability of the power system under RES is significantly\nimproved compared to the conventional grid. The MCDM model demonstrates that\ndifferent reliability measures, such as frequency deviation and harmonic\ncurrent deviation,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.3157894736842105,
          "f": 0.2608695603686201
        },
        "rouge-2": {
          "r": 0.034013605442176874,
          "p": 0.04716981132075472,
          "f": 0.03952568683091501
        },
        "rouge-l": {
          "r": 0.18518518518518517,
          "p": 0.2631578947368421,
          "f": 0.21739129949905492
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ph/2503.10347v1",
      "true_abstract": "Through the black hole (BH) superradiance, ultralight bosons can form dense\nclouds around rotating Kerr BHs. Certain ultralight bosons, such as axions and\naxion-like particles (promising dark matter candidates), naturally possess\nself-interactions, and thus may significantly modify the dynamics of the\nsuperradiance process. Previous studies on the detection or constraint of\nultralight bosons through superradiance have largely neglected the\nself-interaction effects of bosons. In this work, we investigate the formation\nand evolution of self-interacting boson clouds in the full Kerr spacetime\nduring BH superradiance. Using numerical methods, we compute the superradiant\ngrowth rate of boson clouds with self-interactions around Kerr BHs and\nquantitatively evaluate how the self-interaction strength of scalar bosons\naffects the growth rate. We also assess the evolution of the BH's mass and\nspin. Our results reveal that, in addition to the superradiance-imposed upper\nbound on the boson cloud mass, self-interactions of ultralight bosons introduce\na new, lower critical mass limit, beyond which the growth rate of the boson\ncloud approaches zero. This implies that the superradiance process terminates\nearlier when self-interactions are considered. Furthermore, we explore how\nself-interactions affect both the oscillation frequency of boson clouds in\ngravitational atoms and the frequency of gravitational wave (GW) emitted\nthrough cloud annihilation. The anticipated frequency shift could be detectable\nby the GW observatories. Given that self-interactions substantially alter the\nevolution of BH superradiance, their effects can significantly relax existing\nconstraints on scalar boson models derived from superradiance. Taking the spin\nmeasurements from GW190412 and GW190517 as examples, we discuss the impact of\nself-interactions on constraint results in details.",
      "generated_abstract": "ard Model of particle physics is the most successful framework for\ndescribing the observed physics of elementary particles. This framework is\nbased on the assumption that fundamental interactions are caused by the exchange\nof elementary particles. In this paper, we argue that this assumption is\nincomplete. Instead, we propose that the Standard Model is based on the\nassumption that fundamental interactions are caused by the exchange of\nnon-fundamental particles, or particle-like fields. We show that this\nassumption is not incompatible with the Standard Model, but it is\nsignificantly more restrictive. The particle-like fields cannot be fundamental\nparticles, as the Lagrangian does not allow for their mass generation. Instead,\nthe particle-like fields must be composite particles, which are furthermore\nrequired to satisfy certain properties. These properties are then used to\nconstruct a new particle Lagrangian, which we call the extended particle\nLagrangian.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14935064935064934,
          "p": 0.2911392405063291,
          "f": 0.1974248882219235
        },
        "rouge-2": {
          "r": 0.017094017094017096,
          "p": 0.03361344537815126,
          "f": 0.022662885049074384
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.26582278481012656,
          "f": 0.1802575062476746
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ex/2503.09280v1",
      "true_abstract": "An excess observed in the accelerator neutrino experiments in the $\\nu_{\\mu}\n\\rightarrow \\nu_{e}$ channel at high confidence level (CL) has been interpreted\nas due to eV-scale sterile neutrino(s). But, it has been suffered from the\nproblem of ``appearance - disappearance tension'' at the similarly high CL\nbecause the measurements of the $\\nu_{\\mu} \\rightarrow \\nu_{\\mu}$ channel do\nnot observe the expected event number depletion corresponding to the sterile\ncontribution in the appearance channel. We suggest non-unitarity as a simple\nand natural way of resolving the tension, which leads us to construct the\nnon-unitary $(3+1)$ model. With reasonable estimation of the parameters\ngoverning non-unitarity, we perform an illustrative analysis to know if the\ntension is resolved in this model. At the best fit of the appearance signature\nwe have found the unique solution with $\\sin^2 2\\theta_{14} \\approx 0.3$, which\nis consistent with the (reactors + Ga) data combined fit. Unexpectedly, our\ntension-easing mechanism bridges between the two high CL signatures, the BEST\nand LSND-MiniBooNE anomalies.",
      "generated_abstract": "C will provide unprecedented access to the hadronic final states\nthrough its higher luminosity. The discovery of a heavy vector boson, as well\nas the exploration of the underlying theory, will be key to unraveling the\nunderlying dynamics of the LHC. In this paper, we analyze the possible\nhadrons' final states with an invisible Higgs boson decaying into two jets in\nthe HL-LHC. We show that the discovery reach of a Higgs boson with mass in the\nrange of 200 GeV-300 GeV in the two-jet channel is comparable to the current\nexperimental reach of the HL-LHC. The analysis also highlights the\nimportance of the Higgs boson mass in the discovery reach in the three-jet\nchannel. The analysis also reveals that the Higgs",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12931034482758622,
          "p": 0.21428571428571427,
          "f": 0.161290317886461
        },
        "rouge-2": {
          "r": 0.01948051948051948,
          "p": 0.030303030303030304,
          "f": 0.023715410256058698
        },
        "rouge-l": {
          "r": 0.10344827586206896,
          "p": 0.17142857142857143,
          "f": 0.12903225337033203
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/TH/2503.00640v1",
      "true_abstract": "Laplacian matrices are commonly employed in many real applications, encoding\nthe underlying latent structural information such as graphs and manifolds. The\nuse of the normalization terms naturally gives rise to random matrices with\ndependency. It is well-known that dependency is a major bottleneck of new\nrandom matrix theory (RMT) developments. To this end, in this paper, we\nformally introduce a class of generalized (and regularized) Laplacian matrices,\nwhich contains the Laplacian matrix and the random adjacency matrix as a\nspecific case, and suggest the new framework of the asymptotic theory of\neigenvectors for latent embeddings with generalized Laplacian matrices\n(ATE-GL). Our new theory is empowered by the tool of generalized quadratic\nvector equation for dealing with RMT under dependency, and delicate high-order\nasymptotic expansions of the empirical spiked eigenvectors and eigenvalues\nbased on local laws. The asymptotic normalities established for both spiked\neigenvectors and eigenvalues will enable us to conduct precise inference and\nuncertainty quantification for applications involving the generalized Laplacian\nmatrices with flexibility. We discuss some applications of the suggested ATE-GL\nframework and showcase its validity through some numerical examples.",
      "generated_abstract": "e a framework for the study of Gaussian processes that allow\nfor the construction of a single, compact representation of the GPs. In our\nframework, the GPs are parametrized by a single parameter vector $\\boldsymbol{\\theta}\n\\in \\mathbb{R}^d$, and the GPs are assumed to be Gaussian. We show that the\nGaussian processes defined by this single parameter vector can be written as\nmatrices. Moreover, we show that the Gaussian processes defined by this single\nparameter vector are also well-defined in the full-dimensional space. This\nresult is motivated by the fact that, in high-dimensional settings, it is\noften the case that the Gaussian processes are well-defined in the full-dimensional\nspace, but that they are not well-defined in the parameter-vector space. In this\npaper, we present the theory of our framework for Gaussian processes and we\ndemonstrate its usefulness in various",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18421052631578946,
          "p": 0.3,
          "f": 0.22826086485113434
        },
        "rouge-2": {
          "r": 0.03550295857988166,
          "p": 0.05454545454545454,
          "f": 0.04301074791176939
        },
        "rouge-l": {
          "r": 0.18421052631578946,
          "p": 0.3,
          "f": 0.22826086485113434
        }
      }
    },
    {
      "paper_id": "cs.AI.stat/OT/2502.14581v1",
      "true_abstract": "Empirical human-AI alignment aims to make AI systems act in line with\nobserved human behavior. While noble in its goals, we argue that empirical\nalignment can inadvertently introduce statistical biases that warrant caution.\nThis position paper thus advocates against naive empirical alignment, offering\nprescriptive alignment and a posteriori empirical alignment as alternatives. We\nsubstantiate our principled argument by tangible examples like human-centric\ndecoding of language models.",
      "generated_abstract": "ntext of large language models (LLMs), the model's prediction\ndistribution is often evaluated via the Kullback-Leibler (KL) divergence.\nHowever, the KL divergence is not an objective function of the model's\nprediction distribution. Instead, it can be interpreted as an objective\nfunction if the model is trained to maximize the KL divergence.\n  In this paper, we show that the KL divergence is indeed an objective function\nof the model's prediction distribution. Furthermore, we prove that if the model\nis trained to maximize the KL divergence, then it also minimizes the KL\ndivergence. This implies that the KL divergence is indeed a regularizer in the\noptimization problem of maximizing the KL divergence.\n  We also show that if the model is trained to minimize the KL divergence, then\nit maximizes",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2033898305084746,
          "p": 0.20689655172413793,
          "f": 0.20512820012857053
        },
        "rouge-2": {
          "r": 0.015625,
          "p": 0.012345679012345678,
          "f": 0.01379309851700533
        },
        "rouge-l": {
          "r": 0.2033898305084746,
          "p": 0.20689655172413793,
          "f": 0.20512820012857053
        }
      }
    },
    {
      "paper_id": "physics.acc-ph.physics/acc-ph/2503.10122v1",
      "true_abstract": "Recently, the experimental discovery of a new type of wakefield effect, the\n\"skewed wake effect\", has been reported. We provide an explanation of the\nnature of the skewed wake effect based on a simple three-particle model that we\nhave developed. Taking a step forward, we analyze this effect for the case of a\nhighly elliptical beam, provide a simple estimate of the skew angle, and\nanalyze the wake amplitude effects associated with this effect.",
      "generated_abstract": "The study of the interaction of charged particles with the spacecraft bus\nin the vicinity of the Earth's magnetosphere is important for the safety of\nspacecraft operations. In this paper, a method for calculating the impact of\ncharged particles on the bus of a spacecraft is presented. The method is based\non the method of the kinetic approach and takes into account the effects of\nthe geomagnetic field on the charge of the particles. The calculations were\nperformed for the case of an elastic collision between charged particles and\nthe bus of the spacecraft. The obtained results are used to determine the\nprobability of the collision of charged particles with the bus of the spacecraft\nand the spacecraft itself. The obtained results are used to determine the\nprobability of the collision of charged particles with the bus of the spacecraft\nand the spacecraft itself.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24,
          "p": 0.21818181818181817,
          "f": 0.22857142358276658
        },
        "rouge-2": {
          "r": 0.08823529411764706,
          "p": 0.06382978723404255,
          "f": 0.07407406920286573
        },
        "rouge-l": {
          "r": 0.18,
          "p": 0.16363636363636364,
          "f": 0.17142856643990945
        }
      }
    },
    {
      "paper_id": "econ.TH.q-fin/PM/2410.19030v3",
      "true_abstract": "We present a theory of expected utility with state-dependent linear utility\nfunctions for monetary returns, that incorporates the possibility of\nloss-aversion. Our results relate to first order stochastic dominance,\nmean-preserving spread, increasing-concave linear utility profiles and risk\naversion. As an application of the expected utility theory developed here, we\nanalyze the contract that a monopolist would offer in an insurance market that\nallowed for partial coverage of loss.",
      "generated_abstract": "r introduces a new method for pricing options under model\ndifferences, which is more efficient than existing methods based on the\nDirichlet form or the Black-Scholes model. The new method is based on the\nconcept of ``compression'', which is a generalization of the Dirichlet form\nprinciple. In particular, compression is applied to the Black-Scholes model in\nthe same way as the Dirichlet form is applied to the Dirichlet process model,\nwhich is one of the most widely used model for pricing options in finance.\nFurthermore, the new method is also based on a novel approach to the Black-Scholes\nmodel, which is the first time that a method has been developed for the Black-Scholes\nmodel that does not rely on a martingale approximation. The new method has been\ntested on simulated data, and it has been demonstrated that it",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19230769230769232,
          "p": 0.14705882352941177,
          "f": 0.16666666175555572
        },
        "rouge-2": {
          "r": 0.03076923076923077,
          "p": 0.019230769230769232,
          "f": 0.023668634319527575
        },
        "rouge-l": {
          "r": 0.17307692307692307,
          "p": 0.1323529411764706,
          "f": 0.14999999508888903
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/OT/2412.11211v1",
      "true_abstract": "State-space models (SSMs) offer a powerful framework for dynamical system\nanalysis, wherein the temporal dynamics of the system are assumed to be\ncaptured through the evolution of the latent states, which govern the values of\nthe observations. This paper provides a selective review of recent advancements\nin deep neural network-based approaches for SSMs, and presents a unified\nperspective for discrete time deep state space models and continuous time ones\nsuch as latent neural Ordinary Differential and Stochastic Differential\nEquations. It starts with an overview of the classical maximum likelihood based\napproach for learning SSMs, reviews variational autoencoder as a general\nlearning pipeline for neural network-based approaches in the presence of latent\nvariables, and discusses in detail representative deep learning models that\nfall under the SSM framework. Very recent developments, where SSMs are used as\nstandalone architectural modules for improving efficiency in sequence modeling,\nare also examined. Finally, examples involving mixed frequency and\nirregularly-spaced time series data are presented to demonstrate the advantage\nof SSMs in these settings.",
      "generated_abstract": "ntext of large language models (LLMs), we have recently seen\nincreasing attention to the question of how well they can be used to create\nrepresentations that are useful for downstream tasks. A central question is how\nwell a LLM can be trained to generalize to unseen examples. In this work, we\nprovide a theoretical foundation for understanding the limits of how well a LLM\ncan be trained to generalize. We show that when the training dataset is\nlarge, there is a sharp distinction between two types of models: those that can\nbe trained to generalize well (i.e. generalization performance is close to\nperfect) and those that can't (i.e. generalization performance is very poor).\nWe also show that the two types of models are mutually exclusive: a model cannot\nbe trained to generalize well if it cannot be trained to generalize poorly. We\ndemon",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11818181818181818,
          "p": 0.16455696202531644,
          "f": 0.13756613270065246
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10909090909090909,
          "p": 0.1518987341772152,
          "f": 0.12698412211864188
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/SE/2503.09463v1",
      "true_abstract": "Co-developing scientific algorithms and hardware accelerators requires\ndomain-specific knowledge and large engineering resources. This leads to a slow\ndevelopment pace and high project complexity, which creates a barrier to entry\nthat is too high for the majority of developers to overcome. We are developing\na reusable end-to-end compiler toolchain for the Julia language entirely built\non permissively-licensed open-source projects. This unifies accelerator and\nalgorithm development by automatically synthesising Julia source code into\nhigh-performance Verilog.",
      "generated_abstract": "r introduces the Mini-TUV dataset, a novel dataset of small-scale\nvulnerability assessments, which we propose as a complement to the large-scale\nvulnerability assessment (VA) datasets of the Vulnerability Assessment\nBenchmark Suite (VA-BS). The Mini-TUV dataset comprises 233 vulnerabilities\nidentified in 30 organizations across 33 domains, with 100 vulnerabilities per\ndomain and 5 vulnerabilities per organization. The dataset contains a total of\n6,730 vulnerability-vulnerability pairs, with 4,360 vulnerabilities per\nvulnerability. Mini-TUV is characterized by a high degree of variability in\nvulnerability severity, with 25% of vulnerabilities being rated as low, and 50%\nof",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14754098360655737,
          "p": 0.14285714285714285,
          "f": 0.14516128532388153
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14754098360655737,
          "p": 0.14285714285714285,
          "f": 0.14516128532388153
        }
      }
    },
    {
      "paper_id": "math.FA.math/AP/2503.09536v1",
      "true_abstract": "We characterise the normal trace space associated to extended\n(measure-valued) divergence-measure fields on the boundary of a set $E \\subset\n\\mathbb R^n$, as the Arens-Eells space $\\mathrm{AE}(\\partial E)$. Such a trace\noperator is constructed for any Borel set $E$, and under a mild regularity\ncondition, which includes Lipschitz domains, this trace operator is shown to\nmoreover be surjective. This relies in part on a new pointwise description of\nthe Anzellotti pairing $\\overline{\\nabla \\phi \\cdot {\\boldsymbol F}}$ between a\n$\\mathrm{W}^{1,\\infty}$ function $\\phi$ and extended divergence-measure field\n${\\boldsymbol F}$. As an application, we prove extension theorems for\ndivergence-measure fields and divergence-free measures. Results for\n$\\mathrm{L}^1$-fields are also obtained.",
      "generated_abstract": "We prove that the generalized Heisenberg and Wigner representations of the\nalgebraic group $U(n)$ are related by an automorphism. The Heisenberg\nrepresentation is the standard one, whereas the Wigner representation is\ndefined by a Hermitian matrix $A$ such that the Wigner distribution function\n$w(z)$ is a Hermitian matrix function of $z$ with respect to the matrix $A$.\nThis representation is obtained by means of the generalized Heisenberg\nrepresentation. We show that the generalized Wigner representation is obtained\nfrom the generalized Heisenberg representation by an automorphism of the group\n$U(n)$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15853658536585366,
          "p": 0.30952380952380953,
          "f": 0.20967741487513014
        },
        "rouge-2": {
          "r": 0.00980392156862745,
          "p": 0.015384615384615385,
          "f": 0.011976043149630774
        },
        "rouge-l": {
          "r": 0.15853658536585366,
          "p": 0.30952380952380953,
          "f": 0.20967741487513014
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.astro-ph/EP/2503.08670v1",
      "true_abstract": "The Taurid Complex is a large interplanetary system that contains comet\n2P/Encke, several meteoroid streams, and possibly a number of near-Earth\nasteroids. The size and nature of the system has led to the speculation that it\nwas formed through a large-scale cometary breakup. Numerical investigations\nhave suggested that planetary dynamics can create a resonant region with a\nlarge number of objects concentrated in a small segment of the orbit, known as\nthe Taurid swarm, which approaches the Earth in certain years and provides\nfavorable conditions to study the Taurid Complex. Recent meteor observations\nconfirmed the existence of the swarm for mm- to m-sized objects. Here we\npresent a dedicated telescopic search for potentially hazardous asteroids and\nother macroscopic objects in the Taurid swarm using the Zwicky Transient\nFacility survey. We determine from our non-detection that there are no more\nthan 9--14 $H\\leq24$ (equivalent to a diameter of $D\\gtrsim100$~m) objects in\nthe swarm, suggesting that the Encke--Taurid progenitor was $\\sim10$~km in\nsize. A progenitor of such a size is compatible with the prediction of\nstate-of-the-art Solar System dynamical models, which expects $\\sim0.1$\n$D>10$~km objects on Encke-like orbits at any given time.",
      "generated_abstract": "tion of Stellar Populations in Galaxies (EvoSPaG) project aims to\nstudy the evolution of stellar populations in galaxies at redshifts of 0.05\nto 0.5. EvoSPaG is a multi-wavelength census of galaxies that will survey the\nspectra of 200,000 galaxies. We present a selection of the most prominent\ngalaxies in the EvoSPaG survey that have been observed with the Subaru\nHigh-Resolution Spectrograph (HRS). We describe the main features of the\nspectra, the calibration procedures, and the available data products. These\ninclude spectra taken in the HRS-wide and narrow-band filters, as well as the\nSpectral Energy Distributions (SEDs) derived from the observed spectra. We\ndiscuss the main features of the SEDs",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13385826771653545,
          "p": 0.24285714285714285,
          "f": 0.17258882790589825
        },
        "rouge-2": {
          "r": 0.04419889502762431,
          "p": 0.07692307692307693,
          "f": 0.056140346242167204
        },
        "rouge-l": {
          "r": 0.11811023622047244,
          "p": 0.21428571428571427,
          "f": 0.1522842593779795
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.08802v1",
      "true_abstract": "Recent literature highlights the advantages of implementing social rules via\ndynamic game forms. We characterize when truth-telling remains a dominant\nstrategy in gradual mechanisms implementing strategy-proof social rules, where\nagents gradually reveal their private information while acquiring information\nabout others in the process. Our first characterization hinges on the\nincentive-preservation of a basic transformation on gradual mechanisms called\nilluminating that partitions information sets. The second relies on a single\nreaction-proofness condition. We demonstrate the usefulness of both\ncharacterizations through applications to second-price auctions and the\ntop-trading cycles algorithm.",
      "generated_abstract": "This paper studies the problem of maximizing the social welfare under\nconditions that are not necessarily convex. We derive a concise characterization\nof the Nash equilibrium of such problems and show that it is a set of\noptimal solutions. This characterization is applied to a model of two-sided\nmatching where the social welfare depends not only on the number of matches\nperfectly satisfying the Nash equilibrium, but also on the number of\nunmatched agents. We derive a formula for the number of unmatched agents in\nthis model, and we show that the number of unmatched agents tends to infinity\nwhen the social welfare tends to infinity. We also show that the Nash\nequilibrium of the matching problem is a single-peaked solution, and we\nestablish conditions for the Nash equilibrium to be a convex set of\nsolutions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.2222222222222222,
          "f": 0.21052631080332423
        },
        "rouge-2": {
          "r": 0.011627906976744186,
          "p": 0.00980392156862745,
          "f": 0.010638292908558222
        },
        "rouge-l": {
          "r": 0.17142857142857143,
          "p": 0.19047619047619047,
          "f": 0.1804511228333994
        }
      }
    },
    {
      "paper_id": "math.FA.math/FA/2503.10467v1",
      "true_abstract": "This note is to advertise the concept of directed completion of partial\norders as the natural analogue of Cauchy-completion in Lorentzian signature,\nespecially in relation to non-smooth and/or infinite dimensional geometries.\n  A closely related notion appeared in the recent [2] where it was used as\nground for further development of non-smooth calculus in metric spacetimes\nallowing, for instance, for a quite general limit curve theorem in such\nsetting. The proposal in [2] was also in part motivated by the discussion we\nmake here, key points being:\n  - A general existence result, already obtained in the context of theoretical\ncomputer science and for which we give a slightly different presentation.\n  - An example showing that an analogue two-sided completion is unsuitable from\nthe (or at least `some') geometric/analytic perspective.\n  - The existence of natural links with both the the concept of ideal point of\nGeroch--Kronheimer--Penrose and with Beppo Levi's monotone convergence theorem,\nshowing in particular an underlying commonality between these two seemingly far\nconcepts.\n  - The flexibility of the notion, that by nature can cover non-smooth and\ninfinite-dimensional situations.\n  - The fact that the concept emerges spontaneously when investigating the\nduality relations between $L^p$ and $L^q$ spaces where $\\tfrac1p+\\tfrac 1q = 1$\nare H\\\"older conjugate exponents with $p, q < 1$.\n  This note is part of a larger work in progress that aims at laying the\ngrounds of a Lorentzian, or Hyperbolic, theory of Banach spaces. Given that the\nnotion of completion has nothing to do with the linear structure and the\ngrowing interest around this topic, for instance related to convergence of\ngeometric structures, we make available these partial findings.",
      "generated_abstract": "We consider the problem of computing the $r$-th Fourier sum of the\nfinite sum of $r$ linearly independent real polynomials. We prove that the\nFourier sum of a polynomial with $r$ roots can be computed in $O(r^3)$ time.\nThis result is new even for the case $r=1$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.05847953216374269,
          "p": 0.2857142857142857,
          "f": 0.09708737582005852
        },
        "rouge-2": {
          "r": 0.011627906976744186,
          "p": 0.06976744186046512,
          "f": 0.019933552368296456
        },
        "rouge-l": {
          "r": 0.05847953216374269,
          "p": 0.2857142857142857,
          "f": 0.09708737582005852
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.17810v2",
      "true_abstract": "In recent years, with advances in large language models (LLMs), end-to-end\nspoken dialogue models (SDMs) have made significant strides. Compared to\ntext-based LLMs, the evaluation of SDMs needs to take speech-related aspects\ninto account, such as paralinguistic information and speech quality. However,\nthere is still a lack of comprehensive evaluations for SDMs in speech-to-speech\n(S2S) scenarios. To address this gap, we propose URO-Bench, an extensive\nbenchmark for SDMs. Notably, URO-Bench is the first S2S benchmark that covers\nevaluations about multilingualism, multi-round dialogues, and paralinguistics.\nOur benchmark is divided into two difficulty levels: basic track and pro track,\nconsisting of 16 and 20 datasets respectively, evaluating the model's abilities\nin Understanding, Reasoning, and Oral conversation. Evaluations on our proposed\nbenchmark reveal that current open-source SDMs perform rather well in daily QA\ntasks, but lag behind their backbone LLMs in terms of instruction-following\nability and also suffer from catastrophic forgetting. Their performance in\nadvanced evaluations of paralinguistic information and audio understanding\nremains subpar, highlighting the need for further research in this direction.\nWe hope that URO-Bench can effectively facilitate the development of spoken\ndialogue models by providing a multifaceted evaluation of existing models and\nhelping to track progress in this area.",
      "generated_abstract": "y studies have focused on the generation of speech from text,\nmore recently, researchers have begun to investigate the generation of\nspeech from audio. This paper presents a novel method for generating speech\nfrom audio, leveraging a self-supervised speech representation learning model\nthat is pre-trained on a large speech corpus. The model is trained to learn\nspeech representations for both speech and text, enabling it to generate\nspeech from audio with high fidelity. This methodology is demonstrated using\na corpus of 100 hours of audio-text pairs, which are used to train the model,\nand the generated speech is evaluated on a separate set of 100 hours of audio-text\npairs. The results show that the model is able to generate speech that\nsignificantly outperforms baseline models that do not incorporate speech\nrepresentation learning. Additionally, the results demonstrate that the model\nis",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1103448275862069,
          "p": 0.20512820512820512,
          "f": 0.1434977532988801
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1103448275862069,
          "p": 0.20512820512820512,
          "f": 0.1434977532988801
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.05676v1",
      "true_abstract": "Multidimensional poverty measurement is crucial for capturing deprivation\nbeyond income-based metrics. This study compares the Alkire-Foster (AF) method\nand a Markov Random Field (MRF) approach for classifying multidimensional\npoverty using a simulation-based analysis. The AF method applies a\ndeterministic threshold-based classification, while the MRF approach leverages\nprobabilistic graphical modelling to account for correlations between\ndeprivation indicators. Using a synthetic dataset of 50,000 individuals with\nten binary deprivation indicators, we assess classification accuracy, false\npositive/negative trade-offs, and agreement between the methods. Results show\nthat AF achieves higher classification accuracy (89.5%) compared to MRF\n(75.4%), with AF minimizing false negatives and MRF reducing false positives.\nThe overall agreement between the two methods is 65%, with discrepancies\nprimarily occurring when AF classifies individuals as poor while MRF does not.\nWhile AF is transparent and easy to implement, it does not capture\ninterdependencies among indicators, potentially leading to misclassification.\nMRF, though computationally intensive, offers a more nuanced understanding of\ndeprivation clusters. These findings highlight the trade-offs in\nmultidimensional poverty measurement and provide insights for policymakers on\nmethod selection based on data availability and policy objectives. Future\nresearch should extend these approaches to non-binary indicators and real-world\ndatasets.",
      "generated_abstract": "The linear mixed model (LMM) is a widely used statistical framework for\nanalyzing complex longitudinal data, yet its flexibility can sometimes lead to\nhigh computational costs. In this paper, we propose a new LMM framework,\nreferred to as the multi-layer mixed model (MLMM), which addresses the\nchallenges of high computational complexity while preserving its flexibility.\nThe MLMM extends the LMM by introducing a multi-layer structure, which\nintroduces a new parameter for capturing the joint effects of multiple\nsub-populations within the same experiment. We propose a novel algorithm for\nefficiently computing the MLMM, and demonstrate its superiority through both\nsimulation studies and an application to the analysis of a real-world\nepidemiological data set.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10948905109489052,
          "p": 0.18518518518518517,
          "f": 0.13761467422902127
        },
        "rouge-2": {
          "r": 0.005291005291005291,
          "p": 0.009433962264150943,
          "f": 0.006779656412758081
        },
        "rouge-l": {
          "r": 0.10218978102189781,
          "p": 0.1728395061728395,
          "f": 0.1284403623024158
        }
      }
    },
    {
      "paper_id": "cs.SI.cs/SI/2503.07695v1",
      "true_abstract": "On February 7, 2024, Russian President Vladimir Putin gave a two-hour\ninterview with conservative political commentator, Tucker Carlson. This study\ninvestigated the impact of the Carlson- Putin interview on the US X audience.\nWe proposed a framework of social media impact using machine learning (ML) and\nnatural language processing (NLP) by measuring changes in audience, structure,\nand content. Triangulation methods were used to validate the process and\nresults. The interview had a considerable impact among segments of the American\npublic: 1) the reach and engagement of far-right influencers increased after\nthe interview, suggesting Kremlin narratives gained traction within these\ncircles, 2) the communication structure became more vulnerable to\ndisinformation spread after the interview, and 3) the public discourse changed\nfrom support for Ukraine funding to conversations about Putin, Russia, and the\nissue of \"truth\" or the veracity of Putin's claims. This research contributes\nto methods development for social media studies and aids scholars in analyzing\nhow public opinion shapes policy debates. The Carlson-Putin interview sparked a\nbroader discussion about truth-telling. Far from being muted, the broad impact\nof the interview appears considerable and poses challenges for foreign affairs\nleaders who depend on public support and buy-in when formulating national\npolicy.",
      "generated_abstract": "ng adoption of AI in healthcare requires robust assessments of its\nimpact. This paper introduces a framework for assessing the impact of AI in\nhealthcare. We first discuss the challenges in measuring the impact of AI in\nhealthcare, which are caused by the lack of consensus on the appropriate\nmetrics and the complexity of AI systems. We then present a novel framework for\nassessing the impact of AI in healthcare, which integrates multiple\nmeasurement approaches and incorporates the impact of human behavior. The\nframework is validated on three use cases: a diabetes risk prediction model,\na patient management system, and a medical imaging system. The results show that\nthe framework achieves a more comprehensive assessment of the impact of AI in\nhealthcare than previous approaches. The framework is designed to be\ninterpretable and adaptable to different contexts,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1276595744680851,
          "p": 0.22784810126582278,
          "f": 0.1636363590334712
        },
        "rouge-2": {
          "r": 0.031088082901554404,
          "p": 0.05454545454545454,
          "f": 0.03960395577122123
        },
        "rouge-l": {
          "r": 0.11347517730496454,
          "p": 0.20253164556962025,
          "f": 0.14545454085165302
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.21268v1",
      "true_abstract": "Gene drive alleles bias their own inheritance to offspring. They can fix in a\nwild-type population in spite of a fitness cost, and even lead to the\neradication of the target population if the fitness cost is high. However, this\noutcome may be prevented or delayed if areas previously cleared by the drive\nare recolonised by wild-type individuals. Here, we investigate the conditions\nunder which these stochastic wild-type recolonisation events are likely and\nwhen they are unlikely to occur in one spatial dimension. More precisely, we\nexamine the conditions ensuring that the last individual carrying a wild-type\nallele is surrounded by a large enough number of drive homozygous individuals,\nresulting in a very low chance of wild-type recolonisation. To do so, we make a\ndeterministic approximation of the distribution of drive alleles within the\nwave, and we split the distribution of wild-type alleles into a deterministic\npart and a stochastic part. Our analytical and numerical results suggest that\nthe probability of wild-type recolonisation events increases with lower fitness\nof drive individuals, with smaller migration rate, and also with smaller local\ncarrying capacity. Numerical simulations show that these results extend to two\nspatial dimensions. We also demonstrate that, if a wild-type recolonisation\nevent were to occur, the probability of a following drive reinvasion event\ndecreases with smaller values of the intrinsic growth rate of the population.\nOverall, our study paves the way for further analysis of wild-type\nrecolonisation at the back of eradication traveling waves.",
      "generated_abstract": "y investigates the effect of temperature on the performance of\ninsecticide-treated bed nets (ITNs), which play a critical role in malaria\ncontrol. Using a network of replicated laboratory experiments, we examine the\neffect of temperature on the survival, biting, and transmission of Anopheles\ngambiae sensu stricto. In the first set of experiments, we varied the temperature\nof the environment within which the insects were placed and assessed the impact\non the transmission of the malaria parasite. The second set of experiments\ninvestigated the effect of temperature on the survival of the mosquitoes. We\nfound that the survival rate of the mosquitoes increased with increasing\ntemperature. In addition, the transmission of the malaria parasite also\nincreased with increasing temperature. These findings indicate that mosquitoes\nare capable of adapting",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11428571428571428,
          "p": 0.23529411764705882,
          "f": 0.1538461494452664
        },
        "rouge-2": {
          "r": 0.023148148148148147,
          "p": 0.050505050505050504,
          "f": 0.03174602743582825
        },
        "rouge-l": {
          "r": 0.10714285714285714,
          "p": 0.22058823529411764,
          "f": 0.14423076482988179
        }
      }
    },
    {
      "paper_id": "physics.ins-det.physics/ins-det/2503.09495v1",
      "true_abstract": "The calibration of the CR39 and Makrofol Nuclear Track Detectors of the\nMoEDAL experiment at the CERN-LHC was performed by exposing stacks of detector\nfoils to heavy ion beams with energies ranging from 340 MeV/nucleon to 150\nGeV/nucleon. After chemical etching, the base areas and lengths of etch-pit\ncones were measured using automatic and manual optical microscopes. The\nresponse of the detectors, as measured by the ratio of the track-etching rate\nover the bulk-etching rate, was determined over a range extending from their\nthreshold at Z/$\\beta\\sim7$ and $\\sim50$ for CR39 and Makrofol, respectively,\nup to Z/$\\beta\\sim92$",
      "generated_abstract": "/c$^\\prime$ accelerator at the LHC is undergoing a major upgrade\nwhich will enable the installation of a 3.5GeV/c$^\\prime$ proton beam, the\nhighest energy beam in the LHC. In this paper, we present the design and\noptimisation of a new 3.5GeV/c$^\\prime$ proton beam source that will replace the\ncurrent 1.8GeV/c$^\\prime$ beam at the LHC. The new beam source will consist of\na linear accelerator with a 10-GeV/c$^\\prime$ electron storage ring. The\naccelerator will be equipped with a 100kV/200mA constant-current injector and a\n100kV/1000mA constant-current storage ring. The injector will produce",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1,
          "p": 0.1320754716981132,
          "f": 0.11382113330689426
        },
        "rouge-2": {
          "r": 0.01098901098901099,
          "p": 0.0125,
          "f": 0.011695901453440787
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.1320754716981132,
          "f": 0.11382113330689426
        }
      }
    },
    {
      "paper_id": "math.DG.math-ph/2503.10208v1",
      "true_abstract": "We explore the Jordan-Chevalley decomposition problem for an operator field\nin small dimensions. In dimensions three and four, we find tensorial conditions\nfor an operator field $L$, similar to a nilpotent Jordan block, to possess\nlocal coordinates in which $L$ takes a strictly upper triangular form. We prove\nthe Tempesta-Tondo conjecture for higher order brackets of\nFr\\\"olicher-Nijenhuis type.",
      "generated_abstract": "In this paper we construct a class of manifolds, which we call the\nnon-compact manifolds, which can be viewed as the universal cover of a\ncompact manifold. We show that this class is exactly the class of the universal\ncover of the non-compact manifolds. In particular, we show that the class of\nnon-compact manifolds is equivalent to the class of non-compact manifolds and\nthe class of compact manifolds. We also give examples of manifolds in this\nclass.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2127659574468085,
          "p": 0.2857142857142857,
          "f": 0.24390243413146948
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.19148936170212766,
          "p": 0.2571428571428571,
          "f": 0.21951219022903046
        }
      }
    },
    {
      "paper_id": "math.OC.q-fin/RM/2502.16364v1",
      "true_abstract": "As the developed world replaces Defined Benefit (DB) pension plans with\nDefined Contribution (DC) plans, there is a need to develop decumulation\nstrategies for DC plan holders. Optimal decumulation can be viewed as a problem\nin optimal stochastic control. Formulation as a control problem requires\nspecification of an objective function, which in turn requires a definition of\nreward and risk. An intuitive specification of reward is the total withdrawals\nover the retirement period. Most retirees view risk as the possibility of\nrunning out of savings. This paper investigates several possible left tail risk\nmeasures, in conjunction with DC plan decumulation. The risk measures studied\ninclude (i) expected shortfall (ii) linear shortfall and (iii) probability of\nshortfall. We establish that, under certain assumptions, the set of optimal\ncontrols associated with all expected reward and expected shortfall Pareto\nefficient frontier curves is identical to the set of optimal controls for all\nexpected reward and linear shortfall Pareto efficient frontier curves. Optimal\nefficient frontiers are determined computationally for each risk measure, based\non a parametric market model. Robustness of these strategies is determined by\ntesting the strategies out-of-sample using block bootstrapping of historical\ndata.",
      "generated_abstract": "We study the existence of a unique strong solution to a class of stochastic\ninterpolation equations with Lipschitz diffusion and quadratic drift, which\narise in the context of financial option pricing. The underlying stochastic\nprocess is described by a linear stochastic differential equation and the\ninterpolant is a linear combination of the underlying path and a finite\nnumber of nearby paths. The existence and uniqueness of the strong solution is\nestablished via the fixed point theorem. The solution is characterized by the\ncharacteristic function of a suitable function space. The existence of the\ncharacteristic function is proved via the contraction mapping principle. A\nvariety of applications are discussed, including option pricing.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12605042016806722,
          "p": 0.234375,
          "f": 0.16393442168114916
        },
        "rouge-2": {
          "r": 0.005813953488372093,
          "p": 0.01020408163265306,
          "f": 0.0074074027829932835
        },
        "rouge-l": {
          "r": 0.12605042016806722,
          "p": 0.234375,
          "f": 0.16393442168114916
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.21505v2",
      "true_abstract": "The Gulf Cooperation Council countries -- Oman, Bahrain, Kuwait, UAE, Qatar,\nand Saudi Arabia -- holds strategic significance due to its large oil reserves.\nHowever, these nations face considerable challenges in shifting from\noil-dependent economies to more diversified, knowledge-based systems. This\nstudy examines the progress of Gulf Cooperation Council (GCC) countries in\nachieving economic diversification and social development, focusing on the\nSocial Progress Index (SPI), which provides a broader measure of societal\nwell-being beyond just economic growth. Using data from the World Bank,\ncovering 2010 to 2023, the study employs the XGBoost machine learning model to\nforecast SPI values for the period of 2024 to 2026. Key components of the\nmethodology include data preprocessing, feature selection, and the simulation\nof independent variables through ARIMA modeling. The results highlight\nsignificant improvements in education, healthcare, and women's rights,\ncontributing to enhanced SPI performance across the GCC countries. However,\nnotable challenges persist in areas like personal rights and inclusivity. The\nstudy further indicates that despite economic setbacks caused by global\ndisruptions, including the COVID-19 pandemic and oil price volatility, GCC\nnations are expected to see steady improvements in their SPI scores through\n2027. These findings underscore the critical importance of economic\ndiversification, investment in human capital, and ongoing social reforms to\nreduce dependence on hydrocarbons and build knowledge-driven economies. This\nresearch offers valuable insights for policymakers aiming to strengthen both\nsocial and economic resilience in the region while advancing long-term\nsustainable development goals.",
      "generated_abstract": "This paper investigates the joint distribution of two large-scale\ndistributional forecasts, one from a macroeconomic model and the other from a\nstochastic volatility model. We show that these forecasts are mutually\nindependent and jointly normal, but they are not jointly Gaussian. We then\nintroduce a new method for jointly estimating the two forecasts, called\n\"Covariance Weighted Inverse Distance Weighting,\" and propose a method for\nestimating the joint distribution of the two forecasts using this method. The\nproposed estimator is a kernel-based method that allows for flexible\nnonparametric kernel functions. We demonstrate the properties of this method\nthrough simulation studies and an application to the forecasting of quarterly\nGDP growth. We also apply the method to the forecasting of quarterly inflation\nin the United States.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10526315789473684,
          "p": 0.23376623376623376,
          "f": 0.14516128604090803
        },
        "rouge-2": {
          "r": 0.012711864406779662,
          "p": 0.02702702702702703,
          "f": 0.017291061931252082
        },
        "rouge-l": {
          "r": 0.0935672514619883,
          "p": 0.2077922077922078,
          "f": 0.12903225378284355
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.19557v2",
      "true_abstract": "We study how social image concerns affect information sharing patterns\nbetween peers. An individual receives a signal (\"news\") about the state of the\nworld and can either share it with a peer or not. This signal has two\nattributes: a headline -- e.g., arguing for or against human-induced climate\nchange -- and a veracity status, indicating if the signal is based on facts or\nmade-up. The headline is observable at no cost by everyone, while observing the\nveracity status is costly and the cost depends on an individual's type. We\nstudy the sharing patterns induced by two different types of social image\nconcern: wanting to be perceived as talented, which implies being able to\ndistinguish proper from fake news, and wanting to signal one's worldview. Our\nmodel can rationalize the empirical finding that fake news may be shared with a\nhigher propensity than proper news (e.g., Vosoughi et al., 2018). We show that\nboth a veracity and a worldview concern may rationalize this finding, though\nsharing patterns are empirically distinguishable and welfare implications\ndiffer.",
      "generated_abstract": "We study the Nash equilibrium in the game of public goods where a\ndistribution of public goods is provided to a population with a non-linear\nutility function. In this game, a public goods provider receives a\nmonetary payment for each good that is provided to the population. We show that\nthe Nash equilibrium is a Nash equilibrium in the game if and only if the\nutility function is concave. We also provide a simple criterion for the\nexistence of the Nash equilibrium. We then apply our results to the public\ngoods game and demonstrate that the Nash equilibrium is the maximizer of the\npublic goods function in the game.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12396694214876033,
          "p": 0.3125,
          "f": 0.17751478883232388
        },
        "rouge-2": {
          "r": 0.047337278106508875,
          "p": 0.0963855421686747,
          "f": 0.06349205907438933
        },
        "rouge-l": {
          "r": 0.10743801652892562,
          "p": 0.2708333333333333,
          "f": 0.15384614977906946
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2503.01362v1",
      "true_abstract": "This paper describes a streaming audio-to-MIDI piano transcription approach\nthat aims to sequentially translate a music signal into a sequence of note\nonset and offset events. The sequence-to-sequence nature of this task may call\nfor the computationally-intensive transformer model for better performance,\nwhich has recently been used for offline transcription benchmarks and could be\nextended for streaming transcription with causal attention mechanisms. We\nassume that the performance limitation of this naive approach lies in the\ndecoder. Although time-frequency features useful for onset detection are\nconsiderably different from those for offset detection, the single decoder is\ntrained to output a mixed sequence of onset and offset events without guarantee\nof the correspondence between the onset and offset events of the same note. To\novercome this limitation, we propose a streaming encoder-decoder model that\nuses a convolutional encoder aggregating local acoustic features, followed by\nan autoregressive Transformer decoder detecting a variable number of onset\nevents and another decoder detecting the offset events for the active pitches\nwith validation of the sustain pedal at each time frame. Experiments using the\nMAESTRO dataset showed that the proposed streaming method performed comparably\nwith or even better than the state-of-the-art offline methods while\nsignificantly reducing the computational cost.",
      "generated_abstract": "aper, we propose a novel multi-stage pre-training approach for\nconversational speech recognition. Our approach is based on a hierarchical\narchitecture that integrates a pre-trained large language model (LLM) and an\ninverted recurrent neural network (IRNN) for dialogue management. The LLM\nprovides a context-aware representation of the dialogue, while the IRNN\ncaptures the inter-turn interactions. The IRNN is also utilized to enhance\nspeaker embedding, enabling the LLM to understand the speaker's intent and\ntone. The pre-trained LLM and the IRNN are both fine-tuned on a large\nmultilingual corpus to improve the model's performance. The results demonstrate\nthat the proposed model achieves state-of-the-art performance across various\nbenchmarks, including the benchmark of the multilingual dialogue dataset",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15267175572519084,
          "p": 0.2597402597402597,
          "f": 0.19230768764469314
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.045871559633027525,
          "f": 0.03401360077629756
        },
        "rouge-l": {
          "r": 0.1297709923664122,
          "p": 0.22077922077922077,
          "f": 0.16346153379853934
        }
      }
    },
    {
      "paper_id": "nucl-ex.physics/ed-ph/2502.14612v1",
      "true_abstract": "The Modular Neutron Array (MoNA) collaboration was initiated in 2000 at the\nNational Superconducting Cyclotron Laboratory (NSCL) at Michigan State\nUniversity (MSU). Since then, the collaboration studied properties of nuclides\nat and beyond the neutron dripline discovering seven new isotopes between\nlithium to fluorine. The collaboration included liberal arts colleges, regional\ncomprehensive universities, and major research universities with the focus of\ngiving undergraduate students meaningful research experiences. Over the last 25\nyears, the combined efforts of hundreds of undergraduates, dozens of graduate\nstudents and research associates, and faculty from more than a dozen colleges\nand universities produced over fifty publications, won awards for research, and\ncombined research and teaching in new and interesting ways.",
      "generated_abstract": "E2025 evaluation is the latest edition of the Nuclear Data\nSECTION for the Elements, Isotopes and Compounds. This paper reviews the\ncontributions of the evaluation to nuclear data science. In the evaluation\nprocess, the NUBASE2025 team used a broad range of experimental data,\nincluding nuclear data from high-resolution experiments, neutron scattering\ndata, and nuclear structure data from low-energy and high-resolution\nexperiments, and a large number of experimental and theoretical data, including\nneutron-scattering data, neutron-proton-scattering data, and neutron-nucleus\nscattering data. The evaluation is based on the latest experimental data,\nincluding the most recent data of the RIKEN/BNL neutron-scattering and\nlow-energy-energy neutron-proton-scatter",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10588235294117647,
          "p": 0.16363636363636364,
          "f": 0.12857142380102057
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09411764705882353,
          "p": 0.14545454545454545,
          "f": 0.11428570951530632
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.09310v1",
      "true_abstract": "The failure of a system can result from the simultaneous effects of multiple\ncauses, where assigning a specific cause may be inappropriate or unavailable.\nExamples include contributing causes of death in epidemiology and the aetiology\nof neurodegenerative diseases like Alzheimer's. We propose a parametric Weibull\naccelerated failure time model for multiple causes, incorporating a\ndata-driven, individualized, and time-varying winning probability (relative\nimportance) matrix. Using maximum likelihood estimation and the\nexpectation-maximization (EM) algorithm, our approach enables simultaneous\nestimation of regression coefficients and relative cause importance, ensuring\nconsistency and asymptotic normality. A simulation study and an application to\nAlzheimer's disease demonstrate its effectiveness in addressing cause-mixture\nproblems and identifying informative biomarker combinations, with comparisons\nto Weibull and Cox proportional hazards models.",
      "generated_abstract": "This paper develops a methodology to analyze the relationship between a\nmeasurable random variable and a continuous function of this variable. This\nmethodology is used to derive a new version of the central limit theorem for the\ndistribution of a random variable that is the inverse of a continuous function\nof this variable. This new version of the central limit theorem is a general\nversion of the central limit theorem for the distribution of a random variable\nthat is the inverse of a continuous function of another random variable, which\nis a generalization of the central limit theorem for the distribution of the\ninverse of a random variable that is the product of two independent random\nvariables.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06451612903225806,
          "p": 0.15384615384615385,
          "f": 0.09090908674586796
        },
        "rouge-2": {
          "r": 0.008620689655172414,
          "p": 0.015873015873015872,
          "f": 0.011173179795888381
        },
        "rouge-l": {
          "r": 0.06451612903225806,
          "p": 0.15384615384615385,
          "f": 0.09090908674586796
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.09981v1",
      "true_abstract": "This study investigates possibility and impossibility results of the\nrepugnant and sadistic conclusions in population ethics and economics. The\nrepugnant conclusion says that an enormous population with very low well-being\nis socially better than any smaller population with sufficiently high\nwell-being. The sadistic conclusion says that adding individuals with negative\nwell-being to a society is socially better than adding individuals with\npositive well-being to it. Previous studies have often found it challenging to\navoid both undesirable conclusions. However, I demonstrate that a class of\nacceptable social welfare orderings can easily prevent these conclusions while\nadhering to standard axioms, such as anonymity, strong Pareto, Pigou-Dalton\ntransfer, and extended continuity. Nevertheless, if the avoidance requirements\nfor the repugnant and sadistic conclusions are strengthened, it is possible to\nencounter new impossibility results. These results reveal essential conflicts\nbetween the independence axiom and the avoidance of the weak repugnant\nconclusion when evaluating well-being profiles with different populations.",
      "generated_abstract": "r studies the equilibrium allocation of resources in a game in which\nthe agents can choose between two allocations. The first allocation is called\nthe pure allocation, and is achieved by all agents who choose it. The second\nallocation is called the mixed allocation, and is achieved by agents who choose\nit but not the pure allocation. The mixed allocation is preferred by agents who\nchoose it, but not by those who choose the pure allocation. The paper shows\nthat if the pure allocation is an equilibrium allocation, then the mixed\nallocation is also an equilibrium allocation. The paper then shows that the\nmixed allocation is an equilibrium allocation if and only if the pure allocation\nis not a Nash equilibrium. The paper then shows that if the pure allocation is\nan equilibrium allocation, then the mixed allocation is an equilibrium\nallocation if and only if the pure allocation is a Nash equilibrium. The paper\nconcludes that if the pure allocation is an equilibrium allocation, then the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1414141414141414,
          "p": 0.30434782608695654,
          "f": 0.19310344394387643
        },
        "rouge-2": {
          "r": 0.007407407407407408,
          "p": 0.012048192771084338,
          "f": 0.009174307211095765
        },
        "rouge-l": {
          "r": 0.13131313131313133,
          "p": 0.2826086956521739,
          "f": 0.1793103404956006
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.06587v1",
      "true_abstract": "This article presents a comprehensive methodology for processing financial\ndatasets of Apple Inc., encompassing quarterly income and daily stock prices,\nspanning from March 31, 2009, to December 31, 2023. Leveraging 60 observations\nfor quarterly income and 3774 observations for daily stock prices, sourced from\nMacrotrends and Yahoo Finance respectively, the study outlines five distinct\ndatasets crafted through varied preprocessing techniques. Through detailed\nexplanations of aggregation, interpolation (linear, polynomial, and cubic\nspline) and lagged variables methods, the study elucidates the steps taken to\ntransform raw data into analytically rich datasets. Subsequently, the article\ndelves into regression analysis, aiming to decipher which of the five data\nprocessing methods best suits capital market analysis, by employing both linear\nand polynomial regression models on each preprocessed dataset and evaluating\ntheir performance using a range of metrics, including cross-validation score,\nMSE, MAE, RMSE, R-squared, and Adjusted R-squared. The research findings reveal\nthat linear interpolation with polynomial regression emerges as the\ntop-performing method, boasting the lowest validation MSE and MAE values,\nalongside the highest R-squared and Adjusted R-squared values.",
      "generated_abstract": "We propose a novel approach for constructing a large sample distribution\nfor the estimand of a causal effect. This approach is based on the\nrepresentation of the estimand as a sum of terms that are conditional on a set\nof variables. We show that the resulting estimand can be expressed as a sum of\nconditional effects, which can be estimated by a sample mean. We derive\nasymptotic properties of the estimator. Our estimator can be applied to any\ncausal effect of interest, including binary and continuous outcomes, and any\nestimation procedure, including ordinary least squares, propensity score\nmatching, and structural estimation. We illustrate the proposed approach in a\nsimulation study and two empirical applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12,
          "p": 0.21739130434782608,
          "f": 0.15463917067435448
        },
        "rouge-2": {
          "r": 0.006097560975609756,
          "p": 0.009523809523809525,
          "f": 0.0074349394784513825
        },
        "rouge-l": {
          "r": 0.096,
          "p": 0.17391304347826086,
          "f": 0.12371133562280813
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.hep-lat/2503.08414v1",
      "true_abstract": "This work examines the effect of disclinations on the scattering of\nquasipaticles in graphene with the presence of a topological defect. Using the\ntight-binding method, the electronic properties of graphene with disclination\nare described, where the topological defects are introduced in the lattice via\ngeometric theory. The massless Dirac equation is modified to account for the\ncurvature induced by these defects, incorporating a gauge field. The results\nshow that disclinations significantly affect the scattering process, altering\nphase shifts and interference patterns. The differential cross-section and its\ndependence on the scattering angle are analyzed, highlighting the role of\ngeometric factors like the parameter {\\alpha} in shaping the scattering\ndynamics.",
      "generated_abstract": "ork, we perform a systematic analysis of the long-range\ncoulomb interaction in the presence of a local magnetic field in a\none-dimensional electron gas. We employ a recently developed method to\ncalculate the spectral functions of the local magnetic field, and demonstrate\nthat the spectral function exhibits a clear peak at the Fermi energy, which\nshows up as a pairing gap in the electron gas. We find that the peak appears\nin the spectral function in the vicinity of the Fermi energy, and it can be\nsignificantly enhanced by a magnetic field. Furthermore, we calculate the\npairing correlation function, which shows a strong correlation at short\ndistances and weak correlation at long distances. Our results indicate that the\nlong-range interaction and the pairing correlation function are closely\nrelated to each other. Finally, we perform a numerical analysis of the pairing\ncorrelation function in the presence of a magnetic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17333333333333334,
          "p": 0.18055555555555555,
          "f": 0.17687074330140232
        },
        "rouge-2": {
          "r": 0.0392156862745098,
          "p": 0.03571428571428571,
          "f": 0.037383172581012106
        },
        "rouge-l": {
          "r": 0.16,
          "p": 0.16666666666666666,
          "f": 0.16326530112453158
        }
      }
    },
    {
      "paper_id": "cs.AI.q-fin/GN/2407.20377v1",
      "true_abstract": "This paper explores an innovative approach to Environmental, Social, and\nGovernance (ESG) scoring by integrating Natural Language Processing (NLP)\ntechniques with Item Response Theory (IRT), specifically the Rasch model. The\nstudy utilizes a comprehensive dataset of news articles in Portuguese related\nto Petrobras, a major oil company in Brazil, collected from 2022 and 2023. The\ndata is filtered and classified for ESG-related sentiments using advanced NLP\nmethods. The Rasch model is then applied to evaluate the psychometric\nproperties of these ESG measures, providing a nuanced assessment of ESG\nsentiment trends over time. The results demonstrate the efficacy of this\nmethodology in offering a more precise and reliable measurement of ESG factors,\nhighlighting significant periods and trends. This approach may enhance the\nrobustness of ESG metrics and contribute to the broader field of sustainability\nand finance by offering a deeper understanding of the temporal dynamics in ESG\nreporting.",
      "generated_abstract": "r presents a new approach for generating multimodal financial\ne-learning content, which combines the use of Natural Language Processing\n(NLP) techniques with Deep Learning (DL) models. The proposed system\nenables the generation of diverse financial e-learning content, including\ntextual, audio, and video content, tailored to specific user needs. The\nproposed approach incorporates several key components, including a multi-task\nlearning model for text generation, a multi-modal text-to-speech model for\naudio generation, and a multimodal image-text model for visual content\ngeneration. The proposed system is evaluated through a series of experiments\nthat evaluate the effectiveness of the proposed approach in generating\ndiverse, high-quality financial e-learning content. The results demonstrate the\neffectiveness of the proposed system in generating high-quality,\nmulti-modal financial e-learning content that is tailored to specific",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19230769230769232,
          "p": 0.28169014084507044,
          "f": 0.2285714237492246
        },
        "rouge-2": {
          "r": 0.07042253521126761,
          "p": 0.09523809523809523,
          "f": 0.08097165503122519
        },
        "rouge-l": {
          "r": 0.19230769230769232,
          "p": 0.28169014084507044,
          "f": 0.2285714237492246
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/MF/2411.18397v1",
      "true_abstract": "We study optimal payoff choice for an expected utility maximizer under the\nconstraint that their payoff is not allowed to deviate ``too much'' from a\ngiven benchmark. We solve this problem when the deviation is assessed via a\nBregman-Wasserstein (BW) divergence, generated by a convex function $\\phi$.\nUnlike the Wasserstein distance (i.e., when $\\phi(x)=x^2$). The inherent\nasymmetry of the BW divergence makes it possible to penalize positive\ndeviations different than negative ones. As a main contribution, we provide the\noptimal payoff in this setting. Numerical examples illustrate that the choice\nof $\\phi$ allow to better align the payoff choice with the objectives of\ninvestors.",
      "generated_abstract": "This paper studies the model of the mean-variance portfolio selection with\nreinforcement learning. We introduce the value function of the mean-variance\noptimization and the corresponding optimal policy. We also develop a new\noptimization algorithm for the value function and the optimal policy, which\nminimizes the sum of the squared error between the value function and the\noptimal policy. The algorithm is designed to be computationally efficient. We\npresent numerical simulations and empirical results on financial data. Our\nfindings reveal that the portfolio selection based on the value function is\nmore efficient than that based on the optimal policy. Moreover, our method can\neasily be applied to a wide range of portfolio selection problems, including\nportfolio selection with stochastic volatility.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1625,
          "p": 0.19117647058823528,
          "f": 0.17567567070854656
        },
        "rouge-2": {
          "r": 0.02912621359223301,
          "p": 0.03125,
          "f": 0.03015074877503176
        },
        "rouge-l": {
          "r": 0.1625,
          "p": 0.19117647058823528,
          "f": 0.17567567070854656
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2409.13236v1",
      "true_abstract": "Collective decision-making is the process through which diverse stakeholders\nreach a joint decision. Within societal settings, one example is participatory\nbudgeting, where constituents decide on the funding of public projects. How to\nmost efficiently aggregate diverse stakeholder inputs on a portfolio of\nprojects with uncertain long-term benefits remains an open question. We address\nthis problem by studying collective decision-making through the integration of\npreference aggregation and knapsack allocation methods. Since different\nstakeholder groups may evaluate projects differently,we examine several\naggregation methods that combine their diverse inputs. The aggregated\nevaluations are then used to fill a ``collective'' knapsack. Among the methods\nwe consider are the arithmetic mean, Borda-type rankings, and delegation to\nexperts. We find that the factors improving an aggregation method's ability to\nidentify projects with the greatest expected long-term value include having\nmany stakeholder groups, moderate variation in their expertise levels, and some\ndegree of delegation or bias favoring groups better positioned to objectively\nassess the projects. We also discuss how evaluation errors and heterogeneous\ncosts impact project selection. Our proposed aggregation methods are relevant\nnot only in the context of funding public projects but also, more generally,\nfor organizational decision-making under uncertainty.",
      "generated_abstract": "aper, we propose a novel game theory approach to study the optimal\nplacement of a facility in a network of interconnected facilities. The\ninterconnected facilities can be either private or public. In the private\nfacility setting, we show that the optimal placement of the facility is\ndetermined by the location of the facility, and the locations of the\ninterconnected facilities. In the public facility setting, we show that the\noptimal placement of the facility is determined by the location of the\nfacility, and the locations of the interconnected facilities. Furthermore, we\nshow that the optimal placement of the facility in the private facility\nsetting is the same as the optimal placement of the facility in the public\nfacility setting. We further show that the optimal placement of the facility in\nthe private facility setting is the same as the optimal placement of the\nfacility in the public facility setting. Finally, we show that the optimal",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10218978102189781,
          "p": 0.3181818181818182,
          "f": 0.15469612891669982
        },
        "rouge-2": {
          "r": 0.015789473684210527,
          "p": 0.043478260869565216,
          "f": 0.023166019257316442
        },
        "rouge-l": {
          "r": 0.10218978102189781,
          "p": 0.3181818181818182,
          "f": 0.15469612891669982
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.10352v1",
      "true_abstract": "Popular safe Bayesian optimization (BO) algorithms learn control policies for\nsafety-critical systems in unknown environments. However, most algorithms make\na smoothness assumption, which is encoded by a known bounded norm in a\nreproducing kernel Hilbert space (RKHS). The RKHS is a potentially\ninfinite-dimensional space, and it remains unclear how to reliably obtain the\nRKHS norm of an unknown function. In this work, we propose a safe BO algorithm\ncapable of estimating the RKHS norm from data. We provide statistical\nguarantees on the RKHS norm estimation, integrate the estimated RKHS norm into\nexisting confidence intervals and show that we retain theoretical guarantees,\nand prove safety of the resulting safe BO algorithm. We apply our algorithm to\nsafely optimize reinforcement learning policies on physics simulators and on a\nreal inverted pendulum, demonstrating improved performance, safety, and\nscalability compared to the state-of-the-art.",
      "generated_abstract": "ration of cognitive radio (CR) networks with energy harvesting (EH)\nand reconfigurable intelligent surfaces (RIS) has received significant\nattention due to their potential to enhance spectrum efficiency and resource\nallocation. However, the dynamic nature of CR networks, where users dynamically\nmove between the base station (BS) and the edge, poses challenges in designing\noptimal RIS configurations for maximizing spectral efficiency. To address this\nissue, this paper proposes a distributed RIS configuration optimization\nmethodology based on a constrained quadratic program (QP) that optimizes the\nnumber of RIS elements required to provide the highest spectral efficiency\nwithin the constraints of the system parameters. The proposed methodology\nincorporates the dynamic movement of CR users into the RIS configuration,\nleveraging the spatial domain to capture the spatial correlation between users\nand the RIS elements. The proposed methodology is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16161616161616163,
          "p": 0.17204301075268819,
          "f": 0.16666666167154967
        },
        "rouge-2": {
          "r": 0.015267175572519083,
          "p": 0.016129032258064516,
          "f": 0.01568626951357329
        },
        "rouge-l": {
          "r": 0.13131313131313133,
          "p": 0.13978494623655913,
          "f": 0.13541666167154967
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.19574v1",
      "true_abstract": "Large language models (LLMs) have shown promise in various natural language\nprocessing tasks, including their application to proteomics data to classify\nprotein fragments. In this study, we curated a limited mass spectrometry\ndataset with 1000s of protein fragments, consisting of proteins that appear to\nbe attached to the endoplasmic reticulum in cardiac cells, of which a fraction\nwas cloned and characterized for their impact on SERCA, an ER calcium pump.\nWith this limited dataset, we sought to determine whether LLMs could correctly\npredict whether a new protein fragment could bind SERCA, based only on its\nsequence and a few biophysical characteristics, such as hydrophobicity,\ndetermined from that sequence. To do so, we generated random sequences based on\ncloned fragments, embedded the fragments into a retrieval augmented generation\n(RAG) database to group them by similarity, then fine-tuned large language\nmodel (LLM) prompts to predict whether a novel sequence could bind SERCA. We\nbenchmarked this approach using multiple open-source LLMs, namely the\nMeta/llama series, and embedding functions commonly available on the\nHuggingface repository. We then assessed the generalizability of this approach\nin classifying novel protein fragments from mass spectrometry that were not\ninitially cloned for functional characterization. By further tuning the prompt\nto account for motifs, such as ER retention sequences, we improved the\nclassification accuracy by and identified several proteins predicted to\nlocalize to the endoplasmic reticulum and bind SERCA, including Ribosomal\nProtein L2 and selenoprotein S. Although our results were based on proteomics\ndata from cardiac cells, our approach demonstrates the potential of LLMs in\nidentifying novel protein interactions and functions with very limited\nproteomic data.",
      "generated_abstract": "We introduce a novel approach to studying the behavior of dynamical\nsystems, which is based on a probabilistic perspective on the dynamics of\nsystems. This approach is based on the notion of ``decomposability'' of\nsystems, which we define and analyze. We apply this approach to the study of\nbehavior of systems of coupled oscillators, using a simple model of such\nsystems. We provide a complete description of the behavior of the system,\nincluding the emergence of chaotic behavior. In addition, we analyze the\ndynamics of the system with respect to different parameters, including the\ndegree of chaos, and discuss the relation between the parameters of the\nsystem and the degree of chaos. We provide a comprehensive analysis of the\nbehavior of the system, including the emergence of chaotic behavior, and\ndiscuss the relation between the parameters of the system and the degree of\nchaos.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11728395061728394,
          "p": 0.31666666666666665,
          "f": 0.17117116722668627
        },
        "rouge-2": {
          "r": 0.02,
          "p": 0.050505050505050504,
          "f": 0.028653291064934364
        },
        "rouge-l": {
          "r": 0.10493827160493827,
          "p": 0.2833333333333333,
          "f": 0.1531531492086682
        }
      }
    },
    {
      "paper_id": "cs.GR.cs/GR/2503.08370v1",
      "true_abstract": "This paper presents Ev-Layout, a novel large-scale event-based multi-modal\ndataset designed for indoor layout estimation and tracking. Ev-Layout makes key\ncontributions to the community by: Utilizing a hybrid data collection platform\n(with a head-mounted display and VR interface) that integrates both RGB and\nbio-inspired event cameras to capture indoor layouts in motion. Incorporating\ntime-series data from inertial measurement units (IMUs) and ambient lighting\nconditions recorded during data collection to highlight the potential impact of\nmotion speed and lighting on layout estimation accuracy. The dataset consists\nof 2.5K sequences, including over 771.3K RGB images and 10 billion event data\npoints. Of these, 39K images are annotated with indoor layouts, enabling\nresearch in both event-based and video-based indoor layout estimation. Based on\nthe dataset, we propose an event-based layout estimation pipeline with a novel\nevent-temporal distribution feature module to effectively aggregate the\nspatio-temporal information from events. Additionally, we introduce a\nspatio-temporal feature fusion module that can be easily integrated into a\ntransformer module for fusion purposes. Finally, we conduct benchmarking and\nextensive experiments on the Ev-Layout dataset, demonstrating that our approach\nsignificantly improves the accuracy of dynamic indoor layout estimation\ncompared to existing event-based methods.",
      "generated_abstract": "t a novel approach for the analysis of large-scale 3D human\nscenes, based on the combination of multi-modal deep learning models and\noptimization techniques. Our method enables a fine-grained analysis of the\nrelationships between human bodies and objects in a scene, enabling detailed\ncharacterization of the semantic and spatial relationships between them.\n  Our approach utilizes a multi-modal deep learning model to extract\nsemantic-based features from the input images, followed by a set of\noptimization-based transformations to refine these features and enhance their\nrelevance to the scene analysis. The refined features are then used to\noptimize a cost function, enabling the determination of the optimal pose of\neach human body in the scene, as well as the location of the object in the\nscene. We evaluate our method on a large-scale human scene dataset, showing\nthat our approach can achieve state-of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15748031496062992,
          "p": 0.23809523809523808,
          "f": 0.18957345492329472
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.038461538461538464,
          "f": 0.031746026898463835
        },
        "rouge-l": {
          "r": 0.14960629921259844,
          "p": 0.2261904761904762,
          "f": 0.18009478193751274
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.08557v1",
      "true_abstract": "This paper studies Integrated Sensing, Communication, and Powering (ISCAP) as\na novel framework designed to enhance Internet of Things (IoT) applications\nwithin sixth-generation wireless networks. In these applications, in addition\nto IoT devices requiring an energy supply and receiving information or control\ndata to perform their tasks, the base station serving them must sense the\ndevices and their environment to localize them, thereby improving data\ntransmission and enabling simultaneous power delivery. In our multi-node ISCAP\nIoT system, we optimize base station beamforming alongside the receiver's\npower-splitting factor to maximize energy harvesting while adhering to strict\ncommunication and sensing constraints. To effectively tackle this non-convex\noptimization problem, we decompose it into three manageable subproblems and\nemploy several techniques such as semidefinite relaxation and Rayleigh quotient\nmethods to find an efficient solution. Simulation results demonstrate the\neffectiveness of the proposed design, highlighting performance trade-offs among\nsensing accuracy, communication reliability, and power transfer efficiency.",
      "generated_abstract": "r presents a novel wireless communication system, called the\nHarmonized Wireless Communication System (HWCS), which integrates the\nadvanced technologies of millimeter-wave (mmWave) communication and 5G\ncommunication. The HWCS system is designed for multi-user communication in\nterrestrial networks, where multiple users can communicate with the base station\nsimultaneously. The proposed system can support both uplink and downlink\ntransmissions. The uplink transmissions are organized into multiple time\nslots, and each time slot is divided into several sub-slots. The downlink\ntransmissions are organized into multiple sub-slots, and each sub-slot is\ndivided into several frame segments. Each frame segment is divided into several\nslots. The HWCS system includes two types of users: a single-user type and a\nmulti-user type. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12605042016806722,
          "p": 0.2112676056338028,
          "f": 0.157894732161219
        },
        "rouge-2": {
          "r": 0.026845637583892617,
          "p": 0.039603960396039604,
          "f": 0.03199999518432073
        },
        "rouge-l": {
          "r": 0.12605042016806722,
          "p": 0.2112676056338028,
          "f": 0.157894732161219
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.06079v1",
      "true_abstract": "Despite the significance of probabilistic time-series forecasting models,\ntheir evaluation metrics often involve intractable integrations. The most\nwidely used metric, the continuous ranked probability score (CRPS), is a\nstrictly proper scoring function; however, its computation requires\napproximation. We found that popular CRPS estimators--specifically, the\nquantile-based estimator implemented in the widely used GluonTS library and the\nprobability-weighted moment approximation--both exhibit inherent estimation\nbiases. These biases lead to crude approximations, resulting in improper\nrankings of forecasting model performance when CRPS values are close. To\naddress this issue, we introduced a kernel quadrature approach that leverages\nan unbiased CRPS estimator and employs cubature construction for scalable\ncomputation. Empirically, our approach consistently outperforms the two widely\nused CRPS estimators.",
      "generated_abstract": "stical analysis of large datasets is increasingly common in\nnon-technical applications, and has become essential in modern society.\nHowever, the lack of access to computational resources, coupled with the\ncomplexity of the data analysis process, makes it difficult for non-experts to\nappreciate the statistical methods that can be used to analyze their data.\nThis work proposes a new methodology for teaching statistics to non-experts.\nThe methodology involves the use of data from a public dataset to illustrate\nthe main concepts of statistical analysis. The dataset was generated using\nthe R programming language and the open-source package lmtest. The methodology\nis illustrated using the example of linear regression, but it can be applied to\nother statistical methods. The methodology is implemented in the R package\ndataAnalysis. The package can be used to carry out statistical analysis on\ndatasets of any size, and it can be used to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13978494623655913,
          "p": 0.1566265060240964,
          "f": 0.1477272677434144
        },
        "rouge-2": {
          "r": 0.026785714285714284,
          "p": 0.023076923076923078,
          "f": 0.024793383457415104
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.14457831325301204,
          "f": 0.13636363137977806
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.cond-mat/stat-mech/2503.10599v1",
      "true_abstract": "Recently, a thermodynamic bound on correlation times was formulated in [A.\nDechant, J. Garnier-Brun, S.-i. Sasa, Phys. Rev. Lett. 131, 167101 (2023)],\nshowing how the decay of correlations in Langevin dynamics is bounded by\nshort-time fluctuations and dissipation. Whereas these original results only\naddress very long observation times in steady-state dynamics, we here\ngeneralize the respective inequalities to finite observations and general\ninitial conditions. We utilize the connection between correlations and the\nfluctuations of time-integrated density functionals and generalize the direct\nstochastic calculus approach from [C. Dieball and A. Godec, Phys. Rev. Lett.\n130, 087101 (2023)] which paves the way for further generalizations. We address\nthe connection between short and long time scales, as well as the saturation of\nthe bounds via complementary spectral-theoretic arguments. Motivated by the\nspectral insight, we formulate all results also for complex-valued observables.",
      "generated_abstract": "aper we present a new approach to the study of the correlation\nand response of the two-dimensional (2D) Ising model in the presence of\ninteraction. In contrast to the usual approach, in which the interaction\nparameter is set in the thermodynamic limit, we use a finite system. We show\nthat the finite-size effects in the correlations of the 2D Ising model can be\nexplained in terms of the effective interactions of the spin-spin correlation\nfunction of the 1D model, which are determined by the size of the system.\nSimilarly, the response of the system is described by a finite-size effect in\nthe 2D Ising model. To support our theoretical predictions, we perform\nnumerical simulations and present the results. The results of the numerical\nsimulations are in good agreement with the theoretical predictions.\nAdditionally, we use a Monte Carlo method to study the correlation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16346153846153846,
          "p": 0.22666666666666666,
          "f": 0.18994412920945053
        },
        "rouge-2": {
          "r": 0.015037593984962405,
          "p": 0.016666666666666666,
          "f": 0.01581027169304474
        },
        "rouge-l": {
          "r": 0.14423076923076922,
          "p": 0.2,
          "f": 0.16759776049436673
        }
      }
    },
    {
      "paper_id": "math.AP.math/FA/2503.07569v1",
      "true_abstract": "In this paper, we consider second order degenerate parabolic equations with\ncomplex, measurable, and time-dependent coefficients. The degenerate\nellipticity is dictated by a spatial $A_2$-weight. We prove that having a\ngeneralized fundamental solution with upper Gaussian bounds is equivalent to\nMoser's $L^2$-$L^\\infty$ estimates for local weak solutions. In the special\ncase of real coefficients, Moser's $L^2$-$L^\\infty$ estimates are known, which\nprovide an easier proof of Gaussian upper bounds, and a known Harnack\ninequality is then used to derive Gaussian lower bounds.",
      "generated_abstract": "aper, we consider a linear dynamical system defined by\n(1) A-periodic initial data is exponentially attractive if and only if it is\nexponentially asymptotically stable;\n(2) B-periodic initial data is exponentially asymptotically stable if and only\nif it is exponentially asymptotically stable;\n(3) C-periodic initial data is exponentially asymptotically stable if and only\nif it is exponentially asymptotically stable.\n  We study the existence and uniqueness of global attractor for the above\nsystems. The attractors are characterized by the Lyapunov exponents and the\nHamiltonian. We also give the Lyapunov exponents of the attractors for the\nabove dynamical systems.\n  We prove the nonexistence of periodic orbit for A-periodic initial data and\nB-periodic initial data under the assumption of the existence",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.203125,
          "p": 0.24528301886792453,
          "f": 0.22222221726641841
        },
        "rouge-2": {
          "r": 0.02564102564102564,
          "p": 0.025,
          "f": 0.02531645069700467
        },
        "rouge-l": {
          "r": 0.203125,
          "p": 0.24528301886792453,
          "f": 0.22222221726641841
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.01509v1",
      "true_abstract": "A key step in the Bayesian workflow for model building is the graphical\nassessment of model predictions, whether these are drawn from the prior or\nposterior predictive distribution. The goal of these assessments is to identify\nwhether the model is a reasonable (and ideally accurate) representation of the\ndomain knowledge and/or observed data. There are many commonly used visual\npredictive checks which can be misleading if their implicit assumptions do not\nmatch the reality. Thus, there is a need for more guidance for selecting,\ninterpreting, and diagnosing appropriate visualizations. As a visual predictive\ncheck itself can be viewed as a model fit to data, assessing when this model\nfails to represent the data is important for drawing well-informed conclusions.\n  We present recommendations and diagnostic tools to mitigate ad-hoc\ndecision-making in visual predictive checks. These contributions aim to improve\nthe robustness and interpretability of Bayesian model criticism practices. We\noffer recommendations for appropriate visual predictive checks for observations\nthat are: continuous, discrete, or a mixture of the two. We also discuss\ndiagnostics to aid in the selection of visual methods. Specifically, in the\ndetection of an incorrect assumption of continuously-distributed data:\nidentifying when data is likely to be discrete or contain discrete components,\ndetecting and estimating possible bounds in data, and a diagnostic of the\ngoodness-of-fit to data for density plots made through kernel density\nestimates.",
      "generated_abstract": "y investigates the relationship between the U.S. Census Bureau's\nstatistical population data and the American Community Survey (ACS) data from\n2017 to 2021. We conduct a statistical analysis of the geographical\ndistribution of the ACS data with the aim of exploring the spatial patterns of\nthe U.S. population. We apply the geographic information system (GIS) to\nvisualize the data and conduct spatial analysis. The results show that the\npopulation density in the United States has increased significantly over the\nlast decade. The density of the population in the United States has increased\nin all regions except for the U.S. Virgin Islands, which has a low population\ndensity. The density of the population in the United States has increased\nsignificantly in all regions except for the U.S. Virgin Islands, which has a\nlow population density. The density of the population in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11510791366906475,
          "p": 0.25806451612903225,
          "f": 0.1592039758332716
        },
        "rouge-2": {
          "r": 0.014150943396226415,
          "p": 0.03125,
          "f": 0.019480515189746264
        },
        "rouge-l": {
          "r": 0.1079136690647482,
          "p": 0.24193548387096775,
          "f": 0.14925372707705267
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/AP/2502.16118v1",
      "true_abstract": "We study the relationship between the rank of the prior covariance matrix and\nthe local false sign rate in a multivariate empirical Bayes normal mean model.\nIt has been observed that the false sign rate is inflated when the prior\nassigns weight to low-rank covariance matrices. We show that this issue arises\ndue to the rank deficiency of prior covariance matrices and propose an\nadjustment to mitigate it.",
      "generated_abstract": "r introduces a novel approach to modeling longitudinal data using\nthe Gaussian process (GP) regression framework. The model is based on a\nnon-linear GP regression model with a Gaussian process prior that is\nflexible enough to accommodate various types of covariates, as well as\nnon-linear functions of these covariates. This approach is particularly\napplicable to modeling data that have multiple covariates, such as in\nneuroimaging studies, where the number of covariates can be substantial. The\nGP model is parameterized by a covariance function, which represents the\ncorrelation between covariates. The GP prior is a Gaussian mixture, which\nallows the GP to capture the distribution of the covariate values across\ndifferent models, thereby providing a flexible way to model the underlying\ndistribution of covariate values. The GP prior is implemented as a\nmultivariate normal distribution with an unknown",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2978723404255319,
          "p": 0.1794871794871795,
          "f": 0.2239999953075201
        },
        "rouge-2": {
          "r": 0.03278688524590164,
          "p": 0.015873015873015872,
          "f": 0.0213903699356582
        },
        "rouge-l": {
          "r": 0.2553191489361702,
          "p": 0.15384615384615385,
          "f": 0.19199999530752013
        }
      }
    },
    {
      "paper_id": "cs.LG.econ/EM/2503.05800v1",
      "true_abstract": "Understanding consumer choice is fundamental to marketing and management\nresearch, as firms increasingly seek to personalize offerings and optimize\ncustomer engagement. Traditional choice modeling frameworks, such as\nmultinomial logit (MNL) and mixed logit models, impose rigid parametric\nassumptions that limit their ability to capture the complexity of consumer\ndecision-making. This study introduces the Mixture of Experts (MoE) framework\nas a machine learning-driven alternative that dynamically segments consumers\nbased on latent behavioral patterns. By leveraging probabilistic gating\nfunctions and specialized expert networks, MoE provides a flexible,\nnonparametric approach to modeling heterogeneous preferences.\n  Empirical validation using large-scale retail data demonstrates that MoE\nsignificantly enhances predictive accuracy over traditional econometric models,\ncapturing nonlinear consumer responses to price variations, brand preferences,\nand product attributes. The findings underscore MoEs potential to improve\ndemand forecasting, optimize targeted marketing strategies, and refine\nsegmentation practices. By offering a more granular and adaptive framework,\nthis study bridges the gap between data-driven machine learning approaches and\nmarketing theory, advocating for the integration of AI techniques in managerial\ndecision-making and strategic consumer insights.",
      "generated_abstract": "We introduce a new framework for exploring multivariate time series\nmultilevel models (MTS-MLMs) that can simultaneously estimate the parameters\nof a multivariate autoregressive process (AR) and the parameters of a\nmultivariate autoregressive distributed lag (ADL) process. We provide\ncomputational methods for the estimation and inference of the multivariate AR\nand ADL processes. Additionally, we provide the first empirical examples of\ntheoretical results, showing that MTS-MLMs can be used to estimate the\nparameters of a multivariate AR process and the parameters of a multivariate\nADL process simultaneously. The results suggest that MTS-MLMs can be a useful\nalternative to traditional multivariate time series models when dealing with\nhigh-dimensional, high-dimensionality, and multivariate time series.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08270676691729323,
          "p": 0.1896551724137931,
          "f": 0.11518324184424783
        },
        "rouge-2": {
          "r": 0.005847953216374269,
          "p": 0.011494252873563218,
          "f": 0.007751933514514925
        },
        "rouge-l": {
          "r": 0.06766917293233082,
          "p": 0.15517241379310345,
          "f": 0.09424083346728453
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.01708v1",
      "true_abstract": "We develop a pseudo-likelihood theory for rank one matrix estimation problems\nin the high dimensional limit. We prove a variational principle for the\nlimiting pseudo-maximum likelihood which also characterizes the performance of\nthe corresponding pseudo-maximum likelihood estimator. We show that this\nvariational principle is universal and depends only on four parameters\ndetermined by the corresponding null model. Through this universality, we\nintroduce a notion of equivalence for estimation problems of this type and, in\nparticular, show that a broad class of estimation tasks, including community\ndetection, sparse submatrix detection, and non-linear spiked matrix models, are\nequivalent to spiked matrix models. As an application, we obtain a complete\ndescription of the performance of the least-squares (or ``best rank one'')\nestimator for any rank one matrix estimation problem.",
      "generated_abstract": "In this paper we study the statistical properties of the mean of a sum of\nnonnegative random variables. We first establish the central limit theorem for\nthe sum of independent nonnegative random variables and show that it can be\nrepresented as the sum of independent Gaussian random variables. Next, we\nstudy the asymptotic properties of the empirical mean of independent\nnonnegative random variables and show that it converges to a random variable\nwith finite mean and variance. Finally, we propose a method to compute the\nempirical mean of independent nonnegative random variables and derive the\ncentral limit theorem for this quantity as well. The method is based on the\nuse of a random orthogonal matrix and the generalized eigenvalue decomposition.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1875,
          "p": 0.25862068965517243,
          "f": 0.21739129947490035
        },
        "rouge-2": {
          "r": 0.026785714285714284,
          "p": 0.03529411764705882,
          "f": 0.03045684788580047
        },
        "rouge-l": {
          "r": 0.175,
          "p": 0.2413793103448276,
          "f": 0.20289854585171196
        }
      }
    },
    {
      "paper_id": "physics.comp-ph.physics/flu-dyn/2503.10478v1",
      "true_abstract": "A multiscale stochastic-deterministic coupling method is proposed to\ninvestigate the complex interactions between turbulent and rarefied gas flows\nwithin a unified framework. This method intermittently integrates the general\nsynthetic iterative scheme with the shear stress transport turbulence model\ninto the direct simulation Monte Carlo (DSMC) approach, enabling the simulation\nof gas flows across the free-molecular, transition, slip, and turbulent\nregimes. First, the macroscopic synthetic equations, derived directly from\nDSMC, are coupled with the turbulence model to establish a constitutive\nrelation that incorporates not only turbulent and laminar transport\ncoefficients but also higher-order terms accounting for rarefaction effects.\nSecond, the macroscopic properties, statistically sampled over specific time\nintervals in DSMC, along with the turbulent properties provided by the\nturbulence model, serve as initial conditions for solving the macroscopic\nsynthetic equations. Finally, the simulation particles in DSMC are updated\nbased on the macroscopic properties obtained from the synthetic equations.\nNumerical simulations demonstrate that the proposed method asymptotically\nconverges to either the turbulence model or DSMC results, adaptively adjusting\nto different flow regimes. Then, this coupling method is applied to simulate an\nopposing jet surrounded by hypersonic rarefied gas flows, revealing significant\nvariations in surface properties due to the interplay of turbulent and rarefied\neffects. This study presents an efficient methodology for simulating the\ncomplex interplay between rarefied and turbulent flows, establishing a\nfoundational framework for investigating the coupled effects of turbulence,\nhypersonic conditions, and chemical reactions in rarefied gas dynamics in the\nfuture.",
      "generated_abstract": "t a novel experimental and numerical investigation of the\nphysics of a liquid-liquid interface that is driven by a single oscillating\nforcing. The interface is driven by a non-linear harmonic oscillator, whose\nfrequency is varied across a wide range of values. The oscillator is placed\nbetween two planar glass plates, each coated with a single water molecule,\nthus forming a system that is a mixture of liquid and solid states. We\ninvestigate the interplay between the oscillating force and the two-phase\nstructure of the liquid-solid interface. In the absence of the oscillation, the\nliquid-solid interface is fully liquid and the oscillation does not play a role.\nHowever, when the oscillation frequency is high enough, the oscillation\ncouples with the liquid-solid interface and drives a transition from a fully\nliquid to a partially liquid state. In this state",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.2077922077922078,
          "f": 0.14479637555005032
        },
        "rouge-2": {
          "r": 0.01834862385321101,
          "p": 0.03389830508474576,
          "f": 0.023809519252410167
        },
        "rouge-l": {
          "r": 0.09722222222222222,
          "p": 0.18181818181818182,
          "f": 0.1266968280387381
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2502.19862v1",
      "true_abstract": "Decentralized lending protocols within the decentralized finance ecosystem\nenable the lending and borrowing of crypto-assets without relying on\ntraditional intermediaries. Interest rates in these protocols are set\nalgorithmically and fluctuate according to the supply and demand for liquidity.\nIn this study, we propose an agent-based model tailored to a decentralized\nlending protocol and determine the optimal interest rate model. When the\nresponses of the agents are linear with respect to the interest rate, the\noptimal solution is derived from a system of Riccati-type ODEs. For nonlinear\nbehaviors, we propose a Monte-Carlo estimator, coupled with deep learning\ntechniques, to approximate the optimal solution. Finally, after calibrating the\nmodel using block-by-block data, we conduct a risk-adjusted profit and loss\nanalysis of the liquidity pool under industry-standard interest rate models and\nbenchmark them against the optimal interest rate model.",
      "generated_abstract": "We consider the problem of optimal portfolio selection in the presence of\ntwo investors who wish to maximize their expected utility. In particular, we\nfocus on the case of two assets whose prices are both unknown and stochastic,\nand whose prices are correlated with each other. In this case, we derive a\nclosed-form solution for the optimal allocation that is characterized by a\nsingle risk measure. Our result is obtained by solving a stochastic version of\nthe HJB equation, which is an analogue of the Hamilton-Jacobi-Bellman equation\nin the deterministic case.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19101123595505617,
          "p": 0.25757575757575757,
          "f": 0.21935483381977117
        },
        "rouge-2": {
          "r": 0.024193548387096774,
          "p": 0.03529411764705882,
          "f": 0.028708129145395833
        },
        "rouge-l": {
          "r": 0.14606741573033707,
          "p": 0.19696969696969696,
          "f": 0.16774193059396478
        }
      }
    },
    {
      "paper_id": "hep-ex.hep-ex/2503.09773v1",
      "true_abstract": "Current template-based gravitational-wave searches for compact binary mergers\nneglect the general relativistic phenomenon of spin-induced orbital precession.\nOwing to their asymmetric masses, gravitational-waves from neutron star-black\nhole (NSBH) binaries are prime candidates for displaying strong imprints of\nspin-precession. As a result, current searches may be missing a significant\nfraction of the astrophysical population, and the detected NSBH population may\nbe significantly suppressed or biased. Here we report the most sensitive search\nfor NSBH binaries to date by including spin-precession for the first time. We\nanalyze data from the entirety of the third LIGO-Virgo-KAGRA gravitational-wave\nobserving run and show that when accounting for spin-precession, our search is\nup to 100% more sensitive than the search techniques currently adopted by the\nLIGO-Virgo-KAGRA collaboration (for systems with strong precessional effects).\nThis allows us to more tightly constrain the rate of NSBH mergers in the local\nUniverse. Firstly, we focus on a precessing subpopulation of NSBH mergers; the\nlack of observed candidates allows us to place an upper limit on the merger\nrate of $R_{90} = 79\\, \\mathrm{Gpc}^{-3}\\mathrm{yr}^{-1}$ with 90% confidence.\nSecondly, we tighten the overall rate of NSBH mergers; we show that if there is\nno preferred direction of component spin, the rate of NSBH mergers is on\naverage 16% smaller than previously believed. Finally, we report four new\nsubthreshold NSBH candidates, all with strong imprints of spin precession, but\nnote that these are most likely to be of terrestrial origin.",
      "generated_abstract": "t an overview of the LHCb experiment and its results in the field of\ndilepton physics, including the first measurement of the ratio of the\n$\\Lambda\\to\\Lambda\\eta'$ branching fractions, and the first observation of\n$\\Lambda\\to\\Lambda\\eta$ decays in the $\\eta\\eta'$ channel. We also discuss the\nresults of the DY-2023 experiment, which has measured the total cross-section\nof $\\gamma\\gamma\\to\\Lambda\\to\\Lambda\\eta'$ decays. Finally, we review the\nexperimental results from the 2023 LHC Run-3, including the first measurement of\nthe ratio of the $\\Lambda\\to\\Lambda\\eta$ branching fractions and the first\nobservation of $\\Lambda\\to\\Lambda\\eta$ decays in the $\\eta\\eta'$ channel. We\nalso present the new results of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07947019867549669,
          "p": 0.25,
          "f": 0.12060301141486338
        },
        "rouge-2": {
          "r": 0.0273972602739726,
          "p": 0.08450704225352113,
          "f": 0.04137930664708713
        },
        "rouge-l": {
          "r": 0.0728476821192053,
          "p": 0.22916666666666666,
          "f": 0.11055276015858197
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.06020v1",
      "true_abstract": "Using a reaction-diffusion model with free boundaries in one space dimension\nfor a single population species with density $u(t,x)$ and population range\n$[g(t), h(t)]$, we demonstrate that the Allee effects can be eliminated if the\nspecies maintains its population density at a suitable level at the range\nboundary by advancing or retreating the fronts. It is proved that with such a\nstrategy at the range edge the species can invade the environment successfully\nwith all admissible initial populations, exhibiting the dynamics of super\ninvaders. Numerical simulations are used to help understand what happens if the\npopulation density level at the range boundary is maintained at other levels.\nIf the invading cane toads in Australia used this strategy at the range\nboundary to become a super invader, then our results may explain why toads near\nthe invading front evolve to have longer legs and run faster.",
      "generated_abstract": "EM model is widely used to study the structure and dynamics of\nbiological membranes, but it is limited by the difficulty of determining the\nsolvent molecular concentration in 1D-LTEM images. This work presents a\nnovel 1D-LTEM model that extends the 1D-LTEM model by adding the solvent\nmolecular concentration. The model is based on the 1D-LTEM model and uses the\nsolvent molecular concentration to adjust the effective solvent density\nbetween the membrane and the solvent. The effect of the solvent molecular\nconcentration on the structure and dynamics of the membrane is analyzed using\nthe 1D-LTEM model. The results show that the solvent molecular concentration\naffects the structure of the membrane, such as the length of the lipid bilayer,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1595744680851064,
          "p": 0.2727272727272727,
          "f": 0.20134227722174688
        },
        "rouge-2": {
          "r": 0.023076923076923078,
          "p": 0.034482758620689655,
          "f": 0.02764976478158464
        },
        "rouge-l": {
          "r": 0.1276595744680851,
          "p": 0.21818181818181817,
          "f": 0.16107382084590796
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/GA/2503.10532v1",
      "true_abstract": "Type II Cepheids (T2Ceps), alongside RR Lyrae stars, serve as important\ndistance indicators for old population II stars due to their period-luminosity\n(PL) relations. However, studies of these relations in the Sloan photometric\nsystem are rather limited in the literature. Our goal is to calibrate PL\nrelations (and their counterparts in Wesenheit magnitudes) in the\nSloan-Pan-STARSS gP1rP1iP1 bands for Galactic T2Ceps located in the vicinity of\nthe Sun. We collected data for 16 T2Ceps of the BLHer type and 17 of the WVir\ntype using 40 cm telescopes of the Las Cumbres Observatory Global Telescope\nNetwork. Geometric parallaxes were adopted from Gaia Data Release 3. We have\ncalibrated PL and period-Wesenheit relations for Milky Way BLHer and WVir stars\nin the solar neighborhood, as well as for a combined sample of both types. The\nrelationships derived here will allow to determine the distances to T2Ceps that\nwill be discovered by the Legacy Survey of Space and Time survey and, in turn,\nto probe the extended halo of the Milky Way, as well as the halos of nearby\ngalaxies. To the best of our knowledge, the relations derived in this study are\nthe first for Milky Way T2Ceps in the Sloan bands.",
      "generated_abstract": "t the results of a 10-year observation of the planetary nebula\nnucleus NGC 2550 in the V-band using the 4-meter telescope of the National\nOptical Astronomy Observatory (NOAO). Our observations cover a period of\napproximately 10 years, from 2015 September 12 to 2025 September 29. The\nobserved V-band light curve shows a single peak with a maximum flux of 2.37\nmag and a peak-to-peak amplitude of 0.52 mag. The light curve was fitted with a\nsinusoidal function, yielding a decay time constant of 4.84 years. We also\npresented a new light curve of the planetary nebula obtained with the 4-meter\ntelescope at the Observatorio del Teide (",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09448818897637795,
          "p": 0.16,
          "f": 0.11881187651945907
        },
        "rouge-2": {
          "r": 0.010752688172043012,
          "p": 0.02,
          "f": 0.013986009438115812
        },
        "rouge-l": {
          "r": 0.09448818897637795,
          "p": 0.16,
          "f": 0.11881187651945907
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2411.13559v1",
      "true_abstract": "Financial markets are nonlinear with complexity, where different types of\nassets are traded between buyers and sellers, each having a view to maximize\ntheir Return on Investment (ROI). Forecasting market trends is a challenging\ntask since various factors like stock-specific news, company profiles, public\nsentiments, and global economic conditions influence them. This paper describes\na daily price directional predictive system of financial instruments,\naddressing the difficulty of predicting short-term price movements. This paper\nwill introduce the development of a novel trading system methodology by\nproposing a two-layer Composing Ensembles architecture, optimized through grid\nsearch, to predict whether the price will rise or fall the next day. This\nstrategy was back-tested on a wide range of financial instruments and time\nframes, demonstrating an improvement of 20% over the benchmark, representing a\nstandard investment strategy.",
      "generated_abstract": "r examines the performance of a novel approach to quantitative\nfundamental analysis (QFA), a novel model for forecasting stock prices that\nintegrates machine learning and deep learning. The proposed approach is based\non a neural network that is trained to generate price forecasts for individual\nstocks, with the aim of improving stock market forecasting accuracy. The\nproposed methodology leverages the power of deep learning, particularly its\nability to capture complex relationships in data, to generate price forecasts\nthat are more accurate than traditional machine learning methods. This paper\nexamines the performance of the proposed model across various datasets and\ncompares it to other state-of-the-art methods, demonstrating its potential for\nimproved stock market forecasting accuracy. The results of the study indicate\nthat the proposed model is capable of generating more accurate price forecasts\nthan traditional machine learning methods, with a mean absolute error of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16346153846153846,
          "p": 0.20987654320987653,
          "f": 0.1837837788610666
        },
        "rouge-2": {
          "r": 0.03076923076923077,
          "p": 0.03305785123966942,
          "f": 0.03187250496658862
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.16049382716049382,
          "f": 0.1405405356178234
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.16584v1",
      "true_abstract": "Recent advancements in audio tokenization have significantly enhanced the\nintegration of audio capabilities into large language models (LLMs). However,\naudio understanding and generation are often treated as distinct tasks,\nhindering the development of truly unified audio-language models. While\ninstruction tuning has demonstrated remarkable success in improving\ngeneralization and zero-shot learning across text and vision, its application\nto audio remains largely unexplored. A major obstacle is the lack of\ncomprehensive datasets that unify audio understanding and generation. To\naddress this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset\ncovering 80 diverse tasks across speech, music, and sound domains, with over\n100 million instances. Audio-FLAN lays the foundation for unified\naudio-language models that can seamlessly handle both understanding (e.g.,\ntranscription, comprehension) and generation (e.g., speech, music, sound) tasks\nacross a wide range of audio domains in a zero-shot manner. The Audio-FLAN\ndataset is available on HuggingFace and GitHub and will be continuously\nupdated.",
      "generated_abstract": "r presents a novel system for automatic generation of audio-visual\naudio narratives, called AVAN. AVAN is an audio-visual narrative generator\nthat integrates audio-visual generation and music generation. The audio-visual\ngeneration part is based on an audio-visual pre-trained model and the music\ngeneration part is based on a music model. AVAN generates audio-visual narratives\nbased on the input audio and the input text. The audio-visual narratives are\nthen converted to the corresponding music, and then the music is converted to\nthe corresponding audio. AVAN is designed to be customizable and extensible,\nallowing users to customize the audio-visual generation model and the music\ngeneration model. The system can also be integrated with various music\ngeneration models to further enhance its capabilities. AVAN is also open-source\nand available for download on GitHub.\n  This paper presents the architecture of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2072072072072072,
          "p": 0.36507936507936506,
          "f": 0.2643678114724535
        },
        "rouge-2": {
          "r": 0.007042253521126761,
          "p": 0.009174311926605505,
          "f": 0.00796812257647007
        },
        "rouge-l": {
          "r": 0.1891891891891892,
          "p": 0.3333333333333333,
          "f": 0.24137930572532704
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.15131v1",
      "true_abstract": "We study the fundamental problem of calibrating a linear binary classifier of\nthe form $\\sigma(\\hat{w}^\\top x)$, where the feature vector $x$ is Gaussian,\n$\\sigma$ is a link function, and $\\hat{w}$ is an estimator of the true linear\nweight $w^\\star$. By interpolating with a noninformative $\\textit{chance\nclassifier}$, we construct a well-calibrated predictor whose interpolation\nweight depends on the angle $\\angle(\\hat{w}, w_\\star)$ between the estimator\n$\\hat{w}$ and the true linear weight $w_\\star$. We establish that this angular\ncalibration approach is provably well-calibrated in a high-dimensional regime\nwhere the number of samples and features both diverge, at a comparable rate.\nThe angle $\\angle(\\hat{w}, w_\\star)$ can be consistently estimated.\nFurthermore, the resulting predictor is uniquely $\\textit{Bregman-optimal}$,\nminimizing the Bregman divergence to the true label distribution within a\nsuitable class of calibrated predictors. Our work is the first to provide a\ncalibration strategy that satisfies both calibration and optimality properties\nprovably in high dimensions. Additionally, we identify conditions under which a\nclassical Platt-scaling predictor converges to our Bregman-optimal calibrated\nsolution. Thus, Platt-scaling also inherits these desirable properties provably\nin high dimensions.",
      "generated_abstract": "This paper deals with the estimation of two-way random graphs with\ntwo types of edges. We consider the case where the graph is modeled as a\ngraphical model with a random graph structure and a random graph distribution.\nWe propose a method based on a mixture of two models, where the two models\ndepend on the edge type. We show that under some conditions the mixture model\nis well-defined and the mixing process is transiently stable. In this way, we\nestablish a connection between the two-way random graph model and the graphical\nmodel proposed by Boucheron et al. (2017).",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1504424778761062,
          "p": 0.2833333333333333,
          "f": 0.19653178737679186
        },
        "rouge-2": {
          "r": 0.03067484662576687,
          "p": 0.054945054945054944,
          "f": 0.03937007414191882
        },
        "rouge-l": {
          "r": 0.12389380530973451,
          "p": 0.23333333333333334,
          "f": 0.16184970645193636
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.physics/data-an/2503.09328v1",
      "true_abstract": "A Schr\\\"odinger bridge is the most probable time-dependent probability\ndistribution that connects an initial probability distribution $w_{i}$ to a\nfinal one $w_{f}$. The problem has been solved and widely used for the case of\nsimple Brownian evolution (non-interacting particles). It is related to the\nproblem of entropy regularized Wasserstein optimal transport. In this article,\nwe generalize Brownian bridges to systems of interacting particles. We derive\nsome equations for the forward and backward single particle ``wave-functions''\nwhich allow to compute the most probable evolution of the single-particle\nprobability between the initial and final distributions.",
      "generated_abstract": "In a recent work, we introduced a framework for the study of random matrices\nwith non-Hermitian entries. In this work, we extend our framework to the case\nof random matrices with non-Hermitian entries and Hermitian matrices. We show\nthat the properties of the matrix that are independent of the sign of the\nentries are conserved. We further derive a formula for the average of the\ntrace of the matrix. We also provide a new expression for the average of the\ntrace of the matrix in terms of the average of the absolute value of the\nentries. Finally, we derive an expression for the average of the trace of the\nmatrix in terms of the average of the trace of the Hermitian matrix. We also\nshow that the average of the trace of the matrix is independent of the sign of\nthe entries.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22388059701492538,
          "p": 0.3125,
          "f": 0.2608695603538753
        },
        "rouge-2": {
          "r": 0.06818181818181818,
          "p": 0.0759493670886076,
          "f": 0.07185628243967189
        },
        "rouge-l": {
          "r": 0.22388059701492538,
          "p": 0.3125,
          "f": 0.2608695603538753
        }
      }
    },
    {
      "paper_id": "eess.SP.math/GR/2503.09398v1",
      "true_abstract": "Incorporating mathematical properties of a wireless policy to be learned into\nthe design of deep neural networks (DNNs) is effective for enhancing learning\nefficiency. Multi-user precoding policy in multi-antenna system, which is the\nmapping from channel matrix to precoding matrix, possesses a permutation\nequivariance property, which has been harnessed to design the parameter sharing\nstructure of the weight matrix of DNNs. In this paper, we study a stronger\nproperty than permutation equivariance, namely unitary equivariance, for\nprecoder learning. We first show that a DNN with unitary equivariance designed\nby further introducing parameter sharing into a permutation equivariant DNN is\nunable to learn the optimal precoder. We proceed to develop a novel non-linear\nweighting process satisfying unitary equivariance and then construct a joint\nunitary and permutation equivariant DNN. Simulation results demonstrate that\nthe proposed DNN not only outperforms existing learning methods in learning\nperformance and generalizability but also reduces training complexity.",
      "generated_abstract": "aper, we introduce a novel spectral-domain formulation for the\nintegral equation method. This formulation is based on a spectral integral\ntransform, which is used to convert a forward problem into a backward problem.\nWe develop a novel spectral-domain implementation of the transform, which\noffers several advantages over existing implementations. In particular, the\ntransform is non-singular, which allows for efficient implementation and\ncomputational efficiency. We demonstrate the efficiency of the spectral-domain\ntransform by comparing it to the traditional spectral-domain transform. We also\npresent an analysis of the accuracy of the spectral-domain transform. This\nanalysis reveals that the spectral-domain transform produces accurate results\nfor certain forward problems. Additionally, we show that the spectral-domain\ntransform can be used to implement the integral equation method on multi-layer\narrays. This multi-layer implementation can be used to solve forward problems\nwith high spatial resolution. We also provide",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24242424242424243,
          "p": 0.32,
          "f": 0.2758620640606421
        },
        "rouge-2": {
          "r": 0.04861111111111111,
          "p": 0.05982905982905983,
          "f": 0.05363984179680321
        },
        "rouge-l": {
          "r": 0.2222222222222222,
          "p": 0.29333333333333333,
          "f": 0.2528735583135157
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.09150v1",
      "true_abstract": "Personalization is a critical yet often overlooked factor in boosting\nproductivity and wellbeing in knowledge-intensive workplaces to better address\nindividual preferences. Existing tools typically offer uniform guidance whether\nauto-generating email responses or prompting break reminders without accounting\nfor individual behavioral patterns or stress triggers. We introduce AdaptAI, a\nmultimodal AI solution combining egocentric vision and audio, heart and motion\nactivities, and the agentic workflow of Large Language Models LLMs to deliver\nhighly personalized productivity support and context-aware well-being\ninterventions. AdaptAI not only automates peripheral tasks (e.g. drafting\nsuccinct document summaries, replying to emails etc.) but also continuously\nmonitors the users unique physiological and situational indicators to\ndynamically tailor interventions such as micro-break suggestions or exercise\nprompts, at the exact point of need. In a preliminary study with 15\nparticipants, AdaptAI demonstrated significant improvements in task throughput\nand user satisfaction by anticipating user stressors and streamlining daily\nworkflows.",
      "generated_abstract": "llenge in modern deep learning is the development of scalable,\ntransformer-based models that can effectively leverage multi-scale information\nfrom the input image. The Transformer's multi-head attention mechanism\nenables efficient and effective feature aggregation across various image scales,\nbut it also introduces significant computational overhead and memory\nconsumption due to the repeated use of the same head in the multi-head\nattention. This study proposes a novel design for efficient and effective\nfeature aggregation across various image scales. By implementing a\nthree-stage strategy, the proposed method effectively reduces the number of\nattention heads and the number of attention layers without compromising\nperformance. The proposed method significantly reduces the overhead in\nattention heads and attention layers compared to the original Transformer\narchitecture, while maintaining the same level of performance. This approach\nreduces the computational complexity of the model and the memory requirements\nby over 70%,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.112,
          "p": 0.16279069767441862,
          "f": 0.13270141697176632
        },
        "rouge-2": {
          "r": 0.006756756756756757,
          "p": 0.008333333333333333,
          "f": 0.007462681621745314
        },
        "rouge-l": {
          "r": 0.096,
          "p": 0.13953488372093023,
          "f": 0.11374407100020237
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2407.10284v2",
      "true_abstract": "``Self-Organised Criticality'' (SOC) is the mechanism by which complex\nsystems spontaneously settle close to a *critical point*, at the edge between\nstability and chaos, and characterized by fat-tailed fluctuations and\nlong-memory correlations. Such a scenario may explain why insignificant\nperturbations can generate large disruptions, through the propagation of\n``avalanches'' across the system. In this short review, we discuss how SOC\ncould offer a plausible solution to the excess volatility puzzle in financial\nmarkets and the analogue ``small shocks, large business cycle puzzle'' for the\neconomy at large, as initially surmised by Per Bak et al. in 1993. We argue\nthat in general the quest for efficiency and the necessity of *resilience* may\nbe mutually incompatible and require specific policy considerations.",
      "generated_abstract": "The financial markets have been revolutionized by the growth of big data\nin recent years. In particular, the use of big data in the field of financial\ninvestment has been one of the main trends. Big data in finance includes the\nanalysis of various types of data from financial institutions, such as\ntransaction data, market data, and other types of data. In this paper, we\npresent an overview of the main types of big data in finance, including\ntransaction data, market data, and other types of data. We also discuss the\nimportance of big data in financial markets and the challenges and opportunities\nthat it presents.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14736842105263157,
          "p": 0.25,
          "f": 0.18543045890969703
        },
        "rouge-2": {
          "r": 0.0423728813559322,
          "p": 0.06172839506172839,
          "f": 0.05025125145425667
        },
        "rouge-l": {
          "r": 0.1368421052631579,
          "p": 0.23214285714285715,
          "f": 0.1721854257971143
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/CY/2503.10264v1",
      "true_abstract": "While peer review enhances writing and research quality, harsh feedback can\nfrustrate and demotivate authors. Hence, it is essential to explore how\ncritiques should be delivered to motivate authors and enable them to keep\niterating their work. In this study, we explored the impact of appending an\nautomatically generated positive summary to the peer reviews of a writing task,\nalongside varying levels of overall evaluations (high vs. low), on authors'\nfeedback reception, revision outcomes, and motivation to revise. Through a 2x2\nonline experiment with 137 participants, we found that adding an AI-reframed\npositive summary to otherwise harsh feedback increased authors' critique\nacceptance, whereas low overall evaluations of their work led to increased\nrevision efforts. We discuss the implications of using AI in peer feedback,\nfocusing on how AI-driven critiques can influence critique acceptance and\nsupport research communities in fostering productive and friendly peer feedback\npractices.",
      "generated_abstract": "We present an analysis of the implementation of the LMU-VLU framework for\nthe classification of breast cancer from mammograms. We show that the\nalgorithmic approach used to train the LMU-VLU classifier does not provide a\nrobust solution to the problem of data augmentation. In particular, the\nclassifier exhibits large variations in its performance when it is trained\nwith the same data augmentation technique used for the validation set. This\nleads to an increase in the false positive rate for this dataset. To address\nthis issue, we propose a simple modification to the LMU-VLU classifier that\nreduces the variation in its performance. Our proposed method is validated on\nthe LMU-VLU test set. The results show that the modified classifier performs\nbetter than the original classifier.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16,
          "p": 0.20253164556962025,
          "f": 0.17877094478948863
        },
        "rouge-2": {
          "r": 0.014388489208633094,
          "p": 0.018018018018018018,
          "f": 0.01599999506272152
        },
        "rouge-l": {
          "r": 0.16,
          "p": 0.20253164556962025,
          "f": 0.17877094478948863
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.00725v1",
      "true_abstract": "We propose a machine-learning tool that yields causal inference on text in\nrandomized trials. Based on a simple econometric framework in which text may\ncapture outcomes of interest, our procedure addresses three questions: First,\nis the text affected by the treatment? Second, which outcomes is the effect on?\nAnd third, how complete is our description of causal effects? To answer all\nthree questions, our approach uses large language models (LLMs) that suggest\nsystematic differences across two groups of text documents and then provides\nvalid inference based on costly validation. Specifically, we highlight the need\nfor sample splitting to allow for statistical validation of LLM outputs, as\nwell as the need for human labeling to validate substantive claims about how\ndocuments differ across groups. We illustrate the tool in a proof-of-concept\napplication using abstracts of academic manuscripts.",
      "generated_abstract": "The paper introduces a novel methodology to estimate the value of information\nin a dynamic, heterogeneous, and imperfectly observed environment. The method\nis based on a novel decomposition of the value of information and is\napplicable to a wide range of dynamic, heterogeneous, and imperfectly observed\nenvironments. We demonstrate the method's effectiveness through simulation\nexperiments and an empirical application to a real-world financial data\ndataset.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11458333333333333,
          "p": 0.2619047619047619,
          "f": 0.15942028562066804
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.05454545454545454,
          "f": 0.03208555734507761
        },
        "rouge-l": {
          "r": 0.09375,
          "p": 0.21428571428571427,
          "f": 0.13043477837429124
        }
      }
    },
    {
      "paper_id": "q-fin.PM.econ/GN/2503.07498v1",
      "true_abstract": "We examine the problem of optimal portfolio allocation within the framework\nof utility theory. We apply exponential utility to derive the optimal\ndiversification strategy and logarithmic utility to determine the optimal\nleverage. We enhance existing methodologies by incorporating compound\nprobability distributions to model the effects of both statistical and\nnon-stationary uncertainties. Additionally, we extend the maximum expected\nutility objective by including the variance of utility in the objective\nfunction, which we term generalized mean-variance. In the case of logarithmic\nutility, it provides a natural explanation for the half-Kelly criterion, a\nconcept widely used by practitioners.",
      "generated_abstract": "This paper investigates the application of the Expectation-Maximization (EM)\nalgorithm to the problem of optimal investment. We present a theoretical\nfoundation for the EM algorithm, showing that the EM algorithm is equivalent\nto solving an optimization problem with an expectation and a maximum. We then\nderive an EM algorithm for optimal investment, where the expectation is\nobtained using a novel technique, and the maximum is achieved by maximizing a\nutility function. We apply this algorithm to the problem of optimal investment\nin a one-period economy with risk aversion. We show that our algorithm\nprovides an optimal solution with a small regret, and the regret is zero when\nthe investment horizon is infinite.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24615384615384617,
          "p": 0.25396825396825395,
          "f": 0.24999999500122078
        },
        "rouge-2": {
          "r": 0.06593406593406594,
          "p": 0.058823529411764705,
          "f": 0.06217616081935126
        },
        "rouge-l": {
          "r": 0.24615384615384617,
          "p": 0.25396825396825395,
          "f": 0.24999999500122078
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.01704v1",
      "true_abstract": "Discrete Choice Modelling serves as a robust framework for modelling human\nchoice behaviour across various disciplines. Building a choice model is a semi\nstructured research process that involves a combination of a priori\nassumptions, behavioural theories, and statistical methods. This complex set of\ndecisions, coupled with diverse workflows, can lead to substantial variability\nin model outcomes. To better understand these dynamics, we developed the\nSerious Choice Modelling Game, which simulates the real world modelling process\nand tracks modellers' decisions in real time using a stated preference dataset.\nParticipants were asked to develop choice models to estimate Willingness to Pay\nvalues to inform policymakers about strategies for reducing noise pollution.\nThe game recorded actions across multiple phases, including descriptive\nanalysis, model specification, and outcome interpretation, allowing us to\nanalyse both individual decisions and differences in modelling approaches.\nWhile our findings reveal a strong preference for using data visualisation\ntools in descriptive analysis, it also identifies gaps in missing values\nhandling before model specification. We also found significant variation in the\nmodelling approach, even when modellers were working with the same choice\ndataset. Despite the availability of more complex models, simpler models such\nas Multinomial Logit were often preferred, suggesting that modellers tend to\navoid complexity when time and resources are limited. Participants who engaged\nin more comprehensive data exploration and iterative model comparison tended to\nachieve better model fit and parsimony, which demonstrate that the\nmethodological choices made throughout the workflow have significant\nimplications, particularly when modelling outcomes are used for policy\nformulation.",
      "generated_abstract": "This paper proposes a novel method for estimating fixed effects models\nwith heterogeneous treatment effects. The method uses a flexible model for\nestimating the average treatment effect and a penalized maximum likelihood\nestimator for the fixed effects parameters. It is shown that the proposed\nmethod can recover the correct treatment effects even when the average\ntreatment effect is not monotonic. Furthermore, it is shown that the proposed\nmethod can recover the correct fixed effects parameters even when the average\ntreatment effect is not monotonic and the treatment effects are heterogeneous.\nTheoretical and empirical results are provided to illustrate the effectiveness\nof the proposed method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10526315789473684,
          "p": 0.36,
          "f": 0.1628959241006532
        },
        "rouge-2": {
          "r": 0.008032128514056224,
          "p": 0.02666666666666667,
          "f": 0.012345675454390598
        },
        "rouge-l": {
          "r": 0.09941520467836257,
          "p": 0.34,
          "f": 0.1538461503449971
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.10196v1",
      "true_abstract": "In this paper, we present an error estimate for the filtered Lie splitting\nscheme applied to the Zakharov system, characterized by solutions exhibiting\nvery low regularity across all dimensions. Our findings are derived from the\napplication of multilinear estimates established within the framework of\ndiscrete Bourgain spaces. Specifically, we demonstrate that when the solution\n$(E,z,z_t) \\in H^{s+r+1/2}\\times H^{s+r}\\times H^{s+r-1}$, the error in\n$H^{r+1/2}\\times H^{r}\\times H^{r-1}$ is $\\mathcal{O}(\\tau^{s/2})$ for\n$s\\in(0,2]$, where $r=\\max(0,\\frac d2-1)$. To the best of our knowledge, this\nrepresents the first explicit error estimate for the splitting method based on\nthe original Zakharov system, as well as the first instance where low\nregularity error estimates for coupled equations have been considered within\nthe Bourgain framework. Furthermore, numerical experiments confirm the validity\nof our theoretical results.",
      "generated_abstract": "In this paper, we derive a formula for the derivative of the discrete\ndistribution function of the Gaussian random vector with the mean zero and\ncovariance matrix $2\\lambda I$ where $\\lambda > 0$ is a given constant. We\nalso prove that this formula can be used to estimate the mean and covariance\nmatrix of the Gaussian random vector.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14606741573033707,
          "p": 0.3170731707317073,
          "f": 0.1999999956816569
        },
        "rouge-2": {
          "r": 0.03418803418803419,
          "p": 0.08163265306122448,
          "f": 0.04819276692335644
        },
        "rouge-l": {
          "r": 0.12359550561797752,
          "p": 0.2682926829268293,
          "f": 0.16923076491242617
        }
      }
    },
    {
      "paper_id": "math.GM.math/GM/2502.14877v1",
      "true_abstract": "In this paper is studied the problem concerning the angle between two\nsubspaces of arbitrary dimensions in Euclidean space $E_{n}$. It is proven that\nthe angle between two subspaces is equal to the angle between their orthogonal\nsubspaces. Using the eigenvalues and eigenvectors of corresponding matrix\nrepresentations, there are introduced principal values and principal subspaces.\nTheir geometrical interpretation is also given together with the canonical\nrepresentation of the two subspaces. The canonical matrix for the two subspaces\nis introduced and its properties of duality are obtained. Here obtained results\nexpand the classic results given in [1,2].",
      "generated_abstract": "pt of the $A_\\infty$-algebra is introduced, as an extension of the\nalgebraic $A_\\infty$-algebra, and its properties are studied.\n  The $A_\\infty$-algebras are viewed as $A_\\infty$-algebras, and a natural\ncomposition law is defined. It is shown that the composition law satisfies a\nnumber of conditions, including the $A_\\infty$-algebra law and the $A_\\infty$\nequation. It is also shown that the $A_\\infty$-algebra is equivalent to the\nalgebraic $A_\\infty$-algebra, and a $2$-morphism between $A_\\infty$-algebras is a\n$2$-morphism between algebraic $A_\\infty$-algebras.\n  The $A_\\infty$-algebras are then used to construct the homotopy $A_\\infty$-algebras\nand the homotopy $A_\\infty$-morphisms, which general",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2413793103448276,
          "p": 0.30434782608695654,
          "f": 0.26923076429733733
        },
        "rouge-2": {
          "r": 0.08235294117647059,
          "p": 0.09722222222222222,
          "f": 0.08917196955657457
        },
        "rouge-l": {
          "r": 0.22413793103448276,
          "p": 0.2826086956521739,
          "f": 0.24999999506656806
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.16277v1",
      "true_abstract": "Machine learning is critical for innovation and efficiency in financial\nmarkets, offering predictive models and data-driven decision-making. However,\nchallenges such as missing data, lack of transparency, untimely updates,\ninsecurity, and incompatible data sources limit its effectiveness. Blockchain\ntechnology, with its transparency, immutability, and real-time updates,\naddresses these challenges. We present a framework for integrating\nhigh-frequency on-chain data with low-frequency off-chain data, providing a\nbenchmark for addressing novel research questions in economic mechanism design.\nThis framework generates modular, extensible datasets for analyzing economic\nmechanisms such as the Transaction Fee Mechanism, enabling multi-modal insights\nand fairness-driven evaluations. Using four machine learning techniques,\nincluding linear regression, deep neural networks, XGBoost, and LSTM models, we\ndemonstrate the framework's ability to produce datasets that advance financial\nresearch and improve understanding of blockchain-driven systems. Our\ncontributions include: (1) proposing a research scenario for the Transaction\nFee Mechanism and demonstrating how the framework addresses previously\nunexplored questions in economic mechanism design; (2) providing a benchmark\nfor financial machine learning by open-sourcing a sample dataset generated by\nthe framework and the code for the pipeline, enabling continuous dataset\nexpansion; and (3) promoting reproducibility, transparency, and collaboration\nby fully open-sourcing the framework and its outputs. This initiative supports\nresearchers in extending our work and developing innovative financial\nmachine-learning models, fostering advancements at the intersection of machine\nlearning, blockchain, and economics.",
      "generated_abstract": "r explores the potential of using the concept of \"economic\nchallenge\" to advance the development of economic thinking and research.\nChallenges are considered to be \"those that present significant barriers to\neconomic progress\" (Bruno, 1984). This definition is based on the concept of\n\"difficulties\" and the notion of \"challenges\" as problems that need to be\nsolved. The paper discusses the relevance of the concept of economic\nchallenge to the development of economic theory and research. The concept of\neconomic challenge is used to analyze the theoretical development of economic\nthought in recent decades. The paper proposes a method for analyzing the\ndevelopment of economic thought and research and identifies challenges that\nneed to be overcome to achieve the development of economic theory and\nresearch. The paper concludes that economic theory and research must be\ndeveloped based",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11805555555555555,
          "p": 0.26153846153846155,
          "f": 0.16267942155170453
        },
        "rouge-2": {
          "r": 0.014563106796116505,
          "p": 0.03,
          "f": 0.0196078387372388
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.24615384615384617,
          "f": 0.1531100435612739
        }
      }
    },
    {
      "paper_id": "math-ph.nlin/SI/2503.08018v1",
      "true_abstract": "In this paper we consider the Toda lattice $(\\boldsymbol{p}(t);\n\\boldsymbol{q}(t))$ at thermal equilibrium, meaning that its variables $(p_i)$\nand $(e^{q_i-q_{i+1}})$ are independent Gaussian and Gamma random variables,\nrespectively. We justify the notion from the physics literature that this model\ncan be thought of as a dense collection of solitons (or \"soliton gas'') by, (i)\nprecisely defining the locations of these solitons; (ii) showing that local\ncharges and currents for the Toda lattice are well-approximated by simple\nfunctions of the soliton data; and (iii) proving an asymptotic scattering\nrelation that governs the dynamics of the soliton locations. Our arguments are\nbased on analyzing properties about eigenvector entries of the Toda lattice's\n(random) Lax matrix, particularly, their rates of exponential decay and their\nevolutions under inverse scattering.",
      "generated_abstract": "t a new method to construct high-order, non-quadratic, linear\nnonlinear Schr\\\"odinger (NLS) equations. This method involves the use of the\nMellin-Barnes (MB) representation of the NLS equation. MB is a powerful\napproach that has been successfully applied to a variety of problems in\nphysics, chemistry, and biology. The MB representation is particularly useful\nfor describing the evolution of non-quadratic terms in NLS equations, which\ntypically arise in applications in physics, chemistry, and biology. In this\narticle, we apply the MB representation to the construction of high-order,\nnon-quadratic NLS equations. We demonstrate that the MB representation is\neffective in describing non-quadratic terms in NLS equations. We further show\nthat the MB representation provides a powerful tool for constructing high-order\nnon-quadratic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10638297872340426,
          "p": 0.14925373134328357,
          "f": 0.12422359762509183
        },
        "rouge-2": {
          "r": 0.01694915254237288,
          "p": 0.02,
          "f": 0.018348618887300393
        },
        "rouge-l": {
          "r": 0.10638297872340426,
          "p": 0.14925373134328357,
          "f": 0.12422359762509183
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.03019v1",
      "true_abstract": "In this paper, we define an underlying data generating process that allows\nfor different magnitudes of cross-sectional dependence, along with time series\nautocorrelation. This is achieved via high-dimensional moving average processes\nof infinite order (HDMA($\\infty$)). Our setup and investigation integrates and\nenhances homogenous and heterogeneous panel data estimation and testing in a\nunified way. To study HDMA($\\infty$), we extend the Beveridge-Nelson\ndecomposition to a high-dimensional time series setting, and derive a complete\ntoolkit set. We exam homogeneity versus heterogeneity using Gaussian\napproximation, a prevalent technique for establishing uniform inference. For\npost-testing inference, we derive central limit theorems through Edgeworth\nexpansions for both homogenous and heterogeneous settings. Additionally, we\nshowcase the practical relevance of the established asymptotic properties by\nrevisiting the common correlated effects (CCE) estimators, and a classic\nnonstationary panel data process. Finally, we verify our theoretical findings\nvia extensive numerical studies using both simulated and real datasets.",
      "generated_abstract": "the impact of the COVID-19 pandemic on the economy and its\ninfluence on the behavior of the stock market. We consider the case of the\nGerman stock market, where the stock market experienced a significant drop in\nearnings in March 2020 due to the pandemic. We develop a model for the stock\nmarket based on the factor model and analyze the impact of the pandemic on the\nmarket. The model consists of five factors: the factor \"economic conditions\",\nthe factor \"inflation\", the factor \"foreign exchange rate\", the factor \"fiscal\npolicy\", and the factor \"government bond yields\". We use the Bayesian\nestimation method and compare the results of the model with those of the\nstandard factor model. The Bayesian estimation method allows us to investigate\nthe influence of the pandemic on the stock market, including the impact of\ninflation, fiscal policy,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09734513274336283,
          "p": 0.16666666666666666,
          "f": 0.12290502327767565
        },
        "rouge-2": {
          "r": 0.006993006993006993,
          "p": 0.009345794392523364,
          "f": 0.007999995103682996
        },
        "rouge-l": {
          "r": 0.08849557522123894,
          "p": 0.15151515151515152,
          "f": 0.11173183892013377
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.05649v1",
      "true_abstract": "Recent advancements in controllable expressive speech synthesis, especially\nin text-to-speech (TTS) models, have allowed for the generation of speech with\nspecific styles guided by textual descriptions, known as style prompts. While\nthis development enhances the flexibility and naturalness of synthesized\nspeech, there remains a significant gap in understanding how these models\nhandle vague or abstract style prompts. This study investigates the potential\ngender bias in how models interpret occupation-related prompts, specifically\nexamining their responses to instructions like \"Act like a nurse\". We explore\nwhether these models exhibit tendencies to amplify gender stereotypes when\ninterpreting such prompts. Our experimental results reveal the model's tendency\nto exhibit gender bias for certain occupations. Moreover, models of different\nsizes show varying degrees of this bias across these occupations.",
      "generated_abstract": "We introduce the \\textbf{Practical Bilingual BERT for Speech-Language\nLanguage Models} (PB-Bert), a scalable, efficient, and highly accurate\nbilingual speech-language language model (Bert) for speech recognition.\nPB-Bert is built on the original BERT architecture with a novel bilingual\nencoder, which allows it to process both English and Chinese input simultaneously\nwithout compromising accuracy. Our approach achieves state-of-the-art performance\nin both English and Mandarin, outperforming existing models by a substantial\nmargin. The code and models are available at https://github.com/pw-lab/PB-Bert.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12903225806451613,
          "p": 0.18181818181818182,
          "f": 0.15094339137059468
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.18181818181818182,
          "f": 0.15094339137059468
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.astro-ph/EP/2503.08988v1",
      "true_abstract": "We propose a new method for investigating atmospheric inhomogeneities in\nexoplanets through transmission spectroscopy. Our approach links chromatic\nvariations in conventional transit model parameters (central transit time,\ntotal and full durations, and transit depth) to atmospheric asymmetries. By\nseparately analyzing atmospheric asymmetries during ingress and egress, we can\nderive clear connections between these variations and the underlying\nasymmetries of the planetary limbs. Additionally, this approach enables us to\ninvestigate differences between the limbs slightly offset from the terminator\non the dayside and the nightside. We applied this method to JWST's\nNIRSpec/G395H observations of the hot Saturn exoplanet WASP-39 b. Our analysis\nsuggests a higher abundance of CO2 on the evening limb compared to the morning\nlimb and indicates a greater probability of SO2 on the limb slightly offset\nfrom the terminator on the dayside relative to the nightside. These findings\nhighlight the potential of our method to enhance the understanding of\nphotochemical processes in exoplanetary atmospheres.",
      "generated_abstract": "t a new method to constrain the mass and distance of the\ngalaxy-scale halo of the Milky Way (MW) using the observed spatial distribution\nof satellites in the local Universe. We use the $N$-body simulations of the\nMilky Way-mass dark-matter halo and the satellites of the Milky Way to infer\nthe mass and distance of the MW-halo. We find that the mass of the MW-halo\nfollows the expected scaling with the halo mass. The distance is constrained to\nbe $d = 25.5^{+4.7}_{-4.7}$ Mpc at 95\\% confidence. Our new method provides\na more reliable way to infer the properties of the MW-halo and provides a\nmechanism to study the evolution of the MW-halo in the context of the dark\nmatter halo",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10309278350515463,
          "p": 0.15625,
          "f": 0.12422359769453357
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.05102040816326531,
          "f": 0.04201680187839898
        },
        "rouge-l": {
          "r": 0.10309278350515463,
          "p": 0.15625,
          "f": 0.12422359769453357
        }
      }
    },
    {
      "paper_id": "cs.CC.stat/CO/2502.15024v1",
      "true_abstract": "We investigate implications of the (extended) low-degree conjecture (recently\nformalized in [MW23]) in the context of the symmetric stochastic block model.\nAssuming the conjecture holds, we establish that no polynomial-time algorithm\ncan weakly recover community labels below the Kesten-Stigum (KS) threshold. In\nparticular, we rule out polynomial-time estimators that, with constant\nprobability, achieve correlation with the true communities that is\nsignificantly better than random. Whereas, above the KS threshold,\npolynomial-time algorithms are known to achieve constant correlation with the\ntrue communities with high probability[Mas14,AS15].\n  To our knowledge, we provide the first rigorous evidence for the sharp\ntransition in recovery rate for polynomial-time algorithms at the KS threshold.\nNotably, under a stronger version of the low-degree conjecture, our lower bound\nremains valid even when the number of blocks diverges. Furthermore, our results\nprovide evidence of a computational-to-statistical gap in learning the\nparameters of stochastic block models.\n  In contrast to prior work, which either (i) rules out polynomial-time\nalgorithms for hypothesis testing with 1-o(1) success probability [Hopkins18,\nBBK+21a] under the low-degree conjecture, or (ii) rules out low-degree\npolynomials for learning the edge connection probability matrix [LG23], our\napproach provides stronger lower bounds on the recovery and learning problem.\n  Our proof combines low-degree lower bounds from [Hopkins18, BBK+21a] with\ngraph splitting and cross-validation techniques. In order to rule out general\nrecovery algorithms, we employ the correlation preserving projection method\ndeveloped in [HS17].",
      "generated_abstract": "component of any cryptographic scheme is the ability to generate\nrandom numbers, such as in hash functions and digital signatures. Traditionally,\nthis is achieved using a random oracle, which is a function that is known to\nthe adversary. In recent years, randomness extraction attacks have been\nintroduced, which rely on the adversary having access to a non-random oracle.\nThese attacks allow the adversary to obtain the true randomness, by\nconstantly guessing random numbers and observing the results. This paper\nproposes a new approach to randomness extraction that relies on a random oracle\nonly being known to the adversary. In this setting, the adversary does not\nhave access to any additional information, such as the true randomness, and\ntherefore cannot extract it. The adversary's only knowledge is the oracle, and\nwe show that this strategy provides the strongest known security against\nrandomness",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1276595744680851,
          "p": 0.21428571428571427,
          "f": 0.15999999532088902
        },
        "rouge-2": {
          "r": 0.01904761904761905,
          "p": 0.031746031746031744,
          "f": 0.023809519122024732
        },
        "rouge-l": {
          "r": 0.12056737588652482,
          "p": 0.20238095238095238,
          "f": 0.15111110643200015
        }
      }
    },
    {
      "paper_id": "cs.CV.q-bio/CB/2410.14612v2",
      "true_abstract": "High-throughput image analysis in the biomedical domain has gained\nsignificant attention in recent years, driving advancements in drug discovery,\ndisease prediction, and personalized medicine. Organoids, specifically, are an\nactive area of research, providing excellent models for human organs and their\nfunctions. Automating the quantification of organoids in microscopy images\nwould provide an effective solution to overcome substantial manual\nquantification bottlenecks, particularly in high-throughput image analysis.\nHowever, there is a notable lack of open biomedical datasets, in contrast to\nother domains, such as autonomous driving, and, notably, only few of them have\nattempted to quantify annotation uncertainty. In this work, we present MultiOrg\na comprehensive organoid dataset tailored for object detection tasks with\nuncertainty quantification. This dataset comprises over 400 high-resolution 2d\nmicroscopy images and curated annotations of more than 60,000 organoids. Most\nimportantly, it includes three label sets for the test data, independently\nannotated by two experts at distinct time points. We additionally provide a\nbenchmark for organoid detection, and make the best model available through an\neasily installable, interactive plugin for the popular image visualization tool\nNapari, to perform organoid quantification.",
      "generated_abstract": "ion-language models (VLMs) have demonstrated remarkable\ncapabilities in language understanding, their application in other tasks such\nas object recognition remains under-explored. In this work, we introduce\nVLM-based Objects, a novel framework that integrates VLMs with a novel object\nrepresentation, the VLM-Object, to enable efficient and effective object\nrecognition. Specifically, we leverage the VLM-Object to encode the object\nattributes into a compact representation that preserves the key semantics of\nthe original object. We then propose a novel VLM-Object-based\nobject-recognition model that integrates the VLM-Object and a vision-language\nmodel, the VLM-Object-VLM, to effectively capture the object attributes and\nunderstand the object. Experimental results on 13 benchmark datasets\ndemonstrate that the proposed VLM-Object-VLM outperforms traditional\nobject",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16428571428571428,
          "p": 0.2948717948717949,
          "f": 0.21100916971635394
        },
        "rouge-2": {
          "r": 0.02247191011235955,
          "p": 0.038461538461538464,
          "f": 0.028368789670540476
        },
        "rouge-l": {
          "r": 0.15,
          "p": 0.2692307692307692,
          "f": 0.19266054586314293
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.13982v1",
      "true_abstract": "Natural Language Processing (NLP) and Voice Recognition agents are rapidly\nevolving healthcare by enabling efficient, accessible, and professional patient\nsupport while automating grunt work. This report serves as my self project\nwherein models finetuned on medical call recordings are analysed through a\ntwo-stage system: Automatic Speech Recognition (ASR) for speech transcription\nand a Large Language Model (LLM) for context-aware, professional responses.\nASR, finetuned on phone call recordings provides generalised transcription of\ndiverse patient speech over call, while the LLM matches transcribed text to\nmedical diagnosis. A novel audio preprocessing strategy, is deployed to provide\ninvariance to incoming recording/call data, laden with sufficient augmentation\nwith noise/clipping to make the pipeline robust to the type of microphone and\nambient conditions the patient might have while calling/recording.",
      "generated_abstract": "aper, we propose a novel deep learning-based approach for\ntask-oriented speaker verification (SV), with a focus on the early stages of\nthe task. The proposed approach leverages the speaker embeddings, as well as\nthe speaker and task embeddings, to build a task-oriented speaker embedding\nmatrix. This matrix is used for a task-specific speaker embedding\ncomparision, where the speaker embeddings of the same speaker are compared to\ntheir speaker embeddings of a different speaker. The proposed approach is\nevaluated on two publicly available datasets, with the results showing that\nthe proposed approach significantly outperforms the baseline model in terms of\nspeaker verification accuracy. Furthermore, the proposed approach achieves\ncompetitive results compared to state-of-the-art approaches, while also\nachieving higher robustness to noise and varying speaker embeddings. This\nhighlights the potential",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14736842105263157,
          "p": 0.1917808219178082,
          "f": 0.1666666617524094
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14736842105263157,
          "p": 0.1917808219178082,
          "f": 0.1666666617524094
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2501.18909v1",
      "true_abstract": "HIV-1 replication can be suppressed with antiretroviral therapy (ART), but\nindividuals who stop taking ART soon become viremic again. Some people\nexperience extended times of detectable viremia despite optimal adherence to\nART. In the issue of the JCI, White, Wu, and coauthors elucidate a source of\nnonsuppressible viremia (NSV) in treatment-adherent patients clonally expanded\nT cells harboring HIV-1 proviruses with small deletions or mutations in the\n5'-leader, the UTR that includes the major splice donor site of viral RNA.\nThese mutations altered viral RNA-splicing efficiency and RNA dimerization and\npackaging, yet still allowed production of detectable levels of noninfectious\nvirus particles. These particles lacked the HIV-1 Env surface protein required\nfor cell entry and failed to form the mature capsid cone required for\ninfectivity. These studies improve our understanding of NSV and the regulation\nof viral functions in the 5'-leader with implications for rationalized care in\nindividuals with NSV.",
      "generated_abstract": "vancements in deep learning have led to the development of\nrepresentation learning methods, which allow for the learning of low-dimensional\nrepresentations from high-dimensional data. These representations can then be\nused to perform complex inference tasks. In this work, we focus on the\nrepresentation learning of protein sequence data using convolutional neural\nnetworks (CNNs) and introduce a novel approach for protein sequence\nrepresentation learning. Our approach employs an encoder-decoder architecture,\nwhich encodes the protein sequence into a low-dimensional representation, and a\ndecoder that performs inference tasks, such as protein structure prediction,\nsimilarity analysis, and sequence similarity detection. The model is trained\nusing a large-scale dataset of protein sequence pairs, and is evaluated on a\nvariety of tasks, including protein structure prediction, sequence similarity\ndetection, and sequence similarity detection. Our results demonstrate that our\nmodel performs competit",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1320754716981132,
          "p": 0.1728395061728395,
          "f": 0.14973261541022065
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1320754716981132,
          "p": 0.1728395061728395,
          "f": 0.14973261541022065
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.03608v1",
      "true_abstract": "This paper studies games of voluntary disclosure in which a sender discloses\nevidence to a receiver who then offers an allocation and transfers. We\ncharacterize the set of equilibrium payoffs in this setting. Our main result\nestablishes that any payoff profile that can be achieved through information\ndesign can also be supported by an equilibrium of the disclosure game. Hence,\nour analysis suggests an equivalence between disclosure and design in these\nsettings. We apply our results to monopoly pricing, bargaining over policies,\nand insurance markets.",
      "generated_abstract": "This paper introduces a novel approach to the study of the effect of\nchanging the price of government bonds on the allocation of risky assets. The\nframework is based on a discrete-time version of the Nash bargaining problem\nwith imperfect recall, which captures the complexities of the problem. The\nframework is applied to the case of two agents, each of whom has a portfolio\ncomposed of risky assets and government bonds. The agents are exposed to the\nuncertainty of the future price of government bonds, and are allowed to\nrecall their past decisions. The results show that a decrease in the price of\ngovernment bonds leads to an increase in the allocation of risky assets among\nthe agents, while a decrease in the price of government bonds leads to an\nincrease in the allocation of risky assets among the agents. The results are\nillustrated by using a simple example.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23076923076923078,
          "p": 0.21428571428571427,
          "f": 0.22222221722908106
        },
        "rouge-2": {
          "r": 0.023809523809523808,
          "p": 0.018691588785046728,
          "f": 0.020942403449468
        },
        "rouge-l": {
          "r": 0.23076923076923078,
          "p": 0.21428571428571427,
          "f": 0.22222221722908106
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/MF/2411.10386v2",
      "true_abstract": "The fragility of financial systems was starkly demonstrated in early 2023\nthrough a cascade of major bank failures in the United States, including the\nsecond, third, and fourth largest collapses in the US history. The highly\ninterdependent financial networks and the associated high systemic risk have\nbeen deemed the cause of the crashes. The goal of this paper is to enhance\nexisting systemic risk analysis frameworks by incorporating essential debt\nvaluation factors. Our results demonstrate that these additional elements\nsubstantially influence the outcomes of risk assessment. Notably, by modeling\nthe dynamic relationship between interest rates and banks' credibility, our\nframework can detect potential cascading failures that standard approaches\nmight miss. The proposed risk assessment methodology can help regulatory bodies\nprevent future failures, while also allowing companies to more accurately\npredict turmoil periods and strengthen their survivability during such events.",
      "generated_abstract": "r explores the implications of using a multi-asset portfolio\ninstead of the traditional one-asset portfolio for the estimation of the\nGARCH-VAR model. We first develop a simulation framework to evaluate the\nperformance of different GARCH-VAR models and then discuss the implications of\nusing multi-asset portfolios for the estimation of these models. We find that\nmulti-asset portfolios can improve the forecasting accuracy of the GARCH-VAR\nmodels and thus can be used to improve the performance of these models. We\nfurther explore the implications of using multi-asset portfolios for the\nestimation of the GARCH-VAR models in two specific scenarios: (1) a scenario\nwhere the number of assets is unknown and (2) a scenario where the number of\nassets is fixed and the number of observations is large. We show that in these\nsc",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10091743119266056,
          "p": 0.1864406779661017,
          "f": 0.1309523763952666
        },
        "rouge-2": {
          "r": 0.014814814814814815,
          "p": 0.02197802197802198,
          "f": 0.017699110233770594
        },
        "rouge-l": {
          "r": 0.09174311926605505,
          "p": 0.1694915254237288,
          "f": 0.11904761449050472
        }
      }
    },
    {
      "paper_id": "gr-qc.gr-qc/2503.09918v1",
      "true_abstract": "Quasi-normal modes (QNMs) and greybody factors are some of the most\ncharacteristic features of the dynamics of black holes (BHs) and represent the\nbasis for a number of fundamental physics tests with gravitational wave\nobservations. It is therefore important to understand the properties of these\nquantities, naturally introduced within BH perturbation theory, in particular\nthe stability properties under modifications of the BH potential. Instabilities\nin the QNMs have been recently shown to appear in the BH pseudospectrum under\ncertain circumstances. In this work, we give a novel point of view based on the\nexistence of some recently discovered hidden symmetries in BH dynamics and the\nassociated infinite series of conserved quantities, the Korteweg-de Vries (KdV)\nintegrals. We provide different motivations to use the KdV integrals as\nindicators of some crucial BH spectral properties. In particular, by studying\nthem in different scenarios described by modified BH barriers, we find strong\nevidence that the KdV conserved quantities represent a useful tool to look for\ninstabilities in the BH spectrum of QNMs and in their greybody factors.",
      "generated_abstract": "the equation of motion for the two-dimensional scalar field\nin the presence of a pointlike point-like mass. This equation is derived from\nthe Lorentzian scalar field in the presence of a pointlike mass by using the\ngeneralized Weyl and Weyl-Weyl-Weyl transformations. This equation is\nequivalent to the standard Einstein-Hilbert action with a cosmological constant\nin the limit of zero mass. We show that the equation of motion is exact for\nmassless scalar fields, but it is not exact for massive scalar fields in the\nlimit of zero mass. We also derive the equations of motion for the scalar\nfield in the presence of a pointlike massive scalar mass. The equations of motion\nare not exact for massive scalar fields in the limit of zero mass. We show\nthat the equations of motion are exact for massive scalar fields in the\nlimit of infinite mass. We also derive the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12612612612612611,
          "p": 0.2641509433962264,
          "f": 0.17073170294244505
        },
        "rouge-2": {
          "r": 0.012195121951219513,
          "p": 0.024096385542168676,
          "f": 0.016194327521514448
        },
        "rouge-l": {
          "r": 0.09009009009009009,
          "p": 0.18867924528301888,
          "f": 0.12195121513756706
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.02040v1",
      "true_abstract": "This paper introduces a distributed contingency detection algorithm for\ndetecting unobservable contingencies in power distribution systems using\nstochastic hybrid system (SHS) models. We aim to tackle the challenge of\nlimited measurement capabilities in distribution networks that restrict the\nability to detect contingencies promptly. We incorporate the dynamics of\ndistribution network connections, load feeders, PV, and battery energy storage\nsystem (BESS) hybrid resources into a fully correlated SHS model representing\nthe distribution system as a randomly switching system between different\nstructures during contingency occurrence. We show that jumps in the SHS model\ncorrespond to contingencies in the physical power grid. We propose a probing\napproach based on magnitude-modulation inputs (MaMI) to make contingencies\ndetectable. The effectiveness of the proposed approach is validated through\nsimulations on a sample distribution system.",
      "generated_abstract": "aper, a novel and practical control strategy is developed for\nthe multi-agent system composed of an autonomous vehicle and a traffic\nmanagement center, where the vehicle is responsible for driving itself and the\ntraffic management center is responsible for coordinating the movement of all\nvehicles in the network. The control strategy consists of two stages. In the\nfirst stage, a distributed feedback control law is designed for the autonomous\nvehicle. The second stage is the control of the traffic management center using\nthe distributed feedback control law. The controller for the autonomous\nvehicle is based on the optimal control theory, which is formulated as a\nsingle-agent optimal control problem. In the second stage, the controller for\nthe traffic management center is designed using the backstepping technique,\nwhere the backstepping transformation is used to handle the nonlinear\ndifferential equations of the autonomous vehicle. The control performance of\nboth the controllers",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.23529411764705882,
          "f": 0.20512820021038802
        },
        "rouge-2": {
          "r": 0.04065040650406504,
          "p": 0.043478260869565216,
          "f": 0.042016801728338986
        },
        "rouge-l": {
          "r": 0.17045454545454544,
          "p": 0.22058823529411764,
          "f": 0.19230768738987522
        }
      }
    },
    {
      "paper_id": "physics.space-ph.physics/space-ph/2503.04965v1",
      "true_abstract": "We present the first full-wavelength numerical simulations of the electric\nfield generated by cosmic ray impacts into the Moon. Billions of cosmic rays\nfall onto the Moon every year. Ultra-high energy cosmic ray impacts produce\nsecondary particle cascades within the regolith and subsequent coherent,\nwidebandwidth, linearly-polarized radio pulses by the Askaryan Effect.\nObservations of the cosmic ray particle shower radio emissions can reveal\nsubsurface structure on the Moon and enable the broad and deep prospecting\nnecessary to confirm or refute the existence of polar ice deposits. Our\nsimulations show that the radio emissions and reflections could reveal ice\nlayers as thin as 10 cm and buried under regolith as deep as 9 m. The Askaryan\nEffect presents a novel and untapped opportunity for characterizing buried\nlunar ice at unprecedented depths and spatial scales.",
      "generated_abstract": "is a dynamic, changing object in Earth's orbit. It is exposed to\nsuch a variety of physical processes, from impacts, tidal forces, and solar\nradiation to thermal and geophysical processes, that it is a complex system\nsubject to frequent and rapid changes. The Moon's gravitational field,\ntides, and magnetic field have all been observed to vary over time, which\nmakes it difficult to accurately model the evolution of the system. In this\nwork, we present a novel approach to modelling the Moon based on the\ninversion of the 3D gravitational field and the 2D tidal fields using\nanalytical models, combined with a numerical model of the tidal dissipation.\nWe show that the numerical model, using the 2D tidal dissipation, is able to\ncapture the variation of the Moon's gravitational field and tidal fields over",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16853932584269662,
          "p": 0.18072289156626506,
          "f": 0.1744185996572473
        },
        "rouge-2": {
          "r": 0.04838709677419355,
          "p": 0.04878048780487805,
          "f": 0.048582990951499476
        },
        "rouge-l": {
          "r": 0.16853932584269662,
          "p": 0.18072289156626506,
          "f": 0.1744185996572473
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2503.02351v1",
      "true_abstract": "Concept-selective regions within the human cerebral cortex exhibit\nsignificant activation in response to specific visual stimuli associated with\nparticular concepts. Precisely localizing these regions stands as a crucial\nlong-term goal in neuroscience to grasp essential brain functions and\nmechanisms. Conventional experiment-driven approaches hinge on manually\nconstructed visual stimulus collections and corresponding brain activity\nrecordings, constraining the support and coverage of concept localization.\nAdditionally, these stimuli often consist of concept objects in unnatural\ncontexts and are potentially biased by subjective preferences, thus prompting\nconcerns about the validity and generalizability of the identified regions. To\naddress these limitations, we propose a data-driven exploration approach. By\nsynthesizing extensive brain activity recordings, we statistically localize\nvarious concept-selective regions. Our proposed MindSimulator leverages\nadvanced generative technologies to learn the probability distribution of brain\nactivity conditioned on concept-oriented visual stimuli. This enables the\ncreation of simulated brain recordings that reflect real neural response\npatterns. Using the synthetic recordings, we successfully localize several\nwell-studied concept-selective regions and validate them against empirical\nfindings, achieving promising prediction accuracy. The feasibility opens\navenues for exploring novel concept-selective regions and provides prior\nhypotheses for future neuroscience research.",
      "generated_abstract": "e a computational framework to study the dynamics of a population of\ndegrees of freedom (DOFs) governed by a nonlinear stochastic differential\nequation (SDE) that is nonlinear and stochastic in a manner that is both\ntransparent and intuitive. We demonstrate that the SDE governing the evolution of\nthe DOFs in a system of coupled ordinary differential equations (ODEs) can be\ntransformed into a stochastic SDE that is nonlinear in the DOFs and\ntranslational in time. We then demonstrate that the SDE governing the evolution\nof the DOFs in a system of coupled stochastic differential equations (SDEs) can\nbe transformed into a stochastic SDE that is nonlinear in the DOFs and\ntranslational in time. We then extend the framework to a system of coupled\nnonlinear SDEs (NLSDEs) and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.058823529411764705,
          "p": 0.16,
          "f": 0.08602150144525399
        },
        "rouge-2": {
          "r": 0.005681818181818182,
          "p": 0.012048192771084338,
          "f": 0.007722003366678566
        },
        "rouge-l": {
          "r": 0.058823529411764705,
          "p": 0.16,
          "f": 0.08602150144525399
        }
      }
    },
    {
      "paper_id": "cs.NE.cs/NE/2503.08703v1",
      "true_abstract": "Event cameras provide superior temporal resolution, dynamic range, power\nefficiency, and pixel bandwidth. Spiking Neural Networks (SNNs) naturally\ncomplement event data through discrete spike signals, making them ideal for\nevent-based tracking. However, current approaches that combine Artificial\nNeural Networks (ANNs) and SNNs, along with suboptimal architectures,\ncompromise energy efficiency and limit tracking performance. To address these\nlimitations, we propose the first Transformer-based spike-driven tracking\npipeline. Our Global Trajectory Prompt (GTP) method effectively captures global\ntrajectory information and aggregates it with event streams into event images\nto enhance spatiotemporal representation. We then introduce SDTrack, a\nTransformer-based spike-driven tracker comprising a Spiking MetaFormer backbone\nand a simple tracking head that directly predicts normalized coordinates using\nspike signals. The framework is end-to-end, does not require data augmentation\nor post-processing. Extensive experiments demonstrate that SDTrack achieves\nstate-of-the-art performance while maintaining the lowest parameter count and\nenergy consumption across multiple event-based tracking benchmarks,\nestablishing a solid baseline for future research in the field of neuromorphic\nvision.",
      "generated_abstract": "We study the problem of ranking two objects, where the ranking of one object\nis determined by the ordering of another. We focus on the case where the\nsecond object is the reference. In this case, the ranking of the second object\ncan be represented by a permutation of the first object. We consider\nrankings that are ``nearest-neighbor'' in the sense that the ranking of a\nparticular object can be obtained by a sequence of simple permutations of the\nother objects, and we give an algorithm that computes such a sequence in\npolynomial time. We also consider the case where the second object is the\nreference and where the ranking of the second object can be represented by a\npermutation of the first object. We give a polynomial-time algorithm that\ncomputes a sequence of permutations that achieves the same ranking.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0916030534351145,
          "p": 0.21428571428571427,
          "f": 0.12834224179358875
        },
        "rouge-2": {
          "r": 0.012738853503184714,
          "p": 0.020833333333333332,
          "f": 0.015810271970505548
        },
        "rouge-l": {
          "r": 0.07633587786259542,
          "p": 0.17857142857142858,
          "f": 0.10695186746203798
        }
      }
    },
    {
      "paper_id": "cs.GT.cs/ET/2503.07558v1",
      "true_abstract": "We introduce the first formal model capturing the elicitation of unverifiable\ninformation from a party (the \"source\") with implicit signals derived by other\nplayers (the \"observers\"). Our model is motivated in part by applications in\ndecentralized physical infrastructure networks (a.k.a. \"DePIN\"), an emerging\napplication domain in which physical services (e.g., sensor information,\nbandwidth, or energy) are provided at least in part by untrusted and\nself-interested parties. A key challenge in these signal network applications\nis verifying the level of service that was actually provided by network\nparticipants.\n  We first establish a condition called source identifiability, which we show\nis necessary for the existence of a mechanism for which truthful signal\nreporting is a strict equilibrium. For a converse, we build on techniques from\npeer prediction to show that in every signal network that satisfies the source\nidentifiability condition, there is in fact a strictly truthful mechanism,\nwhere truthful signal reporting gives strictly higher total expected payoff\nthan any less informative equilibrium. We furthermore show that this truthful\nequilibrium is in fact the unique equilibrium of the mechanism if there is\npositive probability that any one observer is unconditionally honest (e.g., if\nan observer were run by the network owner). Also, by extending our condition to\ncoalitions, we show that there are generally no collusion-resistant mechanisms\nin the settings that we consider.\n  We apply our framework and results to two DePIN applications: proving\nlocation, and proving bandwidth. In the location-proving setting observers\nlearn (potentially enlarged) Euclidean distances to the source. Here, our\ncondition has an appealing geometric interpretation, implying that the source's\nlocation can be truthfully elicited if and only if it is guaranteed to lie\ninside the convex hull of the observers.",
      "generated_abstract": "r investigates the problem of reconstructing an arbitrary graph from\na set of pairwise connected edges. We propose a novel graph reconstructor that\nintegrates a multi-scale graph reconstructor and a multi-scale edge matching\nreconstructor. The graph reconstructor is a graph isomorphism between the input\ngraph and the reconstructed graph. The edge matching reconstructor is a\nmulti-scale matching algorithm that matches a pair of vertices from the input\ngraph to the reconstructed graph. The multi-scale matching algorithm is\nconstructed such that it can handle small-scale edges while maintaining high\nperformance in the large-scale edge case. The proposed graph reconstructor and\nthe edge matching reconstructor are evaluated using several benchmarks on\nsynthetic and real-world graphs. The results show that the proposed graph\nreconstructor and edge matching reconstructor outperform previous graph\nreconstructors in terms of edge density and vertex density. The proposed",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09826589595375723,
          "p": 0.2537313432835821,
          "f": 0.141666662642014
        },
        "rouge-2": {
          "r": 0.02214022140221402,
          "p": 0.05357142857142857,
          "f": 0.03133158855101664
        },
        "rouge-l": {
          "r": 0.08670520231213873,
          "p": 0.22388059701492538,
          "f": 0.12499999597534735
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.19002v1",
      "true_abstract": "This paper studies the stochastic setting in cooperative games and suggests a\nsolution concept based on second order stochastic dominance (SSD), which is\noften applied to robustly model risk averse behaviour of players in different\neconomic and game theoretic models as it enables to model not specified levels\nof risk aversion among players. The main result of the paper connects this\nsolution concept, \\emph{SSD-core}, in case of uniform distribution of the game\nto cores of two deterministic cooperative games. Interestingly, balancedness of\nboth of these games and convexity of one of these implies non-emptiness of the\nSSD-core. The opposite implication does not, in general, hold and leads to\nquestions about intersections of cores of two games and their relations.\nFinally, we present an application of the SSD-core to the multiple newsvendors\nproblem, where we provide a characterization of risk averse behaviour of\nplayers with an interpretation in terms of the model.",
      "generated_abstract": "er the optimal control of a time-dependent, non-Markovian\nconsumption-production model with a unitary producer and an individual who\nconsumes only when the production is above a threshold. We show that the\nconsumption problem is NP-hard under general non-Markovianity conditions, and\nthat the problem becomes linearly solvable if the non-Markovianity conditions\nare satisfied. We also show that the production problem is tractable, and that\nthe optimal policy depends only on the state of the system at time $t=0$. We\nthen show that the optimal consumption-production policy is linearly\nrepresentable, and that it can be characterized by a system of linear\nequations, which can be solved efficiently. Finally, we show that the optimal\ncontrol policy is in general a non-linear function of the initial state of the\nsystem, and that it can be expressed as a function of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16129032258064516,
          "p": 0.20833333333333334,
          "f": 0.18181817689917368
        },
        "rouge-2": {
          "r": 0.014925373134328358,
          "p": 0.01834862385321101,
          "f": 0.016460900402717873
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.16666666666666666,
          "f": 0.14545454053553736
        }
      }
    },
    {
      "paper_id": "cs.LG.econ/GN/2501.06248v2",
      "true_abstract": "Current methods that train large language models (LLMs) with reinforcement\nlearning feedback, often resort to averaging outputs of multiple rewards\nfunctions during training. This overlooks crucial aspects of individual reward\ndimensions and inter-reward dependencies that can lead to sub-optimal outcomes\nin generations. In this work, we show how linear aggregation of rewards\nexhibits some vulnerabilities that can lead to undesired properties of\ngenerated text. We then propose a transformation of reward functions inspired\nby economic theory of utility functions (specifically Inada conditions), that\nenhances sensitivity to low reward values while diminishing sensitivity to\nalready high values. We compare our approach to the existing baseline methods\nthat linearly aggregate rewards and show how the Inada-inspired reward feedback\nis superior to traditional weighted averaging. We quantitatively and\nqualitatively analyse the difference in the methods, and see that models\ntrained with Inada-transformations score as more helpful while being less\nharmful.",
      "generated_abstract": "r investigates the performance of a group of agents in a\ntransit network with heterogeneous costs, where each agent has a single\npreference over the cost of traveling from one node to another. We introduce a\nsimple model for the cost structure of the network and examine the\noptimal allocation of agents to the network. We also investigate the\nincentives for agents to alter their preferences over the cost structure. We\nshow that a group of agents can find a Nash equilibrium where all agents\nprefer the same cost structure, and the incentive-compatible allocation is\noptimal. We also show that, when the cost structure is known, the optimal\nallocation is not incentive compatible. The optimal allocation is characterized\nby the expected number of travel days per agent. We also examine the\nperformance of the incentive-compatible allocation in terms of the\noptimality gap, which measures the difference between the expected number of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1346153846153846,
          "p": 0.18181818181818182,
          "f": 0.15469612770794558
        },
        "rouge-2": {
          "r": 0.014285714285714285,
          "p": 0.01652892561983471,
          "f": 0.015325665524582874
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.16883116883116883,
          "f": 0.1436464039510395
        }
      }
    },
    {
      "paper_id": "cs.CC.cs/CC/2503.10393v1",
      "true_abstract": "Oredango puzzle, one of the pencil puzzles, was originally created by\nKanaiboshi and published in the popular puzzle magazine Nikoli. In this paper,\nwe show NP- and ASP-completeness of Oredango by constructing a reduction from\nthe 1-in-3SAT problem. Next, we formulate Oredango as an 0-1\ninteger-programming problem, and present numerical results obtained by solving\nOredango puzzles from Nikoli and PuzzleSquare JP using a 0-1 optimization\nsolver.",
      "generated_abstract": "This paper presents a novel approach to enhancing the security of cloud-based\ncomputing infrastructures by leveraging the trusted execution environment (TEE)\nin modern hardware platforms. Specifically, we propose a novel technique that\nintegrates TEE-based cryptography into cloud-based applications, providing\nadditional protection against cloud-side attacks. By employing the TEE, our\nproposed approach not only protects the confidentiality of sensitive data but\nalso enhances the security of cloud-based applications. The TEE provides\nadditional security by protecting sensitive data from being compromised and by\nensuring that the cloud-side code is executed in a trusted environment. Our\nexperimental results demonstrate that our approach significantly enhances the\nsecurity of cloud-based applications, thereby providing additional protection\nagainst cloud-side attacks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18,
          "p": 0.1232876712328767,
          "f": 0.1463414585894641
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.16,
          "p": 0.1095890410958904,
          "f": 0.13008129598783807
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/MF/2502.17417v1",
      "true_abstract": "In this paper, we propose an event-driven Limit Order Book (LOB) model that\ncaptures twelve of the most observed LOB events in exchange-based financial\nmarkets. To model these events, we propose using the state-of-the-art Neural\nHawkes process, a more robust alternative to traditional Hawkes process models.\nMore specifically, this model captures the dynamic relationships between\ndifferent event types, particularly their long- and short-term interactions,\nusing a Long Short-Term Memory neural network. Using this framework, we\nconstruct a midprice process that captures the event-driven behavior of the LOB\nby simulating high-frequency dynamics like how they appear in real financial\nmarkets. The empirical results show that our model captures many of the broader\ncharacteristics of the price fluctuations, particularly in terms of their\noverall volatility. We apply this LOB simulation model within a Deep\nReinforcement Learning Market-Making framework, where the trading agent can now\ncomplete trade order fills in a manner that closely resembles real-market trade\nexecution. Here, we also compare the results of the simulated model with those\nfrom real data, highlighting how the overall performance and the distribution\nof trade order fills closely align with the same analysis on real data.",
      "generated_abstract": "r proposes a novel framework for trading multiple assets using\nsingle-stock data. We incorporate a dual-price mechanism into the multi-asset\nportfolio optimization problem, which allows for a more flexible and\nefficient approach to trading multi-asset strategies. Our model addresses the\nchallenges of volatility clustering and overtrading, which are common in\nmulti-asset portfolio management. The proposed framework is designed to\naccount for the correlation between assets and the correlation between asset\nreturns, thus ensuring the portfolio's stability. In addition, our model\nenhances the portfolio's risk-adjusted performance by incorporating the\nprice-weighted information of multiple assets. We compare our proposed\nframework with two state-of-the-art approaches, which are based on\nstochastic volatility models. Our findings demonstrate that our proposed\nframework outperforms the state-of-the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.3125,
          "f": 0.24390243426531835
        },
        "rouge-2": {
          "r": 0.0223463687150838,
          "p": 0.03669724770642202,
          "f": 0.02777777307315859
        },
        "rouge-l": {
          "r": 0.192,
          "p": 0.3,
          "f": 0.23414633670434276
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/MM/2503.07259v1",
      "true_abstract": "Egocentric video-based models capture rich semantic information and have\ndemonstrated strong performance in human activity recognition (HAR). However,\ntheir high power consumption, privacy concerns, and dependence on lighting\nconditions limit their feasibility for continuous on-device recognition. In\ncontrast, inertial measurement unit (IMU) sensors offer an energy-efficient and\nprivacy-preserving alternative, yet they suffer from limited large-scale\nannotated datasets, leading to weaker generalization in downstream tasks. To\nbridge this gap, we propose COMODO, a cross-modal self-supervised distillation\nframework that transfers rich semantic knowledge from the video modality to the\nIMU modality without requiring labeled annotations. COMODO leverages a\npretrained and frozen video encoder to construct a dynamic instance queue,\naligning the feature distributions of video and IMU embeddings. By distilling\nknowledge from video representations, our approach enables the IMU encoder to\ninherit rich semantic information from video while preserving its efficiency\nfor real-world applications. Experiments on multiple egocentric HAR datasets\ndemonstrate that COMODO consistently improves downstream classification\nperformance, achieving results comparable to or exceeding fully supervised\nfine-tuned models. Moreover, COMODO exhibits strong cross-dataset\ngeneralization. Benefiting from its simplicity, our method is also generally\napplicable to various video and time-series pre-trained models, offering the\npotential to leverage more powerful teacher and student foundation models in\nfuture research. The code is available at https://github.com/Breezelled/COMODO .",
      "generated_abstract": "ning models have proven effective in segmenting pathologies from\nimages, yet the inherent complexities of medical images render traditional\nsegmentation approaches inefficient. This paper introduces a novel deep\nlearning-based framework for pathology segmentation that integrates multi-scale\nfeature extraction with a novel multi-scale multi-head attention mechanism to\nenhance the model's performance. The proposed method employs a dual-stage\narchitecture, first extracting multi-scale features from the image, followed by\na hierarchical attention mechanism that aggregates information from multiple\nscales. This attention mechanism enhances the model's ability to identify\nspecific pathologies, enabling the system to segment pathologies more accurately\nand efficiently. The proposed method was evaluated on a publicly available\ndataset, demonstrating its effectiveness in segmenting various pathologies. The\nmodel's performance was evaluated using the mean intersection-over-union (m",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15286624203821655,
          "p": 0.2727272727272727,
          "f": 0.19591836274352364
        },
        "rouge-2": {
          "r": 0.009900990099009901,
          "p": 0.017543859649122806,
          "f": 0.012658223235861316
        },
        "rouge-l": {
          "r": 0.12738853503184713,
          "p": 0.22727272727272727,
          "f": 0.16326530151903385
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.05766v1",
      "true_abstract": "Audio-visual representation learning is crucial for advancing multimodal\nspeech processing tasks, such as lipreading and audio-visual speech\nrecognition. Recently, speech foundation models (SFMs) have shown remarkable\ngeneralization capabilities across various speech-related tasks. Building on\nthis progress, we propose an audio-visual representation learning model that\nleverages cross-modal knowledge distillation from SFMs. In our method, SFMs\nserve as teachers, from which multi-layer hidden representations are extracted\nusing clean audio inputs. We also introduce a multi-teacher ensemble method to\ndistill the student, which receives audio-visual data as inputs. A novel\nrepresentational knowledge distillation loss is employed to train the student\nduring pretraining, which is also applied during finetuning to further enhance\nthe performance on downstream tasks. Our experiments utilized both a\nself-supervised SFM, WavLM, and a supervised SFM, iFLYTEK-speech. The results\ndemonstrated that our proposed method achieved superior or at least comparable\nperformance to previous state-of-the-art baselines across automatic speech\nrecognition, visual speech recognition, and audio-visual speech recognition\ntasks. Additionally, comprehensive ablation studies and the visualization of\nlearned representations were conducted to evaluate the effectiveness of our\nproposed method.",
      "generated_abstract": "aper, we present a framework for real-time audio streaming over\nthe Internet using a wireless mesh network. Our proposed system consists of\nmultiple base stations (BSs) and a server located at the end of the network,\nwhich provides a low latency streaming service. The BSs are equipped with a\ncomputer with a processing unit (CPU) and a memory. They also have a wireless\ntransmitter (TX) and a receiver (RX) for wireless communication. The BSs are\nconnected to each other through a wireless mesh network. We assume that all the\nBSs have the same processing power, memory, and radio access. The server, on\nthe other hand, has more processing power, memory, and radio access. To\nimplement our proposed system, we propose a three-tier architecture consisting\nof the server, the BSs, and the wireless mesh network. In this architecture,\nthe",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20161290322580644,
          "p": 0.29411764705882354,
          "f": 0.2392344449348688
        },
        "rouge-2": {
          "r": 0.023952095808383235,
          "p": 0.03333333333333333,
          "f": 0.027874559594022873
        },
        "rouge-l": {
          "r": 0.18548387096774194,
          "p": 0.27058823529411763,
          "f": 0.22009568895400755
        }
      }
    },
    {
      "paper_id": "stat.ME.q-bio/SC/2408.17188v2",
      "true_abstract": "We introduce a generalized promotion time cure model motivated by a new\nbiological consideration. The new approach is flexible to model heterogeneous\nsurvival data, in particular for addressing intra-sample heterogeneity. We also\nindicate that the new approach is suited to model a series or parallel system\nconsisting of multiple subsystems in reliability analysis.",
      "generated_abstract": "The aim of this study was to apply the Bayesian inference and modeling\nmethods to the problem of evaluating the probability of a target mutation to\noccur in a population. We aimed to use a Bayesian framework to evaluate the\nprobability of a target mutation to occur in a population. The study was\nconducted in a population of 1200 individuals, where we used the Bayesian\nmodeling framework to evaluate the probability of the target mutation to occur\nin a population of 1200 individuals. In the study, we used the Bayesian model\nto evaluate the probability of the target mutation to occur in a population of\n1200 individuals. The results showed that the probability of the target\nmutation to occur in the population is 0.000108.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23809523809523808,
          "p": 0.22727272727272727,
          "f": 0.232558134537588
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.014084507042253521,
          "f": 0.016666661834723623
        },
        "rouge-l": {
          "r": 0.21428571428571427,
          "p": 0.20454545454545456,
          "f": 0.20930232058409964
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/CV/2503.10635v1",
      "true_abstract": "Despite promising performance on open-source large vision-language models\n(LVLMs), transfer-based targeted attacks often fail against black-box\ncommercial LVLMs. Analyzing failed adversarial perturbations reveals that the\nlearned perturbations typically originate from a uniform distribution and lack\nclear semantic details, resulting in unintended responses. This critical\nabsence of semantic information leads commercial LVLMs to either ignore the\nperturbation entirely or misinterpret its embedded semantics, thereby causing\nthe attack to fail. To overcome these issues, we notice that identifying core\nsemantic objects is a key objective for models trained with various datasets\nand methodologies. This insight motivates our approach that refines semantic\nclarity by encoding explicit semantic details within local regions, thus\nensuring interoperability and capturing finer-grained features, and by\nconcentrating modifications on semantically rich areas rather than applying\nthem uniformly. To achieve this, we propose a simple yet highly effective\nsolution: at each optimization step, the adversarial image is cropped randomly\nby a controlled aspect ratio and scale, resized, and then aligned with the\ntarget image in the embedding space. Experimental results confirm our\nhypothesis. Our adversarial examples crafted with local-aggregated\nperturbations focused on crucial regions exhibit surprisingly good\ntransferability to commercial LVLMs, including GPT-4.5, GPT-4o,\nGemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning\nmodels like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach\nachieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly\noutperforming all prior state-of-the-art attack methods. Our optimized\nadversarial examples under different configurations and training code are\navailable at https://github.com/VILA-Lab/M-Attack.",
      "generated_abstract": "tection and tracking have long been crucial in robotics and\nscenario-planning applications. However, current deep learning-based methods\noften struggle to capture complex object interactions, resulting in\nunder-identification and object misdetection. To address this challenge, we\npropose a novel framework for robust object detection and tracking,\n\\emph{Re-Guidable Inference} (Re-Guid), which leverages a re-guided attention\nmechanism and a re-guided inference mechanism. Specifically, the re-guided\nattention mechanism enhances object detection and classification by leveraging\ncontextual information, while the re-guided inference mechanism enhances\nobject tracking by leveraging localization information. We evaluate our method\non the Cityscapes, SUN RGB-D, and VOC datasets and demonstrate its superior\nperformance over the state-of-the-art methods. Code will be",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11351351351351352,
          "p": 0.2692307692307692,
          "f": 0.15969581331810506
        },
        "rouge-2": {
          "r": 0.01646090534979424,
          "p": 0.041237113402061855,
          "f": 0.02352940768667891
        },
        "rouge-l": {
          "r": 0.10270270270270271,
          "p": 0.24358974358974358,
          "f": 0.1444866878428199
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/ST/2501.16659v1",
      "true_abstract": "Considering the continuous-time Mean-Variance (MV) portfolio optimization\nproblem, we study a regime-switching market setting and apply reinforcement\nlearning (RL) techniques to assist informed exploration within the control\nspace. We introduce and solve the Exploratory Mean Variance with Regime\nSwitching (EMVRS) problem. We also present a Policy Improvement Theorem.\nFurther, we recognize that the widely applied Temporal Difference (TD) learning\nis not adequate for the EMVRS context, hence we consider Orthogonality\nCondition (OC) learning, leveraging the martingale property of the induced\noptimal value function from the analytical solution to EMVRS. We design a RL\nalgorithm that has more meaningful parameterization using the market parameters\nand propose an updating scheme for each parameter. Our empirical results\ndemonstrate the superiority of OC learning over TD learning with a clear\nconvergence of the market parameters towards their corresponding ``grounding\ntrue\" values in a simulated market scenario. In a real market data study, EMVRS\nwith OC learning outperforms its counterparts with the highest mean and\nreasonably low volatility of the annualized portfolio returns.",
      "generated_abstract": "e a novel framework for evaluating the risk-return trade-off of\nportfolio strategies. Our approach is based on a decomposition of the\nexpected risk and return of a portfolio of financial assets into three\ncomponents: the expected volatility of the portfolio, the expected return on\nthe portfolio, and the portfolio's exposure to the market. We then use a\ndynamic programming approach to derive the expected utility of the portfolio.\nOur framework provides a unified view of the trade-offs between different\nstrategies and aids in the interpretation of results from empirical studies.\n  Our framework is illustrated using a portfolio of five financial assets:\nU.S. Treasuries, U.S. Government Agency bonds, a 10-year TIPS bond, a 10-year\nbond, and a 10-year Treasury bond. Our findings highlight",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13934426229508196,
          "p": 0.24285714285714285,
          "f": 0.17708332870008692
        },
        "rouge-2": {
          "r": 0.018518518518518517,
          "p": 0.02727272727272727,
          "f": 0.022058818712155034
        },
        "rouge-l": {
          "r": 0.12295081967213115,
          "p": 0.21428571428571427,
          "f": 0.1562499953667536
        }
      }
    },
    {
      "paper_id": "math.SP.math/SP/2502.18307v1",
      "true_abstract": "The notion of eta invariant is traditionally defined by means of analytic\ncontinuation. We prove, by examining the particular case of the operator curl,\nthat the eta invariant can equivalently be obtained as the trace of the\ndifference of positive and negative spectral projections, appropriately\nregularised. Our construction is direct, in the sense that it does not involve\nanalytic continuation, and is based on the use of pseudodifferential\ntechniques. This provides a novel approach to the study of spectral asymmetry\nof non-semibounded (pseudo)differential systems on manifolds which encompasses\nand extends previous results.",
      "generated_abstract": "In this paper we study the problem of determining the minimal number of\nelements of a group of automorphisms of a compact manifold $M$ which can\ngenerate a group of automorphisms of $M$ of the same order, but with a\ndifferent topology. This problem was first posed by F. Sch\\\"{a}fer in 1983,\nand later solved by M. Ozawa in 1997.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14925373134328357,
          "p": 0.23809523809523808,
          "f": 0.1834862337951352
        },
        "rouge-2": {
          "r": 0.011235955056179775,
          "p": 0.018518518518518517,
          "f": 0.013986009285541215
        },
        "rouge-l": {
          "r": 0.13432835820895522,
          "p": 0.21428571428571427,
          "f": 0.16513760994192422
        }
      }
    },
    {
      "paper_id": "cs.OS.cs/OS/2502.10923v2",
      "true_abstract": "The emergence of symmetric multi-processing (SMP) systems with non-uniform\nmemory access (NUMA) has prompted extensive research on process and data\nplacement to mitigate the performance impact of NUMA on applications. However,\nexisting solutions often overlook the coordination between the CPU scheduler\nand memory manager, leading to inefficient thread and page table placement.\nMoreover, replication techniques employed to improve locality suffer from\nredundant replicas, scalability barriers, and performance degradation due to\nmemory bandwidth and inter-socket interference. In this paper, we present\nPhoenix, a novel integrated CPU scheduler and memory manager with on-demand\npage table replication mechanism. Phoenix integrates the CPU scheduler and\nmemory management subsystems, allowing for coordinated thread and page table\nplacement. By differentiating between data and page table pages, Phoenix\nenables direct migration or replication of page tables based on application\nbehavior. Additionally, Phoenix employs memory bandwidth management mechanism\nto maintain Quality of Service (QoS) while mitigating coherency maintenance\noverhead. We implemented Phoenix as a loadable kernel module for Linux,\nensuring compatibility with legacy applications and ease of deployment. Our\nevaluation on real hardware demonstrates that Phoenix reduces CPU cycles by\n2.09x and page-walk cycles by 1.58x compared to state-of-the-art solutions.",
      "generated_abstract": "growth of mobile device adoption and the rise of the Internet of\nThings (IoT) have significantly increased the demand for network connectivity\nwithin embedded systems. While traditional Ethernet networks are well suited\nfor these applications, they are limited in scale, performance, and\nreliability. To address these challenges, this paper presents a scalable and\nreliable wireless network solution for embedded systems. The solution is\ndesigned to support a wide range of application scenarios, including sensor\nnetworks, wireless IoT devices, and edge computing. It leverages low-power\nwireless communication technologies, such as Wi-Fi and Bluetooth, to provide\nhigh-speed and reliable connectivity for devices. By integrating these\ntechnologies into a unified wireless network framework, the solution offers\nseamless connectivity across various wireless networks, enabling seamless\ncommunication between devices and the cloud. This",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10077519379844961,
          "p": 0.14606741573033707,
          "f": 0.11926605021420776
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09302325581395349,
          "p": 0.1348314606741573,
          "f": 0.11009173828760227
        }
      }
    },
    {
      "paper_id": "math.CV.math/OA/2503.09112v1",
      "true_abstract": "In this paper, we provide a complete characterization of bounded Toeplitz\noperators $T_f$ on the harmonic Bergman space of the unit disk, where the\nsymbol $f$ has a polar decomposition truncated above, that commute with\n$T_{z+\\bar{g}}$, for a bounded analytic function $g$.",
      "generated_abstract": "aper, we study the existence of a weak solution to a system of\nnumerical equations of the form\n$u_{tt}-c(x,t)\\Delta u+q(x,t)u=0$ subject to the initial condition $u(x,0)=\nu_0(x)$ for $x\\in \\mathbb{R}$, where $c(x,t)>0$ is a given function and $q(x,t)$\nis a given function. We apply the method of numerical continuation to\ndiscretize the initial value problem. The problem is reduced to a system of\nnumerical equations of the form\n$v_{tt}-c(x,t)\\Delta v+q(x,t)v=0$ subject to the initial condition $v(x,0)=v_0(x)$\nfor $x\\in \\mathbb{R}$, where $c(x,t)>",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19444444444444445,
          "p": 0.1590909090909091,
          "f": 0.17499999505000016
        },
        "rouge-2": {
          "r": 0.024390243902439025,
          "p": 0.016666666666666666,
          "f": 0.01980197537496441
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.13636363636363635,
          "f": 0.14999999505000017
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.cond-mat/dis-nn/2503.09457v1",
      "true_abstract": "The effective conductivity ($T^{eff}$) of 2D and 3D Random Resistor Networks\n(RRNs) with random edge conductivity is studied. The combined influence of\ngeometrical disorder, which controls the overall connectivity of the medium and\nleads to percolation effects, and conductivity randomness is investigated. A\nformula incorporating connectivity aspects and second-order averaging methods,\nwidely used in the stochastic hydrology community, is derived and extrapolated\nto higher orders using a power averaging formula based on a mean-field\nargument. This approach highlights the role of the so-called resistance\ndistance introduced by graph theorists. Simulations are performed on various\nRRN geometries constructed from 2D and 3D bond-percolation lattices. The\nresults confirm the robustness of the power averaging technique and the\nrelevance of the mean-field assumption.",
      "generated_abstract": "uce a quantum model for the interplay between the quantum and\nquantum-classical dynamics of a quantum system in a strongly-correlated\nenvironment. In this model, the quantum dynamics of the system is described by\na master equation, while the classical dynamics is described by a\nMarkovian-approximated master equation, which is obtained from the master\nequation by neglecting the interaction with the environment. We show that this\napproach leads to a rich phase diagram that describes the quantum-classical\ndynamics of the system in a strongly-correlated environment. In particular, we\ndemonstrate that the system can be in an intermediate phase, where both quantum\nand classical dynamics are strongly-correlated, and where the quantum-classical\ndynamics can be approximated by a master equation. We discuss the effect of\nthe system size on the phase diagram and show that the system size plays an\nimportant role in determ",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19047619047619047,
          "p": 0.24615384615384617,
          "f": 0.2147650957524437
        },
        "rouge-2": {
          "r": 0.017699115044247787,
          "p": 0.018518518518518517,
          "f": 0.018099542513872923
        },
        "rouge-l": {
          "r": 0.13095238095238096,
          "p": 0.16923076923076924,
          "f": 0.1476510017927122
        }
      }
    },
    {
      "paper_id": "math.DG.math/AG/2503.10517v1",
      "true_abstract": "We consider the group $\\mathcal G$ which is the semidirect product of the\ngroup of analytic functions with values in ${\\mathbb C}^*$ on the circle and\nthe group of analytic diffeomorphisms of the circle that preserve the\norientation. Then we construct the central extensions of the group $\\mathcal G$\nby the group ${\\mathbb C}^*$. The first central extension, so-called the\ndeterminant central extension, is constructed by means of determinants of\nlinear operators acting in infinite-dimensional locally convex topological\n$\\mathbb C$-vector spaces. Other central extensions are constructed by\n$\\cup$-products of group $1$-cocycles with the application to them the map\nrelated with algebraic $K$-theory. We prove in the second cohomology group,\ni.e. modulo of a group $2$-coboundary, the equality of the $12$th power of the\n$2$-cocycle constructed by the first central extension and the product of\ninteger powers of the $2$-cocycles constructed above by means of\n$\\cup$-products (in multiplicative notation). As an application of this result\nwe obtain the new topological Riemann-Roch theorem for a complex line bundle\n$L$ on a smooth manifold $M$, where $\\pi :M \\to B$ is a fibration in oriented\ncircles. More precisely, we prove that in the group $H^3(B, {\\mathbb Z})$ the\nelement $12 \\, [ {\\mathcal Det} (L)]$ is equal to the element $6 \\, \\pi_* (\nc_1(L) \\cup c_1(L))$, where $[{\\mathcal Det} (L)]$ is the class of the\ndeterminant gerbe on $B$ constructed by $L$ and the determinant central\nextension.",
      "generated_abstract": "In this paper we study the asymptotic behavior of the $m$-th power series\n$\\sum_{n=0}^\\infty\\frac{1}{n+m}$ as $m\\to\\infty$. Our main result is that\n$\\sum_{n=0}^\\infty\\frac{1}{n+m}$ is asymptotically equivalent to the $m$-th\npower series of the generalized Riemann zeta function, defined by\n$\\zeta_m(s)=\\sum_{n=1}^\\infty\\frac{1}{n^s}$. We also study the asymptotic\nbehavior of the $m$-th power series of the generalized Riemann zeta function\nwhen $m$ tends to infinity. In particular, we show that the $m$-th power\nseries of $\\zeta_m(s)$ converges on the interval $[1,2",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09375,
          "p": 0.2727272727272727,
          "f": 0.13953487991346683
        },
        "rouge-2": {
          "r": 0.015228426395939087,
          "p": 0.05454545454545454,
          "f": 0.02380952039714083
        },
        "rouge-l": {
          "r": 0.09375,
          "p": 0.2727272727272727,
          "f": 0.13953487991346683
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/OT/2407.11518v1",
      "true_abstract": "In this paper, we present a new ensemble-based filter method by\nreconstructing the analysis step of the particle filter through a transport\nmap, which directly transports prior particles to posterior particles. The\ntransport map is constructed through an optimization problem described by the\nMaximum Mean Discrepancy loss function, which matches the expectation\ninformation of the approximated posterior and reference posterior. The proposed\nmethod inherits the accurate estimation of the posterior distribution from\nparticle filtering. To improve the robustness of Maximum Mean Discrepancy, a\nvariance penalty term is used to guide the optimization. It prioritizes\nminimizing the discrepancy between the expectations of highly informative\nstatistics for the approximated and reference posteriors. The penalty term\nsignificantly enhances the robustness of the proposed method and leads to a\nbetter approximation of the posterior. A few numerical examples are presented\nto illustrate the advantage of the proposed method over the ensemble Kalman\nfilter.",
      "generated_abstract": "f a deep neural network (DNN) to predict the state of an\nengine is a common and important application in the automotive industry.\nHowever, DNNs are known to be sensitive to input noise, and this can lead to\nunreliable predictions. In this work, we explore the use of a non-DNN\napproach to improve the reliability of DNN-based predictions of engine states.\nWe propose a novel ensemble method that employs a set of DNNs, each trained on\na different input and output feature set, to improve the reliability of the\nDNNs. The ensemble method is trained on a data set that includes the input\nfeatures of the DNNs, and the output features of the DNNs. We validate the\neffectiveness of our approach on a data set that includes DNN-based engine\nstate predictions. We find that our ensemble method significantly improves the\npredict",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1797752808988764,
          "p": 0.22535211267605634,
          "f": 0.19999999506328137
        },
        "rouge-2": {
          "r": 0.022556390977443608,
          "p": 0.024390243902439025,
          "f": 0.023437495007630458
        },
        "rouge-l": {
          "r": 0.1348314606741573,
          "p": 0.16901408450704225,
          "f": 0.1499999950632814
        }
      }
    },
    {
      "paper_id": "hep-th.gr-qc/2503.10598v1",
      "true_abstract": "The basic observables in cosmology are known as in-in correlators. Recent\ncalculations have revealed that in-in correlators in four dimensional de Sitter\nspace exhibit hidden simplicity stemming from a close relation to scattering\namplitudes in flat space. In this paper we explain how to make this property\nmanifest by dressing flat space Feynman diagrams with certain auxiliary\npropagators. These dressing rules hold for any order in perturbation theory and\ncan be derived for a broad range of scalar theories, including those with\ninfrared divergences. In the latter case we show that they reproduce the same\ninfrared divergences predicted by the Schwinger-Keldysh formalism.",
      "generated_abstract": "In this paper we study the dynamics of a scalar field in a potential that\ncan be described as a product of a quadratic and an exponential potential.\nThe quadratic part of the potential is a generic feature of the many-body\ntheory of integrable systems. We show that, under suitable conditions, this\ndynamics can be mapped onto the dynamics of a class of integrable systems. We\nthen apply this method to the dynamics of the field in a general quartic\npotential.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.225,
          "p": 0.3829787234042553,
          "f": 0.2834645622667246
        },
        "rouge-2": {
          "r": 0.04081632653061224,
          "p": 0.06060606060606061,
          "f": 0.0487804829952414
        },
        "rouge-l": {
          "r": 0.175,
          "p": 0.2978723404255319,
          "f": 0.22047243628247265
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2501.03269v1",
      "true_abstract": "This study investigates the resilience of Environmental, Social, and\nGovernance (ESG) investments during periods of financial instability, comparing\nthem with traditional equity indices across major European markets-Germany,\nFrance, and Italy. Using daily returns from October 2021 to February 2024, the\nanalysis explores the effects of key global disruptions such as the Covid-19\npandemic and the Russia-Ukraine conflict on market performance. A mixture of\ntwo generalised normal distributions (MGND) and EGARCH-in-mean models are used\nto identify periods of market turmoil and assess volatility dynamics. The\nfindings indicate that during crises, ESG investments present higher volatility\nin Germany and Italy than in France. Despite some regional variations, ESG\nportfolios demonstrate greater resilience compared to traditional ones,\noffering potential risk mitigation during market shocks. These results\nunderscore the importance of integrating ESG factors into long-term investment\nstrategies, particularly in the face of unpredictable financial turmoil.",
      "generated_abstract": "The development of blockchain technology has sparked significant interest in\ndifferent applications, such as digital currencies, decentralized exchanges,\nasset management, and smart contracts. However, the security and reliability\nof blockchain systems are still under debate. In this paper, we propose a\ncryptographic framework for secure and reliable smart contract execution on\nblockchain platforms. The proposed framework employs the RSA signature scheme\nto securely sign contracts and the Diffie-Hellman key exchange algorithm to\nestablish secure communication channels. Additionally, we propose a\nreliable-execution algorithm to ensure contract execution integrity, which\nintegrates the Diffie-Hellman key exchange algorithm and the multi-party\ncomputation protocol. The proposed framework is evaluated through a series of\ncase studies. The results demonstrate that our framework offers secure and\nreliable execution of smart contracts on blockchain platforms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1308411214953271,
          "p": 0.17073170731707318,
          "f": 0.14814814323563189
        },
        "rouge-2": {
          "r": 0.014388489208633094,
          "p": 0.01834862385321101,
          "f": 0.016129027331231994
        },
        "rouge-l": {
          "r": 0.12149532710280374,
          "p": 0.15853658536585366,
          "f": 0.1375661326536213
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/SC/2503.00965v1",
      "true_abstract": "G protein-coupled receptors (GPCRs) represent a diverse and vital family of\nmembrane proteins that mediate intracellular signaling in response to\nextracellular stimuli, playing critical roles in physiology and disease.\nTraditionally recognized as chemical signal transducers, GPCRs have recently\nbeen implicated in mechanotransduction, the process of converting mechanical\nstimuli into cellular responses. This review explores the emerging role of\nGPCRs in sensing and responding to mechanical forces, with a particular focus\non the cardiovascular system. Cardiovascular homeostasis is heavily influenced\nby mechanical forces such as shear stress, cyclic stretch, and pressure, which\nare central to both normal physiology and the pathogenesis of diseases like\nhypertension and atherosclerosis. GPCRs, including the angiotensin II type 1\nreceptor (AT1R) and the $\\beta$2-adrenergic receptor ($\\beta$2-AR), have\ndemonstrated the ability to integrate mechanical and chemical signals,\npotentially through conformational changes and/or modulation of lipid\ninteractions, leading to biased signaling. Recent studies highlight the dual\nactivation mechanisms of GPCRs, with $\\beta$2-AR now serving as a key example\nof how mechanical and ligand-dependent pathways contribute to cardiovascular\nregulation. This review synthesizes current knowledge of GPCR\nmechanosensitivity, emphasizing its implications for cardiovascular health and\ndisease, and explores advancements in methodologies poised to further unravel\nthe mechanistic intricacies of these receptors.",
      "generated_abstract": "t a comprehensive review of the emerging field of single-cell RNA\nsequencing (scRNA-seq) in neurobiology. scRNA-seq provides unprecedented\ninsights into the molecular basis of brain function and disease, and has\ntransformed the study of brain development, disease pathogenesis, and therapeutic\nresearch. scRNA-seq provides a powerful tool for studying the complex\ninteractions between genetic, environmental, and epigenetic factors in the\nneurosciences. In this review, we highlight the technical advancements and\nmethodological challenges of scRNA-seq in neurobiology, emphasizing the\nimportance of innovative strategies for sample preparation and data\nanalysis. We discuss the current limitations and future directions for scRNA-seq\nin neurobiology, including the development of next-generation sequencing",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11188811188811189,
          "p": 0.2222222222222222,
          "f": 0.14883720484759344
        },
        "rouge-2": {
          "r": 0.025510204081632654,
          "p": 0.05,
          "f": 0.03378377930971572
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.18055555555555555,
          "f": 0.12093022810340742
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.06769v1",
      "true_abstract": "This paper proposes an innovative solution to the growing issue of greenhouse\ngas emissions: a closed photobioreactor (PBR) fa\\c{c}ade system to mitigate\ngreenhouse gas (GHG) concentrations. With digital fabrication technology, this\nstudy explores the transition from traditional, single function building\nfacades to multifunctional, integrated building systems. It introduces a\nphotobioreactor (PBR) fa\\c{c}ade system to mitigate greenhouse gas (GHG)\nconcentrations while addressing the challenge of large-scale prefabricated\ncomponents transportation. This research introduces a novel approach by\ndesigning the fa\\c{c}ade system as modular, user-friendly and\ntransportation-friendly bricks, enabling the creation of a user-customized and\nself-assembled photobioreactor (PBR) system. The single module in the system is\nproposed to be \"neutralization bricks\", which embedded with algae and equipped\nwith an air circulation system, facilitating the photobioreactor (PBR)'s\nfunctionality. A connection system between modules allows for easy assembly by\nusers, while a limited variety of brick styles ensures modularity in\nmanufacturing without sacrificing customization and diversity. The system is\nalso equipped with an advanced microalgae status detection algorithm, which\nallows users to monitor the condition of the microalgae using monocular camera.\nThis functionality ensures timely alerts and notifications for users to replace\nthe algae, thereby optimizing the operational efficiency and sustainability of\nthe algae cultivation process.",
      "generated_abstract": "t rise of the Metaverse, the next generation of virtual reality,\nhas attracted significant interest from the public, academia, and industry,\nwith the potential to transform many aspects of our daily lives. However, the\ndevelopment of the Metaverse also raises significant challenges, such as\nprivacy and security issues, which must be addressed for the Metaverse to be\nsuccessful. This paper presents a novel framework that combines the concepts of\ncomputational intelligence and deep learning to address these challenges. The\nframework includes a machine learning model that extracts semantic information\nfrom a large number of images to classify them into three categories:\nreal, virtual, and augmented. The model is then used to assess the privacy\nrisks posed by the Metaverse, focusing on the risks of real-world data\nleakage, online privacy violations, and unauthorized use of virtual assets. The\nresults",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14173228346456693,
          "p": 0.1875,
          "f": 0.1614349726750992
        },
        "rouge-2": {
          "r": 0.02197802197802198,
          "p": 0.03007518796992481,
          "f": 0.025396820517813987
        },
        "rouge-l": {
          "r": 0.11023622047244094,
          "p": 0.14583333333333334,
          "f": 0.12556053321321584
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2410.09069v2",
      "true_abstract": "The rapid expansion of e-commerce and the widespread use of credit cards in\nonline purchases and financial transactions have significantly heightened the\nimportance of promptly and accurately detecting credit card fraud (CCF). Not\nonly do fraudulent activities in financial transactions lead to substantial\nmonetary losses for banks and financial institutions, but they also undermine\nuser trust in digital services. This study presents a new stacking-based\napproach for CCF detection by adding two extra layers to the usual\nclassification process: an attention layer and a confidence-based combination\nlayer. In the attention layer, we combine soft outputs from a convolutional\nneural network (CNN) and a recurrent neural network (RNN) using the dependent\nordered weighted averaging (DOWA) operator, and from a graph neural network\n(GNN) and a long short-term memory (LSTM) network using the induced ordered\nweighted averaging (IOWA) operator. These weighted outputs capture different\npredictive signals, increasing the model's accuracy. Next, in the\nconfidence-based layer, we select whichever aggregate (DOWA or IOWA) shows\nlower uncertainty to feed into a meta-learner. To make the model more\nexplainable, we use shapley additive explanations (SHAP) to identify the top\nten most important features for distinguishing between fraud and normal\ntransactions. These features are then used in our attention-based model.\nExperiments on three datasets show that our method achieves high accuracy and\nrobust generalization, making it effective for CCF detection.",
      "generated_abstract": "r provides a review of the literature on the use of\nfinancial variables as predictors in risk management. We focus on the most\nfrequently cited applications in the literature. The review focuses on the use\nof stock market returns, option prices and cross-sectional data on stock\nreturns, with a particular focus on the use of cross-sectional data on stock\nreturns. The review highlights the use of volatility models to predict the\nvolatility of stock returns and the use of market-index-related variables to\npredict the return on an index. The review also examines the use of risk\nfactors and risk-return trade-offs, the use of stock-specific factors to\npredict stock returns, the use of factor models to predict the return on a\nstock-specific index, the use of factor models to predict the return on an\nindex of stocks, and the use of factor models to predict the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08333333333333333,
          "p": 0.24528301886792453,
          "f": 0.12440191008997059
        },
        "rouge-2": {
          "r": 0.014285714285714285,
          "p": 0.03333333333333333,
          "f": 0.01999999580000088
        },
        "rouge-l": {
          "r": 0.07692307692307693,
          "p": 0.22641509433962265,
          "f": 0.11483253209953999
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.cond-mat/dis-nn/2503.07796v1",
      "true_abstract": "Recently, a new frontier in computing has emerged with physical neural\nnetworks(PNNs) harnessing intrinsic physical processes for learning. Here, we\nexplore topological mechanical neural networks(TMNNs) inspired by the quantum\nspin Hall effect(QSHE) in topological metamaterials, for machine learning\nclassification tasks. TMNNs utilize pseudospin states and the robustness of the\nQSHE, making them damage-tolerant for binary classification. We first\ndemonstrate data clustering using untrained TMNNs. Then, for specific tasks, we\nderive an in situ backpropagation algorithm - a two-step, local-rule method\nthat updates TMNNs using only local information, enabling in situ physical\nlearning. TMNNs achieve high accuracy in classifications of Iris flowers,\nPenguins, and Seeds while maintaining robustness against bond pruning.\nFurthermore, we demonstrate parallel classification via frequency-division\nmultiplexing, assigning different tasks to distinct frequencies for enhanced\nefficiency. Our work introduces in situ backpropagation for wave-based\nmechanical neural networks and positions TMNNs as promising neuromorphic\ncomputing hardware for classification tasks.",
      "generated_abstract": "We present a novel approach to the design and characterization of\nsymmetry-preserving neural networks. Our method combines symmetry-aware\ntraining with a gradient-based learning algorithm that allows the network to\nincorporate and exploit symmetries during training. We demonstrate that this\napproach can lead to significantly improved performance on challenging\nsymmetry-constrained tasks. Our approach also outperforms standard\nsymmetry-breaking approaches, offering an effective and reliable method for\nenhancing the robustness of neural networks. Our work provides a practical\nsolution for the design and improvement of neural networks, enabling more\nefficient and effective applications in machine learning and artificial\nintelligence.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2169811320754717,
          "p": 0.34328358208955223,
          "f": 0.2658959490113269
        },
        "rouge-2": {
          "r": 0.03496503496503497,
          "p": 0.05555555555555555,
          "f": 0.04291845019433087
        },
        "rouge-l": {
          "r": 0.20754716981132076,
          "p": 0.3283582089552239,
          "f": 0.25433525536970836
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2502.05336v1",
      "true_abstract": "This paper introduces Monotone Delta, an order-theoretic measure designed to\nenhance the reliability assessment of survey-based instruments in human-machine\ninteractions. Traditional reliability measures, such as Cronbach's Alpha and\nMcDonald's Omega, often yield misleading estimates due to their sensitivity to\nredundancy, multidimensional constructs, and assumptions of normality and\nuncorrelated errors. These limitations can compromise decision-making in\nhuman-centric evaluations, where survey instruments inform adaptive interfaces,\ncognitive workload assessments, and human-AI trust models. Monotone Delta\naddresses these issues by quantifying internal consistency through the\nminimization of ordinal contradictions and alignment with a unidimensional\nlatent order using weighted tournaments. Unlike traditional approaches, it\noperates without parametric or model-based assumptions. We conducted\ntheoretical analyses and experimental evaluations on four challenging\nscenarios: tau-equivalence, redundancy, multidimensionality, and non-normal\ndistributions, and proved that Monotone Delta provides more stable reliability\nassessments compared to existing methods. The Monotone Delta is a valuable\nalternative for evaluating questionnaire-based assessments in psychology, human\nfactors, healthcare, and interactive system design, enabling organizations to\noptimize survey instruments, reduce costly redundancies, and enhance confidence\nin human-system interactions.",
      "generated_abstract": "California drought had a significant impact on water supply,\nrequiring a variety of data-driven approaches to assess the water supply\nchallenges. This study presents a methodology to analyze a dataset that\ncomprises monthly water demand and supply data from 2014 to 2023, using\nmachine learning models and applying a data-driven approach. The dataset was\nderived from the California Department of Water Resources (CDWR) and analyzed\nusing data from the California Drought Monitor, which provides monthly\nrepresentative indicators of drought conditions. This study examined the\ninterplay between water supply and demand and the effects of drought on water\ndemand. A linear regression model was developed to analyze the relationship\nbetween water supply and demand. The results indicated that the drought severity\nwas positively associated with water demand, with the greatest impact occurring",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0948905109489051,
          "p": 0.17567567567567569,
          "f": 0.12322274426091077
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0948905109489051,
          "p": 0.17567567567567569,
          "f": 0.12322274426091077
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/MF/2501.07135v1",
      "true_abstract": "We present a systematic, trend-following strategy, applied to commodity\nfutures markets, that combines univariate trend indicators with cross-sectional\ntrend indicators that capture so-called {\\em momentum spillover}, which can\noccur when there is a lead-lag relationship between the trending behaviour of\ndifferent markets. Our strategy utilises two methods for detecting lead-lag\nrelationships, with a method for computing {\\em network momentum}, to produce a\nnovel trend-following indicator. We use our new trend indicator to construct a\nportfolio whose performance we compare to a baseline model which uses only\nunivariate indicators, and demonstrate statistically significant improvements\nin Sharpe ratio, skewness of returns, and downside performance, using synthetic\nbootstrapped data samples taken from time-series of actual prices.",
      "generated_abstract": "This paper proposes a novel framework for the design of a portfolio that\noptimally allocates among various assets with a minimal average risk. The\nframework is based on the idea of constructing a convex envelope of the\nindividual risk measures, where the risk measure of each asset is used to\nconstruct the convex envelope of the risk measures of the assets. A\ntwo-asset model is considered, where the assets are stocks from the S\\&P500\nindex and the risk measures are standardized Sharpe ratios. The optimization\nproblem is formulated as a convex optimization problem, which is solved using\nthe interior-point method. Numerical examples are provided to illustrate the\neffectiveness of the proposed approach.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20224719101123595,
          "p": 0.2608695652173913,
          "f": 0.22784809634593825
        },
        "rouge-2": {
          "r": 0.036036036036036036,
          "p": 0.04,
          "f": 0.037914686956717715
        },
        "rouge-l": {
          "r": 0.16853932584269662,
          "p": 0.21739130434782608,
          "f": 0.18987341280163447
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.physics/chem-ph/2503.08880v1",
      "true_abstract": "Density matrix embedding theory (DMET) provides a framework to describe\nground-state expectation values in strongly correlated systems, but its\nextension to dynamical quantities is still an open problem. We demonstrate one\nroute to obtaining excitations and dynamical spectral functions by using the\ntechniques of DMET to approximate the matrix elements that arise in a\nsingle-mode inspired excitation ansatz. We demonstrate this approach in the 1D\nHubbard model, comparing the neutral excitations, single-particle density of\nstates, charge, and spin dynamical structure factors to benchmarks from the\nBethe ansatz and density matrix renormalization group. Our work highlights the\npotential of these ideas in building computationally efficient approaches for\ndynamical quantities.",
      "generated_abstract": "We study the competition between decoherence and the decay of electron\nenergy in a two-level system with a static spin-orbit coupling. We show that the\nsystem undergoes a quantum phase transition, from a high-energy to a low-energy\nground state, when the decoherence time is comparable to the decay\ntime. The transition is first-order and is driven by a crossover from\nhigh-energy to low-energy coherent states in the energy spectrum. We find that\nthe decoherence time can be controlled by tuning the spin-orbit coupling. This\nwork demonstrates how a control parameter, which is a characteristic of a\nspecific system, can be manipulated to control the phase transition.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.2,
          "f": 0.17021276106835687
        },
        "rouge-2": {
          "r": 0.01904761904761905,
          "p": 0.02040816326530612,
          "f": 0.019704428503483515
        },
        "rouge-l": {
          "r": 0.13580246913580246,
          "p": 0.18333333333333332,
          "f": 0.1560283639052363
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.02729v1",
      "true_abstract": "This paper introduces a novel low-complexity memoryless linearizer for\nsuppression of distortion in analog frontends. It is based on our recently\nintroduced linearizer which is inspired by neural networks, but with\norders-of-magnitude lower complexity than conventional neural-networks\nconsidered in this context, and it can also outperform the conventional\nparallel memoryless Hammerstein linearizer. Further, it can be designed through\nmatrix inversion and thereby the costly and time consuming numerical\noptimization traditionally used when training neural networks is avoided. The\nlinearizer proposed in this paper is different in that it uses 1-bit\nquantizations as nonlinear activation functions and different bias values.\nThese features enable a look-up table implementation which eliminates all but\none of the multiplications and additions required for the linearization.\nExtensive simulations and comparisons are included in the paper, for distorted\nmulti-tone signals and bandpass filtered white noise, which demonstrate the\nefficacy of the proposed linearizer.",
      "generated_abstract": "opment of 5G networks is transforming the mobile communications\nscape. 5G envisions the integration of multiple wireless technologies, such as\nsmall cell, millimeter-wave (mmWave) and air-gap technologies, to enhance\nthroughput and spectral efficiency. However, these technologies are\ndifficult to integrate due to the unique challenges of coexistence, such as\ninterference management and spectrum sharing. In this paper, we propose a\ncoexistence strategy for mmWave-Air-Gap (MAG) networks, which can handle\nmultiple technologies in a unified way. We first analyze the coexistence\nproblems in MAG networks. Then, we introduce a coexistence strategy based on\nthe generalized non-causal interference alignment (GNCIA) algorithm. By\nidentifying the coexistence-related interference sources, we design a\nco",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1619047619047619,
          "p": 0.2125,
          "f": 0.18378377887509142
        },
        "rouge-2": {
          "r": 0.014084507042253521,
          "p": 0.01904761904761905,
          "f": 0.016194327096003703
        },
        "rouge-l": {
          "r": 0.1523809523809524,
          "p": 0.2,
          "f": 0.17297296806428064
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.09489v1",
      "true_abstract": "Integrated sensing and communications (ISAC) has emerged as a promising\nparadigm to unify wireless communications and radar sensing, enabling efficient\nspectrum and hardware utilization. A core challenge with realizing the gains of\nISAC stems from the unique challenges of dual purpose beamforming design due to\nthe highly non-convex nature of key performance metrics such as sum rate for\ncommunications and the Cramer-Rao lower bound (CRLB) for sensing. In this\npaper, we propose a low-complexity structured approach to ISAC beamforming\noptimization to simultaneously enhance spectral efficiency and estimation\naccuracy. Specifically, we develop a successive convex approximation (SCA)\nbased algorithm which transforms the original non-convex problem into a\nsequence of convex subproblems ensuring convergence to a locally optimal\nsolution. Furthermore, leveraging the proposed SCA framework and the Lagrange\nduality, we derive the optimal beamforming structure for CRLB optimization in\nISAC systems. Our findings characterize the reduction in radar streams one can\nemploy without affecting performance. This enables a dimensionality reduction\nthat enhances computational efficiency. Numerical simulations validate that our\napproach achieves comparable or superior performance to the considered\nbenchmarks while requiring much lower computational costs.",
      "generated_abstract": "cement of 5G and the Internet of Things (IoT) have driven the\ndevelopment of uplink massive multiple-input multiple-output (uMimo) systems,\nwhich have become the dominant wireless communication mode. However, the\ncomplex signal processing challenges posed by the uMimo system remain\nunderexplored. In this paper, we focus on the uMimo system with arbitrary\nmulti-antenna users and arbitrary multi-antenna base stations (Bs), where the\nBs can be any single-antenna base station or any multi-antenna Bs. In this\nsetting, we propose a novel uMimo channel estimation algorithm that\nsimultaneously estimates the channel coefficients of the Bs and the users,\nwith a single channel estimation step. This is achieved by leveraging the\ninterference-free communication channel between the Bs and the users and the\nadditive white Gaussian noise",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16153846153846155,
          "p": 0.26582278481012656,
          "f": 0.2009569330967699
        },
        "rouge-2": {
          "r": 0.03910614525139665,
          "p": 0.06363636363636363,
          "f": 0.048442901859413065
        },
        "rouge-l": {
          "r": 0.13846153846153847,
          "p": 0.22784810126582278,
          "f": 0.17224879912547802
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.01640v1",
      "true_abstract": "We study certain properties of modules over 1-dimensional local integral\ndomains. First, we examine the order of the conductor ideal and its expected\nrelationship with multiplicity. Next, we investigate the reflexivity of certain\ncolength-two ideals. Finally, we consider the freeness problem of the absolute\nintegral closure of a DVR, and connect this to the reflexivity problem of\n$R^{\\frac{1}{p^n}}$.",
      "generated_abstract": "We introduce a new class of metric spaces, which we call ``non-admissible\nmetric spaces'', and study their geometric properties. We provide a complete\ncharacterization of the non-admissible metric spaces, and show that all of the\nclassical metric spaces are non-admissible. We also study the structure of the\nnon-admissible metric spaces. We provide a complete classification of the\nnon-admissible metric spaces with respect to the $L^p$-topology, for $1 \\leq\np < 2$ and the $L^p$-topology, for $2 \\leq p < 2^*$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24390243902439024,
          "p": 0.22727272727272727,
          "f": 0.23529411265328729
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.03225806451612903,
          "f": 0.034482753644471585
        },
        "rouge-l": {
          "r": 0.21951219512195122,
          "p": 0.20454545454545456,
          "f": 0.21176470088858146
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2411.19436v2",
      "true_abstract": "In economic analysis, rational decision-makers often take actions to reduce\ntheir risk exposure. These actions include purchasing market insurance and\nimplementing prevention measures to modify the shape of the loss distribution.\nUnder the assumption that the insureds' actions are fully observed by the\ninsurer, this paper investigates the interaction between self-protection and\ninsurance demand when insurance premiums are determined by convex premium\nprinciples within the framework of distortion risk measures. Specifically, the\ninsured selects an optimal proportional insurance share and prevention effort\nto minimize the risk measure of their end-of-period exposure. We explicitly\ncharacterize the optimal combination of prevention effort and insurance demand\nin a self-protection model when the insured adopts tail value-at-risk or a\nsubclass with strictly concave distortion functions. Additionally, we conduct\ncomparative static analyses to illustrate our main findings under various\npremium structures, risk aversion levels, and loss distributions. Our results\nindicate that market insurance and self-protection are complementary,\nsupporting classical insights from the literature regarding corner insurance\npolicies (i.e., null and full insurance) in the absence of ex ante moral\nhazard. Finally, we consider the effects of moral hazard on the interaction\nbetween self-protection and insurance demand. Our findings show that ex ante\nmoral hazard shifts the complementary effect into substitution effect.",
      "generated_abstract": "In this paper, we analyze the stochastic volatility model with two parameters,\nthe volatility and the volatility index, which are correlated. We consider the\ncase that the volatility index is a Brownian motion and we use the Monte Carlo\nsimulation method to obtain the stochastic volatility model. We use the\nlog-log plot to plot the stochastic volatility model, and then we use the\nKolmogorov-Arnold diagram to analyze the model. In addition, we use the\nBrownian-Bessel model to illustrate the effect of the Brownian motion. We also\nuse the simulation method to obtain the Brownian-Bessel model and we use the\nKolmogorov-Arnold diagram to analyze the Brownian-Bessel model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12403100775193798,
          "p": 0.35555555555555557,
          "f": 0.183908042142291
        },
        "rouge-2": {
          "r": 0.020942408376963352,
          "p": 0.05405405405405406,
          "f": 0.030188675219936463
        },
        "rouge-l": {
          "r": 0.11627906976744186,
          "p": 0.3333333333333333,
          "f": 0.1724137892687278
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2411.16617v1",
      "true_abstract": "Quanto options allow the buyer to exchange the foreign currency payoff into\nthe domestic currency at a fixed exchange rate. We investigate quanto options\nwith multiple underlying assets valued in different foreign currencies each\nwith a different strike price in the payoff function. We carry out a\ncomparative performance analysis of different stochastic volatility (SV),\nstochastic correlation (SC), and stochastic exchange rate (SER) models to\ndetermine the best combination of these models for Monte Carlo (MC) simulation\npricing. In addition, we test the performance of all model variants with\nconstant correlation as a benchmark. We find that a combination of GARCH-Jump\nSV, Weibull SC, and Ornstein Uhlenbeck (OU) SER performs best. In addition, we\nanalyze different discretization schemes and their results. In our simulations,\nthe Milstein scheme yields the best balance between execution times and lower\nstandard deviations of price estimates. Furthermore, we find that incorporating\nmean reversion into stochastic correlation and stochastic FX rate modeling is\nbeneficial for MC simulation pricing. We improve the accuracy of our\nsimulations by implementing antithetic variates variance reduction. Finally, we\nderive the correlation risk parameters Cora and Gora in our framework so that\ncorrelation hedging of quanto options can be performed.",
      "generated_abstract": "r presents a novel method for calculating the aggregate wealth of a\ngroup of people. The method is based on the concept of the aggregate\nwealth-index and is based on the fact that a person's wealth is a function of\ntheir income, expenses, and the value of their assets. This method allows for\nthe calculation of the aggregate wealth of a group of people, regardless of\nwho the group members are. The method also allows for the calculation of the\naggregate wealth of a person, regardless of the value of their assets or their\nincome. The method can be used to calculate the aggregate wealth of a group of\npeople, or it can be used to calculate the aggregate wealth of a person. The\nmethod can be used to calculate the aggregate wealth of a group of people and\nthe aggregate wealth of a person, and it can be used to calculate the aggregate\nwealth of a group of people and the aggregate wealth of a person. This",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10483870967741936,
          "p": 0.2708333333333333,
          "f": 0.15116278667387786
        },
        "rouge-2": {
          "r": 0.0106951871657754,
          "p": 0.024390243902439025,
          "f": 0.014869884237643028
        },
        "rouge-l": {
          "r": 0.0967741935483871,
          "p": 0.25,
          "f": 0.13953487969713368
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2407.05866v1",
      "true_abstract": "We introduce generalizations of the COGARCH model of Kl\\\"uppelberg et al.\nfrom 2004 and the volatility and price model of Barndorff-Nielsen and Shephard\nfrom 2001 to a Markov-switching environment. These generalizations allow for\nexogeneous jumps of the volatility at times of a regime switch. Both models are\nstudied within the framework of Markov-modulated generalized Ornstein-Uhlenbeck\nprocesses which allows to derive conditions for stationarity, formulas for\nmoments, as well as the autocovariance structure of volatility and price\nprocess. It turns out that both models inherit various properties of the\noriginal models and therefore are able to capture basic stylized facts of\nfinancial time-series such as uncorrelated log-returns, correlated squared\nlog-returns and non-existence of higher moments in the COGARCH case.",
      "generated_abstract": "t a novel approach to the estimation of the price impact, the\nprice change that occurs when the underlying asset is traded, which is of\nimportance in determining the price of the underlying asset. The price impact\nis often expressed as a ratio, where the numerator is the price change and the\ndenominator is the volume of trading. We derive a closed-form solution for the\nprice impact, which is valid for any trading volume. The proposed approach\nuses the stochastic volatility model to estimate the volatility of the\nunderlying asset and the implied volatility of the underlying asset. We\nestablish the asymptotic properties of the proposed approach, which allows us\nto establish the convergence of the estimator as the trading volume is\nincreased. We demonstrate the accuracy of the proposed approach through\nnumerical simulations. The proposed approach can be used to estimate the price\nimpact of the underlying",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20987654320987653,
          "p": 0.26153846153846155,
          "f": 0.23287670738881602
        },
        "rouge-2": {
          "r": 0.05454545454545454,
          "p": 0.05309734513274336,
          "f": 0.053811654193730483
        },
        "rouge-l": {
          "r": 0.18518518518518517,
          "p": 0.23076923076923078,
          "f": 0.20547944711484342
        }
      }
    },
    {
      "paper_id": "math.LO.math/CO/2503.09246v1",
      "true_abstract": "We introduce the notion of Ramsey partition regularity, a generalisation of\npartition regularity involving infinitary configurations. We provide\ncharacterisations of this notion in terms of certain ultrafilters related to\ntensor products and dubbed Ramsey's witnesses; and we also consider their\nnonstandard counterparts as pairs of hypernatural numbers, called Ramsey pairs.\nThese characterisations are then used to determine whether various\nconfigurations involving polynomials and exponentials are Ramsey partition\nregular over the natural numbers.",
      "generated_abstract": "We prove a generalization of the Cauchy-Kovalevsky theorem for groups\nthat are either solvable or of type $F_{2^n}$. It generalizes the result for\nfinite groups of type $A$ and $S_n$ for $n\\geq 2$, and extends the result for\nfinite groups of type $D_n$ for $n\\geq 2$ to $D_n$ for $n\\geq 3$. We also\nprovide a new proof of the result for finite groups of type $F_4$, which was\nalready known for $n\\geq 3$ and was conjectured to hold for $n=2$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16981132075471697,
          "p": 0.20930232558139536,
          "f": 0.18749999505425358
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11320754716981132,
          "p": 0.13953488372093023,
          "f": 0.12499999505425367
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2412.05508v1",
      "true_abstract": "Experimentation in online digital platforms is used to inform decision\nmaking. Specifically, the goal of many experiments is to optimize a metric of\ninterest. Null hypothesis statistical testing can be ill-suited to this task,\nas it is indifferent to the magnitude of effect sizes and opportunity costs.\nGiven access to a pool of related past experiments, we discuss how\nexperimentation practice should change when the goal is optimization. We survey\nthe literature on empirical Bayes analyses of A/B test portfolios, and single\nout the A/B Testing Problem (Azevedo et al., 2020) as a starting point, which\ntreats experimentation as a constrained optimization problem. We show that the\nframework can be solved with dynamic programming and implemented by\nappropriately tuning $p$-value thresholds. Furthermore, we develop several\nextensions of the A/B Testing Problem and discuss the implications of these\nresults on experimentation programs in industry. For example, under no-cost\nassumptions, firms should be testing many more ideas, reducing test allocation\nsizes, and relaxing $p$-value thresholds away from $p = 0.05$.",
      "generated_abstract": "We propose a novel method to estimate non-linear time-varying parameters in\nmodel-free settings, where the observed data are not fully correlated. This\napproach can be used in a wide range of situations where the observed data are\ngenerated from a non-linear stochastic process, but are not correlated. Our\nmethod is based on the notion of \\textit{joint asymptotic normality}, which is\nintroduced and used to show that the resulting estimator is asymptotically\nnormal. The method is illustrated through a simulation study and an application\nto the estimation of a binary treatment effect in an experimental setting.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.26153846153846155,
          "f": 0.18478260412629974
        },
        "rouge-2": {
          "r": 0.030864197530864196,
          "p": 0.056818181818181816,
          "f": 0.03999999543808052
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.26153846153846155,
          "f": 0.18478260412629974
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.astro-ph/CO/2503.09718v1",
      "true_abstract": "The strongly lensed Supernova (SN) Encore at a redshift of $z = 1.949$,\ndiscovered behind the galaxy cluster MACS J0138$-$2155 at $z=0.336$, provides a\nrare opportunity for time-delay cosmography and studies of the SN host galaxy,\nwhere previously another SN, called SN Requiem, had appeared. To enable these\nstudies, we combine new James Webb Space Telescope (JWST) imaging, archival\nHubble Space Telescope (HST) imaging, and new Very Large Telescope (VLT)\nspectroscopic data to construct state-of-the-art lens mass models that are\ncomposed of cluster dark-matter (DM) halos and galaxies. We determine the\nphotometric and structural parameters of the galaxies across six JWST and five\nHST filters. We use the color-magnitude and color-color relations of\nspectroscopically-confirmed cluster members to select additional cluster\nmembers, identifying a total of 84 galaxies belonging to the galaxy cluster. We\nconstruct seven different mass models using a variety of DM halo mass profiles,\nand explore both multi-plane and approximate single-plane lens models. As\nconstraints, we use the observed positions of 23 multiple images from eight\nmultiply lensed sources at four distinct spectroscopic redshifts. In addition,\nwe use stellar velocity dispersion measurements to obtain priors on the galaxy\nmass distributions. We find that six of the seven models fit well to the\nobserved image positions. Mass models with cored-isothermal DM profiles fit\nwell to the observations, whereas the mass model with a Navarro-Frenk-White\ncluster DM profile has an image-position $\\chi^2$ value that is four times\nhigher. We build our ultimate model by combining four multi-lens-plane mass\nmodels and predict the image positions and magnifications of SN Encore and SN\nRequiem. Our work lays the foundation for building state-of-the-art mass models\nof the cluster for future cosmological analysis and SN host galaxy studies.",
      "generated_abstract": "gravitational wave observation of a supernova (SN) merger\nis expected to be a milestone in astrophysics. In this work, we analyze the\nobservational properties of the first gravitational wave (GW) detection of a\nSN merger, which occurred on 2021-05-09. We focus on the three most\nsignificant GWs that have been detected, and discuss their impact on the\nunderstanding of the SN merger environment and its progenitors. First, we\ninvestigate the mass distribution of the progenitor stars and the distribution\nof the binary components. We find that the mass distribution of the progenitor\nstars is consistent with a truncated log-normal distribution, which is consistent\nwith previous estimates of the SN progenitors. The binary components exhibit\nan approximately uniform mass distribution, consistent with previous studies.\nSecond, we examine the distribution",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10919540229885058,
          "p": 0.25675675675675674,
          "f": 0.15322580226456828
        },
        "rouge-2": {
          "r": 0.026415094339622643,
          "p": 0.0660377358490566,
          "f": 0.03773584497497156
        },
        "rouge-l": {
          "r": 0.10344827586206896,
          "p": 0.24324324324324326,
          "f": 0.14516128613553603
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/CG/2503.06833v1",
      "true_abstract": "The Hausdorff distance is a fundamental measure for comparing sets of\nvectors, widely used in database theory and geometric algorithms. However, its\nexact computation is computationally expensive, often making it impractical for\nlarge-scale applications such as multi-vector databases. In this paper, we\nintroduce an approximation framework that efficiently estimates the Hausdorff\ndistance while maintaining rigorous error bounds. Our approach leverages\napproximate nearest-neighbor (ANN) search to construct a surrogate function\nthat preserves essential geometric properties while significantly reducing\ncomputational complexity. We provide a formal analysis of approximation\naccuracy, deriving both worst-case and expected error bounds. Additionally, we\nestablish theoretical guarantees on the stability of our method under\ntransformations, including translation, rotation, and scaling, and quantify the\nimpact of non-uniform scaling on approximation quality. This work provides a\nprincipled foundation for integrating Hausdorff distance approximations into\nlarge-scale data retrieval and similarity search applications, ensuring both\ncomputational efficiency and theoretical correctness.",
      "generated_abstract": "r presents a novel approach for the generation of multi-agent\nsafety verification specifications for the context of multi-robot systems.\nSpecifically, we focus on the safety verification of multi-robot teams in\ncontested environments, where robots must collaborate to complete their tasks.\nWhile existing approaches focus on ensuring safety for a single robot, we\npropose a method for generating verification specifications that consider the\nsafety of the entire team. The approach involves identifying a set of\ncontested-collision avoidance regions in which the team must safely cooperate\nto complete their tasks. We then generate a set of verification specifications\nthat ensure the team's safety in these regions. We evaluate our approach using\nthe RoboCup Collaborative Play 2024 competition dataset, and demonstrate that\nour approach achieves higher verification specifications quality and lower\nover",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14912280701754385,
          "p": 0.22077922077922077,
          "f": 0.17801046639182053
        },
        "rouge-2": {
          "r": 0.006944444444444444,
          "p": 0.008849557522123894,
          "f": 0.007782096240067313
        },
        "rouge-l": {
          "r": 0.14035087719298245,
          "p": 0.2077922077922078,
          "f": 0.16753926220333887
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.10478v1",
      "true_abstract": "Existing studies have shown that the monetization of silver in the Ming\nDynasty effectively promoted the prosperity of trade in the Ming Dynasty, while\nthe prices of labor, handicraft products and grain were long suppressed by the\ndeformed economic structure. With the expansion of silver application, the\nfluctuation of silver supply and demand exacerbated the above contradictions.\nCapital accumulation that should have been obtained through the marketization\nof labor was easily plundered by the landlord gentry class through silver. This\narticle re-discusses the issue from the perspective of supply and demand.\nCompared with the increase and then decrease of silver supply, the evolution of\nsilver demand is more complicated: at the tax level, the widespread use of\nsilver leads to a huge difference in the elasticity of production and trade\ntaxes. When government spending surges, the increase in tax burden will be\nmainly borne by agriculture and handicrafts. At the production level, the high\nliquidity of silver makes the concentration of social wealth more convenient,\nwhile the reduction in silver supply and the expansion of demand have rapidly\nexpanded deflation, further exacerbating the gap between the rich and the poor.\nSuch combined effect of supply and demand factors has caused the monetization\nof silver to become an accelerator of the economic collapse of the Ming\nDynasty.",
      "generated_abstract": "This study investigates the effects of the COVID-19 pandemic on the\ncivil society sector, focusing on the nonprofit sector. Using the 2019-2020\nCivil Society Index (CSI) as a benchmark, we compare the pre-pandemic period\n(2017-2019) to the period of the pandemic (2020-2022). The results indicate a\nsignificant decline in the CSI scores, with an average decrease of 2.42 points\n(95% confidence interval [CI",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07377049180327869,
          "p": 0.18,
          "f": 0.1046511586668471
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.04918032786885246,
          "f": 0.024291494256257845
        },
        "rouge-l": {
          "r": 0.05737704918032787,
          "p": 0.14,
          "f": 0.08139534471335878
        }
      }
    },
    {
      "paper_id": "math.PR.math/MP/2503.09486v1",
      "true_abstract": "The directed landscape, the central object in the Kardar-Parisi-Zhang\nuniversality class, is shown to be the scaling limit of various models by\nDauvergne and Vir\\'ag (2022) and Dauvergne, Ortmann and Vir\\'ag (2018). In his\nstudy of geodesics in upper tail deviations of the directed landscape, Liu\n(2022) put forward a conjecture about the rate of the lowest rate metric under\nwhich a geodesic between two points passes through a particular point between\nthem. Das, Dauvergne and Vir\\'ag (2024) disproved his conjecture, and made a\nconjecture of their own. This paper disproves that conjecture and puts the\nquestion to rest with an answer and a proof.",
      "generated_abstract": "We give a simple proof of a conjecture of R. H. Green that for a number field\n$K$ of degree $d$ over $\\mathbb{Q}$, the cardinality of the set of places\n$\\mathcal{P}$ of $K$ such that $K_{\\mathcal{P}}$ is unramified in $K$ is equal to\nthe cardinality of the set of places $\\mathcal{Q}$ of $K$ such that $K_{\\mathcal{Q}}$\nis ramified in $K$. We also prove that this conjecture holds for a general number\nfield $K$ of degree $d$ over $\\mathbb{Q}$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.225,
          "f": 0.16071428112244912
        },
        "rouge-2": {
          "r": 0.030612244897959183,
          "p": 0.05,
          "f": 0.03797467883352086
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.2,
          "f": 0.14285713826530627
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.00085v1",
      "true_abstract": "This study examines the influence of employee education and health on\nfirm-level Total Factor Productivity (TFP) in China, using panel data from\nA-share listed companies spanning from 2007 to 2022. The analysis shows that\nlife expectancy and higher education have a significant impact on TFP. More\noptimal health conditions can result in increased productivity through\ndecreased absenteeism and improved work efficiency. Similarly, higher levels of\neducation can support technological adaptation, innovation, and managerial\nefficiency. Nevertheless, the correlation between health and higher education\nindicates that there may be a point where further improvements in health yield\ndiminishing returns in terms of productivity for individuals with advanced\neducation. These findings emphasise the importance of implementing\ncomprehensive policies that improve both health and education, maximising their\nimpact on productivity. This study adds to the current body of research by\npresenting empirical evidence at the firm-level in China. It also provides\npractical insights for policymakers and business leaders who want to improve\neconomic growth and competitiveness. Future research should take into account\nwider datasets, more extensive health metrics, and delve into the mechanisms\nthat contribute to the diminishing returns observed in the relationship between\nhealth and education.",
      "generated_abstract": "r examines the effect of the introduction of a mandatory minimum\nin wages for all workers on the welfare of individuals and the overall\neconomy. Using data from 2000 to 2017, we find that minimum wages do not\nsignificantly affect wages of the lowest-paid workers. On the other hand, they\nincrease the wages of the middle-income group and decrease the wages of the\nhighest-paid workers. The introduction of the minimum wage also reduces the\nincome of the lowest-paid workers and increases the income of the middle-income\ngroup. Moreover, the minimum wage is associated with a decline in the labor\nforce participation rate of the lowest-paid workers, while it is associated with\nan increase in the labor force participation rate of the middle-income group.\nThese findings suggest that the introduction",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13846153846153847,
          "p": 0.27692307692307694,
          "f": 0.18461538017094029
        },
        "rouge-2": {
          "r": 0.02185792349726776,
          "p": 0.041666666666666664,
          "f": 0.028673830611632114
        },
        "rouge-l": {
          "r": 0.13846153846153847,
          "p": 0.27692307692307694,
          "f": 0.18461538017094029
        }
      }
    },
    {
      "paper_id": "stat.ME.q-bio/QM/2503.05448v1",
      "true_abstract": "Graphical modeling is a widely used tool for analyzing conditional\ndependencies between variables and traditional methods may struggle to capture\nshared and distinct structures in multi-group or multi-condition settings.\nJoint graphical modeling (JGM) extends this framework by simultaneously\nestimating network structures across multiple related datasets, allowing for a\ndeeper understanding of commonalities and differences. This capability is\nparticularly valuable in fields such as genomics and neuroscience, where\nidentifying variations in network topology can provide critical biological\ninsights. Existing JGM methodologies largely fall into two categories:\nregularization-based approaches, which introduce additional penalties to\nenforce structured sparsity, and Bayesian frameworks, which incorporate prior\nknowledge to improve network inference. In this study, we explore an\nalternative method based on two-target linear covariance matrix shrinkage.\nFormula for optimal shrinkage intensities is proposed which leads to the\ndevelopment of JointStein framework. Performance of JointStein framework is\nproposed through simulation benchmarking which demonstrates its effectiveness\nfor large-scale single-cell RNA sequencing (scRNA-seq) data analysis. Finally,\nwe apply our approach to glioblastoma scRNA-seq data, uncovering dynamic shifts\nin T cell network structures across disease progression stages. The result\nhighlights potential of JointStein framework in extracting biologically\nmeaningful insights from high-dimensional data.",
      "generated_abstract": "In this study, we present a novel method for non-invasively estimating\nthe total number of cell nuclei in a 2D slice of a living cell sample. Our\napproach leverages the fact that the cell nucleus is a compact region with\nwell-defined geometrical properties, such as its shape, size, and shape\norientation. We demonstrate that this approach can be used for cellular\nquantification in a wide range of cell types, including human embryonic\nstem cells (hESCs) and mouse embryonic stem cells (mESCs). To validate the\nmethod, we show that the obtained total cell number is in good agreement with\nthe experimental measurements, highlighting its potential for practical\napplications in the field of cellular analysis.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14666666666666667,
          "p": 0.2894736842105263,
          "f": 0.19469026102278966
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.04672897196261682,
          "f": 0.03424657069924064
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.2631578947368421,
          "f": 0.17699114597854188
        }
      }
    },
    {
      "paper_id": "physics.ao-ph.physics/ao-ph/2503.05288v1",
      "true_abstract": "Like Johnson noise, where thermal fluctuations of charge carriers in a\nresistor lead to measurable current fluctuations, the internal variability of\nEarth's atmosphere leads to fluctuations in the infrared radiation emitted to\nspace, creating \"Earth's infrared background\" (EIB). This background consists\nof fluctuations that are isotropic in space and red in time, with an upper\nbound of 400 km and 2.5 days on their spatiotemporal decorrelation, between\nmeso-scale and synoptic-scale weather. Like the anisotropies in the Cosmic\nMicrowave Background (CMB), which represent features of interest in the\nUniverse, the anisotropies in Earth's infrared radiation represent features of\ninterest in Earth's atmosphere. Unlike the CMB, which represents a historical\nrecord of the Universe since the Big Bang, the EIB represents Earth's climate\nin steady state.",
      "generated_abstract": "t a novel method for assessing the predictive capability of\nan ensemble of climate simulations over the next century. This method is\nbased on the ensemble Kalman filter (EnKF) and combines the ensemble mean\n(EM) with the ensemble spread (ES). We first demonstrate the utility of the\nES for assessing the predictive capability of ensemble climate simulations.\nUsing a series of simple climate models, we demonstrate that the ES can be\nhighly correlated with the ensemble mean, and thus provides a powerful tool for\nensuring the accuracy of ensemble predictions. Using a dataset of 117 climate\nmodels simulating the Earth's atmosphere and ocean, we then examine the\npredictive skill of the ensemble mean (EM) and ES. Our results show that the\nES is more predictive than the EM, with the ES outperforming the EM in 100% of\ncases. We further explore the potential of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16049382716049382,
          "p": 0.16666666666666666,
          "f": 0.16352200758039648
        },
        "rouge-2": {
          "r": 0.017699115044247787,
          "p": 0.01652892561983471,
          "f": 0.01709401209986266
        },
        "rouge-l": {
          "r": 0.16049382716049382,
          "p": 0.16666666666666666,
          "f": 0.16352200758039648
        }
      }
    },
    {
      "paper_id": "cs.CL.cs/CL/2503.10533v1",
      "true_abstract": "High-quality test items are essential for educational assessments,\nparticularly within Item Response Theory (IRT). Traditional validation methods\nrely on resource-intensive pilot testing to estimate item difficulty and\ndiscrimination. More recently, Item-Writing Flaw (IWF) rubrics emerged as a\ndomain-general approach for evaluating test items based on textual features.\nHowever, their relationship to IRT parameters remains underexplored. To address\nthis gap, we conducted a study involving over 7,000 multiple-choice questions\nacross various STEM subjects (e.g., math and biology). Using an automated\napproach, we annotated each question with a 19-criteria IWF rubric and studied\nrelationships to data-driven IRT parameters. Our analysis revealed\nstatistically significant links between the number of IWFs and IRT difficulty\nand discrimination parameters, particularly in life and physical science\ndomains. We further observed how specific IWF criteria can impact item quality\nmore and less severely (e.g., negative wording vs. implausible distractors).\nOverall, while IWFs are useful for predicting IRT parameters--particularly for\nscreening low-difficulty MCQs--they cannot replace traditional data-driven\nvalidation methods. Our findings highlight the need for further research on\ndomain-general evaluation rubrics and algorithms that understand\ndomain-specific content for robust item validation.",
      "generated_abstract": "r investigates the impact of domain knowledge on the performance of\ncomputational language models (CLMs) in natural language processing (NLP). To\naddress this issue, we propose an approach that leverages the expertise of\nexperts to enhance the performance of CLMs. The experts are trained to\nincorporate domain knowledge into the CLM, enhancing its ability to process\ndifferent types of data. We evaluate our approach on a number of CLMs,\nincluding GPT-4o, GPT-4, and GPT-3.5, on various tasks such as machine translation,\nquestion answering, and text summarization. Our results demonstrate that the\nexpert-enhanced CLMs outperform their counterparts, highlighting the importance\nof leveraging experts to enhance the performance of CLMs. The expert-enhanced\nCLMs provide a powerful tool for enhan",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1702127659574468,
          "p": 0.2926829268292683,
          "f": 0.21524663212129752
        },
        "rouge-2": {
          "r": 0.016853932584269662,
          "p": 0.02830188679245283,
          "f": 0.021126755884746124
        },
        "rouge-l": {
          "r": 0.15602836879432624,
          "p": 0.2682926829268293,
          "f": 0.19730941239035585
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10107v1",
      "true_abstract": "Perceptive mobile networks (PMNs), integrating ubiquitous sensing\ncapabilities into mobile networks, represent an important application of\nintegrated sensing and communication (ISAC) in 6G. In this paper, we propose a\npractical framework for uplink sensing of angle-of-arrival (AoA), Doppler, and\ndelay in millimeter-wave (mmWave) communication systems, which addresses\nchallenges posed by clock asynchrony and hybrid arrays, while being compatible\nwith existing communication protocols. We first introduce a beam scanning\nmethod and a corresponding AoA estimation algorithm, which utilizes frequency\nsmoothing to effectively estimate AoAs for both static and dynamic paths. We\nthen propose several methods for constructing a ``clean'' reference signal,\nwhich is subsequently used to cancel the effect caused by the clock asynchrony.\nWe further develop a signal ratio-based joint AoA-Doppler-delay estimator and\npropose an AoA-based 2D-FFT-MUSIC (AB2FM) algorithm that applies 2D-FFT\noperations on the signal subspace, which accelerates the computation process\nwith low complexity. Our proposed framework can estimate parameters in pairs,\nremoving the complicated parameter association process. Simulation results\nvalidate the effectiveness of our proposed framework and demonstrate its\nrobustness in both low and high signal-to-noise ratio (SNR) conditions.",
      "generated_abstract": "opment of low-cost, multi-functional, and flexible sensing and\ncommunication systems is crucial for advancing sensor and communication\ntechnologies. In this work, we propose a low-complexity, low-cost, and\nflexible wireless system that utilizes a small, low-cost, and flexible\nsingle-antenna transmitter and a wide-bandwidth, low-cost, and flexible\nmulti-antenna receiver to enable wireless communication and sensing for\nunmanned aerial vehicles (UAVs). Our system consists of a wireless sensor\nnetwork (WSN) with a single UAV that transmits and senses using a single\ntransmitter and receiver, respectively. The proposed system utilizes a\nhigh-frequency, wide-bandwidth, low-cost, and flexible transmitter that can\ncommunicate with a wide range of UAVs. The proposed system utilizes a\nlow-cost,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13636363636363635,
          "p": 0.3103448275862069,
          "f": 0.18947367996897516
        },
        "rouge-2": {
          "r": 0.033707865168539325,
          "p": 0.06666666666666667,
          "f": 0.044776114942081086
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.3103448275862069,
          "f": 0.18947367996897516
        }
      }
    },
    {
      "paper_id": "math.OC.econ/TH/2502.17012v2",
      "true_abstract": "We introduce a model of infinite horizon linear dynamic optimization and\nobtain results concerning existence of solution and satisfaction of the Euler\ncondition and transversality condition being unconditionally sufficient for\noptimality of a trajectory. We show that the optimal value function is concave\nand continuous and the optimal trajectory satisfies the functional equation of\ndynamic programming. Linearity bites when it comes to the definition of optimal\ndecision rules which can no longer be guaranteed to be single-valued. We show\nthat the optimal decision rule is an upper semi-continuous correspondence. For\nlinear cake-eating problems, we obtain monotonicity results for the optimal\nvalue function and a conditional monotonicity result for optimal decision\nrules. We also introduce the concept of a two-phase linear cake eating problem\nand obtain a necessary condition that must be satisfied by all solutions of\nsuch problems.",
      "generated_abstract": "the effect of privacy on the value of a private good that a\nmechanism-design (MD) agent can assign to a public good, given the mechanism's\nability to protect the good's value from the agent. We show that in a setting\nwhere the agent has complete knowledge of the mechanism, the value of the\nprivate good is equal to the value of the public good if and only if the\nmechanism can be designed to protect the value of the private good from the\nagent. We further show that in a setting where the agent is unaware of the\nmechanism's ability to protect the value of the private good, the value of the\nprivate good is always greater than or equal to the value of the public good.\nFinally, we show that in a setting where the agent is unaware of the\nmechanism's ability to protect the value of the private good, the value of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16049382716049382,
          "p": 0.2653061224489796,
          "f": 0.1999999953029587
        },
        "rouge-2": {
          "r": 0.04032258064516129,
          "p": 0.06172839506172839,
          "f": 0.04878048302486663
        },
        "rouge-l": {
          "r": 0.16049382716049382,
          "p": 0.2653061224489796,
          "f": 0.1999999953029587
        }
      }
    },
    {
      "paper_id": "math.OC.math/HO/2503.07134v1",
      "true_abstract": "This paper is dedicated to the elementary proof of Pontryagin's maximum\nprinciple for problems with free right end point. The proof for the standard\nproblem is taken from the monography of Ioffe and Tichomirov. We assume\npiecewise continuous controls and the proof turns out to be very simple. We\ngeneralize the concept to the problem of optimal multiprocesses, to control\nproblems with delays and to the control of Volterra integral equations.\nFurthermore, we discuss the problem on infinite horizon. Moreover, we state\nArrow type sufficiency conditions. The optimality conditions are demonstrated\non illustrative examples.",
      "generated_abstract": "We establish a quantitative version of the $\\epsilon$-approximation theorem\nfor the entropy of a measure-preserving system. Namely, we show that for any\n$\\epsilon>0$, there exist a constant $c=c(\\epsilon)>0$ and a sequence of\nrandom variables $(X_n)_{n\\geq 1}$ such that\n  \\begin{equation*}\n    \\mathbb{E}\\left[e^{\\epsilon\\mathcal{H}(X_1)}\\right",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09375,
          "p": 0.1875,
          "f": 0.12499999555555572
        },
        "rouge-2": {
          "r": 0.011235955056179775,
          "p": 0.02564102564102564,
          "f": 0.015624995762940602
        },
        "rouge-l": {
          "r": 0.09375,
          "p": 0.1875,
          "f": 0.12499999555555572
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/CB/2501.09546v1",
      "true_abstract": "Bacteria can form a great variety of spatially heterogeneous cell density\npatterns, ranging from simple concentric rings to dynamical spiral waves\nappearing in growing colonies. These pattern formation phenomena are important\nas they reflect how cellular processes such as metabolism operate in\nheterogeneous chemical environments. In the laboratory, they can be studied in\nsimplified set-ups, where spatial gradients of oxygen and nutrients are\nexternally imposed, and cells are immobilized in a gel matrix. An intriguing\nexample, observed in such set-ups over 80 years ago, is the sequential\nformation of narrow bands of high cell density, taking place even for a clonal\npopulation. However, key aspects of the dynamics of band formation remained\nobscure. Using time-lapse imaging of replicate transparent columns in\nsimplified growth media, we first quantify the precision of the positioning and\ntiming of band formation. We also show that the appearance and position of\ndifferent bands can be modulated independently. This \"modularity\" is suggested\nby the observation that different bands differ in their gene expression, and it\nis reproduced by a theoretical model based on the existence of internal\nmetabolic states and the induction of a pH gradient. Finally, we can also\nmodify the observed pattern formation by introducing genetic modifications that\nimpair selected metabolic pathways. In our opinion, the possibility of precise\nmeasurements and controls, together with the simplicity and richness of the\n\"proliferation pattern formation\" phenomenon, can make it a model system to\nstudy the response of cellular processes to heterogeneous environments.",
      "generated_abstract": "of neuronal intrinsic plasticity in the regulation of\nthe cardiovascular system is poorly understood, particularly in the context of\ncardiovascular diseases. The intrinsic plasticity of the cardiovascular\nsystem includes changes in the contractile properties of cardiomyocytes, as\nwell as alterations in the expression of genes related to hypertrophy,\nvasodilation and angiogenesis. These changes are likely to be controlled by\ndifferential activation of the neurohumoral and neural systems that regulate\nthem. However, despite recent advances in understanding the mechanisms of\nplasticity in the cardiovascular system, the role of neural control of cardiovascular\ndisease remains unclear. Here, we review the role of the neurohumoral and\nneural systems in the regulation of cardiovascular health",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09433962264150944,
          "p": 0.24193548387096775,
          "f": 0.13574660229806937
        },
        "rouge-2": {
          "r": 0.00423728813559322,
          "p": 0.011494252873563218,
          "f": 0.006191946528388665
        },
        "rouge-l": {
          "r": 0.0880503144654088,
          "p": 0.22580645161290322,
          "f": 0.12669682854241324
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2501.13136v1",
      "true_abstract": "Digital currencies have become popular in the last decade due to their\nnon-dependency and decentralized nature. The price of these currencies has seen\na lot of fluctuations at times, which has increased the need for prediction. As\ntheir most popular, Bitcoin(BTC) has become a research hotspot. The main\nchallenge and trend of digital currencies, especially BTC, is price\nfluctuations, which require studying the basic price prediction model. This\nresearch presents a classification and regression model based on stack deep\nlearning that uses a wavelet to remove noise to predict movements and prices of\nBTC at different time intervals. The proposed model based on the stacking\ntechnique uses models based on deep learning, especially neural networks and\ntransformers, for one, seven, thirty and ninety-day forecasting. Three feature\nselection models, Chi2, RFE and Embedded, were also applied to the data in the\npre-processing stage. The classification model achieved 63\\% accuracy for\npredicting the next day and 64\\%, 67\\% and 82\\% for predicting the seventh,\nthirty and ninety days, respectively. For daily price forecasting, the\npercentage error was reduced to 0.58, while the error ranged from 2.72\\% to\n2.85\\% for seven- to ninety-day horizons. These results show that the proposed\nmodel performed better than other models in the literature.",
      "generated_abstract": "y proposes a novel method to enhance the performance of stochastic\nmodel predictive control (MPC) algorithms for financial markets. The proposed\nalgorithm uses a two-stage approach to train a recurrent neural network (RNN)\nto predict the value of a financial asset in the future. In the first stage,\nthe RNN is trained to estimate the underlying financial variables such as\nprice, volatility, and correlation. In the second stage, the RNN is used to\nestimate the future price and volatility of the asset. This approach provides\nan accurate and scalable solution for financial market analysis, enhancing\npredictive accuracy while reducing computational costs. The proposed method\nprovides a robust framework for financial market analysis, offering significant\nefficiency and accuracy improvements over existing MPC algorithms. The\nproposed approach is validated through simulations, demonstrating its\neffectiveness in forecasting the price of various financial",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13970588235294118,
          "p": 0.2235294117647059,
          "f": 0.17194569662373838
        },
        "rouge-2": {
          "r": 0.02512562814070352,
          "p": 0.04,
          "f": 0.030864192791686448
        },
        "rouge-l": {
          "r": 0.1323529411764706,
          "p": 0.21176470588235294,
          "f": 0.16289592286808227
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.01265v1",
      "true_abstract": "Contrast-enhanced magnetic resonance imaging (CE-MRI) is crucial for tumor\ndetection and diagnosis, but the use of gadolinium-based contrast agents\n(GBCAs) in clinical settings raises safety concerns due to potential health\nrisks. To circumvent these issues while preserving diagnostic accuracy, we\npropose a novel Transformer with Localization Prompts (TLP) framework for\nsynthesizing CE-MRI from non-contrast MR images. Our architecture introduces\nthree key innovations: a hierarchical backbone that uses efficient Transformer\nto process multi-scale features; a multi-stage fusion system consisting of\nLocal and Global Fusion modules that hierarchically integrate complementary\ninformation via spatial attention operations and cross-attention mechanisms,\nrespectively; and a Fuzzy Prompt Generation (FPG) module that enhances the TLP\nmodel's generalization by emulating radiologists' manual annotation through\nstochastic feature perturbation. The framework uniquely enables interactive\nclinical integration by allowing radiologists to input diagnostic prompts\nduring inference, synergizing artificial intelligence with medical expertise.\nThis research establishes a new paradigm for contrast-free MRI synthesis while\naddressing critical clinical needs for safer diagnostic procedures. Codes are\navailable at https://github.com/ChanghuiSu/TLP.",
      "generated_abstract": "face is an essential component of human identity and has been\nused as a subject of research in numerous fields, such as computer vision,\nnatural language processing, and neuroscience. The face is a key component of\nhuman communication, and understanding its structure, dynamics, and dynamics\nof the interaction between the face and the environment is crucial for\nunderstanding the human face.\n  This paper introduces a new dataset for face-related research, including\nimages of the face and its environment, and their annotations. The dataset is\nconstructed through the use of a human face-object interaction dataset\ncollected through a real-world experiment. The dataset consists of 12,000\nimages of the face and its environment. It is annotated with both face and\nenvironment labels.\n  The dataset is used for research on face recognition, face\nidentification, and face verification. We also introduce a new benchmark for\nface-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1276595744680851,
          "p": 0.24,
          "f": 0.16666666213348777
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.04,
          "f": 0.034482753715815204
        },
        "rouge-l": {
          "r": 0.11347517730496454,
          "p": 0.21333333333333335,
          "f": 0.14814814361496928
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2411.17750v1",
      "true_abstract": "This paper statistically analysed pensioner longevity in Ghana. It\nfundamentally sought to ascertain the significant determinants of longevity\namongst Ghanaian pensioners, specifically, SSNIT pensioners by estimating the\nmortality rate of SSNIT pensioners, determining the factors that significantly\naffect the longevity of the average SSNIT pensioner and constructing a\npredictive model for predicting the longevity of SSNIT pensioners in Ghana.\nSecondary data was obtained from the leading pension provider in Ghana, SSNIT.\nThe results of the study revealed that the total number of male deaths was\nsignificantly greater, about four times more, than the total number of female\ndeaths. There was sufficient evidence that there exists a lower rate of death\namong female pensioners as compared to male pensioners. Furthermore, a\nsignificant number of the SSNIT pensioners used in the analysis survived less\nthan 8 years after retirement before their death and the average basic salary\nof the pensioners who lived less than 8 years after retirement was GH 7741.827\ncedis and served for 35.65 years on average. On the contrary, it was observed\nthat pensioners who lived more than 8 years after retirement had an average\nbasic salary of GH 5544.20 cedis and served for 31.49 years on average. In\nconclusion, predictors such as basic salary, the number of years of service in\nthe workforce, the age of the pensioner before death and the gender of the\npensioner were statistically significant in predicting the number of years a\nSSNIT pensioner survived after retirement before death that is, whether a\nparticular SSNIT pensioner survived less than 8 years or 8 years and above\nafter retirement. The authors recommend to employees that, their health should\nbe of great priority as every increase in the service year increases the\nlikelihood of a SSNIT pensioner surviving less than 8 years after retirement by\n22.36%.",
      "generated_abstract": "nt COVID-19 pandemic is a global health crisis, with a significant\neffect on the environment. The ecological impact of the pandemic on\necosystems, particularly on the biosphere and the microbial world, has been\nstudied, but the results have been inconsistent. This is due to the limited\ndata and the lack of a standardized methodology. The objective of this study is\nto analyze the impact of the pandemic on the ecosystems, with a focus on the\nmicrobial world. The data were collected from the literature and databases,\nincluding ECOLIE, Web of Science, Scopus, CAB Abstracts, Google Scholar,\nJSTOR, ProQuest, and ProQuest. The results show that the pandemic has had a\nsignificant impact on the microbial world. The pandemic has had a negative impact\non the bios",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12142857142857143,
          "p": 0.2537313432835821,
          "f": 0.16425120335130353
        },
        "rouge-2": {
          "r": 0.029288702928870293,
          "p": 0.06796116504854369,
          "f": 0.040935668305290954
        },
        "rouge-l": {
          "r": 0.12142857142857143,
          "p": 0.2537313432835821,
          "f": 0.16425120335130353
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.01448v1",
      "true_abstract": "This paper introduces Agency-Driven Labor Theory as a new theoretical\nframework for understanding human work in AI-augmented environments. While\ntraditional labor theories have focused primarily on task execution and labor\ntime, ADLT proposes that human labor value is increasingly derived from agency\n- the capacity to make informed judgments, provide strategic direction, and\ndesign operational frameworks for AI systems. The paper presents a mathematical\nframework expressing labor value as a function of agency quality, direction\neffectiveness, and outcomes, providing a quantifiable approach to analyzing\nhuman value creation in AI-augmented workplaces. Drawing on recent work in\norganizational economics and knowledge worker productivity, ADLT explains how\nhuman workers create value by orchestrating complex systems that combine human\nand artificial intelligence. The theory has significant implications for job\ndesign, compensation structures, professional development, and labor market\ndynamics. Through applications across various sectors, the paper demonstrates\nhow ADLT can guide organizations in managing the transition to AI-augmented\noperations while maximizing human value creation. The framework provides\npractical tools for policymakers and educational institutions as they prepare\nworkers for a labor market where value creation increasingly centers on agency\nand direction rather than execution.",
      "generated_abstract": "nt world economy is characterized by persistent structural\nchanges and economic volatility. These changes are driven by technological\ndisruptions, international trade dynamics, and political and social\ninstitutional shifts. This paper examines the impact of these changes on the\nfuture economic growth of China and the United States. Using a stochastic\ndynamic general equilibrium model, we investigate how the effects of technological\ndisruptions, international trade dynamics, and political and social\ninstitutional shifts influence China's and the U.S.'s economic growth rates,\nfiscal balances, and trade balances. We find that China's economic growth rate\nis likely to decline in the near future, while the U.S.'s is likely to rise.\nHowever, the U.S. is likely to experience a larger fiscal balance and trade\nbalance deficit than China over the next few decades. These",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12,
          "p": 0.189873417721519,
          "f": 0.14705881878364105
        },
        "rouge-2": {
          "r": 0.00558659217877095,
          "p": 0.00909090909090909,
          "f": 0.006920410509934846
        },
        "rouge-l": {
          "r": 0.12,
          "p": 0.189873417721519,
          "f": 0.14705881878364105
        }
      }
    },
    {
      "paper_id": "astro-ph.IM.hep-ex/2503.10521v1",
      "true_abstract": "The Dark Matter Particle Explorer (DAMPE) is a space-based Cosmic-Ray (CR)\nobservatory with the aim, among others, to study Cosmic-Ray Electrons (CREs) up\nto 10 TeV. Due to the low CRE rate at multi-TeV energies, we aim to increasing\nthe acceptance by selecting events outside the fiducial volume. The complex\ntopology of non-fiducial events requires the development of a novel energy\nreconstruction method. We propose the usage of Convolutional Neural Networks\nfor a regression task to recover an accurate estimation of the initial energy.",
      "generated_abstract": "24 LSST season is the most significant in the history of the\nlarge-scale survey, with the opportunity to significantly enhance the science\npotential of the data by improving the photometric and astrometric accuracy of\nthe LSST primary and secondary mirrors. The primary mirror, which is\ncomposed of 40000 mirror segments, has been optimized for the next generation\nof high-precision instrumentation. This optimization resulted in a 10% increase\nin mirror area, a 3% increase in mirror surface, and a 6% increase in the\nrefractive index. This paper describes the current status of the optical\ndesign of the LSST primary mirror, and its optical design optimizations.\nSpecifically, we present the current state of the mirror segments, the\noptical design optimization plan, and the progress made during the 2023",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.136986301369863,
          "f": 0.14492753124868743
        },
        "rouge-2": {
          "r": 0.024096385542168676,
          "p": 0.018518518518518517,
          "f": 0.02094240346262553
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.136986301369863,
          "f": 0.14492753124868743
        }
      }
    },
    {
      "paper_id": "physics.data-an.physics/data-an/2503.09771v1",
      "true_abstract": "We propose a method of estimating the uncertainty of a result obtained\nthrough extrapolation to the complete basis set limit. The method is based on\nan ensemble of random walks which simulate all possible extrapolation outcomes\nthat could have been obtained if results from larger basis sets had been\navailable. The results assembled from a large collection of random walks can be\nthen analyzed statistically, providing a route for uncertainty prediction at a\nconfidence level required in a particular application. The method is free of\nempirical parameters and compatible with any extrapolation scheme. The proposed\ntechnique is tested in a series of numerical trials by comparing the determined\nconfidence intervals with reliable reference data. We demonstrate that the\npredicted error bounds are reliable, tight, yet conservative at the same time.",
      "generated_abstract": "experiment has reported a $B \\to K^* \\ell^+ \\ell^-$ mode with a\n$B^0 - \\bar{B}^0$ mass difference of 2.79 $\\pm$ 0.23 MeV, which is smaller than\nthe previous best measurement by 2.4 $\\pm$ 0.6 MeV. We present an updated\nanalysis of this data, using a new analysis technique to improve the\nstatistical precision. This technique allows us to measure the branching\nfraction of $B \\to K^* \\ell^+ \\ell^-$ in the region of the previous\nexperimental window, where the previously reported value was the best estimate.\nWe find that the new measurement is consistent with the previous best estimate,\nbut with a larger statistical uncertainty. We also present a model-independent\nestimate of the branching fraction for $B^0 - \\bar{B",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.21333333333333335,
          "f": 0.19161676151887852
        },
        "rouge-2": {
          "r": 0.008064516129032258,
          "p": 0.009259259259259259,
          "f": 0.008620684678956499
        },
        "rouge-l": {
          "r": 0.16304347826086957,
          "p": 0.2,
          "f": 0.17964071361468692
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.09434v1",
      "true_abstract": "We derive nonlinear stability results for numerical integrators on Riemannian\nmanifolds, by imposing conditions on the ODE vector field and the step size\nthat makes the numerical solution non-expansive whenever the exact solution is\nnon-expansive over the same time step. Our model case is a geodesic version of\nthe explicit Euler method. Precise bounds are obtained in the case of\nRiemannian manifolds of constant sectional curvature. The approach is based on\na cocoercivity property of the vector field adapted to manifolds from Euclidean\nspace. It allows us to compare the new results to the corresponding well-known\nresults in flat spaces, and in general we find that a non-zero curvature will\ndeteriorate the stability region of the geodesic Euler method. The step size\nbounds depend on the distance traveled over a step from the initial point.\nNumerical examples for spheres and hyperbolic 2-space confirm that the bounds\nare tight.",
      "generated_abstract": "We give a characterization of the limiting behavior of the numerical\nsolution of the problem of the numerical solution of the linear Stokes system\nwith non-homogeneous divergence and without artificial viscosity. In addition,\nwe characterize the convergence rate of the numerical method. Finally, we\ndiscuss the application of our results to the numerical simulation of\nnon-stationary turbulent flow past a flat plate and to the numerical\nsimulation of the incompressible Navier-Stokes equations in a cylindrical\nshell.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14772727272727273,
          "p": 0.2765957446808511,
          "f": 0.19259258805377238
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.08333333333333333,
          "f": 0.049999995800000356
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.2553191489361702,
          "f": 0.17777777323895758
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/OT/2407.18572v1",
      "true_abstract": "An approach to amputation, the process of introducing missing values to a\ncomplete dataset, is presented. It allows to construct missingness indicators\nin a flexible and principled way via copulas and Bernoulli margins and to\nincorporate dependence in missingness patterns. Besides more classical\nmissingness models such as missing completely at random, missing at random, and\nmissing not at random, the approach is able to model structured missingness\nsuch as block missingness and, via mixtures, monotone missingness, which are\npatterns of missing data frequently found in real-life datasets. Properties\nsuch as joint missingness probabilities or missingness correlation are derived\nmathematically. The approach is demonstrated with mathematical examples and\nempirical illustrations in terms of a well-known dataset.",
      "generated_abstract": "y investigates the effectiveness of a 10-minute telemedicine visit\nfor patients with mild to moderate COVID-19 symptoms. The telemedicine visit\ninvolves screening questions, symptom checklist, and a physical examination\n(PE) by a doctor. The telemedicine visit is recorded, and the patient is\ncontacted by a doctor to complete the PE and answer additional questions. The\npurpose of this study is to assess the effectiveness of telemedicine visits in\nminimizing the need for hospitalization and reducing the cost of medical care\nfor patients with mild to moderate COVID-19 symptoms. The study included\nparticipants with mild to moderate COVID-19 symptoms, who had been hospitalized\nfor COVID-19 symptoms, and who were considered at high risk for hospitalization.\nThe study included",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14864864864864866,
          "p": 0.1746031746031746,
          "f": 0.16058393663807358
        },
        "rouge-2": {
          "r": 0.009174311926605505,
          "p": 0.010752688172043012,
          "f": 0.009900985130381868
        },
        "rouge-l": {
          "r": 0.14864864864864866,
          "p": 0.1746031746031746,
          "f": 0.16058393663807358
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.13140v2",
      "true_abstract": "Tensor decomposition has emerged as a powerful framework for feature\nextraction in multi-modal biomedical data. In this review, we present a\ncomprehensive analysis of tensor decomposition methods such as Tucker,\nCANDECOMP/PARAFAC, spiked tensor decomposition, etc. and their diverse\napplications across biomedical domains such as imaging, multi-omics, and\nspatial transcriptomics. To systematically investigate the literature, we\napplied a topic modeling-based approach that identifies and groups distinct\nthematic sub-areas in biomedicine where tensor decomposition has been used,\nthereby revealing key trends and research directions. We evaluated challenges\nrelated to the scalability of latent spaces along with obtaining the optimal\nrank of the tensor, which often hinder the extraction of meaningful features\nfrom increasingly large and complex datasets. Additionally, we discuss recent\nadvances in quantum algorithms for tensor decomposition, exploring how quantum\ncomputing can be leveraged to address these challenges. Our study includes a\npreliminary resource estimation analysis for quantum computing platforms and\nexamines the feasibility of implementing quantum-enhanced tensor decomposition\nmethods on near-term quantum devices. Collectively, this review not only\nsynthesizes current applications and challenges of tensor decomposition in\nbiomedical analyses but also outlines promising quantum computing strategies to\nenhance its impact on deriving actionable insights from complex biomedical\ndata.",
      "generated_abstract": "uce the framework of a dynamical system that describes the\ntransitions between the states of a protein-based cell-cell communication\nsystem. The system is comprised of a protein-based cell, which can be thought\nof as a network of interacting protein molecules, and a set of signal molecules,\nwhich can be thought of as agents that can be transmitted through the network.\nThe system is characterized by a set of parameters that control the interactions\nbetween the protein and the signal molecules. We show that the system can be\ndescribed as a non-linear dynamical system, and that the system exhibits\ndifferent states, such as a quiescent state, a bursty state, and a\nself-sustained state. We show that these states are related to the properties of\nthe protein-based cell. We show that the protein-based cell is able to\ncontrol the transition between the states by controlling",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11029411764705882,
          "p": 0.23809523809523808,
          "f": 0.15075376451705777
        },
        "rouge-2": {
          "r": 0.03225806451612903,
          "p": 0.05357142857142857,
          "f": 0.040268451684158914
        },
        "rouge-l": {
          "r": 0.11029411764705882,
          "p": 0.23809523809523808,
          "f": 0.15075376451705777
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.14069v2",
      "true_abstract": "We study the problem of estimating the barycenter of a distribution given\ni.i.d. data in a geodesic space. Assuming an upper curvature bound in\nAlexandrov's sense and a support condition ensuring the strong geodesic\nconvexity of the barycenter problem, we establish finite-sample error bounds in\nexpectation and with high probability. Our results generalize Hoeffding- and\nBernstein-type concentration inequalities from Euclidean to geodesic spaces.\nBuilding on these concentration inequalities, we derive statistical guarantees\nfor two efficient algorithms for the computation of barycenters.",
      "generated_abstract": "er the problem of estimating a linear model with unknown\nlinear predictor $X \\beta = y$ from data $Y, X, \\bm{1}$, where $Y$ and $X$ are\nbinary or continuous, and $\\bm{1} \\in \\{0, 1\\}^p$ is binary. We assume that\nthe linear predictor $X \\beta$ has rank $r \\leq p$ and that the noise is\ngaussian with covariance $\\sigma^2 I_p$. We first show that under mild\nassumptions, the rank-$r$ approximation $X^{\\top} \\widehat{\\beta}$ is\nconsistent. We then derive the minimax rate of convergence of $X^{\\top}\n\\widehat{\\beta}$ under additional assumptions on the data distribution and the\nerror covariance. The minimax rate depends on the rank $r$ and the\ndimension $p$, and the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.19444444444444445,
          "f": 0.20740740242962977
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.03,
          "f": 0.03314916632581497
        },
        "rouge-l": {
          "r": 0.20634920634920634,
          "p": 0.18055555555555555,
          "f": 0.19259258761481496
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.05087v2",
      "true_abstract": "This paper studies the formation of the grand coalition of a cooperative game\nby investigating its possible internal dynamics. Each coalition is capable of\nforcing all players to reconsider the current state of the game when it does\nnot provide sufficient payoff. Different coalitions may ask for contradictory\nevolutions, leading to the impossibility of the grand coalition forming. In\nthis paper, we give a characterization of the impossibility, for a given state,\nof finding a new state dominating the previous one such that each aggrieved\ncoalition has a satisfactory payoff. To do so, we develop new polyhedral tools\nrelated to a new family of polyhedra, appearing in numerous situations in\ncooperative game theory.",
      "generated_abstract": "We consider a population of heterogeneous individuals with bounded rational\nperception, who are not fully informed about the future. We investigate how\nindividuals can learn about the future by cooperating or competing with each\nother. We find that individuals with limited information about the future can\nlearn about the future by cooperating, while individuals with no information\nabout the future can learn about the future by competing. We further show that\nthe cooperative equilibrium depends on the cooperation costs, while the\ncompetitive equilibrium depends on the cost of information.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10256410256410256,
          "p": 0.17391304347826086,
          "f": 0.1290322533975028
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08974358974358974,
          "p": 0.15217391304347827,
          "f": 0.11290322113943828
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/RM/2503.06806v1",
      "true_abstract": "An important step in the Financial Benchmarks Reform was taken on 13th\nSeptember 2018, when the ECB Working Group on Euro Risk-Free Rates recommended\nthe Euro Short-Term Rate ESTR as the new benchmark rate for the euro area, to\nreplace the Euro OverNight Index Average (EONIA) which will be discontinued at\nthe end of 2021. This transition has a number of important consequences on\nfinancial instruments, OTC derivatives in particular.\n  In this paper we show in detail how the switch from EONIA to ESTR affects the\npricing of OIS, IRS and XVAs. We conclude that the adoption of the \"clean\ndiscounting\" approach recommended by the the ECB, based on ESTR only, is\ntheoretically sound and leads to very limited impacts on financial valuations.\n  This finding ensures the possibility, for the financial industry, to switch\nall EUR OTC derivatives, either cleared with Central Counterparties, or subject\nto bilateral collateral agreements, or non-collateralised, in a safe and\nconsistent manner. The transition to such EONIA-free pricing framework is\nessential for the complete elimination of EONIA before its discontinuation\nscheduled on 31st December 2021.",
      "generated_abstract": "r presents a novel approach for the pricing of American options\nin a stochastic volatility framework with a mean-reverting term structure of\nvolatility. We focus on the pricing of American options in a stochastic\nvolatility model with a mean-reverting term structure of volatility and we\nintroduce a new class of mean-reverting stochastic volatility models. Our\npricing model is based on a Black-Scholes model with a mean-reverting\nvolatility term structure, which is incorporated by using the Black-Scholes\nprice formula with a volatility term structure based on the mean-reverting\nvolatility. We derive the Black-Scholes price formula and the mean-reverting\nvolatility formula with an arbitrary number of terms and we propose a new\nclass of mean-reverting stochastic volatility models. We",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15079365079365079,
          "p": 0.41304347826086957,
          "f": 0.22093022863980535
        },
        "rouge-2": {
          "r": 0.03428571428571429,
          "p": 0.07792207792207792,
          "f": 0.04761904337522084
        },
        "rouge-l": {
          "r": 0.11904761904761904,
          "p": 0.32608695652173914,
          "f": 0.1744186007328286
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.08920v1",
      "true_abstract": "A distributed integrated sensing and communication (D-ISAC) system offers\nsignificant cooperative gains for both sensing and communication performance.\nThese gains, however, can only be fully realized when the distributed nodes are\nperfectly synchronized, which is a challenge that remains largely unaddressed\nin current ISAC research. In this paper, we propose an over-the-air\ntime-frequency synchronization framework for the D-ISAC system, leveraging the\nreciprocity of bistatic sensing channels. This approach overcomes the\nimpractical dependency of traditional methods on a direct line-of-sight (LoS)\nlink, enabling the estimation of time offset (TO) and carrier frequency offset\n(CFO) between two ISAC nodes even in non-LoS (NLOS) scenarios. To achieve this,\nwe introduce a bistatic signal matching (BSM) technique with delay-Doppler\ndecoupling, which exploits offset reciprocity (OR) in bistatic observations.\nThis method compresses multiple sensing links into a single offset for\nestimation. We further present off-grid super-resolution estimators for TO and\nCFO, including the maximum likelihood estimator (MLE) and the matrix pencil\n(MP) method, combined with BSM processing. These estimators provide accurate\noffset estimation compared to spectral cross-correlation techniques. Also, we\nextend the pairwise synchronization leveraging OR between two nodes to the\nsynchronization of $N$ multiple distributed nodes, referred to as centralized\npairwise synchronization. We analyze the Cramer-Rao bounds (CRBs) for TO and\nCFO estimates and evaluate the impact of D-ISAC synchronization on the\nbottom-line target localization performance. Simulation results validate the\neffectiveness of the proposed algorithm, confirm the theoretical analysis, and\ndemonstrate that the proposed synchronization approach can recover up to 96% of\nthe bottom-line target localization performance of the fully-synchronous\nD-ISAC.",
      "generated_abstract": "t a novel data-driven approach for the design of low-complexity\nnonlinear compressive sensing (CS) detectors. Our approach, based on the\nanalysis of a large-scale, high-dimensional sparse signal model, relies on the\nconstruction of a data-driven matrix-free CS detector that, with an\noptimal choice of the number of columns, captures the structure of the sparse\nsignal in a statistically accurate manner. We also present a data-driven\napproach for the design of a sparse signal detector, where the goal is to\nreconstruct the sparse signal by recovering the columns of a low-rank matrix.\nWe show that our data-driven detectors are able to recover the sparse signal\nwithout any prior knowledge of the matrix-free structure of the signal.\nAdditionally, we provide theoretical guarantees on the performance of our\ndetectors and discuss their potential for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15151515151515152,
          "p": 0.3246753246753247,
          "f": 0.20661156590909102
        },
        "rouge-2": {
          "r": 0.01646090534979424,
          "p": 0.03669724770642202,
          "f": 0.022727268451866768
        },
        "rouge-l": {
          "r": 0.1393939393939394,
          "p": 0.2987012987012987,
          "f": 0.1900826402892563
        }
      }
    },
    {
      "paper_id": "quant-ph.physics/hist-ph/2503.07666v1",
      "true_abstract": "The correspondence principle states that classical mechanics emerges from\nquantum mechanics in the appropriate limits. However, beyond this heuristic\nrule, an information-theoretic perspective reveals that classical mechanics is\na compressed, lower-information representation of quantum reality. Quantum\nmechanics encodes significantly more information through superposition,\nentanglement, and phase coherence, which are lost due to decoherence, phase\naveraging, and measurement, reducing the system to a classical probability\ndistribution. This transition is quantified using Kolmogorov complexity, where\nclassical systems require \\( O(N) \\) bits of information, while quantum\ndescriptions require \\( O(2^N) \\), showing an exponential reduction in\ncomplexity. Further justification comes from Ehrenfest's theorem, which ensures\nthat quantum expectation values obey Newton's laws, and path integral\nsuppression, which eliminates non-classical trajectories when \\( S \\gg \\hbar\n\\). Thus, rather than viewing quantum mechanics as an extension of classical\nmechanics, we argue that classical mechanics is a lossy, computationally\nreduced encoding of quantum physics, emerging from a systematic loss of quantum\ncorrelations.",
      "generated_abstract": "t a novel approach to the measurement of the dynamics of\nquantum systems, which is based on the observation of their evolution in\ntime-dependent, quantum-mechanical potentials. In our approach, the time-evolved\nwave-function is measured by a suitable time-dependent measurement scheme,\nwhich allows to access the dynamics of the system at a finite number of\ninstants. We demonstrate the feasibility of our approach using the harmonic\noscillator as a quantum system. We show that the system can be driven out of\nthe classical regime for times much shorter than the relaxation time of the\nsystem, while still being observable at all times. We also discuss how our\napproach can be extended to non-Hermitian Hamiltonians, which is of particular\ninterest in quantum information processing. The approach we present can be\nextended to a broad class of quantum systems, including both non-Hermitian",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15789473684210525,
          "p": 0.21428571428571427,
          "f": 0.18181817693296617
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.024193548387096774,
          "f": 0.022140216438230435
        },
        "rouge-l": {
          "r": 0.12280701754385964,
          "p": 0.16666666666666666,
          "f": 0.1414141365289258
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/NA/2503.09892v1",
      "true_abstract": "As inverter-based resources (IBRs) penetrate power systems, the dynamics\nbecome more complex, exhibiting multiple timescales, including electromagnetic\ntransient (EMT) dynamics of power electronic controllers and electromechanical\ndynamics of synchronous generators. Consequently, the power system model\nbecomes highly stiff, posing a challenge for efficient simulation using\nexisting methods that focus on dynamics within a single timescale. This paper\nproposes a Heterogeneous Multiscale Method for highly efficient multi-timescale\nsimulation of a power system represented by its EMT model. The new method\nalternates between the microscopic EMT model of the system and an automatically\nreduced macroscopic model, varying the step size accordingly to achieve\nsignificant acceleration while maintaining accuracy in both fast and slow\ndynamics of interests. It also incorporates a semi-analytical solution method\nto enable a more adaptive variable-step mechanism. The new simulation method is\nillustrated using a two-area system and is then tested on a detailed EMT model\nof the IEEE 39-bus system.",
      "generated_abstract": "r presents a novel approach for control of a four-wheeled\nunderwater robot, using a nonlinear extended Kalman filter (NKEF) to track a\ntrajectory, and a nonlinear observer to estimate the state. The proposed\nframework utilizes a nonlinear observer to estimate the state of the robot,\nwhich is then used as the initial condition in a NKEF to track the desired\ntrajectory. The system is demonstrated using an underwater quadruped robot,\nwhich was previously tested in the lab, to demonstrate the feasibility of the\nproposed control methodology. The results show that the proposed methodology\nyields a good tracking performance, with minimal drift and high-fidelity\ntrajectory tracking, while maintaining a low computational complexity. The\nperformance of the proposed methodology is compared against a conventional\nquadruped control methodology, which uses a linear observer to track the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.22666666666666666,
          "f": 0.19209039059657196
        },
        "rouge-2": {
          "r": 0.035211267605633804,
          "p": 0.043478260869565216,
          "f": 0.03891050089176281
        },
        "rouge-l": {
          "r": 0.1568627450980392,
          "p": 0.21333333333333335,
          "f": 0.18079095556832342
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/PR/2501.05232v1",
      "true_abstract": "Tether Limited has the sole authority to create (mint) and destroy (burn)\nTether stablecoins (USDT). This paper investigates Bitcoin's response to USDT\nsupply change events between 2014 and 2021 and identifies an interesting\nasymmetry between Bitcoin's responses to USDT minting and burning events.\nBitcoin responds positively to USDT minting events over 5- to 30-minute event\nwindows, but this response begins declining after 60 minutes. State-dependence\nis also demonstrated, with Bitcoin prices exhibiting a greater increase when\nthe corresponding USDT minting event coincides with positive investor sentiment\nand is announced to the public by data service provider, Whale Alert, on\nTwitter.",
      "generated_abstract": "p a model to predict future values of the volatility of a financial\nasset based on the past value. The model consists of an autoregressive\nforecasting model and a volatility smoothing model. The autoregressive\nforecasting model is a high-dimensional regression model of the past values of\nthe asset. The volatility smoothing model is a low-dimensional regression\nmodel of the past values of the volatility. The model can be trained using\nhistorical data for the past value of the asset and the past volatility of the\nasset. The model can be used to predict future values of the volatility of the\nasset based on the past values of the asset and the past volatility of the\nasset. The model can be used to predict future values of the asset based on the\npast value of the asset and the past volatility of the asset. The model can",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10810810810810811,
          "p": 0.22857142857142856,
          "f": 0.14678898646578586
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0945945945945946,
          "p": 0.2,
          "f": 0.12844036261257485
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/GN/2411.13762v1",
      "true_abstract": "This paper delves into the spectrum of credit risks associated with\ndecentralized stablecoin issuance, ranging from overcollateralized lending to\nbusiness-to-business credit. It examines the mechanisms, risks, and mitigation\nstrategies at each layer, highlighting the potential for scaling decentralized\nstablecoins while ensuring systemic health.",
      "generated_abstract": "r studies the optimal risk management of portfolios of stocks\nand bonds under the framework of dynamic pricing, where the stocks and bonds\nare represented by discrete time stochastic processes. The stocks and bonds are\nmarkovian, with the stocks being dependent on the past price of the bonds. The\nstocks are assumed to have the same risk factor, which is also the bond\nrisk. The bond risk factor is subject to a continuous time stochastic process\nthat is driven by a stochastic volatility. The portfolio is also subject to a\ndiscrete time stochastic volatility, which is assumed to be independent of the\nbond risk factor. The objective is to maximize the expected return of the\nportfolio. We propose a dynamic pricing model, and derive an optimal risk\nmanagement strategy. The results show that the optimal risk management strategy\ndepends on the pricing model and the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1282051282051282,
          "p": 0.07462686567164178,
          "f": 0.09433961799038827
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10256410256410256,
          "p": 0.05970149253731343,
          "f": 0.07547169346208643
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.13395v1",
      "true_abstract": "Distributed acoustic sensor (DAS) technology leverages optical fiber cables\nto detect acoustic signals, providing cost-effective and dense monitoring\ncapabilities. It offers several advantages including resistance to extreme\nconditions, immunity to electromagnetic interference, and accurate detection.\nHowever, DAS typically exhibits a lower signal-to-noise ratio (S/N) compared to\ngeophones and is susceptible to various noise types, such as random noise,\nerratic noise, level noise, and long-period noise. This reduced S/N can\nnegatively impact data analyses containing inversion and interpretation. While\nartificial intelligence has demonstrated excellent denoising capabilities, most\nexisting methods rely on supervised learning with labeled data, which imposes\nstringent requirements on the quality of the labels. To address this issue, we\ndevelop a label-free unsupervised learning (UL) network model based on\nContext-Pyramid-UNet (CP-UNet) to suppress erratic and random noises in DAS\ndata. The CP-UNet utilizes the Context Pyramid Module in the encoding and\ndecoding process to extract features and reconstruct the DAS data. To enhance\nthe connectivity between shallow and deep features, we add a Connected Module\n(CM) to both encoding and decoding section. Layer Normalization (LN) is\nutilized to replace the commonly employed Batch Normalization (BN),\naccelerating the convergence of the model and preventing gradient explosion\nduring training. Huber-loss is adopted as our loss function whose parameters\nare experimentally determined. We apply the network to both the 2-D synthetic\nand filed data. Comparing to traditional denoising methods and the latest UL\nframework, our proposed method demonstrates superior noise reduction\nperformance.",
      "generated_abstract": "years, Deep Neural Networks (DNNs) have achieved remarkable\nsuccess in speech recognition. However, due to the high cost of data acquisition,\nlarge-scale speech recognition is still a challenging task. The existing\nspeech recognition systems are mainly based on pre-trained large models, which\nlead to a significant loss of representation ability of the pre-trained\nmodels. In this paper, we propose a novel architecture named Diffusion Turing\nMachine (DTM), which is designed based on the diffusion model. DTM is a\nspeech recognition system that utilizes the diffusion model to generate\nrepresentations of speech, which are then employed to improve the performance\nof speech recognition. This paper analyzes the differences between the diffusion\nmodel and conventional speech recognition systems. In the diffusion model, the\ndiffusion process is used to generate a new representation of the input\nspeech, and the generated representation is then",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14450867052023122,
          "p": 0.30120481927710846,
          "f": 0.19531249561798106
        },
        "rouge-2": {
          "r": 0.02564102564102564,
          "p": 0.04838709677419355,
          "f": 0.033519548544677744
        },
        "rouge-l": {
          "r": 0.13872832369942195,
          "p": 0.2891566265060241,
          "f": 0.18749999561798106
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/BM/2502.12638v2",
      "true_abstract": "3D molecule generation is crucial for drug discovery and material design.\nWhile prior efforts focus on 3D diffusion models for their benefits in modeling\ncontinuous 3D conformers, they overlook the advantages of 1D SELFIES-based\nLanguage Models (LMs), which can generate 100% valid molecules and leverage the\nbillion-scale 1D molecule datasets. To combine these advantages for 3D molecule\ngeneration, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D\nLanguage Modeling for 3D Molecule Generation. NExT-Mol uses an extensively\npretrained molecule LM for 1D molecule generation, and subsequently predicts\nthe generated molecule's 3D conformers with a 3D diffusion model. We enhance\nNExT-Mol's performance by scaling up the LM's model size, refining the\ndiffusion neural architecture, and applying 1D to 3D transfer learning.\nNotably, our 1D molecule LM significantly outperforms baselines in\ndistributional similarity while ensuring validity, and our 3D diffusion model\nachieves leading performances in conformer prediction. Given these improvements\nin 1D and 3D modeling, NExT-Mol achieves a 26% relative improvement in 3D FCD\nfor de novo 3D generation on GEOM-DRUGS, and a 13% average relative gain for\nconditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are\navailable at https://github.com/acharkq/NExT-Mol.",
      "generated_abstract": "of brain networks is of fundamental importance for understanding\nphysiological and pathological processes in the brain. In this work, we develop\na deep learning framework for brain network inference and analysis based on the\ngenerative adversarial network (GAN). We introduce a novel approach that\ncombines generative modeling with attention mechanisms for the learning of\nconnectivity patterns in brain networks. Specifically, we propose to use a\nGAN architecture to learn generative models of brain networks. These models\nare trained using a dataset of brain functional connectivity measurements,\nallowing for the generation of novel brain networks. We demonstrate that the\nproposed approach achieves state-of-the-art performance in brain network\ninference, with the ability to predict brain connectivity patterns with\nhigh-accuracy and with minimal training data. Additionally, we show that the\nproposed method can be applied to a wide range of brain functional connectivity\nmeasurements",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.168,
          "p": 0.25925925925925924,
          "f": 0.2038834903737394
        },
        "rouge-2": {
          "r": 0.0055248618784530384,
          "p": 0.007936507936507936,
          "f": 0.006514653140938778
        },
        "rouge-l": {
          "r": 0.16,
          "p": 0.24691358024691357,
          "f": 0.1941747525096617
        }
      }
    },
    {
      "paper_id": "physics.ao-ph.stat/AP/2503.03990v1",
      "true_abstract": "Accurately quantifying air-sea fluxes is important for understanding air-sea\ninteractions and improving coupled weather and climate systems. This study\nintroduces a probabilistic framework to represent the highly variable nature of\nair-sea fluxes, which is missing in deterministic bulk algorithms. Assuming\nGaussian distributions conditioned on the input variables, we use artificial\nneural networks and eddy-covariance measurement data to estimate the mean and\nvariance by minimizing negative log-likelihood loss. The trained neural\nnetworks provide alternative mean flux estimates to existing bulk algorithms,\nand quantify the uncertainty around the mean estimates. Stochastic\nparameterization of air-sea turbulent fluxes can be constructed by sampling\nfrom the predicted distributions. Tests in a single-column forced upper-ocean\nmodel suggest that changes in flux algorithms influence sea surface temperature\nand mixed layer depth seasonally. The ensemble spread in stochastic runs is\nmost pronounced during spring restratification.",
      "generated_abstract": "llenge in weather forecasting is the prediction of surface temperature\n(T), which is key to forecasting the impacts of climate change on weather and\nclimate extremes. However, accurate temperature prediction has been a long-term\nchallenge, with the traditional methods (e.g., Gaussian process and neural\nnetwork) often under-performing in extreme temperature prediction. In this\nstudy, we propose a novel temperature prediction method, i.e., the Multi-Layer\nPerceptron (MLP) with an Adaptive Neural Network (ANN) based on MLP. We use a\ndeep learning model to capture the nonlinear relationship between T and\nthermodynamic parameters (e.g., temperature, humidity, and wind speed). The\ndeep learning model is based on a neural network, but it is able to\nnon-linearly represent the temperature-thermodynamic parameter relationship.\nUsing this model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1941747572815534,
          "p": 0.23529411764705882,
          "f": 0.21276595249264385
        },
        "rouge-2": {
          "r": 0.03759398496240601,
          "p": 0.04201680672268908,
          "f": 0.03968253469797241
        },
        "rouge-l": {
          "r": 0.1941747572815534,
          "p": 0.23529411764705882,
          "f": 0.21276595249264385
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/CR/2503.09727v1",
      "true_abstract": "Knowledge graph reasoning (KGR), which answers complex, logical queries over\nlarge knowledge graphs (KGs), represents an important artificial intelligence\ntask with a range of applications. Many KGs require extensive domain expertise\nand engineering effort to build and are hence considered proprietary within\norganizations and enterprises. Yet, spurred by their commercial and research\npotential, there is a growing trend to make KGR systems, (partially) built upon\nprivate KGs, publicly available through reasoning APIs.\n  The inherent tension between maintaining the confidentiality of KGs while\nensuring the accessibility to KGR systems motivates our study of KG extraction\nattacks: the adversary aims to \"steal\" the private segments of the backend KG,\nleveraging solely black-box access to the KGR API. Specifically, we present\nKGX, an attack that extracts confidential sub-KGs with high fidelity under\nlimited query budgets. At a high level, KGX progressively and adaptively\nqueries the KGR API and integrates the query responses to reconstruct the\nprivate sub-KG. This extraction remains viable even if any query responses\nrelated to the private sub-KG are filtered. We validate the efficacy of KGX\nagainst both experimental and real-world KGR APIs. Interestingly, we find that\ntypical countermeasures (e.g., injecting noise into query responses) are often\nineffective against KGX. Our findings suggest the need for a more principled\napproach to developing and deploying KGR systems, as well as devising new\ndefenses against KG extraction attacks.",
      "generated_abstract": "-19 pandemic and the resulting economic downturn have significantly\nimpacted the global supply chain. This paper introduces a novel framework\nintegrating the supply chain management (SCM) and the artificial intelligence\n(AI) to enhance supply chain resilience and competitiveness in the context of\nthe COVID-19 pandemic. Our research focuses on the supply chain resilience of\nthe logistics industry in the context of the COVID-19 pandemic. We propose a\ndynamic supply chain resilience model that integrates supply chain\nmanagement, artificial intelligence, and resilience analysis. The model\nincorporates resilience analysis, machine learning, and a resilience\noptimization framework to assess supply chain resilience. Our research\ndemonstrates that the COVID-19 pandemic significantly impacted the global\nsupply chain, resulting in significant economic losses and supply chain\ndis",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08641975308641975,
          "p": 0.23333333333333334,
          "f": 0.12612612218164124
        },
        "rouge-2": {
          "r": 0.009174311926605505,
          "p": 0.02127659574468085,
          "f": 0.012820508610290667
        },
        "rouge-l": {
          "r": 0.08641975308641975,
          "p": 0.23333333333333334,
          "f": 0.12612612218164124
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/CE/2503.06647v1",
      "true_abstract": "Accurate food intake monitoring is crucial for maintaining a healthy diet and\npreventing nutrition-related diseases. With the diverse range of foods consumed\nacross various cultures, classic food classification models have limitations\ndue to their reliance on fixed-sized food datasets. Studies show that people\nconsume only a small range of foods across the existing ones, each consuming a\nunique set of foods. Existing class-incremental models have low accuracy for\nthe new classes and lack personalization. This paper introduces a personalized,\nclass-incremental food classification model designed to overcome these\nchallenges and improve the performance of food intake monitoring systems. Our\napproach adapts itself to the new array of food classes, maintaining\napplicability and accuracy, both for new and existing classes by using\npersonalization. Our model's primary focus is personalization, which improves\nclassification accuracy by prioritizing a subset of foods based on an\nindividual's eating habits, including meal frequency, times, and locations. A\nmodified version of DSN is utilized to expand on the appearance of new food\nclasses. Additionally, we propose a comprehensive framework that integrates\nthis model into a food intake monitoring system. This system analyzes meal\nimages provided by users, makes use of a smart scale to estimate food weight,\nutilizes a nutrient content database to calculate the amount of each\nmacro-nutrient, and creates a dietary user profile through a mobile\napplication. Finally, experimental evaluations on two new benchmark datasets\nFOOD101-Personal and VFN-Personal, personalized versions of well-known datasets\nfor food classification, are conducted to demonstrate the effectiveness of our\nmodel in improving the classification accuracy of both new and existing\nclasses, addressing the limitations of both conventional and class-incremental\nfood classification models.",
      "generated_abstract": "r presents a novel approach to real-time face detection and\nface alignment in video using deep learning. We propose a unified\nmulti-stage architecture that integrates face alignment with video\ninference. The proposed method employs a series of feature extractors and\nclassifiers to capture different features of the face region. Each stage\nconducts a series of transformations, including deformable convolution,\ndeformable pooling, and residual transform. The final stage consists of a\nmultiscale convolutional neural network, which leverages both the\ndeformable convolutional features and residual transform to achieve\nhigh-precision face detection and alignment. The proposed method is trained\nend-to-end and evaluated on multiple datasets, achieving state-of-the-art\nperformance in both face detection and alignment. The code is available at\nhttps://github.com/HuangZhixiang/face_align",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1069182389937107,
          "p": 0.22666666666666666,
          "f": 0.1452991409434584
        },
        "rouge-2": {
          "r": 0.01195219123505976,
          "p": 0.028846153846153848,
          "f": 0.016901404308035137
        },
        "rouge-l": {
          "r": 0.10062893081761007,
          "p": 0.21333333333333335,
          "f": 0.13675213239644984
        }
      }
    },
    {
      "paper_id": "cs.FL.cs/FL/2503.05006v1",
      "true_abstract": "Markov decision process over vector addition system with states (VASS MDP) is\na finite state model combining non-deterministic and probabilistic behavior,\naugmented with non-negative integer counters that can be incremented or\ndecremented during each state transition. VASS MDPs can be used as abstractions\nof probabilistic programs with many decidable properties. In this paper, we\ndevelop techniques for analyzing the asymptotic behavior of VASS MDPs. That is,\nfor every initial configuration of size \\(n\\), we consider the number of\ntransitions needed to reach a configuration with some counter negative. We show\nthat given a strongly connected VASS MDP there either exists an integer \\(k\\leq\n2^d\\cdot 3^{|T|} \\), where \\(d \\) is the dimension and \\(|T|\\) the number of\ntransitions of the VASS MDP, such that for all \\(\\epsilon>0 \\) and all\nsufficiently large \\(n\\) it holds that the complexity of the VASS MDP lies\nbetween \\(n^{k-\\epsilon} \\) and \\(n^{k+\\epsilon} \\) with probability at least\n\\(1-\\epsilon \\), or it holds for all \\(\\epsilon>0 \\) and all sufficiently large\n\\(n\\) that the complexity of the VASS MDP is at least \\(2^{n^{1-\\epsilon}} \\)\nwith probability at least \\(1-\\epsilon \\). We show that it is decidable which\ncase holds and the \\(k\\) is computable in time polynomial in the size of the\nconsidered VASS MDP. We also provide a full classification of asymptotic\ncomplexity for VASS Markov chains.",
      "generated_abstract": "e a novel algorithm for the generation of stochastic\nsignature languages (SSLs), that, in contrast to previous methods, is\nstatistically complete: it can generate a non-trivial signature language with\nhigh probability. Our approach combines a deep generative model with a\ncomprehensive analysis of the SSL language model. This enables us to identify\nthe most important features of the language model and exploit them to ensure\nthat the generated SSLs are sufficiently diverse. Our approach achieves\nstatistical completeness by ensuring that the SSL language model generates\nsignatures that are sufficiently distinct from each other. We validate our\nproposed methodology by analyzing the SSL language models of GPT-4, GPT-3.5,\nGPT-3.7, and GPT-4.1, which are available in OpenAI's GPT-3 models. We find that\nour SSL language model is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1557377049180328,
          "p": 0.2289156626506024,
          "f": 0.1853658488395004
        },
        "rouge-2": {
          "r": 0.015957446808510637,
          "p": 0.02608695652173913,
          "f": 0.01980197548824301
        },
        "rouge-l": {
          "r": 0.13934426229508196,
          "p": 0.20481927710843373,
          "f": 0.1658536537175492
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.06308v1",
      "true_abstract": "Computable phenotypes are used to characterize patients and identify outcomes\nin studies conducted using healthcare claims and electronic health record data.\nChart review studies establish reference labels against which computable\nphenotypes are compared to understand their measurement characteristics, the\nquantity of interest, for instance the positive predictive value. We describe a\nmethod to adaptively evaluate a quantity of interest over sequential samples of\ncharts, with the goal to minimize the number of charts reviewed. With the help\nof a simultaneous confidence band, we stop the reviewing once the confidence\nband meets a pre-specified stopping threshold. The contribution of this article\nis threefold. First, we tested the use of an adaptive approach called Neyman's\nsampling of charts versus random or stratified random sampling. Second, we\npropose frequentist confidence bands and Bayesian credible intervals to\nsequentially evaluate the quantity of interest. Third, we propose a tool to\npredict the stopping time (defined as the number of charts reviewed) at which\nthe chart review would be complete. We observe that Bayesian credible intervals\nproved to be tighter than its frequentist confidence band counterparts.\nMoreover, we observe that simple random sampling is often performing similarly\nto Neyman's sampling.",
      "generated_abstract": "e a novel framework for the analysis of sequential data, which\nintegrates multivariate Gaussian processes (MVGPs) with random-effects models\nfor the latent space. This framework facilitates the analysis of data from\nmultiple independent experiments, where the observations are generated from a\ngiven latent space. In particular, we provide a Bayesian approach for\ninference in the latent space, leveraging the MVGP framework to model the\nlatent space distribution and incorporate prior information from the data. We\ndemonstrate the efficacy of our framework through a series of simulation\nstudies, including one- and two-dimensional experiments with continuous and\nbinary observations, respectively. Additionally, we apply our approach to the\nreal-world study of 10,000-hour piano lessons, where we show that our model\nis able to accurately capture the underlying latent space distribution and\nident",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14516129032258066,
          "p": 0.21951219512195122,
          "f": 0.17475727676124062
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13709677419354838,
          "p": 0.2073170731707317,
          "f": 0.16504853889716292
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.09865v1",
      "true_abstract": "We are concerned with the likelihood ratio tests in the $p_0$ model for\ntesting degree heterogeneity in directed networks. It is an exponential family\ndistribution on directed graphs with the out-degree sequence and the in-degree\nsequence as naturally sufficient statistics. For two growing dimensional null\nhypotheses: a specified null $H_{0}: \\theta_{i}=\\theta_{i}^{0}$ for\n$i=1,\\ldots,r$ and a homogenous null $H_{0}: \\theta_{1}=\\cdots=\\theta_{r}$, we\nreveal high dimensional Wilks' phenomena that the normalized log-likelihood\nratio statistic,\n$[2\\{\\ell(\\widehat{\\bs\\theta})-\\ell(\\widehat{\\bs\\theta}^{0})\\}-r]/(2r)^{1/2}$,\nconverges in distribution to a standard normal distribution as $r\\rightarrow\n\\infty$. Here, $\\ell( \\bs{\\theta})$ is the log-likelihood function,\n$\\widehat{\\bs{\\theta}}$ is the unrestricted maximum likelihood estimator (MLE)\nof $\\bs\\theta$, and $\\widehat{\\bs{\\theta}}^0$ is the restricted MLE for\n$\\bs\\theta$ under the null $H_{0}$. For the homogenous null $H_0:\n\\theta_1=\\cdots=\\theta_r$ with a fixed $r$, we establish the Wilks-type theorem\nthat $2\\{\\ell(\\widehat{\\bs{\\theta}}) - \\ell(\\widehat{\\bs{\\theta}}^0)\\}$\nconverges in distribution to a chi-square distribution with $r-1$ degrees of\nfreedom as $n\\rightarrow \\infty$, not depending on the nuisance parameters.\nThese results extend a recent work by \\cite{yan2023likelihood} to directed\ngraphs. Simulation studies and real data analyses illustrate the theoretical\nresults.",
      "generated_abstract": "In the last few years, the research on the statistical estimation of the\ninteraction coefficients of random variables has been a topic of considerable\ninterest. It is well known that the estimation of the interaction coefficient\nis equivalent to estimating the interaction coefficient of two independent\nrandom variables. In this paper, we study the statistical estimation of the\ninteraction coefficient of two random variables under the assumption that the\nrandom variables are jointly normal. We propose a novel method for the\nestimation of the interaction coefficient of two random variables under the\nassumption that the random variables are jointly normal. The proposed method is\nbased on the proposed in the paper, which is a new method for the estimation of\nthe mean of a multivariate normal random vector. We give a convergence rate\ntheorem for the proposed method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1504424778761062,
          "p": 0.29310344827586204,
          "f": 0.19883040487397843
        },
        "rouge-2": {
          "r": 0.030864197530864196,
          "p": 0.05434782608695652,
          "f": 0.03937007411990879
        },
        "rouge-l": {
          "r": 0.13274336283185842,
          "p": 0.25862068965517243,
          "f": 0.17543859200848136
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08970v1",
      "true_abstract": "Previous studies on echocardiogram segmentation are focused on the left\nventricle in parasternal long-axis views. In this study, deep-learning models\nwere evaluated on the segmentation of the ventricles in parasternal short-axis\nechocardiograms (PSAX-echo). Segmentation of the ventricles in complementary\nechocardiogram views will allow the computation of important metrics with the\npotential to aid in diagnosing cardio-pulmonary diseases and other\ncardiomyopathies. Evaluating state-of-the-art models with small datasets can\nreveal if they improve performance on limited data. PSAX-echo were performed on\n33 volunteer women. An experienced cardiologist identified end-diastole and\nend-systole frames from 387 scans, and expert observers manually traced the\ncontours of the cardiac structures. Traced frames were pre-processed and used\nto create labels to train 2 specific-domain (Unet-Resnet101 and Unet-ResNet50),\nand 4 general-domain (3 Segment Anything (SAM) variants, and the Detectron2)\ndeep-learning models. The performance of the models was evaluated using the\nDice similarity coefficient (DSC), Hausdorff distance (HD), and difference in\ncross-sectional area (DCSA). The Unet-Resnet101 model provided superior\nperformance in the segmentation of the ventricles with 0.83, 4.93 pixels, and\n106 pixel2 on average for DSC, HD, and DCSA respectively. A fine-tuned MedSAM\nmodel provided a performance of 0.82, 6.66 pixels, and 1252 pixel2, while the\nDetectron2 model provided 0.78, 2.12 pixels, and 116 pixel2 for the same\nmetrics respectively. Deep-learning models are suitable for the segmentation of\nthe left and right ventricles in PSAX-echo. This study demonstrated that\nspecific-domain trained models such as Unet-ResNet provide higher accuracy for\necho segmentation than general-domain segmentation models when working with\nsmall and locally acquired datasets.",
      "generated_abstract": "r introduces a novel framework for the simultaneous localization and\nintegrity verification of unmanned aerial vehicles (UAVs) using multispectral\nimagery. The proposed approach leverages a multi-sensing framework to\naccurately determine the location of UAVs and their integrity status. The\nframework incorporates a multi-spectral image classification module to\ndetermine the UAV's location based on the spectral characteristics of the\nimagery. The integrity status of the UAV is verified using the time-of-flight\n(TOF) method, which determines the UAV's motion using the optical characteristics\nof the imagery. The proposed framework is validated using a case study of\nUAVs flying in the vicinity of the Kish Island, Iran. The results show that the\nproposed framework significantly enhances the accuracy of UAV location and\nintegrity status determ",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08125,
          "p": 0.19117647058823528,
          "f": 0.11403508353339506
        },
        "rouge-2": {
          "r": 0.0211864406779661,
          "p": 0.047619047619047616,
          "f": 0.02932550893439231
        },
        "rouge-l": {
          "r": 0.08125,
          "p": 0.19117647058823528,
          "f": 0.11403508353339506
        }
      }
    },
    {
      "paper_id": "q-bio.NC.eess/SP/2503.02685v1",
      "true_abstract": "Precise parcellation of functional networks (FNs) of early developing human\nbrain is the fundamental basis for identifying biomarker of developmental\ndisorders and understanding functional development. Resting-state fMRI\n(rs-fMRI) enables in vivo exploration of functional changes, but adult FN\nparcellations cannot be directly applied to the neonates due to incomplete\nnetwork maturation. No standardized neonatal functional atlas is currently\navailable. To solve this fundamental issue, we propose TReND, a novel and fully\nautomated self-supervised transformer-autoencoder framework that integrates\nregularized nonnegative matrix factorization (RNMF) to unveil the FNs in\nneonates. TReND effectively disentangles spatiotemporal features in voxel-wise\nrs-fMRI data. The framework integrates confidence-adaptive masks into\ntransformer self-attention layers to mitigate noise influence. A self\nsupervised decoder acts as a regulator to refine the encoder's latent\nembeddings, which serve as reliable temporal features. For spatial coherence,\nwe incorporate brain surface-based geodesic distances as spatial encodings\nalong with functional connectivity from temporal features. The TReND clustering\napproach processes these features under sparsity and smoothness constraints,\nproducing robust and biologically plausible parcellations. We extensively\nvalidated our TReND framework on three different rs-fMRI datasets: simulated,\ndHCP and HCP-YA against comparable traditional feature extraction and\nclustering techniques. Our results demonstrated the superiority of the TReND\nframework in the delineation of neonate FNs with significantly better spatial\ncontiguity and functional homogeneity. Collectively, we established TReND, a\nnovel and robust framework, for neonatal FN delineation. TReND-derived neonatal\nFNs could serve as a neonatal functional atlas for perinatal populations in\nhealth and disease.",
      "generated_abstract": "ty to reliably identify genetic variants that influence phenotypes\nis crucial for understanding disease mechanisms and developing personalized\ntreatments. However, existing approaches for this task suffer from low\naccuracy and require extensive manual effort. To address these challenges, we\npropose a novel framework that integrates the power of neural networks with\nexisting genetic analysis tools. Our approach, called PrecisionGen, integrates\na Neural Architecture Search (NAS) framework with a Variant-Guided\nTranscriptional Regulation (VGTR) module. This integration allows PrecisionGen\nto generate diverse variants to enhance their fitness within the genome.\nSpecifically, we train the NAS to find variants that optimize a combination of\ngenetic contextual information and genetic fitness. Then, we incorporate this\nvariants into the VGTR module, which models transcriptional regulation based",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16265060240963855,
          "p": 0.29347826086956524,
          "f": 0.20930232099272894
        },
        "rouge-2": {
          "r": 0.021551724137931036,
          "p": 0.04201680672268908,
          "f": 0.028490024008247395
        },
        "rouge-l": {
          "r": 0.15060240963855423,
          "p": 0.2717391304347826,
          "f": 0.1937984450237367
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/PL/2503.05849v1",
      "true_abstract": "The quality of software products tends to correlate with the quality of the\nabstractions adopted early in the design process. Acknowledging this tendency\nhas led to the development of various tools and methodologies for modeling\nsystems thoroughly before implementing them. However, creating effective\nabstract models of domain problems is difficult, especially if the models are\nalso expected to exhibit qualities such as intuitiveness, being seamlessly\nintegrable with other models, or being easily translatable into code.\n  This thesis describes Conceptual, a DSL for modeling the behavior of software\nsystems using self-contained and highly reusable units of functionally known as\nconcepts. The language's syntax and semantics are formalized based on previous\nwork. Additionally, the thesis proposes a strategy for mapping language\nconstructs from Conceptual into the Alloy modeling language. The suggested\nstrategy is then implemented with a simple compiler, allowing developers to\naccess and utilize Alloy's existing analysis tools for program reasoning.\n  The utility and expressiveness of Conceptual is demonstrated qualitatively\nthrough several practical case studies. Using the implemented compiler, a few\nerroneous specifications are identified in the literature. Moreover, the thesis\nestablishes preliminary tool support in the Visual Studio Code IDE.",
      "generated_abstract": "We consider the problem of evaluating the coverage of a coverage system. A\ncoverage system is a set of rules that determine whether an input string is\ncovered by the system. Coverage systems are often employed in systems where\nerrors can be detected and corrected. The evaluation of a coverage system is\noften performed by examining the number of incorrect inputs. We propose an\nefficient and asymptotically optimal method for evaluating the coverage of a\ncoverage system. Our method uses a randomized prefix tree to examine the\ncoverage system, and the number of steps to perform this examination is\nproportional to the length of the input string. Our approach is simple,\nefficient, and asymptotically optimal.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09090909090909091,
          "p": 0.1875,
          "f": 0.12244897519366944
        },
        "rouge-2": {
          "r": 0.01092896174863388,
          "p": 0.020618556701030927,
          "f": 0.014285709757399392
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.1875,
          "f": 0.12244897519366944
        }
      }
    },
    {
      "paper_id": "math.LO.math/LO/2503.10528v1",
      "true_abstract": "The first-order model theory of modules has been studied for decades. More\nrecently, the model theoretic study of nonelementary classes of\nmodules--especially Abstract Elementary Classes of modules--has produced\ninteresting results. This survey aims to discuss these recent results and give\nan introduction to the framework of Abstract Elementary Classes for module\ntheorists.",
      "generated_abstract": "We give a new proof of the existence of the Riemann-Roch form for a\n(possibly non-quasi-projective) smooth projective variety $X$, using the\nnon-commutative Picard scheme. This generalizes the classical result of\nVoisin (see \\cite{voisin2007theoretical}), and is useful in proving the\nexistence of a global section of the first Chern class of a projective\nvariety. The method is based on a novel application of the trace theorem,\nintroduced by C. P. Chou, to a projective space-valued version of the Riemann-Roch\ntheorem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.14545454545454545,
          "f": 0.16842104775623282
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.14545454545454545,
          "f": 0.16842104775623282
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2412.13040v1",
      "true_abstract": "Epithelial tissues are continuously exposed to cyclic stretch. Physiological\nstretching has been found to regulate soft tissue function at the molecular,\ncellular, and tissue scales, allowing tissues to preserve their homeostasis and\nadapt to challenges. In contrast, dysregulated or pathological stretching can\ninduce damage and tissue fragilisation. Many mechanisms have been described for\nthe repair of epithelial tissues across a range of time-scales. In this review,\nwe present the timescales of (i) physiological cyclic loading regimes, (ii)\nstrain-regulated remodelling and damage accumulation, and (iii) repair\nmechanisms in epithelial tissues. We discuss how fatigue in biological tissues\ndiffers from synthetic materials, in that damage can be partially or fully\nreversed by repair mechanisms acting on timescales shorter than cyclic loading.\nWe highlight that timescales are critical to understanding the interplay\nbetween damage and repair in tissues that experience cyclic loading, opening up\nnew avenues for exploring soft tissue homeostasis.",
      "generated_abstract": "The cell-cell junctions that connect epithelial cells are important for\ncell-cell communication and cell migration. To investigate the role of these\njunctions in cell migration, we developed a cell migration model based on\nLagrangian dynamics. The model incorporates the effects of the cell-cell junction\ninteractions and cell-matrix interactions. We first conducted numerical\nsimulations to explore the effect of the junctions on cell migration. The\nresults show that the cell-cell junctions are important for cell migration.\nAdditionally, we analyzed the effect of the cell-cell junctions on cell\nmigration. The results indicate that the cell-cell junctions are essential for\ncell migration, especially in the direction opposite to the cell flow.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13541666666666666,
          "p": 0.24528301886792453,
          "f": 0.1744966397117248
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13541666666666666,
          "p": 0.24528301886792453,
          "f": 0.1744966397117248
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.13076v1",
      "true_abstract": "Machine learning predictions are typically interpreted as the sum of\ncontributions of predictors. Yet, each out-of-sample prediction can also be\nexpressed as a linear combination of in-sample values of the predicted\nvariable, with weights corresponding to pairwise proximity scores between\ncurrent and past economic events. While this dual route leads nowhere in some\ncontexts (e.g., large cross-sectional datasets), it provides sparser\ninterpretations in settings with many regressors and little training data-like\nmacroeconomic forecasting. In this case, the sequence of contributions can be\nvisualized as a time series, allowing analysts to explain predictions as\nquantifiable combinations of historical analogies. Moreover, the weights can be\nviewed as those of a data portfolio, inspiring new diagnostic measures such as\nforecast concentration, short position, and turnover. We show how weights can\nbe retrieved seamlessly for (kernel) ridge regression, random forest, boosted\ntrees, and neural networks. Then, we apply these tools to analyze post-pandemic\nforecasts of inflation, GDP growth, and recession probabilities. In all cases,\nthe approach opens the black box from a new angle and demonstrates how machine\nlearning models leverage history partly repeating itself.",
      "generated_abstract": "r extends the seminal theory of heteroskedasticity and autocorrelation\ncontinuous-time models (HAC-CTMs) to discrete-time models, providing a unified\nframework for modeling the time-varying autocorrelations of discrete-time\ndata. We show that the HAC-CTM can be recovered from a Gaussian mixture of\nHAC-CTMs, and we derive a closed-form solution for the variance of the\nGaussian mixture. We then extend the HAC-CTM to the discrete-time setting by\nintroducing a generalized autoregressive (GAR) process as a component, and we\ndemonstrate that the GAR-CTM can be recovered from the HAC-CTM. The GAR-CTM\nprovides a flexible and intuitive model for the time-varying autocorrelations\nof discrete-time",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11510791366906475,
          "p": 0.27586206896551724,
          "f": 0.16243654406864397
        },
        "rouge-2": {
          "r": 0.02824858757062147,
          "p": 0.058823529411764705,
          "f": 0.0381679345478124
        },
        "rouge-l": {
          "r": 0.10071942446043165,
          "p": 0.2413793103448276,
          "f": 0.1421319755407252
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2411.09517v1",
      "true_abstract": "We study a setting where agents use no-regret learning algorithms to\nparticipate in repeated auctions. \\citet{kolumbus2022auctions} showed, rather\nsurprisingly, that when bidders participate in second-price auctions using\nno-regret bidding algorithms, no matter how large the number of interactions\n$T$ is, the runner-up bidder may not converge to bidding truthfully. Our first\nresult shows that this holds for \\emph{general deterministic} truthful\nauctions. We also show that the ratio of the learning rates of the bidders can\n\\emph{qualitatively} affect the convergence of the bidders. Next, we consider\nthe problem of revenue maximization in this environment. In the setting with\nfully rational bidders, \\citet{myerson1981optimal} showed that revenue can be\nmaximized by using a second-price auction with reserves.We show that, in stark\ncontrast, in our setting with learning bidders, \\emph{randomized} auctions can\nhave strictly better revenue guarantees than second-price auctions with\nreserves, when $T$ is large enough. Finally, we study revenue maximization in\nthe non-asymptotic regime. We define a notion of {\\em auctioneer regret}\ncomparing the revenue generated to the revenue of a second price auction with\ntruthful bids. When the auctioneer has to use the same auction throughout the\ninteraction, we show an (almost) tight regret bound of $\\smash{\\widetilde\n\\Theta(T^{3/4})}.$ If the auctioneer can change auctions during the\ninteraction, but in a way that is oblivious to the bids, we show an (almost)\ntight bound of $\\smash{\\widetilde \\Theta(\\sqrt{T})}.$",
      "generated_abstract": "the problem of determining the number of agents in a two-agent\ngame with private information about the actions of the other agent. We show\nthat this problem can be solved in polynomial time in the number of agents,\nassuming the existence of a polynomial-time algorithm for the standard\ntwo-agent problem. Moreover, we give an algorithm that runs in\n$\\mathrm{poly}(n)$ time for any $n\\in \\mathbb{N}$, assuming that $n$ is a\npolynomial of the number of agents. We then consider the problem of deciding\nwhether two agents have a common neighbor in a two-agent game. We show that the\nproblem can be solved in polynomial time for any number of agents, assuming the\nexistence of a polynomial-time algorithm for the standard two-agent problem.\nMoreover, we give an algorithm that runs in $\\mathrm{poly}(n)$ time for any $n\\in\n\\",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15873015873015872,
          "p": 0.37735849056603776,
          "f": 0.22346368298242883
        },
        "rouge-2": {
          "r": 0.0673076923076923,
          "p": 0.16470588235294117,
          "f": 0.09556313581288092
        },
        "rouge-l": {
          "r": 0.15873015873015872,
          "p": 0.37735849056603776,
          "f": 0.22346368298242883
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.06913v1",
      "true_abstract": "Given a finite collection of stochastic alternatives, we study the problem of\nsequentially allocating a fixed sampling budget to identify the optimal\nalternative with a high probability, where the optimal alternative is defined\nas the one with the smallest value of extreme tail risk. We particularly\nconsider a situation where these alternatives generate heavy-tailed losses\nwhose probability distributions are unknown and may not admit any specific\nparametric representation. In this setup, we propose data-driven sequential\nsampling policies that maximize the rate at which the likelihood of falsely\nselecting suboptimal alternatives decays to zero. We rigorously demonstrate the\nsuperiority of the proposed methods over existing approaches, which is further\nvalidated via numerical studies.",
      "generated_abstract": "This paper addresses the problem of determining the location of a\nrandom vector of independent random variables by assuming that the\ndistribution of the vector is a product of two independent distributions. We\nprove that the location of the vector can be determined by solving the\nlinear equations arising from the product of the first-order conditions.\nMoreover, we show that the optimal solution is a non-Gaussian distribution with\nmean zero and covariance matrix equal to the identity matrix. The algorithm\nproposed for the solution of the linear system is based on the use of a\nconjugate gradient method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1744186046511628,
          "p": 0.2542372881355932,
          "f": 0.2068965468975031
        },
        "rouge-2": {
          "r": 0.03669724770642202,
          "p": 0.047058823529411764,
          "f": 0.04123710847858492
        },
        "rouge-l": {
          "r": 0.1511627906976744,
          "p": 0.22033898305084745,
          "f": 0.17931034000095136
        }
      }
    },
    {
      "paper_id": "eess.SP.stat/TH/2502.17981v1",
      "true_abstract": "This work deals with the generation of theoretical correlation matrices with\nspecific sparsity patterns, associated to graph structures. We present a novel\napproach based on convex optimization, offering greater flexibility compared to\nexisting techniques, notably by controlling the mean of the entry distribution\nin the generated correlation matrices. This allows for the generation of\ncorrelation matrices that better represent realistic data and can be used to\nbenchmark statistical methods for graph inference.",
      "generated_abstract": "We present a novel method for estimating the posterior distribution of a\nparameters of a parametric linear model from a large number of data points. Our\nmethod relies on a novel kernel density estimator based on the Gaussian\ndistribution of the joint distribution of the parameters and the data. We show\nthat this estimator is optimal in the sense that it achieves the same\ninformation rate as the likelihood ratio test for testing the null hypothesis\nof a particular parametric model. Furthermore, we derive a closed-form expression\nfor the posterior distribution of the parameters in terms of the joint\ndistribution of the parameters and the data. Finally, we use the kernel density\nestimator to obtain a novel estimator for the predictive distribution of the\nparameters of the parametric model. We demonstrate the usefulness of our\nmethod on simulated and real data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2727272727272727,
          "p": 0.22727272727272727,
          "f": 0.24793387933884303
        },
        "rouge-2": {
          "r": 0.1044776119402985,
          "p": 0.06363636363636363,
          "f": 0.07909604049283439
        },
        "rouge-l": {
          "r": 0.2727272727272727,
          "p": 0.22727272727272727,
          "f": 0.24793387933884303
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.05591v1",
      "true_abstract": "The article proposes a computational approach that can generate a descending\norder of the IUPAC-notated functional groups based on their importance for a\ngiven case study. Thus, a reduced list of functional groups could be obtained\nfrom which drug discovery can be successfully initiated. The approach,\napplicable to any study case with sufficient data, was demonstrated using a\nPubChem bioassay focused on TDP1 inhibitors. The Scikit Learn interpretation of\nthe Random Forest Classifier (RFC) algorithm was employed. The machine learning\n(ML) model RFC obtained 70.9% accuracy, 73.1% precision, 66.1% recall, 69.4% F1\nand 70.8% receiver-operating characteristic (ROC). In addition to the main\nstudy, the CID_SID ML model was developed, which, using only the PubChem\ncompound and substance identifiers (CIDs and SIDs) data, can predict with 85.2%\naccuracy, 94.2% precision, 75% precision, F1 of 83.5% F1 and 85.2% ROC whether\na compound is a TDP1 inhibitor.",
      "generated_abstract": "t a novel method for the prediction of protein-protein interactions\n(PPIs) using the latent space of a deep neural network trained on a large\ndatabase of protein-protein interactions. This approach, which we refer to as\nthe latent space method, relies on the representation of PPIs in the latent\nspace of the network. We demonstrate that the latent space method outperforms\nother state-of-the-art methods for predicting PPIs, such as the probabilistic\nmatrix factorization (PMF) method, the deep generative model (DGM) method, and\nthe deep belief network (DBN) method, by a large margin. The latent space method\nis trained on 1.5 billion PPIs, which we consider to be the largest database of\nPPIs. We further demonstrate that the method can be applied to other types of\nbiological interactions, such as",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1509433962264151,
          "p": 0.2318840579710145,
          "f": 0.18285713808065318
        },
        "rouge-2": {
          "r": 0.013422818791946308,
          "p": 0.018691588785046728,
          "f": 0.015624995134584034
        },
        "rouge-l": {
          "r": 0.1509433962264151,
          "p": 0.2318840579710145,
          "f": 0.18285713808065318
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/EC/2502.17044v1",
      "true_abstract": "Supply chain disruptions constitute an often underestimated risk for\nfinancial stability. As in financial networks, systemic risks in production\nnetworks arises when the local failure of one firm impacts the production of\nothers and might trigger cascading disruptions that affect significant parts of\nthe economy. Here, we study how systemic risk in production networks translates\ninto financial systemic risk through a mechanism where supply chain contagion\nleads to correlated bank-firm loan defaults. We propose a financial\nstress-testing framework for micro- and macro-prudential applications that\nfeatures a national firm level supply chain network in combination with\ninterbank network layers. The model is calibrated by using a unique data set\nincluding about 1 million firm-level supply links, practically all bank-firm\nloans, and all interbank loans in a small European economy. As a showcase we\nimplement a real COVID-19 shock scenario on the firm level. This model allows\nus to study how the disruption dynamics in the real economy can lead to\ninterbank solvency contagion dynamics. We estimate to what extent this\namplifies financial systemic risk. We discuss the relative importance of these\ncontagion channels and find an increase of interbank contagion by 70% when\nproduction network contagion is present. We then examine the financial systemic\nrisk firms bring to banks and find an increase of up to 28% in the presence of\nthe interbank contagion channel. This framework is the first financial systemic\nrisk model to take agent-level dynamics of the production network and shocks of\nthe real economy into account which opens a path for directly, and event-driven\nunderstanding of the dynamical interaction between the real economy and\nfinancial systems.",
      "generated_abstract": "cial sector is one of the largest actors in the planet, contributing\nto the economy and employment in many countries. Despite this, it is still\nexposed to the risk of cyber-attacks, which can cause significant financial\nlosses. This paper examines the financial sector's vulnerability to cyber-\nattacks, focusing on the cryptocurrency sector. We use the Vulnerability\nAssessment Model (VAM) to assess the vulnerability of the financial sector to\ncyber-attacks. The results show that the financial sector is significantly\nvulnerable to cyber-attacks. The most vulnerable sectors are financial\ninstitutions, followed by banking and insurance. We also find that the\nfinancial sector is particularly susceptible to attacks on cloud services,\nwhile attacks on local servers are less common. These findings provide\ninsights into the financial sector'",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1527777777777778,
          "p": 0.275,
          "f": 0.1964285668367348
        },
        "rouge-2": {
          "r": 0.025,
          "p": 0.05504587155963303,
          "f": 0.0343839498591971
        },
        "rouge-l": {
          "r": 0.14583333333333334,
          "p": 0.2625,
          "f": 0.1874999954081634
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2411.03699v4",
      "true_abstract": "We study a multivariate autoregressive stochastic volatility model for the\nfirst 3 principal components (level, slope, curvature) of 10 series of\nzero-coupon Treasury bond rates with maturities from 1 to 10 years. We fit this\nmodel using monthly data from 1990. Unlike classic models with hidden\nstochastic volatility, here it is observed as VIX: the volatility index for the\nS&P 500 stock market index. Surprisingly, this stock index volatility works for\nTreasury bonds, too. Next, we prove long-term stability and the Law of Large\nNumbers. We express total returns of zero-coupon bonds using these principal\ncomponents. We prove the Law of Large Numbers for these returns. All results\nare done for discrete and continuous time.",
      "generated_abstract": "y investigates the interplay between macroeconomic factors and\nfinancial market dynamics by applying a large language model (LLM) to forecast\nfinancial market returns. The research examines the effects of macroeconomic\nfactors such as economic growth, inflation, and interest rates on stock and\nbond returns. Additionally, the study explores the impact of LLMs on financial\nmarket forecasting, analyzing their accuracy, robustness, and computational\ncomplexity. The results indicate that LLMs are capable of accurately\nforecasting financial market returns, with the best results achieved for the\ninterest rate LLM. Additionally, the research finds that LLMs are more\nrobust and efficient than traditional machine learning methods, with their\ncomputational complexity and computational efficiency making them suitable\noptions for financial applications. The study concludes by offering insights\ninto the potential of LLMs for financial market",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22077922077922077,
          "p": 0.2073170731707317,
          "f": 0.21383647299236594
        },
        "rouge-2": {
          "r": 0.009345794392523364,
          "p": 0.008547008547008548,
          "f": 0.008928566438539141
        },
        "rouge-l": {
          "r": 0.19480519480519481,
          "p": 0.18292682926829268,
          "f": 0.18867924028796343
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.01352v1",
      "true_abstract": "Polarization, as a new optical imaging tool, has been explored to assist in\nthe diagnosis of pathology. Moreover, converting the polarimetric Mueller\nMatrix (MM) to standardized stained images becomes a promising approach to help\npathologists interpret the results. However, existing methods for\npolarization-based virtual staining are still in the early stage, and the\ndiffusion-based model, which has shown great potential in enhancing the\nfidelity of the generated images, has not been studied yet. In this paper, a\nRegulated Bridge Diffusion Model (RBDM) for polarization-based virtual staining\nis proposed. RBDM utilizes the bidirectional bridge diffusion process to learn\nthe mapping from polarization images to other modalities such as H\\&E and\nfluorescence. And to demonstrate the effectiveness of our model, we conduct the\nexperiment on our manually collected dataset, which consists of 18,000 paired\npolarization, fluorescence and H\\&E images, due to the unavailability of the\npublic dataset. The experiment results show that our model greatly outperforms\nother benchmark methods. Our dataset and code will be released upon acceptance.",
      "generated_abstract": "years, deep learning has made remarkable progress in medical\nimage analysis, particularly in diabetic retinopathy (DR) classification. However,\nthe vast amount of DR images, combined with the complex structure and\nheterogeneity of DR lesions, hinders the accurate classification of DR. To\naddress this issue, we propose a deep learning-based DR classification model\nwith the Transformer architecture. Specifically, we introduce a multi-stage\ntransformation strategy that integrates the multi-scale feature extraction and\nrepresentation learning. To enhance the representation learning ability, we\npropose a novel attention mechanism and transformer layers, which can effectively\nfocus on the relevant features of DR lesions. To further improve the\nperformance, we introduce a multi-scale fusion strategy to fuse the feature\nextracted from different scales, which effectively reduces the influence of\nbackground noise. To handle the imbalanced dataset, we propose",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14655172413793102,
          "p": 0.19767441860465115,
          "f": 0.16831682679345175
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13793103448275862,
          "p": 0.18604651162790697,
          "f": 0.15841583669444187
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2501.07737v1",
      "true_abstract": "Understanding how molecular changes caused by genetic variation drive disease\nrisk is crucial for deciphering disease mechanisms. However, interpreting\ngenome sequences is challenging because of the vast size of the human genome,\nand because its consequences manifest across a wide range of cells, tissues and\nscales -- spanning from molecular to whole organism level. Here, we present\nPhenformer, a multi-scale genetic language model that learns to generate\nmechanistic hypotheses as to how differences in genome sequence lead to\ndisease-relevant changes in expression across cell types and tissues directly\nfrom DNA sequences of up to 88 million base pairs. Using whole genome\nsequencing data from more than 150 000 individuals, we show that Phenformer\ngenerates mechanistic hypotheses about disease-relevant cell and tissue types\nthat match literature better than existing state-of-the-art methods, while\nusing only sequence data. Furthermore, disease risk predictors enriched by\nPhenformer show improved prediction performance and generalisation to diverse\npopulations. Accurate multi-megabase scale interpretation of whole genomes\nwithout additional experimental data enables both a deeper understanding of\nmolecular mechanisms involved in disease and improved disease risk prediction\nat the level of individuals.",
      "generated_abstract": "nerative diseases are a major cause of mortality and disability\nin the world. They are characterized by a progressive loss of neurons and\nsynapses. The pathology is often characterized by the presence of tangles and\nfibrils of misfolded proteins, and the disease progresses in a cascade of\nevents. This review focuses on the pathogenesis of Alzheimer's disease,\nconsidering the contribution of genetic, environmental and lifestyle factors to\nthis pathology. We discuss the role of microglia, astrocytes and neurons in\nthe disease pathology, and the role of amyloid beta and tau proteins in the\ndisease. We also discuss the contribution of oxidative stress to the pathology\nof Alzheimer's disease. We also discuss the impact of COVID-19 on Alzheimer's",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.075,
          "p": 0.14285714285714285,
          "f": 0.09836065122278977
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.075,
          "p": 0.14285714285714285,
          "f": 0.09836065122278977
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.04278v1",
      "true_abstract": "This study addresses the challenge of access point (AP) and user equipment\n(UE) association in cell-free massive MIMO networks. It introduces a deep\nlearning algorithm leveraging Bidirectional Long Short-Term Memory cells and a\nhybrid probabilistic methodology for weight updating. This approach enhances\nscalability by adapting to variations in the number of UEs without requiring\nretraining. Additionally, the study presents a training methodology that\nimproves scalability not only with respect to the number of UEs but also to the\nnumber of APs. Furthermore, a variant of the proposed AP-UE algorithm ensures\nrobustness against pilot contamination effects, a critical issue arising from\npilot reuse in channel estimation. Extensive numerical results validate the\neffectiveness and adaptability of the proposed methods, demonstrating their\nsuperiority over widely used heuristic alternatives.",
      "generated_abstract": "r proposes a novel algorithm for solving the non-convex mixed-integer\nnonlinear programming (MINLP) problem, which is a key sub-problem in many\nreal-world applications. This problem is widely used in the field of decision\nanalysis and is also a fundamental component of many applications in other\nareas such as operations research, management science, and engineering\nsimulation. Recently, the research community has recognized the potential of\ndeep learning in solving non-convex optimization problems, with great success.\nHowever, most existing deep learning algorithms for solving non-convex\nMINLPs focus on solving a series of sub-problems, resulting in a\ncomputationally expensive process. In this paper, we propose a novel approach\nbased on a single-layer neural network, which enables a faster and more\nefficient solution of the MINLP problem. Specifically, we first construct a\nlarge-scale graph to represent the objective function and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17204301075268819,
          "p": 0.16842105263157894,
          "f": 0.17021276095801283
        },
        "rouge-2": {
          "r": 0.034482758620689655,
          "p": 0.030534351145038167,
          "f": 0.03238865898605201
        },
        "rouge-l": {
          "r": 0.13978494623655913,
          "p": 0.1368421052631579,
          "f": 0.1382978673409916
        }
      }
    },
    {
      "paper_id": "q-bio.QM.quant-ph/2503.10510v1",
      "true_abstract": "Whole-slide image classification represents a key challenge in computational\npathology and medicine. Attention-based multiple instance learning (MIL) has\nemerged as an effective approach for this problem. However, the effect of\nattention mechanism architecture on model performance is not well-documented\nfor biomedical imagery. In this work, we compare different methods and\nimplementations of MIL, including deep learning variants. We introduce a new\nmethod using higher-dimensional feature spaces for deep MIL. We also develop a\nnovel algorithm for whole-slide image classification where extreme machine\nlearning is combined with attention-based MIL to improve sensitivity and reduce\ntraining complexity. We apply our algorithms to the problem of detecting\ncirculating rare cells (CRCs), such as erythroblasts, in peripheral blood. Our\nresults indicate that nonlinearities play a key role in the classification, as\nremoving them leads to a sharp decrease in stability in addition to a decrease\nin average area under the curve (AUC) of over 4%. We also demonstrate a\nconsiderable increase in robustness of the model with improvements of over 10%\nin average AUC when higher-dimensional feature spaces are leveraged. In\naddition, we show that extreme learning machines can offer clear improvements\nin terms of training efficiency by reducing the number of trained parameters by\na factor of 5 whilst still maintaining the average AUC to within 1.5% of the\ndeep MIL model. Finally, we discuss options of enriching the classical\ncomputing framework with quantum algorithms in the future. This work can thus\nhelp pave the way towards more accurate and efficient single-cell diagnostics,\none of the building blocks of precision medicine.",
      "generated_abstract": "t a novel quantum protocol for the measurement of a time-dependent\nquantum coherent state, which allows for both time-independent and time-dependent\nmeasurements. The protocol is based on the measurement of a local unitary\noperator in the time-dependent basis of the coherent state. In this basis, the\nmeasurement operator can be written as a direct product of the time-dependent\nunitary operator with a time-independent one. We show that the final\nclassical-quantum state can be written as a product state of the original\nquantum coherent state with the time-dependent unitary operator. Our protocol\nrelies on a unitary transformation between the state of the system and the\nclassical state of the quantum control field, which is achieved by a series of\nunitary operations. We present numerical simulations to demonstrate the\nsuccessful application of the protocol to the measurement of the time-dependent\nquant",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.36923076923076925,
          "f": 0.2060085796680728
        },
        "rouge-2": {
          "r": 0.03278688524590164,
          "p": 0.07142857142857142,
          "f": 0.04494381591213272
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.3230769230769231,
          "f": 0.18025750670669943
        }
      }
    },
    {
      "paper_id": "math.SG.math/SG/2503.09209v1",
      "true_abstract": "Time-dependent Stark-Zeeman systems describe the motion of an electron\nattracted by a proton subject to a magnetic and a time-dependent electric\nfield. For instance the study of the dynamics of a gateway around the moon\nwhich is subject to the joint attraction of the moon, the earth and the sun\nleads to time-dependent Stark-Zeeman systems. In the time-dependent case there\nis no preserved energy. Therefore collisions cannot be regularized by blowing\nup the energy hypersurface. A new regularization technique of blowing up\ninstead of the energy hypersurface the loop space was recently discovered by\nBarutello, Ortega, and Verzini. In this article we explain how this new\nregularization technique can be applied to the study of periodic orbits in\ntime-dependent planar Stark-Zeeman systems. Since the regularization by\nblowing-up the loop space is nonlocal the regularized periodic orbits will not\nsatisfy an ODE anymore but a delay equation.",
      "generated_abstract": "We prove the existence of a weak solution to a stochastic optimal control\nprocess with a quadratic cost functional in a non-smooth setting. The\nframework is based on the functional differential geometry approach. We show\nthat the control problem is equivalent to the optimal control problem of the\nclassical Stochastic Differential Equation (SDE) with the quadratic cost\nfunctional. The solution is obtained as a limit of the solutions to a sequence\nof SDEs with a quadratic cost functional. We obtain the strong solution of the\ncontrol problem in the limit as the solution of the SDEs converges to the\nweak solution of the control problem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07142857142857142,
          "p": 0.13043478260869565,
          "f": 0.09230768773491146
        },
        "rouge-2": {
          "r": 0.031007751937984496,
          "p": 0.04819277108433735,
          "f": 0.037735844292008436
        },
        "rouge-l": {
          "r": 0.07142857142857142,
          "p": 0.13043478260869565,
          "f": 0.09230768773491146
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2503.08533v1",
      "true_abstract": "Advancements in audio foundation models (FMs) have fueled interest in\nend-to-end (E2E) spoken dialogue systems, but different web interfaces for each\nsystem makes it challenging to compare and contrast them effectively. Motivated\nby this, we introduce an open-source, user-friendly toolkit designed to build\nunified web interfaces for various cascaded and E2E spoken dialogue systems.\nOur demo further provides users with the option to get on-the-fly automated\nevaluation metrics such as (1) latency, (2) ability to understand user input,\n(3) coherence, diversity, and relevance of system response, and (4)\nintelligibility and audio quality of system output. Using the evaluation\nmetrics, we compare various cascaded and E2E spoken dialogue systems with a\nhuman-human conversation dataset as a proxy. Our analysis demonstrates that the\ntoolkit allows researchers to effortlessly compare and contrast different\ntechnologies, providing valuable insights such as current E2E systems having\npoorer audio quality and less diverse responses. An example demo produced using\nour toolkit is publicly available here:\nhttps://huggingface.co/spaces/Siddhant/Voice_Assistant_Demo.",
      "generated_abstract": "This paper presents a novel framework for learning dynamic and context-aware\nclustering models using a multi-task self-supervised pretraining paradigm.\nSpecifically, we propose a hierarchical self-supervised pretraining framework\nthat enables the model to learn both the global and local contextual\nproperties of the input documents. By training the model on both global and\nlocal features, our approach enables it to identify the underlying semantic\nclusters, which can then be fine-tuned using a pre-trained contextual\nrepresentation for the task at hand. We demonstrate the effectiveness of our\nmodel through several experiments, including a comparison with existing\nstate-of-the-art methods. Our results show that our approach consistently\noutperforms existing state-of-the-art methods across different benchmarks,\nhighlighting the effectiveness of our multi-task self-supervised pretraining\nframework.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13392857142857142,
          "p": 0.19230769230769232,
          "f": 0.15789473200221624
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11607142857142858,
          "p": 0.16666666666666666,
          "f": 0.1368421004232689
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2411.00071v2",
      "true_abstract": "Musculoskeletal (MSK) models offer a non-invasive way to understand\nbiomechanical loads on joints and tendons, which are difficult to measure\ndirectly. Variations in muscle strength, especially relative differences\nbetween muscles, significantly impact model outcomes. Typically, scaled generic\nMSK models use maximum isometric forces that are not adjusted for different\ndemographics, raising concerns about their accuracy. This review provides an\noverview on experimentally derived strength parameters, including physiological\ncross-sectional area (PCSA), muscle mass (Mm), and relative muscle mass (%Mm),\nwhich is the relative distribution of muscle mass across the leg. We analysed\ndifferences by age and sex, and compared open-source lower limb MSK model\nparameters with experimental data from 57 studies. Our dataset, with records\ndating back to 1884, shows that uniformly increasing all maximum isometric\nforces in MSK models does not capture key muscle ratio differences due to age\nand sex. Males have a higher proportion of muscle mass in the rectus femoris\nand semimembranosus muscles, while females have a greater relative muscle mass\nin the pelvic (gluteus maximus and medius) and ankle muscles (tibialis\nanterior, tibialis posterior, and extensor digitorum longus). Older adults have\na higher relative muscle mass in the gluteus medius, while younger individuals\nshow more in the gastrocnemius. Current MSK models do not accurately represent\nmuscle mass distribution for specific age or sex groups, and none of them\naccurately reflect female muscle mass distribution. Further research is needed\nto explore musculotendon age- and sex differences.",
      "generated_abstract": "In this paper, we propose a novel framework for evaluating the performance\nof protein sequence-based models. The framework enables comparison of\nindividual proteins based on their predicted scores, and can be applied to\nproteins that have been pre-trained on a large database of sequence-based\nmodels. We show that the performance of these models can be improved by\nre-training them on a new dataset of proteins, using a novel training scheme.\nThis method can be applied to many existing protein sequence-based models,\nincluding those that are pre-trained on large datasets of sequence-based models.\nWe show that this approach can improve the performance of such models,\nparticularly in cases where the existing models do not achieve optimal\nperformance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1346153846153846,
          "p": 0.3,
          "f": 0.18584070368862096
        },
        "rouge-2": {
          "r": 0.014084507042253521,
          "p": 0.030927835051546393,
          "f": 0.01935483440978243
        },
        "rouge-l": {
          "r": 0.1346153846153846,
          "p": 0.3,
          "f": 0.18584070368862096
        }
      }
    },
    {
      "paper_id": "cs.CL.q-fin/RM/2503.01886v1",
      "true_abstract": "This study presents a comparative analysis of deep learning methodologies\nsuch as BERT, FinBERT and ULMFiT for sentiment analysis of earnings call\ntranscripts. The objective is to investigate how Natural Language Processing\n(NLP) can be leveraged to extract sentiment from large-scale financial\ntranscripts, thereby aiding in more informed investment decisions and risk\nmanagement strategies. We examine the strengths and limitations of each model\nin the context of financial sentiment analysis, focusing on data preprocessing\nrequirements, computational efficiency, and model optimization. Through\nrigorous experimentation, we evaluate their performance using key metrics,\nincluding accuracy, precision, recall, and F1-score. Furthermore, we discuss\npotential enhancements to improve the effectiveness of these models in\nfinancial text analysis, providing insights into their applicability for\nreal-world financial decision-making.",
      "generated_abstract": "Stock market forecasting has become an increasingly important task in financial\ncryptocurrency trading. However, existing approaches often rely on\nstate-of-the-art models that are not easily transferable to new tasks. In this\npaper, we propose a novel framework that unifies stock market prediction,\nreinforcement learning, and neural language models. Our approach integrates\nreinforcement learning and transformer architectures to enhance the model's\nperformance. We also introduce a novel data augmentation technique that\nenhances the training efficiency of the reinforcement learning agent. Our\nexperiments demonstrate that our approach outperforms existing state-of-the-art\nmodels, demonstrating its effectiveness in stock market prediction.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15625,
          "p": 0.2,
          "f": 0.17543859156663602
        },
        "rouge-2": {
          "r": 0.008403361344537815,
          "p": 0.010869565217391304,
          "f": 0.009478668067656019
        },
        "rouge-l": {
          "r": 0.13541666666666666,
          "p": 0.17333333333333334,
          "f": 0.15204677870113897
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.02327v2",
      "true_abstract": "Nonlinear frequency hopping has emerged as a promising approach for\nmitigating interference and enhancing range resolution in automotive FMCW radar\nsystems. Achieving an optimal balance between high range-resolution and\neffective interference mitigation remains challenging, especially without\ncentralized frequency scheduling. This paper presents a game-theoretic\nframework for interference avoidance, in which each radar operates as an\nindependent player, optimizing its performance through decentralized\ndecision-making. We examine two equilibrium concepts--Nash Equilibrium (NE) and\nCoarse Correlated Equilibrium (CCE)--as strategies for frequency band\nallocation, with CCE demonstrating particular effectiveness through regret\nminimization algorithms. We propose two interference avoidance algorithms: Nash\nHopping, a model-based approach, and No-Regret Hopping, a model-free adaptive\nmethod. Simulation results indicate that both methods effectively reduce\ninterference and enhance the signal-to-interference-plus-noise ratio (SINR).\nNotably, No-regret Hopping further optimizes frequency spectrum utilization,\nachieving improved range resolution compared to Nash Hopping.",
      "generated_abstract": "r proposes a novel hybrid distributed multi-user detection framework\nfor the downlink of multi-cell multi-input multi-output (MIMO) wireless\nsystems. In this framework, a multi-cell MIMO channel model is developed to\nenable the simultaneous detection of multiple users, and a hybrid centralized\nand decentralized distributed detection framework is proposed to address the\ndifficulties of decentralized detection, namely the limited number of antennas\nat the base station (BS) and the lack of channel state information (CSI) at the\nuser terminals. The proposed framework is composed of two main components:\ncentralized detection, which is based on the proposed centralized detection\nframework, and decentralized detection, which is based on the proposed\ndecentralized detection framework. Centralized detection includes two\nsub-components: sub-component 1 for single-user detection and sub-component\n2 for multi",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10091743119266056,
          "p": 0.15942028985507245,
          "f": 0.1235955008704711
        },
        "rouge-2": {
          "r": 0.007462686567164179,
          "p": 0.008928571428571428,
          "f": 0.008130076340805457
        },
        "rouge-l": {
          "r": 0.10091743119266056,
          "p": 0.15942028985507245,
          "f": 0.1235955008704711
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/SC/2401.04786v1",
      "true_abstract": "The method developed by Michaelis and Menten was foundational in the\ndevelopment of our understanding of biochemical reaction kinetics. Extended\nmodels of metabolism encapsulated by reaction rate theory, stochastic reaction\nmodels, and dynamic flux estimation, amongst others, address aspects of this\nfundamental idea. The limitations of these approaches are well understood, and\nefforts to overcome those issues so far have been plentiful but with limited\nsuccess. The known issues can be summarised as the sole dependent relation with\nsubstrate concentration, the encapsulation of rate in a single relevant scalar,\nand the subsequent lack of functional control that results from this\nassumption. The Rate Control of Chaos (RCC) is a nonlinear control method that\nhas been shown to be effective in controlling the dynamic state of biological\noscillators based on the concept of rate limitation of the exponential growth\nin chaotic systems. Extending RCC with allosteric properties allows robust\ncontrol of the enzymatic process, and replicates the Michaelis-Menten kinetics.\nThe emergent dynamics is robust to perturbations and noise but susceptible to\nregulatory adjustments. This control method adapts the control parameters\ndynamically in the presence of a ligand, and permits introduction of energy\nrelations into the control function. The dynamic nature of the control\neliminates the steady-state requirements and allows the modelling of\nlarge-scale dynamic behaviour, potentially addressing issues in metabolic\ndisorder and failure of metabolic control.",
      "generated_abstract": "of deep learning-based tools for molecular analysis has created\nlarge-scale protein datasets that are expanding rapidly, yet existing\nmethods for data analysis remain limited by their ability to handle diverse\ndata types and complex data structures. In this paper, we present a\nmulti-scale, hierarchical, and data-agnostic framework for large-scale protein\nanalysis. Our framework includes a hierarchical structure that is flexible and\nscalable to diverse datasets, and provides a unified, data-agnostic approach\nfor analyzing protein structures, sequences, and datasets. We demonstrate the\neffectiveness of our framework by analyzing the protein structures and\nsequences of 1530 species, 3585 proteins, and 30922 amino acids. Our analysis\nshows that the protein structure and sequence datasets are diverse, with\ncomparable data types. Our framework allows for scalable analysis across these\nd",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12056737588652482,
          "p": 0.21794871794871795,
          "f": 0.15525113696628523
        },
        "rouge-2": {
          "r": 0.004608294930875576,
          "p": 0.008264462809917356,
          "f": 0.005917155166664402
        },
        "rouge-l": {
          "r": 0.12056737588652482,
          "p": 0.21794871794871795,
          "f": 0.15525113696628523
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2404.14137v2",
      "true_abstract": "Providing a measure of market risk is an important issue for investors and\nfinancial institutions. However, the existing models for this purpose are per\ndefinition symmetric. The current paper introduces an asymmetric capital asset\npricing model for measurement of the market risk. It explicitly accounts for\nthe fact that falling prices determine the risk for a long position in the\nrisky asset and the rising prices govern the risk for a short position. Thus, a\nposition dependent market risk measure that is provided accords better with\nreality. The empirical application reveals that Apple stock is more volatile\nthan the market only for the short seller. Surprisingly, the investor that has\na long position in this stock is facing a lower volatility than the market.\nThis property is not captured by the standard asset pricing model, which has\nimportant implications for the expected returns and hedging designs.",
      "generated_abstract": "We present a novel framework for the pricing of options with correlated\ncash flows, with applications to the pricing of European call options. We\nintroduce a stochastic volatility model that accounts for both time-varying\nvolatility and correlations in the volatility between the underlying and the\ncall price. We then derive a closed-form expression for the payoff of the\nunderlying option, which is of the form of a sum of multivariate Gamma random\nvariables. We provide a numerical method to approximate the solution, and show\nthat the solution is a Gaussian process. We then consider the pricing of\nEuropean call options with correlated cash flows, and show that the problem\nadmits a closed-form solution. The results are validated against numerical\nsimulations, and the proposed method is shown to be efficient and accurate.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18604651162790697,
          "p": 0.2318840579710145,
          "f": 0.20645160796337161
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.045454545454545456,
          "f": 0.04166666170138949
        },
        "rouge-l": {
          "r": 0.1744186046511628,
          "p": 0.21739130434782608,
          "f": 0.19354838215692
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.06329v1",
      "true_abstract": "In this paper, we introduce and study a class of monoids, called Layered\nCatalan Monoids (\\( {LC}_n \\)), which satisfy the structural conditions for\n$\\ll$-smoothness as defined in~\\cite{Sha-Det2}. These monoids are defined by\nspecific identities inspired by Catalan monoids. We establish their canonical\nforms and compute their determinant, proving that it is non-zero for \\(1 \\leq n\n\\leq 7\\) but vanishes for \\(n \\geq 8\\).",
      "generated_abstract": "We prove that, for any finite set $A$ of positive integers, the group\n$\\operatorname{Aut}(A)$ is finitely presented if and only if $A$ is a finite\npermutation group. In particular, the group of automorphisms of a finite set\n$A$ is finitely presented if and only if $A$ is a finite group. This result is\na generalization of the result of Kadison and Serre for groups.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.25806451612903225,
          "f": 0.1839080413898799
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.25806451612903225,
          "f": 0.1839080413898799
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.04648v1",
      "true_abstract": "The time-varying reproduction number ($R_t$) gives an indication of the\ntrajectory of an infectious disease outbreak. Commonly used frameworks for\ninferring $R_t$ from epidemiological time series include those based on\ncompartmental models (such as the SEIR model) and renewal equation models.\nThese inference methods are usually validated using synthetic data generated\nfrom a simple model, often from the same class of model as the inference\nframework. However, in a real outbreak the transmission processes, and thus the\ninfection data collected, are much more complex. The performance of common\n$R_t$ inference methods on data with similar complexity to real world scenarios\nhas been subject to less comprehensive validation. We therefore propose\nevaluating these inference methods on outbreak data generated from a\nsophisticated, geographically accurate agent-based model. We illustrate this\nproposed method by generating synthetic data for two outbreaks in Northern\nIreland: one with minimal spatial heterogeneity, and one with additional\nheterogeneity. We find that the simple SEIR model struggles with the greater\nheterogeneity, while the renewal equation model demonstrates greater robustness\nto spatial heterogeneity, though is sensitive to the accuracy of the generation\ntime distribution used in inference. Our approach represents a principled way\nto benchmark epidemiological inference tools and is built upon an open-source\nsoftware platform for reproducible epidemic simulation and inference.",
      "generated_abstract": "aper, we present a comprehensive, multi-scale analysis of the\nperformance of a large number of stochastic reaction-diffusion (RD) models\ncoupled to a large number of stochastic reaction-diffusion (RD) models,\nwhere the coupling is modeled by a continuous-time Markov chain (CTMC). We\nformalize this coupling as a stochastic differential equation (SDE) modelled by\nan infinite-dimensional, non-Markovian, CTMC. We present a detailed analysis\nof the performance of these stochastic reaction-diffusion models, using\neither a finite-dimensional CTMC or a Markov chain as the coupling model. We\nshow that these models have different statistical properties than a Markov\nchain and that the performance of the models can be substantially improved by\nintroducing appropriate noise. We also show that the performance of the models\ncan be improved by introdu",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.296875,
          "f": 0.19289339662861715
        },
        "rouge-2": {
          "r": 0.02512562814070352,
          "p": 0.05263157894736842,
          "f": 0.034013601067842666
        },
        "rouge-l": {
          "r": 0.12781954887218044,
          "p": 0.265625,
          "f": 0.1725888281006984
        }
      }
    },
    {
      "paper_id": "math.GN.math/GN/2502.18833v1",
      "true_abstract": "A topological space is domain-representable (or, has a domain model) if it is\nhomeomorphic to the maximal point space $\\mbox{Max}(P)$ of a domain $P$ (with\nthe relative Scott topology). We first construct an example to show that the\nset of maximal points of an ideal domain $P$ need not be a $G_{\\delta}$-set in\nthe Scott space $\\Sigma P$, thereby answering an open problem from Martin\n(2003). In addition, Bennett and Lutzer (2009) asked whether $X$ and $Y$ are\ndomain-representable if their product space $X \\times Y$ is\ndomain-representable. This problem was first solved by \\\"{O}nal and Vural\n(2015). In this paper, we provide a new approach to Bennett and Lutzer's\nproblem.",
      "generated_abstract": "aper we study the limit of the sequence of probability measures\n$\\{p_{\\varepsilon,n}\\}_{n \\in \\mathbb{N}}$ that converges weakly to the\nmeasure $\\mu$ such that $\\int_0^1 \\frac{1}{x} \\mathrm{d} \\mu(x) =\n\\int_0^1 \\frac{1}{x^2} \\mathrm{d} \\mu(x)$. In particular, we consider the case\n$\\mu(A) = \\int_0^1 \\frac{1}{x^2} \\mathrm{d} \\mu(x)$ for every Borel set $A$ and\nwe prove that the limit is the Lebesgue measure. In the second part we\nconsider the case $\\mu(A) = \\int_0^1 \\frac{1}{x^3} \\mathrm{d} \\mu(x)$ for every\nB",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11538461538461539,
          "p": 0.2,
          "f": 0.14634145877453913
        },
        "rouge-2": {
          "r": 0.018867924528301886,
          "p": 0.03278688524590164,
          "f": 0.023952091171430493
        },
        "rouge-l": {
          "r": 0.10256410256410256,
          "p": 0.17777777777777778,
          "f": 0.1300812961729131
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06755v1",
      "true_abstract": "In this paper, we study a transfer learning framework for Linear\n  Quadratic Regulator (LQR) control, where (i) the dynamics of the\n  system of interest (target system) are unknown and only a short\n  trajectory of impulse responses from the target system is provided,\n  and (ii) impulse responses are available from $N$ source systems\n  with different dynamics. We show that the LQR controller can be\n  learned from a sufficiently long trajectory of impulse\n  responses. Further, a transferable mode set can be identified using\n  the available data from source systems and the target system,\n  enabling the reconstruction of the target system's impulse responses\n  for controller design. By leveraging data from the source systems we\n  demonstrate that only n+1 (n being the system dimension) samples\n  of data from the target system are needed to learn the LQR\n  controller, this yields a significant reduction of the required\n  data.",
      "generated_abstract": "asing availability of massive amounts of data has led to the\ndevelopment of large language models (LLMs), which can process enormous\namounts of information. However, the computational power required for training\nLLMs has become a significant challenge. As a result, there is a growing\nrequirement for efficient and scalable methods for LLM training. In this paper,\nwe propose a novel framework that integrates a neural network with an\noptimization algorithm to address the problem of training LLMs. Our approach\nutilizes a sequence-to-sequence model to optimize the parameters of the\noptimization algorithm, which is trained to minimize the loss function. This\napproach allows for the efficient training of LLMs without the need for\nspecialized hardware. The proposed method is evaluated using the SuperGLUE\nbenchmark, demonstrating its efficiency and effectiveness in training LLMs. The\nresults show that the proposed method achieves superior",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24691358024691357,
          "p": 0.21505376344086022,
          "f": 0.22988505249504568
        },
        "rouge-2": {
          "r": 0.07377049180327869,
          "p": 0.06666666666666667,
          "f": 0.07003890551863046
        },
        "rouge-l": {
          "r": 0.2345679012345679,
          "p": 0.20430107526881722,
          "f": 0.2183907996214825
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/CB/2409.05333v1",
      "true_abstract": "We examine the difference in motion ordering between cellular systems with\nand without information transfer to evaluate the effect of the polar--polar\ninteraction through mutual guiding, which enables cells to inform other cells\nof their moving directions. We compare this interaction with the\npolar--nonpolar interaction through cell motion triggered by cellular contact,\nwhich cannot provide information on the moving directions. We model these\ninteractions on the basis of the cellular Potts model. We calculate the order\nparameter of the polar direction in the interactions and examine the cell\nconcentration and surface tension conditions of ordering. The results suggest\nthat the polar--polar interaction through mutual guiding efficiently induces\nthe motion ordering in comparison with the polar-nonpolar interaction for\ncontact triggering, except in cases of weak driving. The results also show that\nthe polar--polar interaction efficiently accelerates the collective motion\ncompared with the polar--nonpolar interaction.",
      "generated_abstract": "tudy, we examine the impact of long-term exposure to an\nenvironmentally contaminated field on the germination and growth of\nfungal seedlings. We utilized a 2-week field exposure to a mixture of\nbenzene, toluene, xylene, and formaldehyde (BTX) and a 2-day exposure to a\nsingle concentration of BTX, with both treatments applied to a field of\nFusarium verticillioides. We measured germination rates and growth rates of\nseeds germinating from seedlings grown in soil and seedlings grown in the\nfield. We found that fungal germination and growth rates were negatively\naffected by exposure to BTX, with germination rates and growth rates decreasing\nby 56% and 40%, respectively, compared to controls. Exposure to BTX for 2 weeks",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.21875,
          "f": 0.19858155532619096
        },
        "rouge-2": {
          "r": 0.02459016393442623,
          "p": 0.030927835051546393,
          "f": 0.027397255339130597
        },
        "rouge-l": {
          "r": 0.15584415584415584,
          "p": 0.1875,
          "f": 0.17021276099994984
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/OT/2406.11940v1",
      "true_abstract": "The stable unit treatment value assumption states that the outcome of an\nindividual is not affected by the treatment statuses of others, however in many\nreal world applications, treatments can have an effect on many others beyond\nthe immediately treated. Interference can generically be thought of as mediated\nthrough some network structure. In many empirically relevant situations\nhowever, complete network data (required to adjust for these spillover effects)\nare too costly or logistically infeasible to collect. Partially or indirectly\nobserved network data (e.g., subsamples, aggregated relational data (ARD),\negocentric sampling, or respondent-driven sampling) reduce the logistical and\nfinancial burden of collecting network data, but the statistical properties of\ntreatment effect adjustments from these design strategies are only beginning to\nbe explored. In this paper, we present a framework for the estimation and\ninference of treatment effect adjustments using partial network data through\nthe lens of structural causal models. We also illustrate procedures to assign\ntreatments using only partial network data, with the goal of either minimizing\nestimator variance or optimally seeding. We derive single network asymptotic\nresults applicable to a variety of choices for an underlying graph model. We\nvalidate our approach using simulated experiments on observed graphs with\napplications to information diffusion in India and Malawi.",
      "generated_abstract": "This paper provides a novel, simple and efficient method to identify\nthe most significant predictors of a binary response variable using only the\ninformation contained in the original data. We propose a method to estimate the\ncovariate importance of each variable in the data set, and we provide an\nalgorithm to obtain the set of variables that are most important. We demonstrate\nthe method's effectiveness in simulated data and in a real dataset. This\napproach can be used in many applications, including the identification of\nimportant covariates in a regression model, the selection of important\nfeatures in an input feature selection algorithm, and the identification of\nimportant features in a clustering algorithm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1527777777777778,
          "p": 0.3333333333333333,
          "f": 0.20952380521360556
        },
        "rouge-2": {
          "r": 0.009950248756218905,
          "p": 0.019801980198019802,
          "f": 0.013245028660805847
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.2727272727272727,
          "f": 0.17142856711836746
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/MN/2409.17488v1",
      "true_abstract": "Controlling the stochastic dynamics of biological populations is a challenge\nthat arises across various biological contexts. However, these dynamics are\ninherently nonlinear and involve a discrete state space, i.e., the number of\nmolecules, cells, or organisms. Additionally, the possibility of extinction has\na significant impact on both the dynamics and control strategies, particularly\nwhen the population size is small. These factors hamper the direct application\nof conventional control theories to biological systems. To address these\nchallenges, we formulate the optimal control problem for stochastic population\ndynamics by utilizing a control cost function based on the Kullback-Leibler\ndivergence. This approach naturally accounts for population-specific factors\nand simplifies the complex nonlinear Hamilton-Jacobi-Bellman equation into a\nlinear form, facilitating efficient computation of optimal solutions. We\ndemonstrate the effectiveness of our approach by applying it to the control of\ninteracting random walkers, Moran processes, and SIR models, and observe the\nmode-switching phenomena in the control strategies. Our approach provides new\nopportunities for applying control theory to a wide range of biological\nproblems.",
      "generated_abstract": "pt of \"permanent genes\" has been proposed as a way to better\nunderstand how genetic variation leads to phenotypic diversity. However,\nexisting methods often lack theoretical rigor and suffer from limited\ncomputational efficiency, making them unsuitable for large-scale analysis. In\nthis paper, we propose a novel approach that is based on the principle of\nMarkovianity and introduces a new term, \"permanence\", to account for the\npersistence of genes in the population. By incorporating the notion of\npersistence, we improve the computational efficiency of existing methods and\nenhance their ability to accommodate the complex interactions between genes\nand environments. Our proposed approach is demonstrated through two case studies\ninvolving Arabidopsis thaliana and Escherichia coli. Our results indicate that\nthe proposed method can effectively capture the influence of genetic variation\non phenotypic divers",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.20212765957446807,
          "f": 0.18269230273853562
        },
        "rouge-2": {
          "r": 0.029940119760479042,
          "p": 0.0390625,
          "f": 0.03389830017213516
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.19148936170212766,
          "f": 0.173076918123151
        }
      }
    },
    {
      "paper_id": "math.ST.stat/ME/2503.03567v1",
      "true_abstract": "We propose a new statistical hypothesis testing framework which decides\nvisually, using confidence intervals, whether the means of two samples are\nequal or if one is larger than the other. With our method, the user can at the\nsame time visualize the confidence region of the means and do a test to decide\nif the means of the two populations are significantly different or not by\nlooking whether the two confidence intervals overlap. To design this test we\nuse confidence intervals constructed using e-variables, which provide a measure\nof evidence in hypothesis testing. We propose both a sequential test and a\nnon-sequential test based on the overlap of confidence intervals and for each\nof these tests we give finite-time error bounds on the probabilities of error.\nWe also illustrate the practicality of our method by applying it to the\ncomparison of sequential learning algorithms.",
      "generated_abstract": "presents a novel approach to the analysis of data generated by a\nmixed-effects model, where the response variable is subject to both direct\nand indirect effects. This type of data arises in many contexts, including\nclimatic studies and studies of the influence of environmental factors on\nhealth. In the first part of the paper, we provide a brief overview of the\nmixed-effects model and its use in statistical analysis. In the second part, we\nintroduce a novel approach for the analysis of the mixed-effects model, which\nis based on the use of a novel approach for the analysis of the mixed-effects\nmodel, which is based on the use of a novel approach for the analysis of the\nmixed-effects model. The methodology of the analysis is based on the\nnon-parametric bootstrap. The methodology is illustrated through an example\nusing a data set from a climate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20930232558139536,
          "p": 0.26865671641791045,
          "f": 0.235294112724166
        },
        "rouge-2": {
          "r": 0.03787878787878788,
          "p": 0.05,
          "f": 0.043103443370987475
        },
        "rouge-l": {
          "r": 0.18604651162790697,
          "p": 0.23880597014925373,
          "f": 0.2091503218744928
        }
      }
    },
    {
      "paper_id": "cs.CL.cs/IR/2503.08398v1",
      "true_abstract": "In this paper, we analyze and empirically show that the learned relevance for\nconventional information retrieval (IR) scenarios may be inconsistent in\nretrieval-augmented generation (RAG) scenarios. To bridge this gap, we\nintroduce OpenRAG, a RAG framework that is optimized end-to-end by tuning the\nretriever to capture in-context relevance, enabling adaptation to the diverse\nand evolving needs. Extensive experiments across a wide range of tasks\ndemonstrate that OpenRAG, by tuning a retriever end-to-end, leads to a\nconsistent improvement of 4.0% over the original retriever, consistently\noutperforming existing state-of-the-art retrievers by 2.1%. Additionally, our\nresults indicate that for some tasks, an end-to-end tuned 0.2B retriever can\nachieve improvements that surpass those of RAG-oriented or instruction-tuned 8B\nlarge language models (LLMs), highlighting the cost-effectiveness of our\napproach in enhancing RAG systems.",
      "generated_abstract": "development of large language models (LLMs) has transformed the\nprocess of text generation from a human-driven task to a self-supervised\nlearning process. However, the current research on self-supervised learning\n(SSL) of LLMs is limited by the scarcity of large-scale, publicly available\ndatasets. This paper proposes the first publicly available dataset for the\nanalysis of the performance of LLMs on text generation tasks. The dataset\ncontains 414,710 training and 414,710 test examples, each with a length of 10,000\nwords. The dataset was collected through a large-scale crawling of the Web,\nusing the Google Web Search API. The dataset was collected over a 10-year\nperiod from 2016 to 2026, and the training set covers 2016",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14,
          "p": 0.19444444444444445,
          "f": 0.16279069280692282
        },
        "rouge-2": {
          "r": 0.023255813953488372,
          "p": 0.02912621359223301,
          "f": 0.025862064028315448
        },
        "rouge-l": {
          "r": 0.13,
          "p": 0.18055555555555555,
          "f": 0.15116278583017864
        }
      }
    },
    {
      "paper_id": "cs.OS.cs/OS/2502.07118v1",
      "true_abstract": "File systems play an essential role in modern society for managing precious\ndata. To meet diverse needs, they often support many configuration parameters.\nSuch flexibility comes at the price of additional complexity which can lead to\nsubtle configuration-related issues. To address this challenge, we study the\nconfiguration-related issues of two major file systems (i.e., Ext4 and XFS) in\ndepth, and identify a prevalent pattern called multilevel configuration\ndependencies. Based on the study, we build an extensible tool called ConfD to\nextract the dependencies automatically, and create a set of plugins to address\ndifferent configuration-related issues. Our experiments on Ext4, XFS and a\nmodern copy-on-write file system (i.e., ZFS) show that ConfD was able to\nextract 160 configuration dependencies for the file systems with a low false\npositive rate. Moreover, the dependency-guided plugins can identify various\nconfiguration issues (e.g., mishandling of configurations, regression test\nfailures induced by valid configurations). In addition, we also explore the\napplicability of ConfD on a popular storage engine (i.e., WiredTiger). We hope\nthat this comprehensive analysis of configuration dependencies of storage\nsystems can shed light on addressing configuration-related challenges for the\nsystem community in general.",
      "generated_abstract": "The Linux kernel is a complex system with many different layers and\ncontainers. This paper proposes a framework for system-level analysis of the\nLinux kernel using the Open Source System Analysis (OSSA) framework.\nSpecifically, we focus on the Linux Kernel Module (LKM) layer and its\nsub-layers, and the various functions within the LKM layer, such as the\nkernel-space and user-space interface. We introduce an experimental\nimplementation of the OSSA framework and demonstrate its capability by\nanalyzing the Linux Kernel Module for the purpose of system-level analysis.\nThis paper also introduces a novel approach for analyzing the kernel-space\nand user-space interfaces and provides a comprehensive evaluation of the\nframework using real-world kernel source code.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14516129032258066,
          "p": 0.2571428571428571,
          "f": 0.18556700569667356
        },
        "rouge-2": {
          "r": 0.016216216216216217,
          "p": 0.030303030303030304,
          "f": 0.021126756021871636
        },
        "rouge-l": {
          "r": 0.13709677419354838,
          "p": 0.24285714285714285,
          "f": 0.17525772734615805
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.physics/chem-ph/2503.09855v1",
      "true_abstract": "Spatially varying electric fields are prevalent throughout nature and\ntechnology, arising from heterogeneity inherent to all physical systems.\nInhomogeneous electric fields can originate naturally, such as in nanoporous\nmaterials and biological membranes, or be engineered, e.g., with patterned\nelectrodes or layered van der Waals heterostructures. While uniform fields\ncause free ions to migrate, for polar fluids they simply act to reorient the\nconstituent molecules. In contrast, electric field gradients (EFGs) induce a\ndielectrophoretic force, offering exquisite electrokinetic control of a fluid,\neven in the absence of free charge carriers. EFGs, therefore, offer vast\npotential for optimizing fluid behavior under confinement, such as in\nnanoporous electrodes, nanofluidic devices, and chemical separation materials.\nYet, EFGs remain largely unexplored at the microscopic level owing to the\nabsence of a rigorous, first principles theoretical treatment of\nelectrostrictive effects. By integrating state-of-the-art advances in liquid\nstate theory and deep learning, we reveal how EFGs modulate fluid structure and\ncapillary phenomena. We demonstrate, from first principles, that\ndielectrophoretic coupling enables tunable control over the liquid-gas phase\ntransition, capillary condensation, and fluid uptake into porous media. Our\nfindings establish \"dielectrocapillarity'' -- the use of EFGs to control\nconfined fluids -- as a powerful tool for controlling volumetric capacity in\nnanopores, which holds immense potential for optimizing energy storage in\nsupercapacitors, selective gas separation, and tunable hysteresis in\nneuromorphic nanofluidic devices.",
      "generated_abstract": "y presents a first-principles theoretical study of the magnetic\nproperties of nanoporous carbon nitride (CNx) at finite temperatures and\nmagnetic fields. We analyze the magnetic ordering temperature and magnetic\nhysteresis in the presence of a magnetic field, focusing on the effect of\ndifferent carbonyl and nitrogen bonding sites. We find that the presence of a\nmagnetic field significantly modifies the magnetic behavior of the system,\nresulting in a magnetic ordering temperature of approximately 160 K, which\nis close to the Curie temperature of the parent compound. The magnetic\nhysteresis loops exhibit a non-monotonic behavior, with a prominent hysteresis\nloop at lower temperatures, suggesting the presence of structural order. Our\nresults provide insight into the magnetic properties of nanoporous carbon\nnitride and highlight the importance of considering the effects of magnetic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12048192771084337,
          "p": 0.26666666666666666,
          "f": 0.16597509944732367
        },
        "rouge-2": {
          "r": 0.018604651162790697,
          "p": 0.037383177570093455,
          "f": 0.0248447160593735
        },
        "rouge-l": {
          "r": 0.10240963855421686,
          "p": 0.22666666666666666,
          "f": 0.14107883388715772
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/RM/2502.11706v1",
      "true_abstract": "A deep BSDE approach is presented for the pricing and delta-gamma hedging of\nhigh-dimensional Bermudan options, with applications in portfolio risk\nmanagement. Large portfolios of a mixture of multi-asset European and Bermudan\nderivatives are cast into the framework of discretely reflected BSDEs. This\nsystem is discretized by the One Step Malliavin scheme (Negyesi et al. [2024,\n2025]) of discretely reflected Markovian BSDEs, which involves a $\\Gamma$\nprocess, corresponding to second-order sensitivities of the associated option\nprices. The discretized system is solved by a neural network regression Monte\nCarlo method, efficiently for a large number of underlyings. The resulting\noption Deltas and Gammas are used to discretely rebalance the corresponding\nreplicating strategies. Numerical experiments are presented on both\nhigh-dimensional basket options and large portfolios consisting of multiple\noptions with varying early exercise rights, moneyness and volatility. These\nexamples demonstrate the robustness and accuracy of the method up to $100$ risk\nfactors. The resulting hedging strategies significantly outperform benchmark\nmethods both in the case of standard delta- and delta-gamma hedging.",
      "generated_abstract": "We investigate the application of deep learning in the analysis of stock\nmarket\niments, focusing on their predictive ability. We employ a deep\nconvolutional neural network (CNN) to analyze the daily closing prices of\nseveral stocks. The results show that the CNN can effectively detect market\ntrends and predict future stock prices, with a mean absolute error of 4.61% and\na root mean squared error of 10.23%. These results suggest that CNNs can\neffectively predict the future performance of stocks, providing valuable\ninsights for investors and traders.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13513513513513514,
          "p": 0.234375,
          "f": 0.17142856678922463
        },
        "rouge-2": {
          "r": 0.0125,
          "p": 0.023255813953488372,
          "f": 0.016260158054069617
        },
        "rouge-l": {
          "r": 0.11711711711711711,
          "p": 0.203125,
          "f": 0.14857142393208178
        }
      }
    },
    {
      "paper_id": "physics.optics.nlin/SI/2503.08513v1",
      "true_abstract": "We report the results of experimental studies of recurrent spectral dynamics\nof the two component Akhmediev breathers (ABs) in a single mode optical fibre.\nWe also provide the theoretical analysis and numerical simulations of the ABs\nbased on the two component Manakov equations that confirm the experimental\ndata. In particular, we observed spectral asymmetry of fundamental ABs and\ncomplex spectral evolution of second-order nondegenerate ABs.",
      "generated_abstract": "We introduce a general framework to study the phase and group coherence\nof a nonlinear system, focusing on the case of nonlinear dispersive optics. We\ndiscuss the relevance of the phase coherence, its relation to the group\ncoherence, and we show that both quantities are related by the complex\ncharacteristic function of the phase space. This characterization provides\na unifying framework to analyze various nonlinear phenomena such as optical\nphase modulation, coherent control, and supercontinuum generation. We\nillustrate the applicability of our framework by considering the nonlinear\nmicrocavity and discussing its implications for the optical microcavity\nphotodetectors.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21739130434782608,
          "p": 0.16393442622950818,
          "f": 0.18691588294872927
        },
        "rouge-2": {
          "r": 0.03278688524590164,
          "p": 0.021739130434782608,
          "f": 0.026143786054937015
        },
        "rouge-l": {
          "r": 0.1956521739130435,
          "p": 0.14754098360655737,
          "f": 0.16822429416368254
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2402.15828v4",
      "true_abstract": "Geometric Asian options are a type of options where the payoff depends on the\ngeometric mean of the underlying asset over a certain period of time. This\npaper is concerned with the pricing of such options for the class of\nVolterra-Heston models, covering the rough Heston model. We are able to derive\nsemi-closed formulas for the prices of geometric Asian options with fixed and\nfloating strikes for this class of stochastic volatility models. These formulas\nrequire the explicit calculation of the conditional joint Fourier transform of\nthe logarithm of the stock price and the logarithm of the geometric mean of the\nstock price over time. Linking our problem to the theory of affine Volterra\nprocesses, we find a representation of this Fourier transform as a suitably\nconstructed stochastic exponential, which depends on the solution of a\nRiccati-Volterra equation. Finally we provide a numerical study for our results\nin the rough Heston model.",
      "generated_abstract": "In this paper, we propose a novel approach for pricing credit default\noversampling (CDO) swaps. Using a large dataset of credit default swaps, we\nestimate the credit spread and the default probability of the swap. We then\ngenerate a large number of synthetic swaps using these parameters. We then use\nthe synthetic swaps to price the original swaps. The CDO swaps are priced by\ncomparing the synthetic swaps with the reference swaps. We find that the\nreference swaps are a good proxy for the credit spread. We also find that\ndefault probability is an important factor in pricing CDO swaps. This study\nillustrates the importance of using synthetic swaps to price CDO swaps and\nprovides a framework for further research.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21176470588235294,
          "p": 0.28125,
          "f": 0.24161073335435349
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.04950495049504951,
          "f": 0.04329003836884673
        },
        "rouge-l": {
          "r": 0.21176470588235294,
          "p": 0.28125,
          "f": 0.24161073335435349
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2409.10938v2",
      "true_abstract": "Modern macroeconomic models, particularly those grounded in Rational\nExpectation Dynamic Stochastic General Equilibrium (DSGE), operate under the\nassumption of fully rational decision-making. This paper examines the impact of\nbehavioral factors on the communication index/sentiment index of the US Federal\nReserve. [Upon receiving the review comments, I found some technical errors in\nthe paper. I shall update it accordingly. Please do not cite this paper without\nauthor's permission.]",
      "generated_abstract": "We show that a class of (weak) monotonicity conditions on the payoffs of\na set of agents with two agents, can be used to characterize the equilibrium\nsolution in a game with a single agent. We prove that in such a game the\nsolution can be written as a limit of a sequence of equilibrium solutions\nobtained by replacing the agents with copies of themselves and replacing the\npayoffs with the payoffs of their copies. We then use this characterization to\nshow that the class of monotonicity conditions has no natural generalization\nin the setting of games with more than two agents.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08771929824561403,
          "p": 0.09259259259259259,
          "f": 0.09009008509374267
        },
        "rouge-2": {
          "r": 0.029850746268656716,
          "p": 0.022222222222222223,
          "f": 0.02547770211367695
        },
        "rouge-l": {
          "r": 0.08771929824561403,
          "p": 0.09259259259259259,
          "f": 0.09009008509374267
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.03012v1",
      "true_abstract": "We develop a framework to holistically test for and monitor the impact of\ndifferent types of events affecting a country's housing market, yet originating\nfrom housing-external sources. We classify events along three dimensions\nleading to testable hypotheses: prices versus quantities, supply versus demand,\nand immediate versus gradually evolving. These dimensions translate into\nguidance about which data type, statistical measure and testing strategy should\nbe used. To perform such test suitable statistical models are needed which we\nimplement as a hierarchical hedonic price model and a complementary count\nmodel. These models are amended by regime and contextual variables as suggested\nby our classification strategy. We apply this framework to the Austrian real\nestate market together with three disruptive events triggered by the COVID-19\npandemic, a policy tightening mortgage lending standards, as well as the\ncost-of-living crisis that came along with increased financing costs. The tests\nyield the expected results and, by that, some housing market puzzles are\nresolved. Deviating from the prior classification exercise means that some\ndevelopments would have been undetected. Further, adopting our framework\nconsistently when performing empirical research on residential real estate\nwould lead to better comparable research results and, by that, would allow\nresearchers to draw meta-conclusions from the bulk of studies available across\ntime and space.",
      "generated_abstract": "e the effects of the adoption of a digital currency on\ncurrency exchange rates and the foreign exchange market. Our analysis\nfocuses on the effects of the adoption of digital currencies on foreign\nexchange market efficiency and on the foreign exchange market. Our empirical\nanalysis is based on a panel of 160 countries, covering the period from 2000\nto 2021, with a focus on the period from 2010 to 2021. Our findings reveal\nthat the adoption of digital currencies has increased the efficiency of the\nforeign exchange market and reduced the transaction costs and the market\ninstability. The adoption of digital currencies has also enhanced the\nefficiency of the foreign exchange market and reduced transaction costs and\nmarket instability. Our findings also suggest that the adoption of digital\ncurrencies has increased the efficiency of the foreign exchange market and\nreduced",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09655172413793103,
          "p": 0.2916666666666667,
          "f": 0.1450777164702409
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0896551724137931,
          "p": 0.2708333333333333,
          "f": 0.13471502216972278
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/CO/2501.11743v1",
      "true_abstract": "We consider the constrained sampling problem where the goal is to sample from\na target distribution on a constrained domain. We propose skew-reflected\nnon-reversible Langevin dynamics (SRNLD), a continuous-time stochastic\ndifferential equation with skew-reflected boundary. We obtain non-asymptotic\nconvergence rate of SRNLD to the target distribution in both total variation\nand 1-Wasserstein distances. By breaking reversibility, we show that the\nconvergence is faster than the special case of the reversible dynamics. Based\non the discretization of SRNLD, we propose skew-reflected non-reversible\nLangevin Monte Carlo (SRNLMC), and obtain non-asymptotic discretization error\nfrom SRNLD, and convergence guarantees to the target distribution in\n1-Wasserstein distance. We show better performance guarantees than the\nprojected Langevin Monte Carlo in the literature that is based on the\nreversible dynamics. Numerical experiments are provided for both synthetic and\nreal datasets to show efficiency of the proposed algorithms.",
      "generated_abstract": "This paper presents a novel approach for estimating the mean and variance of\nthe logarithm of the number of trials in a binomial experiment. By leveraging\nthe cumulative distribution function of the binomial distribution, we derive\nanalytical expressions for the mean and variance of the logarithm of the number\nof trials. Our approach provides an accurate and computationally efficient\nsolution for large sample sizes and also applies to binary and multinomial\nexperiments. We illustrate the efficiency of our method through simulation\nstudies and two real-world applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1794871794871795,
          "p": 0.25,
          "f": 0.208955219015371
        },
        "rouge-2": {
          "r": 0.016260162601626018,
          "p": 0.027777777777777776,
          "f": 0.020512815854833406
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.23214285714285715,
          "f": 0.19402984588104266
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.07889v1",
      "true_abstract": "We present a simple method to enable processing of Spotlight Synthetic\nAperture Radar (SAR) imagery distributed in Polar Format (PFA) using standard\nRange-Doppler (RDA) geometry algorithms. Our approach is applicable to PFA SAR\nimages characterized by a constant value of the Center of Aperture (COA) time.\nWe present simplified expressions for forward (image-to-ground) and inverse\n(ground-to-image) geometry mapping using Sensor Independent Complex Data (SICD)\nconventions. We discuss simple changes needed to current open source SAR\nsoftware that implement Range-Doppler algorithms, to enable support within them\nfor Spotlight data distributed in SICD format. We include a proof-of-concept\nscript that utilizes the Python packages sarpy and isce3 to demonstrate the\ncorrectness of the proposed approach.",
      "generated_abstract": "This paper presents a novel method for 3D reconstruction of the head from\nthree images of a single individual, without head-mounted sensors or\nphotometric stereo. The method is based on the observation that the 3D\nposition of the head can be estimated by fitting a parametric surface to the\nthree images, and that the three images can be used to estimate the camera\nparameters. We present a novel approach for computing the surface parameters,\nwhich requires only a single image of the head, and does not require\nphotometric stereo or a head-mounted sensor. We demonstrate the effectiveness\nof our method on simulated and real datasets, and compare our approach to\ntraditional approaches. The results indicate that our method is capable of\nreconstructing the head in 3D without any additional sensor, and is more\nrobust than traditional methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1927710843373494,
          "p": 0.20253164556962025,
          "f": 0.1975308592005793
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.032520325203252036,
          "f": 0.03463202965311816
        },
        "rouge-l": {
          "r": 0.18072289156626506,
          "p": 0.189873417721519,
          "f": 0.18518518018823363
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2411.06076v1",
      "true_abstract": "This paper introduces BreakGPT, a novel large language model (LLM)\narchitecture adapted specifically for time series forecasting and the\nprediction of sharp upward movements in asset prices. By leveraging both the\ncapabilities of LLMs and Transformer-based models, this study evaluates\nBreakGPT and other Transformer-based models for their ability to address the\nunique challenges posed by highly volatile financial markets. The primary\ncontribution of this work lies in demonstrating the effectiveness of combining\ntime series representation learning with LLM prediction frameworks. We showcase\nBreakGPT as a promising solution for financial forecasting with minimal\ntraining and as a strong competitor for capturing both local and global\ntemporal dependencies.",
      "generated_abstract": "t a novel, general approach for analyzing the risk profile of\nopportunistic contracts, using a stochastic modeling framework. Our approach\nincludes both the pricing of contracts and the identification of their\nrisk-averse and risk-seeking aspects. We first develop a model for the\nunderlying financial market, and then use it to derive a stochastic model for\nthe risk-averse and risk-seeking behaviors of the contract holders. Using this\nframework, we analyze the risk profiles of various contracts, including\ncontracts in the commodities market, and we show that our model can capture\nimportant features of these contracts. We also examine the implications of our\napproach for the design of contracts in the commodities market. Our results\ndemonstrate that a contract that satisfies the risk-averse behavioral\nprinciples of the model can reduce the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16455696202531644,
          "p": 0.18055555555555555,
          "f": 0.17218542547432142
        },
        "rouge-2": {
          "r": 0.019417475728155338,
          "p": 0.017857142857142856,
          "f": 0.018604646171553527
        },
        "rouge-l": {
          "r": 0.1518987341772152,
          "p": 0.16666666666666666,
          "f": 0.15894039236173868
        }
      }
    },
    {
      "paper_id": "physics.plasm-ph.physics/plasm-ph/2503.10067v1",
      "true_abstract": "The significance of laser-driven polarized beam acceleration has been\nincreasingly recognized in recent years. We propose an efficient method for\ngenerating polarized proton beams from a pre-polarized hydrogen halide gas jet,\nutilizing magnetic vortex acceleration enhanced by a laser-driven plasma\nbubble. When a petawatt laser pulse passes through a pre-polarized gas jet, a\nbubble-like ultra-nonlinear plasma wave is formed. As part of the wave\nparticles, background protons are swept by the acceleration field of the bubble\nand oscillate significantly along the laser propagation axis. Some of the\npre-accelerated protons in the plasma wave are trapped by the acceleration\nfield at the rear side of the target. This acceleration field is intensified by\nthe transverse expansion of the laser-driven magnetic vortex, resulting in\nenergetic polarized proton beams. The spin of energetic protons is determined\nby their precession within the electromagnetic field, as described by the\nThomas-Bargmann-Michel-Telegdi equation in analytical models and\nparticle-in-cell simulations. Multidimensional simulations reveal that\nmonoenergetic proton beams with hundreds of MeV in energy, a beam charge of\nhundreds of pC, and a beam polarization of tens of percent can be produced at\nlaser powers of several petawatts. Laser-driven polarized proton beams offer\npromising potential for application in polarized beam colliders, where they can\nbe utilized to investigate particle interactions and to explore the properties\nof matter under unique conditions.",
      "generated_abstract": "the interaction of a single-component, weakly ionized plasma with\neither a single-component plasma or an ionized gas in the presence of\nelectrostatic fields. We numerically solve the coupled set of equations of\nmotion for the plasma and ion, including the Poisson equation, and the\nelectrostatic field. Our approach relies on the use of the numerical solution\nof the Poisson equation as a test function to construct a regularized potential\nthat is used to compute the electric field. We show that the interaction\nparameters are governed by the ratio between the ion and plasma mass\nconcentrations. We also show that the interaction parameters can be determined\nby solving the Poisson equation for the unbounded case, which allows us to\nidentify the conditions under which the interaction is stable or unstable,\nrespectively. We also show that the interaction parameters can be determined\nby solving the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19230769230769232,
          "p": 0.32051282051282054,
          "f": 0.24038461069711545
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.05042016806722689,
          "f": 0.037854884900437445
        },
        "rouge-l": {
          "r": 0.13846153846153847,
          "p": 0.23076923076923078,
          "f": 0.1730769183894232
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2502.05839v1",
      "true_abstract": "In this paper, we examine a modified version of de Finetti's optimal dividend\nproblem, incorporating fixed transaction costs and altering the surplus process\nby introducing two-valued drift and two-valued volatility coefficients. This\nmodification aims to capture the transitions or adjustments in the company's\nfinancial status. We identify the optimal dividend strategy, which maximizes\nthe expected total net dividend payments (after accounting for transaction\ncosts) until ruin, as a two-barrier impulsive dividend strategy. Notably, the\noptimal strategy can be explicitly determined for almost all scenarios\ninvolving different drifts and volatility coefficients. Our primary focus is on\nexploring how changes in drift and volatility coefficients influence the\noptimal dividend strategy.",
      "generated_abstract": "er the problem of modeling the distribution of the number of\nactions taken by a single buyer in a competitive auction. We focus on the\napplication of a class of Markov Decision Processes (MDP) with the discrete state\nspace, where the buyer selects the number of actions to be taken in each round.\nWe propose a reinforcement learning (RL) algorithm that selects the number of\nactions to be taken, given the current state of the market and the history of\nprevious actions. The algorithm is based on a value iteration algorithm,\nassuming that the expected number of actions taken by the buyer at a given\npoint in time is known. We provide theoretical guarantees on the\nperformance of our algorithm, and show that the algorithm performs better than\nthe benchmark of the expected number of actions taken. We also propose a\ndynamical version of the algorithm, where the state space is continuously\ndiscretized",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.175,
          "p": 0.18421052631578946,
          "f": 0.17948717449046694
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.015873015873015872,
          "f": 0.01785713793526921
        },
        "rouge-l": {
          "r": 0.1625,
          "p": 0.17105263157894737,
          "f": 0.16666666166995414
        }
      }
    },
    {
      "paper_id": "math.ST.math/ST/2503.06889v1",
      "true_abstract": "Community detection, which focuses on recovering the group structure within\nnetworks, is a crucial and fundamental task in network analysis. However, the\ndetection process can be quite challenging and unstable when community signals\nare weak. Motivated by a newly collected large-scale academic network dataset\nfrom the Web of Science, which includes multi-layer network information, we\npropose a Bipartite Assisted Spectral-clustering approach for Identifying\nCommunities (BASIC), which incorporates the bipartite network information into\nthe community structure learning of the primary network. The accuracy and\nstability enhancement of BASIC is validated theoretically on the basis of the\ndegree-corrected stochastic block model framework, as well as numerically\nthrough extensive simulation studies. We rigorously study the convergence rate\nof BASIC even under weak signal scenarios and prove that BASIC yields a tighter\nupper error bound than that based on the primary network information alone. We\nutilize the proposed BASIC method to analyze the newly collected large-scale\nacademic network dataset from statistical papers. During the author\ncollaboration network structure learning, we incorporate the bipartite network\ninformation from author-paper, author-institution, and author-region\nrelationships. From both statistical and interpretative perspectives, these\nbipartite networks greatly aid in identifying communities within the primary\ncollaboration network.",
      "generated_abstract": "We present a novel approach to the study of the entropy production of\nnon-equilibrium systems with a specific form of dissipation. This approach\nconsists in the identification of the entropy production as the rate of\nstochastic processes which can be identified with the entropy production. We\ndiscuss the consequences of this identification for the description of the\nentropy production in stochastic processes and we prove that in the specific\ncase of the Markovian stochastic processes, the entropy production is the\nexpectation value of the corresponding stochastic differential equation. This\nresult has an important consequence for the estimation of the entropy\nproduction of stochastic processes. We also discuss the connection between the\nentropy production and the entropy production rate of a Markovian system.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15625,
          "p": 0.3508771929824561,
          "f": 0.21621621195266624
        },
        "rouge-2": {
          "r": 0.02247191011235955,
          "p": 0.0425531914893617,
          "f": 0.02941176018274291
        },
        "rouge-l": {
          "r": 0.1484375,
          "p": 0.3333333333333333,
          "f": 0.20540540114185546
        }
      }
    },
    {
      "paper_id": "math.OC.q-fin/MF/2501.17577v1",
      "true_abstract": "We study a class of singular stochastic control problems for a\none-dimensional diffusion $X$ in which the performance criterion to be\noptimised depends explicitly on the running infimum $I$ (or supremum $S$) of\nthe controlled process. We introduce two novel integral operators that are\nconsistent with the Hamilton-Jacobi-Bellman equation for the resulting\ntwo-dimensional singular control problems. The first operator involves\nintegrals where the integrator is the control process of the two-dimensional\nprocess $(X,I)$ or $(X,S)$; the second operator concerns integrals where the\nintegrator is the running infimum or supremum process itself. Using these\ndefinitions, we prove a general verification theorem for problems involving\ntwo-dimensional state-dependent running costs, costs of controlling the\nprocess, costs of increasing the running infimum (or supremum) and exit times.\nFinally, we apply our results to explicitly solve an optimal dividend problem\nin which the manager's time-preferences depend on the company's historical\nworst performance.",
      "generated_abstract": "We introduce a general framework for the analysis of financial market\nparameters and model the evolution of financial variables using stochastic\nprocesses. This approach provides a rigorous mathematical basis for the\nanalysis of the market dynamics and allows to characterize the market in a\nmore comprehensive way. We develop a model for the evolution of the\nmarginal distribution of asset prices and show that it can be described using\nstochastic processes. We also introduce a model for the evolution of the\nmarginal distribution of asset returns and demonstrate that it can be\ndescribed using a stochastic volatility model. Our analysis allows us to\ncharacterize the market dynamics in a more general way than before. We show\nthat the market dynamics are determined by two parameters: the market depth\nparameter, which characterizes the volume of financial transactions, and the\nrisk aversion parameter, which characterizes the volatility of financial\nvolumes.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17204301075268819,
          "p": 0.24242424242424243,
          "f": 0.2012578567793997
        },
        "rouge-2": {
          "r": 0.030534351145038167,
          "p": 0.0380952380952381,
          "f": 0.033898300145433075
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.18181818181818182,
          "f": 0.15094339137059468
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.10791v1",
      "true_abstract": "We investigate methods for forecasting multivariate realized covariances\nmatrices applied to a set of 30 assets that were included in the DJ30 index at\nsome point, including two novel methods that use existing (univariate) log of\nrealized variance models that account for attenuation bias and time-varying\nparameters. We consider the implications of some modeling choices within the\nclass of heterogeneous autoregressive models. The following are our key\nfindings. First, modeling the logs of the marginal volatilities is strongly\npreferred over direct modeling of marginal volatility. Thus, our proposed model\nthat accounts for attenuation bias (for the log-response) provides superior\none-step-ahead forecasts over existing multivariate realized covariance\napproaches. Second, accounting for measurement errors in marginal realized\nvariances generally improves multivariate forecasting performance, but to a\nlesser degree than previously found in the literature. Third, time-varying\nparameter models based on state-space models perform almost equally well.\nFourth, statistical and economic criteria for comparing the forecasting\nperformance lead to some differences in the models' rankings, which can\npartially be explained by the turbulent post-pandemic data in our out-of-sample\nvalidation dataset using sub-sample analyses.",
      "generated_abstract": "igate the use of multiple imputation in the context of a\nmodel for the price of a single asset, where the data are incomplete across\nmultiple periods. We compare the performance of two imputation methods,\nmultiple imputation with conditional expectation (MICE) and multiple imputation\nwith partial data (MIPD), in terms of predictive accuracy, consistency, and\neffectiveness in estimating the parameter of interest. We find that MICE is\nmore efficient in terms of predictive accuracy and consistency, while MIPD\nprovides a better estimate of the parameter of interest in terms of\nregression-based forecasting accuracy. We also find that MIPD performs better\nin terms of estimating the parameter of interest and producing a more\nconsistent forecast across different periods. We provide some suggestions for\nusing multiple imputation in the context of modeling the price of a single asset.\nWe also discuss the potential pitfalls and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.30434782608695654,
          "f": 0.21538461081183444
        },
        "rouge-2": {
          "r": 0.017341040462427744,
          "p": 0.026785714285714284,
          "f": 0.021052626808003544
        },
        "rouge-l": {
          "r": 0.1349206349206349,
          "p": 0.2463768115942029,
          "f": 0.1743589697861934
        }
      }
    },
    {
      "paper_id": "cs.LG.math/ST/2503.07453v1",
      "true_abstract": "Language model alignment (or, reinforcement learning) techniques that\nleverage active exploration -- deliberately encouraging the model to produce\ndiverse, informative responses -- offer the promise of super-human\ncapabilities. However, current understanding of algorithm design primitives for\ncomputationally efficient exploration with language models is limited. To\nbetter understand how to leverage access to powerful pre-trained generative\nmodels to improve the efficiency of exploration, we introduce a new\ncomputational framework for RL with language models, in which the learner\ninteracts with the model through a sampling oracle. Focusing on the linear\nsoftmax model parameterization, we provide new results that reveal the\ncomputational-statistical tradeoffs of efficient exploration:\n  1. Necessity of coverage: Coverage refers to the extent to which the\npre-trained model covers near-optimal responses -- a form of hidden knowledge.\nWe show that coverage, while not necessary for data efficiency, lower bounds\nthe runtime of any algorithm in our framework.\n  2. Inference-time exploration: We introduce a new algorithm, SpannerSampling,\nwhich obtains optimal data efficiency and is computationally efficient whenever\nthe pre-trained model enjoys sufficient coverage, matching our lower bound.\nSpannerSampling leverages inference-time computation with the pre-trained model\nto reduce the effective search space for exploration.\n  3. Insufficiency of training-time interventions: We contrast the result above\nby showing that training-time interventions that produce proper policies cannot\nachieve similar guarantees in polynomial time.\n  4. Computational benefits of multi-turn exploration: Finally, we show that\nunder additional representational assumptions, one can achieve improved runtime\n(replacing sequence-level coverage with token-level coverage) through\nmulti-turn exploration.",
      "generated_abstract": "aper, we present a new class of non-uniform lattice codes based on\nthe generalized Latin hypercube design (GLHD) and its randomized version (RGLHD).\nThe construction of the GLHD is based on the well-known Latin hypercube\nmeasurement, which is applied to the set of all possible subsets of a given\nfinite set. In our proposed GLHD, we allow the set of all possible subsets to\nbe constructed using the RAND and RAND-2 measures. The RGLHD is based on the\nRAND measure and it is randomized by the RAND-2 measure. The RGLHD is\nuniformly distributed over the set of all possible RAND measures. In this\npaper, we present an explicit construction of the RGLHD. We use the RGLHD to\nconstruct a class of non-uniform lattice codes with a number of codewords\nlinearly dependent on the dimension of the code",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0967741935483871,
          "p": 0.21428571428571427,
          "f": 0.1333333290469137
        },
        "rouge-2": {
          "r": 0.012987012987012988,
          "p": 0.027777777777777776,
          "f": 0.017699110702483638
        },
        "rouge-l": {
          "r": 0.0967741935483871,
          "p": 0.21428571428571427,
          "f": 0.1333333290469137
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.15090v1",
      "true_abstract": "The R package bsvars provides a wide range of tools for empirical\nmacroeconomic and financial analyses using Bayesian Structural Vector\nAutoregressions. It uses frontier econometric techniques and C++ code to ensure\nfast and efficient estimation of these multivariate dynamic structural models,\npossibly with many variables, complex identification strategies, and non-linear\ncharacteristics. The models can be identified using adjustable exclusion\nrestrictions and heteroskedastic or non-normal shocks. They feature a flexible\nthree-level equation-specific local-global hierarchical prior distribution for\nthe estimated level of shrinkage for autoregressive and structural parameters.\nAdditionally, the package facilitates predictive and structural analyses such\nas impulse responses, forecast error variance and historical decompositions,\nforecasting, statistical verification of identification and hypotheses on\nautoregressive parameters, and analyses of structural shocks, volatilities, and\nfitted values. These features differentiate bsvars from existing R packages\nthat either focus on a specific structural model, do not consider\nheteroskedastic shocks, or lack the implementation using compiled code.",
      "generated_abstract": "This paper extends the analysis of a model in which individuals' preferences\nfor consumption, investment, and savings are based on their subjective\nexpectations of future outcomes, where the model is specified as a system of\nnonlinear equations with high-dimensional parameters. The model is motivated by\nthe study of consumption-savings-investment behavior, and it is shown that the\nmodel can be well approximated by a model in which the parameters are\nconstrained to be linear functions of past consumption, investment, and\nsavings. The model is solved by applying the Newton-Raphson method to the\nequations of the system of nonlinear equations. The results indicate that\ninvestment is more sensitive to changes in consumption and savings than is\nsavings, and the model is shown to be very flexible.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13392857142857142,
          "p": 0.22058823529411764,
          "f": 0.16666666196543226
        },
        "rouge-2": {
          "r": 0.006711409395973154,
          "p": 0.009523809523809525,
          "f": 0.007874010898074782
        },
        "rouge-l": {
          "r": 0.11607142857142858,
          "p": 0.19117647058823528,
          "f": 0.14444443974321003
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2502.13431v1",
      "true_abstract": "This study proposes a novel functional vector autoregressive framework for\nanalyzing network interactions of functional outcomes in panel data settings.\nIn this framework, an individual's outcome function is influenced by the\noutcomes of others through a simultaneous equation system. To estimate the\nfunctional parameters of interest, we need to address the endogeneity issue\narising from these simultaneous interactions among outcome functions. This\nissue is carefully handled by developing a novel functional moment-based\nestimator. We establish the consistency, convergence rate, and pointwise\nasymptotic normality of the proposed estimator. Additionally, we discuss the\nestimation of marginal effects and impulse response analysis. As an empirical\nillustration, we analyze the demand for a bike-sharing service in the U.S. The\nresults reveal statistically significant spatial interactions in bike\navailability across stations, with interaction patterns varying over the time\nof day.",
      "generated_abstract": "e a novel framework for modeling and estimating nonparametric\ndistributions with heteroskedasticity, which is based on the use of\nnonparametric kernel density estimators. By extending the existing\nnonparametric kernel density estimator (NKDE) framework to the nonparametric\nestimation of nonlinear and non-Gaussian distributions, our approach offers\nsignificant advantages over existing methods. First, we provide a new\nconsistent and asymptotically normal estimator for the mean function, which\nmakes our approach more stable and robust. Second, our estimator is more\nflexible than existing approaches, as it is applicable to a broad class of\ndistributions, including multivariate distributions and distributions with\nnon-Gaussian kernels. Third, we provide a computationally efficient estimator\nfor the variance function, which is particularly important in applications.\nFinally, we provide theoretical guarantees",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16161616161616163,
          "p": 0.21333333333333335,
          "f": 0.18390804107213649
        },
        "rouge-2": {
          "r": 0.022556390977443608,
          "p": 0.027522935779816515,
          "f": 0.024793383478930023
        },
        "rouge-l": {
          "r": 0.1414141414141414,
          "p": 0.18666666666666668,
          "f": 0.16091953532501008
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.q-bio/SC/2503.04677v1",
      "true_abstract": "We present a minimal model to analyze the capacitive response of a biological\nmembrane subjected to a step voltage via blocking electrodes. Through a\nperturbative analysis of the underlying electrolyte transport equations, we\nshow that the leading-order relaxation of the transmembrane potential is\ngoverned by a capacitive timescale, ${\\tau_{\\rm C} =\\dfrac{\\lambda_{\\rm\nD}L}{D}\\left(\\dfrac{2+\\Gamma\\delta^{\\rm M}/L}{4+\\Gamma\\delta^{\\rm\nM}/\\lambda_{\\rm D}}\\right)}$, where $\\lambda_{\\rm D}$ is the Debye screening\nlength, $L$ is the electrolyte width, $\\Gamma$ is the ratio of the dielectric\npermittivity of the electrolyte to the membrane, $\\delta^{\\rm M}$ is the\nmembrane thickness, and $D$ is the ionic diffusivity. This timescale is\nconsiderably shorter than the traditional RC timescale ${\\lambda_{\\rm D} L /\nD}$ for a bare electrolyte due to the membrane's low dielectric permittivity\nand finite thickness. Beyond the linear regime, however, salt diffusion in the\nbulk electrolyte drives a secondary, nonlinear relaxation process of the\ntransmembrane potential over a longer timescale ${\\tau_{\\rm L} =L^2/4\\pi^2 D}$.\nA simple equivalent-circuit model accurately captures the linear behavior, and\nthe perturbation expansion remains applicable across the entire range of\nobserved physiological transmembrane potentials. Together, these findings\nunderscore the importance of the faster capacitive timescale and nonlinear\neffects on the bulk diffusion timescale in determining transmembrane potential\ndynamics for a range of biological systems.",
      "generated_abstract": "ence of an increasingly complex, dynamic and integrated\nenvironment presents significant challenges to both the functioning of\norganisms and the performance of industrial processes. As a result, the\ndevelopment of new materials and processes that can adapt and enhance their\nperformance under these conditions is crucial. To address these challenges,\nresearchers have developed hybrid organic-inorganic nanoparticles (HONPs),\nwhich combine the physical and chemical properties of organic molecules with\nthe mechanical strength and corrosion resistance of inorganic materials. These\nparticles have been shown to enhance performance in a range of applications,\nincluding water purification and wastewater treatment, photocatalysis,\nphotovoltaics, and energy storage. However, the integration of HONPs into\ncomplex systems, such as biological and industrial systems, remains\nchallenging. This review provides an overview of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.104,
          "p": 0.14444444444444443,
          "f": 0.12093022769064378
        },
        "rouge-2": {
          "r": 0.021505376344086023,
          "p": 0.032520325203252036,
          "f": 0.025889962845383738
        },
        "rouge-l": {
          "r": 0.096,
          "p": 0.13333333333333333,
          "f": 0.11162790210924846
        }
      }
    },
    {
      "paper_id": "nucl-th.nucl-th/2503.09204v1",
      "true_abstract": "The possibility of observing wobbling mode in the even-even systems of 76Ge,\n112Ru, 188,192Os, 192Pt and 232Th is explored using the triaxial projected\nshell model approach. These nuclei are known to have {\\gamma}-bands whose\nodd-spin members are lower than the average of the neighbouring even-spin\nstates. It is shown through a detailed analysis of the excitation energies and\nthe electromagnetic transition probabilities that the observed band structures\nin these nuclei except for 232Th can be characterised as originating from the\nwobbling motion. It is further demonstrated that quasiparticle alignment is\nresponsible for driving the systems to the wobbling mode.",
      "generated_abstract": "alculated the energy-momentum tensor of a relativistic, non-relativistic\nand quantum-mechanically interacting quark-gluon plasma (QGP) using a\nphenomenological model based on the quark model. The energy-momentum tensor is\nobtained by solving the relativistic Boltzmann equation. We have considered\nthree cases: one without the gluonic component, one with the gluonic component\nand the gluon distribution function is that of the gluon distribution in\nQGP and the other with the gluon distribution function of the quark model. We\nhave also considered a case with the gluon distribution function of the\nquark-gluon plasma model. The energy-momentum tensor is calculated for a\nnon-relativistic, relativistic and QGP plasma at finite temperature. We have\nalso calculated the energy-m",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16901408450704225,
          "p": 0.21818181818181817,
          "f": 0.19047618555681545
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.023809523809523808,
          "f": 0.022471905128141487
        },
        "rouge-l": {
          "r": 0.16901408450704225,
          "p": 0.21818181818181817,
          "f": 0.19047618555681545
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/ST/2411.06080v1",
      "true_abstract": "Portfolio diversification, traditionally measured through asset correlations\nand volatilitybased metrics, is fundamental to managing financial risk.\nHowever, existing diversification metrics often overlook non-numerical\nrelationships between assets that can impact portfolio stability, particularly\nduring market stresses. This paper introduces the lexical ratio (LR), a novel\nmetric that leverages textual data to capture diversification dimensions absent\nin standard approaches. By treating each asset as a unique document composed of\nsectorspecific and financial keywords, the LR evaluates portfolio\ndiversification by distributing these terms across assets, incorporating\nentropy-based insights from information theory. We thoroughly analyze LR's\nproperties, including scale invariance, concavity, and maximality,\ndemonstrating its theoretical robustness and ability to enhance risk-adjusted\nportfolio returns. Using empirical tests on S&P 500 portfolios, we compare LR's\nperformance to established metrics such as Markowitz's volatility-based\nmeasures and diversification ratios. Our tests reveal LR's superiority in\noptimizing portfolio returns, especially under varied market conditions. Our\nfindings show that LR aligns with conventional metrics and captures unique\ndiversification aspects, suggesting it is a viable tool for portfolio managers.",
      "generated_abstract": "r examines the pricing of credit default swaps (CDS) on mortgage\nbonds issued by U.S. states. We focus on the effect of credit spreads on CDS\nprices, particularly the role of maturity in determining the level of CDS\nvolatility. Our findings indicate that the spread of credit spreads on\nmortgage bonds, as well as the maturity of the bonds, influence the\npricing of CDS on these bonds. Additionally, we examine the effect of\nvolatility of credit spreads on CDS prices, and show that volatility of the\ncredit spreads has a significant impact on the CDS prices. We also analyze the\nimpact of the spread of credit spreads on the CDS prices on the basis of the\nmaturity of the bonds. Our findings suggest that the CDS prices are sensitive\nto",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1417910447761194,
          "p": 0.3333333333333333,
          "f": 0.1989528753937667
        },
        "rouge-2": {
          "r": 0.011904761904761904,
          "p": 0.021505376344086023,
          "f": 0.015325665910953941
        },
        "rouge-l": {
          "r": 0.1417910447761194,
          "p": 0.3333333333333333,
          "f": 0.1989528753937667
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.10285v1",
      "true_abstract": "Accurate prediction of expected concentrations is essential for effective\ncatchment management, requiring both extensive monitoring and advanced modeling\ntechniques. However, due to limitations in the equation solving capacity, the\nintegration of monitoring and modeling has been suffering suboptimal\nstatistical approaches. This limitation results in models that can only\npartially leverage monitoring data, thus being an obstacle for realistic\nuncertainty assessments by overlooking critical correlations between both\nmeasurements and model parameters. This study presents a novel solution that\nintegrates catchment monitoring and a unified hieratical statistical catchment\nmodeling that employs a log-normal distribution for residuals within a\nleft-censored likelihood function to address measurements below detection\nlimits. This enables the estimation of concentrations within sub-catchments in\nconjunction with a source/fate sub-catchment model and monitoring data. This\napproach is possible due to a model builder R package denoted RTMB. The\nproposed approach introduces a statistical paradigm based on a hierarchical\nstructure, capable of accommodating heterogeneous sampling across various\nsampling locations and the authors suggest that this also will encourage\nfurther refinement of other existing modeling platforms within the scientific\ncommunity to improve synergy with monitoring programs. The application of the\nmethod is demonstrated through an analysis of nickel concentrations in Danish\nsurface waters.",
      "generated_abstract": "r introduces a novel, multi-agent, multi-objective optimization\nmethod for energy-efficient vehicle routing problems. The proposed method\nintegrates two advanced agents, the global agent and the local agent, which\ncan be trained using the same objective function. The global agent learns the\noptimal route by exploring the environment, while the local agent optimizes the\nroute according to the global agent's feedback. The local agent can adjust its\noptimization strategy based on the global agent's feedback, thereby ensuring\nthat the final solution is in accordance with the user's requirements.\nAdditionally, the local agent can be trained using any available objective\nfunction, which is independent of the chosen global agent. This paper\ndemonstrates that the proposed method significantly improves the performance\ncompared to the state-of-the-art approaches, with a 44% reduction in the\nnumber of vehicles and a 6",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17037037037037037,
          "p": 0.27710843373493976,
          "f": 0.21100916959641453
        },
        "rouge-2": {
          "r": 0.04081632653061224,
          "p": 0.06779661016949153,
          "f": 0.050955409321271065
        },
        "rouge-l": {
          "r": 0.16296296296296298,
          "p": 0.26506024096385544,
          "f": 0.20183485766980905
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09577v1",
      "true_abstract": "Chip-firing is a combinatorial game played on a graph in which we place and\ndisperse chips on vertices until a stable configuration is reached. We study a\nchip-firing variant played on an infinite rooted directed $k$-ary tree, where\nwe place $k^n$ chips labeled $0,1,\\dots, k^n-1$ on the root for some\nnonnegative integer $n$, and we say a vertex $v$ can fire if it has at least\n$k$ chips. A vertex fires by dispersing one chip to each out-neighbor. Once\nevery vertex has less than $k$ chips, we reach a stable configuration since no\nvertex can fire. In this paper, we focus on stable configurations resulting\nfrom applying a strategy $F_w$ corresponding to a permutation $w = w_1w_2\\dots\nw_n\\in S_n$: for each vertex $v$ on level $i$ of the $k$-ary tree, the chip\nwith $j$ as its $w_i$th most significant digit in the $k$-ary expansion gets\nsent to the $(j+1)$st child of $v$. We express the stable configuration as a\npermutation, and we explore the properties of these permutations, such as the\nnumber of inversions, descents, and the descent set.",
      "generated_abstract": "We consider the problem of computing the number of isomorphic classes of\nHamiltonian chains on a finite graph $G$ and prove that it is NP-hard. We\ngeneralize this to the case of non-planar graphs. In the non-planar case, we\nprove a stronger result: that any Hamiltonian chain on a non-planar graph is\ngenerated by a Hamiltonian cycle.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.39473684210526316,
          "f": 0.19607842763894234
        },
        "rouge-2": {
          "r": 0.023809523809523808,
          "p": 0.07272727272727272,
          "f": 0.035874435745742304
        },
        "rouge-l": {
          "r": 0.13043478260869565,
          "p": 0.39473684210526316,
          "f": 0.19607842763894234
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.q-bio/SC/2311.13203v2",
      "true_abstract": "Filamentous viruses like influenza and torovirus often display systematic\nbends and arcs of mysterious physical origin. We propose that such viruses\nundergo an instability from a cylindrically symmetric to a toroidally curved\nstate. This ``toro-elastic'' state emerges via spontaneous symmetry breaking\nunder prestress due to short range spike protein interactions magnified by %the\nfilament's surface topography. Once surface stresses are sufficiently large,\nthe filament buckles and the curved state constitutes a soft mode that can\npotentially propagate through the filament's material frame around a\nmexican-hat-type potential. In the mucus of our airways, which constitutes a\nsoft, porous 3D network, glycan chains are omnipresent and influenza's spike\nproteins are known to efficiently bind and cut them. We next show that such a\nnon-equilibrium enzymatic reaction can induce spontaneous rotation of the\ncurved state, leading to a whole body reshaping propulsion similar to -- but\ndifferent from -- eukaryotic flagella and spirochetes.",
      "generated_abstract": "opment of a more efficient and robust model for the self-assembly\nof protein-polymer complexes is essential for the study of protein-polymer\ninteractions and their regulation. We present a self-consistent model for the\nself-assembly of the protein-polymer complex, HIV-1 Tat, into nanoparticles\nbased on the crystal structure of the complex. The model is based on the\nconcept of particle clusters, which are formed by a protein and a polymer\nmolecule, each embedded in a water environment. We study the dynamics of the\nparticle clusters, including the dynamics of the protein and polymer molecules\nin the complex. We determine the critical conditions for the formation of the\nparticle clusters, and investigate the effects of the protein-polymer\ninteraction on the particle clustering. We also explore the influence of\ndifferent interactions between the protein and polymer",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1,
          "p": 0.171875,
          "f": 0.1264367769586473
        },
        "rouge-2": {
          "r": 0.006944444444444444,
          "p": 0.009615384615384616,
          "f": 0.008064511259108039
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.171875,
          "f": 0.1264367769586473
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.08938v1",
      "true_abstract": "Alzheimer's disease (AD) is driven by the accumulation of amyloid-beta\n(Abeta) proteins in the brain, leading to memory loss and cognitive decline.\nWhile monoclonal antibodies targeting Abetahave been approved, optimizing their\nuse to maximize benefits while minimizing side effects remains a challenge.\nThis study develops a mathematical model to describe Abeta aggregation,\ncapturing its progression from monomers to toxic oligomers, protofibrils, and\nfibrils using mass-action kinetics and coarse-grained modeling. The model is\ncalibrated with experimental data, incorporating parameter estimation and\nsensitivity analysis to ensure accuracy. An optimal control framework is\nintroduced to determine the best drug dosing strategy that reduces toxic Abeta\naggregates while minimizing adverse effects, such as amyloid-related imaging\nabnormalities (ARIA). Results indicate that Donanemab achieves the greatest\nreduction in fibrils. This work provides a quantitative framework for\noptimizing AD treatment strategies, offering insights into balancing\ntherapeutic efficacy and safety.",
      "generated_abstract": "The study of how a protein can influence the cell's response to an\nenvironment is a fundamental problem in biology. This paper presents a\ntheoretical framework for studying this problem, based on a model of the\nprotein-membrane interaction. We find a closed-form solution for the steady\nstate response of the protein to a small perturbation of the membrane. This\nsolution can be easily implemented in computer simulations, which allows us to\nperform extensive numerical experiments. We demonstrate that the model\npredicts the correct trend of the response, but fails to correctly describe the\nfull range of the response.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12173913043478261,
          "p": 0.21212121212121213,
          "f": 0.15469612796312704
        },
        "rouge-2": {
          "r": 0.007142857142857143,
          "p": 0.010752688172043012,
          "f": 0.008583686190575356
        },
        "rouge-l": {
          "r": 0.11304347826086956,
          "p": 0.19696969696969696,
          "f": 0.14364640420622093
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.physics/space-ph/2503.08520v1",
      "true_abstract": "Solitons are predominantly observed in near-earth plasmas as well as\nplanetary magnetospheres; however, their existence in the solar corona remains\nlargely unexplored, despite theoretical investigations. This study aims to\naddress this gap by examining the presence and dynamics of solitons in the\nsolar corona, particularly in the context of coronal heating. Utilizing\nobservational data from the Parker Solar Probe (PSP) and Solar and Heliospheric\nObservatory (SOHO) during the onset of a strong Coronal Mass Ejection (CME)\nevent, the analyses reveal a train of aperiodic solitons with increasing\namplitude preceding the eruption. A key finding of this study is that the\nobserved aperiodic soliton train serves as a potential candidate in\nfacilitating energy transfer through dissipation within the coronal plasma,\nhereby, influencing the initiation of solar eruptive events such as a CME. A\ndefining characteristic of this solitary train is its hypersonic and\nsuper-Alfvenic nature, evident from the presence of high Mach numbers that\nreinforces its role in plasma energy equilibration in the solar corona, thereby\ncontributing to plasma heating.",
      "generated_abstract": "r presents the first three-dimensional (3D) simulation of a\ngalactic wind driven by a high-mass X-ray binary (HMXB) outburst in the\nrelativistic magnetohydrodynamic (MHD) framework. The simulation is performed\nusing the MUSIC code, which is a 3D MHD code that allows for the\nself-consistent treatment of radiative cooling, magnetic field amplification,\nand non-ideal MHD effects. The HMXB is assumed to be a typical young-type\nX-ray binary with a mass-accretion rate of $2.5 \\times 10^{-8}$ M$_{\\odot}$\nyr$^{-1}$, a black-hole mass of $5.5 \\times 10^7$ M$_{\\odot}$, and a radius of\n$10^{10}$ cm. The simulation is performed in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09009009009009009,
          "p": 0.14492753623188406,
          "f": 0.11111110638333353
        },
        "rouge-2": {
          "r": 0.012658227848101266,
          "p": 0.02197802197802198,
          "f": 0.01606425239012408
        },
        "rouge-l": {
          "r": 0.07207207207207207,
          "p": 0.11594202898550725,
          "f": 0.08888888416111136
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.01514v1",
      "true_abstract": "Advancements in data collection have led to increasingly common repeated\nobservations with complex structures in biomedical studies. Treating these\nobservations as random objects, rather than summarizing features as vectors,\navoids feature extraction and better reflects the data's nature. Examples\ninclude repeatedly measured activity intensity distributions in physical\nactivity analysis and brain networks in neuroimaging. Testing whether these\nrepeated random objects differ across groups is fundamentally important;\nhowever, traditional statistical tests often face challenges due to the\nnon-Euclidean nature of metric spaces, dependencies from repeated measurements,\nand the unequal number of repeated measures. By defining within-subject\nvariability using pairwise distances between repeated measures and extending\nFr\\'echet analysis of variance, we develop a generalized Fr\\'echet test for\nexchangeable repeated random objects, applicable to general metric space-valued\ndata with unequal numbers of repeated measures. The proposed test can\nsimultaneously detect differences in location, scale, and within-subject\nvariability. We derive the asymptotic distribution of the test statistic, which\nfollows a weighted chi-squared distribution. Simulations demonstrate that the\nproposed test performs well across different types of random objects. We\nillustrate its effectiveness through applications to physical activity data and\nresting-state functional magnetic resonance imaging data.",
      "generated_abstract": "We propose a novel method to estimate the location of the true underlying\ndistribution of the mean of a random sample from a latent variable model\nconsidering an arbitrary number of latent variables. The proposed estimator\nfollows a multivariate Gaussian distribution, which allows for a direct\ncomparison with existing estimators. We derive an asymptotic distribution of\nthe estimator and present numerical results for the proposed estimator and the\nalternative estimator proposed by Kwong et al. (2015).",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13846153846153847,
          "p": 0.34615384615384615,
          "f": 0.19780219372056523
        },
        "rouge-2": {
          "r": 0.05,
          "p": 0.12857142857142856,
          "f": 0.07199999596800023
        },
        "rouge-l": {
          "r": 0.12307692307692308,
          "p": 0.3076923076923077,
          "f": 0.1758241717425433
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.07102v1",
      "true_abstract": "The paper proposes a novel Economic Model Predictive Control (EMPC) scheme\nfor Autonomous Surface Vehicles (ASVs) to simultaneously address path following\naccuracy and energy constraints under environmental disturbances. By\nformulating lateral deviations as energy-equivalent penalties in the cost\nfunction, our method enables explicit trade-offs between tracking precision and\nenergy consumption. Furthermore, a motion-dependent decomposition technique is\nproposed to estimate terminal energy costs based on vehicle dynamics. Compared\nwith the existing EMPC method, simulations with real-world ocean disturbance\ndata demonstrate the controller's energy consumption with a 0.06 energy\nincrease while reducing cross-track errors by up to 18.61. Field experiments\nconducted on an ASV equipped with an Intel N100 CPU in natural lake\nenvironments validate practical feasibility, achieving 0.22 m average\ncross-track error at nearly 1 m/s and 10 Hz control frequency. The proposed\nscheme provides a computationally tractable solution for ASVs operating under\nresource constraints.",
      "generated_abstract": "tion of the electrical power system requires the integration of\nmultiple technologies, such as renewable energy sources, energy storage, and\nelectric vehicles, to meet the increasing energy demands. The integration of\nthese technologies in power systems is a complex task requiring a high level\nof integration. The integration of renewable energy sources, for example,\nrequires an efficient integration of renewable energy resources. However, the\ncomplexity of this task increases with the number of renewable energy sources\nintegrated in the system. This paper proposes a new approach for the integration\nof renewable energy sources in power systems. The proposed methodology uses a\ndynamical system model to represent the power system and a genetic algorithm to\noptimize the integration of renewable energy sources in the system. The\nproposed methodology has been tested in a real power system model, and the\nresults show that the proposed methodology is able to improve",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12605042016806722,
          "p": 0.20833333333333334,
          "f": 0.1570680581299856
        },
        "rouge-2": {
          "r": 0.034722222222222224,
          "p": 0.042735042735042736,
          "f": 0.0383141712987191
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.19444444444444445,
          "f": 0.14659685394150393
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.04067v1",
      "true_abstract": "As whole genomes become widely available, maximum likelihood and Bayesian\nphylogenetic methods are demonstrating their limits in meeting the escalating\ncomputational demands. Conversely, distance-based phylogenetic methods are\nefficient, but are rarely favoured due to their inferior performance. Here, we\nextend distance-based phylogenetics using an entropy-based likelihood of the\nevolution among pairs of taxa, allowing for fast Bayesian inference in\ngenome-scale datasets. We provide evidence of a close link between the\ninference criteria used in distance methods and Felsenstein's likelihood, such\nthat the methods are expected to have comparable performance in practice. Using\nthe entropic likelihood, we perform Bayesian inference on three phylogenetic\nbenchmark datasets and find that estimates closely correspond with previous\ninferences. We also apply this rapid inference approach to a 60-million-site\nalignment from 363 avian taxa, covering most avian families. The method has\noutstanding performance and reveals substantial uncertainty in the avian\ndiversification events immediately after the K-Pg transition event. The\nentropic likelihood allows for efficient Bayesian phylogenetic inference,\naccommodating the analysis demands of the genomic era.",
      "generated_abstract": "r provides a novel method to model the dynamics of a class of\nmodels known as the two-species two-population model. The model is defined as\nthe combination of a logistic growth model with a simple linear model, and is\ncharacterized by two parameters: the growth rate of the logistic model and the\nrate of growth of the linear model. The model is parameterized using a single\nparameter, a hyperparameter, and is studied for two parameter settings. The\nfirst setting is known as the two-parameter model, where the growth rate of the\nlogistic model is a function of the growth rate of the linear model. The second\nsetting is the two-population model, where the growth rate of the logistic model\nis a function of the growth rate of the linear model. In this paper, we\nintroduce a new parameterization for the two-population model, where the growth\nrate of the logistic model is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10434782608695652,
          "p": 0.2222222222222222,
          "f": 0.1420118299709395
        },
        "rouge-2": {
          "r": 0.012345679012345678,
          "p": 0.02127659574468085,
          "f": 0.015624995352784582
        },
        "rouge-l": {
          "r": 0.10434782608695652,
          "p": 0.2222222222222222,
          "f": 0.1420118299709395
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.15634v2",
      "true_abstract": "Instrumental variables (IV) estimation is a fundamental method in\neconometrics and statistics for estimating causal effects in the presence of\nunobserved confounding. However, challenges such as untestable model\nassumptions and poor finite sample properties have undermined its reliability\nin practice. Viewing common issues in IV estimation as distributional\nuncertainties, we propose DRIVE, a distributionally robust IV estimation\nmethod. We show that DRIVE minimizes a square root variant of ridge regularized\ntwo stage least squares (TSLS) objective when the ambiguity set is based on a\nWasserstein distance. In addition, we develop a novel asymptotic theory for\nthis estimator, showing that it achieves consistency without requiring the\nregularization parameter to vanish. This novel property ensures that the\nestimator is robust to distributional uncertainties that persist in large\nsamples. We further derive the asymptotic distribution of Wasserstein DRIVE and\npropose data-driven procedures to select the regularization parameter based on\ntheoretical results. Simulation studies demonstrate the superior finite sample\nperformance of Wasserstein DRIVE in terms of estimation error and out-of-sample\nprediction. Due to its regularization and robustness properties, Wasserstein\nDRIVE presents an appealing option when the practitioner is uncertain about\nmodel assumptions or distributional shifts in data.",
      "generated_abstract": "er the estimation of the average treatment effect on average in\nrepeated-measures randomized experiments. We show that, under mild conditions,\nthe average treatment effect is uniquely identified, and that it can be\nidentified even when the treatment effect is not linear. We also show that,\nunder mild conditions, the average treatment effect on average can be\nidentified if the repeated measurement time-point intervals are non-overlapping\nand the treatment effects are monotone in the time-point intervals. We show\nthat, under mild conditions, the average treatment effect can be identified if\nthe repeated measurement time-point intervals are overlapping and the treatment\neffects are monotone in the time-point intervals. Finally, we show that, under\nmild conditions, the average treatment effect on average can be identified if\nthe repeated measurement time-point intervals are overlapping and the treatment\neffects are non-mon",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11023622047244094,
          "p": 0.3111111111111111,
          "f": 0.16279069381084377
        },
        "rouge-2": {
          "r": 0.02197802197802198,
          "p": 0.06349206349206349,
          "f": 0.03265305740408208
        },
        "rouge-l": {
          "r": 0.11023622047244094,
          "p": 0.3111111111111111,
          "f": 0.16279069381084377
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.07509v1",
      "true_abstract": "Non-orthogonal multiple access (NOMA) has gained significant attention as a\npotential next-generation multiple access technique. However, its\nimplementation with finite-alphabet inputs faces challenges. Particularly, due\nto inter-user interference, superimposed constellations may have overlapping\nsymbols leading to high bit error rates when successive interference\ncancellation (SIC) is applied. To tackle the issue, this paper employs\nautoencoders to design interference-aware super-constellations. Unlike\nconventional methods where superimposed constellation may have overlapping\nsymbols, the proposed autoencoder-based NOMA (AE-NOMA) is trained to design\nsuper-constellations with distinguishable symbols at receivers, regardless of\nchannel gains. The proposed architecture removes the need for SIC, allowing\nmaximum likelihood-based approaches to be used instead. The paper presents the\nconceptual architecture, loss functions, and training strategies for AE-NOMA.\nVarious test results are provided to demonstrate the effectiveness of\ninterference-aware constellations in improving the bit error rate, indicating\nthe adaptability of AE-NOMA to different channel scenarios and its promising\npotential for implementing NOMA systems",
      "generated_abstract": "ork, we propose a novel deep learning-based framework to detect\nmotorcycle accidents by analyzing videos recorded from a fleet of motorcycles.\nThe system leverages the concept of deep learning to identify motorcycles and\nthen classifies the event type (e.g., collision, stunt, etc.) using a\nmulti-task convolutional neural network (CNN). In our approach, we use a\nsingle-layer CNN for the classification task, but we show that using multiple\nlayers and multi-task CNNs can significantly improve the detection accuracy\nand generalization ability. Moreover, we propose an ensemble method that combines\nthe classification results of the individual CNNs to improve the overall\nperformance of the system. The performance of our system was evaluated using\nthe KITTI dataset, which contains 200 videos with 17 event types. The results\nshow that the proposed approach achieves an accuracy of 8",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09821428571428571,
          "p": 0.12222222222222222,
          "f": 0.10891088614841704
        },
        "rouge-2": {
          "r": 0.00684931506849315,
          "p": 0.007692307692307693,
          "f": 0.0072463718284008225
        },
        "rouge-l": {
          "r": 0.09821428571428571,
          "p": 0.12222222222222222,
          "f": 0.10891088614841704
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.10600v1",
      "true_abstract": "This study examines the impact of monetary factors on the conversion of\nfarmland to renewable energy generation, specifically solar and wind, in the\ncontext of expanding U.S. energy production. We propose a new econometric\nmethod that accounts for the diverse circumstances of landowners, including\ntheir unordered alternative land use options, non-monetary benefits from\nfarming, and the influence of local regulations. We demonstrate that\nidentifying the cross elasticity of landowners' farming income in relation to\nthe conversion of farmland to renewable energy requires an understanding of\ntheir preferences. By utilizing county legislation that we assume to be shaped\nby land-use preferences, we estimate the cross-elasticities of farming income.\nOur findings indicate that monetary incentives may only influence landowners'\ndecisions in areas with potential for future residential development,\nunderscoring the importance of considering both preferences and regulatory\ncontexts.",
      "generated_abstract": "p a new framework for quantifying the heterogeneity of treatment\neffects (HOTE) across individuals, and we introduce a novel HOTE measure. Our\nframework extends the existing HOTE measures, which rely on a single\ntreatment-outcome pair. Specifically, we consider a binary treatment, and we\nassume that the treatment is determined by a binary outcome, and that the\noutcome is determined by a binary treatment. The treatment is determined by a\nbinary outcome if the outcome is binary, and otherwise the treatment is\ndetermined by a single variable. We derive the HOTE measure for the binary\noutcome-treatment model, and we derive the HOTE measure for the binary outcome-\noutcome model. The HOTE measure is derived using the joint distribution of the\nbinary outcome and binary treatment. We also show that the HOTE measure can be\nderived using the marginal",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14736842105263157,
          "p": 0.22950819672131148,
          "f": 0.17948717472468784
        },
        "rouge-2": {
          "r": 0.023255813953488372,
          "p": 0.030303030303030304,
          "f": 0.026315784560250226
        },
        "rouge-l": {
          "r": 0.14736842105263157,
          "p": 0.22950819672131148,
          "f": 0.17948717472468784
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.16313v1",
      "true_abstract": "Player tracking data have provided great opportunities to generate novel\ninsights into understudied areas of American football, such as pre-snap motion.\nUsing a Bayesian multilevel model with heterogeneous variances, we provide an\nassessment of NFL quarterbacks and their ability to synchronize the timing of\nthe ball snap with pre-snap movement from their teammates. We focus on passing\nplays with receivers in motion at the snap and running a route, and define the\nsnap timing as the time between the moment a receiver begins motioning and the\nball snap event. We assume a Gamma distribution for the play-level snap timing\nand model the mean parameter with player and team random effects, along with\nrelevant fixed effects such as the motion type identified via a Gaussian\nmixture model. Most importantly, we model the shape parameter with quarterback\nrandom effects, which enables us to estimate the differences in snap timing\nvariability among NFL quarterbacks. We demonstrate that higher variability in\nsnap timing is beneficial for the passing game, as it relates to facing less\nhavoc created by the opposing defense. We also obtain a quarterback leaderboard\nbased on our snap timing variability measure, and Patrick Mahomes stands out as\nthe top player.",
      "generated_abstract": "y investigates the application of the method of moments to estimate\nthe population variance of the logarithm of the logarithm of the logarithm of\nthe logarithm of the logarithm of the logarithm of the logarithm of the logarithm\nof the logarithm of the logarithm of the logarithm of the logarithm of the\nlogarithm of the logarithm of the logarithm of the logarithm of the logarithm\nof the logarithm of the logarithm of the logarithm of the logarithm of the\nlogarithm of the logarithm of the logarithm of the logarithm of the logarithm\nof the logarithm of the logarithm of the logarithm of the logarithm of the\nlogarithm of the logarithm of the logarithm of the logarithm of the logarithm",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.03305785123966942,
          "p": 0.3333333333333333,
          "f": 0.06015037429815144
        },
        "rouge-2": {
          "r": 0.016574585635359115,
          "p": 0.1875,
          "f": 0.030456851299440924
        },
        "rouge-l": {
          "r": 0.03305785123966942,
          "p": 0.3333333333333333,
          "f": 0.06015037429815144
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2412.11602v1",
      "true_abstract": "Multivariate Distributions are needed to capture the correlation structure of\ncomplex systems. In previous works, we developed a Random Matrix Model for such\ncorrelated multivariate joint probability density functions that accounts for\nthe non-stationarity typically found in complex systems. Here, we apply these\nresults to the returns measured in correlated stock markets. Only the knowledge\nof the multivariate return distributions allows for a full-fledged risk\nassessment. We analyze intraday data of 479 US stocks included in the S&P500\nindex during the trading year of 2014. We focus particularly on the tails which\nare algebraic and heavy. The non-stationary fluctuations of the correlations\nmake the tails heavier. With the few-parameter formulae of our Random Matrix\nModel we can describe and quantify how the empirical distributions change for\nvarying time resolution and in the presence of non-stationarity.",
      "generated_abstract": "The present paper aims to study the effect of market structure on the\nmarket impact and the arbitrage potential. The market structure is analyzed\nthrough the use of a simple model, which allows the quantification of the\neffect of the market structure on the market impact. The model is then used to\nevaluate the arbitrage potential in the market and the impact of the market\nstructure on the arbitrage potential. The results show that the market\nstructure significantly affects the market impact, and that the market\nstructure has a significant impact on the arbitrage potential.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13978494623655913,
          "p": 0.3333333333333333,
          "f": 0.19696969280647392
        },
        "rouge-2": {
          "r": 0.0234375,
          "p": 0.047619047619047616,
          "f": 0.03141360814451421
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.3076923076923077,
          "f": 0.18181817765495878
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.08411v1",
      "true_abstract": "In this article, we prove that, given two finite connected graphs $\\Gamma_1$\nand $\\Gamma_2$, if the two right-angled Artin groups $A(\\Gamma_1)$ and\n$A(\\Gamma_2)$ are quasi-isometric, then the infinite pointed sums\n$\\bigvee_\\mathbb{N} \\Gamma_1^{\\bowtie}$ and $\\bigvee_\\mathbb{N}\n\\Gamma_2^{\\bowtie}$ are homotopy equivalent, where $\\Gamma_i^{\\bowtie}$ denotes\nthe simplicial complex whose vertex-set is $\\Gamma_i$ and whose simplices are\ngiven by joins. These invariants are extracted from a study, of independent\ninterest, of the homotopy types of several complexes of hyperplanes in\nquasi-median graphs (such as one-skeleta of CAT(0) cube complexes). For\ninstance, given a quasi-median graph $X$, the \\emph{crossing complex}\n$\\mathrm{Cross}^\\triangle(X)$ is the simplicial complex whose vertices are the\nhyperplanes (or $\\theta$-classes) of $X$ and whose simplices are collections of\npairwise transverse hyperplanes. When $X$ has no cut-vertex, we show that\n$\\mathrm{Cross}^\\triangle(X)$ is homotopy equivalent to the pointed sum of the\nlinks of all the vertices in the prism-completion $X^\\square$ of $X$.",
      "generated_abstract": "aper, we study the $d$-dimensional analog of the Bismut-Kirillov\nresidue for the Hitchin system, which is the $d$-dimensional analog of the\nHitchin system. In the case of $d=1$, this is the Hitchin system with one\nsuperpotential. We prove that, if the Hitchin system is equipped with a\nnon-degenerate superpotential, then the $d$-dimensional analog of the Bismut-Kirillov\nresidue is given by a sum of $\\frac{d-1}{2}$ integrals. In the case of $d=2$,\nthis is the Hitchin system with two superpotentials. We prove that, if the\nHitchin system is equipped with two non-degenerate superpotentials, then the\n$d$-dimensional analog of the Bismut-Kirillov",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16304347826086957,
          "p": 0.375,
          "f": 0.22727272304866855
        },
        "rouge-2": {
          "r": 0.051470588235294115,
          "p": 0.12280701754385964,
          "f": 0.0725388559413678
        },
        "rouge-l": {
          "r": 0.15217391304347827,
          "p": 0.35,
          "f": 0.2121212078971534
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/AR/2503.08968v1",
      "true_abstract": "Homomorphic encryption (HE) allows secure computation on encrypted data\nwithout revealing the original data, providing significant benefits for\nprivacy-sensitive applications. Many cloud computing applications (e.g., DNA\nread mapping, biometric matching, web search) use exact string matching as a\nkey operation. However, prior string matching algorithms that use homomorphic\nencryption are limited by high computational latency caused by the use of\ncomplex operations and data movement bottlenecks due to the large encrypted\ndata size. In this work, we provide an efficient algorithm-hardware codesign to\naccelerate HE-based secure exact string matching. We propose CIPHERMATCH, which\n(i) reduces the increase in memory footprint after encryption using an\noptimized software-based data packing scheme, (ii) eliminates the use of costly\nhomomorphic operations (e.g., multiplication and rotation), and (iii) reduces\ndata movement by designing a new in-flash processing (IFP) architecture. We\ndemonstrate the benefits of CIPHERMATCH using two case studies: (1) Exact DNA\nstring matching and (2) encrypted database search. Our pure software-based\nCIPHERMATCH implementation that uses our memory-efficient data packing scheme\nimproves performance and reduces energy consumption by 42.9X and 17.6X,\nrespectively, compared to the state-of-the-art software baseline. Integrating\nCIPHERMATCH with IFP improves performance and reduces energy consumption by\n136.9X and 256.4X, respectively, compared to the software-based CIPHERMATCH\nimplementation.",
      "generated_abstract": "This paper introduces a novel framework for the design of distributed\nsecurity and privacy protection for blockchain-based systems. The framework\nintegrates security and privacy protection mechanisms, which are essential for\nensuring the integrity, authenticity, and confidentiality of blockchain data.\nThe security and privacy protection mechanisms are implemented in a\ndistributed manner through the participation of different actors. The framework\nis designed to ensure the security and privacy of blockchain-based systems\nwithout requiring a centralized authority, such as a blockchain network or a\ndistributed ledger. This framework provides a framework for designing and\nimplementing distributed security and privacy protection mechanisms for blockchain\nsystems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09420289855072464,
          "p": 0.24528301886792453,
          "f": 0.1361256504405034
        },
        "rouge-2": {
          "r": 0.0053475935828877,
          "p": 0.011904761904761904,
          "f": 0.007380069523020956
        },
        "rouge-l": {
          "r": 0.07971014492753623,
          "p": 0.20754716981132076,
          "f": 0.11518324206354008
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.14498v1",
      "true_abstract": "This paper deals with a nonparametric Nadaraya-Watson (NW) estimator of the\ntransition density function computed from independent continuous observations\nof a diffusion process. A risk bound is established on this estimator. The\npaper also deals with an extension of the penalized comparison to overfitting\nbandwidths selection method for our NW estimator. Finally, numerical\nexperiments are provided.",
      "generated_abstract": "In this paper, we propose a novel methodology for the estimation of the\nrandom variables of the generalized Breusch-Godfrey test statistic. The\nproposed method is based on a new kernel density estimator, which can be\nefficiently computed in high-dimensional settings. We show that the proposed\nmethodology provides consistent estimators of the test statistic and the\nnull distribution, and we provide theoretical properties of the estimators.\nAdditionally, we present an R implementation of the proposed methodology.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2553191489361702,
          "p": 0.22641509433962265,
          "f": 0.2399999950180001
        },
        "rouge-2": {
          "r": 0.018867924528301886,
          "p": 0.014925373134328358,
          "f": 0.01666666173472368
        },
        "rouge-l": {
          "r": 0.23404255319148937,
          "p": 0.20754716981132076,
          "f": 0.21999999501800013
        }
      }
    },
    {
      "paper_id": "math.AP.math/AP/2503.10495v1",
      "true_abstract": "In the present work, we develop a comprehensive and rigorous analytical\nframework for a non-local phase-field model that describes tumour growth\ndynamics. The model is derived by coupling a non-local Cahn-Hilliard equation\nwith a parabolic reaction-diffusion equation, which accounts for both phase\nsegregation and nutrient diffusion. Previous studies have only considered\nsymmetric potentials for similar models. However, in the biological context of\ncell-to-cell adhesion, single-well potentials, like the so-called Lennard-Jones\npotential, seem physically more appropriate. The Cahn-Hilliard equation with\nthis kind of potential has already been analysed. Here, we take a step forward\nand consider a more refined model. First, we analyse the model with a viscous\nrelaxation term in the chemical potential and subject to suitable initial and\nboundary conditions. We prove the existence of solutions, a separation property\nfor the phase variable, and a continuous dependence estimate with respect to\nthe initial data. Finally, via an asymptotic analysis, we recover the existence\nof a weak solution to the initial and boundary value problem without viscosity,\nprovided that the chemotactic sensitivity is small enough.",
      "generated_abstract": "a generalization of the Gauss-Bonnet theorem, which gives an\nexpression for the first Chern class of the cotangent bundle of a compact\nmanifold in terms of its Chern class. Our formula is an extension of the\nclassification of K\\\"ahler manifolds in terms of their first Chern classes,\nwhich was first obtained by Kodaira in 1954. We also show that our formula is\na special case of a generalization of the Gauss-Bonnet theorem by\nSuzuki-Tate-Watanabe, which gives an expression for the first Chern class of\nthe cotangent bundle of a compact complex surface in terms of its Chern class.\nOur formula is also a special case of the Gauss-Bonnet theorem by\nLanglands-Pappas-Weil, which gives an expression for the first Chern class of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09243697478991597,
          "p": 0.23404255319148937,
          "f": 0.13253011642255783
        },
        "rouge-2": {
          "r": 0.012269938650306749,
          "p": 0.028985507246376812,
          "f": 0.017241375131169263
        },
        "rouge-l": {
          "r": 0.07563025210084033,
          "p": 0.19148936170212766,
          "f": 0.10843373088038917
        }
      }
    },
    {
      "paper_id": "cs.CE.q-bio/QM/2503.05113v2",
      "true_abstract": "The process of setting up and successfully running Molecular Dynamics\nSimulations (MDS) is outlined to be incredibly labour and computationally\nexpensive with a very high barrier to entry for newcomers wishing to utilise\nthe benefits and insights of MDS. Here, presented, is a unique Free and\nOpen-Source Software (FOSS) solution that aims to not only reduce the barrier\nof entry for new Molecular Dynamics (MD) users, but also significantly reduce\nthe setup time and hardware utilisation overhead for even highly experienced MD\nresearchers. This is accomplished through the creation of the Molecular\nDynamics Simulation Generator and Analysis Tool (MDSGAT) which currently serves\nas a viable alternative to other restrictive or privatised MDS Graphical\nsolutions with a unique design that allows for seamless collaboration and\ndistribution of exact MD simulation setups and initialisation parameters\nthrough a single setup file. This solution is designed from the start with a\nmodular mindset allowing for additional software expansion to incorporate\nnumerous extra MDS packages and analysis methods over time",
      "generated_abstract": "The study of gene regulation is fundamental to biology and medicine.\nThe study of gene regulation is fundamental to biology and medicine. In this\narticle, we review recent advances in gene regulatory network (GRN) inference,\nincluding methodologies and open problems. We discuss the importance of GRNs\nin biology and medicine, including cancer, neurodegeneration, and inflammation.\nWe also discuss the challenges of GRN inference, including data\nincompleteness, data heterogeneity, and the complexities of gene regulation.\nFinally, we conclude with future research directions and potential\napplications of GRNs in biology and medicine.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07142857142857142,
          "p": 0.1568627450980392,
          "f": 0.09815950490270633
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07142857142857142,
          "p": 0.1568627450980392,
          "f": 0.09815950490270633
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/CR/2503.09726v1",
      "true_abstract": "Graph Neural Networks (GNNs) are widely used and deployed for graph-based\nprediction tasks. However, as good as GNNs are for learning graph data, they\nalso come with the risk of privacy leakage. For instance, an attacker can run\ncarefully crafted queries on the GNNs and, from the responses, can infer the\nexistence of an edge between a pair of nodes. This attack, dubbed as a\n\"link-stealing\" attack, can jeopardize the user's privacy by leaking\npotentially sensitive information. To protect against this attack, we propose\nan approach called \"$(N)$ode $(A)$ugmentation for $(R)$estricting $(G)$raphs\nfrom $(I)$nsinuating their $(S)$tructure\" ($NARGIS$) and study its feasibility.\n$NARGIS$ is focused on reshaping the graph embedding space so that the\nposterior from the GNN model will still provide utility for the prediction task\nbut will introduce ambiguity for the link-stealing attackers. To this end,\n$NARGIS$ applies spectral clustering on the given graph to facilitate it being\naugmented with new nodes -- that have learned features instead of fixed ones.\nIt utilizes tri-level optimization for learning parameters for the GNN model,\nsurrogate attacker model, and our defense model (i.e. learnable node features).\nWe extensively evaluate $NARGIS$ on three benchmark citation datasets over\neight knowledge availability settings for the attackers. We also evaluate the\nmodel fidelity and defense performance on influence-based link inference\nattacks. Through our studies, we have figured out the best feature of $NARGIS$\n-- its superior fidelity-privacy performance trade-off in a significant number\nof cases. We also have discovered in which cases the model needs to be\nimproved, and proposed ways to integrate different schemes to make the model\nmore robust against link stealing attacks.",
      "generated_abstract": "asing availability of large language models (LLMs) has transformed\nthe way researchers and practitioners create, train, and evaluate language\nmodels, allowing them to create, train, and evaluate larger language models\nmore quickly and more efficiently. This paper explores the challenges of\ncreating, training, and evaluating large language models using large language\nmodels, and describes the tools and techniques used to address these challenges.\nThe paper begins with a brief overview of the language modeling field, then\nexplores the challenges of creating, training, and evaluating large language\nmodels. The paper concludes with a discussion of tools and techniques used to\naddress these challenges. The paper provides an overview of the language model\nevolution, followed by a discussion of the challenges of creating, training,\nand evaluating large language models. The paper concludes with a discussion of\ntools and techniques used to address these challenges",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07909604519774012,
          "p": 0.25,
          "f": 0.12017167016817416
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.06779661016949153,
          "p": 0.21428571428571427,
          "f": 0.10300428819392522
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.cond-mat/dis-nn/2503.09539v1",
      "true_abstract": "Quenched disorder in a solid state system can result in Anderson localization\nwhere electrons are exponentially localized and the system behaves like an\ninsulator. In this study, we investigate the effect of a DC electric field on\nAnderson localization. The study highlights the case of a one-dimensional\ninsulator chain with on-site disorder when a DC electric field is applied\nthroughout the chain. We study spectral properties of an Anderson localized\nsystem in equilibrium and out-of-equilibrium using a full lattice\nnonequilibrium Green's function method in the steady-state limit. Tuning the\ndisorder and the electric field strength results in the creation of exponential\nLifshitz tails near the band edge by strongly localized levels. These Lifshtiz\ntails create effects like insulator-to-metal transitions and contribute to\nnon-local hopping. The electric field causes gradual delocalization of the\nsystem and Anderson localization crossing over to Wannier Stark ladders at very\nstrong fields. Our study makes a comparison with the coherent potential\napproximation (CPA) highlighting some major differences and similarities in the\nphysics of disorder.",
      "generated_abstract": "the dynamics of a single spin in a two-dimensional quantum spin\nchain. We find that the system exhibits a rich phase diagram with two\ndifferent types of spin-spin correlations, depending on the strength of the\ninter-spin interaction. In the weak-interaction limit, the system displays\nantiferromagnetic correlations. As the strength of the interaction is\nincreased, the system undergoes a transition to a ferromagnetic state, which\nis characterized by the existence of long-ranged ferromagnetic correlations.\nThese correlations can be identified with the so-called spin-density waves,\nwhich are also observed in the standard Hubbard model. Interestingly, we show\nthat the long-range ferromagnetic correlations are not destroyed by a\ndecoherence process, but are instead preserved by a dissipative process that\ninvolves an entanglement-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17757009345794392,
          "p": 0.24675324675324675,
          "f": 0.2065217342633508
        },
        "rouge-2": {
          "r": 0.04516129032258064,
          "p": 0.06363636363636363,
          "f": 0.05283018382342515
        },
        "rouge-l": {
          "r": 0.17757009345794392,
          "p": 0.24675324675324675,
          "f": 0.2065217342633508
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/atm-clus/2502.16979v1",
      "true_abstract": "Technological advancements in generation of ultrafast and intense laser\npulses have enabled the real-time observation and control of charge migration\nin molecules on their natural timescale, which ranges from few femtoseconds to\nseveral hundreds of attoseconds. Present thesis discusses the effect of\nsymmetry on the adiabatic attosecond charge migration in different molecular\nsystems. The spatial representation of the charge migration is documented by\ntime-dependent electronic charge and flux densities. Furthermore, the induced\ncharge migration is imaged via time resolved x-ray diffraction (TRXD) with\natomic-scale spatiotemporal resolution in few cases.",
      "generated_abstract": "al design of the Space Station (SS) is optimized to maximize\nthe power-to-weight ratio (PWR) for the crew's daily activities, but it\ncurrently overlooks the impact of the PWR on the environmental impact of the\nstation's operations. This study aims to optimize the PWR by considering\nenvironmental impacts, including the radiation exposure of the crew and the\ncost of maintaining the station's power system. The PWR is optimized using the\nFour-Dimensional Optimal Design (4D-OD) method, in which the PWR is optimized\nfor each combination of radiation dose, crew exposure time, and operating\nmode, considering the impact of the PWR on the crew's radiation exposure and\nthe power system's cost. The results indicate that the crew's radiation dose\ncan be reduced to 25.9 Gy (30 Gy",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15151515151515152,
          "p": 0.14084507042253522,
          "f": 0.14598539646651412
        },
        "rouge-2": {
          "r": 0.024096385542168676,
          "p": 0.019230769230769232,
          "f": 0.02139036939460779
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.1267605633802817,
          "f": 0.1313868563205287
        }
      }
    },
    {
      "paper_id": "physics.atom-ph.physics/atom-ph/2503.08855v1",
      "true_abstract": "Assembly of ultracold polar molecules containing silver (Ag) from\nlaser-cooled atoms requires knowledge of the dynamic polarizabilities of Ag at\nconvenient laser wavelengths. We present calculations and analysis of the\nenergies and electric-dipole dc and ac polarizabilities of the low-lying states\nof neutral Ag. Calculations of the properties of the 4d^{10}x states, where\nx=5s,6s,7s,5p,6p,7p,5d,6d, and 4f, are performed using the linearized coupled\ncluster single-double method. The properties of the 4d^9 5s^2 ^2D_{5/2,3/2}\nstates are obtained within the framework of configuration interaction with 11\nand 17 electrons in the valence field. We analyze the different contributions\nto the polarizabilities and estimate the uncertainties of our predictions.",
      "generated_abstract": "study of the $^6$Li(p,$\\gamma$)$^6$He reaction, which was performed\nin 2018 using the PIPI-CLOUD facility at the LCLS-II synchrotron radiation\nfacility, demonstrated the potential of the technique to provide new insights\ninto the dynamics of the $p$+$d$ reaction. In this paper, we report on the\nmeasurement of the $p$+$d$ reaction of $^{197}$Au at the energy of 1.51 MeV\nusing the PIPI-CLOUD facility. The $p$+$d$ reaction was performed using the\n$^6$Li(p,$\\gamma$)$^6$He target. The data were analyzed using the\nexperimental-data-driven $p$+$d$ model, which includes a description of the\nnuc",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.15384615384615385,
          "f": 0.12903225319458916
        },
        "rouge-2": {
          "r": 0.030927835051546393,
          "p": 0.04225352112676056,
          "f": 0.03571428083404262
        },
        "rouge-l": {
          "r": 0.08333333333333333,
          "p": 0.11538461538461539,
          "f": 0.09677418867846019
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.05691v1",
      "true_abstract": "Prevailing methods of course allocation at undergraduate institutions involve\nreserving seats to give priority to designated groups of students. We introduce\na competitive equilibrium-based mechanism that assigns course seats using\nstudent preferences and course priorities. This mechanism satisfies approximate\nnotions of stability, efficiency, envy-freeness, and strategy-proofness. We\nevaluate its performance relative to a mechanism widely used in practice using\npreferences estimated from university data. Our empirical findings demonstrate\nan improvement in student satisfaction and allocation fairness. The number of\nstudents who envy another student of weakly lower priority declines by 8\npercent, or roughly 500 students.",
      "generated_abstract": "r introduces a general framework for studying the interplay between\noptimization and the dynamics of a multi-agent system. We introduce the notion\nof a ``cost-efficient'' optimization, which is defined as a minimizer of a\ncost function that depends on both the system's state and the actions taken by\nthe agents. We show that, under certain conditions, such a cost-efficient\noptimization can be realized by a limited number of agents acting jointly. We\nalso characterize the set of cost-efficient optimization policies, and we show\nthat it coincides with the set of optimal policies. Finally, we introduce a\nclass of problems that is a special case of the class of problems considered\nin this paper. The class of problems we consider is a subclass of the class of\nproblems considered in the seminal paper by Krasnoselskii and Stigum (1995), and\nwe show that our class of problems",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1388888888888889,
          "p": 0.12658227848101267,
          "f": 0.13245032613657315
        },
        "rouge-2": {
          "r": 0.031914893617021274,
          "p": 0.02459016393442623,
          "f": 0.02777777286179785
        },
        "rouge-l": {
          "r": 0.1388888888888889,
          "p": 0.12658227848101267,
          "f": 0.13245032613657315
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/CB/2411.00948v1",
      "true_abstract": "Conventional histopathology has long been essential for disease diagnosis,\nrelying on visual inspection of tissue sections. Immunohistochemistry aids in\ndetecting specific biomarkers but is limited by its single-marker approach,\nrestricting its ability to capture the full tissue environment. The advent of\nmultiplexed imaging technologies, like multiplexed immunofluorescence and\nspatial transcriptomics, allows for simultaneous visualization of multiple\nbiomarkers in a single section, enhancing morphological data with molecular and\nspatial information. This provides a more comprehensive view of the tissue\nmicroenvironment, cellular interactions, and disease mechanisms - crucial for\nunderstanding disease progression, prognosis, and treatment response. However,\nthe extensive data from multiplexed imaging necessitates sophisticated\ncomputational methods for preprocessing, segmentation, feature extraction, and\nspatial analysis. These tools are vital for managing large, multidimensional\ndatasets, converting raw imaging data into actionable insights. By automating\nlabor-intensive tasks and enhancing reproducibility and accuracy, computational\ntools are pivotal in diagnostics and research. This review explores the current\nlandscape of multiplexed imaging in pathology, detailing workflows and key\ntechnologies like PathML, an AI-powered platform that streamlines image\nanalysis, making complex dataset interpretation accessible for clinical and\nresearch settings.",
      "generated_abstract": "development of next-generation sequencing technologies has\ninduced a surge in the number of genome-wide association studies (GWAS)\nresults. Although the number of genetic variants associated with complex\ndiseases has increased, the performance of GWAS has remained poor, primarily\ndue to the scarcity of phenotypic information. In this paper, we propose a\nnovel method for integrating phenotypic information and GWAS data to enhance\nthe performance of GWAS. Specifically, we first integrate phenotypic and\ngenetic information into a latent variable framework, and then construct a\nlatent variable model to model the relationship between genetic variants and\nphenotypes. Then, we propose a novel method based on the Markov chain Monte\nCarlo (MCMC) method to perform the inference of the latent variable model. The\nresults show that the proposed method significantly impro",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12781954887218044,
          "p": 0.2236842105263158,
          "f": 0.16267942120922155
        },
        "rouge-2": {
          "r": 0.005780346820809248,
          "p": 0.008928571428571428,
          "f": 0.0070175390887074595
        },
        "rouge-l": {
          "r": 0.12781954887218044,
          "p": 0.2236842105263158,
          "f": 0.16267942120922155
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/NC/2502.13729v2",
      "true_abstract": "Human and animal memory for sequentially presented items is well-documented\nto be more accurate for those at the beginning and end of the sequence,\nphenomena known as the primacy and recency effects, respectively. By contrast,\nartificial neural network (ANN) models are typically designed with a memory\nthat decays monotonically over time. Accordingly, ANNs are expected to show the\nrecency effect but not the primacy effect. Contrary to this theoretical\nexpectation, however, the present study reveals a counterintuitive finding: a\nrecently developed ANN architecture, called structured state-space models,\nexhibits the primacy effect when trained and evaluated on a synthetic task that\nmirrors psychological memory experiments. Given that this model was originally\ndesigned for recovering neuronal activity patterns observed in biological\nbrains, this result provides a novel perspective on the psychological primacy\neffect while also posing a non-trivial puzzle for the current theories in\nmachine learning.",
      "generated_abstract": "This paper presents a novel method for estimating the posterior\ndistribution of a continuous-valued stochastic variable based on a single\nobservation. The method uses a novel Bayesian mixture model, which is a general\napproach for estimating posterior distributions based on a single observation.\nThis approach enables more efficient inference in scenarios where the\nprobability distribution of the stochastic variable is unknown. The proposed\nmethod is demonstrated through a case study on the distribution of the\nposterior distribution of a stochastic variable in the context of\nnon-Gaussian and non-linear stochastic processes. The proposed method is\napplied to a real-world example from the biomedical field.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11428571428571428,
          "p": 0.21818181818181817,
          "f": 0.14999999548828136
        },
        "rouge-2": {
          "r": 0.02877697841726619,
          "p": 0.047619047619047616,
          "f": 0.03587443476603251
        },
        "rouge-l": {
          "r": 0.10476190476190476,
          "p": 0.2,
          "f": 0.1374999954882814
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.10251v1",
      "true_abstract": "Large language models based on transformer architectures have become integral\nto state-of-the-art natural language processing applications. However, their\ntraining remains computationally expensive and exhibits instabilities, some of\nwhich are expected to be caused by finite-precision computations. We provide a\ntheoretical analysis of the impact of round-off errors within the forward pass\nof a transformer architecture which yields fundamental bounds for these\neffects. In addition, we conduct a series of numerical experiments which\ndemonstrate the practical relevance of our bounds. Our results yield concrete\nguidelines for choosing hyperparameters that mitigate round-off errors, leading\nto more robust and stable inference.",
      "generated_abstract": "We give an algebraic formulation of the theory of regularity for linear\nlinear partial differential equations. Our approach is based on the notion of\n``non-degeneracy'' of the coefficients, and relies on the use of the\nH\\\"ormander-Kohn-Riesz Theorem, which we extend to non-constant coefficients.\nThis approach provides a unified and more general treatment of the subject,\nunlike the more recent works which are restricted to constant coefficients.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17721518987341772,
          "p": 0.2978723404255319,
          "f": 0.22222221754472168
        },
        "rouge-2": {
          "r": 0.030927835051546393,
          "p": 0.04918032786885246,
          "f": 0.03797467880387818
        },
        "rouge-l": {
          "r": 0.1518987341772152,
          "p": 0.2553191489361702,
          "f": 0.19047618579869
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/SP/2503.07435v1",
      "true_abstract": "The adoption of Millimeter-Wave (mmWave) radar devices for human sensing,\nparticularly gait recognition, has recently gathered significant attention due\nto their efficiency, resilience to environmental conditions, and\nprivacy-preserving nature. In this work, we tackle the challenging problem of\nOpen-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlike\nmost existing research, which assumes a closed-set scenario, our work considers\nthe more realistic open-set case, where unknown subjects might be present at\ninference time, and should be correctly recognized by the system. Point clouds\nare well-suited for edge computing applications with resource constraints, but\nare more significantly affected by noise and random fluctuations than other\nrepresentations, like the more common micro-Doppler signature. This is the\nfirst work addressing open-set gait recognition with sparse point cloud data.\nTo do so, we propose a novel neural network architecture that combines\nsupervised classification with unsupervised reconstruction of the point clouds,\ncreating a robust, rich, and highly regularized latent space of gait features.\nTo detect unknown subjects at inference time, we introduce a probabilistic\nnovelty detection algorithm that leverages the structured latent space and\noffers a tunable trade-off between inference speed and prediction accuracy.\nAlong with this paper, we release mmGait10, an original human gait dataset\nfeaturing over five hours of measurements from ten subjects, under varied\nwalking modalities. Extensive experimental results show that our solution\nattains F1-Score improvements by 24% over state-of-the-art methods, on average,\nand across multiple openness levels.",
      "generated_abstract": "This paper introduces a novel deep learning based method for the\nconvolutional key encoder (CKE) of the 3D semantic segmentation task in\nsitu-sensing. The proposed method is based on the 3D convolutional neural\nnetwork (3D CNN) with the depth-first search tree (DFST) structure, and\ncombined with the multi-scale feature pooling strategy. The proposed method\nachieves a significant improvement in both the segmentation accuracy and the\nobject detection performance compared with the conventional 3D CKE method.\nAdditionally, the proposed method can also be applied to other 3D semantic\nsegmentation tasks. This study provides a method for the 3D CKE of the\nsitu-sensing task, and it is expected that the proposed method can be applied\nto various 3D semantic segmentation tasks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11299435028248588,
          "p": 0.29411764705882354,
          "f": 0.16326530211212004
        },
        "rouge-2": {
          "r": 0.012987012987012988,
          "p": 0.030927835051546393,
          "f": 0.018292678761340924
        },
        "rouge-l": {
          "r": 0.096045197740113,
          "p": 0.25,
          "f": 0.13877550619375273
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.physics/app-ph/2503.08941v1",
      "true_abstract": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors.",
      "generated_abstract": "tudy, we demonstrate that the microscopic spin-dependent\ncharacteristics of the spin-orbit interaction in bulk and surface Mott insulators\ncan be controlled by the magnitude of the magnetic anisotropy energy (MAE)\nbetween the $c$ and $a$ sublattices. Specifically, we demonstrate that the\nanisotropic spin-orbit interaction can be tuned by varying the magnitude of\nthe MAE between the $c$ and $a$ sublattices, and the magnitude of the\nrotational invariance of the MAE can be tuned by varying the MAE between the\n$c$ and $b$ sublattices. In addition, we show that the anisotropic spin-orbit\ninteraction can be further tuned by varying the magnitude of the MAE\nbetween the $c$ and $b$ sublattices. This study establishes that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08433734939759036,
          "p": 0.15217391304347827,
          "f": 0.10852712719427939
        },
        "rouge-2": {
          "r": 0.009615384615384616,
          "p": 0.015384615384615385,
          "f": 0.011834314792901302
        },
        "rouge-l": {
          "r": 0.07228915662650602,
          "p": 0.13043478260869565,
          "f": 0.09302325122528717
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.08885v1",
      "true_abstract": "Quasilinear systems with piecewise constant arguments of generalized type are\nunder investigation from the asymptotic point of view. The systems have\ndiscontinuous right-hand sides which are identified via a discrete-time map. It\nis rigorously proved that homoclinic and heteroclinic solutions are generated,\nand they are taken into account in the functional sense. The Banach fixed point\ntheorem is used for the verification. The hyperbolic set of solutions is also\ndiscussed, and an example supporting the theoretical findings is provided.",
      "generated_abstract": "We study a problem from the classical theory of non-commutative\ncomplex analysis. In this paper we show that the class of non-commutative\nRiemann integrable functions on $\\mathbb{R}$ is equivalent to the class of\nHilbert-Schmidt functions on $\\mathbb{R}$. This equivalence is established by\nmeans of the Weyl's inequality, which is a generalization of the classical\nWeyl's inequality for complex analysis.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13114754098360656,
          "p": 0.20512820512820512,
          "f": 0.15999999524200015
        },
        "rouge-2": {
          "r": 0.01282051282051282,
          "p": 0.02,
          "f": 0.015624995239259263
        },
        "rouge-l": {
          "r": 0.11475409836065574,
          "p": 0.1794871794871795,
          "f": 0.13999999524200016
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.14041v1",
      "true_abstract": "This study investigates the effectiveness of fiscal policies on household\nconsumption, disposable income, and the propensity to consume during the\nCOVID-19 pandemic across Croatia, Slovakia, and Poland. The purpose is to\nassess how variations in government debt, expenditures, revenue, and subsidies\ninfluenced household financial behaviors in response to economic shocks. Using\na Markov Switching VAR model across three regimes: initial impact, peak crisis,\nand recovery.This analysis captures changes in household consumption,\ndisposable income, and consumption propensities under different fiscal policy\nmeasures.\n  The findings reveal that the Slovak Republic exhibited the highest fiscal\neffectiveness, demonstrating effective government policies that stimulated\nconsumer spending and supported household income during the pandemic. Croatia\nalso showed positive outcomes, particularly in terms of income, although rising\ngovernment debt posed challenges to overall effectiveness. Conversely, Poland\nfaced significant obstacles, with its fiscal measures leading to lower\nconsumption and income outcomes, indicating limited policy efficacy.\n  Conclusions emphasize the importance of tailored fiscal measures, as their\neffectiveness varied across countries and economic contexts. Recommendations\ninclude reinforcing consumption-supportive policies, particularly during crisis\nperiods, to stabilize income and consumption expectations. This study\nunderscores the significance of targeted fiscal actions in promoting household\nresilience and economic stability, as exemplified by the successful approach\ntaken by the Slovak Republic.",
      "generated_abstract": "aper, we consider a large-scale market for public goods where the\nconsumer has heterogeneous preferences over the goods, but is uncertain about\nthe exact distribution of the goods. We consider the problem of designing\npublic goods policies that maximize social welfare under a set of assumptions.\nWe show that the optimal policy in this setting is a two-stage procedure in\nwhich the first stage is to allocate goods as much as possible to the most\npreferred goods, and the second stage is to allocate the remaining goods\naccording to a social welfare-maximizing allocation rule. We show that, in\npractice, this procedure is feasible in the sense that the expected value of\nthe social welfare over time is attained. Moreover, we show that the social\nwelfare can be computed as a special case of the expected value of the\ndistribution of the total consumption over time. Finally, we derive a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1044776119402985,
          "p": 0.1794871794871795,
          "f": 0.132075467046992
        },
        "rouge-2": {
          "r": 0.020512820512820513,
          "p": 0.03125,
          "f": 0.024767797072722794
        },
        "rouge-l": {
          "r": 0.09701492537313433,
          "p": 0.16666666666666666,
          "f": 0.12264150478284103
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2503.10435v1",
      "true_abstract": "When detecting anomalous sounds in complex environments, one of the main\ndifficulties is that trained models must be sensitive to subtle differences in\nmonitored target signals, while many practical applications also require them\nto be insensitive to changes in acoustic domains. Examples of such domain\nshifts include changing the type of microphone or the location of acoustic\nsensors, which can have a much stronger impact on the acoustic signal than\nsubtle anomalies themselves. Moreover, users typically aim to train a model\nonly on source domain data, which they may have a relatively large collection\nof, and they hope that such a trained model will be able to generalize well to\nan unseen target domain by providing only a minimal number of samples to\ncharacterize the acoustic signals in that domain. In this work, we review and\ndiscuss recent publications focusing on this domain generalization problem for\nanomalous sound detection in the context of the DCASE challenges on acoustic\nmachine condition monitoring.",
      "generated_abstract": "aper, we propose a novel deep learning-based model for the\nconversion of continuous audio signals into discrete speech signals. The\nproposed model consists of a transformer encoder and a speech decoder,\nwhich are trained jointly to convert continuous audio signals into discrete\nspeech signals. The proposed model can be trained end-to-end, without any\nadditional pre-processing, using only a few minutes of audio data. The model\nis trained on a dataset of 100 hours of audio data, including speech, noise,\nand music. The performance of the proposed model is evaluated using a\nspeech-to-speech speech synthesis task, in which the synthesized speech\nagrees with the ground truth speech. The results demonstrate that the proposed\nmodel can convert continuous audio signals into discrete speech signals,\nachieving a mean speech-to-speech error of 0.14% (100",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1834862385321101,
          "p": 0.2702702702702703,
          "f": 0.21857923015557357
        },
        "rouge-2": {
          "r": 0.012738853503184714,
          "p": 0.01834862385321101,
          "f": 0.01503758914777701
        },
        "rouge-l": {
          "r": 0.1651376146788991,
          "p": 0.24324324324324326,
          "f": 0.19672130665830584
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.06178v1",
      "true_abstract": "Parasite quiescence is the ability for the pathogen to be inactive, with\nrespect to metabolism and infectiousness, for some amount of time and then\nbecome active (infectious) again. The population is thus composed of an\ninactive proportion, and an active part in which evolution and reproduction\ntakes place. In this paper, we investigate the effect of parasite quiescence on\nthe time to extinction of infectious disease epidemics. We build a\nSusceptible-Infected-Quiescent-Susceptible (SIQS) epidemiological model.\nHereby, host individuals infected by a quiescent parasite strain cannot\nrecover, but are not infectious. We particularly focus on stochastic effects.\nWe show that the quiescent state does not affect the reproduction number, but\nfor a wide range of parameters the model behaves as an SIS model at a slower\ntime scale, given by the fraction of time infected individuals are within the I\nstate (and not in the Q state). This finding, proven using a time scale\nargument and singular perturbation theory for Markov processes, is illustrated\nand validated by numerical experiments based on the quasi-steady state\ndistribution. We find here that the result even holds without a distinct time\nscale separation. Our results highlight the influence of quiescence as a\nbet-hedging strategy against disease stochastic extinction, and are relevant\nfor predicting infectious disease dynamics in small populations.",
      "generated_abstract": "The study of social behavior has become increasingly important in understanding\nthe role of social interaction in complex biological systems. In this paper, we\nintroduce a new model of a social behavior, the social behavioral oscillator,\nwhich is a dynamical system that exhibits a complex pattern of oscillatory\nbehavior. We discuss the theoretical properties of the model, including the\nfrequency and period of oscillation, as well as its generalizations. We also\nexplore how the model can be used to study the emergence of collective behavior\nin social systems. We conclude by discussing future research directions and\npotential applications of the model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14925373134328357,
          "p": 0.29850746268656714,
          "f": 0.19900497067993375
        },
        "rouge-2": {
          "r": 0.01932367149758454,
          "p": 0.041237113402061855,
          "f": 0.026315785128333474
        },
        "rouge-l": {
          "r": 0.12686567164179105,
          "p": 0.2537313432835821,
          "f": 0.1691542244112771
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.cond-mat/mes-hall/2503.10059v1",
      "true_abstract": "Surface structure affects the growth, shape and properties of nanoparticles.\nIn wet chemical syntheses, metal additives and surfactants are used to modify\nsurfaces and guide nanocrystal growth. To understand this process, it is\ncritical to understand how the surface structure is modified. However,\nmeasuring the type and arrangement of atoms at hard-soft interfaces on\nnanoscale surfaces, especially in the presence of surfactants, is extremely\nchallenging. Here, we determine the atomic structure of the hard-soft interface\nin a metallic nanoparticle by developing low-dose imaging conditions in\nfour-dimensional scanning transmission electron microscopy that are\npreferentially sensitive to surface adatoms. By revealing experimentally the\ncopper additives and bromide surfactant counterion at the surface of a gold\nnanocuboid and quantifying their interatomic distances, our direct, low-dose\nimaging method provides atomic-level understanding of chemically sophisticated\nnanomaterial surface structures. These measurements of the atomic structure of\nthe hard-soft interface provide the information necessary to understand and\nquantify surface chemistries and energies and their pivotal role in nanocrystal\ngrowth.",
      "generated_abstract": "al studies of metal-insulator transitions (MITs) have traditionally\nrelied on the use of local density approximation (LDA) or GGA functionals.\nHowever, in recent years, there has been a resurgence in the use of the\nprojected augmented plane-wave (PAPW) method, which has been shown to be\nconsistent with LDA and GGA results in the absence of charge-density-wave\n(CDW) orders, and to be more accurate in the presence of CDW orders. Here, we\nemploy the PAPW method to study the MIT of the metal-insulator transition (MITT)\nin the cuprate family of superconductors. We find that the MITT in the cuprates\ncan be understood as a hybridization-mediated transition from the metallic to\nthe insulating phase, which is characterized",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14018691588785046,
          "p": 0.19230769230769232,
          "f": 0.1621621572850257
        },
        "rouge-2": {
          "r": 0.03355704697986577,
          "p": 0.047619047619047616,
          "f": 0.03937007389019837
        },
        "rouge-l": {
          "r": 0.12149532710280374,
          "p": 0.16666666666666666,
          "f": 0.14054053566340413
        }
      }
    },
    {
      "paper_id": "cs.AR.cs/AR/2503.09975v1",
      "true_abstract": "Low-precision data types are essential in modern neural networks during both\ntraining and inference as they enhance throughput and computational capacity by\nbetter exploiting available hardware resources. Despite the incorporation of\nFP8 in commercially available neural network accelerators, a comprehensive\nexposition of its underlying mechanisms, along with rigorous performance and\naccuracy evaluations, is still lacking. In this work, we contribute in three\nsignificant ways. First, we analyze the implementation details and quantization\noptions associated with FP8 for inference on the Intel Gaudi AI accelerator.\nSecond, we empirically quantify the throughput improvements afforded by the use\nof FP8 at both the operator level and in end-to-end scenarios. Third, we assess\nthe accuracy impact of various FP8 quantization methods. Our experimental\nresults indicate that the Intel Gaudi 2 accelerator consistently achieves high\ncomputational unit utilization, frequently exceeding 90\\% MFU, while incurring\nan accuracy degradation of less than 1\\%.",
      "generated_abstract": "t a novel, scalable approach to large-scale audio-to-text\nrepresentation learning. Our approach builds upon the recent advances in\nmultimodal transformer architectures, with particular attention to the\nintegrated architecture of multi-modal transformer networks. This architecture\nenables the joint representation of audio and text, enabling effective\nrepresentation learning across multiple modalities. Our approach utilizes\nlarge-scale audio-text datasets, enabling scalable model training and\nrepresentation learning. We introduce a new methodology for model training,\nenabling efficient and scalable training of large-scale models. Additionally,\nwe propose a novel fine-tuning strategy, enabling the seamless integration of\npre-trained models with multimodal transformer architectures. Our approach\ndemonstrates superior performance in audio-to-text representation learning,\ncompared to existing methods. Our methodology and approach are",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12962962962962962,
          "p": 0.2,
          "f": 0.15730336601439227
        },
        "rouge-2": {
          "r": 0.014084507042253521,
          "p": 0.018691588785046728,
          "f": 0.016064252126902673
        },
        "rouge-l": {
          "r": 0.12037037037037036,
          "p": 0.18571428571428572,
          "f": 0.1460674109582125
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2502.20177v1",
      "true_abstract": "The paper extends in two directions the work of \\cite{Plackett77} who studied\nhow, in a $2\\times 2$ table, the likelihood of the column totals depends on the\nodds ratio. First, we study the marginal likelihood of a single $R\\times C$\nfrequency table when only the marginal frequencies are observed and then\nconsider a collection of, say, $s$ $R\\times C$ tables, where only the row and\ncolumn totals can be observed, which is the basic framework which in\napplications of Ecological Inference. In the simpler context, we derive the\nlikelihood equations and show that the likelihood has a collection of local\nmaxima which, after a suitable rearrangement of the row and column categories,\nexhibit the strongest positive association compatible with the marginals, a\nkind of paradox, considering that the available data are so poor. Next, we\nderive the likelihood equations for the marginal likelihood of a collection of\ntow-way tables, under the assumption that they share the same row conditional\ndistributions and derive a necessary condition for the information matrix to be\nwell defined. We also describe a Fisher-scoring algorithm for maximizing the\nmarginal likelihood which, however, can be used only if the number of available\nreplications reaches a given threshold.",
      "generated_abstract": "A simple, efficient, and interpretable method for estimating the\ndistribution of a random variable is presented. The method is based on a\ncombination of regression and moment-generating functions, and is illustrated\nby using it to estimate the distribution of the number of significant words in\na document. The method is shown to be more flexible than commonly used\nprocedures and to yield more accurate results than a more restrictive\nprocedure.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11016949152542373,
          "p": 0.2765957446808511,
          "f": 0.1575757535015612
        },
        "rouge-2": {
          "r": 0.03488372093023256,
          "p": 0.09230769230769231,
          "f": 0.05063290741156185
        },
        "rouge-l": {
          "r": 0.09322033898305085,
          "p": 0.23404255319148937,
          "f": 0.13333332925913696
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.05435v1",
      "true_abstract": "Teacher-forcing training for audio captioning usually leads to exposure bias\ndue to training and inference mismatch. Prior works propose the contrastive\nmethod to deal with caption degeneration. However, the contrastive method\nignores the temporal information when measuring similarity across acoustic and\nlinguistic modalities, leading to inferior performance. In this work, we\ndevelop the temporal-similarity score by introducing the unbiased sliced\nWasserstein RBF (USW-RBF) kernel equipped with rotary positional embedding to\naccount for temporal information across modalities. In contrast to the\nconventional sliced Wasserstein RBF kernel, we can form an unbiased estimation\nof USW-RBF kernel via Monte Carlo estimation. Therefore, it is well-suited to\nstochastic gradient optimization algorithms, and its approximation error\ndecreases at a parametric rate of $\\mathcal{O}(L^{-1/2})$ with $L$ Monte Carlo\nsamples. Additionally, we introduce an audio captioning framework based on the\nunbiased sliced Wasserstein kernel, incorporating stochastic decoding methods\nto mitigate caption degeneration during the generation process. We conduct\nextensive quantitative and qualitative experiments on two datasets, AudioCaps\nand Clotho, to illustrate the capability of generating high-quality audio\ncaptions. Experimental results show that our framework is able to increase\ncaption length, lexical diversity, and text-to-audio self-retrieval accuracy.",
      "generated_abstract": "r presents a novel approach to improving the performance of a\nbi-directional wireless system, with a focus on improving the quality of\nservice (QoS) for both directions of the channel. The proposed methodology\nintegrates the use of artificial intelligence (AI) and deep learning (DL) to\nimprove the classification performance of the channel state information (CSI).\nBy using DL, the proposed methodology provides a better classification of the\nchannel state information (CSI), resulting in a significant improvement in the\nQoS for both directions of the wireless system. In addition, the proposed\nmethodology also addresses the issues of complexity and computational\ncomplexity in traditional methods for channel estimation. Finally, the\nproposed methodology is applied to the FDD scenario of a 5G multi-user\nwireless system. The results demonstrate that the proposed methodology\nsignificantly improves the performance of the bi-direction",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12307692307692308,
          "p": 0.21333333333333335,
          "f": 0.1560975563355147
        },
        "rouge-2": {
          "r": 0.005649717514124294,
          "p": 0.009009009009009009,
          "f": 0.006944439707034482
        },
        "rouge-l": {
          "r": 0.11538461538461539,
          "p": 0.2,
          "f": 0.14634145877453913
        }
      }
    },
    {
      "paper_id": "cs.SI.q-bio/PE/2502.12847v1",
      "true_abstract": "Understanding how cognitive and social mechanisms shape the evolution of\ncomplex artifacts such as songs is central to cultural evolution research.\nSocial network topology (what artifacts are available?), selection (which are\nchosen?), and reproduction (how are they copied?) have all been proposed as key\ninfluencing factors. However, prior research has rarely studied them together\ndue to methodological challenges. We address this gap through a controlled\nnaturalistic paradigm whereby participants (N=2,404) are placed in networks and\nare asked to iteratively choose and sing back melodies from their neighbors. We\nshow that this setting yields melodies that are more complex and more pleasant\nthan those found in the more-studied linear transmission setting, and exhibits\nrobust differences across topologies. Crucially, these differences are\ndiminished when selection or reproduction bias are eliminated, suggesting an\ninteraction between mechanisms. These findings shed light on the interplay of\nmechanisms underlying the evolution of cultural artifacts.",
      "generated_abstract": "In this paper, we propose a novel method for modeling and analyzing\nthe dynamics of biological networks by integrating an evolutionary\ndynamics model with a stochastic functional connectivity model. We first\nintroduce a method to generate functional connectivity matrices from network\ndynamics data. Then, we propose an algorithm to integrate the evolutionary\ndynamics model with the functional connectivity model, and demonstrate that the\nalgorithm is effective in analyzing a variety of network models. We apply the\nalgorithm to analyze the dynamics of a complex human brain network, and show\nthat it is able to capture the functional connectivity patterns and their\nevolutionary dynamics in this network.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15454545454545454,
          "p": 0.29310344827586204,
          "f": 0.2023809478599774
        },
        "rouge-2": {
          "r": 0.006896551724137931,
          "p": 0.010869565217391304,
          "f": 0.008438813815452476
        },
        "rouge-l": {
          "r": 0.11818181818181818,
          "p": 0.22413793103448276,
          "f": 0.15476190024092987
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.09678v1",
      "true_abstract": "For the first time, quality distribution of trees is introduced in a tree\ngrowth model. Consequently, the effects of quality thinning on stand\ndevelopment can be investigated. Quality thinning improves the financial return\nin all cases studied, but the effect is small. Rotation ages, timber stocks and\nmaturity diameters are not much affected by quality thinning. Bare land\nvaluation neither changes the contribution of the quality thinning. The reason\nfor the small effect apparently lies in the value development of individual\ntrees. The relative value development of small pulpwood trunks is large, since\nthe harvesting expense per volume unit is reduced along with size increment.\nSuch trees are not feasible objects for quality thinning, unless quality\ncorrelates with growth rate. Another enhanced stage of value development is\nwhen pulpwood trunks turn to sawlog trunks. For large pulpwood trunks, quality\nthinning is feasible. Existing sawlog content in trees dilutes the effect of\nquality thinning on the financial return. The results change if the growth rate\nis positively correlated with quality, quality thinning becoming feasible in\nall commercial diameter classes.",
      "generated_abstract": "This paper introduces a novel approach to quantify the impact of financial\nfacilities on the business environment. The proposed methodology uses\nstochastic volatility models to estimate the probability of default of\nportfolio companies. It then integrates the estimated default probability\ndistributions with a real-valued business impact function to determine the\ncorresponding impact on a target firm. The methodology is applied to analyze\nthe impact of four financial facilities on the business environment of\nBrazil's manufacturing sector. The results highlight the importance of\nfinancial intermediaries in supporting businesses and fostering growth in\neconomic activity.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1262135922330097,
          "p": 0.22033898305084745,
          "f": 0.16049382252934016
        },
        "rouge-2": {
          "r": 0.0125,
          "p": 0.023809523809523808,
          "f": 0.01639343810803672
        },
        "rouge-l": {
          "r": 0.1262135922330097,
          "p": 0.22033898305084745,
          "f": 0.16049382252934016
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.06009v1",
      "true_abstract": "In this paper, we investigate one of the most fundamental nonconvex learning\nproblems, ReLU regression, in the Differential Privacy (DP) model. Previous\nstudies on private ReLU regression heavily rely on stringent assumptions, such\nas constant bounded norms for feature vectors and labels. We relax these\nassumptions to a more standard setting, where data can be i.i.d. sampled from\n$O(1)$-sub-Gaussian distributions. We first show that when $\\varepsilon =\n\\tilde{O}(\\sqrt{\\frac{1}{N}})$ and there is some public data, it is possible to\nachieve an upper bound of $\\Tilde{O}(\\frac{d^2}{N^2 \\varepsilon^2})$ for the\nexcess population risk in $(\\epsilon, \\delta)$-DP, where $d$ is the dimension\nand $N$ is the number of data samples. Moreover, we relax the requirement of\n$\\epsilon$ and public data by proposing and analyzing a one-pass mini-batch\nGeneralized Linear Model Perceptron algorithm (DP-MBGLMtron). Additionally,\nusing the tracing attack argument technique, we demonstrate that the minimax\nrate of the estimation error for $(\\varepsilon, \\delta)$-DP algorithms is lower\nbounded by $\\Omega(\\frac{d^2}{N^2 \\varepsilon^2})$. This shows that\nDP-MBGLMtron achieves the optimal utility bound up to logarithmic factors.\nExperiments further support our theoretical results.",
      "generated_abstract": "em of predicting the future of a target agent in a dynamic\nenvironment is of increasing importance in a wide variety of applications.\nExisting approaches often assume that the target agent follows a fixed\ntransformation policy or that its trajectories are known. In this paper, we\nintroduce a novel framework that enables the prediction of the future of an\nagent in an unknown dynamic environment. Our approach is based on a\nrepresentation of the dynamics of the agent's state and action spaces as\nconditional random fields (CRFs), which allows us to model the dynamics\nnon-parametrically. Furthermore, we propose a novel method for learning\nCRFs. Unlike most existing approaches, our method does not require any\nassumptions on the dynamics or the shape of the target agent's state and action\nspaces. We show that our method can learn CRFs that outperform standard\nstate-of-the-art approaches. Finally",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18796992481203006,
          "p": 0.2808988764044944,
          "f": 0.22522522042163798
        },
        "rouge-2": {
          "r": 0.034482758620689655,
          "p": 0.04838709677419355,
          "f": 0.04026845151659894
        },
        "rouge-l": {
          "r": 0.16541353383458646,
          "p": 0.24719101123595505,
          "f": 0.19819819339461092
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2408.07497v1",
      "true_abstract": "This paper presents a method for accurately predicting the full distribution\nof stock returns, given a comprehensive set of 194 stock characteristics and\nmarket variables. Such distributions, learned from rich data using a machine\nlearning algorithm, are not constrained by restrictive model assumptions and\nallow the exploration of non-Gaussian, heavy-tailed data and their non-linear\ninteractions. The method uses a two-stage quantile neural network combined with\nspline interpolation. The results show that the proposed approach outperforms\nalternative models in terms of out-of-sample losses. Furthermore, we show that\nthe moments derived from such distributions can be useful as alternative\nempirical estimates in many cases, including mean estimation and forecasting.\nFinally, we examine the relationship between cross-sectional returns and\nseveral distributional characteristics. The results are robust to a wide range\nof US and international data.",
      "generated_abstract": "In this work, we propose a novel framework for the pricing of European call\ninstruments under stochastic volatility with arbitrary jumps. We use a\nmultilevel Monte Carlo method to obtain a highly accurate approximation of the\nstochastic volatility model, which enables us to efficiently obtain high-precision\npricing results. The methodology is applied to the pricing of European call\noptions on the S\\&P 500 index. The results show that the proposed methodology\nsignificantly outperforms existing pricing methods, with a mean absolute error\nof less than 1.69% for the S\\&P 500 index and less than 3.31% for the NASDAQ\nindex.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16,
          "p": 0.24242424242424243,
          "f": 0.19277107954710418
        },
        "rouge-2": {
          "r": 0.0390625,
          "p": 0.056818181818181816,
          "f": 0.04629629146776456
        },
        "rouge-l": {
          "r": 0.16,
          "p": 0.24242424242424243,
          "f": 0.19277107954710418
        }
      }
    },
    {
      "paper_id": "cs.IR.cs/IR/2503.09492v1",
      "true_abstract": "Cascade Ranking is a prevalent architecture in large-scale top-k selection\nsystems like recommendation and advertising platforms. Traditional training\nmethods focus on single-stage optimization, neglecting interactions between\nstages. Recent advances such as RankFlow and FS-LTR have introduced\ninteraction-aware training paradigms but still struggle to 1) align training\nobjectives with the goal of the entire cascade ranking (i.e., end-to-end\nrecall) and 2) learn effective collaboration patterns for different stages. To\naddress these challenges, we propose LCRON, which introduces a novel surrogate\nloss function derived from the lower bound probability that ground truth items\nare selected by cascade ranking, ensuring alignment with the overall objective\nof the system. According to the properties of the derived bound, we further\ndesign an auxiliary loss for each stage to drive the reduction of this bound,\nleading to a more robust and effective top-k selection. LCRON enables\nend-to-end training of the entire cascade ranking system as a unified network.\nExperimental results demonstrate that LCRON achieves significant improvement\nover existing methods on public benchmarks and industrial applications,\naddressing key limitations in cascade ranking training and significantly\nenhancing system performance.",
      "generated_abstract": "r explores the potential of audio-visual content for supporting\nrecent advancements in the area of computer vision. Audio-visual content\ngeneration, also known as A2V, has been a long-standing challenge in\nunderstanding audio-visual data. While several recent research efforts have\nattempted to address this challenge, most focus on capturing audio and video\ntrajectories, and only a few have explored the integration of audio-visual\ncontent. This paper explores the potential of audio-visual content for\nsupporting recent advancements in the area of computer vision. Audio-visual\ncontent generation, also known as A2V, has been a long-standing challenge in\nunderstanding audio-visual data. While several recent research efforts have\nattempted to address this challenge, most focus on capturing audio and video\ntrajectories, and only a few have explored the integration of audio-visual\ncontent",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09923664122137404,
          "p": 0.25,
          "f": 0.14207649866403907
        },
        "rouge-2": {
          "r": 0.005780346820809248,
          "p": 0.015873015873015872,
          "f": 0.008474572357442197
        },
        "rouge-l": {
          "r": 0.09923664122137404,
          "p": 0.25,
          "f": 0.14207649866403907
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2405.10920v1",
      "true_abstract": "We study the data-generating processes for factors expressed in return\ndifferences, which the literature on time-series asset pricing seems to have\noverlooked. For the factors' data-generating processes or long-short zero-cost\nportfolios, a meaningful definition of returns is impossible; further, the\ncompounded market factor (MF) significantly underestimates the return\ndifference between the market and the risk-free rate compounded separately.\nSurprisingly, if MF were treated coercively as periodic-rebalancing long-short\n(i.e., the same as size and value), Fama-French three-factor (FF3) would be\neconomically unattractive for lacking compounding and irrelevant for suffering\nfrom the small \"size of an effect.\" Otherwise, FF3 might be misspecified if MF\nwere buy-and-hold long-short. Finally, we show that OLS with net returns for\nsingle-index models leads to inflated alphas, exaggerated t-values, and\noverestimated Sharpe ratios (SR); worse, net returns may lead to pathological\nalphas and SRs. We propose defining factors (and SRs) with non-difference\ncompound returns.",
      "generated_abstract": "y examines the determinants of credit risk in the U.S. retail banking\nmarket, focusing on the impact of credit risk metrics on credit risk premiums.\nOur findings suggest that credit risk metrics, such as Net Interest Income\n(NII) and Asset Quality (AQR), are strongly correlated with credit risk\npremiums, suggesting that the financial health of a bank is an important\nfactor influencing its credit risk premium. Additionally, the study explores\nthe impact of loan-to-value (LTV) ratio, credit score, and debt-to-income\nratio on credit risk premiums. Our findings suggest that these metrics have a\npositive and significant correlation with credit risk premiums, with LTV and\ncredit score having the highest impact. Furthermore, our analysis highlights\nthe importance of evaluating a bank's credit risk metrics to identify high",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13274336283185842,
          "p": 0.19230769230769232,
          "f": 0.1570680579951209
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12389380530973451,
          "p": 0.1794871794871795,
          "f": 0.14659685380663923
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/GR/2503.08061v2",
      "true_abstract": "Realistic hand manipulation is a key component of immersive virtual reality\n(VR), yet existing methods often rely on a kinematic approach or motion-capture\ndatasets that omit crucial physical attributes such as contact forces and\nfinger torques. Consequently, these approaches prioritize tight,\none-size-fits-all grips rather than reflecting users' intended force levels. We\npresent ForceGrip, a deep learning agent that synthesizes realistic hand\nmanipulation motions, faithfully reflecting the user's grip force intention.\nInstead of mimicking predefined motion datasets, ForceGrip uses generated\ntraining scenarios-randomizing object shapes, wrist movements, and trigger\ninput flows-to challenge the agent with a broad spectrum of physical\ninteractions. To effectively learn from these complex tasks, we employ a\nthree-phase curriculum learning framework comprising Finger Positioning,\nIntention Adaptation, and Dynamic Stabilization. This progressive strategy\nensures stable hand-object contact, adaptive force control based on user\ninputs, and robust handling under dynamic conditions. Additionally, a proximity\nreward function enhances natural finger motions and accelerates training\nconvergence. Quantitative and qualitative evaluations reveal ForceGrip's\nsuperior force controllability and plausibility compared to state-of-the-art\nmethods. The video presentation of our paper is accessible at\nhttps://youtu.be/lR-YAfninJw.",
      "generated_abstract": "vigation in dynamic environments requires an agent to be able to\nadapt its trajectory by learning from its experiences. This learning is\ninherently challenging due to the high dimensionality of trajectories and the\ncomplex interactions among the agent's states and the environment. To address\nthese issues, we propose a trajectory learning framework that uses a\ndiscrete-time dynamical system to model the dynamics of the agent and the\nenvironment, and then learns to predict the dynamics of the agent's future\ntrajectory. Our approach is inspired by the work on trajectory prediction in\nrobotics, where a learned dynamical model of the robot's dynamics is used to\npredict the robot's future trajectory. However, unlike the work on trajectory\nprediction, our approach is designed to learn a dynamical model for the\nenvironment. In particular, we design a neural network model that learns to\npredict the dynamics",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15436241610738255,
          "p": 0.2987012987012987,
          "f": 0.2035398185163287
        },
        "rouge-2": {
          "r": 0.011235955056179775,
          "p": 0.016666666666666666,
          "f": 0.013422813981353914
        },
        "rouge-l": {
          "r": 0.14093959731543623,
          "p": 0.2727272727272727,
          "f": 0.1858407034720809
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2503.08638v1",
      "true_abstract": "We tackle the task of long-form music generation--particularly the\nchallenging \\textbf{lyrics-to-song} problem--by introducing YuE, a family of\nopen foundation models based on the LLaMA2 architecture. Specifically, YuE\nscales to trillions of tokens and generates up to five minutes of music while\nmaintaining lyrical alignment, coherent musical structure, and engaging vocal\nmelodies with appropriate accompaniment. It achieves this through (1)\ntrack-decoupled next-token prediction to overcome dense mixture signals, (2)\nstructural progressive conditioning for long-context lyrical alignment, and (3)\na multitask, multiphase pre-training recipe to converge and generalize. In\naddition, we redesign the in-context learning technique for music generation,\nenabling versatile style transfer (e.g., converting Japanese city pop into an\nEnglish rap while preserving the original accompaniment) and bidirectional\ngeneration. Through extensive evaluation, we demonstrate that YuE matches or\neven surpasses some of the proprietary systems in musicality and vocal agility.\nIn addition, fine-tuning YuE enables additional controls and enhanced support\nfor tail languages. Furthermore, beyond generation, we show that YuE's learned\nrepresentations can perform well on music understanding tasks, where the\nresults of YuE match or exceed state-of-the-art methods on the MARBLE\nbenchmark. Keywords: lyrics2song, song generation, long-form, foundation model,\nmusic generation",
      "generated_abstract": "In the past decade, the advancements in deep learning have significantly\nchanged the way we perceive the world. However, the design of effective\naudio-visual representations is still a challenge. In this paper, we present\nan audio-visual representation that leverages both the audio and the visual\ncues. We propose a novel architecture, named Audio-Visual Embedding (AVE), that\ncaptures the audio and the visual information jointly. The architecture is\ndesigned to be robust to both low-quality and high-quality audio and visual\ninputs. Additionally, AVE introduces a multi-scale fusion mechanism to enhance\nthe representation. Experimental results show that AVE significantly outperforms\nstate-of-the-art approaches in various audio-visual tasks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1232876712328767,
          "p": 0.25,
          "f": 0.16513761025502915
        },
        "rouge-2": {
          "r": 0.005291005291005291,
          "p": 0.01020408163265306,
          "f": 0.006968636617662459
        },
        "rouge-l": {
          "r": 0.1095890410958904,
          "p": 0.2222222222222222,
          "f": 0.14678898640181817
        }
      }
    },
    {
      "paper_id": "cs.AI.stat/OT/2412.14222v1",
      "true_abstract": "In recent years, data science agents powered by Large Language Models (LLMs),\nknown as \"data agents,\" have shown significant potential to transform the\ntraditional data analysis paradigm. This survey provides an overview of the\nevolution, capabilities, and applications of LLM-based data agents,\nhighlighting their role in simplifying complex data tasks and lowering the\nentry barrier for users without related expertise. We explore current trends in\nthe design of LLM-based frameworks, detailing essential features such as\nplanning, reasoning, reflection, multi-agent collaboration, user interface,\nknowledge integration, and system design, which enable agents to address\ndata-centric problems with minimal human intervention. Furthermore, we analyze\nseveral case studies to demonstrate the practical applications of various data\nagents in real-world scenarios. Finally, we identify key challenges and propose\nfuture research directions to advance the development of data agents into\nintelligent statistical analysis software.",
      "generated_abstract": "r investigates the problem of modeling the evolution of a social\nnetwork with multiple layers, where each layer is characterized by a different\nset of variables. We propose a novel model for this problem that combines the\nstructure of a directed graph with the random-walk approach to the evolution of\na social network. In particular, we introduce the notion of a network of\ninteraction spaces, which captures the evolution of interactions between\nindividuals at different layers of the social network. Our approach can be\nseen as a generalization of the random-walk model, and it is applicable to\ndifferent social networks, including social networks and the social web. We\nstudy the convergence of the model and its dynamics, and we show that our model\ncan capture the behavior of real-world networks. Furthermore, we apply our\nmodel to a dataset of Twitter users and we analyze their interactions, which\nare characterized by layers with different degrees of complexity",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.21428571428571427,
          "f": 0.18749999507812512
        },
        "rouge-2": {
          "r": 0.022388059701492536,
          "p": 0.021739130434782608,
          "f": 0.022058818530494212
        },
        "rouge-l": {
          "r": 0.12962962962962962,
          "p": 0.16666666666666666,
          "f": 0.1458333284114585
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.02899v1",
      "true_abstract": "Accurately discriminating progressive stages of Alzheimer's Disease (AD) is\ncrucial for early diagnosis and prevention. It often involves multiple imaging\nmodalities to understand the complex pathology of AD, however, acquiring a\ncomplete set of images is challenging due to high cost and burden for subjects.\nIn the end, missing data become inevitable which lead to limited sample-size\nand decrease in precision in downstream analyses. To tackle this challenge, we\nintroduce a holistic imaging feature imputation method that enables to leverage\ndiverse imaging features while retaining all subjects. The proposed method\ncomprises two networks: 1) An encoder to extract modality-independent\nembeddings and 2) A decoder to reconstruct the original measures conditioned on\ntheir imaging modalities. The encoder includes a novel {\\em ordinal contrastive\nloss}, which aligns samples in the embedding space according to the progression\nof AD. We also maximize modality-wise coherence of embeddings within each\nsubject, in conjunction with domain adversarial training algorithms, to further\nenhance alignment between different imaging modalities. The proposed method\npromotes our holistic imaging feature imputation across various modalities in\nthe shared embedding space. In the experiments, we show that our networks\ndeliver favorable results for statistical analysis and classification against\nimputation baselines with Alzheimer's Disease Neuroimaging Initiative (ADNI)\nstudy.",
      "generated_abstract": "r introduces a novel approach for the analysis of magnetic resonance\nimages (MRI) that integrates a multi-scale graph-based feature extraction with\na deep learning-based segmentation framework. The proposed method integrates\ngraph convolutional neural networks (GCN) with a self-attention mechanism to\ncapture the intrinsic structure of MRI data, which enhances feature\nrepresentation. Furthermore, we incorporate a segmentation module based on\ndeep learning to enhance image segmentation accuracy. We demonstrate the\neffectiveness of our proposed framework on a set of challenging MRI datasets\nincluding Alzheimer's Disease (AD), Parkinson's Disease (PD), and Multiple\nSclerosis (MS), where our approach outperforms the state-of-the-art methods in\nterms of segmentation accuracy and feature extraction. Our results highlight\nthe significant potential of GCN-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19444444444444445,
          "p": 0.34146341463414637,
          "f": 0.24778760599577104
        },
        "rouge-2": {
          "r": 0.02617801047120419,
          "p": 0.04504504504504504,
          "f": 0.03311257813231941
        },
        "rouge-l": {
          "r": 0.19444444444444445,
          "p": 0.34146341463414637,
          "f": 0.24778760599577104
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.02683v1",
      "true_abstract": "Streaming multi-talker speech translation is a task that involves not only\ngenerating accurate and fluent translations with low latency but also\nrecognizing when a speaker change occurs and what the speaker's gender is.\nSpeaker change information can be used to create audio prompts for a zero-shot\ntext-to-speech system, and gender can help to select speaker profiles in a\nconventional text-to-speech model. We propose to tackle streaming speaker\nchange detection and gender classification by incorporating speaker embeddings\ninto a transducer-based streaming end-to-end speech translation model. Our\nexperiments demonstrate that the proposed methods can achieve high accuracy for\nboth speaker change detection and gender classification.",
      "generated_abstract": "We propose a novel method for estimating the pitch and harmonic structure of\nmusic signals using a single RNN encoder-decoder architecture. Our model\nconsists of two stages: (1) the mel-spectrogram encoder produces a continuous\ntime series, and (2) a pitch-related latent space is generated by a\ntransformer-based decoder, which is then integrated with the mel-spectrogram\nto produce the pitch and harmonic structure. We further introduce an\ninverse-time-warping (ITW) method for the harmonic estimation, which is\ncomputationally efficient and can be used in a real-time setting. Experimental\nresults on a synthetic dataset demonstrate that our method outperforms existing\nstate-of-the-art methods in terms of both pitch and harmonic estimation\naccuracy.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3,
          "p": 0.2727272727272727,
          "f": 0.2857142807256236
        },
        "rouge-2": {
          "r": 0.05319148936170213,
          "p": 0.05102040816326531,
          "f": 0.052083328335503956
        },
        "rouge-l": {
          "r": 0.21428571428571427,
          "p": 0.19480519480519481,
          "f": 0.20408162766439922
        }
      }
    },
    {
      "paper_id": "cs.HC.q-bio/OT/2410.10513v1",
      "true_abstract": "Structuring data analysis projects, that is, defining the layout of files and\nfolders needed to analyze data using existing tools and novel code, largely\nfollows personal preferences. In this work, we look at the structure of several\ndata analysis project templates and find little structural overlap. We\nhighlight the parts that are similar between them, and propose guiding\nprinciples to keep in mind when one wishes to create a new data analysis\nproject. Finally, we present Kerblam!, a project management tool that can\nexpedite project data management, execution of workflow managers, and sharing\nof the resulting workflow and analysis outputs. We hope that, by following\nthese principles and using Kerblam!, the landscape of data analysis projects\ncan become more transparent, understandable, and ultimately useful to the wider\ncommunity.",
      "generated_abstract": "hic computing systems that mimic the structure and function of\nneural networks can be used to accelerate machine learning and AI. Neuromorphic\nhardware, however, faces significant challenges in designing efficient,\nlow-power circuits and algorithms. In this paper, we present a novel\nframework for designing neural circuits and algorithms using an open-source\nplatform, iMind, to overcome these challenges. The framework employs the\nhigh-level language HL4ML, a language designed for the simulation of\nneuromorphic hardware, to define neural circuits and algorithms. The framework\nalso supports the definition of custom hardware layers using the hardware\nprogramming language HP4ML. We showcase the use of the framework to design and\nevaluate a wide range of neural circuits and algorithms, including convolutional\nneural networks, random forests, and gradient descent, on various neuromorphic\nhardware platforms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19540229885057472,
          "p": 0.20481927710843373,
          "f": 0.1999999950027683
        },
        "rouge-2": {
          "r": 0.032520325203252036,
          "p": 0.03389830508474576,
          "f": 0.03319501574904089
        },
        "rouge-l": {
          "r": 0.1724137931034483,
          "p": 0.18072289156626506,
          "f": 0.17647058323806242
        }
      }
    },
    {
      "paper_id": "math.GM.math/GM/2502.14873v1",
      "true_abstract": "We examine the well-posedness of inverse eigenstrain problems for residual\nstress analysis from the perspective of the non-uniqueness of solutions,\nstructure of the corresponding null space and associated orthogonal range-null\ndecompositions. Through this process we highlight the existence of a trivial\nsolution to all inverse eigenstrain problems, with all other solutions\ndiffering from this trivial version by an unobservable null component. From one\nperspective, this implies that no new information can be gained though\neigenstrain analysis, however we also highlight the utility of the eigenstrain\nframework for enforcing equilibrium while estimating residual stress from\nincomplete experimental data. Two examples based on measured experimental data\nare given; one axisymmetric system involving ancient Roman medical tools, and\none more-general system involving an additively manufactured Inconel sample. We\nconclude by drawing a link between eigenstrain and reconstruction formulas\nrelated to strain tomography based on the Longitudinal Ray Transform (LRT).\nThrough this link, we establish a potential means for tomographic\nreconstruction of residual stress from LRT measurements.",
      "generated_abstract": "We prove that every $\\mathbb{Z}[\\alpha",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.01834862385321101,
          "p": 0.4,
          "f": 0.035087718459526025
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.01834862385321101,
          "p": 0.4,
          "f": 0.035087718459526025
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/PM/2502.19213v1",
      "true_abstract": "We consider an optimal investment-consumption problem for a\nutility-maximizing investor who has access to assets with different liquidity\nand whose consumption rate as well as terminal wealth are subject to\nlower-bound constraints. Assuming utility functions that satisfy standard\nconditions, we develop a methodology for deriving the optimal strategies in\nsemi-closed form. Our methodology is based on the generalized martingale\napproach and the decomposition of the problem into subproblems. We illustrate\nour approach by deriving explicit formulas for agents with power-utility\nfunctions and discuss potential extensions of the proposed framework.",
      "generated_abstract": "We consider the problem of constructing efficient trading strategies for a\nmarket with an infinite number of tradable assets. We show that, under certain\nconditions, the class of trading strategies that are optimal in the sense of\nthe optimal trading rule is not closed under arbitrary compositions of\narbitrage-free trading rules. We prove that this problem is NP-hard and\nprovide a polynomial-time approximation scheme for the problem of computing the\noptimal trading rule. We also show that the optimal trading rule is the\naggregation of the optimal trading rules of individual agents. Finally, we\nprove that the problem of computing the optimal trading rule is in NP and show\nthat this problem is polynomial-time solvable for the class of agents whose\ntrading rules are arbitrage-free.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.30434782608695654,
          "p": 0.38181818181818183,
          "f": 0.3387096724830906
        },
        "rouge-2": {
          "r": 0.05747126436781609,
          "p": 0.05434782608695652,
          "f": 0.055865916791611195
        },
        "rouge-l": {
          "r": 0.2608695652173913,
          "p": 0.32727272727272727,
          "f": 0.290322575708897
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.10008v1",
      "true_abstract": "We study whether ChatGPT and DeepSeek can extract information from the Wall\nStreet Journal to predict the stock market and the macroeconomy. We find that\nChatGPT has predictive power. DeepSeek underperforms ChatGPT, which is trained\nmore extensively in English. Other large language models also underperform.\nConsistent with financial theories, the predictability is driven by investors'\nunderreaction to positive news, especially during periods of economic downturn\nand high information uncertainty. Negative news correlates with returns but\nlacks predictive value. At present, ChatGPT appears to be the only model\ncapable of capturing economic news that links to the market risk premium.",
      "generated_abstract": "e the relationship between globalization and economic growth\nthrough a novel multilevel approach that integrates aggregate economic\ngrowth with firm-level growth in 176 countries from 1970 to 2015. Our findings\nshow that globalization has a significant and positive effect on economic\ngrowth, with a robust and significant coefficient of 0.17. However, the\nrelationship is moderated by the presence of a negative elasticity of substitution\nbetween globalization and growth. This suggests that globalization has a\ndiminishing impact on economic growth as countries become more integrated.\n  The findings highlight the importance of globalization in shaping economic\ngrowth. The positive and significant effect of globalization on economic growth\nsupports the view that the current globalization process is beneficial for\neconomic development. However, the negative elasticity of substitution\nhighlights the limitations",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18421052631578946,
          "p": 0.20588235294117646,
          "f": 0.19444443945987666
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.18421052631578946,
          "p": 0.20588235294117646,
          "f": 0.19444443945987666
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/TH/2502.09525v1",
      "true_abstract": "We study the task of learning Multi-Index Models (MIMs) with label noise\nunder the Gaussian distribution. A $K$-MIM is any function $f$ that only\ndepends on a $K$-dimensional subspace. We focus on well-behaved MIMs with\nfinite ranges that satisfy certain regularity properties. Our main contribution\nis a general robust learner that is qualitatively optimal in the Statistical\nQuery (SQ) model. Our algorithm iteratively constructs better approximations to\nthe defining subspace by computing low-degree moments conditional on the\nprojection to the subspace computed thus far, and adding directions with\nrelatively large empirical moments. This procedure efficiently finds a subspace\n$V$ so that $f(\\mathbf{x})$ is close to a function of the projection of\n$\\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional\nmoments do not help, we prove an SQ lower bound suggesting that no efficient\nlearner exists.\n  As applications, we provide faster robust learners for the following concept\nclasses:\n  * {\\bf Multiclass Linear Classifiers} We give a constant-factor approximate\nagnostic learner with sample complexity $N = O(d)\n2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N\n,d)$. This is the first constant-factor agnostic learner for this class whose\ncomplexity is a fixed-degree polynomial in $d$.\n  * {\\bf Intersections of Halfspaces} We give an approximate agnostic learner\nfor this class achieving 0-1 error $K \\tilde{O}(\\mathrm{OPT}) + \\epsilon$ with\nsample complexity $N=O(d^2) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational\ncomplexity $\\mathrm{poly}(N ,d)$. This is the first agnostic learner for this\nclass with near-linear error dependence and complexity a fixed-degree\npolynomial in $d$.\n  Furthermore, we show that in the presence of random classification noise, the\ncomplexity of our algorithm scales polynomially with $1/\\epsilon$.",
      "generated_abstract": "years, large language models (LLMs) have demonstrated state-of-the-art\nperformance in a variety of tasks, such as question answering, natural language\nprocessing, and generative modeling. While LLMs have shown impressive\nperformance, their internal structure remains a mystery. In this paper, we\npropose a novel framework to analyze the internal structure of LLMs. Specifically,\nwe develop a framework based on the entropy-based information bottleneck (EB\nmechanism) and propose a new metric to quantify the quality of LLMs. We\ndemonstrate that the EB-based metric can capture various features of LLMs,\nincluding their ability to generate plausible and coherent responses, and their\nability to generate meaningful contextualized answers. This framework offers\ninsights into the inner workings of LLMs, which can be used to enhance their",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09032258064516129,
          "p": 0.16666666666666666,
          "f": 0.11715480715673764
        },
        "rouge-2": {
          "r": 0.004347826086956522,
          "p": 0.008849557522123894,
          "f": 0.005830899371863712
        },
        "rouge-l": {
          "r": 0.09032258064516129,
          "p": 0.16666666666666666,
          "f": 0.11715480715673764
        }
      }
    },
    {
      "paper_id": "physics.data-an.nucl-th/2503.09415v1",
      "true_abstract": "SPARKX is an open-source Python package developed to analyze simulation data\nfrom heavy-ion collision experiments. By offering a comprehensive suite of\ntools, SPARKX simplifies data analysis workflows, supports multiple formats\nsuch as OSCAR2013, and integrates seamlessly with SMASH and JETSCAPE/X-SCAPE.\nThis paper describes SPARKX's architecture, features, and applications and\ndemonstrates its effectiveness through detailed examples and performance\nbenchmarks. SPARKX enhances productivity and precision in relativistic\nkinematics studies.",
      "generated_abstract": "t a new method to calculate the spin-orbit coupling, $\\lambda_s$, in\na broad range of nuclei, starting from the neutron-proton mass difference and\nthe effective mass of the nucleus. In the present work, we use the\nHamiltonian-based method to calculate $\\lambda_s$ for the nuclei 204Pb,\n204Sb, 216Pb, 216Sb, 230Y, 230Te, 240Tc, 244Cm, 248Cn, 250Ni, 252Cu, 254Np,\n254Pu, 256Cf, 258Bh, 258Nh, 260Cg, 260Te, 26",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1016949152542373,
          "p": 0.11764705882352941,
          "f": 0.1090909041173556
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1016949152542373,
          "p": 0.11764705882352941,
          "f": 0.1090909041173556
        }
      }
    },
    {
      "paper_id": "math.RT.math/RT/2503.06432v3",
      "true_abstract": "We prove that the Lusztig's $a$-function is bounded for any Coxeter group of\nfinite rank.",
      "generated_abstract": "We introduce a general framework to study the convergence of iterates of a\ngeneralized spectral method for the Schr\\\"odinger equation, where the\ndiscretization is based on a collection of commuting self-adjoint operators.\nThe convergence of the iterates is then analyzed in terms of the convergence of\nthe associated iterates of the operators. As a consequence, we obtain a new\nversion of the convergence theorem for the wave equation established by\nVasak in [10",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3333333333333333,
          "p": 0.1111111111111111,
          "f": 0.16666666291666676
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.3333333333333333,
          "p": 0.1111111111111111,
          "f": 0.16666666291666676
        }
      }
    },
    {
      "paper_id": "cs.LG.q-fin/CP/2412.10199v1",
      "true_abstract": "This document presents an in-depth examination of stock market sentiment\nthrough the integration of Convolutional Neural Networks (CNN) and Gated\nRecurrent Units (GRU), enabling precise risk alerts. The robust feature\nextraction capability of CNN is utilized to preprocess and analyze extensive\nnetwork text data, identifying local features and patterns. The extracted\nfeature sequences are then input into the GRU model to understand the\nprogression of emotional states over time and their potential impact on future\nmarket sentiment and risk. This approach addresses the order dependence and\nlong-term dependencies inherent in time series data, resulting in a detailed\nanalysis of stock market sentiment and effective early warnings of future\nrisks.",
      "generated_abstract": "In this paper, we propose a new approach to the prediction of the next stock\nsell signal in financial markets. The proposed approach is based on the\nanalysis of the historical data and the development of a model to predict the\nnext sell signal using a series of features and the correlation matrix of the\nlast 12 months of data. The proposed model is compared with the state-of-the-art\nmethods, including the BMA and the ANN models, in terms of the performance in\npredicting the next sell signal. The results show that the proposed model has\nbetter performance in predicting the next sell signal, compared to the BMA and\nthe ANN models, and that it is suitable for forecasting the next sell signal\nin the financial markets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18518518518518517,
          "p": 0.25,
          "f": 0.21276595255771855
        },
        "rouge-2": {
          "r": 0.02912621359223301,
          "p": 0.030927835051546393,
          "f": 0.029999995004500832
        },
        "rouge-l": {
          "r": 0.14814814814814814,
          "p": 0.2,
          "f": 0.17021276106835687
        }
      }
    },
    {
      "paper_id": "cs.AI.q-bio/NC/2502.21142v1",
      "true_abstract": "Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.",
      "generated_abstract": "tudy, we propose an efficient method for generating synthetic\nmolecular structures in a single step. Our approach consists of three\nsequential steps: (1) generating random chemical species, (2) constructing\nrandomly placed atoms and bonds, and (3) connecting them to form molecules.\nUsing this method, we can generate novel structures with high quality and\nefficiency, even for those that are challenging to synthesize. We show that\nour approach can generate molecules that mimic real-world biological systems,\nsuch as the A\u03b2 peptide, the protein that causes Alzheimer's disease, and the\ndrug LY294002, which is used to treat cancer and cardiovascular diseases. Our\nresults show that our method can generate molecules that mimic real-world\nbiological systems, such as the A\u03b2 peptide,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11038961038961038,
          "p": 0.20987654320987653,
          "f": 0.1446808465463107
        },
        "rouge-2": {
          "r": 0.014354066985645933,
          "p": 0.029411764705882353,
          "f": 0.019292600093465714
        },
        "rouge-l": {
          "r": 0.09740259740259741,
          "p": 0.18518518518518517,
          "f": 0.12765956995056602
        }
      }
    },
    {
      "paper_id": "math.LO.math/LO/2503.04471v1",
      "true_abstract": "We give a survey of cardinal charcteristics of the higher Cicho\\'n diagram\ndefined on the higher Baire space ${}^\\kappa\\kappa$ for $\\kappa$ regular with\n$2^{<\\kappa}=\\kappa$. Specifically, we will compare consistency proofs from the\nclassical Cicho\\'n diagram with various well-known forcing notions to similar\nconstructions generalised to the higher Cicho\\'n diagram. We are especially\ninterested in separation in a horizontal direction, that is, the consistency of\n$\\mathrm{add}(\\mathcal M_\\kappa)<\\mathrm{non}(\\mathcal M_\\kappa)$ and of\n$\\mathrm{cov}(\\mathcal M_\\kappa)<\\mathrm{cof}(\\mathcal M_\\kappa)$.\n  We will have a look at (higher analogues of) Cohen, Hechler, localisation,\neventually different, Sacks, random, Laver, Mathias and Miller forcing, and\ntheir effect on the cardinal characteristics of the higher Cicho\\'n diagram.",
      "generated_abstract": "Let $X$ be a topological space, $T$ a closed subset of $X$, and $A$ a set.\nWe show that the set of all continuous functions $F:X\\rightarrow A$ such that\n$F(T)\\subseteq T$ is a closed subset of $C(T)$ that is invariant under the\naction of $C(X)$ on $C(T)$. We give an example of a topological space $X$ such\nthat $C(X)$ does not have a closed invariant subset of functions. We also\nshow that the set of all continuous functions $F:X\\rightarrow A$ such that\n$F(T)\\subseteq T$ is not closed in $C(T)$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1527777777777778,
          "p": 0.2619047619047619,
          "f": 0.19298245148661142
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.029850746268656716,
          "f": 0.024844715637514937
        },
        "rouge-l": {
          "r": 0.1388888888888889,
          "p": 0.23809523809523808,
          "f": 0.17543859183748858
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/CP/2502.01495v1",
      "true_abstract": "We investigate the application of quantum cognition machine learning (QCML),\na novel paradigm for both supervised and unsupervised learning tasks rooted in\nthe mathematical formalism of quantum theory, to distance metric learning in\ncorporate bond markets. Compared to equities, corporate bonds are relatively\nilliquid and both trade and quote data in these securities are relatively\nsparse. Thus, a measure of distance/similarity among corporate bonds is\nparticularly useful for a variety of practical applications in the trading of\nilliquid bonds, including the identification of similar tradable alternatives,\npricing securities with relatively few recent quotes or trades, and explaining\nthe predictions and performance of ML models based on their training data.\nPrevious research has explored supervised similarity learning based on\nclassical tree-based models in this context; here, we explore the application\nof the QCML paradigm for supervised distance metric learning in the same\ncontext, showing that it outperforms classical tree-based models in high-yield\n(HY) markets, while giving comparable or better performance (depending on the\nevaluation metric) in investment grade (IG) markets.",
      "generated_abstract": "ration of Machine Learning (ML) and financial markets is a crucial\nprocess that aims to improve decision-making and trading strategies. However,\nthe complexity of financial data and the need to integrate multiple sources of\ninformation present challenges for the ML community. In this paper, we propose\na novel approach for financial sentiment analysis using a hybrid ensemble of\nconvolutional neural networks (CNNs) and long short-term memory (LSTM)\nnetworks. We explore the effectiveness of combining different models for\nsentiment analysis, as well as the benefits of combining different layers and\nsizes of convolutional and LSTM layers. Our approach is evaluated using the\nOanda VWAP dataset, demonstrating the effectiveness of combining different\nmodels and the benefits of combining different layers and sizes of convolutional\nand LSTM layers. The results show that combining different layers and\nsizes of convolutional and L",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16822429906542055,
          "p": 0.23684210526315788,
          "f": 0.1967213066188899
        },
        "rouge-2": {
          "r": 0.013071895424836602,
          "p": 0.01834862385321101,
          "f": 0.015267170713538052
        },
        "rouge-l": {
          "r": 0.1588785046728972,
          "p": 0.2236842105263158,
          "f": 0.18579234487025603
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2503.07837v1",
      "true_abstract": "To double the cellular population of ribosomes, a fraction of the active\nribosomes is allocated to synthesize ribosomal proteins. Subsequently, these\nribosomal proteins enter the ribosome self-assembly process, synthesizing new\nribosomes and forming the well-known ribosome autocatalytic subcycle.\nNeglecting ribosome lifetime and the duration of the self-assembly process, the\ndoubling rate of all cellular biomass can be equated with the fraction of\nribosomes allocated to synthesize an essential ribosomal protein times its\nsynthesis rate. However, ribosomes have a finite lifetime, and the assembly\nprocess has a finite duration. Furthermore, the number of ribosomes is known to\ndecrease with slow growth rates. The finite lifetime of ribosomes and the\ndecline in their numbers present a challenge in sustaining slow growth solely\nthrough controlling the allocation of ribosomes to synthesize more ribosomal\nproteins. When the number of ribosomes allocated per mRNA of an essential\nribosomal protein is approximately one, the resulting fluctuations in the\nproduction rate of new ribosomes increase, causing a potential risk that the\nactual production rate will fall below the ribosome death rate. Thus, in this\nregime, a significant risk of extinction of the ribosome population emerges. To\nmitigate this risk, we suggest that the ribosome translation speed is used as\nan alternative control parameter, which facilitates the maintenance of slow\ngrowth rates with a larger ribosome pool. We clarify the observed reduction in\ntranslation speed at harsh environments in E. coli and C. Glutamicum, explore\nother mitigation strategies, and suggest additional falsifiable predictions of\nour model.",
      "generated_abstract": "f this study is to investigate the role of the endocannabinoid\nsystem (ECS) in the regulation of immune response and in the control of\ninflammation. We analyzed the effect of chronic inflammation on ECS activity\nand the impact of cannabidiol (CBD) on the immune response. We also studied\nthe role of the ECS in the development of autoimmune diseases. To investigate\nthe role of the ECS in the regulation of immune response, we used a mouse\nmodel of acute inflammation. The mouse was injected with 100 micrograms of\nLPS (lipopolysaccharide), which activated the innate immune system. To\ninvestigate the role of the ECS in the control of inflammation, we used a mouse\nmodel of chronic inflammation. The mouse was injected with",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12686567164179105,
          "p": 0.3148148148148148,
          "f": 0.1808510597351744
        },
        "rouge-2": {
          "r": 0.014150943396226415,
          "p": 0.036585365853658534,
          "f": 0.020408159242908838
        },
        "rouge-l": {
          "r": 0.1044776119402985,
          "p": 0.25925925925925924,
          "f": 0.1489361661181531
        }
      }
    },
    {
      "paper_id": "math.NT.math/IT/2503.10201v1",
      "true_abstract": "The theories of automorphic forms and self-dual linear codes share many\nremarkable analogies. In both worlds there are functions invariant under an\naction of a group, notions of cusp forms and Hecke operators, also projections\nand lifts between different geni. It is then natural to ask if other important\nautomorphic objects or techniques could be introduced into coding theory. In\nthis article we propose a way to introduce the doubling method, an efficient\ntechnique used to construct and study $L$-functions. As a result, we prove the\nso-called doubling identity, which usually forms a base of many applications.\nHere we use it to solve an analogue of the \"basis problem\". Namely, we express\na cusp form as an explicit linear combination of complete weight enumerators of\nthe same type.",
      "generated_abstract": "n of a group with an action of a semisimple Lie group was introduced\nin the work of Drinfeld. In this paper, we study the group of unitary elements\nof a general complex semisimple Lie group and show that it is a semisimple\ngroup with a natural action of the group of unitary elements of the Lie group\nunder consideration. We also prove the existence of an irreducible unitary\nrepresentation of this group. In particular, we show that there is a unitary\nrepresentation of the group of unitary elements of a general complex semisimple\nLie group if and only if the Lie group under consideration has a finite\ndimensional complex representation. In addition, we show that there is a unitary\nrepresentation of the group of unitary elements of a general complex semisimple\nLie group if and only if the Lie group under consideration has a finite\ndimensional complex representation and the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.41304347826086957,
          "f": 0.26950354170313373
        },
        "rouge-2": {
          "r": 0.048,
          "p": 0.07407407407407407,
          "f": 0.05825242241257462
        },
        "rouge-l": {
          "r": 0.16842105263157894,
          "p": 0.34782608695652173,
          "f": 0.226950350213772
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.00209v1",
      "true_abstract": "We explore the influence of framing on decision-making, where some products\nare framed (e.g., displayed, recommended, endorsed, or labeled). We introduce a\nnovel choice function that captures observed variations in framed alternatives.\nBuilding on this, we conduct a comprehensive revealed preference analysis,\nemploying the concept of frame-dependent utility using both deterministic and\nprobabilistic data. We demonstrate that simple and intuitive behavioral\nprinciples characterize our frame-dependent random utility model (FRUM), which\noffers testable conditions even with limited data. Finally, we introduce a\nparametric model to increase the tractability of FRUM. We also discuss how to\nrecover the choice types in our framework.",
      "generated_abstract": "the problem of selecting a portfolio of financial assets with a\nportfolio loss that is in general non-convex. We develop a portfolio selection\nalgorithm that, to the best of our knowledge, is the first to achieve\nconvexity in the selection of the portfolio. We establish that the portfolio\nselection problem is convex if and only if the expected loss function is\nconvex. We show that the expected loss function is convex in the case of\nsimple random portfolios and that it is convex in the case of risk-neutral\nvalued portfolios. We also show that the expected loss function is convex in\nthe case of continuous-time portfolios and that it is convex in the case of\ndiscrete-time portfolios. We propose a novel loss function that is convex in\nthe case of simple random portfolios and risk-neutral valuation. We show that\nthis convex loss function is equivalent",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.189873417721519,
          "p": 0.3,
          "f": 0.23255813478757295
        },
        "rouge-2": {
          "r": 0.0297029702970297,
          "p": 0.03296703296703297,
          "f": 0.031249995013564166
        },
        "rouge-l": {
          "r": 0.189873417721519,
          "p": 0.3,
          "f": 0.23255813478757295
        }
      }
    },
    {
      "paper_id": "math.CO.math/GR/2503.07299v1",
      "true_abstract": "Let $\\varphi:V\\times V\\to W$ be a bilinear map of finite vector spaces $V$\nand $W$ over a finite field $\\mathbb{F}_q$. We present asymptotic bounds on the\nnumber of isomorphism classes of bilinear maps under the natural action of\n$\\mathrm{GL}(V)$ and $\\mathrm{GL}(W)$, when $\\dim(V)$ and $\\dim(W)$ are\nlinearly related.\n  As motivations and applications of the results, we present almost tight upper\nbounds on the number of $p$-groups of Frattini class $2$ as first studied by\nHigman (Proc. Lond. Math. Soc., 1960). Such bounds lead to answers for some\nopen questions by Blackburn, Neumann, and Venkataraman (Cambridge Tracts in\nMathematics, 2007). Further applications include sampling matrix spaces with\nthe trivial automorphism group, and asymptotic bounds on the number of\nisomorphism classes of finite cube-zero commutative algebras.",
      "generated_abstract": "e a unified treatment of the classical and non-classical theories\nof gravity in curved spacetime, with an emphasis on the quantization and\nnon-perturbative aspects of the matter content. We consider two classes of\ngravitational theories: the Einstein-Hilbert action with a cosmological constant,\nand the action of a general relativity-based theory of matter. The first\nclass of theories is usually described as the classical gravity background, and\nis related to the metric of the spacetime through the Einstein tensor. The\nsecond class of theories is non-classical, and is defined by the action of a\ngeneral relativity-based matter field. We show that the gravitational and\nmatter sectors are naturally separated into two distinct sectors, with a\nnon-trivial interplay between them. We identify the gravitational and matter\nsector as two distinct sectors of the same theory,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20224719101123595,
          "p": 0.2465753424657534,
          "f": 0.2222222172709954
        },
        "rouge-2": {
          "r": 0.036036036036036036,
          "p": 0.03508771929824561,
          "f": 0.03555555055644515
        },
        "rouge-l": {
          "r": 0.1348314606741573,
          "p": 0.1643835616438356,
          "f": 0.14814814319692138
        }
      }
    },
    {
      "paper_id": "quant-ph.cs/IT/2503.09012v1",
      "true_abstract": "The thought experiment of Maxwell's demon highlights the effect of side\ninformation in thermodynamics. In this paper, we present an axiomatic treatment\nof a quantum Maxwell's demon, by introducing a resource-theoretic framework of\nquantum thermodynamics in the presence of quantum side information. Under\nminimal operational assumptions that capture the demon's behaviour, we derive\nthe one-shot work costs of preparing, as well as erasing, a thermodynamic\nsystem whose coupling with the demon's mind is described by a bipartite quantum\nstate. With trivial Hamiltonians, these work costs are precisely captured by\nthe smoothed conditional min- and max-entropies, respectively, thus providing\noperational interpretations for these one-shot information-theoretic quantities\nin microscopic thermodynamics. An immediate, information-theoretic implication\nof our results is an affirmative proof of the conjectured maximality of the\nconditional max-entropy among all axiomatically plausible conditional\nentropies, complementing the recently established minimality of the conditional\nmin-entropy. We then generalize our main results to the setting with nontrivial\nHamiltonians, wherein the work costs of preparation and erasure are captured by\na generalized type of mutual information. Finally, we present a macroscopic\nsecond law of thermodynamics in the presence of quantum side information, in\nterms of a conditional version of the Helmholtz free energy. Our results extend\nthe conceptual connection between thermodynamics and quantum information theory\nby refining the axiomatic common ground between the two theories and revealing\nfundamental insights of each theory in light of the other.",
      "generated_abstract": "er the problem of estimating the expectation of a stochastic\n(random) function of two independent random variables. This problem is well\nknown to be NP-hard and has been studied in the context of quantum\ninformation theory, where it is known as the quantum expectation problem. We\npropose a new algorithm that, to the best of our knowledge, is the first to\nprovide an approximation guarantee for this problem. The algorithm is based on\nan approach known as the ``sparse sampling'' algorithm. This method has been\nwidely used in various applications, and we provide a brief review of the\nsparse sampling approach. We then proceed to describe our new algorithm.\nSpecifically, we propose a novel algorithm that consists of two parts. In the\nfirst part, we use the sparse sampling approach to generate a sample of\napproximately $n$ random variables from a distribution that is close to the\ntarget distribution. In the second part",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17037037037037037,
          "p": 0.27058823529411763,
          "f": 0.20909090434917366
        },
        "rouge-2": {
          "r": 0.0380952380952381,
          "p": 0.05755395683453238,
          "f": 0.04584526741324014
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.21176470588235294,
          "f": 0.16363635889462821
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ME/2503.02108v1",
      "true_abstract": "Generalized Bayesian Inference (GBI) provides a flexible framework for\nupdating prior distributions using various loss functions instead of the\ntraditional likelihoods, thereby enhancing the model robustness to model\nmisspecification. However, GBI often suffers the problem associated with\nintractable likelihoods. Kernelized Stein Discrepancy (KSD), as utilized in a\nrecent study, addresses this challenge by relying only on the gradient of the\nlog-likelihood. Despite this innovation, KSD-Bayes suffers from critical\npathologies, including insensitivity to well-separated modes in multimodal\nposteriors. To address this limitation, we propose a weighted KSD method that\nretains computational efficiency while effectively capturing multimodal\nstructures. Our method improves the GBI framework for handling intractable\nmultimodal posteriors while maintaining key theoretical properties such as\nposterior consistency and asymptotic normality. Experimental results\ndemonstrate that our method substantially improves mode sensitivity compared to\nstandard KSD-Bayes, while retaining robust performance in unimodal settings and\nin the presence of outliers.",
      "generated_abstract": "er the problem of inferring the parameter of a functional regression\nmodel from data. In this setting, the functional data are generated by a\nfunctional process $X(t)$, $t \\in \\mathbb{T}$, where $\\mathbb{T}$ is the unit\ncircle. The goal is to infer the parameter $\\theta$ of the functional regression\nmodel $Y(t) = X(t) + \\epsilon(t)$, $t \\in \\mathbb{T}$, where $\\epsilon(t)$ is a\nnoise process. This is a classical regression problem in functional statistics\nand has a long history in signal processing. In functional regression models,\nthe noise is often correlated with the functional data, which makes inference\nchallenging. In this work, we propose a new method based on the functional\nM-estimation principle to perform parameter inference in functional regression\nmodels. In particular, we propose a method that uses a modified version of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16071428571428573,
          "p": 0.24,
          "f": 0.1925133641797022
        },
        "rouge-2": {
          "r": 0.04225352112676056,
          "p": 0.05309734513274336,
          "f": 0.04705881859407972
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.21333333333333335,
          "f": 0.17112298984815136
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/LO/2503.06284v1",
      "true_abstract": "Isolation bugs, stemming especially from design-level defects, have been\nrepeatedly found in carefully designed and extensively tested production\ndatabases over decades. In parallel, various frameworks for modeling database\ntransactions and reasoning about their isolation guarantees have been\ndeveloped. What is missing however is a mathematically rigorous and systematic\nframework with tool support for formally verifying a wide range of such\nguarantees for all possible system behaviors. We present the first such\nframework, VerIso, developed within the theorem prover Isabelle/HOL. To\nshowcase its use in verification, we model the strict two-phase locking\nconcurrency control protocol and verify that it provides strict serializability\nisolation guarantee. Moreover, we show how VerIso helps identify isolation bugs\nduring protocol design. We derive new counterexamples for the TAPIR protocol\nfrom failed attempts to prove its claimed strict serializability. In\nparticular, we show that it violates a much weaker isolation level, namely,\natomic visibility.",
      "generated_abstract": "This paper presents a novel approach to the problem of detecting\nsubgraphs in graphs, which can be represented as a graph with multiple\nsubgraphs. We propose a novel algorithm, called GREP, which is based on a\nsimple graph transformation and can detect multiple subgraphs in a graph in\npolynomial time. GREP is simple to implement and runs in polynomial time on\nthe largest graph of size 100,000,000. We compare GREP to state-of-the-art\nalgorithms and show that GREP is competitive with them in terms of both\ncomputational efficiency and run-time.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09821428571428571,
          "p": 0.2037037037037037,
          "f": 0.1325301160923212
        },
        "rouge-2": {
          "r": 0.007042253521126761,
          "p": 0.012345679012345678,
          "f": 0.008968605239601008
        },
        "rouge-l": {
          "r": 0.09821428571428571,
          "p": 0.2037037037037037,
          "f": 0.1325301160923212
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.11183v3",
      "true_abstract": "An agent engages in sequential search. He does not directly observe the\nquality of the goods he samples, but he can purchase signals designed by profit\nmaximizing principal(s). We formulate the principal-agent relationship as a\nrepeated contracting problem within a stopping game and characterize the set of\nequilibrium payoffs. We show that when the agent's search cost falls below a\ngiven threshold, competition does not impact how much surplus is generated in\nequilibrium nor how the surplus is divided. In contrast, competition benefits\nthe agent at the expense of total surplus when the search cost exceeds that\nthreshold. Our results challenge the view that monopoly decreases market\nefficiency, and moreover, suggest that it generates the highest value of\ninformation for the agent.",
      "generated_abstract": "This paper explores the economic foundations of the idea that people will\nestablish a reputation for themselves by engaging in social interactions. We\nshow that, contrary to popular belief, the \"reputation problem\" has a\nmathematical foundation, and we develop a novel economic framework that\nexplains the observed phenomenon of reputation formation. We demonstrate that\nsocial interactions are necessary and sufficient for establishing a reputation\nfor one's character, and we show that reputation formation is a Nash bargaining\nproblem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1411764705882353,
          "p": 0.22641509433962265,
          "f": 0.1739130387471121
        },
        "rouge-2": {
          "r": 0.02586206896551724,
          "p": 0.041666666666666664,
          "f": 0.03191488889090156
        },
        "rouge-l": {
          "r": 0.12941176470588237,
          "p": 0.20754716981132076,
          "f": 0.15942028512392367
        }
      }
    },
    {
      "paper_id": "math.ST.q-fin/MF/2412.06343v1",
      "true_abstract": "We propose analytically tractable SDE models for correlation in financial\nmarkets. We study diffusions on the circle, namely the Brownian motion on the\ncircle and the von Mises process, and consider these as models for correlation.\nThe von Mises process was proposed in Kent (1975) as a probabilistic\njustification for the von Mises distribution which is widely used in Circular\nstatistics. The transition density of the von Mises process has been unknown,\nwe identify an approximate analytic transition density for the von Mises\nprocess. We discuss the estimation of these diffusion models and a stochastic\ncorrelation model in finance. We illustrate the application of the proposed\nmodel on real-data of equity-currency pairs.",
      "generated_abstract": "aper, we study the asymptotic behavior of the optimal investment\ndecision for a mean-variance portfolio subject to a finite horizon. We propose\nthe so-called mean-variance portfolio, which consists of a fractional part of\nthe portfolio and a continuous part with a fixed investment horizon. The\nportfolio is optimal under a given set of constraints on the fractional part. We\nprovide a sufficient condition for the portfolio to be optimal under the\nassumption that the fractional part is convex. Our analysis reveals that the\noptimal fractional part is the limiting value of the fractional part of a\ncontinuous-time Markov chain. We further provide an explicit formula for the\nlimit of the fractional part of the continuous-time Markov chain. We also\npropose a numerical method to compute the limit of the fractional part of the\ncontinuous-time Markov chain. In",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2153846153846154,
          "p": 0.22580645161290322,
          "f": 0.220472435947672
        },
        "rouge-2": {
          "r": 0.052083333333333336,
          "p": 0.049019607843137254,
          "f": 0.05050504550964237
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.20967741935483872,
          "f": 0.20472440445160905
        }
      }
    },
    {
      "paper_id": "physics.gen-ph.physics/gen-ph/2503.09604v1",
      "true_abstract": "The article studies the extension of the internal spaces of fermion and boson\nsecond quantized fields, described by the superposition of odd (for fermions)\nand even (for bosons) products of the operators $\\gamma^ {a}$, to strings and\nodd dimensional spaces.\\\\ For any symmetry $SO(d-1,1)$ of the internal spaces,\nit is the number of fermion fields (they appear in families and have their\nHermitian conjugated partners in a separate group) equal to the number of boson\nfields (they appear in two orthogonal groups), manifesting a kind of\nsupersymmetry, which differs from the usual supersymmetry.\\\\ The article\nsearches for the supersymmetry arising from extending the ``basis vectors'' of\nsecond quantized fermion and boson fields described in $d=2(2n+1)$ (in\nparticular $d=(13+1)$) either to strings or to odd-dimensional spaces\n($d=2(2n+1)+1$).",
      "generated_abstract": "r introduces a novel framework for the study of the quantum properties\nof the electromagnetic field in a rotating and curved spacetime background.\nThe framework is based on the concept of the quantum metric, which describes\nthe metric tensor in a curved background using quantum operators. We show how\nthis framework can be used to investigate the quantum properties of the electromagnetic\nfield in a rotating spacetime. We study the behaviour of the electromagnetic\nfield in the presence of an external magnetic field and an external electric\nfield, and we explore the effect of rotation on the electromagnetic field. We\nalso investigate the behaviour of the photon spectrum and polarization state\nas a function of the frequency of the electromagnetic field. Our results show\nthat the quantum metric allows us to study the quantum properties of the\nelectromagnetic field in a curved spacetime in a more accurate and\nquantitative manner than conventional methods. The quantum",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.15492957746478872,
          "f": 0.1486486436568665
        },
        "rouge-2": {
          "r": 0.026785714285714284,
          "p": 0.026785714285714284,
          "f": 0.026785709285715217
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.15492957746478872,
          "f": 0.1486486436568665
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.17883v1",
      "true_abstract": "The expected utility theorem of von Neumann and Morgenstern (1947) has been a\nmilestone in economics, describing rational behavior by two axioms on a weak\npreference on lotteries on a finite set of outcomes: the Independence Axiom and\nthe Continuity Axiom. For a weak preference fulfilling the Independence Axiom,\nI prove that continuity is equivalent to the existence of a set indifferent\nlotteries spanning a hyperplane.",
      "generated_abstract": "We show that the optimal allocation of labor and capital under perfect\ncapacity and complete information is linear. We also provide a necessary and\nsufficient condition for this linearity, and show that it does not hold in\ngeneral. Finally, we discuss the implications of our results for the\nclassification of the optimal allocation of labor and capital under perfect\ncapacity.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14893617021276595,
          "p": 0.1794871794871795,
          "f": 0.16279069271768537
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10638297872340426,
          "p": 0.1282051282051282,
          "f": 0.11627906481070871
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.17284v1",
      "true_abstract": "We test and study the variation in speech recognition of fine-tuned versions\nof the Whisper model on child, elderly and non-native Dutch speech from the\nJASMIN-CGN corpus. Our primary goal is to evaluate how speakers' age and\nlinguistic background influence Whisper's performance. Whisper achieves varying\nWord Error Rates (WER) when fine-tuned on subpopulations of specific ages and\nlinguistic backgrounds. Fine-tuned performance is remarkably better than\nzero-shot performance, achieving a relative reduction in WER of 81% for native\nchildren, 72% for non-native children, 67% for non-native adults, and 65% for\nnative elderly people. Our findings underscore the importance of training\nspeech recognition models like Whisper on underrepresented subpopulations such\nas children, the elderly, and non-native speakers.",
      "generated_abstract": "he advances in large language models (LLMs), many existing methods\nhave limitations in their ability to generate complex and diverse\nrepresentations. This is due to the inherent limitations in the LLM's processing\ncapabilities, including the inability to capture nuanced context and the\ninability to effectively model long-range dependencies. In this paper, we\npropose a novel approach that integrates the concept of transformer\nencoders with the concept of GPT-4o's transformer-style decoders. This approach\nenables the model to capture more nuanced contextual information and model\nlong-range dependencies more effectively, resulting in improved generation\nquality. Additionally, we propose a novel training method that utilizes\nself-supervised learning (SSL) to improve the LLM's representation learning\nability. Through extensive experiments, we demonstrate that our approach\noutperforms existing methods in generating diverse and high-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1282051282051282,
          "p": 0.12345679012345678,
          "f": 0.12578615852379277
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1282051282051282,
          "p": 0.12345679012345678,
          "f": 0.12578615852379277
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/OT/2404.08738v2",
      "true_abstract": "Air quality is a critical component of environmental health. Monitoring and\nanalysis of particulate matter with a diameter of 2.5 micrometers or smaller\n(PM2.5) plays a pivotal role in understanding air quality changes. This study\nfocuses on the application of a new bandpass bootstrap approach, termed the\nVariable Bandpass Periodic Block Bootstrap (VBPBB), for analyzing time series\ndata which provides modeled predictions of daily mean PM2.5 concentrations over\n16 years in Manhattan, New York, the United States. The VBPBB can be used to\nexplore periodically correlated (PC) principal components for this daily mean\nPM2.5 dataset. This method uses bandpass filters to isolate distinct PC\ncomponents from datasets, removing unwanted interference including noise, and\nbootstraps the PC components. This preserves the PC structure and permits a\nbetter understanding of the periodic characteristics of time series data. The\nresults of the VBPBB are compared against outcomes from alternative block\nbootstrapping techniques. The findings of this research indicate potential\ntrends of elevated PM2.5 levels, providing evidence of significant semi-annual\nand weekly patterns missed by other methods.",
      "generated_abstract": "In this paper, we develop a novel model for the estimation of the\ngeneralized Breusch-Godfrey (BG) screen test statistic for the presence of\ninteraction effects in linear regression models. We show that the BG test can\nbe cast as a likelihood ratio test with a flexible density. Using a\ntwo-step method, we derive the asymptotic distribution of the BG test statistic\nand the corresponding critical values. Numerical simulations demonstrate the\neffectiveness of the proposed method. Finally, we apply the proposed method to\nanalyze the interaction effects of an insurance company's underwriting\nstrategy and the economic effects of COVID-19.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1015625,
          "p": 0.2,
          "f": 0.13471502143950187
        },
        "rouge-2": {
          "r": 0.017857142857142856,
          "p": 0.03488372093023256,
          "f": 0.023622042765206377
        },
        "rouge-l": {
          "r": 0.09375,
          "p": 0.18461538461538463,
          "f": 0.12435232713898377
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.09062v1",
      "true_abstract": "Knowledge dissemination in educational settings is profoundly influenced by\nthe curse of knowledge, a cognitive bias that causes experts to underestimate\nthe challenges faced by learners due to their own in-depth understanding of the\nsubject. This bias can hinder effective knowledge transfer and pedagogical\neffectiveness, and may be exacerbated by inadequate instructor-student\ncommunication. To encourage more effective feedback and promote empathy, we\nintroduce TSConnect, a bias-aware, adaptable interactive MOOC (Massive Open\nOnline Course) learning system, informed by a need-finding survey involving 129\nstudents and 6 instructors. TSConnect integrates instructors, students, and\nArtificial Intelligence (AI) into a cohesive platform, facilitating diverse and\ntargeted communication channels while addressing previously overlooked\ninformation needs. A notable feature is its dynamic knowledge graph, which\nenhances learning support and fosters a more interconnected educational\nexperience. We conducted a between-subjects user study with 30 students\ncomparing TSConnect to a baseline system. Results indicate that TSConnect\nsignificantly encourages students to provide more feedback to instructors.\nAdditionally, interviews with 4 instructors reveal insights into how they\ninterpret and respond to this feedback, potentially leading to improvements in\nteaching strategies and the development of broader pedagogical skills.",
      "generated_abstract": "r proposes a new paradigm for the design of multi-agent systems\n(MASs) that are capable of adapting their policies to changing environments,\nas well as their internal states. Our approach is based on the idea of\nadaptive neural networks (ANNs), where the policy and state representations are\nderived from a self-organizing map (SOM). This approach allows us to use\nexisting SOM-based models to model the state of the system, enabling the use of\nexisting learning algorithms. We have implemented a simple ANN-based policy\nrepresentation for the MAS, and we show that it can learn to adapt its policy\nto changes in the environment while maintaining a high level of accuracy. We\nhave also implemented a multi-agent reinforcement learning (MARL) agent that\nis trained to learn to adapt its policy in the environment. We have evaluated\nthe performance of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12318840579710146,
          "p": 0.19767441860465115,
          "f": 0.15178570955516596
        },
        "rouge-2": {
          "r": 0.005376344086021506,
          "p": 0.007874015748031496,
          "f": 0.006389771535489355
        },
        "rouge-l": {
          "r": 0.11594202898550725,
          "p": 0.18604651162790697,
          "f": 0.14285713812659454
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2409.04772v1",
      "true_abstract": "Cell motility is fundamental to many biological processes, and cells exhibit\na variety of migration patterns. Many motile cell types follow a universal law\nthat connects their speed and persistency, a property that can originate from\nthe intracellular transport of polarity cues due to the global actin retrograde\nflow. This mechanism was termed the ``Universal Coupling between cell Speed and\nPersistency\"(UCSP). Here we implemented a simplified version of the UCSP\nmechanism in a coarse-grained ``minimal-cell\" model, which is composed of a\nthree-dimensional vesicle that contains curved active proteins. This model\nspontaneously forms a lamellipodia-like motile cell shape, which is however\nsensitive and can depolarize into a non-motile form due to random fluctuations\nor when interacting with external obstacles. The UCSP implementation introduces\nlong-range inhibition, which stabilizes the motile phenotype. This allows our\nmodel to describe the robust polarity observed in cells and explain a large\nvariety of cellular dynamics, such as the relation between cell speed and\naspect ratio, cell-barrier scattering, and cellular oscillations in different\ntypes of geometric confinements.",
      "generated_abstract": "t a novel framework for modeling the dynamics of a cellular population\nof neurons, based on the concept of a bifurcation. We introduce the concept of\nan \\textit{adaptive bifurcation}, a bifurcation that is driven by the\ninterplay of adaptation and sensitivity. We show that this concept allows for\na richer and more flexible modeling of the dynamics of a cellular population.\nUsing the adaptive bifurcation framework, we introduce a model of\nneural-plasticity-driven bistability in the dynamics of a cellular population.\nWe show that this model can be mapped to the well-known two-level system model\nof neuroplasticity. We also introduce a model of bistability in the\ndynamics of a cellular population, which is driven by a non-linear\ninteraction between the sensitivity of a neuron and its",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.2542372881355932,
          "f": 0.17241378862135034
        },
        "rouge-2": {
          "r": 0.024539877300613498,
          "p": 0.0425531914893617,
          "f": 0.031128400029675634
        },
        "rouge-l": {
          "r": 0.13043478260869565,
          "p": 0.2542372881355932,
          "f": 0.17241378862135034
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.00934v2",
      "true_abstract": "Background: Global viral threats underscore the need for effective genomic\nsurveillance, but high costs and uneven resource distribution hamper its\nimplementation. Targeting surveillance to international travelers in major\ntravel hubs may offer a more efficient strategy for the early detection of\nSARS-CoV-2 variants.\n  Methods: We developed and calibrated a multiple-strain metapopulation model\nof global SARS-CoV-2 transmission using extensive epidemiological,\nphylogenetic, and high-resolution air travel data. We then compared baseline\nsurveillance with various resource-allocation approaches that prioritize\ntravelers, focusing on Omicron BA.1/BA.2 retrospectively and on hypothetical\nfuture variants under different emergence, transmission and vaccine\neffectiveness scenarios.\n  Findings: Focusing existing surveillance resources on travelers at key global\nhubs significantly shortened detection delays without increasing total\nsurveillance efforts. In retrospective analyses of Omicron BA.1/BA.2,\ntraveler-targeted approaches consistently outperformed baseline strategies,\neven when overall resources were reduced. Simulations indicate that focusing\nsurveillance on key travel hubs outperform baseline practices in detecting\nfuture variants, across different possible origins, even with reduced\nresources. This approach also remains effective in future pandemic scenarios\nwith varying reproductive numbers and vaccine effectiveness.\n  Interpretation: These findings provide a quantitative, cost-effective\nframework for strengthening global genomic surveillance. By reallocating\nresources toward international travelers in select travel hubs, early detection\nof emerging variants can be enhanced, informing rapid public health\ninterventions and bolstering preparedness for future pandemics.",
      "generated_abstract": "of plant-pathogen interactions has traditionally relied on\nthe use of a single host species. However, advances in genomics and molecular\nbiology have enabled the identification of a wide range of plant species that\ncan serve as hosts for pathogens. The aim of this study is to develop a\nmethodology for the identification of host species in pathogen communities.\nThis methodology is based on the use of multivariate analysis, which is a\nwell-established technique in ecology and biology. This methodology is applied\nto a dataset of 1043 pathogen communities from 162 plant species, which were\ncollected from 107 locations in Europe, North Africa, and the Americas. The\nmain objective of this study is to identify the most prevalent host species in\npathogen communities, and to evaluate the potential of this approach in\nidentifying the most pre",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08666666666666667,
          "p": 0.17105263157894737,
          "f": 0.11504424332367472
        },
        "rouge-2": {
          "r": 0.004784688995215311,
          "p": 0.008695652173913044,
          "f": 0.006172834927034337
        },
        "rouge-l": {
          "r": 0.08,
          "p": 0.15789473684210525,
          "f": 0.10619468580155081
        }
      }
    },
    {
      "paper_id": "cs.CL.q-bio/NC/2503.04848v2",
      "true_abstract": "Human language and logic abilities are computationally quantified within the\nwell-studied grammar-automata hierarchy. We identify three hierarchical tiers\nand two corresponding transitions and show their correspondence to specific\nabilities in transformer-based language models (LMs). These emergent abilities\nhave often been described in terms of scaling; we show that it is the\ntransition between tiers, rather than scaled size itself, that determines a\nsystem's capabilities. Specifically, humans effortlessly process language yet\nrequire critical training to perform arithmetic or logical reasoning tasks; and\nLMs possess language abilities absent from predecessor systems, yet still\nstruggle with logical processing. We submit a novel benchmark of computational\npower, provide empirical evaluations of humans and fifteen LMs, and, most\nsignificantly, provide a theoretically grounded framework to promote careful\nthinking about these crucial topics. The resulting principled analyses provide\nexplanatory accounts of the abilities and shortfalls of LMs, and suggest\nactionable insights into the expansion of their logic abilities.",
      "generated_abstract": "t of sequencing technologies has transformed the study of complex\nsequences into a data-driven discipline. While traditional text-based approaches\nare efficient and robust, they are limited by the ability to capture the full\nsemantic context of a sequence, which can be critical for understanding its\nfunctionality. To address this limitation, we introduce Molecular Context\nGenerator (MCG), a novel approach for generating contextualized molecular\ndescriptions from textual data. Our approach employs a pre-trained large\nlanguage model (LLM) to extract molecular features and fine-tune the model on\ncontextualized molecular descriptions. Additionally, we introduce a novel\nframework for molecular context generation, which leverages a graph-based\nrepresentation of molecular descriptors to capture relationships between\nmolecules. This approach enhances the model's understanding of molecular\nstructures, facilitating the generation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12280701754385964,
          "p": 0.16279069767441862,
          "f": 0.13999999509800015
        },
        "rouge-2": {
          "r": 0.006666666666666667,
          "p": 0.008620689655172414,
          "f": 0.0075187920741736055
        },
        "rouge-l": {
          "r": 0.10526315789473684,
          "p": 0.13953488372093023,
          "f": 0.11999999509800019
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.05905v1",
      "true_abstract": "Recent developments in sequential experimental design look to construct a\npolicy that can efficiently navigate the design space, in a way that maximises\nthe expected information gain. Whilst there is work on achieving tractable\npolicies for experimental design problems, there is significantly less work on\nobtaining policies that are able to generalise well - i.e. able to give good\nperformance despite a change in the underlying statistical properties of the\nexperiments. Conducting experiments sequentially has recently brought about the\nuse of reinforcement learning, where an agent is trained to navigate the design\nspace to select the most informative designs for experimentation. However,\nthere is still a lack of understanding about the benefits and drawbacks of\nusing certain reinforcement learning algorithms to train these agents. In our\nwork, we investigate several reinforcement learning algorithms and their\nefficacy in producing agents that take maximally informative design decisions\nin sequential experimental design scenarios. We find that agent performance is\nimpacted depending on the algorithm used for training, and that particular\nalgorithms, using dropout or ensemble approaches, empirically showcase\nattractive generalisation properties.",
      "generated_abstract": "the problem of model selection in learning with a single latent\nvariable. In this setting, the data are generated from a latent variable that\nis a linear combination of the observed variables. We focus on the case of a\nlinear transformation of the latent variable, which is the case considered by\nthe likelihood-free inference literature. We propose a data-driven method for\nmodel selection based on the Fisher information matrix. We derive an\nasymptotic expansion of the information matrix, which shows that the method\ntends to the maximum likelihood estimator in the limit of large data\ndimensionality. We show that the information matrix has the same asymptotic\nbehavior for any linear transformation of the latent variable and that the\ninformation matrix does not depend on the particular choice of linear\ntransformation. We apply our method to the problem of model selection in\nadversarially robust learning and show that it yields optimal performance in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.2564102564102564,
          "f": 0.20725388119412613
        },
        "rouge-2": {
          "r": 0.024242424242424242,
          "p": 0.03225806451612903,
          "f": 0.027681656000288216
        },
        "rouge-l": {
          "r": 0.16521739130434782,
          "p": 0.24358974358974358,
          "f": 0.19689118689360802
        }
      }
    },
    {
      "paper_id": "physics.geo-ph.physics/geo-ph/2503.04227v1",
      "true_abstract": "We present the first extensive analysis of K/Ka-band ranging post-fit\nresiduals of an official Level-2 product, characterised as Line-of-Sight\nGravity Differences (LGD), which exhibit and showcase interesting sub-monthly\ngeophysical signals. These residuals, provided by CSR, were derived from the\ndifference between spherical harmonic coefficient least-squares fits and\nreduced Level-1B range-rate observations. We classified the geophysical signals\ninto four distinct categories: oceanic, meteorological, hydrological, and solid\nEarth, focusing primarily on the first three categories in this study. In our\nexamination of oceanic processes, we identified notable mass anomalies in the\nArgentine basin, specifically within the Zapiola Rise, where persistent\nremnants of the rotating dipole-like modes are evident in the LGD post-fit\nresiduals. Our analysis extended to the Gulf of Carpentaria and Australia\nduring the 2013 Oswald cyclone, revealing significant LGD residual anomalies\nthat correlate with cyclone tracking and precipitation data. Additionally, we\ninvestigated the monsoon seasons in Bangladesh, particularly from June to\nSeptember 2007, where we observed peaks in sub-monthly variability. These\nfindings were further validated by demonstrating high spatial and temporal\ncorrelations between gridded LGD residuals and ITSG-Grace2018 daily solutions.\nGiven that these anomalies are associated with significant mass change\nphenomena, it is essential to integrate the post-fit residuals into a\nhigh-frequency mass change framework, with the purpose of providing enhanced\nspatial resolution compared to conventional Kalman-filtered methods.",
      "generated_abstract": "r presents a novel, non-invasive, and inexpensive method for\nreconstructing the ground motion of earthquakes from seismic data. The proposed\nmethod relies on a novel approach combining the geophone-based spectral\nanalysis (GSS) with a new non-linear, non-parametric method to predict the\nground motion of earthquakes. This approach is based on a simple yet\neffective methodology to predict the ground motion based on the seismic\nfrequencies. The prediction is obtained by applying a non-linear spectral\nanalysis (GSS) to the seismic data, and then using a neural network model to\npredict the ground motion. The prediction is then compared with the ground\nmotion measured by the geophones, and the differences are analyzed. The\nproposed method is validated using a series of real earthquakes data from the\nJapan Trench,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0967741935483871,
          "p": 0.23076923076923078,
          "f": 0.13636363220041336
        },
        "rouge-2": {
          "r": 0.018957345971563982,
          "p": 0.039603960396039604,
          "f": 0.02564102126253362
        },
        "rouge-l": {
          "r": 0.09032258064516129,
          "p": 0.2153846153846154,
          "f": 0.12727272310950427
        }
      }
    },
    {
      "paper_id": "quant-ph.physics/hist-ph/2503.00966v1",
      "true_abstract": "The Frauchiger-Renner argument purports to show that the standard framework\nof quantum mechanics yields a contradiction when used to reason about systems\ncontaining agents who are themselves using quantum mechanics to perform\ndeductions. This has been framed as an obstacle to taking quantum mechanics to\nbe a complete theory. I formalize the argument in two closely related ways and\nelucidate the source of the paradox, clarifying the flaw in the original\nargument.",
      "generated_abstract": "t a method for extracting information from a quantum state that\nis independent of the specific quantum implementation. The method relies on a\nclass of non-unitary transformations called generalized measurements, which\nleverage the fact that any unitary transformation can be decomposed into\nproducts of unitary and projective measurements. The method enables the\nextracting information about the state of a quantum system from any\ncompletely positive trace-preserving (CPTP) map, regardless of the\nimplementation. The method also provides a general framework for\nnon-classical measurements that can be implemented by non-unitary\ntransformations. We demonstrate this method by showing how it can be used to\nextract the entanglement of a state from a quantum system. We also show how it\ncan be used to extract the number of particles of a system. We apply the method\nto extracting the entanglement of a system from a Gaussian state, and to\nextract",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24074074074074073,
          "p": 0.18309859154929578,
          "f": 0.2079999950924801
        },
        "rouge-2": {
          "r": 0.029411764705882353,
          "p": 0.017241379310344827,
          "f": 0.021739125775048256
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.1267605633802817,
          "f": 0.14399999509248015
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/RO/2503.10630v1",
      "true_abstract": "In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods.",
      "generated_abstract": "vancements in Large Language Models (LLMs) have revolutionized\nthe field of robotics by enabling robots to communicate with humans in natural\nways. However, existing methods often struggle to capture complex human\ninteractions, such as nuanced emotional cues and nonverbal signals, which\nhinders their real-world applicability. To address this, we propose a novel\nframework that employs a dual-pathway architecture, integrating a\nmultimodal LLM-based representation with a human-like speech generator. The\nformer captures core human intentions, while the latter provides contextual\ncues for complex, nonverbal interactions. Through extensive experiments, we\ndemonstrate that our approach significantly outperforms existing LLM-based\nmethods, achieving superior performance on both emotion classification and\nnonverbal interaction prediction tasks. Our code is available at\nhttps://github.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.21212121212121213,
          "f": 0.17073170250743622
        },
        "rouge-2": {
          "r": 0.023809523809523808,
          "p": 0.04424778761061947,
          "f": 0.03095974777291137
        },
        "rouge-l": {
          "r": 0.1292517006802721,
          "p": 0.1919191919191919,
          "f": 0.15447153990581017
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2407.07797v2",
      "true_abstract": "The mechanics of animal cells is strongly determined by stress fibers, which\nare contractile filament bundles that form dynamically in response to\nextracellular cues. Stress fibers allow the cell to adapt its mechanics to\nenvironmental conditions and to protect it from structural damage. While the\nphysical description of single stress fibers is well-developed, much less is\nknown about their spatial distribution on the level of whole cells. Here, we\ncombine a finite element method for one-dimensional fibers embedded in an\nelastic bulk medium with dynamical rules for stress fiber formation based on\ngenetic algorithms. We postulate that their main goal is to achieve minimal\nmechanical stress in the bulk material with as few fibers as possible. The\nfiber positions and configurations resulting from this optimization task alone\nare in good agreement with those found in experiments where cells in\n3D-scaffolds were mechanically strained at one attachment point. For optimized\nconfigurations, we find that stress fibers typically run through the cell in a\ndiagonal fashion, similar to reinforcement strategies used for composite\nmaterial.",
      "generated_abstract": "hallenge in the study of cellular processes is the scarcity of\ncellular imaging tools. In recent years, the development of light-activated\ndye-sensitized solar cells (LASCS) has led to the development of light-activated\nsensitizers, which can be used as imaging agents in biological systems. In\nparticular, the use of LASCS as imaging agents has the advantage that the\nsensitizers are sensitive to all wavelengths, meaning that they can be used for\nthe imaging of cells in different wavelength ranges. In this work, we propose\na LASCS-based imaging technique for cellular imaging. We first show how the\nsensitizers in LASCS can be used to image cells using the concept of\ndye-sensitized solar cells (DSSCs). We then show how the sensitizer in LASCS",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13114754098360656,
          "p": 0.24615384615384617,
          "f": 0.17112299011696086
        },
        "rouge-2": {
          "r": 0.01775147928994083,
          "p": 0.029411764705882353,
          "f": 0.022140216707834493
        },
        "rouge-l": {
          "r": 0.13114754098360656,
          "p": 0.24615384615384617,
          "f": 0.17112299011696086
        }
      }
    },
    {
      "paper_id": "quant-ph.eess/IV/2503.01916v1",
      "true_abstract": "In transportation cyber-physical systems (CPS), ensuring safety and\nreliability in real-time decision-making is essential for successfully\ndeploying autonomous vehicles and intelligent transportation networks. However,\nthese systems face significant challenges, such as computational complexity and\nthe ability to handle ambiguous inputs like shadows in complex environments.\nThis paper introduces a Quantum Deep Convolutional Neural Network (QDCNN)\ndesigned to enhance the safety and reliability of CPS in transportation by\nleveraging quantum algorithms. At the core of QDCNN is the UU{\\dag} method,\nwhich is utilized to improve shadow detection through a propagation algorithm\nthat trains the centroid value with preprocessing and postprocessing operations\nto classify shadow regions in images accurately. The proposed QDCNN is\nevaluated on three datasets on normal conditions and one road affected by rain\nto test its robustness. It outperforms existing methods in terms of\ncomputational efficiency, achieving a shadow detection time of just 0.0049352\nseconds, faster than classical algorithms like intensity-based thresholding\n(0.03 seconds), chromaticity-based shadow detection (1.47 seconds), and local\nbinary pattern techniques (2.05 seconds). This remarkable speed, superior\naccuracy, and noise resilience demonstrate the key factors for safe navigation\nin autonomous transportation in real-time. This research demonstrates the\npotential of quantum-enhanced models in addressing critical limitations of\nclassical methods, contributing to more dependable and robust autonomous\ntransportation systems within the CPS framework.",
      "generated_abstract": "In this paper, we develop an analytical method to predict the evolution of\nthe probability of detection (POD) for a single-photon detector in an\noptical parametric amplifier (OPA) as a function of the OPA parameters. The\nmain focus is to highlight the effect of the amplification gain, the\ndephasing rate, and the number of photons on the POD of the OPA, which is\nimportant for the design and optimization of OPAs. Our theoretical analysis\nshows that the POD can be expressed as a function of the OPA parameters. The\nresults of the analysis are in good agreement with the experimental data\nobtained using the OPA we have developed for the first time, which demonstrates\nthe effectiveness of our approach in analyzing the POD.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11038961038961038,
          "p": 0.2236842105263158,
          "f": 0.14782608253156912
        },
        "rouge-2": {
          "r": 0.014218009478672985,
          "p": 0.028037383177570093,
          "f": 0.018867920063091913
        },
        "rouge-l": {
          "r": 0.09740259740259741,
          "p": 0.19736842105263158,
          "f": 0.13043477818374305
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.18328v1",
      "true_abstract": "Recent advances in Visual Anomaly Detection (VAD) have introduced\nsophisticated algorithms leveraging embeddings generated by pre-trained feature\nextractors. Inspired by these developments, we investigate the adaptation of\nsuch algorithms to the audio domain to address the problem of Audio Anomaly\nDetection (AAD). Unlike most existing AAD methods, which primarily classify\nanomalous samples, our approach introduces fine-grained temporal-frequency\nlocalization of anomalies within the spectrogram, significantly improving\nexplainability. This capability enables a more precise understanding of where\nand when anomalies occur, making the results more actionable for end users. We\nevaluate our approach on industrial and environmental benchmarks, demonstrating\nthe effectiveness of VAD techniques in detecting anomalies in audio signals.\nMoreover, they improve explainability by enabling localized anomaly\nidentification, making audio anomaly detection systems more interpretable and\npractical.",
      "generated_abstract": "ech enhancement (ASE) is a fundamental task in speech signal\nprocessing, aiming to improve speech quality by removing or reducing\nspeech-related noise. Recently, deep learning-based methods have achieved\nstate-of-the-art results in ASE. However, existing deep learning-based ASE\nmodels face significant challenges in real-world scenarios due to the complexity\nof speech-related noise. This paper presents a unified ASE framework based on\nthe transformer architecture, which effectively addresses the complexity of\nspeech-related noise. The proposed framework employs a multi-scale transformer\nto capture the temporal and spectral information of the speech signal, and\nintegrates the multi-level feature fusion mechanism to enhance the model's\nperformance. The proposed framework has two key components: the speech\nembedding layer and the multi-level feature fusion layer. The former is used to\nembed the input audio signal into",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.19047619047619047,
          "f": 0.17777777280000015
        },
        "rouge-2": {
          "r": 0.008130081300813009,
          "p": 0.008620689655172414,
          "f": 0.008368195841112206
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.19047619047619047,
          "f": 0.17777777280000015
        }
      }
    },
    {
      "paper_id": "physics.optics.eess/SP/2503.04402v1",
      "true_abstract": "Chaos lidars detect targets through the cross-correlation between the\nback-scattered chaos signal from the target and the local reference one. Chaos\nlidars have excellent anti-jamming and anti-interference capabilities, owing to\nthe random nature of chaotic oscillations. However, most chaos lidars operate\nin the near-infrared spectral regime, where the atmospheric attenuation is\nsignificant. Here we show a mid-infrared chaos lidar, which is suitable for\nlong-reach ranging and imaging applications within the low-loss transmission\nwindow of the atmosphere. The proof-of-concept mid-infrared chaos lidar\nutilizes an interband cascade laser with optical feedback as the laser chaos\nsource. Experimental results reveal that the chaos lidar achieves an accuracy\nbetter than 0.9 cm and a precision better than 0.3 cm for ranging distances up\nto 300 cm. In addition, it is found that a minimum signal-to-noise ratio of\nonly 1 dB is required to sustain both sub-cm accuracy and sub-cm precision.\nThis work paves the way for developing remote chaos lidar systems in the\nmid-infrared spectral regime.",
      "generated_abstract": "y investigates the interaction between a high-power femtosecond laser and\na liquid crystal display (LCD) by analyzing the nonlinear refractive index\n(RI) effect and nonlinear absorption (NA) of the LCD. The LCD is treated as a\nphotonic crystal (PC) film, and the refractive index and NA of the LCD are\ncalculated. It is found that the nonlinear refractive index is larger than the\nlinear refractive index, which results in a nonlinear optical effect. The\nnonlinear absorption of the LCD is also analyzed, and it is found that the\nnonlinear absorption coefficient is larger than the linear absorption coefficient.\nThe calculated results show that the nonlinear refractive index of the LCD can\nbe greater than the linear refractive index, and the nonlinear optical effect\ncan be observed in the LCD",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16363636363636364,
          "p": 0.32727272727272727,
          "f": 0.21818181373737383
        },
        "rouge-2": {
          "r": 0.05128205128205128,
          "p": 0.08695652173913043,
          "f": 0.06451612436524488
        },
        "rouge-l": {
          "r": 0.15454545454545454,
          "p": 0.3090909090909091,
          "f": 0.20606060161616171
        }
      }
    },
    {
      "paper_id": "econ.EM.stat/CO/2502.04945v1",
      "true_abstract": "We study an alternative use of machine learning. We train neural nets to\nprovide the parameter estimate of a given (structural) econometric model, for\nexample, discrete choice or consumer search. Training examples consist of\ndatasets generated by the econometric model under a range of parameter values.\nThe neural net takes the moments of a dataset as input and tries to recognize\nthe parameter value underlying that dataset. Besides the point estimate, the\nneural net can also output statistical accuracy. This neural net estimator\n(NNE) tends to limited-information Bayesian posterior as the number of training\ndatasets increases. We apply NNE to a consumer search model. It gives more\naccurate estimates at lighter computational costs than the prevailing approach.\nNNE is also robust to redundant moment inputs. In general, NNE offers the most\nbenefits in applications where other estimation approaches require very heavy\nsimulation costs. We provide code at: https://nnehome.github.io.",
      "generated_abstract": "uce a novel method for estimating the joint distribution of\ntwo random variables from jointly distributed observations. Our method is\nbased on a generalization of the semiparametric generalized estimating equation\n(SPIE) model that includes a Gaussian process prior on the joint distribution.\nThe model is estimated using a novel Bayesian nonparametric method that combines\nthe nonparametric sparsity regularization of the SPIE model with a\nnonparametric Gaussian process prior. We show that the method can be applied\nto a wide range of settings, including the case of discrete random variables,\nas well as the case of continuous random variables with known or unknown\ndistributions. The method can be used to estimate the joint distribution of\nmultiple random variables from jointly distributed observations, including\nconditional distributions. The method is illustrated through simulation\nstudies and a case study using real data. Finally, we apply the method to the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19444444444444445,
          "p": 0.2692307692307692,
          "f": 0.22580644674297617
        },
        "rouge-2": {
          "r": 0.020833333333333332,
          "p": 0.02459016393442623,
          "f": 0.022556386011646747
        },
        "rouge-l": {
          "r": 0.19444444444444445,
          "p": 0.2692307692307692,
          "f": 0.22580644674297617
        }
      }
    },
    {
      "paper_id": "cs.GL.cs/GL/2311.03292v4",
      "true_abstract": "Consensus on the definition of data science remains low despite the\nwidespread establishment of academic programs in the field and continued demand\nfor data scientists in industry. Definitions range from rebranded statistics to\ndata-driven science to the science of data to simply the application of machine\nlearning to so-called big data to solve real-world problems. Current efforts to\ntrace the history of the field in order to clarify its definition, such as\nDonoho's \"50 Years of Data Science\" (Donoho 2017), tend to focus on a short\nperiod when a small group of statisticians adopted the term in an unsuccessful\nattempt to rebrand their field in the face of the overshadowing effects of\ncomputational statistics and data mining. Using textual evidence from primary\nsources, this essay traces the history of the term to the 1960s, when it was\nfirst used by the US Air Force in a surprisingly similar way to its current\nusage, to 2012, the year that Harvard Business Review published the enormously\ninfluential article \"Data Scientist: The Sexiest Job of the 21st Century\"\n(Davenport and Patil 2012) and the American Statistical Association\nacknowledged a profound disconnect between statistics and data science\n(Rodriguez 2012). Among the themes that emerge from this review are (1) the\nlong-standing opposition between data analysts and data miners that continues\nto animate the field, (2) an established definition of the term as the practice\nof managing and processing scientific data that has been occluded by recent\nusage, and (3) the phenomenon of data impedance -- the disproportion between\nsurplus data, indexed by phrases like data deluge and big data, and the\nlimitations of computational machinery and methods to process them. This\npersistent condition appears to have motivated the use of the term and the\nfield itself since its beginnings.",
      "generated_abstract": "aper, we consider the problem of generating a compact, connected\nand oriented $4$-manifold from a given pair of $3$-manifolds $M_1$ and $M_2$\nsuch that the boundary of the $4$-manifold is a union of the boundary of $M_1$\nand the boundary of $M_2$. We show that the problem can be solved by\nconstructing a finite list of minimal surfaces in $\\mathbb{CP}^3$ and\nconstructing a minimal surface for each such minimal surface in the list.\n  We also show that the problem can be solved by constructing a finite list of\nclosed surfaces in $\\mathbb{CP}^3$ and constructing a closed surface for each\nsuch minimal surface in the list. We give a simple construction for the first\ncase and we give a simple construction for the second case. We show that the\nproblem can be solved by construct",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06285714285714286,
          "p": 0.22448979591836735,
          "f": 0.0982142822963171
        },
        "rouge-2": {
          "r": 0.011194029850746268,
          "p": 0.03571428571428571,
          "f": 0.017045450911674326
        },
        "rouge-l": {
          "r": 0.05142857142857143,
          "p": 0.1836734693877551,
          "f": 0.08035713943917425
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.05504v2",
      "true_abstract": "This paper explores whether and to what extent ambiguous communication can be\nbeneficial to the sender in a persuasion problem, when the receiver (and\npossibly the sender) is ambiguity averse. We provide a concavification-like\ncharacterization of the sender's optimal ambiguous communication. The\ncharacterization highlights the necessity of using a collection of experiments\nthat form a splitting of an obedient (i.e., incentive compatible) experiment.\nSome experiments in the collection must be Pareto-ranked in the sense that both\nplayers agree on their payoff ranking. The existence of a binary such\nPareto-ranked splitting is necessary for ambiguous communication to benefit the\nsender, and, if an optimal Bayesian persuasion experiment can be split in this\nway, this is sufficient for an ambiguity-neutral sender as well as the receiver\nto benefit. Such gains are impossible when the receiver has only two actions.\nThe possibility of gains is substantially robust to (non-extreme) sender\nambiguity aversion.",
      "generated_abstract": "We study the optimal taxation of a firm's labor and capital, when a firm\nsells its output to an economy with imperfect competition. We show that\nfirm-specific capital and labor taxes can be designed to maximize the firm's\nprofit, while preserving the firms' ability to sell their output. Moreover,\nwe show that the optimal taxation is a concave function of the firm's\nperformance, which is a convex combination of its capital and labor\nsubsidies. We then analyze the optimal taxation for a general class of\ncapital-labor subsidies and derive a novel taxation scheme that is\nconcave-convex-concave in the firm's performance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.26666666666666666,
          "f": 0.21052631101108046
        },
        "rouge-2": {
          "r": 0.028169014084507043,
          "p": 0.0449438202247191,
          "f": 0.03463202989524249
        },
        "rouge-l": {
          "r": 0.16304347826086957,
          "p": 0.25,
          "f": 0.19736841627423835
        }
      }
    },
    {
      "paper_id": "quant-ph.econ/TH/2501.17189v3",
      "true_abstract": "Quantum games, like quantum algorithms, exploit quantum entanglement to\nestablish strong correlations between strategic player actions. This paper\nintroduces quantum game-theoretic models applied to trading and demonstrates\ntheir implementation on an ion-trap quantum computer. The results showcase a\nquantum advantage, previously known only theoretically, realized as\nhigher-paying market Nash equilibria. This advantage could help uncover alpha\nin trading strategies, defined as excess returns compared to established\nbenchmarks. These findings suggest that quantum computing could significantly\ninfluence the development of financial strategies.",
      "generated_abstract": "We introduce a new paradigm for studying the emergence of stable equilibria\nin economics. We consider a class of models where the individuals are\nendowed with an individual utility function, a state-dependent payoff function,\nand a state-independent cost function. The utility function is assumed to be\nconvex, and the cost function is assumed to be convex and lower-semicontinuous\n(LSC). We prove that if the individual utility functions are monotone and\nconcave, then the equilibria are unique and the payoff functions are\nconcave. Furthermore, we show that if the individual utility functions are\nconcave and convex, then the equilibria are unique and the cost functions are\nconcave. We provide examples to illustrate our results, and we discuss how the\nconcept of convexity is related to the concepts of convexity and LSC.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15714285714285714,
          "p": 0.16923076923076924,
          "f": 0.16296295796982183
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.15714285714285714,
          "p": 0.16923076923076924,
          "f": 0.16296295796982183
        }
      }
    },
    {
      "paper_id": "cs.GL.cs/GL/2304.12898v1",
      "true_abstract": "The development of advanced generative chat models, such as ChatGPT, has\nraised questions about the potential consciousness of these tools and the\nextent of their general artificial intelligence. ChatGPT consistent avoidance\nof passing the test is here overcome by asking ChatGPT to apply the Turing test\nto itself. This explores the possibility of the model recognizing its own\nsentience. In its own eyes, it passes this test. ChatGPT's self-assessment\nmakes serious implications about our understanding of the Turing test and the\nnature of consciousness. This investigation concludes by considering the\nexistence of distinct types of consciousness and the possibility that the\nTuring test is only effective when applied between consciousnesses of the same\nkind. This study also raises intriguing questions about the nature of AI\nconsciousness and the validity of the Turing test as a means of verifying such\nconsciousness.",
      "generated_abstract": "We consider the problem of constructing a path between two vertices of a graph\ngiven the two vertices and the set of edges that connect them. We consider two\nkinds of paths: paths that start and end at the same vertex and paths that\nstart at different vertices. We show that any path between two vertices can be\nconstructed by a sequence of vertex moves and edge cuts. In particular, we\ndemonstrate that any path between two vertices can be constructed by a\nsequence of vertex moves and edge cuts. We also show that any path between two\nvertices that can be constructed by a sequence of vertex moves and edge cuts\nalso can be constructed by a sequence of edge moves and vertex cuts. We then\nshow that the problem of constructing a path between two vertices can be\nsolved in polynomial time if the graph is connected or in polynomial time if\nthe graph is bipartite.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13095238095238096,
          "p": 0.20754716981132076,
          "f": 0.1605839368618468
        },
        "rouge-2": {
          "r": 0.025423728813559324,
          "p": 0.03296703296703297,
          "f": 0.0287081290547386
        },
        "rouge-l": {
          "r": 0.13095238095238096,
          "p": 0.20754716981132076,
          "f": 0.1605839368618468
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/SR/2503.10019v1",
      "true_abstract": "Millinovae are a new class of transient supersoft X-ray sources with no clear\nsignature of mass ejection. They show similar triangle shapes of $V/I$ band\nlight curves with thousand times fainter peaks than typical classical novae.\nMaccarone et al. regarded the prototype millinova, ASASSN-16oh, as a dwarf nova\nand interpreted the supersoft X-rays to originate from an accretion belt on a\nwhite dwarf (WD). Kato et al. proposed a nova model induced by a high-rate\nmass-accretion during a dwarf nova outburst; the X-rays originate from the\nphotosphere of a hydrogen-burning hot WD whereas the $V/I$ band photons are\nfrom the irradiated accretion disk. Because each peak brightness differs\nlargely from millinova to millinova, we suspect that not all the millinova\ncandidates host a hydrogen burning WD. Based on the light curve analysis of the\nclassical nova KT Eri that has a bright disk, we find that the disk is more\nthan two magnitudes brighter when the disk is irradiated by the hydrogen\nburning WD than when not irradiated. We present the demarcation criterion for\nhydrogen burning to be $I_{\\rm q} - I_{\\rm max} > 2.2$, where $I_q$ and $I_{\\rm\nmax}$ are the $I$ magnitudes in quiescence and at maximum light, respectively.\nAmong many candidates, this requirement is satisfied with the two millinovae in\nwhich soft X-rays were detected.",
      "generated_abstract": "ork, we explore the use of deep learning models to predict\nstellar parameters from asteroseismic data. Specifically, we investigate the\neffect of including an artificial neural network (ANN) in the ensemble\nlearning framework, and compare its performance with the use of a traditional\nleast-squares fit. Our approach is based on the application of the Bayesian\nInformation Criterion (BIC) to select the best model from a set of candidates,\nusing a grid of hyperparameters. The ANN model was developed using 500\nepochs of training data from the DUNE-SNeIa database, which comprises 2000\nmeasurements of the $B$- and $V$-band stellar parameters and their uncertainties\nfrom the SN Ia 2017T and 2021A/B supernovae. We found that the ANN model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1223021582733813,
          "p": 0.2125,
          "f": 0.1552511369154106
        },
        "rouge-2": {
          "r": 0.028985507246376812,
          "p": 0.05504587155963303,
          "f": 0.03797467902519682
        },
        "rouge-l": {
          "r": 0.1079136690647482,
          "p": 0.1875,
          "f": 0.1369862967327622
        }
      }
    },
    {
      "paper_id": "math.OC.q-fin/MF/2412.11383v1",
      "true_abstract": "In this paper, we explore a new class of stochastic control problems\ncharacterized by specific control constraints. Specifically, the admissible\ncontrols are subject to the ratcheting constraint, meaning they must be\nnon-decreasing over time and are thus self-path-dependent. This type of\nproblems is common in various practical applications, such as optimal\nconsumption problems in financial engineering and optimal dividend payout\nproblems in actuarial science. Traditional stochastic control theory does not\nreadily apply to these problems due to their unique self-path-dependent control\nfeature. To tackle this challenge, we introduce a new class of\nHamilton-Jacobi-Bellman (HJB) equations, which are variational inequalities\nconcerning the derivative of a new spatial argument that represents the\nhistorical maximum control value. Under the standard Lipschitz continuity\ncondition, we demonstrate that the value functions for these\nself-path-dependent control problems are the unique solutions to their\ncorresponding HJB equations in the viscosity sense.",
      "generated_abstract": "We consider the optimal execution problem in a model of market microstructure\nwhere an investor can bid for and receive an auction price, which can be\nnegotiated through a bidding process. We show that under a general set of\nassumptions, the optimal bid strategy is always a non-negative linear combination\nof the bidder's initial position and the bid price, and the optimal execution\nis the best response to the bidder's optimal bid strategy. We derive these\nresults using a novel approach that combines the approach of Chatterjee and\nFoster (2016) with a more general version of the model of Chatterjee and\nGollier (2017) and the model of Gollier and Hackbusch (2019).",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13402061855670103,
          "p": 0.20634920634920634,
          "f": 0.16249999522578137
        },
        "rouge-2": {
          "r": 0.007462686567164179,
          "p": 0.010416666666666666,
          "f": 0.008695647310399695
        },
        "rouge-l": {
          "r": 0.10309278350515463,
          "p": 0.15873015873015872,
          "f": 0.12499999522578144
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PR/2411.10079v1",
      "true_abstract": "Abstract This paper proposes a novel approach to Bermudan swaption hedging by\napplying the deep hedging framework to address limitations of traditional\narbitrage-free methods. Conventional methods assume ideal conditions, such as\nzero transaction costs, perfect liquidity, and continuous-time hedging, which\noften differ from real market environments. This discrepancy can lead to\nresidual profit and loss (P&L), resulting in two primary issues. First,\nresidual P&L may prevent achieving the initial model price, especially with\nimproper parameter settings, potentially causing a negative P&L trend and\nsignificant financial impacts. Second, controlling the distribution of residual\nP&L to mitigate downside risk is challenging, as hedged positions may become\ncurve gamma-short, making them vulnerable to large interest rate movements. The\ndeep hedging approach enables flexible selection of convex risk measures and\nhedge strategies, allowing for improved residual P&L management. This study\nalso addresses challenges in applying the deep hedging approach to Bermudan\nswaptions, such as efficient arbitrage-free market scenario generation and\nmanaging early exercise conditions. Additionally, we introduce a unique \"Option\nSpread Hedge\" strategy, which allows for robust hedging and provides intuitive\ninterpretability. Numerical analysis results demonstrate the effectiveness of\nour approach.",
      "generated_abstract": "r presents a novel approach to market maker design using a\nmarket maker-seller strategy in a continuous-time stochastic model with\ndiscrete-time trading. Our method is based on the assumption that the market\nmaker can use a trading strategy that generates returns that are a function of\nthe current market price and the realized price-time correlation. We show that\nthe resulting market maker strategy is efficient and incentive-compatible. We\nalso derive the value of the market maker's risk-neutral return, and derive\nthe value of the market maker's realized price-time correlation, as well as the\nvalue of the market maker's realized price-time correlation with respect to the\nmarket maker's realized price-time correlation with the seller. We also show\nthat the market maker's realized price-time correlation with the seller\nsatisfies the Bellman equation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.3050847457627119,
          "f": 0.18274111255533523
        },
        "rouge-2": {
          "r": 0.017045454545454544,
          "p": 0.03260869565217391,
          "f": 0.022388055192694156
        },
        "rouge-l": {
          "r": 0.11594202898550725,
          "p": 0.2711864406779661,
          "f": 0.16243654402741642
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/SC/2404.19158v1",
      "true_abstract": "The three-dimensional organization of chromatin is thought to play an\nimportant role in controlling gene expression. Specificity in expression is\nachieved through the interaction of transcription factors and other nuclear\nproteins with particular sequences of DNA. At unphysiological concentrations\nmany of these nuclear proteins can phase-separate in the absence of DNA, and it\nhas been hypothesized that, in vivo, the thermodynamic forces driving these\nphases help determine chromosomal organization. However it is unclear how DNA,\nitself a long polymer subject to configurational transitions, interacts with\nthree-dimensional protein phases. Here we show that a long compressible polymer\ncan be coupled to interacting protein mixtures, leading to a generalized\nprewetting transition where polymer collapse is coincident with a locally\nstabilized liquid droplet. We use lattice Monte-Carlo simulations and a\nmean-field theory to show that these phases can be stable even in regimes where\nboth polymer collapse and coexisting liquid phases are unstable in isolation,\nand that these new transitions can be either abrupt or continuous. For polymers\nwith internal linear structure we further show that changes in the\nconcentration of bulk components can lead to changes in three-dimensional\npolymer structure. In the nucleus there are many distinct proteins that\ninteract with many different regions of chromatin, potentially giving rise to\nmany different Prewet phases. The simple systems we consider here highlight\nchromatin's role as a lower-dimensional surface whose interactions with\nproteins are required for these novel phases.",
      "generated_abstract": "The emergence of spatially structured epidemics is a central phenomenon in\nthe evolution of infectious diseases. Here, we investigate how spatial\nstructuring and environmental heterogeneity influence the emergence of\nepidemic-like outbreaks in a two-species model of influenza. Our analysis\nshows that spatial structure enhances the transmission of the disease by\nexploiting the connectivity between the two species. Additionally, we demonstrate\nthat environmental heterogeneity has a significant impact on the emergence of\nepidemic-like outbreaks, where environmental heterogeneity increases the\nspread of the epidemic. Our findings highlight the critical role of spatial\nstructure and environmental heterogeneity in influencing the emergence of\nepidemic-like outbreaks in spatially structured epidemics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1048951048951049,
          "p": 0.25,
          "f": 0.14778324706738832
        },
        "rouge-2": {
          "r": 0.0045045045045045045,
          "p": 0.011494252873563218,
          "f": 0.0064724878637659996
        },
        "rouge-l": {
          "r": 0.0979020979020979,
          "p": 0.23333333333333334,
          "f": 0.13793103031861983
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.06846v1",
      "true_abstract": "Chytridiomycosis is a fungal disease, primarily caused by Batrachochytrium\ndendrobatidis, that poses a major threat to frog populations worldwide, driving\nat least 90 amphibian species to extinction, and severely affecting hundreds of\nothers. Difficulties in management of this disease have shown a need for novel\nconservation approaches.\n  In this paper, we present a novel dynamic mathematical model for\nchytridiomycosis transmission in frogs that includes the natural history of\ninfection, to test the hypothesis that sunlight-heated refugia reduce\ntransmission in frog populations. This model was fit using approximate Bayesian\ncomputation to experimental data where frogs were separated into cohorts based\nthe amount of heat the refugia received.\n  Our results show that the effect of sunlight-heating the refugia reduced\ninfection in frogs by 40%. Further, frogs that were infected and recovered had\nsignificant protection, with a reduction in susceptibility of approximately 97%\ncompared to naive frogs. The mathematical model can be used to gain further\ninsight into using sunlight-heated refugia to reduce chytridiomycosis\nprevalence amongst amphibians.",
      "generated_abstract": "chanical properties of tissues can be used to predict their\neffective stress, which can be used to predict the deformation of the tissue.\nHowever, there is a gap between the predictions of biomechanical models and the\ndata on tissue deformation. This gap can be reduced by incorporating more data\ninto the model. The main objective of this paper is to improve the predictions\nof biomechanical models by incorporating data on tissue deformation. This\nincorporation of data on tissue deformation is achieved by modifying the\nbiomechanical model, such that the deformation of the tissue is taken into\naccount in the model. To evaluate the improvements in the predictions of the\nbiomechanical models, data from the deformation of tissues has been used. The\nresults show that the incorporation of deformation data improves the\npredictions of the biomechanical models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1875,
          "p": 0.35,
          "f": 0.24418604196863172
        },
        "rouge-2": {
          "r": 0.05128205128205128,
          "p": 0.08163265306122448,
          "f": 0.06299212124496284
        },
        "rouge-l": {
          "r": 0.16964285714285715,
          "p": 0.31666666666666665,
          "f": 0.22093022801514342
        }
      }
    },
    {
      "paper_id": "stat.ME.q-bio/QM/2502.15848v1",
      "true_abstract": "This paper introduces a non-parametric estimation algorithm designed to\neffectively estimate the joint distribution of model parameters with\napplication to population pharmacokinetics. Our research group has previously\ndeveloped the non-parametric adaptive grid (NPAG) algorithm, which while\naccurate, explores parameter space using an ad-hoc method to suggest new\nsupport points. In contrast, the non-parametric optimal design (NPOD) algorithm\nuses a gradient approach to suggest new support points, which reduces the\namount of time spent evaluating non-relevant points and by this the overall\nnumber of cycles required to reach convergence. In this paper, we demonstrate\nthat the NPOD algorithm achieves similar solutions to NPAG across two datasets,\nwhile being significantly more efficient in both the number of cycles required\nand overall runtime. Given the importance of developing robust and efficient\nalgorithms for determining drug doses quickly in pharmacokinetics, the NPOD\nalgorithm represents a valuable advancement in non-parametric modeling. Further\nanalysis is needed to determine which algorithm performs better under specific\nconditions.",
      "generated_abstract": "ration of next-generation sequencing technologies into cancer\nanalysis has transformed the field, enabling the detection of rare variants\nand their impact on cancer risk. However, conventional methods for variant\ncalling often struggle to accurately quantify rare variants, leading to\nsuboptimal interpretation and biased cancer risk estimates. In this work, we\npropose a novel approach to variant call generation that leverages\nneighbor-joining tree reconstruction, a powerful method for estimating\nrelationships between organisms. By incorporating this information, we\nintroduce the Rare-Variant Neighbor-Joining (RVNJ) variant call method. The\nRVNJ method generates variant calls that are more consistent with the\nrelationships inferred from the tree, leading to more reliable results. By\nevaluating the RVNJ method against state-of-the-art methods, we demonstrate its",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1592920353982301,
          "p": 0.21176470588235294,
          "f": 0.18181817691817176
        },
        "rouge-2": {
          "r": 0.020134228187919462,
          "p": 0.026785714285714284,
          "f": 0.02298850084761044
        },
        "rouge-l": {
          "r": 0.1592920353982301,
          "p": 0.21176470588235294,
          "f": 0.18181817691817176
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.04518v1",
      "true_abstract": "We introduce Dirichlet Process Posterior Sampling (DPPS), a Bayesian\nnon-parametric algorithm for multi-arm bandits based on Dirichlet Process (DP)\npriors. Like Thompson-sampling, DPPS is a probability-matching algorithm, i.e.,\nit plays an arm based on its posterior-probability of being optimal. Instead of\nassuming a parametric class for the reward generating distribution of each arm,\nand then putting a prior on the parameters, in DPPS the reward generating\ndistribution is directly modeled using DP priors. DPPS provides a principled\napproach to incorporate prior belief about the bandit environment, and in the\nnoninformative limit of the DP posteriors (i.e. Bayesian Bootstrap), we recover\nNon Parametric Thompson Sampling (NPTS), a popular non-parametric bandit\nalgorithm, as a special case of DPPS. We employ stick-breaking representation\nof the DP priors, and show excellent empirical performance of DPPS in\nchallenging synthetic and real world bandit environments. Finally, using an\ninformation-theoretic analysis, we show non-asymptotic optimality of DPPS in\nthe Bayesian regret setup.",
      "generated_abstract": "ork, we introduce a novel approach to Bayesian model selection\nthat focuses on the posterior predictive distribution (PPD) of the true model\nunder consideration. The PPD is defined as a distribution over the predictive\nvalues of the true model. We propose to use the PPD to guide the selection of\nthe prior distribution of the parameters. The PPD is also a useful tool for\nuncertainty quantification of the model parameters, especially in cases where\nthe model is complex and the computational cost of sampling from the posterior\ndistribution is high. Our approach is based on a model of the prior predictive\ndistribution and the model of the posterior predictive distribution. We show\nthat the model of the posterior predictive distribution can be derived from a\nmodel of the prior predictive distribution. We then propose to use the\ninformation obtained from the posterior predictive distribution to select the\nprior distribution of the model parameters. We show that this approach",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21,
          "p": 0.328125,
          "f": 0.25609755621653785
        },
        "rouge-2": {
          "r": 0.04794520547945205,
          "p": 0.06363636363636363,
          "f": 0.05468749509887739
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.3125,
          "f": 0.24390243426531835
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.physics/bio-ph/2503.05401v1",
      "true_abstract": "We study the first-passage-time (FPT) properties of active Brownian particles\nto reach an absorbing wall in two dimensions. Employing a perturbation approach\nwe obtain exact analytical predictions for the survival and FPT distributions\nfor small P\\'eclet numbers, measuring the importance of self-propulsion\nrelative to diffusion. While randomly oriented active agents reach the wall\nfaster than their passive counterpart, their initial orientation plays a\ncrucial role in the FPT statistics. Using the median as a metric, we quantify\nthis anisotropy and find that it becomes more pronounced at distances where\npersistent active motion starts to dominate diffusion.",
      "generated_abstract": "e a simple, intuitive, and quantitative method for estimating the\ntemperature of a liquid droplet at the nanoscale. Our approach involves a\nsimplified model for the interfacial tension between the liquid droplet and\nsubstrate and a parameterization of the temperature of a liquid droplet at\nnanoscale. We find that this method provides a good approximation for droplets\nin aqueous solutions, particularly in the absence of shear and for temperatures\nup to a few tens of degrees Celsius. The method is based on the assumption that\nthe liquid droplet is in thermal equilibrium with the substrate at a temperature\nthat is determined by the interfacial tension between the droplet and the\nsubstrate. This assumption is not necessarily valid in real droplet-substrate\nsystems, but it provides a useful approximation when droplets are in\nthermal-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18666666666666668,
          "p": 0.2,
          "f": 0.1931034432818075
        },
        "rouge-2": {
          "r": 0.031578947368421054,
          "p": 0.02608695652173913,
          "f": 0.028571423616780903
        },
        "rouge-l": {
          "r": 0.17333333333333334,
          "p": 0.18571428571428572,
          "f": 0.17931033983353167
        }
      }
    },
    {
      "paper_id": "cs.AI.cs/LO/2503.09730v1",
      "true_abstract": "The most promising recent methods for AI reasoning require applying variants\nof reinforcement learning (RL) either on rolled out trajectories from the\nmodel, even for the step-wise rewards, or large quantities of human annotated\ntrajectory data. The reliance on the rolled-out trajectory renders the compute\ncost and time prohibitively high. In particular, the correctness of a reasoning\ntrajectory can typically only be judged at its completion, leading to sparse\nrewards in RL or requiring expensive synthetic data generation in expert\niteration-like methods. In this work, we focus on the Automatic Theorem Proving\n(ATP) task and propose a novel verifier-in-the-loop design, which unlike\nexisting approaches that leverage feedback on the entire reasoning trajectory,\nemploys an automated verifier to give intermediate feedback at each step of the\nreasoning process. Using Lean as the verifier, we empirically show that the\nstep-by-step local verification produces a global improvement in the model's\nreasoning accuracy and efficiency.",
      "generated_abstract": "aper, we study the problem of learning a classifier for a given\nnon-linear function. We show that the problem is equivalent to learning a\nclassifier for a class of functions. We provide a polynomial-time algorithm\nthat learns a classifier that generalizes any classifier in the class of\nfunctions that are learned by the algorithm. This result generalizes the\nclassification result of Kumar and Raghavan in 2013. Our algorithm uses a\nvariant of the classifier-learner system that was proposed by Raghavan in 1992.\nThe algorithm uses a set of $O(n^2\\log^2 n)$ decision trees, where $n$ is the\nnumber of training examples. We also provide an $O(n\\log^2 n)$ algorithm for\nthe case where the non-linear function is represented as a polynomial. The\nalgorithm is based on the class",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13392857142857142,
          "p": 0.22727272727272727,
          "f": 0.16853932117661924
        },
        "rouge-2": {
          "r": 0.04054054054054054,
          "p": 0.05405405405405406,
          "f": 0.04633204143408767
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.21212121212121213,
          "f": 0.15730336612043946
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/TH/2502.16336v1",
      "true_abstract": "We present a new method for generating confidence sets within the split\nconformal prediction framework. Our method performs a trainable transformation\nof any given conformity score to improve conditional coverage while ensuring\nexact marginal coverage. The transformation is based on an estimate of the\nconditional quantile of conformity scores. The resulting method is particularly\nbeneficial for constructing adaptive confidence sets in multi-output problems\nwhere standard conformal quantile regression approaches have limited\napplicability. We develop a theoretical bound that captures the influence of\nthe accuracy of the quantile estimate on the approximate conditional validity,\nunlike classical bounds for conformal prediction methods that only offer\nmarginal coverage. We experimentally show that our method is highly adaptive to\nthe local data structure and outperforms existing methods in terms of\nconditional coverage, improving the reliability of statistical inference in\nvarious applications.",
      "generated_abstract": "In this paper, we introduce a novel paradigm for learning probabilistic\nmodeling. The paradigm is based on the idea that a probabilistic modeling\nframework can be defined using a graphical model. We show how this paradigm can\nbe extended to deal with high-dimensional data, which is the focus of this\npaper. We show how to construct probabilistic modeling that is based on a\ngraphical model and how to leverage the structure of the graph to make the\nlearning process more efficient. We illustrate our approach with two simple\nexamples: (i) a model that is based on a directed acyclic graph, and (ii) a\nmodel that is based on a simple graphical model with a hidden node. We also\nshow that our paradigm can be applied to other probabilistic modeling\nframeworks, such as the Gaussian mixture model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.2318840579710145,
          "f": 0.20382165112418368
        },
        "rouge-2": {
          "r": 0.046153846153846156,
          "p": 0.05405405405405406,
          "f": 0.049792526151409736
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.2318840579710145,
          "f": 0.20382165112418368
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.09212v1",
      "true_abstract": "We study how Generative AI (GenAI) adoption is reshaping work. While prior\nstudies show that GenAI enhances role-level productivity and task composition,\nits influence on skills - the fundamental enablers of task execution, and the\nultimate basis for employability - is less understood. Using job postings from\n378 US public firms that recruited explicitly for GenAI skills (2021-2023), we\nanalyze how GenAI adoption shifts the demand for workers' skills. Our findings\nreveal that the advertised roles which explicitly rely on GenAI tools such as\nChatGPT, Copilot, etc., have 36.7 percent higher requirements for cognitive\nskills. Further, a difference-in-differences analysis shows that the demand for\nsocial skills within GenAI roles increases by 5.2 percent post-ChatGPT launch.\nThese emerging findings indicate the presence of a hierarchy of skills in\norganizations with GenAI adoption associated with roles that rely on cognitive\nskills and social skills.",
      "generated_abstract": "r introduces a novel framework for assessing the competitiveness of\nequity funds. We model the equity market as a stochastic, multi-stage game\nwith endogenous capital accumulation and investment decisions. The market\nequilibrium is characterized by the value of the market portfolio and a\nmarket-neutral portfolio. We quantify the market's information content with\nrespect to the value of the market portfolio and the market-neutral portfolio\nusing a novel information-theoretic measure, the Shannon entropy. This\ninformation-theoretic framework allows us to quantify the competitive advantages\nof equity funds by comparing their market-neutral portfolios with the market\nportfolio. We develop a simple, yet effective, methodology for ranking equity\nfunds based on their market-neutral portfolios. This ranking provides a\nuseful perspective on the competitive",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1134020618556701,
          "p": 0.171875,
          "f": 0.13664595794298076
        },
        "rouge-2": {
          "r": 0.0072992700729927005,
          "p": 0.01,
          "f": 0.008438813687268036
        },
        "rouge-l": {
          "r": 0.1134020618556701,
          "p": 0.171875,
          "f": 0.13664595794298076
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/GN/2501.11552v1",
      "true_abstract": "We explore the interplay between sovereign debt default/renegotiation and\nenvironmental factors (e.g., pollution from land use, natural resource\nexploitation). Pollution contributes to the likelihood of natural disasters and\ninfluences economic growth rates. The country can default on its debt at any\ntime while also deciding whether to invest in pollution abatement. The\nframework provides insights into the credit spreads of sovereign bonds and\nexplains the observed relationship between bond spread and a country's climate\nvulnerability. Through calibration for developing and low-income countries, we\ndemonstrate that there is limited incentive for these countries to address\nclimate risk, and the sensitivity of bond spreads to climate vulnerability\nremains modest. Climate risk does not play a relevant role on the decision to\ndefault on sovereign debt. Financial support for climate abatement expenditures\ncan effectively foster climate adaptation actions, instead renegotiation\nconditional upon pollution abatement does not produce any effect.",
      "generated_abstract": "st decade, the U.S. Department of Labor (DOL) has established a\nsystem of benchmarks for determining compensation payable to employees. The\nDOL has developed the following compensation standards: (1) 100th percentile,\n(2) 75th percentile, and (3) 50th percentile. However, the 50th percentile\nremains a contentious metric for measuring wage disparities. This study aims to\nprovide a systematic review of the literature on wage disparities using the 50th\npercentile benchmark. The research employs a systematic literature review\napproach and includes 43 relevant studies. The findings indicate that the 50th\npercentile benchmark does not accurately reflect the wage gap between male and\nfemale workers, particularly in the U.S. and Canada. Additionally",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14423076923076922,
          "p": 0.20270270270270271,
          "f": 0.1685393209847243
        },
        "rouge-2": {
          "r": 0.007042253521126761,
          "p": 0.01,
          "f": 0.008264457960524668
        },
        "rouge-l": {
          "r": 0.14423076923076922,
          "p": 0.20270270270270271,
          "f": 0.1685393209847243
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2503.03232v1",
      "true_abstract": "Prior approaches to lead instrument detection primarily analyze mixture\naudio, limited to coarse classifications and lacking generalization ability.\nThis paper presents a novel approach to lead instrument detection in multitrack\nmusic audio by crafting expertly annotated datasets and designing a novel\nframework that integrates a self-supervised learning model with a track-wise,\nframe-level attention-based classifier. This attention mechanism dynamically\nextracts and aggregates track-specific features based on their auditory\nimportance, enabling precise detection across varied instrument types and\ncombinations. Enhanced by track classification and permutation augmentation,\nour model substantially outperforms existing SVM and CRNN models, showing\nrobustness on unseen instruments and out-of-domain testing. We believe our\nexploration provides valuable insights for future research on audio content\nanalysis in multitrack music settings.",
      "generated_abstract": "r presents a novel approach to enhancing the speech quality of\npure-tone speech signals by leveraging the temporal context of the speech.\nSpecifically, we propose a novel temporal context modeling framework, Temporal\nContext Modeling (TCM), which takes into account the temporal relationships\namong speech signals to enhance speech quality. TCM consists of two key\ncomponents: a Temporal Context Feature Extraction (TCFE) module and a Temporal\nContext Modeling (TCM) module. The TCFE module extracts contextual features\nfrom speech signals using a hybrid convolutional neural network (CNN) and a\nlong short-term memory (LSTM) network. The TCM module then leverages these\ncontextual features to train a deep neural network (DNN) model that\npredicts the temporal context of the speech signal. The model is trained using\nthe speech quality metric",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11956521739130435,
          "p": 0.1506849315068493,
          "f": 0.13333332839963286
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.038461538461538464,
          "f": 0.037037032043896426
        },
        "rouge-l": {
          "r": 0.10869565217391304,
          "p": 0.136986301369863,
          "f": 0.12121211627842077
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.08986v1",
      "true_abstract": "This paper considers communication between a base station (BS) to two users,\neach from one side of a simultaneously transmitting-reflecting reconfigurable\nintelligent surface (STAR-RIS) in the absence of a direct link. Rate-splitting\nmultiple access (RSMA) strategy is employed and the STAR-RIS is subjected to\nphase errors. The users are equipped with a planar fluid antenna system (FAS)\nwith position reconfigurability for spatial diversity. First, we derive the\ndistribution of the equivalent channel gain at the FAS-equipped users,\ncharacterized by a t-distribution. We then obtain analytical expressions for\nthe outage probability (OP) and average capacity (AC), with the latter obtained\nvia a heuristic approach. Our findings highlight the potential of FAS to\nmitigate phase imperfections in STAR-RIS-assisted communications, significantly\nenhancing system performance compared to traditional antenna systems (TAS).\nAlso, we quantify the impact of practical phase errors on system efficiency,\nemphasizing the importance of robust strategies for next-generation wireless\nnetworks.",
      "generated_abstract": "a of digital transformation, there is a growing demand for real-time\ndata processing and analysis. As a result, the need for fast, scalable, and\naccurate data processing algorithms becomes essential. One such algorithm is\nK-means, which is a data clustering algorithm that groups data points into\nclusters based on their proximity. The number of clusters to be determined\ndepends on the data and the number of data points. Traditional approaches to\nthis problem involve iterative optimization, which is time-consuming and\ninaccurate. In this paper, we propose a new approach that uses the\npre-computed K-means results to solve the clustering problem in an\nefficient manner. We use the pre-computed results as the starting point for the\nclustering process, enabling a faster, more accurate solution. We demonstrate\nthe effectiveness of our approach by comparing it to traditional approaches",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13392857142857142,
          "p": 0.1744186046511628,
          "f": 0.15151514660136736
        },
        "rouge-2": {
          "r": 0.02054794520547945,
          "p": 0.0234375,
          "f": 0.021897805240557365
        },
        "rouge-l": {
          "r": 0.13392857142857142,
          "p": 0.1744186046511628,
          "f": 0.15151514660136736
        }
      }
    },
    {
      "paper_id": "math.GR.math/KT/2503.09264v1",
      "true_abstract": "Let $p$ be a prime, we say that a Kummerian oriented pro-$p$ group\n$(G,\\theta)$ has the Bogomolov-Positselski property if $I_\\theta(G)$ is a free\npro-$p$ group. We give a new criterion for an oriented pro-$p$ group to have\nthe Bogomolov-Positselski property based on previous work by Positselski\n(arXiv:1405.0965) and Quadrelli and Weigel (arXiv:2103.12438) linking their\nseemingly unrelated approaches and thereby answering a question posed by\nQuadrelli and Weigel.\n  Under further assumptions, we derive two additional criteria. The first of\nwhich strongly resembles an analogue of the Merkujev-Suslin theorem. The second\nallows to relax the conditions given by Positselski in Theorem 2 of\narXiv:1405.0965. In addition, we show how to make those weaker assumptions\ncomputationally effective in some special cases.",
      "generated_abstract": "The aim of this paper is to construct a new class of solutions to the\ngeneralized KdV equation in the presence of a background magnetic field. The\ngeneralized KdV equation is a nonlinear partial differential equation with\nhypergeometric and quartic nonlinearity terms. In this paper, we introduce a\nclass of solutions to the generalized KdV equation by using a generalized\nhypergeometric function. We further obtain a class of solutions to the\ngeneralized KdV equation by using a generalized hypergeometric function and a\nmagnetic field. We also investigate the stability of the solutions constructed\nby using the generalized hypergeometric function and a magnetic field.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15555555555555556,
          "p": 0.3181818181818182,
          "f": 0.20895521946981518
        },
        "rouge-2": {
          "r": 0.026785714285714284,
          "p": 0.04285714285714286,
          "f": 0.032967028233305835
        },
        "rouge-l": {
          "r": 0.15555555555555556,
          "p": 0.3181818181818182,
          "f": 0.20895521946981518
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.03214v1",
      "true_abstract": "The rice grain quality can be determined from its size and chalkiness. The\ntraditional approach to measure the rice grain size involves manual inspection,\nwhich is inefficient and leads to inconsistent results. To address this issue,\nan image processing based approach is proposed and developed in this research.\nThe approach takes image of rice grains as input and outputs the number of rice\ngrains and size of each rice grain. The different steps, such as extraction of\nregion of interest, segmentation of rice grains, and sub-contours removal,\ninvolved in the proposed approach are discussed. The approach was tested on\nrice grain images captured from different height using mobile phone camera. The\nobtained results show that the proposed approach successfully detected 95\\% of\nthe rice grains and achieved 90\\% accuracy for length and width measurement.",
      "generated_abstract": "nt standard for the assessment of the health of a brain tumor is\ndifferentiating it from benign tumors and cancer. The task is challenging due\nto the complexity of tumor morphology, varying tissue properties, and the\ndifferences in the response of different tumor types to treatments. We propose\na novel approach for distinguishing brain tumors from benign lesions, where the\nbrain is the only target and the tumor is the only object of interest. The\napproach uses a deep learning method, trained on a large dataset of brain\nCT scans, to identify features that distinguish tumors from healthy tissue. Our\napproach uses a convolutional neural network, with a convolutional layer, a\npooling layer, and a fully connected layer. The convolutional layer uses a\nconvolutional filter with a kernel size of 7x7, stride of 1x1,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17073170731707318,
          "p": 0.18421052631578946,
          "f": 0.17721518488062826
        },
        "rouge-2": {
          "r": 0.03305785123966942,
          "p": 0.03305785123966942,
          "f": 0.03305784623967018
        },
        "rouge-l": {
          "r": 0.15853658536585366,
          "p": 0.17105263157894737,
          "f": 0.164556957032527
        }
      }
    },
    {
      "paper_id": "physics.flu-dyn.physics/flu-dyn/2503.09461v1",
      "true_abstract": "We investigate convection in a thin cylindrical gas layer with an imposed\nflux at the bottom and a fixed temperature along the side, using a combination\nof direct numerical simulations and laboratory experiments. The experimental\napproach allows us to extend by two orders of magnitude the explored range in\nterms of flux Rayleigh number. We identify a scaling law governing the\nroot-mean-square horizontal velocity and explain it through a dimensional\nanalysis based on heat transport in the turbulent regime. Using particle image\nvelocimetry, we experimentally confirm, for the most turbulent regimes, the\npresence of a drifting persistent pattern consisting of radial branches, as\nidentified by Rein et al. (2023, J. Fluid Mech. 977, A26). We characterise the\nangular drift frequency and azimuthal wavenumber of this pattern as functions\nof the Rayleigh number. The system exhibits a wide distribution of heat flux\nacross various time scales, with the longest fluctuations attributed to the\nbranch pattern and the shortest to turbulent fluctuations. Consequently, the\nbranch pattern must be considered to better forecast important wall heat flux\nfluctuations, a result of great relevance in the context of nuclear safety, the\ninitial motivation for our study.",
      "generated_abstract": "In this study, we present a novel method to assess the dynamic response of\na cylindrical, rotating nanoparticle undergoing oscillatory flow. We use\nthree-dimensional numerical simulations of nanoparticles oscillating around a\nrotating cylindrical axis to determine the particle response to different\nflow regimes, including those characterized by varying Reynolds numbers. Our\nresults indicate that the flow behavior changes significantly when the Reynolds\nnumber is increased from 100 to 1000, with the particle undergoing both\nperiodic and oscillatory motion. Furthermore, we find that the particle's\nresponse to flow varies depending on the particle size and aspect ratio, with\nsmaller particles showing more complex responses. These results contribute to\nour understanding of the dynamic behavior of nanoparticles undergoing\noscillatory flow, and we discuss how our findings can be applied to future\nnanotechnology applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1450381679389313,
          "p": 0.21348314606741572,
          "f": 0.17272726790950424
        },
        "rouge-2": {
          "r": 0.021505376344086023,
          "p": 0.032520325203252036,
          "f": 0.025889962845383738
        },
        "rouge-l": {
          "r": 0.09923664122137404,
          "p": 0.14606741573033707,
          "f": 0.11818181336404979
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/MN/2409.07812v1",
      "true_abstract": "Emergence during mammalian evolution of concordant and divergent traits of\ngenomic regulatory networks encompassing ubiquitous, qualitatively nearly\nidentical yet quantitatively distinct arrays of sequences of transcription\nfactor binding sites (TFBS) for 716 proteins is reported. A vast majority of\nTFs (770 of 716; 98%) comprising protein constituents of these networks appear\nto share common Gene Ontology (GO) features of sequence-specific\ndouble-stranded DNA binding (GO: 1990837). Genome-wide and individual\nchromosome-level analyses of 17,935 ATAC-seq-defined brain development\nregulatory regions (BDRRs) revealed nearly universal representations of TFBS\nfor TF-constituents of these networks, TFBS densities of which appear\nconsistently higher within thousands BDRRs of Modern Humans compare to\nChimpanzee. Transposable elements (TE), including LTR/HERV, SINE/Alu, SVA, and\nLINE families, appear to harbor and spread genome-wide consensus regulatory\nnodes of identified herein highly conserved sequence-specific double-stranded\nDNA binding networks, selections of TFBS panels of which manifest individual\nchromosome-specific profiles and species-specific divergence patterns.\nCollectively, observations reported in this contribution highlight a previously\nunrecognized essential function of human genomic DNA sequences encoded by TE in\nproviding genome-wide regulatory seed templates of highly conserved\nsequence-specific double-stranded DNA binding networks likely contributing to\ncontinuing divergent genomic evolution of human and chimpanzee brain\ndevelopment.",
      "generated_abstract": "of computational methods in cancer research has revolutionized\nthe field, allowing for more rapid discovery and targeted therapies. However,\nthese advancements are often driven by limited data, which inhibits the\ndevelopment of more effective treatments. We present a framework for integrating\ngenomics, proteomics, and metabolomics data into drug discovery, leveraging\nthese methods to identify biomarkers and pathways that predict drug response. By\nusing the integrated data to drive predictions, we can identify potential\ntherapeutic targets and improve drug discovery. The framework consists of three\nmain steps: (1) data curation, (2) data integration, and (3) prediction. In\nstep (1), we curate the data by integrating multiple datasets, removing\nduplicate records, and standardizing sample properties. In step (2), we\nintegrate the data through machine learning algorithms. Finally",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07633587786259542,
          "p": 0.10752688172043011,
          "f": 0.08928570942960805
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07633587786259542,
          "p": 0.10752688172043011,
          "f": 0.08928570942960805
        }
      }
    },
    {
      "paper_id": "math.FA.math/GN/2502.16712v1",
      "true_abstract": "Let $G$ and $H$ be locally compact groups and consider their associate spaces\nof almost periodic functions $AP(G)$ and $AP(H)$. We investigate the continuous\ngroup homomorphisms induced by isometries of $AP(G)$ into $AP(H)$. Among\nothers, the following results are proved:\n  {\\bf Theorem} Let $G$ and $H$ be $\\sigma$-compact maximally almost periodic\nlocally compact groups. Suppose that $T$ is a non-vanishing linear isometry of\n$AP(G)$ into $AP(H)$ that respects finite dimensional unitary representations.\nThen there is a closed subgroup $H_0\\subseteq H$, a continuous group\nhomomorphism $t$ of $H_0$ onto $G$ and an character $\\gamma\\in \\widehat{H}$\nsuch that $(Tf)(h)=\\gamma (h)~f(t(h))$ for all $h\\in H_0$ and for all $f\\in\nC(G)$.\n  {\\bf Theorem} Let $G$ and $H$ be $LC$ Abelian groups and $H$ is connected.\nSuppose that $T$ is a non-vanishing linear isometry of $AP(G)$ into $AP(H)$\nthat preserves trigonometric polynomials. Then there is a closed subgroup\n$H_0\\subseteq H$, a continuous group homomorphism $t$ of $H_0$ onto $G$, an\nelement $h_0\\in H_0$, a character $\\alpha \\in \\widehat{H}$ and an unimodular\ncomplex number $a$ such that $(Tf)(h)=a\\cdot \\alpha (h)~\\cdot f(t(h-h_0))\\text{\nfor\n  all }h\\in H_0\\text{ and for all }f\\in C(G)\\text{.}$",
      "generated_abstract": "This paper is a continuation of the recent work \\cite{Chen2025} on the\ncharacteristic function of the Bessel process. We extend the results of the\nfirst paper to the case of non-Hermitian Bessel processes. In particular, we\nconsider the case of non-Hermitian Bessel process driven by Brownian motion,\nand prove the existence of the characteristic function of the process. In\naddition, we derive the formula of the characteristic function of the process\nby using the martingale representation theorem. We also derive the formula of\nthe characteristic function of the process by using the Fourier transform. We\nalso present the formula of the characteristic function of the process by using\nthe Fourier transform.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09090909090909091,
          "p": 0.2,
          "f": 0.12499999570312517
        },
        "rouge-2": {
          "r": 0.007692307692307693,
          "p": 0.015151515151515152,
          "f": 0.01020407716576622
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.2,
          "f": 0.12499999570312517
        }
      }
    },
    {
      "paper_id": "math.PR.math/PR/2503.09732v1",
      "true_abstract": "We investigate a modified one-dimensional contact process with varying\ninfection rates. Specifically, the infection spreads at rate $\\lambda_e$ along\nthe boundaries of the infected region and at rate $\\lambda_i$ elsewhere. We\nestablish the existence of an invariant measure when $\\lambda_i = \\lambda_c$\nand $\\lambda_e > \\lambda_c$, where $\\lambda_c$ is the critical parameter of the\nstandard contact process. Moreover, we show that, when viewed from the\nrightmost infected site, the process converges weakly to this invariant\nmeasure. Finally, we prove that along the critical curve within the attractive\nregion of the phase space, the infection almost surely dies out.",
      "generated_abstract": "We study the existence of a solution to the Cauchy problem for the partial\ndifferential equation (PDE)\n\\begin{align*}\n  \\frac{\\BTHYERWB u}{\\BTHYERWB N} = \\frac{\\BTHYERWB v}{\\BTHYERWB M} +\n  \\frac{\\BTHYERWB w}{\\BTHYERWB L} + f \\inon{in $(0,T)\\times\\Omegae$}\n\\end{align*}\nwith the initial data $u(0,\\cdot)\\in L^2(\\Omegae)$. Under some regularity\nconditions on the initial data, we prove the existence of unique solution which\nis then continuously extended to the neighborhood of the unit ball in\n$L^2(\\Omegae)$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16417910447761194,
          "p": 0.20754716981132076,
          "f": 0.18333332840138902
        },
        "rouge-2": {
          "r": 0.0449438202247191,
          "p": 0.0625,
          "f": 0.052287576832842526
        },
        "rouge-l": {
          "r": 0.14925373134328357,
          "p": 0.18867924528301888,
          "f": 0.16666666173472236
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.10029v1",
      "true_abstract": "Hand interactions are increasingly used as the primary input modality in\nimmersive environments, but they are not always feasible due to situational\nimpairments, motor limitations, and environmental constraints. Speech\ninterfaces have been explored as an alternative to hand input in research and\ncommercial solutions, but are limited to initiating basic hand gestures and\nsystem controls. We introduce HandProxy, a system that expands the affordances\nof speech interfaces to support expressive hand interactions. Instead of\nrelying on predefined speech commands directly mapped to possible interactions,\nHandProxy enables users to control the movement of a virtual hand as an\ninteraction proxy, allowing them to describe the intended interactions\nnaturally while the system translates speech into a sequence of hand controls\nfor real-time execution. A user study with 20 participants demonstrated that\nHandProxy effectively enabled diverse hand interactions in virtual\nenvironments, achieving a 100% task completion rate with an average of 1.09\nattempts per speech command and 91.8% command execution accuracy, while\nsupporting flexible, natural speech input with varying levels of control and\ngranularity.",
      "generated_abstract": "t of deep learning has revolutionized computer vision and\ngraph analysis. However, the explosion of data and computational demands\npresent significant challenges for modern systems. In this paper, we present a\ncomprehensive analysis of the memory requirements of popular deep learning\nmodels for graph-structured data, including GIN, GIN-CIFAR, and GIN-STL. We\nexamine the memory requirements of each model, including the model's size,\nnumber of parameters, and the sizes of its input and output tensors. Additionally,\nwe explore the impact of model compression techniques on memory usage,\nparticularly the impact of the use of quantization. Our results reveal that,\nwhen trained on small datasets, models such as GIN-CIFAR and GIN-STL require\nbetween 1.6 GB and 2.5 GB of memory, while models such as GIN and GIN",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09649122807017543,
          "p": 0.13095238095238096,
          "f": 0.11111110622589553
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09649122807017543,
          "p": 0.13095238095238096,
          "f": 0.11111110622589553
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2412.12025v1",
      "true_abstract": "This study is a continuation of previous work, which highlights the nutrient\nenhancement by using rice straw (RS) and pressmud (PM) on vermicomposting.\nHerein, we demonstrate the significant impact of Moringa oleifera derived\nCu/Ni/Co oxide nanoparticles (TmONs) in conjunction with these vermicompost on\nthe growth performance of Abelmoschus esculentus. Vermicompost produced under\nvarious combinations (T0, cow dung (CD) only; T1, 1CD:1RS; T2, 1CD:1PM, and T3,\n1CD:1RS:1PM) were further enriched by blending with biogenic nanoparticles.\nThis strategic combination enhances the nutritional composition of the\nvermicompost, contributing to its overall effectiveness in promoting plant\ngrowth and health. Various analytical techniques, including FTIR, XRD, XPS,\nFESEM-EDX, TEM, and ICP-OES, were employed for comprehensive characterization.\nThe synthesized TmONs with sizes ranging from 13 to 54 nm exhibited distinct\nCuO, NiO, and CoO phases. The vermicompost blended TmONs demonstrated\nsignificant improvements (P < 0.05) in seed germination (167%), coefficient\nvelocity (67%), and vigour index (95%), while reducing the mean germination\ntime by 41% for A. esculentus compared to the control group. The plant culture\ngroup nT3 (T3 + TmONs) showed the best growth performance. Furthermore, trace\nelement concentrations in both soil and plant leaves were found to be below the\nmaximum permissible limits set by WHO (1996). This investigation extends the\nunderstanding of the role played by these nanoparticles in fostering optimal\nconditions for plant growth and development as these micronutrients are\nessential components for several plant enzymes.",
      "generated_abstract": "f this study is to investigate the impact of the epigenetic\ntracking technique, chromatin immunoprecipitation sequencing (ChIP-seq), on\nexperimental data obtained from the SARS-CoV-2 virus. The study was conducted\nusing the MG1655 cell line and the SARS-CoV-2 RNA-12v1 genome, as well as\nexperimental data obtained from the SARS-CoV-2 Spike protein. The ChIP-seq\nmethod was applied to the RNA-12v1 Spike protein and the SARS-CoV-2 RNA-12v1\ngenome, which were then subjected to a bioinformatic analysis. The results\nshowed that ChIP-seq can provide a more detailed understanding of the\nrelationship between DNA and RNA, and the impact of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09883720930232558,
          "p": 0.288135593220339,
          "f": 0.14718614338262034
        },
        "rouge-2": {
          "r": 0.021834061135371178,
          "p": 0.06329113924050633,
          "f": 0.032467528653441165
        },
        "rouge-l": {
          "r": 0.0872093023255814,
          "p": 0.2542372881355932,
          "f": 0.12987012606660306
        }
      }
    },
    {
      "paper_id": "stat.AP.q-bio/QM/2502.16130v1",
      "true_abstract": "The COVID-19 pandemic has adversely affected US public health, resulting in\nover a hundred million cases and more than one million deaths. Vaccination is\nthe key intervention against the COVID-19 pandemic. Multiple COVID-19 vaccines\nare now available for human use. However, a number of factors, including\nsocio-demographic variables, impact the uptake of COVID-19 vaccines. In this\nstudy, we apply a Bayesian mixed-effects model to assess different\nsocio-demographic and spatial factors that influence the acceptance of COVID-19\nvaccines in the US. The fitted mixed-effects model provides the probabilistic\ninference about the vaccine acceptance determinants with uncertainty\nquantification.",
      "generated_abstract": "We present a novel, simple, and powerful framework for evaluating\ninference accuracy and confidence in a Bayesian framework. We introduce a\nnovel approach for constructing and evaluating a credible region for a Bayesian\nmodel using the marginal posterior distribution. We also introduce a novel\nframework for assessing the credibility of Bayesian inference for a single\nparameter, including the case of a single-factor model. This approach\nsignificantly extends existing methods for assessing inference accuracy,\nproviding a framework for incorporating uncertainty into Bayesian model\ninference. We apply these methods to two examples: (1) a single-factor model\nwhere we compare different Bayesian inference methods for inferring a\nsingle-factor model, and (2) a case study of the Bayesian inference for\nmulti-factor models with a single-factor model of interest.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22857142857142856,
          "p": 0.25,
          "f": 0.23880596515927835
        },
        "rouge-2": {
          "r": 0.011111111111111112,
          "p": 0.009433962264150943,
          "f": 0.010204076665974927
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.21875,
          "f": 0.20895521889062166
        }
      }
    },
    {
      "paper_id": "cs.SC.cs/SC/2503.03337v1",
      "true_abstract": "We identify a common scheme in several existing algorithms adressing\ncomputational problems on linear differential equations with polynomial\ncoefficients. These algorithms reduce to computing a linear relation between\nvectors obtained as iterates of a simple differential operator known as\npseudo-linear map.\n  We focus on establishing precise degree bounds on the output of this class of\nalgorithms. It turns out that in all known instances (least common left\nmultiple, symmetric product,. . . ), the bounds that are derived from the\nlinear algebra step using Cramer's rule are pessimistic. The gap with the\nbehaviour observed in practice is often of one order of magnitude, and better\nbounds are sometimes known and derived from ad hoc methods and independent\narguments. We propose a unified approach for proving output degree bounds for\nall instances of the class at once. The main technical tools come from the\ntheory of realisations of matrices of rational functions and their\ndeterminantal denominators.",
      "generated_abstract": "We introduce the Hidden-SAT problem, a new, hard-to-solve variant of\nSAT where the input is hidden. In this setting, each clause contains a\nrandomly-chosen variable and a random value. The goal is to check if the\nvariable is in a subset of the variables in the clause. We show that this problem\nis in PSPACE, and that the complexity of this problem is a function of the\nsize of the input. We also show that the Hidden-SAT problem can be reduced to\nthe Hidden-OR problem, which is in NP.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10679611650485436,
          "p": 0.22916666666666666,
          "f": 0.14569535990175883
        },
        "rouge-2": {
          "r": 0.013245033112582781,
          "p": 0.02564102564102564,
          "f": 0.01746724441639291
        },
        "rouge-l": {
          "r": 0.10679611650485436,
          "p": 0.22916666666666666,
          "f": 0.14569535990175883
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.22001v1",
      "true_abstract": "We examine the effect of item arrangement on choices using a novel\ndecision-making model based on the Markovian exploration of choice sets. This\nmodel is inspired by experimental evidence suggesting that the decision-making\nprocess involves sequential search through rapid stochastic pairwise\ncomparisons. Our findings show that decision-makers following a reversible\nprocess are unaffected by item rearrangements, and further demonstrate that\nthis property can be inferred from their choice behavior. Additionally, we\nprovide a characterization of the class of Markovian models in which the agent\nmakes all possible pairwise comparisons with positive probability. The\nintersection of reversible models and those allowing all pairwise comparisons\nis observationally equivalent to the well-known Luce model. Finally, we\ncharacterize the class of Markovian models for which the initial fixation does\nnot impact the final choice and show that choice data reveals the existence and\ncomposition of consideration sets.",
      "generated_abstract": "We study the equilibrium allocation problem in a two-sided matching market\nwith a single buyer and a single seller. We first characterize the equilibrium\nunder the uniform allocation rule. Then, we analyze the equilibrium under the\nindependent allocation rule. In the independent allocation rule, we characterize\nthe equilibrium for the case of $n=2$ and show that, under the uniform\nallocation rule, the equilibrium allocation is the same as that under the\nindependent allocation rule. We also consider the case of $n\\ge 3$. Under the\nuniform allocation rule, the equilibrium allocation is unique. Under the\nindependent allocation rule, the equilibrium allocation is unique if and only if\nthere are at most two buyers and at most three sellers. In the case of $n=2$,\nwe show that the equilibrium allocation is unique if and only if there are at\nmost two buyers.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14893617021276595,
          "p": 0.25925925925925924,
          "f": 0.18918918455441938
        },
        "rouge-2": {
          "r": 0.03731343283582089,
          "p": 0.060240963855421686,
          "f": 0.0460829445849354
        },
        "rouge-l": {
          "r": 0.14893617021276595,
          "p": 0.25925925925925924,
          "f": 0.18918918455441938
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/AP/2503.10481v1",
      "true_abstract": "In clinical trials involving both mortality and morbidity, an active\ntreatment can influence the observed risk of the first non-fatal event either\ndirectly, through its effect on the non-fatal event process, or indirectly,\nthrough its effect on the death process, or both. Discerning the direct effect\nof treatment on the first non-fatal event holds clinical interest. However,\nwith the competing risk of death, the Cox proportional hazards model that\ntreats death as non-informative censoring and evaluates treatment effects on\ntime to the first non-fatal event provides an estimate of the cause-specific\nhazard ratio, which may not correspond to the direct effect. To obtain the\ndirect effect on the first non-fatal event, within the principal stratification\nframework, we define the principal stratum hazard and introduce the\nProportional Principal Stratum Hazards model. This model estimates the\nprincipal stratum hazard ratio, which reflects the direct effect on the first\nnon-fatal event in the presence of death and simplifies to the hazard ratio in\nthe absence of death. The principal stratum membership is identified using the\nshared frailty model, which assumes independence between the first non-fatal\nevent process and the potential death process from the counterfactual arm,\nconditional on per-subject random frailty. Simulation studies are conducted to\nverify the reliability of our estimators. We illustrate the method using the\nCarvedilol Prospective Randomized Cumulative Survival trial which involves\nheart-failure events.",
      "generated_abstract": "f this paper is to investigate the statistical properties of the\n(signed) distance between two random vectors in a two-dimensional (2D)\nhyperplane. This distance is defined as the minimum of the distance between the\ntwo vectors and the vector of unit length. A key feature of this distance is\nthat it is invariant to the sign of the two vectors. In this paper, we\ninvestigate the properties of this distance with respect to the sign of the\nvectors. We show that the sign of the distance between two vectors in a\n2D hyperplane is invariant under the permutation of the coordinates of the\nvectors. Moreover, we show that the sign of the distance between two vectors in\na 3D hyperplane is invariant under the permutation of the coordinates of the\nvectors. Furthermore, we show that the sign of the distance between two vectors\nin a 4D hyperplane is invariant under the permutation of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12295081967213115,
          "p": 0.29411764705882354,
          "f": 0.17341040046643735
        },
        "rouge-2": {
          "r": 0.0223463687150838,
          "p": 0.047058823529411764,
          "f": 0.030303025936926707
        },
        "rouge-l": {
          "r": 0.10655737704918032,
          "p": 0.2549019607843137,
          "f": 0.15028901318320034
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2409.18443v1",
      "true_abstract": "The primary objective of this study was to examine the impact of the US\nsovereign credit rating downgrade on its equity market. Utilizing the event\nstudy methodology, a sample of three most capitalized listed companies --\nMicrosoft, Apple, and Amazon -- and the equity market index -- S&P500 -- were\nused as the proxy for the overall equity market. Three market models were\nconstructed within the estimation window to determine the expected daily\nreturns on the selected companies stocks. The result showed that the sovereign\ncredit rating downgrade of the US government debt did not have any significant\neffects on the US equity market.",
      "generated_abstract": "f this paper is to provide a framework for analyzing the\nfinancial risks posed by cryptocurrencies. We present a systematic approach to\nquantifying the risk exposure of investors in different cryptocurrencies and\ntheir portfolios, and the effect of different risk management strategies on\nthese risks. We also explore how the use of machine learning can help identify\nthese risks more accurately. The framework we present is based on the\nprinciples of quantitative finance, and provides a clear understanding of the\nrisks associated with cryptocurrencies and their portfolios. The framework\nallows for the evaluation of different risk management strategies, including\nrisk aversion, risk neutral, risk minimization, and risk diversification. The\nresults of this study provide investors with a clear understanding of the\nrisks associated with cryptocurrencies and their portfolios,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15151515151515152,
          "p": 0.14705882352941177,
          "f": 0.14925372634439757
        },
        "rouge-2": {
          "r": 0.06451612903225806,
          "p": 0.05504587155963303,
          "f": 0.0594059356254293
        },
        "rouge-l": {
          "r": 0.15151515151515152,
          "p": 0.14705882352941177,
          "f": 0.14925372634439757
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.09979v1",
      "true_abstract": "Silicon has striking similarity with carbon and is found in plant cells.\nHowever, there is no specific role that has been assigned to silicon in the\nlife cycle of plants. The amount of silicon in plant cells is species specific\nand can reach levels comparable to macronutrients. Silicon is the central\nelement for artificial intelligence, nanotechnology and digital revolution thus\ncan act as an informational molecule like nucleic acids while the diverse\nbonding potential of silicon with different chemical species is analogous to\ncarbon and thus can serve as a structural candidate such as proteins. The\ndiscovery of large amounts of silicon on Mars and the moon along with the\nrecent developments of enzyme that can incorporate silicon into organic\nmolecules has propelled the theory of creating silicon-based life. More\nrecently, bacterial cytochrome has been modified through directed evolution\nsuch that it could cleave silicon-carbon bonds in organo-silicon compounds thus\nconsolidating on the idea of utilizing silicon in biomolecules. In this article\nthe potential of silicon-based life forms has been hypothesized along with the\nreasoning that autotrophic virus-like particles can be a lucrative candidate to\ninvestigate such potential. Such investigations in the field of synthetic\nbiology and astrobiology will have corollary benefit on Earth in the areas of\nmedicine, sustainable agriculture and environmental sustainability.\nBibliometric analysis indicates an increasing interest in synthetic biology.\nGermany leads in research related to plant synthetic biology, while\nBiotechnology and Biological Sciences Research Council (BBSRC) at UK has\nhighest financial commitments and Chinese Academy of Sciences generates the\nhighest number of publications in the field.",
      "generated_abstract": "opment of novel diagnostic tools is crucial for improving\ncrop protection strategies. However, current diagnostic tools often suffer from\nhigh variability, low sensitivity, and low specificity. To address these\nlimitations, we developed a novel approach that combines artificial intelligence\n(AI) and machine learning (ML) with a diagnostic tool to detect disease\nindicators. The methodology integrates the use of artificial neural networks\n(ANNs) and reinforcement learning (RL), which are powerful machine learning\ntechniques. ANNs are widely used in diagnostic applications because they\nsimulate complex biological processes, such as the interaction of pathogens and\nhost cells. RL, on the other hand, is used to model complex decision-making\nprocesses in various fields, including healthcare. In this study, we used a\ndiagnostic tool called the Plant Pathogen Detection System (PP",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12101910828025478,
          "p": 0.19791666666666666,
          "f": 0.15019762374916043
        },
        "rouge-2": {
          "r": 0.0125,
          "p": 0.024793388429752067,
          "f": 0.016620494158271926
        },
        "rouge-l": {
          "r": 0.10828025477707007,
          "p": 0.17708333333333334,
          "f": 0.13438734706931857
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.10264v1",
      "true_abstract": "While peer review enhances writing and research quality, harsh feedback can\nfrustrate and demotivate authors. Hence, it is essential to explore how\ncritiques should be delivered to motivate authors and enable them to keep\niterating their work. In this study, we explored the impact of appending an\nautomatically generated positive summary to the peer reviews of a writing task,\nalongside varying levels of overall evaluations (high vs. low), on authors'\nfeedback reception, revision outcomes, and motivation to revise. Through a 2x2\nonline experiment with 137 participants, we found that adding an AI-reframed\npositive summary to otherwise harsh feedback increased authors' critique\nacceptance, whereas low overall evaluations of their work led to increased\nrevision efforts. We discuss the implications of using AI in peer feedback,\nfocusing on how AI-driven critiques can influence critique acceptance and\nsupport research communities in fostering productive and friendly peer feedback\npractices.",
      "generated_abstract": "This paper explores the challenges of designing effective digital\nlearning ecosystems that enable student success, particularly for students\nwith disabilities. We present the results of a survey of students with\ndisabilities and educators from the University of California system to\ninvestigate the barriers to access, affordability, and accessibility in\nlearning ecosystems. Our findings highlight the need for digital tools and\nservices that are accessible, inclusive, and support student success. We\nconducted focus groups to explore the experiences of students with disabilities\nwho use assistive technology in the classroom. The findings revealed\ndisparities in the accessibility of educational technologies, including\ninstructional materials and tools, and the cost of access. These findings\nhighlight the need for digital tools and services that are accessible,\ninclusive, and support student success.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12,
          "p": 0.16901408450704225,
          "f": 0.14035087233678756
        },
        "rouge-2": {
          "r": 0.02158273381294964,
          "p": 0.029411764705882353,
          "f": 0.02489626067801959
        },
        "rouge-l": {
          "r": 0.11,
          "p": 0.15492957746478872,
          "f": 0.12865496590403902
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2411.12769v1",
      "true_abstract": "The Genebass dataset, released by Karczewski et al. (2022), provides a\ncomprehensive resource elucidating associations between genes and 4,529\nphenotypes based on nearly 400,000 exomes from the UK Biobank. This extensive\ndataset enables the evaluation of gene set enrichment across a wide range of\nphenotypes, facilitating the inference of associations between specified gene\nsets and phenotypic traits. Despite its potential, no established method for\napplying gene set enrichment analysis (GSEA) to Genebass data exists. To\naddress this gap, we propose utilizing fast pre-ranked gene set enrichment\nanalysis (FGSEA) as a novel approach to determine whether a specified set of\ngenes is significantly enriched in phenotypes within the UK Biobank. We\ndeveloped an R package, ukbFGSEA, to implement this analysis, completed with a\nhands-on tutorial. Our approach has been validated by analyzing gene sets\nassociated with autism spectrum disorder, developmental disorder, and\nneurodevelopmental disorders, demonstrating its capability to reveal\nestablished and novel associations.",
      "generated_abstract": "opment of new and more effective drugs is a vital task in modern\nbiology and medicine. A significant challenge in drug discovery is the\nunderstanding of the underlying mechanisms of cellular responses to drugs.\nMolecular dynamics simulations are an important tool for studying the\nresponse of cells to drugs. However, the calculation of the total free energy\nis a challenging problem in molecular dynamics simulations, particularly when\nthe number of degrees of freedom is large. In this study, we develop a\nsimple-to-use, efficient, and accurate software package for the calculation of\nthe total free energy using the Langevin Monte Carlo method. Our method\nprovides an efficient and reliable way to calculate the total free energy\nduring the drug-response simulation. The method can be applied to a wide range\nof systems, including cellular responses to drugs. The efficiency of our method\nis demonstrated through a variety of simulations, including drug-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1574074074074074,
          "p": 0.18681318681318682,
          "f": 0.17085426639327303
        },
        "rouge-2": {
          "r": 0.02112676056338028,
          "p": 0.022556390977443608,
          "f": 0.021818176823538333
        },
        "rouge-l": {
          "r": 0.1574074074074074,
          "p": 0.18681318681318682,
          "f": 0.17085426639327303
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.01533v5",
      "true_abstract": "Elite economics PhD programs aim to train graduate students for a lifetime of\nacademic research. This paper asks how advising affects graduate students'\npost-PhD research productivity. Advising is highly concentrated: at the eight\nhighly-selective schools in our study, a minority of advisors do most of the\nadvising work. We quantify advisor attributes such as an advisor's own research\noutput and aspects of the advising relationship like coauthoring and research\nfield affinity that might contribute to student research success. Students\nadvised by research-active, prolific advisors tend to publish more, while\ncoauthoring has no effect. Student-advisor research affinity also predicts\nstudent success. But a school-level aggregate production function provides much\nweaker evidence of causal effects, suggesting that successful advisors attract\nstudents likely to succeed-without necessarily boosting their students' chances\nof success. Evidence for causal effects is strongest for a measure of advisors'\nown research output. Aggregate student research output appears to scale\nlinearly with graduate student enrollment, with no evidence of negative\nclass-size effects. An analysis of gender differences in research output shows\nmale and female graduate students to be equally productive in the first few\nyears post-PhD, but female productivity peaks sooner than male productivity.",
      "generated_abstract": "r studies the impact of the COVID-19 pandemic on the labour market,\nusing a dynamic panel data model to analyze the evolution of unemployment\nrates and labour market participation over the period 2010-2021. Our results\nshow that the COVID-19 crisis had a significant negative impact on the\nemployment rate, with a 5% reduction in employment. The main factors behind\nthis decrease were the closure of businesses and the impact of the pandemic on\neducation and training. The impact of the pandemic on labour market\nparticipation was more limited, with a 2% reduction. The results suggest that\npolicymakers should take into account the effects of the pandemic on the\nlabour market in their decisions, such as increasing social protection and\ntraining for job creation. The paper also suggests that policy interventions\nshould focus on improving access to education and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12403100775193798,
          "p": 0.19753086419753085,
          "f": 0.152380947642177
        },
        "rouge-2": {
          "r": 0.01092896174863388,
          "p": 0.017241379310344827,
          "f": 0.0133779216724662
        },
        "rouge-l": {
          "r": 0.08527131782945736,
          "p": 0.13580246913580246,
          "f": 0.10476190002312948
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/DC/2503.10217v1",
      "true_abstract": "Fine-tuning plays a crucial role in enabling pre-trained LLMs to evolve from\ngeneral language comprehension to task-specific expertise. To preserve user\ndata privacy, federated fine-tuning is often employed and has emerged as the de\nfacto paradigm. However, federated fine-tuning is prohibitively inefficient due\nto the tension between LLM complexity and the resource constraint of end\ndevices, incurring unaffordable fine-tuning overhead. Existing literature\nprimarily utilizes parameter-efficient fine-tuning techniques to mitigate\ncommunication costs, yet computational and memory burdens continue to pose\nsignificant challenges for developers. This work proposes DropPEFT, an\ninnovative federated PEFT framework that employs a novel stochastic transformer\nlayer dropout method, enabling devices to deactivate a considerable fraction of\nLLMs layers during training, thereby eliminating the associated computational\nload and memory footprint. In DropPEFT, a key challenge is the proper\nconfiguration of dropout ratios for layers, as overhead and training\nperformance are highly sensitive to this setting. To address this challenge, we\nadaptively assign optimal dropout-ratio configurations to devices through an\nexploration-exploitation strategy, achieving efficient and effective\nfine-tuning. Extensive experiments show that DropPEFT can achieve a\n1.3-6.3\\times speedup in model convergence and a 40%-67% reduction in memory\nfootprint compared to state-of-the-art methods.",
      "generated_abstract": "We study the problem of designing and evaluating a classifier for a given\nproblem instance, given access to only a limited set of examples. To\naccommodate this, we propose a novel generalization of the Convex Maximum\nAverage Point (CMACP) algorithm, which has been shown to be a powerful tool for\ngeneralization analysis. In particular, we show that for any classifier\n$\\mathcal{C}$ for the problem instance, the CMACP algorithm can produce a\nclassifier $\\widehat{\\mathcal{C}}$ for the problem instance with the same\noptimality guarantee as $\\mathcal{C}$. We provide a detailed theoretical\nanalysis of the CMACP algorithm, providing both theoretical guarantees for\noptimality and computational complexity. Our theoretical analysis is\naccompanied by extensive experiments on various classification problems,\ndemonstrating the effectiveness of the CMACP algorithm in practice.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13986013986013987,
          "p": 0.25316455696202533,
          "f": 0.180180175595731
        },
        "rouge-2": {
          "r": 0.010582010582010581,
          "p": 0.017857142857142856,
          "f": 0.013289031872056033
        },
        "rouge-l": {
          "r": 0.11888111888111888,
          "p": 0.21518987341772153,
          "f": 0.153153148568704
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.16677v1",
      "true_abstract": "Understanding cooperation in social systems is challenging because the\never-changing rules that govern societies interact with individual actions,\nresulting in intricate collective outcomes. In virtual-world experiments, we\nallowed people to make changes in the systems that they are making decisions\nwithin and investigated how they weigh the influence of different rules in\ndecision-making. When choosing between worlds differing in more than one rule,\na naive heuristics model predicted participants decisions as well, and in some\ncases better, than game earnings (utility) or by the subjective quality of\nsingle rules. In contrast, when a subset of engaged participants made\ninstantaneous (within-world) decisions, their behavior aligned very closely\nwith objective utility and not with the heuristics model. Findings suggest\nthat, whereas choices between rules may deviate from rational benchmarks, the\nfrequency of real time cooperation decisions to provide feedback can be a\nreliable indicator of the objective utility of these rules.",
      "generated_abstract": "igate the role of gene expression dynamics in the emergence of\ngenetic polymorphisms. We consider a population of $N$ individuals whose\ngene expression levels follow a stochastic process with a single stochastic\ndynamics that depends on the gene expression levels of their neighbors. We\nformulate a general model for this process and study its dynamics by analyzing\ntheir autocorrelation function. We derive the mean and variance of the\nautocorrelation function and show that it exhibits two distinct regimes:\n``short-range autocorrelation'' and ``long-range autocorrelation''. We then\nshow how the autocorrelation function of gene expression can be computed using\na deterministic algorithm and discuss its impact on the analysis of gene\nexpression dynamics. We also provide a heuristic approach to infer the\ndynamics of gene expression from the mean and variance of the autocorrelation\nfunction",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1574074074074074,
          "p": 0.23943661971830985,
          "f": 0.18994412929184495
        },
        "rouge-2": {
          "r": 0.02054794520547945,
          "p": 0.02702702702702703,
          "f": 0.02334629859468074
        },
        "rouge-l": {
          "r": 0.12962962962962962,
          "p": 0.19718309859154928,
          "f": 0.15642457621921926
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2502.07518v1",
      "true_abstract": "Implied volatility IV is a key metric in financial markets, reflecting market\nexpectations of future price fluctuations. Research has explored IV's\nrelationship with moneyness, focusing on its connection to the implied Hurst\nexponent H. Our study reveals that H approaches 1/2 when moneyness equals 1,\nmarking a critical point in market efficiency expectations. We developed an IV\nmodel that integrates H to capture these dynamics more effectively. This model\nconsiders the interaction between H and the underlying-to-strike price ratio\nS/K, crucial for capturing IV variations based on moneyness. Using Optuna\noptimization across multiple indexes, the model outperformed SABR and fSABR in\naccuracy. This approach provides a more detailed representation of market\nexpectations and IV-H dynamics, improving options pricing and volatility\nforecasting while enhancing theoretical and pratcical financial analysis.",
      "generated_abstract": "aper, we develop a novel approach for constructing a risk-adjusted\nhigh-frequency trading strategy, which is based on the concept of a\nstochastic-volatility-based mean-variance optimization. Specifically, we use\nthe Monte Carlo simulation method to simulate the prices of high-frequency\nstocks, and then construct a risk-adjusted return based on the estimated\nhigh-frequency stock prices. The return of a stock is calculated by the\ndifference between the stock's actual and estimated prices. In the risk-adjusted\nreturn, we consider two factors: the stock's mean return and the stock's\nvolatility. The mean return is calculated based on the actual high-frequency\nstock prices, while the volatility is calculated based on the estimated high-\nfrequency stock prices. We also consider the mean-variance optimization problem\nin the high-fre",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.26229508196721313,
          "f": 0.2038216512994443
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.019801980198019802,
          "f": 0.01762114043509618
        },
        "rouge-l": {
          "r": 0.15625,
          "p": 0.2459016393442623,
          "f": 0.1910827977962596
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/RO/2503.10349v1",
      "true_abstract": "This study proposes a new Gaussian Mixture Filter (GMF) to improve the\nestimation performance for the autonomous robotic radio signal source search\nand localization problem in unknown environments. The proposed filter is first\ntested with a benchmark numerical problem to validate the performance with\nother state-of-practice approaches such as Particle Gaussian Mixture (PGM)\nfilters and Particle Filter (PF). Then the proposed approach is tested and\ncompared against PF and PGM filters in real-world robotic field experiments to\nvalidate its impact for real-world robotic applications. The considered\nreal-world scenarios have partial observability with the range-only measurement\nand uncertainty with the measurement model. The results show that the proposed\nfilter can handle this partial observability effectively whilst showing\nimproved performance compared to PF, reducing the computation requirements\nwhile demonstrating improved robustness over compared techniques.",
      "generated_abstract": "This paper presents a novel framework for the analysis of robotic\nmanipulators, focusing on the interactions between the robotic system and\nenvironmental dynamics. We introduce a novel framework for the analysis of\nmanipulators, focusing on the interactions between the robotic system and\nenvironmental dynamics. Our approach is based on a hybrid system model that\ncombines a dynamical model of the manipulator with a non-linear model of the\nenvironment, allowing for the analysis of the system dynamics in terms of\ndynamical and non-dynamical components. The framework is capable of analyzing\nthe interactions between the manipulator and the environment, including\ncollisions, non-linear dynamics, and uncertainty, providing a comprehensive\nframework for the analysis of robotic systems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1511627906976744,
          "p": 0.26,
          "f": 0.19117646593858142
        },
        "rouge-2": {
          "r": 0.024193548387096774,
          "p": 0.0379746835443038,
          "f": 0.02955664549200494
        },
        "rouge-l": {
          "r": 0.1511627906976744,
          "p": 0.26,
          "f": 0.19117646593858142
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2411.05851v1",
      "true_abstract": "This paper explores a GIS-based application of the conditional p-median\nproblem (where p = 1) in last-mile delivery logistics. The rapid growth of\ne-commerce in Pakistan has primarily benefited logistics companies, which face\nthe challenge of resolving inefficiencies in the existing infrastructure and\nscaling effectively to meet increasing demand. Addressing these challenges\nwould not only reduce operational costs but also lower carbon footprints. We\npresent an algorithm that utilizes road-network-based distances to determine\nthe optimal location for a new hub facility, a problem known in operations\nresearch as the conditional p-median problem. The algorithm optimizes the\nplacement of a new facility, given q existing facilities. The past delivery\ndata for this research was provided by Muller and Phipps Logistics Pakistan.\nOur method involves constructing a distance matrix between candidate hub\nlocations and past delivery points, followed by a grid search to identify the\noptimal hub location. To simulate the absence of past delivery data, we\nrepeated the process using the population distribution of Lahore. Our results\ndemonstrate a 16% reduction in average delivery distance with the addition of a\nnew hub.",
      "generated_abstract": "In this paper, we propose a novel, generic, and efficient algorithm for\ndecomposing and solving polynomial systems in a distributed fashion. Our\napproach can be applied to both sparse and dense polynomial systems and does not\nrequire the knowledge of the system's structure. The algorithm is based on\nusing a combination of the LU decomposition, rank-revealing matrices, and\nlow-rank matrix approximation. Furthermore, it allows for an efficient\nsolution of the resulting linear systems. We demonstrate the effectiveness of\nour approach on several real-world benchmarks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1487603305785124,
          "p": 0.2857142857142857,
          "f": 0.1956521694098536
        },
        "rouge-2": {
          "r": 0.011764705882352941,
          "p": 0.025,
          "f": 0.015999995648001183
        },
        "rouge-l": {
          "r": 0.1322314049586777,
          "p": 0.25396825396825395,
          "f": 0.17391303897507102
        }
      }
    },
    {
      "paper_id": "math.OC.math/OC/2503.10006v1",
      "true_abstract": "We study a novel class of algorithms for solving model-free feedback\noptimization problems in dynamical systems. The key novelty is the introduction\nof \\emph{persistent resetting learning integrators} (PRLI), which are\nintegrators that are reset at the same frequency at which the plant is dithered\nusing exploratory signals for model-free optimization. It is shown that PRLIs\ncan serve as core mechanisms for real-time gradient estimation in online\nfeedback-optimization tasks where only cost function measurements are\navailable. In particular, unlike existing approaches based on approximation\ntheory, such as averaging or finite-differences, PRLIs can produce global\nreal-time gradient estimates of cost functions, with uniformly bounded\nperturbations of arbitrarily small magnitude. In this sense, PRLIs function as\nrobust \\emph{hybrid} \"Oracles\" suitable for interconnection with discrete-time\noptimization algorithms that optimize the performance of continuous-time\ndynamical plants in closed-loop operation. Compared to existing methods, PRLIs\nyield \\emph{global} stability properties for a broad class of cost functions,\nsurpassing the local or semi-global guarantees offered by traditional\napproaches based on perturbation and approximation theory. The proposed\nframework naturally bridges physical systems, modeled as continuous-time plants\nwhere continuous exploration is essential, with digital algorithms, represented\nas discrete-time optimization methods. The main results are illustrated using\ndifferent numerical examples.",
      "generated_abstract": "We introduce a new class of variational inequalities for functions,\nnamed as the generalized Stokes systems, which generalize the classical Stokes\nsystems. We show that these generalized Stokes systems have the same variational\ncharacteristics as the classical Stokes systems and also share the same\ngeometric properties as the classical Stokes systems. In particular, we show\nthat the generalized Stokes systems have the same convexity properties as the\nclassical Stokes systems and the same convexity properties as the generalized\nHamilton-Jacobi equations. In addition, we show that the generalized Stokes\nsystems can be seen as the generalizations of the Hamilton-Jacobi equations,\nand the generalized Hamilton-Jacobi equations are the generalizations of the\nclassical Stokes systems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13970588235294118,
          "p": 0.4318181818181818,
          "f": 0.211111107417284
        },
        "rouge-2": {
          "r": 0.015789473684210527,
          "p": 0.046875,
          "f": 0.023622043474487556
        },
        "rouge-l": {
          "r": 0.1323529411764706,
          "p": 0.4090909090909091,
          "f": 0.19999999630617288
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.14258v1",
      "true_abstract": "Accelerating the deep transformation and upgrading of industrial structure\nand forming new quality productive forces are essential components for China to\nachieve the great rejuvenation of the Chinese Dream. After more than 40 years\nof rapid development, China has entered the \"new normal\" of development, making\nthe advancement of new quality productive forces an urgent task. This paper\nreviews the evolution of China's industrial structure, argues the necessity for\na new round of deep industrial transformation, and explores the impact of\nindustrial structure transformation and upgrading on the level of new quality\nproductive forces using various methods. The research findings are as\nfollows:(1)The deep transformation and upgrading of the industrial structure\ncan significantly promote the development of new quality productive forces, but\nthere are obvious regional differences.(2)The core indicator of the improvement\nin the level of new quality productive forces is the enhancement of total\nfactor productivity. Furthermore, this paper summarizes past industrial\ndevelopment processes and the challenges faced, and analyzes and discusses the\npotential challenges that may arise in promoting the development of new quality\nproductive forces through deep industrial structure transformation, based on\nempirical research results.",
      "generated_abstract": "-19 pandemic has caused a widespread impact on the global economy\nand exposed the fragility of the existing economic systems. The COVID-19\npandemic has also highlighted the need to enhance the resilience of the global\neconomy, especially in the face of uncertainties. In this paper, we propose a\nframework to measure the resilience of the global economy through the\nmacro-financial variables and economic indicators. Our methodology integrates\nthe empirical analysis of macro-financial variables and economic indicators,\nthe empirical analysis of the correlation between the macro-financial variables\nand economic indicators, and the analysis of the correlation between the\ncorrelation between the macro-financial variables and economic indicators and\nthe global economic system. Our results show that the global economy is not\nonly vulnerable to external shocks but also exposed to internal",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.27419354838709675,
          "f": 0.20731706846817383
        },
        "rouge-2": {
          "r": 0.026143790849673203,
          "p": 0.04395604395604396,
          "f": 0.0327868805687322
        },
        "rouge-l": {
          "r": 0.14705882352941177,
          "p": 0.24193548387096775,
          "f": 0.1829268245657348
        }
      }
    },
    {
      "paper_id": "hep-ex.hep-ex/2503.09742v1",
      "true_abstract": "Measurements are presented of the W and Z boson production cross sections in\nproton-proton collisions at a center-of-mass energy of 13.6 TeV. Data collected\nin 2022 and corresponding to an integrated luminosity of 5.01 fb$^{-1}$ with\none or two identified muons in the final state are analyzed. The results for\nthe products of total inclusive cross sections and branching fractions for\nmuonic decays of W and Z bosons are 11.93 $\\pm$ 0.08 (syst) $\\pm$ 0.17 (lumi)\n$^{+0.07}_{-0.07}$ (acc) nb for W$^+$ boson production, 8.86 $\\pm$ 0.06 (syst)\n$\\pm$ 0.12 (lumi) $^{+0.05}_{-0.06}$ (acc) nb for W$^-$ boson production, and\n2.021 $\\pm$ 0.009 (syst) $\\pm$ 0.028 (lumi) $^{+0.011}_{-0.013}$ (acc) nb for\nthe Z boson production in the dimuon mass range of 60-120 GeV, all with\nnegligible statistical uncertainties. Furthermore, the corresponding fiducial\ncross sections, as well as cross section ratios for both fiducial and total\nphase space, are provided. The ratios include charge-separated results for W\nboson production (W$^+$ and W$^-$) and the sum of the two contributions\n(W$^\\pm$), each relative to the measured Z boson production cross section.\nAdditionally, the ratio of the measured cross sections for W$^+$ and W$^-$\nboson production is reported. All measurements are in agreement with\ntheoretical predictions, calculated at next-to-next-to-leading order accuracy\nin quantum chromodynamics.",
      "generated_abstract": "We report on a new technique to measure the absolute cross section of\nexotic processes, using a 500 MeV/c muon beam. The technique is based on a\nmulti-wire proportional chamber (MWPC) calorimeter, which is placed inside a\nscintillator chamber. The calorimeter detects the energy deposited by the muon\nin the scintillator chamber, and is able to measure the energy of the muon\nbeam, in addition to the muon energy. The calorimeter was constructed for this\npurpose, and a proof-of-principle measurement of the cross section of the\nexotic process $e^+e^-\\to e^+e^-e^-\\mu^+\\mu^-$ was performed. The results are\ncompared to the expected theoretical cross section, and are consistent with\ntheoretical expectations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12698412698412698,
          "p": 0.25396825396825395,
          "f": 0.169312164867725
        },
        "rouge-2": {
          "r": 0.03664921465968586,
          "p": 0.07368421052631578,
          "f": 0.04895104451440208
        },
        "rouge-l": {
          "r": 0.12698412698412698,
          "p": 0.25396825396825395,
          "f": 0.169312164867725
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.06596v1",
      "true_abstract": "Intelligent reflecting surfaces (IRSs) are envisioned to enhance the\nperformance of mmWave wireless systems. In practice, multiple mobile operators\n(MO) coexist in an area and provide simultaneous and independent services to\nuser-equipments (UEs) on different frequency bands. Then, if each MO deploys an\nIRS to enhance its performance, the IRSs also alter the channels of UEs of\nother MOs. In this context, this paper addresses the following questions: can\nan MO still continue to control its IRS independently of other MOs and IRSs? Is\njoint optimization of IRSs deployed by different MOs and inter-MO cooperation\nneeded? To that end, by considering the mmWave bands, we first derive the\nergodic sum spectral efficiency (SE) in a $2$-MO system for the following\nschemes: 1) joint optimization of an overall phase angle of the IRSs with MO\ncooperation, 2) MO cooperation via time-sharing, and 3) no cooperation between\nthe MOs. We find that even with no cooperation between the MOs, the performance\nof a given MO is not degraded by the presence of an out-of-band (OOB) MO\ndeploying and independently controlling its own IRS. On the other hand, the SE\ngain obtained at a given MO using joint optimization and cooperation over the\nno-cooperation scheme decreases inversely with the number of elements in the\nIRS deployed by the other MO. We generalize our results to a multiple MO setup\nand show that the gain in the sum-SE over the no-cooperation case increases at\nleast linearly with the number of OOB MOs. Finally, we numerically verify our\nfindings and conclude that every MO can independently operate and tune its IRS;\ncooperation via optimizing an overall phase only brings marginal benefits in\npractice.",
      "generated_abstract": "t advancements in wireless communication technology have enabled\nlarge-scale distributed antenna systems (LASS) that provide massive\ntransmission capacity by distributing antennas among the users. However, the\ncomplexity of the problem remains challenging due to the non-convexity of the\noptimization problem and the difficulty in obtaining closed-form solutions.\nExisting solutions are often based on the alternating optimization strategy,\nwhich requires a large number of iterations and is computationally expensive.\nIn this paper, we propose a novel solution to the problem of large-scale LASS\noptimization by leveraging the alternating direction method of multipliers\n(ADMM). Specifically, we introduce a novel weighted sum-rate-based objective\nfunction that balances the throughput and the sum-rate constraints. To\nfacilitate the convergence, we introduce a novel weighted sum-rate-based\npenalty function that balances the sum-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12987012987012986,
          "p": 0.24096385542168675,
          "f": 0.16877636675675206
        },
        "rouge-2": {
          "r": 0.012096774193548387,
          "p": 0.02727272727272727,
          "f": 0.01675977227926827
        },
        "rouge-l": {
          "r": 0.12337662337662338,
          "p": 0.2289156626506024,
          "f": 0.16033754819135124
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.07296v1",
      "true_abstract": "This paper introduces the concept of wireless-powered zero-energy\nreconfigurable intelligent surface (zeRIS), and investigates a wireless-powered\nzeRIS aided communication system in terms of security, reliability and energy\nefficiency. In particular, we propose three new wireless-powered zeRIS modes:\n1) in mode-I, N reconfigurable reflecting elements are adjusted to the optimal\nphase shift design of information user to maximize the reliability of the\nsystem; 2) in mode-II, N reconfigurable reflecting elements are adjusted to the\noptimal phase shift design of cooperative jamming user to maximize the security\nof the system; 3) in mode-III, N1 and N2 (N1+N2=N) reconfigurable reflecting\nelements are respectively adjusted to the optimal phase shift designs of\ninformation user and cooperative jamming user to balance the reliability and\nsecurity of the system. Then, we propose three new metrics, i.e., joint outage\nprobability (JOP), joint intercept probability (JIP), and secrecy energy\nefficiency (SEE), and analyze their closed-form expressions in three modes,\nrespectively. The results show that under high transmission power, all the\ndiversity gains of three modes are 1, and the JOPs of mode-I, mode-II and\nmode-III are improved by increasing the number of zeRIS elements, which are\nrelated to N2, N, and N^2_1, respectively. In addition, mode-I achieves the\nbest JOP, while mode-II achieves the best JIP among three modes. We exploit two\nsecurity-reliability trade-off (SRT) metrics, i.e., JOP versus JIP, and\nnormalized joint intercept and outage probability (JIOP), to reveal the SRT\nperformance of the proposed three modes. It is obtained that mode-II\noutperforms the other two modes in the JOP versus JIP, while mode-III and\nmode-II achieve the best performance of normalized JIOP at low and high\ntransmission power, respectively.",
      "generated_abstract": "In this paper, we propose a novel deep learning approach for the multi-channel\n(4x4) array source separation problem, aiming to learn a mapping from the\ntime-frequency (T-F) spectra of the source signals to the separation masks.\nUnlike traditional separation methods that require the ground truth\nseparation masks, our approach learns the separation masks from the T-F spectra\nof the source signals. Moreover, we develop a novel neural network that\nintroduces a multi-scale hierarchical attention mechanism, enabling the network\nto learn the source separation masks with different scales. To further\nimprove the performance, we propose a novel method for enhancing the network\nwith multi-scale features. Extensive experiments demonstrate that our\napproach outperforms existing methods in terms of separation accuracy,\nspeech-preservation ability, and computational efficiency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10218978102189781,
          "p": 0.1917808219178082,
          "f": 0.13333332879773258
        },
        "rouge-2": {
          "r": 0.02262443438914027,
          "p": 0.047619047619047616,
          "f": 0.030674842258836106
        },
        "rouge-l": {
          "r": 0.10218978102189781,
          "p": 0.1917808219178082,
          "f": 0.13333332879773258
        }
      }
    },
    {
      "paper_id": "cs.GR.cs/GR/2503.09640v1",
      "true_abstract": "Rendering realistic human-object interactions (HOIs) from sparse-view inputs\nis challenging due to occlusions and incomplete observations, yet crucial for\nvarious real-world applications. Existing methods always struggle with either\nlow rendering qualities (\\eg, visual fidelity and physically plausible HOIs) or\nhigh computational costs. To address these limitations, we propose HOGS\n(Human-Object Rendering via 3D Gaussian Splatting), a novel framework for\nefficient and physically plausible HOI rendering from sparse views.\nSpecifically, HOGS combines 3D Gaussian Splatting with a physics-aware\noptimization process. It incorporates a Human Pose Refinement module for\naccurate pose estimation and a Sparse-View Human-Object Contact Prediction\nmodule for efficient contact region identification. This combination enables\ncoherent joint rendering of human and object Gaussians while enforcing\nphysically plausible interactions. Extensive experiments on the HODome dataset\ndemonstrate that HOGS achieves superior rendering quality, efficiency, and\nphysical plausibility compared to existing methods. We further show its\nextensibility to hand-object grasp rendering tasks, presenting its broader\napplicability to articulated object interactions.",
      "generated_abstract": "We introduce a novel 3D semantic segmentation method, FuseVision, which\nintegrates a fusion network with a vision transformer for scene understanding.\nFuseVision employs a two-stage fusion mechanism, which first fuses semantic\ninformation from the image and the scene, then feeds the fused feature into the\nvision transformer. To better align the semantic features of different scales,\nwe propose a multi-scale fusion module to fuse the feature maps of different\nscales. Furthermore, we introduce a hierarchical attention module to better\ncapture scene context information. Experimental results show that our method\nachieves state-of-the-art performance across various benchmarks,\ndemonstrating its superiority over existing methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.2972972972972973,
          "f": 0.22564102093149255
        },
        "rouge-2": {
          "r": 0.02666666666666667,
          "p": 0.041666666666666664,
          "f": 0.03252032044418075
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.2972972972972973,
          "f": 0.22564102093149255
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/TH/2503.05542v2",
      "true_abstract": "We consider standard gradient descent, gradient flow and conjugate gradients\nas iterative algorithms for minimizing a penalized ridge criterion in linear\nregression. While it is well known that conjugate gradients exhibit fast\nnumerical convergence, the statistical properties of their iterates are more\ndifficult to assess due to inherent nonlinearities and dependencies. On the\nother hand, standard gradient flow is a linear method with well known\nregularizing properties when stopped early. By an explicit non-standard error\ndecomposition we are able to bound the prediction error for conjugate gradient\niterates by a corresponding prediction error of gradient flow at transformed\niteration indices. This way, the risk along the entire regularisation path of\nconjugate gradient iterations can be compared to that for regularisation paths\nof standard linear methods like gradient flow and ridge regression. In\nparticular, the oracle conjugate gradient iterate shares the optimality\nproperties of the gradient flow and ridge regression oracles up to a constant\nfactor. Numerical examples show the similarity of the regularisation paths in\npractice.",
      "generated_abstract": "the problem of estimating the mean of a Gaussian mixture model\n(GMM) in the high-dimensional, low-sample-complexity regime. The GMM is often\nconstructed from data via an orthogonal decomposition, where each component is\na Gaussian. This decomposition may be random or deterministic. The goal is to\nestimate the mean of the mixture while controlling the relative error in the\nGaussian components. We propose a two-stage estimation procedure. In the first\nstage, we estimate the mean of the Gaussian components. In the second stage,\nwe estimate the mean of the Gaussian mixture. We establish that our method\nachieves optimal convergence rates for the second stage. We also show that\nassuming the first stage is error-controlled, our second stage estimator\nconverges at a rate of $O(1/\\sqrt{n})$ in the high-dimensional regime. Our\nmethod is general, applicable to both determin",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19,
          "p": 0.24358974358974358,
          "f": 0.2134831411437951
        },
        "rouge-2": {
          "r": 0.013513513513513514,
          "p": 0.018018018018018018,
          "f": 0.015444010546057816
        },
        "rouge-l": {
          "r": 0.17,
          "p": 0.21794871794871795,
          "f": 0.19101123103143555
        }
      }
    },
    {
      "paper_id": "physics.ins-det.physics/ins-det/2503.10395v1",
      "true_abstract": "One of the main issues of the satellite-to-ground optical communication,\nincluding free-space satellite quantum key distribution (QKD), is an\nachievement of the reasonable accuracy of positioning, navigation and optical\nstabilization. Proportional-integral-derivative (PID) controllers can handle\nwith various control tasks in optical systems. Recent research shows the\npromising results in the area of composite control systems including classical\ncontrol via PID controllers and reinforcement learning (RL) approach. In this\nwork we apply RL agent to an experimental stand of the optical stabilization\nsystem of QKD terminal. We find via agent control history more precise PID\nparameters and also provide effective combined RL-PID dynamic control approach\nfor the optical stabilization of satellite-to-ground communication system.",
      "generated_abstract": "xt-generation high-energy physics experiments, the detectors are\nlarge, complex structures made of a large number of components, requiring\nefficient data acquisition and analysis to maintain their high performance.\nThe particle identification (PID) systems are the main tool for selecting the\ntracking objects in the detectors. In the particle identification, the\ntrajectories of the particles are tracked and the PID systems are used to\nestimate the track parameters. These parameters are then used to perform the\ntracking reconstruction. The track parameters can be computed by the\nreconstructed tracks. In this paper, we consider the reconstruction of the\nreconstructed tracks and the PID system. We propose a novel approach for the\ntrack reconstruction using the track-level information. In this approach, the\nreconstructed tracks are generated using the track-level information. Then,\nthe PID systems are trained to classify the track",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23684210526315788,
          "p": 0.23684210526315788,
          "f": 0.23684210026315797
        },
        "rouge-2": {
          "r": 0.05714285714285714,
          "p": 0.05217391304347826,
          "f": 0.05454544955578558
        },
        "rouge-l": {
          "r": 0.23684210526315788,
          "p": 0.23684210526315788,
          "f": 0.23684210026315797
        }
      }
    },
    {
      "paper_id": "nucl-th.nucl-ex/2503.08396v1",
      "true_abstract": "This study investigates the moderation of 14.1 MeV neutrons in a natural\nberyllium moderator arranged in a spherical geometry. The neutron interactions\nand moderation efficiency were analyzed using Monte Carlo simulations with the\nGEANT4 toolkit. Various sphere radii were tested to determine the optimal\nmoderator thickness for neutron thermalization.",
      "generated_abstract": "The theoretical study of hadron-nucleon interactions is one of the key\npoints in the study of the nuclear matter in the nuclear equation of state.\nThe recent theoretical progress in the hadron-nucleon interactions in the\ndensity region of the nuclear matter is based on the QCD sum rule method. The\nQCD sum rule method is a powerful tool to estimate the hadron-nucleon\ninteraction parameters, which is a crucial step in the theoretical study of\nnuclear matter. This paper reviews the recent progress in the hadron-nucleon\ninteraction parameters in the density region of the nuclear matter. The\nresults are compared with the available experimental data. The results show\nthat the latest theoretical progress in the hadron-nucleon interactions in the\ndensity region of the nuclear matter is promising for the study of the nuclear\nmatter in the nuclear equation of state.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2619047619047619,
          "p": 0.21153846153846154,
          "f": 0.2340425482480762
        },
        "rouge-2": {
          "r": 0.020833333333333332,
          "p": 0.012195121951219513,
          "f": 0.015384610726628627
        },
        "rouge-l": {
          "r": 0.2619047619047619,
          "p": 0.21153846153846154,
          "f": 0.2340425482480762
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/DB/2503.08087v1",
      "true_abstract": "Entity resolution (ER) is a critical task in data management which identifies\nwhether multiple records refer to the same real-world entity. Despite its\nsignificance across domains such as healthcare, finance, and machine learning,\nimplementing effective ER systems remains challenging due to the abundance of\nmethodologies and tools, leading to a paradox of choice for practitioners. This\npaper proposes Resolvi, a reference architecture aimed at enhancing\nextensibility, interoperability, and scalability in ER systems. By analyzing\nexisting ER frameworks and literature, we establish a structured approach to\ndesigning ER solutions that address common challenges. Additionally, we explore\nbest practices for system implementation and deployment strategies to\nfacilitate largescale entity resolution. Through this work, we aim to provide a\nfoundational blueprint that assists researchers and practitioners in developing\nrobust, scalable ER systems while reducing the complexity of architectural\ndecisions.",
      "generated_abstract": "ntext of a global pandemic, the spread of misinformation online\nhas become a growing threat. This paper investigates the use of blockchain\ntechnology to combat misinformation. Blockchain technology is a decentralized\ndatabase that maintains a digital ledger of transactions. The blockchain is\ndesigned to be tamper-proof and immutable, ensuring the accuracy of data\nstored within it. This paper explores the potential of blockchain technology to\naddress the challenges of misinformation, particularly the lack of transparency\nand trust in traditional media. By leveraging blockchain technology, it is\npossible to establish a decentralized and transparent platform for sharing\ninformation, ensuring accuracy and veracity. The use of blockchain technology\ncan provide a secure, trustworthy, and transparent platform for sharing\ninformation, allowing for greater transparency and accountability. By\nimplementing blockchain technology, it is possible",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17475728155339806,
          "p": 0.2465753424657534,
          "f": 0.2045454496907284
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.036036036036036036,
          "f": 0.032921805736931105
        },
        "rouge-l": {
          "r": 0.1650485436893204,
          "p": 0.2328767123287671,
          "f": 0.19318181332709208
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.03633v1",
      "true_abstract": "Autonomous motion planning under unknown nonlinear dynamics presents\nsignificant challenges. An agent needs to continuously explore the system\ndynamics to acquire its properties, such as reachability, in order to guide\nsystem navigation adaptively. In this paper, we propose a hybrid\nplanning-control framework designed to compute a feasible trajectory toward a\ntarget. Our approach involves partitioning the state space and approximating\nthe system by a piecewise affine (PWA) system with constrained control inputs.\nBy abstracting the PWA system into a directed weighted graph, we incrementally\nupdate the existence of its edges via affine system identification and reach\ncontrol theory, introducing a predictive reachability condition by exploiting\nprior information of the unknown dynamics. Heuristic weights are assigned to\nedges based on whether their existence is certain or remains indeterminate.\nConsequently, we propose a framework that adaptively collects and analyzes data\nduring mission execution, continually updates the predictive graph, and\nsynthesizes a controller online based on the graph search outcomes. We\ndemonstrate the efficacy of our approach through simulation scenarios involving\na mobile robot operating in unknown terrains, with its unknown dynamics\nabstracted as a single integrator model.",
      "generated_abstract": "r presents a multi-agent system (MAS) for driving and maneuvering\nmultiple autonomous vehicles in dynamic traffic scenarios. A two-layer\nnetwork architecture is proposed to integrate a single-agent (SA) and a\nmulti-agent (MA) MAS. The SA network is a single agent designed to operate in\npurely autonomous driving modes, while the MA network is a multi-agent network\nwith a controller agent that facilitates cooperative driving and maneuvering\nbehavior. The SA network is trained to perform pure autonomous driving using\na deep learning approach, while the MA network is designed to support the\nSA network in real-time. The SA network has two distinct modes: one mode is for\nnavigation and the other is for pure autonomous driving. The MA network\ncooperates with the SA network to achieve a smooth transition between pure\nautonomous driving and navigation modes. The MA network",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.2318840579710145,
          "f": 0.16243654367182883
        },
        "rouge-2": {
          "r": 0.01675977653631285,
          "p": 0.027777777777777776,
          "f": 0.02090591865094983
        },
        "rouge-l": {
          "r": 0.1015625,
          "p": 0.18840579710144928,
          "f": 0.1319796908799507
        }
      }
    },
    {
      "paper_id": "cs.CV.q-bio/TO/2409.20407v3",
      "true_abstract": "Periorbital segmentation and distance prediction using deep learning allows\nfor the objective quantification of disease state, treatment monitoring, and\nremote medicine. However, there are currently no reports of segmentation\ndatasets for the purposes of training deep learning models with sub mm accuracy\non the regions around the eyes. All images (n=2842) had the iris, sclera, lid,\ncaruncle, and brow segmented by five trained annotators. Here, we validate this\ndataset through intra and intergrader reliability tests and show the utility of\nthe data in training periorbital segmentation networks. All the annotations are\npublicly available for free download. Having access to segmentation datasets\ndesigned specifically for oculoplastic surgery will permit more rapid\ndevelopment of clinically useful segmentation networks which can be leveraged\nfor periorbital distance prediction and disease classification. In addition to\nthe annotations, we also provide an open-source toolkit for periorbital\ndistance prediction from segmentation masks. The weights of all models have\nalso been open-sourced and are publicly available for use by the community.",
      "generated_abstract": "In this paper, we present a novel method to enhance the visual understanding\nof large-scale biological data by integrating image-based and text-based\ninformation. The proposed approach employs a two-stage framework to extract\nimage features from biological images and generate textual descriptions based\non the features. This framework allows us to effectively leverage the\nimage-based information to enhance the understanding of text-based information.\nThe textual description is generated by leveraging the word embedding\nmethodology. The generated textual descriptions are then used to augment the\nvisual features of the images. We validate the effectiveness of our approach\nby integrating it with the Visual Genome dataset and demonstrate its\nbenefits for improving visual understanding of biological data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17272727272727273,
          "p": 0.2753623188405797,
          "f": 0.21229049805561634
        },
        "rouge-2": {
          "r": 0.013245033112582781,
          "p": 0.0196078431372549,
          "f": 0.01581027186739511
        },
        "rouge-l": {
          "r": 0.15454545454545454,
          "p": 0.2463768115942029,
          "f": 0.18994412934053256
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/NA/2503.09898v1",
      "true_abstract": "Dynamic simulation plays a crucial role in power system transient stability\nanalysis, but traditional numerical integration-based methods are\ntime-consuming due to the small time step sizes. Other semi-analytical solution\nmethods, such as the Differential Transformation method, often struggle to\nselect proper orders and steps, leading to slow performance and numerical\ninstability. To address these challenges, this paper proposes a novel adaptive\ndynamic simulation approach for power system transient\n  stability analysis. The approach adds feedback control and optimization to\nselecting the step and order, utilizing the Differential Transformation method\nand a proportional-integral control strategy to control truncation errors.\nOrder selection is formulated as an optimization problem resulting in a\nvariable-step-optimal-order method that achieves significantly larger time step\nsizes without violating numerical stability. It is applied to three systems:\nthe IEEE 9-bus, 3-generator system, IEEE 39-bus, 10-generator system, and a\nPolish 2383-bus, 327-generator system, promising computational efficiency and\nnumerical robustness for large-scale power system is demonstrated in\ncomprehensive case studies.",
      "generated_abstract": "aper, we consider a multi-agent system that is composed of two\nconsecutive stages, where agents interact during the first stage and act on\ntheir own during the second stage. In the first stage, agents exchange information\nand collaborate to achieve their goals, while in the second stage, agents\ncompete to achieve their individual goals. We study the problem of multi-agent\ndifferentially private decision-making in the second stage. We first consider\nthe case where agents have private preferences over their own actions and\npublicly available preferences over the actions of the other agents. We then\nconsider the case where agents have no private preferences over their own\nactions and agents' preferences are publicly available. Our main contribution\nis to derive a differentially private algorithm that achieves the optimal\nsublinear regret for both cases. Our analysis uses a novel approach based on the\nanalysis of a game with a single equilibrium. Our",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.20512820512820512,
          "f": 0.17204300588276114
        },
        "rouge-2": {
          "r": 0.013605442176870748,
          "p": 0.016129032258064516,
          "f": 0.014760142637492986
        },
        "rouge-l": {
          "r": 0.1388888888888889,
          "p": 0.19230769230769232,
          "f": 0.16129031771071814
        }
      }
    },
    {
      "paper_id": "cs.LG.econ/EM/2502.14131v2",
      "true_abstract": "We study the problem of estimating Dynamic Discrete Choice (DDC) models, also\nknown as offline Maximum Entropy-Regularized Inverse Reinforcement Learning\n(offline MaxEnt-IRL) in machine learning. The objective is to recover reward or\n$Q^*$ functions that govern agent behavior from offline behavior data. In this\npaper, we propose a globally convergent gradient-based method for solving these\nproblems without the restrictive assumption of linearly parameterized rewards.\nThe novelty of our approach lies in introducing the Empirical Risk Minimization\n(ERM) based IRL/DDC framework, which circumvents the need for explicit state\ntransition probability estimation in the Bellman equation. Furthermore, our\nmethod is compatible with non-parametric estimation techniques such as neural\nnetworks. Therefore, the proposed method has the potential to be scaled to\nhigh-dimensional, infinite state spaces. A key theoretical insight underlying\nour approach is that the Bellman residual satisfies the Polyak-Lojasiewicz (PL)\ncondition -- a property that, while weaker than strong convexity, is sufficient\nto ensure fast global convergence guarantees. Through a series of synthetic\nexperiments, we demonstrate that our approach consistently outperforms\nbenchmark methods and state-of-the-art alternatives.",
      "generated_abstract": "nt literature on the economics of political campaigns is dominated\nby two mainstream approaches: the model-based approach and the data-driven\napproach. The former focuses on the analysis of campaigns using econometric\nmodels, whereas the latter employs data-driven techniques to analyze campaign\neffects. While the model-based approach has made significant progress, the\ndata-driven approach is often criticized for its inability to capture the\ncomplexities of political campaigns. In this paper, we introduce the\nNeural-Bayesian Constrained Optimization (NBCO) framework, which combines the\npower of the Bayesian optimization method with the strength of deep learning.\nThe NBCO framework can effectively optimize the model parameters by combining\nthe benefits of Bayesian optimization and deep learning. The NBCO framework\nallows for the systematic exploration of the parameter space while ensuring\nthat",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16296296296296298,
          "p": 0.2682926829268293,
          "f": 0.20276497225679044
        },
        "rouge-2": {
          "r": 0.03529411764705882,
          "p": 0.05357142857142857,
          "f": 0.04255318670087072
        },
        "rouge-l": {
          "r": 0.16296296296296298,
          "p": 0.2682926829268293,
          "f": 0.20276497225679044
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.04040v1",
      "true_abstract": "The fluid antenna system (FAS) has emerged as a disruptive technology for\nfuture wireless networks, offering unprecedented degrees of freedom (DoF)\nthrough the dynamic configuration of antennas in response to propagation\nenvironment variations. The integration of fluid antennas (FAs) with multiuser\nmultiple-input multiple-output (MU-MIMO) networks promises substantial weighted\nsum rate (WSR) gains via joint beamforming and FA position optimization.\nHowever, the joint design is challenging due to the strong coupling between\nbeamforming matrices and antenna positions. To address the challenge, we\npropose a novel block coordinate ascent (BCA)-based method in FA-assisted\nMU-MIMO networks. Specifically, we first employ matrix fractional programming\ntechniques to reformulate the original complex problem into a more tractable\nform. Then, we solve the reformulated problem following the BCA principle,\nwhere we develop a low-complexity majorization maximization algorithm capable\nof optimizing all FA positions simultaneously. To further reduce the\ncomputational, storage, and interconnection costs, we propose a decentralized\nimplementation for our proposed algorithm by utilizing the decentralized\nbaseband processing (DBP) architecture. Simulation results demonstrate that\nwith our proposed algorithm, the FA-assisted MU-MIMO system achieves up to a\n47% WSR improvement over conventional MIMO networks equipped with\nfixed-position antennas. Moreover, the decentralized implementation reduces\ncomputation time by approximately 70% and has similar performance compared with\nthe centralized implementation.",
      "generated_abstract": "t of 6G promises enhanced capabilities for both voice and data\ncommunications, including 100Gbps of downlink data rates and 5Gbps of\nuplink data rates. To achieve these data rates, 6G systems will utilize\nmulti-antenna transmitters, which will require massive MIMO to achieve the\ndesired data rates. As a result, massive MIMO will be an important enabling\ntechnology for 6G systems. This paper investigates the design of massive MIMO\nsystems with orthogonal frequency-division multiplexing (OFDM), considering\nboth frequency and time domains. The main contributions of this paper are:\n(i) we derive the optimal power allocation and channel matrix design in both\nfrequency and time domains, and (ii) we formulate the joint power allocation and\nchannel matrix design problem in both frequency and time domains. The\nformulation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12751677852348994,
          "p": 0.25333333333333335,
          "f": 0.1696428526885365
        },
        "rouge-2": {
          "r": 0.009900990099009901,
          "p": 0.01904761904761905,
          "f": 0.013029311460070095
        },
        "rouge-l": {
          "r": 0.12751677852348994,
          "p": 0.25333333333333335,
          "f": 0.1696428526885365
        }
      }
    },
    {
      "paper_id": "nlin.CD.physics/data-an/2503.07582v1",
      "true_abstract": "Small, forested catchments are prototypes of terrestrial ecosystems and have\nbeen studied in several disciplines of environmental sciences since several\ndecades. Time series of water and matter fluxes and nutrient concentrations\nfrom these systems exhibit a bewildering diversity of spatio-temporal patterns,\nindicating the intricate nature of processes acting on a large range of time\nscales. Nonlinear dynamics is an obvious framework to investigate catchment\ntime series. We analyze selected long-term data from three headwater catchments\nin the Bramke valley, Harz mountains, Lower Saxony in Germany at common\nbiweekly resolution for the period 1991 to 2023. For every time series, we\nperform gap filling, detrending and removal of the annual cycle using Singular\nSystem Analysis (SSA), and then calculate metrics based on ordinal pattern\nstatistics: the permutation entropy, permutation complexity and Fisher\ninformation, as well as their generalized versions (q-entropy and\n{\\alpha}-entropy). Further, the position of each variable in Tarnopolski\ndiagrams is displayed and compared to reference stochastic processes, like\nfractional Brownian motion, fractional Gaussian noise, and \\b{eta} noise. Still\nanother way of distinguishing deterministic chaos and structured noise, and\nquantifying the latter, is provided by the complexity from ordinal pattern\npositioned slopes (COPPS). We also construct Horizontal Visibility Graphs and\nestimate the exponent of the decay of the degree distribution. Taken together,\nthe analyses create a characterization of the dynamics of these systems which\ncan be scrutinized for universality, either across variables or between the\nthree geographically very close catchments.",
      "generated_abstract": "This paper presents a new methodology to measure the frequency of the\ntransition of the supercritical two-dimensional Ising model to the critical\none. The methodology relies on the analysis of the spectral properties of the\ntwo-dimensional Ising model on a triangular lattice. We find that the critical\ntransition is shifted from the critical point to the second-order phase\ntransition line. In addition, we also find that the critical point is not\nsaturated, which is due to the finite size of the system. Finally, we show that\nthe critical line and the critical point are not the same, which is due to the\nnon-monotonic behavior of the critical point. Our results are in good agreement\nwith those of the Monte Carlo simulations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08092485549132948,
          "p": 0.21875,
          "f": 0.1181434559732238
        },
        "rouge-2": {
          "r": 0.008583690987124463,
          "p": 0.02127659574468085,
          "f": 0.012232411805591996
        },
        "rouge-l": {
          "r": 0.08092485549132948,
          "p": 0.21875,
          "f": 0.1181434559732238
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.05481v1",
      "true_abstract": "Large language models (LLMs) often generate inaccurate yet credible-sounding\ncontent, known as hallucinations. This inherent feature of LLMs poses\nsignificant risks, especially in critical domains. I analyze LLMs as a new\nclass of engineering products, treating hallucinations as a product attribute.\nI demonstrate that, in the presence of imperfect awareness of LLM\nhallucinations and misinformation externalities, net welfare improves when the\nmaximum acceptable level of LLM hallucinations is designed to vary with two\ndomain-specific factors: the willingness to pay for reduced LLM hallucinations\nand the marginal damage associated with misinformation.",
      "generated_abstract": "This paper explores the impact of the COVID-19 pandemic on the consumption\nfears of households in the US. Using data from the Consumer Financial\nProtection Bureau's (CFPB) Household Financial Security Index (HFSI), we\nexamine the impact of the pandemic on households' fears of debt, interest\nrates, and inflation. We find that the pandemic led to a decline in households'\nfear of debt, but had a significant negative impact on their fear of\ninterest rates and inflation. The negative impact on interest rates and\ninflation was driven by the increased uncertainty in household finances,\nresulting in a decline in consumption and the rise in debt. This study reveals\nthe impact of the pandemic on household's financial security and its\nimplications for the future.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13432835820895522,
          "p": 0.1323529411764706,
          "f": 0.13333332833360784
        },
        "rouge-2": {
          "r": 0.023809523809523808,
          "p": 0.02,
          "f": 0.021739125472590923
        },
        "rouge-l": {
          "r": 0.1044776119402985,
          "p": 0.10294117647058823,
          "f": 0.10370369870397829
        }
      }
    },
    {
      "paper_id": "math.DS.q-bio/MN/2409.12487v2",
      "true_abstract": "We present a systematic procedure for testing whether reaction networks\nexhibit non-expansivity or monotonicity. This procedure identifies explicit\nnorms under which a network is non-expansive or cones for which the system is\nmonotone-or provides proof that no such structures exist. Our approach\nreproduces known results, generates novel findings, and demonstrates that\ncertain reaction networks cannot exhibit monotonicity or non-expansivity with\nrespect to any cone or norm. Additionally, we establish a duality relationship\nwhich states that if a network is monotone, so is its dual network.",
      "generated_abstract": "y of the Kac-Moody algebra $K(g)$ is the algebraic study of the\nLie algebras $g$. In the present paper, we develop the theory of the\nKac-Moody algebra $K(A_n)$, which is the algebra of symmetries of the\n$n$-dimensional binary tree. The Lie algebra $A_n$ is an enlargement of the\nWeyl algebra $W(A_n)$ which is the algebra of symmetries of the $n$-dimensional\noctahedron. The study of the Kac-Moody algebras is of fundamental importance in\nthe theory of differential equations and the theory of quantum groups. We show\nthat the Kac-Moody algebra $K(A_n)$ is a generalization of the Weyl algebra\n$W(A_n)$ and that the Lie algebras $K(A_n)$ and $K(A",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.1956521739130435,
          "f": 0.16513760980052197
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.1956521739130435,
          "f": 0.16513760980052197
        }
      }
    },
    {
      "paper_id": "math.RT.math/RT/2503.07809v1",
      "true_abstract": "For a permutation $w$ in the symmetric group $\\mathfrak{S}_{n}$, let $L(w)$\ndenote the simple highest weight module in the principal block of the BGG\ncategory $\\mathcal{O}$ for the Lie algebra $\\mathfrak{sl}_{n}(\\mathbb{C})$. We\nfirst prove that $L(w)$ is Kostant negative whenever $w$ consecutively contains\ncertain patterns. We then provide a complete answer to Kostant's problem in\ntype $A_{6}$ and show that the indecomposability conjecture also holds in type\n$A_{6}$, that is, applying an indecomposable projective functor to a simple\nmodule outputs either an indecomposable module or zero.",
      "generated_abstract": "We present a simple construction of a randomized algorithm for solving the\nvertex cover problem in a weighted bipartite graph. The algorithm uses\npolynomial time to sample a weighted bipartite graph with $n$ vertices and\n$m = 2^n$ edges, where the edge weights are drawn independently from a\nGaussian distribution with mean zero and variance $\\sigma^2$. We prove that,\nunder the uniform distribution of weights, the algorithm runs in polynomial\ntime, and for $\\sigma = 1$ it achieves the $O(n)$ lower bound on the running\ntime. We also present an implementation of the algorithm in C++.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21875,
          "p": 0.2153846153846154,
          "f": 0.21705425856619204
        },
        "rouge-2": {
          "r": 0.036585365853658534,
          "p": 0.03296703296703297,
          "f": 0.034682075938388235
        },
        "rouge-l": {
          "r": 0.203125,
          "p": 0.2,
          "f": 0.20155038259719982
        }
      }
    },
    {
      "paper_id": "cs.CL.stat/ME/2503.04910v1",
      "true_abstract": "The emergence of powerful LLMs has led to a paradigm shift in Natural\nLanguage Understanding and Natural Language Generation. The properties that\nmake LLMs so valuable for these tasks -- creativity, ability to produce fluent\nspeech, and ability to quickly and effectively abstract information from large\ncorpora -- also present new challenges to evaluating their outputs. The rush to\nmarket has led teams to fall back on quick, cost-effective automatic\nevaluations which offer value, but do not obviate the need for human judgments\nin model training and evaluation. This paper argues that in cases in which end\nusers need to agree with the decisions made by ML models -- e.g. in toxicity\ndetection or extraction of main points for summarization -- models should be\ntrained and evaluated on data that represent the preferences of those users. We\nsupport this argument by explicating the role of human feedback in labeling and\njudgment tasks for model training and evaluation. First, we propose methods for\ndisentangling noise from signal in labeling tasks. Then we show that noise in\nlabeling disagreement can be minimized by adhering to proven methodological\nbest practices, while signal can be maximized to play an integral role in model\ntraining and evaluation tasks. Finally, we illustrate best practices by\nproviding a case study in which two guardrails classifiers are evaluated using\nhuman judgments to align final model behavior to user preferences. We aim for\nthis paper to provide researchers and professionals with guidelines to\nintegrating human judgments into their ML and generative AI evaluation toolkit,\nparticularly when working toward achieving accurate and unbiased features that\nalign with users' needs and expectations.",
      "generated_abstract": "em of learning a probability distribution over a finite alphabet is\nthe core of several applications in information retrieval, machine translation,\nand statistics. In this work, we study the problem of learning a probability\ndistribution over a finite alphabet with a single token. We focus on the case of\na discrete alphabet, and we show that the problem is NP-hard. To solve the\nproblem, we introduce a new notion of \"soft-soft-hard\" which captures the\ncomplexity of the problem and we present an algorithm that runs in polynomial\ntime for each of the three known cases of the problem. The results of our\nanalysis are based on the results of Arora, Bandeira, and Mendelson (2019),\nwhich state that the problem is in NP if the alphabet is finite and has at most\nthree symbols, and in coNP if the alphabet is finite and has at most two symbols.\nOur",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15060240963855423,
          "p": 0.30864197530864196,
          "f": 0.2024291453896967
        },
        "rouge-2": {
          "r": 0.00784313725490196,
          "p": 0.01680672268907563,
          "f": 0.010695182826934185
        },
        "rouge-l": {
          "r": 0.13855421686746988,
          "p": 0.2839506172839506,
          "f": 0.18623481340589104
        }
      }
    },
    {
      "paper_id": "cs.AI.q-fin/CP/2501.12399v1",
      "true_abstract": "Current financial Large Language Models (LLMs) struggle with two critical\nlimitations: a lack of depth in stock analysis, which impedes their ability to\ngenerate professional-grade insights, and the absence of objective evaluation\nmetrics to assess the quality of stock analysis reports. To address these\nchallenges, this paper introduces FinSphere, a conversational stock analysis\nagent, along with three major contributions: (1) Stocksis, a dataset curated by\nindustry experts to enhance LLMs' stock analysis capabilities, (2) AnalyScore,\na systematic evaluation framework for assessing stock analysis quality, and (3)\nFinSphere, an AI agent that can generate high-quality stock analysis reports in\nresponse to user queries. Experiments demonstrate that FinSphere achieves\nsuperior performance compared to both general and domain-specific LLMs, as well\nas existing agent-based systems, even when they are enhanced with real-time\ndata access and few-shot guidance. The integrated framework, which combines\nreal-time data feeds, quantitative tools, and an instruction-tuned LLM, yields\nsubstantial improvements in both analytical quality and practical applicability\nfor real-world stock analysis.",
      "generated_abstract": "This study examines the impact of market microstructure on the pricing of\ninterest rate derivatives, specifically the pricing of the credit default swap\n(CDS) market. Using a large sample of historical CDS prices, we find that\nmarket microstructure affects CDS pricing through a variety of mechanisms,\nincluding the impact of arbitrage and the effect of market volatility on CDS\npricing. Additionally, we explore how the structure of the CDS market, such as\nthe role of the clearing house, affects CDS pricing. Our findings suggest that\nthe CDS market is more efficient when it is more concentrated, which may be\nrelated to the fact that the CDS market is more sensitive to market\nvolatility.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07563025210084033,
          "p": 0.14285714285714285,
          "f": 0.09890109437447188
        },
        "rouge-2": {
          "r": 0.006493506493506494,
          "p": 0.010752688172043012,
          "f": 0.008097161296860564
        },
        "rouge-l": {
          "r": 0.07563025210084033,
          "p": 0.14285714285714285,
          "f": 0.09890109437447188
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.15945v1",
      "true_abstract": "A unified framework is provided for analysing check-all-that-apply (CATA)\nproduct data following the ``one citation, one vote\" principle. CATA data arise\nfrom studies where A consumers evaluate P products by describing samples by\nchecking all of the T terms that apply. Giving every citation the same weight,\nregardless of the assessor, product, or term, leads to analyses based on the L1\nnorm where the median absolute deviation is the measure of dispersion. Five\npermutation tests are proposed to answer the following questions. Do any\nproducts differ? For which terms do products differ? Within each of the terms,\nwhich products differ? Which product pairs differ? On which terms does each\nproduct pair differ? Additionally, we show how products and terms can be\nclustered following the ``one citation, one vote\" principle and how L1-norm\nprincipal component analysis (L1-norm PCA) can be applied to visualize CATA\nresults in few dimensions. Together, the permutation tests, clustering methods,\nand L1-norm PCA provide a unified approach. The proposed methods are\nillustrated using a data set in which 100 consumers evaluated 11 products using\n34 CATA terms.R code is provided to perform the analyses.",
      "generated_abstract": "We consider the problem of estimating the mean and covariance of a Gaussian\ndistribution from a finite set of samples. This problem arises, for example, in\nthe analysis of experimental data, or in the estimation of parameters in\nstatistical models. The key challenge in this setting is that, although\nsamples from the Gaussian are independent, the covariance between them is not\nindependent of the samples. This problem is addressed using a generalization of\nthe maximum likelihood estimator. We derive a closed-form expression for the\nestimator, and show that the resulting estimator is consistent and asymptotically\nnormal. Furthermore, we propose a simple and computationally efficient algorithm\nto compute the estimator. We illustrate the performance of our estimator using\nsimulations and a real-world application.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15833333333333333,
          "p": 0.25675675675675674,
          "f": 0.19587628394090775
        },
        "rouge-2": {
          "r": 0.011494252873563218,
          "p": 0.017241379310344827,
          "f": 0.01379309864827753
        },
        "rouge-l": {
          "r": 0.14166666666666666,
          "p": 0.22972972972972974,
          "f": 0.17525772723987684
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.06404v1",
      "true_abstract": "Reinsurance optimization is critical for insurers to manage risk exposure,\nensure financial stability, and maintain solvency. Traditional approaches often\nstruggle with dynamic claim distributions, high-dimensional constraints, and\nevolving market conditions. This paper introduces a novel hybrid framework that\nintegrates {Generative Models}, specifically Variational Autoencoders (VAEs),\nwith {Reinforcement Learning (RL)} using Proximal Policy Optimization (PPO).\nThe framework enables dynamic and scalable optimization of reinsurance\nstrategies by combining the generative modeling of complex claim distributions\nwith the adaptive decision-making capabilities of reinforcement learning.\n  The VAE component generates synthetic claims, including rare and catastrophic\nevents, addressing data scarcity and variability, while the PPO algorithm\ndynamically adjusts reinsurance parameters to maximize surplus and minimize\nruin probability. The framework's performance is validated through extensive\nexperiments, including out-of-sample testing, stress-testing scenarios (e.g.,\npandemic impacts, catastrophic events), and scalability analysis across\nportfolio sizes. Results demonstrate its superior adaptability, scalability,\nand robustness compared to traditional optimization techniques, achieving\nhigher final surpluses and computational efficiency.\n  Key contributions include the development of a hybrid approach for\nhigh-dimensional optimization, dynamic reinsurance parameterization, and\nvalidation against stochastic claim distributions. The proposed framework\noffers a transformative solution for modern reinsurance challenges, with\npotential applications in multi-line insurance operations, catastrophe\nmodeling, and risk-sharing strategy design.",
      "generated_abstract": "e a novel approach for estimating the parameters of an\nregression model in a hierarchical setting. Our approach involves a sequence\nof estimators for the parameters, each of which is based on a restricted\nenvelope of the full model. We establish convergence rates of the sequence of\nestimators, and we show that the sequence of estimators converges to the true\nparameters at a rate that is faster than the rate that would be obtained by\nsimply taking the full model estimator. We also show that the sequence of\nestimators converges to the true parameters at a rate that is slower than the\nrate that would be obtained by using the restricted envelope of the full model\nestimator. Finally, we show that the sequence of estimators has a consistent\nand asymptotically normal estimator of the parameters, and we show that the\nestimator is asymptotically normal for any fixed parameter. We illustrate our\nproposed method",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0949367088607595,
          "p": 0.22727272727272727,
          "f": 0.1339285672720027
        },
        "rouge-2": {
          "r": 0.009950248756218905,
          "p": 0.019417475728155338,
          "f": 0.013157890256450625
        },
        "rouge-l": {
          "r": 0.0949367088607595,
          "p": 0.22727272727272727,
          "f": 0.1339285672720027
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.16088v1",
      "true_abstract": "The question of whether insects experience pain has long been debated in\nneuroscience and animal behavior research. Increasing evidence suggests that\ninsects possess the ability to detect and respond to noxious stimuli,\nexhibiting behaviors indicative of pain perception. This study investigates the\nrelationship between pain stimuli and physiological responses in crickets\n(Gryllidae), focusing on heart rate (ECG) and brain wave (EEG) patterns. We\napplied a range of mechanical, chemical, thermal, and electrical stimuli to\ncrickets, recording ECG and EEG data while employing a deep learning-based\nmodel to classify pain levels. Our findings revealed significant heart rate\nchanges and EEG fluctuations in response to various stimuli, with the highest\nintensity stimuli inducing marked physiological stress. The AI-based analysis,\nutilizing AlexNet for EEG signal classification, achieved 90% accuracy in\ndistinguishing between resting, low-pain, and high-pain states. While no social\nsharing of pain was observed through ECG measurements, these results contribute\nto the growing body of evidence supporting insect nociception and offer new\ninsights into their physiological responses to external stressors. This\nresearch advances the understanding of insect pain mechanisms and demonstrates\nthe potential for AI-driven analysis in entomological studies.",
      "generated_abstract": "r explores the computational challenges of simulating the evolution\nof large and complex networks. Specifically, we examine the problem of\nconstructing and analyzing models that capture the interplay between network\ntopology and gene expression. To address this, we consider three key\nscenarios: (1) the dynamics of a single gene, (2) the dynamics of a single\ncell, and (3) the dynamics of a network of cells. We explore the\ncomputational challenges of modeling gene expression as a function of topological\nfeatures, gene expression as a function of cellular states, and gene expression\nas a function of both topological and cellular features. We introduce a\nmethodology for constructing and analyzing models that capture these\ninterplay. We also consider the challenges of analyzing these models using\ncomputational techniques. We show how the analysis of these models can be\nfacilitated by leveraging the power of modern graph neural",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07518796992481203,
          "p": 0.136986301369863,
          "f": 0.09708737406494508
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.06766917293233082,
          "p": 0.1232876712328767,
          "f": 0.08737863620086743
        }
      }
    },
    {
      "paper_id": "eess.SY.q-fin/PM/2412.07688v1",
      "true_abstract": "Given the vital role that smart meter data could play in handling uncertainty\nin energy markets, data markets have been proposed as a means to enable\nincreased data access. However, most extant literature considers energy markets\nand data markets separately, which ignores the interdependence between them. In\naddition, existing data market frameworks rely on a trusted entity to clear the\nmarket. This paper proposes a joint energy and data market focusing on the\nday-ahead retailer energy procurement problem with uncertain demand. The\nretailer can purchase differentially-private smart meter data from consumers to\nreduce uncertainty. The problem is modelled as an integrated forecasting and\noptimisation problem providing a means of valuing data directly rather than\nvaluing forecasts or forecast accuracy. Value is determined by the Wasserstein\ndistance, enabling privacy to be preserved during the valuation and procurement\nprocess. The value of joint energy and data clearing is highlighted through\nnumerical case studies using both synthetic and real smart meter data.",
      "generated_abstract": "In this paper, we present a novel approach for the continuous-time optimal\nsystem control problem with state constraints. The state constraints are\nobserved and are based on the market price of the underlying asset. The\ncontrollability graph is constructed using the Markov chain theory and the\nstate constraints are identified by applying the algebraic Riccati equation\n(ARE). The ARE is solved using a multigrid algorithm. The identification\nresults are verified through a numerical example and the algorithm is also\napplied to solve a real-world problem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1509433962264151,
          "p": 0.2711864406779661,
          "f": 0.1939393893450873
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.0375,
          "f": 0.02643171349725476
        },
        "rouge-l": {
          "r": 0.14150943396226415,
          "p": 0.2542372881355932,
          "f": 0.18181817722387525
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/GN/2502.01311v1",
      "true_abstract": "Transcription factors are proteins that regulate the expression of genes by\nbinding to specific genomic regions known as Transcription Factor Binding Sites\n(TFBSs), typically located in the promoter regions of those genes. Accurate\nprediction of these binding sites is essential for understanding the complex\ngene regulatory networks underlying various cellular functions. In this regard,\nmany deep learning models have been developed for such prediction, but there is\nstill scope of improvement. In this work, we have developed a deep learning\nmodel which uses pre-trained DNABERT, a Convolutional Neural Network (CNN)\nmodule, a Modified Convolutional Block Attention Module (MCBAM), a Multi-Scale\nConvolutions with Attention (MSCA) module and an output module. The pre-trained\nDNABERT is used for sequence embedding, thereby capturing the long-term\ndependencies in the DNA sequences while the CNN, MCBAM and MSCA modules are\nuseful in extracting higher-order local features. TFBS-Finder is trained and\ntested on 165 ENCODE ChIP-seq datasets. We have also performed ablation studies\nas well as cross-cell line validations and comparisons with other models. The\nexperimental results show the superiority of the proposed method in predicting\nTFBSs compared to the existing methodologies. The codes and the relevant\ndatasets are publicly available at\nhttps://github.com/NimishaGhosh/TFBS-Finder/.",
      "generated_abstract": "y investigates the relationship between social media posts and\nsocial network structure in the context of the COVID-19 pandemic. Using\nTwitter as a social media platform, we analyzed the evolution of social media\nposts related to COVID-19 from 2020 to 2022. We then applied network\nanalytical techniques, including centrality measures and graph analysis, to\ncharacterize the network structure of COVID-19 social media posts. Our findings\nshow that COVID-19 posts often circulate in \"bubbles\" of like-minded users,\ncharacterized by high centrality scores and low average path length. These\nbubbles exhibit distinct characteristics, including high levels of\ndisagreement and low levels of consensus. The analysis also reveals that\nCOVID-19 posts are often shared by individuals with similar political\nviews. These findings provide insights into the dynamics",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.2,
          "f": 0.14285713826530627
        },
        "rouge-2": {
          "r": 0.010362694300518135,
          "p": 0.017699115044247787,
          "f": 0.01307189076658716
        },
        "rouge-l": {
          "r": 0.10416666666666667,
          "p": 0.1875,
          "f": 0.13392856683673485
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/CC/2503.05062v1",
      "true_abstract": "Despite of tremendous research on decoding Reed-Solomon (RS) and algebraic\ngeometry (AG) codes under the random and adversary substitution error models,\nfew studies have explored these codes under the burst substitution error model.\nBurst errors are prevalent in many communication channels, such as wireless\nnetworks, magnetic recording systems, and flash memory. Compared to random and\nadversarial errors, burst errors often allow for the design of more efficient\ndecoding algorithms. However, achieving both an optimal decoding radius and\nquasi-linear time complexity for burst error correction remains a significant\nchallenge. The goal of this paper is to design (both list and probabilistic\nunique) decoding algorithms for RS and AG codes that achieve the Singleton\nbound for decoding radius while maintaining quasi-linear time complexity.\n  Our idea is to build a one-to-one correspondence between AG codes (including\nRS codes) and interleaved RS codes with shorter code lengths (or even constant\nlengths). By decoding the interleaved RS codes with burst errors, we derive\nefficient decoding algorithms for RS and AG codes. For decoding interleaved RS\ncodes with shorter code lengths, we can employ either the naive methods or\nexisting algorithms. This one-to-one correspondence is constructed using the\ngeneralized fast Fourier transform (G-FFT) proposed by Li and Xing (SODA 2024).\nThe G-FFT generalizes the divide-and-conquer technique from polynomials to\nalgebraic function fields. More precisely speaking, assume that our AG code is\ndefined over a function field $E$ which has a sequence of subfields\n$\\mathbb{F}_q(x)=E_r\\subseteq E_{r-1}\\subseteq \\cdots\\subset E_1\\subseteq\nE_0=E$ such that $E_{i-1}/E_i$ are Galois extensions for $1\\le i\\le r$. Then\nthe AG code based on $E$ can be transformed into an interleaved RS code over\nthe rational function field $\\mathbb{F}_q(x)$.",
      "generated_abstract": "This paper presents a novel framework for the detection and mitigation of\ndynamically generated noise in cellular networks. The proposed method employs\na novel deep learning-based framework that leverages the inter-cell interference\ncompensation (ICIC) feature to identify and mitigate the interference\ngenerated by dynamic traffic patterns. The ICIC feature is extracted from the\nspectral domain using a time-frequency feature extractor, which is trained\nusing an adversarial training approach. This approach leverages the\ninter-cell interference compensation feature to detect dynamic traffic patterns\nand generate a fake interference signal. The fake interference signal is then\nused to generate the ICIC feature. The ICIC feature is subsequently used for\nmitigation. The performance of the proposed framework is evaluated using a\nreal-world dataset. Results demonstrate that the proposed framework outperforms\nother baseline methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11904761904761904,
          "p": 0.2857142857142857,
          "f": 0.16806722273850724
        },
        "rouge-2": {
          "r": 0.00411522633744856,
          "p": 0.009009009009009009,
          "f": 0.005649713209330102
        },
        "rouge-l": {
          "r": 0.09523809523809523,
          "p": 0.22857142857142856,
          "f": 0.13445377736035605
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.09740v1",
      "true_abstract": "In this paper, we prove a KAM theorem in a-posteriori format, using the\nparameterization method to look invariant tori in non-autonomous Hamiltonian\nsystems with $n$ degrees of freedom that depend periodically or\nquasi-periodically (QP) on time, with $\\ell$ external frequencies. Such a\nsystem is described by a Hamiltonian function in the $2n$-dimensional phase\nspace, $\\mathscr{M}$, that depends also on $\\ell$ angles, $\\varphi\\in\n\\mathbb{T}^\\ell$. We take advantage of the fibbered structure of the extended\nphase space $\\mathscr{M} \\times \\mathbb{T}^\\ell$. As a result of our approach,\nthe parameterization of tori requires the last $\\ell$ variables, to be precise\n$\\varphi$, while the first $2n$ components are determined by an invariance\nequation. This reduction decreases the dimension of the problem where the\nunknown is a parameterization from $2(n+\\ell)$ to $2n$. We employ a\nquasi-Newton method, in order to prove the KAM theorem. This iterative method\nbegins with an initial parameterization of an approximately invariant torus,\nmeaning it approximately satisfies the invariance equation. The approximation\nis refined by applying corrections that reduce quadratically the invariance\nequation error. This process converges to a torus in a complex strip of size\n$\\rho_\\infty$, provided suitable Diophantine $(\\gamma,\\tau)$ conditions and a\nnon-degeneracy condition on the torsion are met. Given the nature of the proof,\nthis provides a numerical method that can be effectively implemented on a\ncomputer, the details are given in the companion paper [CHP25]. This approach\nleverages precision and efficiency to compute invariant tori.",
      "generated_abstract": "aper, we study the linear Diophantine equations of the form\n$$\\sum_{i=1}^n a_i x_i^d - b_n = 0,$$\nwhere $a_i \\in \\mathbb{Z}$, $b_n \\in \\mathbb{Z}$, $a_1 \\geq a_2 \\geq\n\\cdots \\geq a_n$ and $a_n \\geq 1$, $d$ is a positive integer.\n  We prove that if $\\gcd(d,n) = 1$, then there exists a solution to the\nlinear Diophantine equation if and only if $d = 1$, $n = 2$ or $n = 3$.\n  In particular, this proves that there is no solution to the linear Diophantine\nequation\n$$\\sum_{i=1}^n a_i x_i^d - b_n",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1095890410958904,
          "p": 0.2857142857142857,
          "f": 0.15841583757670827
        },
        "rouge-2": {
          "r": 0.008888888888888889,
          "p": 0.02666666666666667,
          "f": 0.013333329583334387
        },
        "rouge-l": {
          "r": 0.1095890410958904,
          "p": 0.2857142857142857,
          "f": 0.15841583757670827
        }
      }
    },
    {
      "paper_id": "math.RA.math/GR/2503.06221v1",
      "true_abstract": "Let $\\mathbf{O}(\\mathbb{F})$ be the split octonion algebra over an\nalgebraically closed field $\\mathbb{F}$. For positive integers $k_1, k_2\\geq\n2$, we study surjectivity of the map $A_1(x^{k_1}) + A_2(y^{k_2}) \\in\n\\mathbf{O}(\\mathbb{F})\\langle x, y\\rangle$ on $\\mathbf{O}(\\mathbb{F})$. For\nthis, we use the orbit representatives of the ${G}_2(\\mathbb{F})$-action on\n$\\mathbf{O}(\\mathbb{F}) \\times \\mathbf{O}(\\mathbb{F}) $ for the tuple $(A_1,\nA_2)$, and characterize the ones which give a surjective map.",
      "generated_abstract": "se of this work is to study the asymptotic behaviour of the\nfinite time existence of a nonlinear fluid-structure interaction system under\nthe action of an external perturbation. We consider a nonlinear fluid-structure\ninteraction system of the form:\n  \\begin{align}\n    \\begin{split}\n      &\\partial_t \\mathbf{u} - \\Delta \\mathbf{u} = \\mathbf{f}, \\\\\n      &\\partial_t \\mathbf{w} - \\Delta \\mathbf{w} + \\nabla p = \\mathbf{0}, \\\\\n      & \\nabla \\cdot \\mathbf{u} = 0, \\quad \\nabla \\cdot \\mathbf{w} = 0,\n      \\quad \\nabla p = \\mu(\\nabla \\mathbf{u})^2,\n    \\end{split}\n  \\end{align}\n  where $\\mathbf{u}$, $\\mathbf{w}$",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11538461538461539,
          "p": 0.11764705882352941,
          "f": 0.11650484936940356
        },
        "rouge-2": {
          "r": 0.01639344262295082,
          "p": 0.014705882352941176,
          "f": 0.015503870983716523
        },
        "rouge-l": {
          "r": 0.11538461538461539,
          "p": 0.11764705882352941,
          "f": 0.11650484936940356
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/app-ph/2503.09048v1",
      "true_abstract": "In recent years, twisting has emerged as a new degree of freedom that plays\nan increasingly important role in Bloch bands of various physical systems.\nHowever, there is currently a lack of reports on the non-trivial physics of\ntopological degeneracy in twisted systems. In this work, we investigated the\nintrinsic physical correlation between twisting and topological degeneracy. We\nfound that twisting not only breaks the symmetry of the system but also\nintroduces topological degeneracy that does not exist under the original\nsymmetric system without twisting. Furthermore, the topological degeneracy can\nbe easily tuned through twisting. This new twist-induced topological degeneracy\ngives rise to a unique polarization-degenerate birefringent medium, wherein the\ntwist angle acts as a novel degree of freedom for dispersion and polarization\nmanagement of interface states. Exhibiting fascinating properties and\nexperimental feasibilities, our work points to new possibilities in the\nresearch of various topological physics in twisted photonics.",
      "generated_abstract": "um state of a superconducting quantum bit (SQB) is a key resource\nfor quantum information processing. Here, we report the experimental realization\nof a high-fidelity single-qubit gate using a SQB as a qubit, which is\ndemonstrated to be in a coherent state. Using a nonlinear phase shifter, we\nmeasure the SQB's phase shift as a function of the phase difference between\nthe qubit's input and output ports, showing that the qubit's state evolves\nunder the gate operation. This evolution is characterized by a linear phase\nshift in the SQB's phase shift, which is measured to be 9.787(5) $\\times$ 10$^{-5}$\nrad, with a standard deviation of 0.245(5) $\\times$ 10$^{-5}$",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16326530612244897,
          "p": 0.22857142857142856,
          "f": 0.19047618561507945
        },
        "rouge-2": {
          "r": 0.02877697841726619,
          "p": 0.04,
          "f": 0.03347279848041947
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.2,
          "f": 0.1666666618055557
        }
      }
    },
    {
      "paper_id": "math.CO.math/CT/2503.06722v1",
      "true_abstract": "In this paper we explore the algebraic structure and combinatorial properties\nof eulerian magnitude homology. First, we analyze the diagonality conditions of\neulerian magnitude homology, providing a characterization of complete graphs.\nThen, we construct the regular magnitude-path spectral sequence as the spectral\nsequence of the (filtered) injective nerve of the reachability category, and\nexplore its consequences. Among others, we show that such spectral sequence\nconverges to the complex of injective words on a digraph, and yields\ncharacterization results for the regular path homology of diagonal directed\ngraphs.",
      "generated_abstract": "We study a certain class of non-commutative integral operators on the space\nof holomorphic functions defined on the unit disk $D$, which is a natural\ngeneralization of the classical Hilbert transform. We give a characterization\nof the class of such operators and we discuss some properties of them, such as\ntheir boundedness and trace properties. In particular, we prove that the\nclassical Hilbert transform and the class of non-commutative integral\noperators are isomorphic.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21428571428571427,
          "p": 0.2727272727272727,
          "f": 0.23999999507200007
        },
        "rouge-2": {
          "r": 0.05,
          "p": 0.06557377049180328,
          "f": 0.0567375837432729
        },
        "rouge-l": {
          "r": 0.17857142857142858,
          "p": 0.22727272727272727,
          "f": 0.19999999507200011
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.physics/soc-ph/2503.05380v1",
      "true_abstract": "This paper presents a quantitative comparison of power grid reinforcement\nstrategies. We evaluate three approaches: (1) doubling transmission links\n(bridges) between different communities, (2) adding bypasses around weakly\nsynchronized nodes, and (3) reinforcing edges that trigger the largest cascade\nfailures. We use two different models of the Hungarian high-voltage network.\nThese models are built from the official data provided by the transmission\nsystem operator, thus eliminating the assumptions typically used in other\nstudies. The coupling strength distribution of the Hungarian models shows good\nagreement with our previous works using the European and North American grids.\n  Additionally, we examine the occurrence of Braess' paradox, where added\ntransmission capacity unexpectedly reduces overall stability. Our results show\nthat reinforcement through community-based bridge duplication yields the most\nsignificant improvements across all parameters. A visual comparison highlights\ndifferences between this method and traditional reinforcement approaches. To\nthe authors' knowledge, this is the first attempt to quantitatively compare\nresults of oscillator-based studies with those relying on power system analysis\nsoftware.\n  Characteristic results of line-cut simulations reveal cascade size\ndistributions with fat-tailed decays for medium coupling strengths, while\nexponential behavior emerges for small and large couplings. The observed\nexponents are reminiscent of the continuously changing exponents due to\nGriffiths effects near a hybrid type of phase transition.",
      "generated_abstract": "of social networks is a fundamental research topic in physics,\nmathematics, and economics, as it provides a valuable insight into human\nbehavior and how it can be improved. The study of social networks can be\ndivided into two main categories: the topology of the network, and the\nproperties of the nodes within the network. In this work, we study the\nproperties of the nodes within the network, which include the number of\nconnections, the degree of the node, and the degree of centrality. In this\ncontext, we define the centrality index as the number of connections and the\ndegree of the node. We use the Watts-Strogatz model as a model for the\nnetwork, and we investigate the properties of the centrality indices of the\nnodes in this model. We use a novel approach to analyze the centrality\nindices, which consists of using the Fourier transform of the centrality index\nmatrix",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10559006211180125,
          "p": 0.23943661971830985,
          "f": 0.1465517198903836
        },
        "rouge-2": {
          "r": 0.014634146341463415,
          "p": 0.02631578947368421,
          "f": 0.018808772836352964
        },
        "rouge-l": {
          "r": 0.09316770186335403,
          "p": 0.2112676056338028,
          "f": 0.1293103405800388
        }
      }
    },
    {
      "paper_id": "math.CO.cs/CG/2503.09919v1",
      "true_abstract": "We provide a family of $5$-dimensional prismatoids whose width grows linearly\nin the number of vertices. This provides a new infinite family of\ncounter-examples to the Hirsch conjecture whose excess width grows linearly in\nthe number of vertices, and answers a question of Matschke, Santos and Weibel.",
      "generated_abstract": "This paper studies the $k$-center problem for $k$-regular graphs with\ndiameter at most $D$ and a minimum degree of at least $d$. In this problem,\neach vertex is assigned to one of $k$ centers, and the objective is to find\nthese centers such that their cardinalities are minimized. We prove that the\nproblem is NP-hard even when the diameter of the graph is 3, and that it is\nNP-hard even when the diameter is 4. We also show that the problem is\nNP-hard even when the graph is a complete graph with at most $D$ vertices. In\naddition, we show that the problem is NP-hard even when the graph is a\ncomplete graph with at least $d$ vertices.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25806451612903225,
          "p": 0.14035087719298245,
          "f": 0.18181817725464886
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.25806451612903225,
          "p": 0.14035087719298245,
          "f": 0.18181817725464886
        }
      }
    },
    {
      "paper_id": "physics.flu-dyn.physics/ao-ph/2503.03009v1",
      "true_abstract": "Wave breaking is a critical process in the upper ocean: an energy sink for\nthe surface wave field and a source for turbulence in the ocean surface\nboundary layer. We apply a novel multi-layer numerical solver resolving\nupper-ocean dynamics over scales from O(50cm) to O(1km), including a\nbroad-banded wave field and wave breaking. The present numerical study isolates\nthe effect of wave breaking and allows us to study the surface layer in\nwave-influenced and wave-breaking-dominated regimes. Following our previous\nwork showing wave breaking statistics in agreement with field observations, we\nextend the analysis to underwater breaking-induced turbulence and related\ndissipation (in freely decaying conditions). We observe a rich field of\nvorticity resulting from the turbulence generation by breaking waves. We\ndiscuss the vertical profiles of dissipation rate which are compared with field\nobservations, and propose an empirical universal shape function. Good agreement\nis found, further demonstrating that wave breaking can dominate turbulence\ngeneration in the near-surface layer. We examine the dissipation from different\nangles: the global dissipation of the wave field computed from the decaying\nwave field, the spectral dissipation from the fifth moment of breaking front\ndistribution, and a turbulence dissipation estimated from the underwater strain\nrate tensor. Finally, we consider how these different estimates can be\nunderstood as part of a coherent framework.",
      "generated_abstract": "t a new computational method to simulate the motion of a cylindrical\nfluid particle in a viscous flow. The particle is modeled as a fluid-filled\ncylinder with a finite radius, and the motion is described by the\nLax-Wendroff method. We develop a novel boundary condition to ensure the\nuniformity of the flow in the circumferential direction, and we apply it to\nsimulate the motion of a single-cylinder particle in a viscous flow. We show\nthat the new boundary condition is essential for accurately simulating the\nmotion of the particle in a viscous flow. We also show that the new boundary\ncondition can be implemented using a simple modification to the existing\nLax-Wendroff boundary condition. The proposed method is validated through a\ncomparative study with a traditional boundary condition. The results show that\nthe new boundary condition provides an accurate description",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17886178861788618,
          "p": 0.3283582089552239,
          "f": 0.23157894280277014
        },
        "rouge-2": {
          "r": 0.025510204081632654,
          "p": 0.04716981132075472,
          "f": 0.03311257822551705
        },
        "rouge-l": {
          "r": 0.17073170731707318,
          "p": 0.31343283582089554,
          "f": 0.22105262701329648
        }
      }
    },
    {
      "paper_id": "stat.ML.cs/SI/2503.09660v1",
      "true_abstract": "Point signatures based on the Laplacian operators on graphs, point clouds,\nand manifolds have become popular tools in machine learning for graphs,\nclustering, and shape analysis. In this work, we propose a novel point\nsignature, the power spectrum signature, a measure on $\\mathbb{R}$ defined as\nthe squared graph Fourier transform of a graph signal. Unlike eigenvectors of\nthe Laplacian from which it is derived, the power spectrum signature is\ninvariant under graph automorphisms. We show that the power spectrum signature\nis stable under perturbations of the input graph with respect to the\nWasserstein metric. We focus on the signature applied to classes of indicator\nfunctions, and its applications to generating descriptive features for vertices\nof graphs. To demonstrate the practical value of our signature, we showcase\nseveral applications in characterizing geometry and symmetries in point cloud\ndata, and graph regression problems.",
      "generated_abstract": "r introduces a novel framework for large-scale unsupervised\nlearning and recommender systems that leverages generative adversarial\nnetworks (GANs) to synthesize high-quality, domain-invariant representations\nfor user-item interaction data. Our framework, GAN-IRL, consists of two\ncomponents: (1) a generative GAN model that generates user and item representations\nfor unlabeled data, and (2) a replay-based recommendation model that uses the\ngenerated representations to enhance user-item interactions. The generative\nmodel is trained to generate high-quality representations while simultaneously\nbalancing the generation of realistic but unrepresentative fake data and\nincorrectly generated fake data. The recommendation model is trained to\nenhance user-item interactions by incorporating the generated representations\ninto the interaction process. Experiments on two large-scale datasets demonstrate\nthe effectiveness of GAN-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.1780821917808219,
          "f": 0.15853658042608582
        },
        "rouge-2": {
          "r": 0.022900763358778626,
          "p": 0.02912621359223301,
          "f": 0.02564102071261692
        },
        "rouge-l": {
          "r": 0.13186813186813187,
          "p": 0.1643835616438356,
          "f": 0.14634145847486635
        }
      }
    },
    {
      "paper_id": "math.ST.q-bio/PE/2501.16526v1",
      "true_abstract": "Ancestral inference for branching processes in random environments involves\ndetermining the ancestor distribution parameters using the population sizes of\ndescendant generations. In this paper, we introduce a new methodology for\nancestral inference utilizing the generalized method of moments. We demonstrate\nthat the estimator's behavior is critically influenced by the coefficient of\nvariation of the environment sequence. Furthermore, despite the process's\nevolution being heavily dependent on the offspring means of various\ngenerations, we show that the joint limiting distribution of the ancestor and\noffspring estimators of the mean, under appropriate centering and scaling,\ndecouple and converge to independent Gaussian random variables when the ratio\nof the number of generations to the logarithm of the number of replicates\nconverges to zero. Additionally, we provide estimators for the limiting\nvariance and illustrate our findings through numerical experiments and data\nfrom Polymerase Chain Reaction experiments and COVID-19 data.",
      "generated_abstract": "The recent emergence of large language models (LLMs) has sparked renewed\ninterest in their use in computational biology. While some approaches have\ndemonstrated potential for improving scientific discovery, they have also been\nassociated with risks of bias, privacy violations, and lack of transparency.\nThis paper introduces a novel framework for evaluating LLMs in scientific\ndiscovery, based on their ability to generate accurate and comprehensive\nrepresentations of scientific concepts. We introduce a set of metrics that\nquantify the reliability and generalizability of LLM-generated scientific\nrepresentations, and demonstrate how they can be used to assess the impact of\nLLMs on scientific discovery. We conclude by discussing potential applications\nof our framework in the context of open science and responsible artificial\nintelligence in scientific research.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14583333333333334,
          "p": 0.16091954022988506,
          "f": 0.153005459492968
        },
        "rouge-2": {
          "r": 0.007462686567164179,
          "p": 0.008403361344537815,
          "f": 0.007905133357499742
        },
        "rouge-l": {
          "r": 0.13541666666666666,
          "p": 0.14942528735632185,
          "f": 0.14207649774433415
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2503.07203v1",
      "true_abstract": "Network pharmacology (NP) explores pharmacological mechanisms through\nbiological networks. Multi-omics data enable multi-layer network construction\nunder diverse conditions, requiring integration into NP analyses. We developed\nPOINT, a novel NP platform enhanced by multi-omics biological networks,\nadvanced algorithms, and knowledge graphs (KGs) featuring network-based and\nKG-based analytical functions. In the network-based analysis, users can perform\nNP studies flexibly using 1,158 multi-omics biological networks encompassing\nproteins, transcription factors, and non-coding RNAs across diverse cell line-,\ntissue- and disease-specific conditions. Network-based analysis-including\nrandom walk with restart (RWR), GSEA, and diffusion profile (DP) similarity\nalgorithms-supports tasks such as target prediction, functional enrichment, and\ndrug screening. We merged networks from experimental sources to generate a\npre-integrated multi-layer human network for evaluation. RWR demonstrated\nsuperior performance with a 33.1% average ranking improvement over the\nsecond-best algorithm, PageRank, in identifying known targets across 2,002\ndrugs. Additionally, multi-layer networks significantly improve the ability to\nidentify FDA-approved drug-disease pairs compared to the single-layer network.\nFor KG-based analysis, we compiled three high-quality KGs to construct POINT\nKG, which cross-references over 90% of network-based predictions. We\nillustrated the platform's capabilities through two case studies. POINT bridges\nthe gap between multi-omics networks and drug discovery; it is freely\naccessible at http://point.gene.ac/.",
      "generated_abstract": "networks are complex dynamical systems, and their behavior is\ndependent on the neurons' internal states and their interactions with their\nenvironment. In this study, we develop a model of a neuronal network consisting\nof two types of neurons: excitatory and inhibitory, each with different\nresponses to the input signals. The model is based on the non-linear\nHodgkin-Huxley model and its generalization to the case of multiple types of\nneurons. In our model, the synaptic transmission is determined by the\ninteraction between the membrane potential of a neuron and the input current\nsurrounding it. We show that the network can be divided into two sub-networks,\ndepending on the values of the input currents. In the case of small input\ncurrents, the sub-network consists of the excitatory neurons only, while in the\ncase of large input",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12658227848101267,
          "p": 0.24390243902439024,
          "f": 0.1666666621680557
        },
        "rouge-2": {
          "r": 0.010101010101010102,
          "p": 0.016666666666666666,
          "f": 0.01257861165302181
        },
        "rouge-l": {
          "r": 0.12658227848101267,
          "p": 0.24390243902439024,
          "f": 0.1666666621680557
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/SC/2503.10416v1",
      "true_abstract": "Runtime repeated recursion unfolding was recently introduced as a\njust-in-time program transformation strategy that can achieve super-linear\nspeedup. So far, the method was restricted to single linear direct recursive\nrules in the programming language Constraint Handling Rules (CHR). In this\ncompanion paper, we generalize the technique to multiple recursion and to\nmultiple recursive rules and provide an implementation of the generalized\nmethod in the logic programming language Prolog.\n  The basic idea of the approach is as follows: When a recursive call is\nencountered at runtime, the recursive rule is unfolded with itself and this\nprocess is repeated with each resulting unfolded rule as long as it is\napplicable to the current call. In this way, more and more recursive steps are\ncombined into one recursive step. Then an interpreter applies these rules to\nthe call starting from the most unfolded rule. For recursions which have\nsufficiently simplifyable unfoldings, a super-linear can be achieved, i.e. the\ntime complexity is reduced.\n  We implement an unfolder, a generalized meta-interpreter and a novel\nround-robin rule processor for our generalization of runtime repeated recursion\nunfolding with just ten clauses in Prolog. We illustrate the feasibility of our\ntechnique with worst-case time complexity estimates and benchmarks for some\nbasic classical algorithms that achieve a super-linear speedup.",
      "generated_abstract": "This paper introduces a novel approach to the analysis of probabilistic\ndifferential equations. By leveraging the recent advancements in the\ncomputation of conditional expectation operators, we develop a framework for\nstudying the dynamics of random systems governed by probabilistic differential\nequations. Our approach combines the benefits of the randomized time-stepping\nscheme and the stochastic simulation framework, offering significant\ncomputational advantages over the state-of-the-art. In particular, our\nproposal provides a robust solution to the state-space reconstruction of\nprobabilistic differential equations and enables the efficient simulation of\nstochastic differential equations. The effectiveness of our method is\ndemonstrated through various numerical examples.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12,
          "p": 0.22058823529411764,
          "f": 0.15544040994389124
        },
        "rouge-2": {
          "r": 0.025380710659898477,
          "p": 0.054945054945054944,
          "f": 0.03472221789954722
        },
        "rouge-l": {
          "r": 0.112,
          "p": 0.20588235294117646,
          "f": 0.14507771564337313
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2410.23275v1",
      "true_abstract": "We introduce a novel Dynamic Graph Neural Network (DGNN) architecture for\nsolving conditional $m$-steps ahead forecasting problems in temporal financial\nnetworks. The proposed DGNN is validated on simulated data from a temporal\nfinancial network model capturing stylized features of Interest Rate Swaps\n(IRSs) transaction networks, where financial entities trade swap contracts\ndynamically and the network topology evolves conditionally on a reference rate.\nThe proposed model is able to produce accurate conditional forecasts of net\nvariation margins up to a $21$-day horizon by leveraging conditional\ninformation under pre-determined stress test scenarios. Our work shows that the\nnetwork dynamics can be successfully incorporated into stress-testing\npractices, thus providing regulators and policymakers with a crucial tool for\nsystemic risk monitoring.",
      "generated_abstract": "tion of financial markets has been marked by a series of\nchanges in the structure of financial products and their use. The paper\nexplores the evolution of the investment fund industry in the United States,\nexploring the role of market structure in its development. The analysis focuses\non the evolution of investment funds, their products and the role of market\nstructure. The paper also explores the evolution of investment fund managers and\ntheir role in the industry. The paper explores the evolution of the\ninvestment fund industry in the United States, exploring the role of market\nstructure in its development. The analysis focuses on the evolution of\ninvestment funds, their products and the role of market structure. The paper\nalso explores the evolution of investment fund managers and their role in the\nindustry. The paper explores the evolution of the investment fund industry in\nthe United States, exploring the role of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09375,
          "p": 0.23684210526315788,
          "f": 0.1343283541456896
        },
        "rouge-2": {
          "r": 0.008849557522123894,
          "p": 0.016666666666666666,
          "f": 0.011560689110897561
        },
        "rouge-l": {
          "r": 0.09375,
          "p": 0.23684210526315788,
          "f": 0.1343283541456896
        }
      }
    },
    {
      "paper_id": "cs.GT.cs/CR/2503.10185v1",
      "true_abstract": "Following the publication of Bitcoin's arguably most famous attack, selfish\nmining, various works have introduced mechanisms to enhance blockchain systems'\ngame theoretic resilience. Some reward mechanisms, like FruitChains, have been\nshown to be equilibria in theory. However, their guarantees assume\nnon-realistic parameters and their performance degrades significantly in a\npractical deployment setting. In this work we introduce a reward allocation\nmechanism, called Proportional Splitting (PRS), which outperforms existing\nstate of the art. We show that, for large enough parameters, PRS is an\nequilibrium, offering the same theoretical guarantees as the state of the art.\nIn addition, for practical, realistically small, parameters, PRS outperforms\nall existing reward mechanisms across an array of metrics. We implement PRS on\ntop of a variant of PoEM, a Proof-of-Work (PoW) protocol that enables a more\naccurate estimation of each party's mining power compared to e.g., Bitcoin. We\nthen evaluate PRS both theoretically and in practice. On the theoretical side,\nwe show that our protocol combined with PRS is an equilibrium and guarantees\nfairness, similar to FruitChains. In practice, we compare PRS with an array of\nexisting reward mechanisms and show that, assuming an accurate estimation of\nthe mining power distribution, it outperforms them across various\nwell-established metrics. Finally, we realize this assumption by approximating\nthe power distribution via low-work objects called \"workshares\" and quantify\nthe tradeoff between the approximation's accuracy and storage overhead.",
      "generated_abstract": "t a novel framework for the design and evaluation of\ncryptographic protocols. Our approach is based on the notion of\n\\emph{safety properties} and \\emph{security properties}. A protocol satisfies a\nsecurity property if it protects a given target against a given attack. A\nsafety property is a property that ensures that a protocol does not violate\nthe target. We show that the \\emph{transfer property} and the\n\\emph{composition property} are both safety properties, and that the\n\\emph{consistency property} is a security property. We then present an\nalgorithm for checking whether a given protocol satisfies a given safety\nproperty. We prove that checking whether a given protocol satisfies a given\nsafety property is \\textsf{NP}-hard and show that checking whether a given\nprotocol satisfies a given security property is \\textsf{NP}-hard as well. We\nthen present an",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10884353741496598,
          "p": 0.27586206896551724,
          "f": 0.15609755691802507
        },
        "rouge-2": {
          "r": 0.018691588785046728,
          "p": 0.042105263157894736,
          "f": 0.0258899633791023
        },
        "rouge-l": {
          "r": 0.10884353741496598,
          "p": 0.27586206896551724,
          "f": 0.15609755691802507
        }
      }
    },
    {
      "paper_id": "quant-ph.cond-mat/other/2503.09292v1",
      "true_abstract": "In the pursuit of robust quantum computing, we put forth a platform based on\nphotonic qubits in a circuit-QED environment. Specifically, we propose a\nversatile two-qubit gate based on two cavities coupled via a transmon,\nconstituting a selective number-dependent phase gate operating on the in-phase\neigenmodes of the two cavities, the Eigen-SNAP gate. This gate natively\noperates in the dispersive coupling regime of the cavities and the transmon,\nand operates by driving the transmon externally, to imprint desired phases on\nthe number states. As an example for the utility of the Eigen-SNAP gate, we\nimplement a $\\sqrt{\\text{SWAP}}$ gate on a system of two logical bosonic qubits\nencoded in the cavities. Further, we use numerical optimization to determine\nthe optimal implementation of the $\\sqrt{\\text{SWAP}}$. We find that the\nfidelities of these optimal protocols are only limited by the coherence times\nof the system's components. These findings pave the way to continuous variable\nquantum computing in cavity-transmon systems.",
      "generated_abstract": "the effect of the interaction between an $s$-wave superconductor\nand a normal metal on the superconducting transition temperature $T_c$ and\nthermal conductivity $\\kappa$ of the superconductor. The normal metal is\ninteracting with the superconductor through a $t$-$t'$ hopping channel. We\nanalyze the effects of the energy scales of the normal metal and the\nsuperconductor on $T_c$ and $\\kappa$. We find that for a finite $t'$, $T_c$ is\nreduced, and the normal metal thermal conductivity is enhanced, compared to the\ncase of $t'=0$. When $t'$ is large, the effect of the superconductor is to\nsuppress $T_c$, and enhance $\\kappa$. However, when $t'$ is small, $T_c$ is\nincre",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11458333333333333,
          "p": 0.2037037037037037,
          "f": 0.1466666620586668
        },
        "rouge-2": {
          "r": 0.03424657534246575,
          "p": 0.05747126436781609,
          "f": 0.04291845025622184
        },
        "rouge-l": {
          "r": 0.10416666666666667,
          "p": 0.18518518518518517,
          "f": 0.1333333287253335
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09367v1",
      "true_abstract": "Shi, Walsh and Yu demonstrated that any dense circuit graph contains a large\nnear-triangulation. We extend the result to $2$-connected plane graphs, thereby\naddressing a question posed by them. Using the result, we prove that the planar\nTu\\'{a}n number of $2C_k$ is $\\left[3-\\Theta(k^{\\log_23})^{-1}\\right]n$ when\n$k\\geq 5$.",
      "generated_abstract": "In this paper, we provide a new proof of the classical result of H\\\"older\ninverse function theorem for the sum of two holomorphic functions in the case\nof compact Riemann surfaces. In the case of a Riemann surface with a non-zero\nconstant holomorphic sectional curvature, we obtain a new proof of a classical\nresult of A. Birkhoff in 1937. We also prove the H\\\"older inverse function\ntheorem for the sum of two holomorphic functions in the case of a Riemann\nsurface with a constant sectional curvature.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.175,
          "f": 0.17073170232004772
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.175,
          "f": 0.17073170232004772
        }
      }
    },
    {
      "paper_id": "cs.MA.cs/MA/2503.08728v1",
      "true_abstract": "Multi-agent reinforcement learning (MARL) has shown significant potential in\ntraffic signal control (TSC). However, current MARL-based methods often suffer\nfrom insufficient generalization due to the fixed traffic patterns and road\nnetwork conditions used during training. This limitation results in poor\nadaptability to new traffic scenarios, leading to high retraining costs and\ncomplex deployment. To address this challenge, we propose two algorithms:\nPLight and PRLight. PLight employs a model-based reinforcement learning\napproach, pretraining control policies and environment models using predefined\nsource-domain traffic scenarios. The environment model predicts the state\ntransitions, which facilitates the comparison of environmental features.\nPRLight further enhances adaptability by adaptively selecting pre-trained\nPLight agents based on the similarity between the source and target domains to\naccelerate the learning process in the target domain. We evaluated the\nalgorithms through two transfer settings: (1) adaptability to different traffic\nscenarios within the same road network, and (2) generalization across different\nroad networks. The results show that PRLight significantly reduces the\nadaptation time compared to learning from scratch in new TSC scenarios,\nachieving optimal performance using similarities between available and target\nscenarios.",
      "generated_abstract": "st decade, Deep Learning (DL) has been a powerful tool for\nresearchers in Machine Learning (ML). However, the complexities of DL make it\ndifficult to train on large datasets, especially when the size of the dataset\nexceeds the capabilities of the available computing resources. In this paper,\nwe propose a novel approach for large-scale learning in the context of\nMulti-class-Multi-label (MCML) classification tasks. Our approach is based on\na data-driven method to reduce the dimensionality of the dataset by\npruning-based methods. We further propose an ensemble-based method to combine\nthe predictions of the pruned-based models to reduce the computational costs\nof training the final model. The effectiveness of our approach is evaluated on\na real-world dataset, where it achieved a 63% improvement in accuracy,\nachieving the best performance in terms of accuracy and model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.216,
          "p": 0.2967032967032967,
          "f": 0.24999999512388554
        },
        "rouge-2": {
          "r": 0.017142857142857144,
          "p": 0.024193548387096774,
          "f": 0.020066884777576368
        },
        "rouge-l": {
          "r": 0.192,
          "p": 0.26373626373626374,
          "f": 0.2222222173461078
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2503.00391v1",
      "true_abstract": "In this working paper, I developed a suite of macroeconomic models that shed\nlight on the intricate relationship between economic development, health, and\nfertility. These innovative models conceptualize health as an intermediate\ngood, paving the way for new interpretations of dynamic socio-economic\nphenomena, particularly the non-monotonic effects of health on economic and\npopulation growth. The evolving dynamic interactions among economic growth,\npopulation, and health during the early stages of human development have been\nwell interpreted in this research.",
      "generated_abstract": "aper, we introduce a novel approach to designing optimal policy\nfor the management of a single, non-hierarchical, resource pool, where each\nworker is given a budget to spend on a single task. The key challenge is to\ndesign a policy that maximizes the overall outcome while ensuring that workers\ndo not exceed their budget. We propose an online policy that minimizes the\naggregate cost of each worker's consumption, and we prove that the policy\nexists and is unique. We also show that the policy is efficient in that it\nminimizes the average cost of the workers' consumption and the average cost\nof the pool. Moreover, we show that the optimal policy is also efficient, and\nthat the optimal policy is a linear combination of the online policy and an\nexpensive policy that works as a safety net for workers who exceed their budgets.\nWe also provide a simple example that illustrates the utility of the\nonline",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1774193548387097,
          "p": 0.13580246913580246,
          "f": 0.15384614893442236
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14516129032258066,
          "p": 0.1111111111111111,
          "f": 0.12587412096239442
        }
      }
    },
    {
      "paper_id": "astro-ph.IM.astro-ph/IM/2503.10106v1",
      "true_abstract": "This work presents GalProTE, a proof-of-concept Machine Learning model\nutilizing a Transformer Encoder to determine stellar age, metallicity, and dust\nattenuation from optical spectra. Designed for large astronomical surveys,\nGalProTE significantly accelerates processing while maintaining accuracy. Using\nthe E-MILES spectral library, we construct a dataset of 111,936 diverse\ntemplates by expanding 636 simple stellar population models with varying\nextinction, spectral combinations, and noise modifications. This ensures robust\ntraining over 4750 to 7100 Angstrom at 2.5 Angstrom resolution. GalProTE\nemploys four parallel attention-based encoders with varying kernel sizes to\ncapture spectral features. On synthetic test data, it achieves a mean squared\nerror (MSE) of 0.27% between input and predicted spectra. Validation on\nPHANGS-MUSE galaxies NGC4254 and NGC5068 confirms its ability to extract\nphysical parameters efficiently, with residuals averaging -0.02% and 0.28% and\nstandard deviations of 4.3% and 5.3%, respectively. To contextualize these\nresults, we compare GalProTE's age, metallicity, and dust attenuation maps with\npPXF, a state-of-the-art spectral fitting tool. While pPXF requires\napproximately 11 seconds per spectrum, GalProTE processes one in less than 4\nmilliseconds, offering a 2750 times speedup and consuming 68 times less power\nper spectrum. The strong agreement between pPXF and GalProTE highlights the\npotential of machine learning to enhance traditional methods, paving the way\nfor faster, energy-efficient, and scalable analyses of galactic properties in\nmodern surveys.",
      "generated_abstract": "presents the first comprehensive study of the X-ray emission from\nthe core of the Chandra X-ray Observatory (CXO) X-ray source SXDF J0010+1818,\nwhich was discovered as a CXO source in the CXO All-Sky Survey (CSAS).\n  The SXDF J0010+1818 system was identified as a source of high-luminosity\nradio and X-ray emission. The optical counterpart of the CXO source,\nSXDFJ0010+1819, was observed by the Hubble Space Telescope (HST) and found to\nhave an extremely red optical color. We performed aperture photometry of the\noptical and X-ray sources and found that the optical source is a point source\nwith a flux of $\\sim 10^{-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07017543859649122,
          "p": 0.1935483870967742,
          "f": 0.1030042879397301
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07017543859649122,
          "p": 0.1935483870967742,
          "f": 0.1030042879397301
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.07649v1",
      "true_abstract": "We propose a method to learn the nonlinear impulse responses to structural\nshocks using neural networks, and apply it to uncover the effects of US\nfinancial shocks. The results reveal substantial asymmetries with respect to\nthe sign of the shock. Adverse financial shocks have powerful effects on the US\neconomy, while benign shocks trigger much smaller reactions. Instead, with\nrespect to the size of the shocks, we find no discernible asymmetries.",
      "generated_abstract": "p a novel methodology to evaluate the accuracy of parametric\ndistributions in applications. Our approach employs a Monte Carlo simulation\nstudy to assess the accuracy of parametric distributions. We evaluate the\naccuracy of the normal, Student's t-distribution, and beta distributions for\nvarious sample sizes. We also investigate the accuracy of the Chi-square\ndistribution in small samples. We compare the accuracy of the parametric\ndistributions with the accuracy of the corresponding nonparametric alternatives\nin the presence of outliers. We find that the parametric distributions exhibit\nlesser accuracy compared to the nonparametric alternatives. We also compare the\naccuracy of the parametric distributions in the presence of heteroskedasticity\nand serial correlation. We find that the parametric distributions exhibit lesser\naccuracy compared to the nonparametric alternatives in the presence of\nheteroskedasticity and serial correlation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16,
          "p": 0.15384615384615385,
          "f": 0.1568627400999617
        },
        "rouge-2": {
          "r": 0.03076923076923077,
          "p": 0.025974025974025976,
          "f": 0.02816900912021512
        },
        "rouge-l": {
          "r": 0.12,
          "p": 0.11538461538461539,
          "f": 0.11764705382545197
        }
      }
    },
    {
      "paper_id": "quant-ph.cond-mat/stat-mech/2503.10308v1",
      "true_abstract": "We consider learnability transitions in monitored quantum systems that\nundergo noisy evolution, subject to a global strong symmetry -- i.e., in\naddition to the measuring apparatus, the system can interact with an unobserved\nenvironment, but does not exchange charge with it. As in the pure-state\nsetting, we find two information-theoretic phases -- a sharp (fuzzy) phase in\nwhich an eavesdropper can rapidly (slowly) learn the symmetry charge. However,\nbecause the dynamics is noisy, both phases can be simulated efficiently using\ntensor networks. Indeed, even when the true dynamics is unitary, introducing\nnoise by hand allows an eavesdropper to efficiently learn the symmetry charge\nfrom local measurements, as we demonstrate. We identify the fuzzy phase in this\nsetting as a mixed-state phase that exhibits spontaneous strong-to-weak\nsymmetry breaking.",
      "generated_abstract": "t a new quantum algorithm for the design of optimal superconducting\nquantum devices, including single-qubit gates and Majorana-based control\noperations, by simultaneously optimizing the device geometry and\ncircuital topology. Our algorithm relies on a gradient-based optimization\nmethodology that employs an iterative, gradient-based optimization framework\nthat iteratively evaluates the cost function and updates the device's\nparameters to minimize this cost. We demonstrate that our algorithm provides\nunprecedented flexibility in the design of optimal superconducting devices by\nenabling the simultaneous optimization of both device geometry and\ncircuital topology. The algorithm also exhibits significant efficiency,\nachieving a 1000-fold reduction in the computational time required to find the\noptimal device parameters compared to previous methods. Our algorithm\ndemonstrates that a single quantum computer can efficiently and effectively\noptimize the design",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17582417582417584,
          "p": 0.2,
          "f": 0.1871344979446669
        },
        "rouge-2": {
          "r": 0.00819672131147541,
          "p": 0.008849557522123894,
          "f": 0.008510633305208903
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.1625,
          "f": 0.15204677864642127
        }
      }
    },
    {
      "paper_id": "cs.AI.cs/GT/2503.09858v1",
      "true_abstract": "This paper investigates the complex interplay between AI developers,\nregulators, users, and the media in fostering trustworthy AI systems. Using\nevolutionary game theory and large language models (LLMs), we model the\nstrategic interactions among these actors under different regulatory regimes.\nThe research explores two key mechanisms for achieving responsible governance,\nsafe AI development and adoption of safe AI: incentivising effective regulation\nthrough media reporting, and conditioning user trust on commentariats'\nrecommendation. The findings highlight the crucial role of the media in\nproviding information to users, potentially acting as a form of \"soft\"\nregulation by investigating developers or regulators, as a substitute to\ninstitutional AI regulation (which is still absent in many regions). Both\ngame-theoretic analysis and LLM-based simulations reveal conditions under which\neffective regulation and trustworthy AI development emerge, emphasising the\nimportance of considering the influence of different regulatory regimes from an\nevolutionary game-theoretic perspective. The study concludes that effective\ngovernance requires managing incentives and costs for high quality\ncommentaries.",
      "generated_abstract": "e the problem of generating synthetic examples for learning\nin an unbalanced dataset setting. In the unbalanced setting, the dataset\ndistribution is biased in favor of one class and is unfair to the other. This\nresults in a situation where the learner is learning from a dataset that is\nbiased and is unaware of this bias. We consider a synthetic data generation\nframework where the learner generates synthetic examples from a dataset that is\nbiased and is unaware of this bias. In this paper, we consider the problem of\ngenerating synthetic examples from a dataset that is biased in favor of one\nclass and is unfair to the other. We provide theoretical guarantees for the\nproposed synthetic data generation framework, and we demonstrate that it can be\nused to generate synthetic examples that are comparable to the true examples.\nOur results show that the learner can generate synthetic examples that\nout",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11403508771929824,
          "p": 0.20967741935483872,
          "f": 0.14772726816373982
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11403508771929824,
          "p": 0.20967741935483872,
          "f": 0.14772726816373982
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.03973v1",
      "true_abstract": "Range-only Simultaneous Localisation and Mapping (RO-SLAM) is of interest due\nto its practical applications in ultra-wideband (UWB) and Bluetooth Low Energy\n(BLE) localisation in terrestrial and aerial applications and acoustic beacon\nlocalisation in submarine applications. In this work, we consider a mobile\nrobot equipped with an inertial measurement unit (IMU) and a range sensor that\nmeasures distances to a collection of fixed landmarks. We derive an equivariant\nfilter (EqF) for the RO-SLAM problem based on a symmetry Lie group that is\ncompatible with the range measurements. The proposed filter does not require\nbootstrapping or initialisation of landmark positions, and demonstrates\nrobustness to the no-prior situation. The filter is demonstrated on a\nreal-world dataset, and it is shown to significantly outperform a\nstate-of-the-art EKF alternative in terms of both accuracy and robustness.",
      "generated_abstract": "guage models (LLMs) have been demonstrated to have substantial\napplications in robotics, particularly in high-risk scenarios. However,\nexisting methods often rely on pre-trained LLMs with limited domain\nknowledge, limiting their practical value. In this paper, we propose a novel\nframework for enhancing the safety of autonomous driving (AD) by leveraging\nLLMs for scenario understanding. Specifically, we integrate LLMs with\nsemi-supervised reinforcement learning (SSLRL) to achieve safe decision-making\nin autonomous driving. We first train a LLM to predict the future trajectory\nof the robot. Then, we train an SSLRL agent to optimize the trajectory in\ncollision-free manner. In addition, we propose a LLM-based scenario\nunderstanding module that employs multi-modal information, including LLM\noutputs, visual observations, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20430107526881722,
          "p": 0.22093023255813954,
          "f": 0.2122904978009427
        },
        "rouge-2": {
          "r": 0.015625,
          "p": 0.01834862385321101,
          "f": 0.016877632162938312
        },
        "rouge-l": {
          "r": 0.1827956989247312,
          "p": 0.19767441860465115,
          "f": 0.1899441290858589
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.10419v1",
      "true_abstract": "In motion simulation, motion cueing algorithms are used for the trajectory\nplanning of the motion simulator platform, where workspace limitations prevent\ndirect reproduction of reference trajectories. Strategies such as motion\nwashout, which return the platform to its center, are crucial in these\nsettings. For serial robotic MSPs with highly nonlinear workspaces, it is\nessential to maximize the efficient utilization of the MSPs kinematic and\ndynamic capabilities. Traditional approaches, including classical washout\nfiltering and linear model predictive control, fail to consider\nplatform-specific, nonlinear properties, while nonlinear model predictive\ncontrol, though comprehensive, imposes high computational demands that hinder\nreal-time, pilot-in-the-loop application without further simplification. To\novercome these limitations, we introduce a novel approach using deep\nreinforcement learning for motion cueing, demonstrated here for the first time\nin a 6-degree-of-freedom setting with full consideration of the MSPs kinematic\nnonlinearities. Previous work by the authors successfully demonstrated the\napplication of DRL to a simplified 2-DOF setup, which did not consider\nkinematic or dynamic constraints. This approach has been extended to all 6 DOF\nby incorporating a complete kinematic model of the MSP into the algorithm, a\ncrucial step for enabling its application on a real motion simulator. The\ntraining of the DRL-MCA is based on Proximal Policy Optimization in an\nactor-critic implementation combined with an automated hyperparameter\noptimization. After detailing the necessary training framework and the\nalgorithm itself, we provide a comprehensive validation, demonstrating that the\nDRL MCA achieves competitive performance against established algorithms.\nMoreover, it generates feasible trajectories by respecting all system\nconstraints and meets all real-time requirements with low...",
      "generated_abstract": "This paper presents a novel method for estimating the state of the system\nunder the influence of external disturbance. The method employs a linear time\ninvariant (LTI) controller, where the control gain is determined by a\nnonlinear optimization problem. The optimization problem is formulated in a\nrepresentation that can be efficiently solved by a pre-trained neural network\n(NN) architecture. The NN is trained with a dataset generated from the\nestablished model. The performance of the NN-based controller is verified\nthrough numerical simulations. The results show that the NN-based controller\nprovides a reliable solution for the estimation of the system state. Additionally,\nthe proposed method is found to be robust against model misspecification,\nparameter errors, and disturbance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13872832369942195,
          "p": 0.3157894736842105,
          "f": 0.19277108009612756
        },
        "rouge-2": {
          "r": 0.02032520325203252,
          "p": 0.046296296296296294,
          "f": 0.028248583330461232
        },
        "rouge-l": {
          "r": 0.13872832369942195,
          "p": 0.3157894736842105,
          "f": 0.19277108009612756
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/TR/2411.13564v1",
      "true_abstract": "According to The Exchange Act, 1934 unlawful insider trading is the abuse of\naccess to privileged corporate information. While a blurred line between\n\"routine\" the \"opportunistic\" insider trading exists, detection of strategies\nthat insiders mold to maneuver fair market prices to their advantage is an\nuphill battle for hand-engineered approaches. In the context of detailed\nhigh-dimensional financial and trade data that are structurally built by\nmultiple covariates, in this study, we explore, implement and provide detailed\ncomparison to the existing study (Deng et al. (2019)) and independently\nimplement automated end-to-end state-of-art methods by integrating principal\ncomponent analysis to the random forest (PCA-RF) followed by a standalone\nrandom forest (RF) with 320 and 3984 randomly selected, semi-manually labeled\nand normalized transactions from multiple industry. The settings successfully\nuncover latent structures and detect unlawful insider trading. Among the\nmultiple scenarios, our best-performing model accurately classified 96.43\npercent of transactions. Among all transactions the models find 95.47 lawful as\nlawful and $98.00$ unlawful as unlawful percent. Besides, the model makes very\nfew mistakes in classifying lawful as unlawful by missing only 2.00 percent. In\naddition to the classification task, model generated Gini Impurity based\nfeatures ranking, our analysis show ownership and governance related features\nbased on permutation values play important roles. In summary, a simple yet\npowerful automated end-to-end method relieves labor-intensive activities to\nredirect resources to enhance rule-making and tracking the uncaptured unlawful\ninsider trading transactions. We emphasize that developed financial and trading\nfeatures are capable of uncovering fraudulent behaviors.",
      "generated_abstract": "uce a novel framework for risk-aware trading with a novel risk\nmeasure, the conditional mean of transaction costs, which we call the\nConditional Mean of Transaction Costs (CMTC). We show that the CMTC is a\nconvex risk measure that is invariant to rebalancing, and is thus well-suited\nfor risk-averse trading. We demonstrate that the CMTC outperforms the existing\nrisk measures in risk-averse trading, and also shows good performance in\nrisk-neutral trading. We further show that the CMTC is a good risk measure in\nthe high-frequency trading context. We also show that the CMTC can be\ncomputationally efficient in a wide range of problems, from portfolio\noptimization to reinforcement learning to hedging. Finally, we show that the\nCMTC is equivalent to the conditional variance of transaction costs,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09941520467836257,
          "p": 0.25,
          "f": 0.14225941015458424
        },
        "rouge-2": {
          "r": 0.008368200836820083,
          "p": 0.01904761904761905,
          "f": 0.01162790273543286
        },
        "rouge-l": {
          "r": 0.0935672514619883,
          "p": 0.23529411764705882,
          "f": 0.13389120931776416
        }
      }
    },
    {
      "paper_id": "cs.FL.cs/LO/2503.04525v1",
      "true_abstract": "We give an active learning algorithm for deterministic one-counter automata\n(DOCAs) where the learner can ask the teacher membership and minimal\nequivalence queries. The algorithm called OL* learns a DOCA in time polynomial\nin the size of the smallest DOCA, recognising the target language.\n  All existing algorithms for learning DOCAs, even for the subclasses of\ndeterministic real-time one-counter automata (DROCAs) and visibly one-counter\nautomata (VOCAs), in the worst case, run in exponential time with respect to\nthe size of the DOCA under learning. Furthermore, previous learning algorithms\nare ``grey-box'' algorithms relying on an additional query type - counter value\nquery - where the teacher returns the counter value reached on reading a given\nword. In contrast, our algorithm is a ``black-box'' algorithm.\n  It is known that the minimisation of VOCAs is NP-hard. However, OL* can be\nused for approximate minimisation of DOCAs. In this case, the output size is at\nmost polynomial in the size of a minimal DOCA.",
      "generated_abstract": "t a novel method for constructing a set of subformulas that is\nsublinear in the number of formulas in a formula language, and polynomial in the\nnumber of clauses in the language. Our method uses a sequence of rules that\ntransform a given formula into a new one that is either a subset of the original\nformula, or else contains it as a subformula. The rules are evaluated in\npolynomial time in the number of clauses. We prove that the number of rules in\nour construction is polynomial in the number of formulas, and in the size of\nthe formula language. We also provide a polynomial-time algorithm for\nconstructing a subformula that is either a subset of the original formula, or\ncontains it as a subformula, given a formula and a subformula. Our results\nprovide a foundation for the design of efficient algorithms for constructing\nsubsets of formulas, or subsets of subformulas,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20430107526881722,
          "p": 0.30158730158730157,
          "f": 0.24358973877465492
        },
        "rouge-2": {
          "r": 0.06944444444444445,
          "p": 0.08928571428571429,
          "f": 0.07812499507812533
        },
        "rouge-l": {
          "r": 0.1935483870967742,
          "p": 0.2857142857142857,
          "f": 0.23076922595414212
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.08100v1",
      "true_abstract": "We study contests in which two groups compete to win (or not to win) a\ngroup-specific public-good/bad prize. Each player in the groups can exert two\ntypes of effort: one to help her own group win the prize, and one to sabotage\nher own group's chances of winning it. The players in the groups choose their\neffort levels simultaneously and independently. We introduce a specific form of\ncontest success function that determines each group's probability of winning\nthe prize, taking into account players' sabotage activities. We show that two\ntypes of purestrategy Nash equilibrium occur, depending on parameter values:\none without sabotage activities and one with sabotage activities. In the first\ntype, only the highest-valuation player in each group expends positive effort,\nwhereas, in the second type, only the lowest-valuation player in each group\nexpends positive effort.",
      "generated_abstract": "uce a model of game theory between a firm and a number of buyers\nthat each buy one unit of a product. The firm and the buyers are all\nindependent, and the buyers' utility functions depend only on the firm's\nutility. We derive the firm's optimal demand function, and the optimal\ndemand-and-price strategy of the firm and the buyers. We also derive the\noptimal demand function, the optimal price, and the optimal buyers'\nutility-of-bidding strategy, and show that the firm's demand function\ncorresponds to the utility of bidding. We also derive the price and the\noptimal buyers' strategy that minimizes the firm's expected loss. We show\nthat the optimal buyers' strategy maximizes the firm's expected profit. We\nalso derive the firm's and the buyers' expected profits and losses, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17073170731707318,
          "p": 0.2641509433962264,
          "f": 0.20740740263813456
        },
        "rouge-2": {
          "r": 0.017241379310344827,
          "p": 0.021505376344086023,
          "f": 0.019138751041415162
        },
        "rouge-l": {
          "r": 0.14634146341463414,
          "p": 0.22641509433962265,
          "f": 0.17777777300850492
        }
      }
    },
    {
      "paper_id": "cs.LG.q-fin/CP/2412.01062v1",
      "true_abstract": "High-frequency trading (HFT) represents a pivotal and intensely competitive\ndomain within the financial markets. The velocity and accuracy of data\nprocessing exert a direct influence on profitability, underscoring the\nsignificance of this field. The objective of this work is to optimise the\nreal-time processing of data in high-frequency trading algorithms. The dynamic\nfeature selection mechanism is responsible for monitoring and analysing market\ndata in real time through clustering and feature weight analysis, with the\nobjective of automatically selecting the most relevant features. This process\nemploys an adaptive feature extraction method, which enables the system to\nrespond and adjust its feature set in a timely manner when the data input\nchanges, thus ensuring the efficient utilisation of data. The lightweight\nneural networks are designed in a modular fashion, comprising fast\nconvolutional layers and pruning techniques that facilitate the expeditious\ncompletion of data processing and output prediction. In contrast to\nconventional deep learning models, the neural network architecture has been\nspecifically designed to minimise the number of parameters and computational\ncomplexity, thereby markedly reducing the inference time. The experimental\nresults demonstrate that the model is capable of maintaining consistent\nperformance in the context of varying market conditions, thereby illustrating\nits advantages in terms of processing speed and revenue enhancement.",
      "generated_abstract": "We introduce a novel framework for generating synthetic equity portfolios,\nusing a stochastic gradient descent algorithm that integrates the\nMarkov-chain Monte Carlo (MCMC) method with the Gaussian process (GP)\nregression. The stochastic gradient descent algorithm is used to optimize\nGaussian process priors over a series of synthetic portfolio distributions\ngenerated by the GP regression. Our results show that the synthetic portfolios\ngenerated by the algorithm outperform benchmarks and other synthetic portfolio\ngeneration methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08695652173913043,
          "p": 0.23076923076923078,
          "f": 0.12631578549806108
        },
        "rouge-2": {
          "r": 0.010101010101010102,
          "p": 0.03076923076923077,
          "f": 0.015209121753965837
        },
        "rouge-l": {
          "r": 0.07246376811594203,
          "p": 0.19230769230769232,
          "f": 0.10526315391911373
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2502.19839v1",
      "true_abstract": "We consider the problem of estimating complex statistical latent variable\nmodels using variational Bayes methods. These methods are used when exact\nposterior inference is either infeasible or computationally expensive, and they\napproximate the posterior density with a family of tractable distributions. The\nparameters of the approximating distribution are estimated using optimisation\nmethods. This article develops a flexible Gaussian mixture variational\napproximation, where we impose sparsity in the precision matrix of each\nGaussian component to reflect the appropriate conditional independence\nstructure in the model. By introducing sparsity in the precision matrix and\nparameterising it using the Cholesky factor, each Gaussian mixture component\nbecomes parsimonious (with a reduced number of non-zero parameters), while\nstill capturing the dependence in the posterior distribution. Fast estimation\nmethods based on global and local variational boosting moves combined with\nnatural gradients and variance reduction methods are developed. The local\nboosting moves adjust an existing mixture component, and optimisation is only\ncarried out on a subset of the variational parameters of a new component. The\nsubset is chosen to target improvement of the current approximation in aspects\nwhere it is poor. The local boosting moves are fast because only a small number\nof variational parameters need to be optimised. The efficacy of the approach is\nillustrated by using simulated and real datasets to estimate generalised linear\nmixed models and state space models.",
      "generated_abstract": "The generalized linear mixed model (GLMM) is a popular model for\ngeneralized linear and additive nonlinear models, yet it is often challenging to\nevaluate its performance in practice. To address this, we propose a novel\nprocedure to evaluate the performance of GLMMs in simulation studies. Our\nprocedure combines simulation studies with a robust calibration procedure that\nassesses the calibration of the model. We demonstrate the utility of our\nprocedure through a simulation study that evaluates the performance of two\nwidely used GLMMs, the generalized additive model (GAM) and the generalized\nadditive mixed model (GAMM). We further apply our procedure to evaluate the\nperformance of a novel model, the generalized additive mixed linear model (GAMLM)\nthat integrates the GAM and GAMM models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12878787878787878,
          "p": 0.265625,
          "f": 0.17346938335693474
        },
        "rouge-2": {
          "r": 0.029556650246305417,
          "p": 0.058823529411764705,
          "f": 0.03934425784337594
        },
        "rouge-l": {
          "r": 0.12121212121212122,
          "p": 0.25,
          "f": 0.16326530172428166
        }
      }
    },
    {
      "paper_id": "astro-ph.IM.astro-ph/CO/2503.09842v1",
      "true_abstract": "Radiowave Observations on the Lunar Surface of the photo-Electron Sheath\ninstrument (ROLSES- 1) onboard the Intuitive Machines' Odysseus lunar lander\nrepresents NASA's first radio telescope on the Moon, and the first United\nStates spacecraft landing on the lunar surface in five decades. Despite a host\nof challenges, ROLSES-1 managed to collect a small amount of data over\nfractions of one day during cruise phase and two days on the lunar surface with\nfour monopole stacer antennas that were in a non-ideal deployment. All antennas\nrecorded shortwave radio transmissions breaking through the Earth's ionosphere\n-- or terrestrial technosignatures -- from spectral and raw waveform data.\nThese technosignatures appear to be modulated by density fluctuations in the\nEarth's ionosphere and could be used as markers when searching for\nextraterrestrial intelligence from habitable exoplanets. After data reduction\nand marshaling a host of statistical and sampling techniques, five minutes of\nraw waveforms from the least noisy antenna were used to generate covariances\nconstraining both the antenna parameters and the amplitude of the low-frequency\nisotropic galactic spectrum. ROLSES- 2 and LuSEE-Night, both lunar radio\ntelescopes launching later in the decade, will have significant upgrades from\nROLSES-1 and will be set to take unprecedented measurements of the\nlow-frequency sky, lunar surface, and constrain the cosmological 21-cm signal.\nROLSES-1 represents a trailblazer for lunar radio telescopes, and many of the\nstatistical tools and data reduction techniques presented in this work will be\ninvaluable for upcoming lunar radio telescope missions.",
      "generated_abstract": "t the first results of a large-scale, deep, and high-resolution\nspectroscopic survey of the Magellanic Clouds with the GMOS spectrograph on the\n2.5-m Magellan Clay telescope, which was carried out over two nights in July\n2025. Our survey covers 156 square degrees in the vicinity of the Magellanic\nClouds, with a spectral resolution of R ~ 2000. We find that our data sample is\ncomprised of 16,047 objects, including 15,821 stars and 223 galaxies. The\nspectroscopic data were obtained over two nights in July 2025, with an\nobservational time of 10 hours per night. We present the resulting spectra in\nfive sub-samples, including 8533 red clump stars,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10738255033557047,
          "p": 0.2,
          "f": 0.1397379867203144
        },
        "rouge-2": {
          "r": 0.02262443438914027,
          "p": 0.04950495049504951,
          "f": 0.031055896315536267
        },
        "rouge-l": {
          "r": 0.09395973154362416,
          "p": 0.175,
          "f": 0.12227073781201747
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/QM/2502.13398v1",
      "true_abstract": "Despite recent advancements, most computational methods for molecule\noptimization are constrained to single- or double-property optimization tasks\nand suffer from poor scalability and generalizability to novel optimization\ntasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable\nout-of-domain generalizability to novel tasks. To demonstrate LLMs' potential\nfor molecule optimization, we introduce $\\mathtt{MoMUInstruct}$, the first\nhigh-quality instruction-tuning dataset specifically focused on complex\nmulti-property molecule optimization tasks. Leveraging $\\mathtt{MoMUInstruct}$,\nwe develop $\\mathtt{GeLLM^3O}$s, a series of instruction-tuned LLMs for\nmolecule optimization. Extensive evaluations across 5 in-domain and 5\nout-of-domain tasks demonstrate that $\\mathtt{GeLLM^3O}$s consistently\noutperform state-of-the-art baselines. $\\mathtt{GeLLM^3O}$s also exhibit\noutstanding zero-shot generalization to unseen tasks, significantly\noutperforming powerful closed-source LLMs. Such strong generalizability\ndemonstrates the tremendous potential of $\\mathtt{GeLLM^3O}$s as foundational\nmodels for molecule optimization, thereby tackling novel optimization tasks\nwithout resource-intensive retraining. $\\mathtt{MoMUInstruct}$, models, and\ncode are accessible through https://github.com/ninglab/GeLLMO.",
      "generated_abstract": "em of identifying the underlying regulatory mechanism of a\ngene network is of great interest in both theoretical and applied sciences.\nHowever, the computational complexity of finding the optimal regulatory network\nis extremely high, which makes it difficult to obtain accurate results in\npractice. In this paper, we propose a novel method for identifying the\nunderlying regulatory mechanism of gene networks based on the Wasserstein\ndistance between the transition matrix of the gene network and the\nmultilayer-structured transition matrix of a network of mutual\ninterconnections. By using the multilayer-structured transition matrix, we\nobtain the optimal regulatory network and obtain the optimal regulatory\nmechanism by solving the linear programming problem. Our method is based on\nthe Wasserstein distance, which is an effective distance metric for\nprobability distributions. Theoretically, we prove that the optimal\nregulatory network and optimal regulatory mechanism are unique",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.16216216216216217,
          "f": 0.1411764656719725
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.16216216216216217,
          "f": 0.1411764656719725
        }
      }
    },
    {
      "paper_id": "math.LO.math/GN/2502.20887v1",
      "true_abstract": "We prove that it is consistent with ZFC that for every non-decreasing\nfunction $f:[0,1]\\to [0,1]$, each subset of $[0,1]$ of cardinality $\\mathfrak\nc$ contains a set of cardinality $\\mathfrak c$ on which $f$ is uniformly\ncontinuous. We show that this statement follows from the assumptions that\n$\\mathfrak d^* < \\mathfrak c$ and $\\mathfrak c$ is regular, where $\\mathfrak\nd^*\\leq \\mathfrak d$ is the smallest cardinality $\\kappa$ such that any two\ndisjoint countable dense sets in the Cantor set can be separated by sets each\nof which is an intersection of at most $\\kappa$-many open sets in the Cantor\nset. We establish also that $\\mathfrak d^*=\\min\\{\\mathfrak u, \\mathfrak\nd\\}=\\min\\{\\mathfrak r, \\mathfrak d\\}$, thus giving an alternative proof of the\nlatter equality established by J. Aubrey in 2004.",
      "generated_abstract": "We construct a new construction of the Birkhoff sums of the Laplacian on\nthe boundary of a compact Riemannian manifold. Our construction generalizes a\nwell-known construction of the Birkhoff sums of the Laplacian on the boundary\nof the sphere by J. G. J\\\"uttner. Our construction is based on the notion of\nthe ``fine structure'' of the boundary of the manifold and is based on the\nanalysis of the boundary homotopy of the manifolds in the category of\nHausdorff spaces. We obtain new formulas for the Birkhoff sums of the\nLaplacian and prove that they are equal to the Birkhoff sums of the\nLaplacian on the boundary of the sphere.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15853658536585366,
          "p": 0.28888888888888886,
          "f": 0.20472440487320984
        },
        "rouge-2": {
          "r": 0.034482758620689655,
          "p": 0.06060606060606061,
          "f": 0.04395603933341433
        },
        "rouge-l": {
          "r": 0.13414634146341464,
          "p": 0.24444444444444444,
          "f": 0.17322834188108388
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/PL/2503.02768v1",
      "true_abstract": "We develop a denotational model for programs that have standard programming\nconstructs such as conditionals and while-loops, as well as probabilistic and\nconcurrent commands. Whereas semantic models for languages with either\nconcurrency or randomization are well studied, their combination is limited to\nlanguages with bounded loops. Our work is the first to consider both\nrandomization and concurrency for a language with unbounded looping constructs.\nThe interaction between Boolean tests (arising from the control flow\nstructures), probabilistic actions, and concurrent execution creates challenges\nin generalizing previous work on pomsets and convex languages, prominent models\nfor those effects, individually. To illustrate the generality of our model, we\nshow that it recovers a typical powerdomain semantics for concurrency, as well\nas the convex powerset semantics for probabilistic nondeterminism.",
      "generated_abstract": "hat there are many natural and surprisingly rich classes of\nfinitely presented groups for which the group of automorphisms is isomorphic\nto the group of all permutations of a finite set. We show that these classes\ninclude all finitely presented groups which are presented by a regular\npolynomial in the generators, as well as some of the classes of groups which\nare presented by a polynomial in the generators and by a non-regular polynomial\nin the generators, and of groups which are presented by a polynomial in the\ngenerators and by a non-regular polynomial in the generators and by a regular\npolynomial in the generators. We also show that these classes include all\nfinitely presented groups which are presented by a regular polynomial in the\ngenerators and by a non-regular polynomial in the generators, and of groups\nwhich are presented by a polynomial in the generators and by a non-regular\npolynomial in the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15730337078651685,
          "p": 0.3333333333333333,
          "f": 0.21374045365887778
        },
        "rouge-2": {
          "r": 0.025423728813559324,
          "p": 0.046153846153846156,
          "f": 0.03278688066529373
        },
        "rouge-l": {
          "r": 0.15730337078651685,
          "p": 0.3333333333333333,
          "f": 0.21374045365887778
        }
      }
    },
    {
      "paper_id": "cs.LG.math/OC/2503.09903v1",
      "true_abstract": "The integration of machine learning (ML) has significantly enhanced the\ncapabilities of Earth Observation (EO) systems by enabling the extraction of\nactionable insights from complex datasets. However, the performance of\ndata-driven EO applications is heavily influenced by the data collection and\ntransmission processes, where limited satellite bandwidth and latency\nconstraints can hinder the full transmission of original data to the receivers.\nTo address this issue, adopting the concepts of Semantic Communication (SC)\noffers a promising solution by prioritizing the transmission of essential data\nsemantics over raw information. Implementing SC for EO systems requires a\nthorough understanding of the impact of data processing and communication\nchannel conditions on semantic loss at the processing center. This work\nproposes a novel data-fitting framework to empirically model the semantic loss\nusing real-world EO datasets and domain-specific insights. The framework\nquantifies two primary types of semantic loss: (1) source coding loss, assessed\nvia a data quality indicator measuring the impact of processing on raw source\ndata, and (2) transmission loss, evaluated by comparing practical transmission\nperformance against the Shannon limit. Semantic losses are estimated by\nevaluating the accuracy of EO applications using four task-oriented ML models,\nEfficientViT, MobileViT, ResNet50-DINO, and ResNet8-KD, on lossy image datasets\nunder varying channel conditions and compression ratios. These results underpin\na framework for efficient semantic-loss modeling in bandwidth-constrained EO\nscenarios, enabling more reliable and effective operations.",
      "generated_abstract": "the estimation of the maximum eigenvalue of a Hermitian matrix.\nHermitian matrices have a natural decomposition into a sum of two factors:\none factor is the covariance matrix, the other factor is the spectral\ndecomposition of the covariance matrix. We show that the maximum eigenvalue of\nsuch a matrix can be estimated using the sum of two techniques:\n  * The Singular Value Decomposition (SVD) of the covariance matrix.\n  * The block Singular Value Decomposition (bSVD) of the covariance matrix\n  combined with the Singular Value Decomposition (SVD) of the eigenvalues.\nThis decomposition provides a way to combine the advantages of the SVD and\nbSVD, with the advantage of the SVD to reduce the number of parameters to\nestimate and the advantage of the bSVD to allow for the presence of small\neigenvalues",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08783783783783784,
          "p": 0.20967741935483872,
          "f": 0.1238095196480727
        },
        "rouge-2": {
          "r": 0.0045662100456621,
          "p": 0.01,
          "f": 0.006269588172288998
        },
        "rouge-l": {
          "r": 0.06756756756756757,
          "p": 0.16129032258064516,
          "f": 0.09523809107664417
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/MM/2503.10078v1",
      "true_abstract": "Image Quality Assessment (IQA) based on human subjective preferences has\nundergone extensive research in the past decades. However, with the development\nof communication protocols, the visual data consumption volume of machines has\ngradually surpassed that of humans. For machines, the preference depends on\ndownstream tasks such as segmentation and detection, rather than visual appeal.\nConsidering the huge gap between human and machine visual systems, this paper\nproposes the topic: Image Quality Assessment for Machine Vision for the first\ntime. Specifically, we (1) defined the subjective preferences of machines,\nincluding downstream tasks, test models, and evaluation metrics; (2)\nestablished the Machine Preference Database (MPD), which contains 2.25M\nfine-grained annotations and 30k reference/distorted image pair instances; (3)\nverified the performance of mainstream IQA algorithms on MPD. Experiments show\nthat current IQA metrics are human-centric and cannot accurately characterize\nmachine preferences. We sincerely hope that MPD can promote the evolution of\nIQA from human to machine preferences. Project page is on:\nhttps://github.com/lcysyzxdxc/MPD.",
      "generated_abstract": "ting is a challenging task that requires synthesizing high-quality\nimages by manipulating existing images. A key challenge is how to effectively\ncapture the style of an image while preserving the integrity of its content.\nGenerative Adversarial Networks (GANs) have demonstrated success in image\nediting tasks, but their limitations in modeling content often hinder their\neffectiveness. To address this, we introduce Diffusion-Style-Editing (DSE), a\nnew paradigm that leverages diffusion models to generate high-quality images\nwithout requiring content information. Our approach is based on a diffusion\nmodel, which iteratively generates a sequence of latent vectors to produce\nimages. We propose a novel approach for image editing using the diffusion\nmodel, where we first train a GAN to synthesize high-quality images. Then, we\ntrain a diffusion model to generate images that are similar to the high-quality",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13675213675213677,
          "p": 0.1797752808988764,
          "f": 0.1553398009176173
        },
        "rouge-2": {
          "r": 0.00641025641025641,
          "p": 0.008,
          "f": 0.007117432783276182
        },
        "rouge-l": {
          "r": 0.13675213675213677,
          "p": 0.1797752808988764,
          "f": 0.1553398009176173
        }
      }
    },
    {
      "paper_id": "math.GM.math/GM/2502.14876v4",
      "true_abstract": "Series involving hypergeometric functions are used to derive, extend and\nevaluate integrals involving the product of two Bessel functions of the first\nkind $J_{u}(a z)$ $J_{v}(b z)$ with order $u,v$, studied by Landau et al. The\nmethod used in this work is contour integration.",
      "generated_abstract": "In this paper, we study a nonlinear Schr\\\"odinger equation in a torus with\nthe Dirichlet boundary condition and a nonlinear Schr\\\"odinger equation in a\ntorus with the Neumann boundary condition. In both cases, we show that the\nsolutions of the two equations possess a natural symmetry. In the case of the\nDirichlet boundary condition, we prove that the solutions are characterized by\nthe coefficients in a suitable way. In the case of the Neumann boundary\ncondition, we show that the solutions are characterized by a certain function\nin a suitable way.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23684210526315788,
          "p": 0.225,
          "f": 0.23076922577251818
        },
        "rouge-2": {
          "r": 0.023255813953488372,
          "p": 0.017543859649122806,
          "f": 0.019999995098001197
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.15,
          "f": 0.15384614884944134
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.10208v1",
      "true_abstract": "We explore the Jordan-Chevalley decomposition problem for an operator field\nin small dimensions. In dimensions three and four, we find tensorial conditions\nfor an operator field $L$, similar to a nilpotent Jordan block, to possess\nlocal coordinates in which $L$ takes a strictly upper triangular form. We prove\nthe Tempesta-Tondo conjecture for higher order brackets of\nFr\\\"olicher-Nijenhuis type.",
      "generated_abstract": "uct a topological realization of the theory of $K$-theory of\nalgebraic groups over a field $k$ using the homotopy category of a\n$k$-linear triangulated category. The construction is based on a triangulated\ncategory $T$ which is a $k$-linear analog of the symmetric monoidal category\nof modules over a dg $k$-algebra $A$ and a $k$-linear analog of the\ntriangulated category of $A$-modules. The dg $k$-algebra $A$ is the universal\nalgebra over a field $k$ that is equipped with a $k$-linear triangulated\ncategory $T$. The construction of $T$ is obtained by an application of the\nBorel--Moore homology of $T$ to the Borel--Moore homology of $A$ over $k$.\nFurthermore, the homotopy category of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1702127659574468,
          "p": 0.1702127659574468,
          "f": 0.17021276095744697
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14893617021276595,
          "p": 0.14893617021276595,
          "f": 0.14893616521276612
        }
      }
    },
    {
      "paper_id": "cs.CL.econ/GN/2412.04505v2",
      "true_abstract": "Accurately interpreting words is vital in political science text analysis;\nsome tasks require assuming semantic stability, while others aim to trace\nsemantic shifts. Traditional static embeddings, like Word2Vec effectively\ncapture long-term semantic changes but often lack stability in short-term\ncontexts due to embedding fluctuations caused by unbalanced training data.\nBERT, which features transformer-based architecture and contextual embeddings,\noffers greater semantic consistency, making it suitable for analyses in which\nstability is crucial. This study compares Word2Vec and BERT using 20 years of\nPeople's Daily articles to evaluate their performance in semantic\nrepresentations across different timeframes. The results indicate that BERT\noutperforms Word2Vec in maintaining semantic stability and still recognizes\nsubtle semantic variations. These findings support BERT's use in text analysis\ntasks that require stability, where semantic changes are not assumed, offering\na more reliable foundation than static alternatives.",
      "generated_abstract": "r explores how a digital transformation in the financial services\nmarketplace could create an ecosystem that enables the development of\nadvanced, automated financial decision-making (ADM) tools. Specifically, we\nexplore how machine learning (ML) models could be used to develop more\nefficient credit scoring models, as well as the role of blockchain technology in\nenhancing the security and integrity of financial transactions. We consider a\nscenario in which a bank uses a combination of ML models and blockchain to\nidentify high-risk borrowers and ensure that the loan is repaid on time. This\npaper explores the implications of implementing ML models in the banking\nsector, and how blockchain could enhance the security and integrity of financial\ntransactions. The results show that ML models can enhance the accuracy of loan\napprovals, while also reducing the cost of loan servicing. Additionally,\nblockchain",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12264150943396226,
          "p": 0.1511627906976744,
          "f": 0.1354166617209203
        },
        "rouge-2": {
          "r": 0.014814814814814815,
          "p": 0.015873015873015872,
          "f": 0.015325665504031222
        },
        "rouge-l": {
          "r": 0.12264150943396226,
          "p": 0.1511627906976744,
          "f": 0.1354166617209203
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.06878v1",
      "true_abstract": "Let $G$ be a finite group and $p$ be a prime. We prove that if $G$ has three\ncodegrees, then $G$ is an $M$-group. We prove for some prime $p$ that if every\nirreducible Brauer character of $G$ is a prime, then for every normal subgroup\n$N$ of $G$ either $G/N$ or $N$ is an $M_p$-group.",
      "generated_abstract": "We study the asymptotic behavior of the generalized Ginzburg-Landau\nparameters for the non-linear Schrodinger equation, for both the KdV and the\nYang-Mills-Higgs equations. In the KdV case, we find a simple formula for the\nasymptotic behavior of the parameter $C$ for the special case $m=0$, when\n$g=\\mu=1$. In the Yang-Mills-Higgs case, we find an explicit formula for the\nasymptotic behavior of the parameter $\\alpha$, when $g=0$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17142857142857143,
          "p": 0.16666666666666666,
          "f": 0.16901407950803427
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.17142857142857143,
          "p": 0.16666666666666666,
          "f": 0.16901407950803427
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/AP/2503.01566v1",
      "true_abstract": "Extreme response assessment is important in the design and operation of\nengineering structures, and is a crucial part of structural risk and\nreliability analyses. Structures should be designed in a way that enables them\nto withstand the environmental loads they are expected to experience over their\nlifetime, without designs being unnecessarily conservative and costly. An\naccurate risk estimate is essential but difficult to obtain because the\nlong-term behaviour of a structure is typically too complex to calculate\nanalytically or with brute force Monte Carlo simulation. Therefore,\napproximation methods are required to estimate the extreme response using only\na limited number of short-term conditional response calculations. Combining\nsurrogate models with Design of Experiments is an approximation approach that\nhas gained popularity due to its ability to account for both long-term\nenvironment variability and short-term response variability. In this paper, we\npropose a method for estimating the extreme response of black-box, stochastic\nmodels with heteroscedastic non-Gaussian noise. We present a mathematically\nfounded extreme response estimation process that enables Design of Experiment\napproaches that are prohibitively expensive with surrogate Monte Carlo. The\ntheory leads us to speculate this method can robustly produce more confident\nextreme response estimates, and is suitable for a variety of domains. While\nthis needs to be further validated empirically, the method offers a promising\ntool for reducing the uncertainty decision-makers face, allowing them to make\nbetter informed choices and create more optimal structures.",
      "generated_abstract": "pproach for the identification of the global structural parameters of\nrecently acquired genome-wide association studies (GWAS) is presented. This\nmethod is based on the identification of the global structural parameters of a\nGWAS using the method of principal component analysis (PCA) with an\nalternative score, the global structural parameters score (GSPS). The GSPS\nprovides a global summary of the global structural parameters of a GWAS. The\nGSPS can be used to select the number of global structural parameters to be\ninvestigated. The GSPS can be computed in parallel for each sample and can be\nused as a basis for the selection of the number of global structural parameters\nto be investigated. The GSPS is also used to select the number of genetic\nalleles to be investigated and the number of genetic loci to be investigated.\nThe",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11038961038961038,
          "p": 0.2982456140350877,
          "f": 0.16113743681498627
        },
        "rouge-2": {
          "r": 0.017937219730941704,
          "p": 0.047058823529411764,
          "f": 0.025974021977779333
        },
        "rouge-l": {
          "r": 0.1038961038961039,
          "p": 0.2807017543859649,
          "f": 0.1516587638292043
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2412.05323v1",
      "true_abstract": "In application-specific designs, owing to the trade-off between power\nconsumption and speed, optimization of various circuit parameters has become a\nchallenging task. Several of the performance metrics, viz. energy efficiency,\ngain, performance, and noise immunity, are interrelated and difficult to tune.\nSuch efforts may result in a great deal of manual iterations which in turn\nincrease the computational overhead. Thus, it is important to develop a\nmethodology that not only explores large design space but also reduces the\ncomputational time. In this work, we investigate the viability of using a SPICE\nand Python IDE (PIDE) interface to optimize integrated circuits. The SPICE\nsimulations are carried out using 22 nm technology node with a nominal supply\nvoltage of 0.8 V. The SPICE-PIDE optimizer, as delineated in this work, is able\nto provide the best solution sets considering various performance metrics and\ndesign complexities for 5 transistor level converters.",
      "generated_abstract": "aper, we present a novel approach for building large language models\n(LLMs) that are able to effectively generate diverse and complex natural\nlanguage (NL) while preserving the integrity of the original text. This is\nachieved through a novel architecture that combines a self-attention mechanism\nwith a multi-headed attention mechanism, which we call the Dual Multi-Headed\nAttention (DMHA). We also propose a novel dataset called Diversity Dataset\n(DDI) for generating diverse and complex NL. The DDI dataset consists of\nsynthetic and real-world samples that contain a diverse range of information\nsuch as emojis, punctuation, abbreviations, and puns. Our experiments\ndemonstrate that the proposed model can generate diverse and complex NL while\npreserving the original text, and it is also able to generate high-quality text\nwithout the need for fine-t",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1592920353982301,
          "p": 0.21176470588235294,
          "f": 0.18181817691817176
        },
        "rouge-2": {
          "r": 0.027586206896551724,
          "p": 0.034782608695652174,
          "f": 0.030769225835799607
        },
        "rouge-l": {
          "r": 0.1592920353982301,
          "p": 0.21176470588235294,
          "f": 0.18181817691817176
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.05098v1",
      "true_abstract": "Information-directed sampling (IDS) is a powerful framework for solving\nbandit problems which has shown strong results in both Bayesian and frequentist\nsettings. However, frequentist IDS, like many other bandit algorithms, requires\nthat one have prior knowledge of a (relatively) tight upper bound on the norm\nof the true parameter vector governing the reward model in order to achieve\ngood performance. Unfortunately, this requirement is rarely satisfied in\npractice. As we demonstrate, using a poorly calibrated bound can lead to\nsignificant regret accumulation. To address this issue, we introduce a novel\nfrequentist IDS algorithm that iteratively refines a high-probability upper\nbound on the true parameter norm using accumulating data. We focus on the\nlinear bandit setting with heteroskedastic subgaussian noise. Our method\nleverages a mixture of relevant information gain criteria to balance\nexploration aimed at tightening the estimated parameter norm bound and directly\nsearching for the optimal action. We establish regret bounds for our algorithm\nthat do not depend on an initially assumed parameter norm bound and demonstrate\nthat our method outperforms state-of-the-art IDS and UCB algorithms.",
      "generated_abstract": "We consider the problem of recovering the latent representations of a\ndataset from a small number of data samples. Our goal is to recover the\nrepresentations without access to the full dataset. We focus on the case where\nthe latent space is a high-dimensional Euclidean space, where we propose a\nstatistical learning approach based on the Gaussian process (GP) regression\nframework. Our approach is based on a novel variational GP that combines the\nrepresentation learning from the latent space with the GP regression framework.\nWe provide theoretical guarantees on the performance of the proposed method.\nFinally, we implement our method and provide a simulation study to evaluate\nthe performance of our method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1557377049180328,
          "p": 0.3064516129032258,
          "f": 0.2065217346620984
        },
        "rouge-2": {
          "r": 0.04878048780487805,
          "p": 0.07920792079207921,
          "f": 0.06037735377315808
        },
        "rouge-l": {
          "r": 0.1557377049180328,
          "p": 0.3064516129032258,
          "f": 0.2065217346620984
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2409.16613v2",
      "true_abstract": "Oral exams are a powerful tool for educators to gauge student's learning.\nThis is particularly important in introductory statistics classes where many\nstudents struggle to grasp a deep meaning of topics like $p$-values, confidence\nintervals, hypothesis testing, and more. These challenges are only heightened\nin a context where students are learning in a second language. In this paper, I\nshare my experience administering oral exams to an introductory statistics\nclass of non-native English speakers at a Japanese university. I explain the\ncontext of the university and course that the exam was given in, before sharing\ndetails about the two exams. Despite the challenges the students (and I myself)\nfaced, the exams seemed to truly test their statistical knowledge and not\nmerely their English proficiency, as I found little relationship between a\nstudent's English ability and performance. I close with encouragements and\nrecommendations for practitioners hoping to implement these exams, all while\nkeeping an eye towards the unique difficulties faced by students not learning\nin their mother tongue.",
      "generated_abstract": "In this paper, we propose a new method for the estimation of the covariance\nsparse matrix. This method is based on the block-sparsity of the covariance\nmatrix, which is a natural assumption in many applications. We derive the\nasymptotic normality of the proposed estimator and discuss its convergence\nproperties. Numerical simulations demonstrate the effectiveness of the proposed\nmethod. The proposed estimator can be easily implemented in R and MATLAB.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10526315789473684,
          "p": 0.24489795918367346,
          "f": 0.14723925959878065
        },
        "rouge-2": {
          "r": 0.018404907975460124,
          "p": 0.04838709677419355,
          "f": 0.02666666267417344
        },
        "rouge-l": {
          "r": 0.10526315789473684,
          "p": 0.24489795918367346,
          "f": 0.14723925959878065
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.01760v1",
      "true_abstract": "Adding a capacity constraint to a hidden-action principal-agent problem\nresults in the same set of Pareto optimal contracts as the unconstrained\nproblem where output is scaled down by a constant factor. This scaling factor\nis increasing in the agent's capacity to exert effort.",
      "generated_abstract": "We explore a framework that combines a structural model of market power with\nan analytically tractable model of consumer preferences. This framework\nincorporates the well-known trade-off between consumer welfare and market\npower, and allows for a flexible way to parameterize the consumer preferences\nand the market power. We find that the model with the strongest trade-off\nbetween welfare and market power is the one that minimizes the market power\nwhile preserving the same welfare level. Furthermore, this model is\nillustrated with an application to the case of a government subsidizing the\npurchases of a specific product in the market. We show that this approach\nproduces a market that is more efficient than a market in which the government\ndoes not subsidize the purchase of the product.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24242424242424243,
          "p": 0.12698412698412698,
          "f": 0.16666666215494802
        },
        "rouge-2": {
          "r": 0.04878048780487805,
          "p": 0.017857142857142856,
          "f": 0.02614378692639642
        },
        "rouge-l": {
          "r": 0.21212121212121213,
          "p": 0.1111111111111111,
          "f": 0.14583332882161473
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.cond-mat/dis-nn/2503.09389v2",
      "true_abstract": "We study canonical-equilibrium properties of Random Field $O(n)$ Models\ninvolving classical continuous vector spins of $n$ components with mean-field\ninteractions and subject to disordered fields acting on individual spins. To\nthis end, we employ two complementary approaches: the mean-field approximation,\nvalid for any disorder distribution, and the replica trick, applicable when the\ndisordered fields are sampled from a Gaussian distribution. On the basis of an\nexact analysis, we demonstrate that when replica symmetry holds, both the\napproaches yield identical expression for the free energy per spin of the\nsystem. As consequences, we study the case of $n=2$ ($XY$ spins) and that of\n$n=3$ (Heisenberg spins) for two representative choices of the disorder\ndistribution, namely, a Gaussian and a symmetric bimodal distribution. For both\n$n=2$ and $n=3$, we demonstrate that while the magnetization exhibits a\ncontinuous phase transition as a function of temperature for the Gaussian case,\nthe transition could be either continuous or first-order with an emergent\ntricriticality when the disorder distribution is bimodal. We also discuss in\nthe context of our models the issue of self-averaging of extensive variables\nnear the critical point of a continuous phase transition.",
      "generated_abstract": "aper, we propose a new quantum-based architecture for neural\nnetworks, the Quantum Neural Network (QNN), which is based on a quantum\ncircuit that can simulate the dynamics of a quantum system with a classical\nneural network. The proposed architecture can learn arbitrary quantum states\nwithout requiring any specific initial state. We show that the QNN is able to\nlearn complex, non-Markovian quantum dynamics, which are not accessible to\ntraditional neural networks. We also propose a general scheme for constructing\nthe quantum circuit that can simulate the dynamics of a quantum system. We\ndemonstrate that the proposed architecture can learn the dynamics of a\nsingle-qubit Heisenberg model, as well as the dynamics of the spin-1/2 model\nwith nearest-neighbor interactions, which is a non-trivial task in\nstate-of-the-art neural networks. Furthermore, we show that the Q",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16379310344827586,
          "p": 0.25675675675675674,
          "f": 0.19999999524432147
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.037383177570093455,
          "f": 0.028268546533981725
        },
        "rouge-l": {
          "r": 0.13793103448275862,
          "p": 0.21621621621621623,
          "f": 0.16842104787590043
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.q-bio/TO/2412.15774v1",
      "true_abstract": "A key process during animal morphogenesis is oriented tissue deformation,\nwhich is often driven by internally generated active stresses. Yet, such active\noriented materials are prone to well-known instabilities, raising the question\nof how oriented tissue deformation can be robust during morphogenesis. In a\nsimple scenario, we recently showed that active oriented deformation can be\nstabilized by the boundary-imposed gradient of a scalar field, which\nrepresents, e.g., a morphogen gradient in a developing embryo. Here, we discuss\na more realistic scenario, where the morphogen is produced by a localized\nsource region, diffuses across the tissue, and degrades. Consistent with our\nearlier results, we find that oriented tissue deformation is stable in the\ngradient-extensile case, i.e. when active stresses act to extend the tissue\nalong the direction of the gradient, but it is unstable in the\ngradient-contractile case. In addition, we now show that gradient-contractile\ntissues can not be stabilized even by morphogen diffusion. Finally, we point\nout the existence of an additional instability, which results from the\ninterplay of tissue shear and morphogen diffusion. Our theoretical results\nexplain the lack of gradient-contractile tissues in the biological literature,\nsuggesting that the active matter instability acts as an evolutionary selection\ncriterion.",
      "generated_abstract": "very of the role of the actin cytoskeleton in cellular motility\nwas one of the most revolutionary events in the history of biology. This\ndevelopment was initially based on the observation that the actin network\ncontributes to the mechanical properties of the cell. This paper provides an\nintroduction to the theory of the actin network, which is the main force\nresponsible for cellular motility. The theory of the actin network is\nformulated in the framework of the statistical mechanics of disordered systems,\nand it is based on the concept of a dynamical system. This concept is\nintroduced, and a simple example of a dynamical system is presented.\nSubsequently, the concept of a dynamical system is generalized, and the theory\nof the actin network is formulated in the framework of a stochastic\ndynamical system. This is done by defining a stochastic variable as the\ndis",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11382113821138211,
          "p": 0.208955223880597,
          "f": 0.14736841648698074
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.06796116504854369,
          "f": 0.04794520091316425
        },
        "rouge-l": {
          "r": 0.10569105691056911,
          "p": 0.19402985074626866,
          "f": 0.13684210069750707
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/PM/2410.03552v1",
      "true_abstract": "The growth of the tech startup ecosystem in Latin America (LATAM) is driven\nby innovative entrepreneurs addressing market needs across various sectors.\nHowever, these startups encounter unique challenges and risks that require\nspecific management approaches. This paper explores a case study with the Total\nAddressable Market (TAM), Serviceable Available Market (SAM), and Serviceable\nObtainable Market (SOM) metrics within the context of the online food delivery\nindustry in LATAM, serving as a model for valuing startups using the Discounted\nCash Flow (DCF) method. By analyzing key emerging powers such as Argentina,\nColombia, Uruguay, Costa Rica, Panama, and Ecuador, the study highlights the\npotential and profitability of AI-driven startups in the region through the\ndevelopment of a ranking of emerging powers in Latin America for tech startup\ninvestment. The paper also examines the political, economic, and competitive\nrisks faced by startups and offers strategic insights on mitigating these risks\nto maximize investment returns. Furthermore, the research underscores the value\nof diversifying investment portfolios with startups in emerging markets,\nemphasizing the opportunities for substantial growth and returns despite\ninherent risks.",
      "generated_abstract": "e a novel framework for risk management in portfolio construction\nusing a stochastic volatility model with market impact. This framework\naddresses two key challenges in traditional portfolio construction: (1) the\nuncertainty in the volatility process, which is typically modeled through a\nGaussian process, and (2) the lack of information about the market impact\nmodel, which is usually estimated from historical data. The framework introduces\na risk-neutral measure that incorporates the uncertainty in both the volatility\nand market impact parameters. By introducing a new measure of risk, we\nestablish a link between the volatility and market impact parameters. This link\nallows us to adjust the market impact parameters during the portfolio\nconstruction process, thereby ensuring that the portfolio is optimized for\nrisk-neutral risk. The framework is then used to construct portfolios for\ndifferent risk profiles,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17796610169491525,
          "p": 0.27631578947368424,
          "f": 0.21649484059517496
        },
        "rouge-2": {
          "r": 0.0058823529411764705,
          "p": 0.008620689655172414,
          "f": 0.006993002171258643
        },
        "rouge-l": {
          "r": 0.16101694915254236,
          "p": 0.25,
          "f": 0.195876283894144
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PM/2410.20597v1",
      "true_abstract": "We investigate the effectiveness of a momentum trading signal based on the\ncoverage network of financial analysts. This signal builds on the key\ninformation-brokerage role financial sell-side analysts play in modern stock\nmarkets. The baskets of stocks covered by each analyst can be used to construct\na network between firms whose edge weights represent the number of analysts\njointly covering both firms. Although the link between financial analysts\ncoverage and co-movement of firms' stock prices has been investigated in the\nliterature, little effort has been made to systematically learn the most\neffective combination of signals from firms covered jointly by analysts in\norder to benefit from any spillover effect. To fill this gap, we build a\ntrading strategy which leverages the analyst coverage network using a graph\nattention network. More specifically, our model learns to aggregate information\nfrom individual firm features and signals from neighbouring firms in a\nnode-level forecasting task. We develop a portfolio based on those predictions\nwhich we demonstrate to exhibit an annualized returns of 29.44% and a Sharpe\nratio of 4.06 substantially outperforming market baselines and existing graph\nmachine learning based frameworks. We further investigate the performance and\nrobustness of this strategy through extensive empirical analysis. Our paper\nrepresents one of the first attempts in using graph machine learning to extract\nactionable knowledge from the analyst coverage network for practical financial\napplications.",
      "generated_abstract": "We study the problem of allocating capital among various firms in a\nbanking system, where the capital of a firm is a function of its risk profile.\nWe assume that the system is characterized by a single risk measure, and we\nconsider the problem of allocating the firm's capital across the system's\nfirm. We derive a Nash equilibrium for the problem and show that it is\ncharacterized by the risk measure itself. We also derive the value of the\nrisk measure and the value of the system's capital. Finally, we study the\nefficiency of the system under the risk measure and show that the system's\nefficiency is a monotonic function of the risk measure. We also study the\nefficiency of the system with the system's capital as the risk measure.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07801418439716312,
          "p": 0.22,
          "f": 0.11518324220827295
        },
        "rouge-2": {
          "r": 0.018604651162790697,
          "p": 0.043478260869565216,
          "f": 0.026058627724432765
        },
        "rouge-l": {
          "r": 0.06382978723404255,
          "p": 0.18,
          "f": 0.0942408338313096
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/MF/2502.17906v2",
      "true_abstract": "In econophysics, there are several enigmatic empirical laws: (i)~the\nmarket-order flow has strong persistence (long-range order-sign correlation),\nwell formulated as the Lillo-Mike-Farmer model. This phenomenon seems\nparadoxical given the diffusive and unpredictable price dynamics; (ii)~the\nprice impact $I(Q)$ of a large metaorder $Q$ follows the square-root law,\n$I(Q)\\propto \\sqrt{Q}$. In this Letter, we propose an exactly solvable model of\nthe nonlinear price-impact dynamics that unifies these enigmas. We generalize\nthe Lillo-Mike-Farmer model to nonlinear price-impact dynamics, which is mapped\nto an exactly solvable L\\'evy-walk model. Our exact solution and numerical\nsimulations reveal three important points: First, the price dynamics remains\ndiffusive under the square-root law, even under the long-range correlation.\nSecond, price-movement statistics follows truncated power laws with typical\nexponent around three. Third, volatility has long memory. While this simple\nmodel lacks adjustable free parameters, it naturally aligns even with other\nenigmatic empirical laws, such as (iii)~the inverse-cubic law for price\nstatistics and (iv)~volatility clustering. This work illustrates the crucial\nrole of the square-root law in understanding rich and complex financial price\ndynamics from a single coherent viewpoint.",
      "generated_abstract": "r presents a novel approach to assess the risk of unhedged\nportfolios of financial derivatives. Our methodology combines the\nrepresentation of financial derivatives as stochastic processes with the\nheteroskedasticity-robust estimation of the risk of unhedged portfolios using\nthe Generalized Cram\\'er-Lundberg (GCL) model. The proposed approach is\nillustrated through a case study of a portfolio of 12 financial derivatives\nwith the use of high frequency data, including daily returns, volatilities and\ncorrelations. The results of the GCL model estimation show that the\nuncertainty of unhedged portfolios is more concentrated in the short-term\nperiods, while the uncertainty is more evenly distributed over the whole time\nhorizon. The use of the GCL model enhances the accuracy of the risk\nestimation and improves the performance of portfolio",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1015625,
          "p": 0.18840579710144928,
          "f": 0.1319796908799507
        },
        "rouge-2": {
          "r": 0.012121212121212121,
          "p": 0.019417475728155338,
          "f": 0.01492536840192843
        },
        "rouge-l": {
          "r": 0.1015625,
          "p": 0.18840579710144928,
          "f": 0.1319796908799507
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/TR/2501.07489v1",
      "true_abstract": "The efficient market hypothesis (EMH) famously stated that prices fully\nreflect the information available to traders. This critically depends on the\ntransfer of information into prices through trading strategies. Traders\noptimise their strategy with models of increasing complexity that identify the\nrelationship between information and profitable trades more and more\naccurately. Under specific conditions, the increased availability of low-cost\nuniversal approximators, such as AI systems, should be naturally pushing\ntowards more advanced trading strategies, potentially making it harder and\nharder for inefficient traders to profit. In this paper, we leverage on a\ngeneralised notion of market efficiency, based on the definition of an\nequilibrium price process, that allows us to distinguish different levels of\nmodel complexity through investors' beliefs, and trading strategies\noptimisation, and discuss the relationship between AI-powered trading and the\ntime-evolution of market efficiency. Finally, we outline the need for and the\nchallenge of describing out-of-equilibrium market dynamics in an adaptive\nmulti-agent environment.",
      "generated_abstract": "We study the robust performance of portfolio optimization algorithms in\napplications with correlated risk. We focus on portfolio optimization with\nnonlinear risk models and derive a risk-neutral-adjusted (RNA) portfolio\noptimization problem. Our approach involves a reformulation of the original\nproblem into a constrained optimization problem, which is solved by the\nproximal gradient algorithm. We introduce a RNA-specific framework for\noptimization with correlated risk, which provides a novel perspective on\nportfolio optimization with correlated risk. To solve the framework, we propose\na novel RNA-specific proximal gradient algorithm. We show that the proposed\nframework and algorithm can handle correlated risk and is robust to outliers,\nas demonstrated through numerical experiments. We validate the framework and\nalgorithm through numerical examples.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14018691588785046,
          "p": 0.22727272727272727,
          "f": 0.17341039990510887
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1308411214953271,
          "p": 0.21212121212121213,
          "f": 0.16184970626349038
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2502.19391v1",
      "true_abstract": "Antibody co-design represents a critical frontier in drug development, where\naccurate prediction of both 1D sequence and 3D structure of\ncomplementarity-determining regions (CDRs) is essential for targeting specific\nepitopes. Despite recent advances in equivariant graph neural networks for\nantibody design, current approaches often fall short in capturing the intricate\ninteractions that govern antibody-antigen recognition and binding specificity.\nIn this work, we present Igformer, a novel end-to-end framework that addresses\nthese limitations through innovative modeling of antibody-antigen binding\ninterfaces. Our approach refines the inter-graph representation by integrating\npersonalized propagation with global attention mechanisms, enabling\ncomprehensive capture of the intricate interplay between local chemical\ninteractions and global conformational dependencies that characterize effective\nantibody-antigen binding. Through extensive validation on epitope-binding CDR\ndesign and structure prediction tasks, Igformer demonstrates significant\nimprovements over existing methods, suggesting that explicit modeling of\nmulti-scale residue interactions can substantially advance computational\nantibody design for therapeutic applications.",
      "generated_abstract": "tion of the human genome is driven by the interplay between the\ngeneration of new genetic variants and the evolution of the genome itself.\nVariation in the human genome has been found to be significantly more\nvariable than variation in the genomes of other animals. To explore the\nrelationship between genetic variation and the evolution of the human genome,\nwe used an algorithm for the detection of genomic regions that exhibit\nheterogeneity of mutation rates between individuals. We identified 200 regions\nwith heterogeneous mutation rates in the human genome, including the\nintergenic regions of the genome, which are believed to be under\nsignificant evolutionary pressure. The regions were found to be enriched in\nvariation and show evidence of increased fitness. A comparison of the\nevolutionary dynamics of these regions in the human genome with those in other\nspecies shows that they are",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1282051282051282,
          "p": 0.189873417721519,
          "f": 0.1530612196777386
        },
        "rouge-2": {
          "r": 0.013986013986013986,
          "p": 0.01680672268907563,
          "f": 0.01526717061447629
        },
        "rouge-l": {
          "r": 0.10256410256410256,
          "p": 0.1518987341772152,
          "f": 0.12244897477977945
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.12860v4",
      "true_abstract": "We study the problem of an organization that matches agents to objects where\nagents have preference rankings over objects and the organization uses\nalgorithms to construct a ranking over objects on behalf of each agent. Our new\nframework carries the interpretation that the organization and its agents may\nbe misaligned in pursuing some underlying matching goal. We design matching\nmechanisms that integrate agent decision-making and the algorithm by avoiding\nmatches that are unanimously disagreeable between the two parties. Our\nmechanisms also satisfy restricted efficiency properties. Subsequently, we\nprove that no unanimous mechanism is strategy-proof but that ours can be\nnon-obviously manipulable. We generalize our framework to allow for any\npreference aggregation rules and extend the famed Gibbard-Satterthwaite Theorem\nto our setting. We apply our framework to place foster children in foster homes\nto maximize welfare. Using a machine learning model that predicts child welfare\nin placements and a (planned) novel lab-in-the-field eliciting real\ncaseworkers' preferences, we empirically demonstrate that there are important\nmatch-specific welfare gains that our mechanisms extract that are not realized\nunder the status quo.",
      "generated_abstract": "the economic impact of the COVID-19 pandemic on a society of\nsatisfaction consumers. Consumers can obtain satisfaction from both tangible\nand intangible goods, and satisfaction is measured by a utility function.\nSatisfaction consumption is constrained by the budget constraint and the\nconsumer's preferences. We analyze the effects of the pandemic on satisfaction\nconsumption and the society's economy. The effects of the pandemic on\nsatisfaction consumption are positive and exponential, and the society's\neconomic impact is also positive and exponential. The results show that the\neffects of the pandemic on satisfaction consumption are similar to the effects\nof the pandemic on the society's economy. The effects of the pandemic on\nsatisfaction consumption are similar to the effects of the pandemic on the\nsociety's economy. The effects of the pandemic on satisfaction consumption are\nsimilar to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11016949152542373,
          "p": 0.2708333333333333,
          "f": 0.15662650191319505
        },
        "rouge-2": {
          "r": 0.011764705882352941,
          "p": 0.02702702702702703,
          "f": 0.016393438396937396
        },
        "rouge-l": {
          "r": 0.11016949152542373,
          "p": 0.2708333333333333,
          "f": 0.15662650191319505
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2410.04149v1",
      "true_abstract": "This paper introduces Mov-Avg, the Python software package for time series\nanalysis that requires little computer programming experience from the user.\nThe package allows the identification of trends, patterns, and the prediction\nof future events based on data collected over time. In this regard, the Mov-Avg\nimplementation provides three indicators to apply, namely: Simple Moving\nAverage, Weighted Moving Average and Exponential Moving Average. Due to its\ngeneric design, the Mov-Avg software package can be used in any field where the\napplication of moving averages is valid. In general, the Mov-Avg library for\ntime series analysis contributes to a better understanding of data-driven\nprocesses over time by taking advantage of moving averages in any way adapted\nto the research context.",
      "generated_abstract": "t the first universal quantum machine learning (QML) algorithm that\ncan solve the widely used QCQP benchmark problem, which has been a fundamental\nmodel for QML research. Our algorithm is a universal quantum circuit\ngeneralization of the quantum counterpart of the classical CQCQP solver, and\ncan be used for solving other QML problems such as the QCQP-Q, QCQP-QCQP, and\nQCQP-CQCQP benchmarks. We demonstrate that our algorithm is more than 200\ntimes faster than the classical CQCQP solver. Additionally, we show that\ntheoretical analysis of our algorithm reveals that it can solve the QCQP-Q\nproblem in polynomial time, and also establish a new lower bound for the\napproximation factor of the QCQP-Q problem. This work significantly\nenhances",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1686746987951807,
          "p": 0.1794871794871795,
          "f": 0.17391303848308334
        },
        "rouge-2": {
          "r": 0.018691588785046728,
          "p": 0.019230769230769232,
          "f": 0.018957340972576057
        },
        "rouge-l": {
          "r": 0.1686746987951807,
          "p": 0.1794871794871795,
          "f": 0.17391303848308334
        }
      }
    },
    {
      "paper_id": "math.RT.math/RA/2503.10461v1",
      "true_abstract": "This article studies the compatibility of Koenig's notion of an exact Borel\nsubalgebra of a quasi-hereditary or, more generally, standardly stratified\nalgebra with taking idempotent subalgebras or quotients. As an application, we\nprovide bounds for the multiplicities of indecomposable projectives in the\nprincipal blocks of BGG category $\\mathcal{O}$ having basic regular exact Borel\nsubalgebras.",
      "generated_abstract": "We investigate the properties of the set of all solutions of a linear\ninterpolation equation in a complex Hilbert space. We show that this set is\ncontained in the unit ball, and that it has the following properties: it is\nnon-empty, it is open, and it is connected. We also show that it can be\ninterpreted as the set of all linear combinations of the solutions of the\ninterpolation equation, and we prove that this set is a closed subspace of the\nHilbert space. We also show that the linear map that maps a solution of the\ninterpolation equation to its corresponding linear combination of the\nsolutions is an isometry. Finally, we show that the linear map that maps a\nsolution of the interpolation equation to its corresponding linear combination\nof the solutions is a contraction.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13636363636363635,
          "p": 0.11320754716981132,
          "f": 0.12371133524922966
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.02247191011235955,
          "f": 0.028368789670540476
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.11320754716981132,
          "f": 0.12371133524922966
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.10114v1",
      "true_abstract": "We design specific neural networks (NNs) for the identification of switching\nnonlinear systems in the state-space form, which explicitly model the switching\nbehavior and address the inherent coupling between system parameters and\nswitching modes. This coupling is specifically addressed by leveraging the\nexpectation-maximization (EM) framework. In particular, our technique will\ncombine a moving window approach in the E-step to efficiently estimate the\nswitching sequence, together with an extended Kalman filter (EKF) in the M-step\nto train the NNs with a quadratic convergence rate. Extensive numerical\nsimulations, involving both academic examples and a battery charge management\nsystem case study, illustrate that our technique outperforms available ones in\nterms of parameter estimation accuracy, model fitting, and switching sequence\nidentification.",
      "generated_abstract": "This paper studies the distributed optimization problem of minimizing the\noptimistic objective function of a network of uncertain distributed\ninterconnected systems while satisfying the constraints of the entire system\nunder uncertainty. The problem is formulated as a constrained constrained\noptimal control problem and solved by applying the method of successive\napproximations. Numerical examples demonstrate the effectiveness of the proposed\nmethodology.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11363636363636363,
          "p": 0.22727272727272727,
          "f": 0.1515151470707072
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10227272727272728,
          "p": 0.20454545454545456,
          "f": 0.1363636319191921
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.04555v1",
      "true_abstract": "This article analyzes a key exchange protocol based on the triad tropical\nsemiring, recently proposed by Jackson, J. and Perumal, R. We demonstrate that\nthe triad tropical semiring is isomorphic to a circulant matrix over tropical\nnumbers. Consequently, matrices in this semiring can be represented as tropical\nmatrices. As a result, we conduct a cryptanalysis of the key exchange protocol\nusing an algorithm introduced by Sulaiman Alhussaini, Craig Collett, and Sergei\nSergeev to solve the double discrete logarithm problem over tropical matrices",
      "generated_abstract": "We prove that the sum of two nonnegative numbers is nonnegative if and only if\ntheir sum is nonnegative. This result generalizes a classical result of\nNagata and Szemeredi on the sum of nonnegative numbers.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16393442622950818,
          "p": 0.45454545454545453,
          "f": 0.24096385152562058
        },
        "rouge-2": {
          "r": 0.02666666666666667,
          "p": 0.06666666666666667,
          "f": 0.03809523401360588
        },
        "rouge-l": {
          "r": 0.14754098360655737,
          "p": 0.4090909090909091,
          "f": 0.2168674659834519
        }
      }
    },
    {
      "paper_id": "math.GR.math/AT/2503.08411v1",
      "true_abstract": "In this article, we prove that, given two finite connected graphs $\\Gamma_1$\nand $\\Gamma_2$, if the two right-angled Artin groups $A(\\Gamma_1)$ and\n$A(\\Gamma_2)$ are quasi-isometric, then the infinite pointed sums\n$\\bigvee_\\mathbb{N} \\Gamma_1^{\\bowtie}$ and $\\bigvee_\\mathbb{N}\n\\Gamma_2^{\\bowtie}$ are homotopy equivalent, where $\\Gamma_i^{\\bowtie}$ denotes\nthe simplicial complex whose vertex-set is $\\Gamma_i$ and whose simplices are\ngiven by joins. These invariants are extracted from a study, of independent\ninterest, of the homotopy types of several complexes of hyperplanes in\nquasi-median graphs (such as one-skeleta of CAT(0) cube complexes). For\ninstance, given a quasi-median graph $X$, the \\emph{crossing complex}\n$\\mathrm{Cross}^\\triangle(X)$ is the simplicial complex whose vertices are the\nhyperplanes (or $\\theta$-classes) of $X$ and whose simplices are collections of\npairwise transverse hyperplanes. When $X$ has no cut-vertex, we show that\n$\\mathrm{Cross}^\\triangle(X)$ is homotopy equivalent to the pointed sum of the\nlinks of all the vertices in the prism-completion $X^\\square$ of $X$.",
      "generated_abstract": "In this paper we extend the theory of the $C^*$-algebras of the\nfinite dimensional $C^*$-algebras to the case of infinite dimensional\n$C^*$-algebras, by constructing an approximate identity for the $C^*$-algebra\nof the set of all infinite dimensional $C^*$-algebras. In addition, we show\nthat the approximate identity constructed here is the analogue of the\napproximate identity for the $C^*$-algebra of the set of all compact groups.\n  The new theory allows us to construct a wide variety of new $C^*$-algebras,\nand we prove that the associated $C^*$-algebras are in one-to-one correspondence\nwith the $C^*$-algebras of the finite dimensional $C^*$-algebras.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20652173913043478,
          "p": 0.3877551020408163,
          "f": 0.26950354156430767
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.1095890410958904,
          "f": 0.07655501937776177
        },
        "rouge-l": {
          "r": 0.16304347826086957,
          "p": 0.30612244897959184,
          "f": 0.21276595291182546
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.13720v1",
      "true_abstract": "Ecosystems often demonstrate the coexistence of numerous species competing\nfor limited resources, with pronounced rarity and abundance patterns. A\npotential driver of such coexistence is environmental fluctuations that favor\ndifferent species over time. However, how to include and treat such temporal\nvariability in existing consumer-resource models is still an open problem. In\nthis study, we examine the role of correlated temporal fluctuations in\nmetabolic strategies within a stochastic consumer-resource framework,\nreflecting change of species behavior in response to the environment. In some\nconditions, we are able to solve analytically the species abundance\ndistributions, through path integral formalism. Our results reveal that\nstochastic dynamic metabolic strategies induce community structures that align\nmore closely with empirical ecological observations and contribute to the\nviolation of the Competitive Exclusion Principle (CEP). The degree of CEP\nviolation is maximized under intermediate competition strength, leading to an\nintermediate competition hypothesis. Furthermore, when non-neutral effects are\npresent, maximal biodiversity is achieved for intermediate values of the\namplitude of fluctuations. This work not only challenges traditional ecological\nparadigms, but also establishes a robust theoretical framework for exploring\nhow temporal dynamics and stochasticity drive biodiversity and community.",
      "generated_abstract": "of protein-protein interactions (PPIs) is crucial for the\nunderstanding of biological systems. However, the existing methods face\nsignificant challenges, including limited sample efficiency, high computational\ncomplexity, and limited interpretability. To address these issues, we propose\na novel PPI prediction framework, named PPI-Guide, which leverages the\npotential of large language models (LLMs) to enhance the interpretability of\nPPI predictions. Specifically, we first introduce a multi-head attention\nmechanism to capture the interactions between PPIs and proteins, which enhances\nthe model's understanding of protein-protein interactions. To further improve\nthe interpretability of PPI predictions, we propose a novel guided attention\nmechanism, which captures the interactions between the predicted PPI and the\nprotein binding sites in the protein sequence. Finally, we propose a\ngraph",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11538461538461539,
          "p": 0.2,
          "f": 0.14634145877453913
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11538461538461539,
          "p": 0.2,
          "f": 0.14634145877453913
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.16407v1",
      "true_abstract": "Various factors influence why some countries are more open to immigration\nthan others. Policy is only one of them. We design country-specific measures of\nopenness to immigration that aim to capture de facto levels of openness to\nimmigration, complementing existing de jure measures of immigration, based on\nenacted immigration laws and policy measures. We estimate these for 148\ncountries and three years (2000, 2010, and 2020). For a subset of countries, we\nalso distinguish between openness towards tertiary-educated migrants and less\nthan tertiary-educated migrants. Using the measures, we show that most places\nin the World today are closed to immigration, and a few regions are very open.\nThe World became more open in the first decade of the millennium, an opening\nmainly driven by the Western World and the Gulf countries. Moreover, we show\nthat other factors equal, countries that increased their openness to\nimmigration, reduced their old-age dependency ratios, and experienced slower\nreal wage growth, arguably a sign of relaxing labor and skill shortages.",
      "generated_abstract": "the effect of trade on employment in a developing country.\nUsing panel data from 128 countries from 1990 to 2017, we find that trade\nincreases the employment rate by 3.8 percentage points in the short run and\nby 2.2 percentage points in the long run. The impact is driven by workers in\nlow-skill occupations who are more exposed to trade. A 1 percent increase in\ntrade leads to a 0.3 to 0.5 percentage point increase in the employment rate\nof these workers. The effect is stronger in countries with lower levels of\neducation, which are more exposed to trade. The effect is robust to alternative\nmodels of trade and to excluding countries with very low levels of education.\nOur results suggest that trade can boost employment in developing countries,\nbut only for those who are already relatively well-ed",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2169811320754717,
          "p": 0.2875,
          "f": 0.24731182305468852
        },
        "rouge-2": {
          "r": 0.02631578947368421,
          "p": 0.03278688524590164,
          "f": 0.0291970753519109
        },
        "rouge-l": {
          "r": 0.2169811320754717,
          "p": 0.2875,
          "f": 0.24731182305468852
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.15097v1",
      "true_abstract": "This paper advances a variable screening approach to enhance conditional\nquantile forecasts using high-dimensional predictors. We have refined and\naugmented the quantile partial correlation (QPC)-based variable screening\nproposed by Ma et al. (2017) to accommodate $\\beta$-mixing time-series data.\nOur approach is inclusive of i.i.d scenarios but introduces new convergence\nbounds for time-series contexts, suggesting the performance of QPC-based\nscreening is influenced by the degree of time-series dependence. Through Monte\nCarlo simulations, we validate the effectiveness of QPC under weak dependence.\nOur empirical assessment of variable selection for growth-at-risk (GaR)\nforecasting underscores the method's advantages, revealing that specific labor\nmarket determinants play a pivotal role in forecasting GaR. While prior\nempirical research has predominantly considered a limited set of predictors, we\nemploy the comprehensive Fred-QD dataset, retaining a richer breadth of\ninformation for GaR forecasts.",
      "generated_abstract": "the estimation and inference of a linear mixed-effects model\nwith time-varying fixed effects. To accommodate the potentially nonlinear\nresponse of the dependent variable to the fixed effects, we propose to model the\nresponse function as a random-effects functional linear regression with a\ncompound-likelihood-based estimator. We also introduce a functional regression\nwith a compound-likelihood-based estimator for the variance component. The\ncompound-likelihood-based estimator of the variance component is\nhigh-dimensional, and we propose a robust, asymptotically normal estimator of\nthe variance. We show that the proposed functional regression estimators\noutperform the standard functional regression estimator in terms of asymptotic\nnormality and finite-sample performance. We also study the identification\nproblem of the functional regression estimators, and we show that the\ncompound-likelihood-based estimator of the functional regression is consistent",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1485148514851485,
          "p": 0.25,
          "f": 0.18633539905096264
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13861386138613863,
          "p": 0.23333333333333334,
          "f": 0.1739130388025155
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.00290v1",
      "true_abstract": "I present a novel uniform law of large numbers (ULLN) for network-dependent\ndata. While Kojevnikov, Marmer, and Song (KMS, 2021) provide a comprehensive\nsuite of limit theorems and a robust variance estimator for network-dependent\nprocesses, their analysis focuses on pointwise convergence. On the other hand,\nuniform convergence is essential for nonlinear estimators such as M and GMM\nestimators (e.g., Newey and McFadden, 1994, Section 2). Building on KMS, I\nestablish the ULLN under network dependence and demonstrate its utility by\nproving the consistency of both M and GMM estimators. A byproduct of this work\nis a novel maximal inequality for network data, which may prove useful for\nfuture research beyond the scope of this paper.",
      "generated_abstract": "This paper investigates the potential of Bayesian nonparametric methods to\nreconstruct the spatial interaction structure of two count data models with\nnonlinear interactions. The first model is a random graph model with a\nnonlinear interaction structure. The second model is a generalized random\ngraph model with a linear interaction structure. A Bayesian nonparametric\nreconstruction method based on the posterior predictive distribution is\ndeveloped. The simulation study demonstrates that the Bayesian nonparametric\nreconstruction method is more effective than the traditional approach in\nreconstructing the interaction structure of the two models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10588235294117647,
          "p": 0.17647058823529413,
          "f": 0.13235293648897078
        },
        "rouge-2": {
          "r": 0.00909090909090909,
          "p": 0.013513513513513514,
          "f": 0.010869560408792295
        },
        "rouge-l": {
          "r": 0.09411764705882353,
          "p": 0.1568627450980392,
          "f": 0.1176470541360296
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.cond-mat/stat-mech/2503.10575v1",
      "true_abstract": "We discuss non-reversible Markov-chain Monte Carlo algorithms that, for\nparticle systems, rigorously sample the positional Boltzmann distribution and\nthat have faster than physical dynamics. These algorithms all feature a\nnon-thermal velocity distribution. They are exemplified by the lifted TASEP\n(totally asymmetric simple exclusion process), a one-dimensional lattice\nreduction of event-chain Monte Carlo. We analyze its dynamics in terms of a\nvelocity trapping that arises from correlations between the local density and\nthe particle velocities. This allows us to formulate a conjecture for its\nout-of-equilibrium mixing time scale, and to rationalize its equilibrium\nsuperdiffusive time scale. Both scales are faster than for the (unlifted)\nTASEP. They are further justified by our analysis of the lifted TASEP in terms\nof many-particle realizations of true self-avoiding random walks. We discuss\nvelocity trapping beyond the case of one-dimensional lattice models and in more\nthan one physical dimensions. Possible applications beyond physics are pointed\nout.",
      "generated_abstract": "st decade, there has been an explosion of interest in the quantum\nquantum dynamics (QQD) of many-body systems with a particular focus on\nexact-diagonalization (ED) of the Hamiltonian. However, the ED of the\nHamiltonian is only possible for a limited range of systems, such as\none-dimensional (1D) systems, two-dimensional (2D) systems, and systems with\nnearly-hermitian Hamiltonians. To address this limitation, we have recently\nproposed a novel approach, known as the Generalized Quantum QQD (GQQD)\nmethod, where the Hamiltonian is expressed as a product of a matrix with\ndiagonal entries and a matrix with off-diagonal entries. In this approach,\nthe off-diagonal entries are approximated by a truncated series of\nHermitian operators. The GQQ",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.16,
          "f": 0.14035087226839044
        },
        "rouge-2": {
          "r": 0.014388489208633094,
          "p": 0.019417475728155338,
          "f": 0.016528920730484335
        },
        "rouge-l": {
          "r": 0.11458333333333333,
          "p": 0.14666666666666667,
          "f": 0.12865496583564195
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2412.18776v1",
      "true_abstract": "The Virtual Traffic Light (VTL) eliminates the need for physical traffic\nsignal infrastructure at intersections, leveraging Connected Vehicles (CVs) to\noptimize traffic flow. VTL assigns right-of-way dynamically based on factors\nsuch as estimated times of arrival (ETAs), the number of CVs in various lanes,\nand emission rates. These factors are considered in line with the objectives of\nthe VTL application. Aiming to optimize traffic flow and reduce delays, the VTL\nsystem generates Signal Phase and Timing (SPaT) data for CVs approaching an\nintersection, while considering the impact of each CV movement on others.\nHowever, the stochastic nature of vehicle arrivals at intersections complicates\nreal-time optimization, challenging classical computing methods. To address\nthis limitation, we develop a VTL method that leverages quantum computing to\nminimize stopped delays for CVs. The method formulates the VTL problem as a\nQuadratic Unconstrained Binary Optimization (QUBO) problem, a mathematical\nframework well-suited for quantum computing. Using D-Wave cloud-based quantum\ncomputer, our approach determines optimal solutions for right-of-way\nassignments under the standard National Electrical Manufacturers Association\n(NEMA) phasing system. The system was evaluated using the microscopic traffic\nsimulator SUMO under varying traffic volumes. Our results demonstrate that the\nquantum-enabled VTL system reduces stopped delays and travel times compared to\nclassical optimization-based systems. This approach not only enhances traffic\nmanagement efficiency but also reduces the infrastructure costs associated with\ntraditional traffic signals. The quantum computing-supported VTL system offers\na transformative solution for large-scale traffic control, providing superior\nperformance across diverse traffic scenarios and paving the way for advanced,\ncost-effective traffic management.",
      "generated_abstract": "llenge in designing and deploying machine learning (ML) systems is\nchoosing the best set of hyperparameters. In this work, we explore the\napplication of deep learning (DL) methods to the problem of hyperparameter\ntuning. We propose a novel Deep Neural Network (DNN) architecture, which\nintegrates a DL-based autoencoder with a conventional feed-forward neural\nnetwork, for hyperparameter optimization. Our approach is based on a\nconvolutional autoencoder, which is pre-trained on a large-scale dataset of\nhyperparameter tuning results. This dataset is used to train a feed-forward\nnetwork to predict the optimal hyperparameters. The resulting network, which we\ncall an autoencoder-based hyperparameter optimizer (AAH), is evaluated on two\nwell-known benchmark datasets. The AAH outperforms the existing state-of-the-art\nmethods in both datasets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14534883720930233,
          "p": 0.3048780487804878,
          "f": 0.19685038932853877
        },
        "rouge-2": {
          "r": 0.004149377593360996,
          "p": 0.008771929824561403,
          "f": 0.0056337984568174985
        },
        "rouge-l": {
          "r": 0.13372093023255813,
          "p": 0.2804878048780488,
          "f": 0.18110235783247577
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/ET/2503.07799v1",
      "true_abstract": "Congenital Heart Disease (CHD) is one of the leading causes of fetal\nmortality, yet the scarcity of labeled CHD data and strict privacy regulations\nsurrounding fetal ultrasound (US) imaging present significant challenges for\nthe development of deep learning-based models for CHD detection. Centralised\ncollection of large real-world datasets for rare conditions, such as CHD, from\nlarge populations requires significant co-ordination and resource. In addition,\ndata governance rules increasingly prevent data sharing between sites. To\naddress these challenges, we introduce, for the first time, a novel\nprivacy-preserving, zero-shot CHD detection framework that formulates CHD\ndetection as a normality modeling problem integrated with model merging. In our\nframework dubbed Sparse Tube Ultrasound Distillation (STUD), each hospital site\nfirst trains a sparse video tube-based self-supervised video anomaly detection\n(VAD) model on normal fetal heart US clips with self-distillation loss. This\nenables site-specific models to independently learn the distribution of healthy\ncases. To aggregate knowledge across the decentralized models while maintaining\nprivacy, we propose a Divergence Vector-Guided Model Merging approach,\nDivMerge, that combines site-specific models into a single VAD model without\ndata exchange. Our approach preserves domain-agnostic rich spatio-temporal\nrepresentations, ensuring generalization to unseen CHD cases. We evaluated our\napproach on real-world fetal US data collected from 5 hospital sites. Our\nmerged model outperformed site-specific models by 23.77% and 30.13% in accuracy\nand F1-score respectively on external test sets.",
      "generated_abstract": "Recent advances in large language models (LLMs) have led to the development of\nmodels that can produce images with human-like visual features. These models\ncan generate images with realistic textures, colors, and lighting, but their\nability to create complex and realistic scenes remains limited. In this work,\nwe propose a novel method for generating scenes in 3D using LLMs. Our method\nleverages LLMs to generate text descriptions for 3D models, which are then\nused to generate synthetic 3D meshes. We demonstrate that our approach can\ngenerate high-quality scenes with realistic lighting, textures, and colors,\nwhile preserving the semantic integrity of the 3D models. Our code is available\nat https://github.com/Ray-of-Light/3D-LLM-Scene-Generation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13664596273291926,
          "p": 0.29333333333333333,
          "f": 0.1864406736300633
        },
        "rouge-2": {
          "r": 0.03167420814479638,
          "p": 0.06666666666666667,
          "f": 0.04294478090914268
        },
        "rouge-l": {
          "r": 0.13043478260869565,
          "p": 0.28,
          "f": 0.17796609735887686
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.02802v1",
      "true_abstract": "In this work, we show the first average-case reduction transforming the\nsparse Spiked Covariance Model into the sparse Spiked Wigner Model and as a\nconsequence obtain the first computational equivalence result between two\nwell-studied high-dimensional statistics models. Our approach leverages a new\nperturbation equivariance property for Gram-Schmidt orthogonalization, enabling\nremoval of dependence in the noise while preserving the signal.",
      "generated_abstract": "er a setting in which a finite number of individuals participate in\ntwo-sided matching markets with a common randomness. The individuals' preferences\nare revealed at the beginning of the experiment, and they can choose to exchange\ninformation with one another to improve their preference scores. We analyze the\neffect of this exchange information on the matching outcome. Our analysis is\nbased on a novel approach combining matching theory and martingale theory. We\nshow that the exchange information leads to a monotone and additive degradation\nof the expected matching value, which is a monotone and additive decreasing\nfunction of the exchange information. Furthermore, we show that the exchange\ninformation can be used to improve the matching outcome by a constant amount.\nThis can be viewed as a version of the well-known \"matching pumping lemma\". We\nalso show that the exchange information can be used to improve the matching\noutcome by a linear amount.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22448979591836735,
          "p": 0.1375,
          "f": 0.17054263094765956
        },
        "rouge-2": {
          "r": 0.03636363636363636,
          "p": 0.01680672268907563,
          "f": 0.02298850142357064
        },
        "rouge-l": {
          "r": 0.1836734693877551,
          "p": 0.1125,
          "f": 0.13953487900967507
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10360v1",
      "true_abstract": "Time-frequency concentration and resolution of the Cohen's class\ntime-frequency distribution (CCTFD) has attracted much attention in\ntime-frequency analysis. A variety of uncertainty principles of the CCTFD is\ntherefore derived, including the weak Heisenberg type, the Hardy type, the\nNazarov type, and the local type. However, the standard Heisenberg type still\nremains unresolved. In this study, we address the question of how the standard\nHeisenberg's uncertainty principle of the CCTFD is affected by fundamental\nproperties. The investigated distribution properties are Parseval's relation\nand the concise frequency domain definition (i.e., only frequency variables are\nexplicitly found in the tensor product), based on which we confine our\nattention to the CCTFD with some specific kernels. That is the unit modulus and\nv-independent time translation, reversal and scaling invariant kernel CCTFD\n(UMITRSK-CCTFD). We then extend the standard Heisenberg's uncertainty\nprinciples of the Wigner distribution to those of the UMITRSK-CCTFD, giving\nbirth to various types of attainable lower bounds on the uncertainty product in\nthe UMITRSK-CCTFD domain. The derived results strengthen the existing weak\nHeisenberg type and fill gaps in the standard Heisenberg type.",
      "generated_abstract": "f large language models (LLMs) in audio and speech processing\nenvironments has seen remarkable advancements over the past few years.\nHowever, the increasing use of LLMs in audio and speech applications has\nexposed them to privacy threats. In this work, we study the potential of\nadversarially-trained LLMs to extract sensitive information from audio data. We\nfocus on the audio fingerprinting task, where an LLM is trained to extract\nfingerprints from audio data. We propose an adversarial attack that\nexploits the capability of the LLM to generate fake audio samples. We analyze\nthe attack's effectiveness and assess its impact on the privacy of the\nfingerprinted audio. We demonstrate the effectiveness of our attack on the\nSAT-12 dataset, a publicly available audio dataset. Our results show that our\nattack successfully generates high-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13513513513513514,
          "p": 0.17857142857142858,
          "f": 0.153846148942012
        },
        "rouge-2": {
          "r": 0.02531645569620253,
          "p": 0.03389830508474576,
          "f": 0.028985502351397588
        },
        "rouge-l": {
          "r": 0.13513513513513514,
          "p": 0.17857142857142858,
          "f": 0.153846148942012
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2502.12453v1",
      "true_abstract": "Drug discovery is crucial for identifying candidate drugs for various\ndiseases.However, its low success rate often results in a scarcity of\nannotations, posing a few-shot learning problem. Existing methods primarily\nfocus on single-scale features, overlooking the hierarchical molecular\nstructures that determine different molecular properties. To address these\nissues, we introduce Universal Matching Networks (UniMatch), a dual matching\nframework that integrates explicit hierarchical molecular matching with\nimplicit task-level matching via meta-learning, bridging multi-level molecular\nrepresentations and task-level generalization. Specifically, our approach\nexplicitly captures structural features across multiple levels, such as atoms,\nsubstructures, and molecules, via hierarchical pooling and matching,\nfacilitating precise molecular representation and comparison. Additionally, we\nemploy a meta-learning strategy for implicit task-level matching, allowing the\nmodel to capture shared patterns across tasks and quickly adapt to new ones.\nThis unified matching framework ensures effective molecular alignment while\nleveraging shared meta-knowledge for fast adaptation. Our experimental results\ndemonstrate that UniMatch outperforms state-of-the-art methods on the\nMoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87% in AUROC and\n6.52% in delta AUPRC. UniMatch also shows excellent generalization ability on\nthe Meta-MolNet benchmark.",
      "generated_abstract": "nt treatment of chronic low back pain (CLBP) is based on\nadherence to the National Institute for Health and Care Excellence (NICE)\nguidance. However, NICE guidelines are not always followed by primary care\nphysicians, which may lead to suboptimal treatment outcomes. Aim: The aim of\nthis study is to develop a clinical decision support system (CDSS) to support\nprimary care physicians (PCPs) in the management of CLBP. Methods: A\ncognitive engineering-based approach was used to develop a CDSS. An expert panel\nreviewed the prototype and provided feedback. The prototype was further\nrefined by applying iterative refinement and user testing. The prototype was\nevaluated by the panel and the experts. Results: The prototype was evaluated\nby the panel and experts. The prototype was evaluated by the panel and the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08759124087591241,
          "p": 0.14814814814814814,
          "f": 0.11009173844920482
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08029197080291971,
          "p": 0.13580246913580246,
          "f": 0.10091742652259932
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.17911v1",
      "true_abstract": "Speech enhancement plays an essential role in improving the quality of speech\nsignals in noisy environments. This paper investigates the efficacy of\nintegrating Bidirectional Gated Recurrent Units (BGRU) and Transformer models\nfor speech enhancement tasks. Through a comprehensive experimental evaluation,\nour study demonstrates the superiority of this hybrid architecture over\ntraditional methods and standalone models. The combined BGRU-Transformer\nframework excels in capturing temporal dependencies and learning complex signal\npatterns, leading to enhanced noise reduction and improved speech quality.\nResults show significant performance gains compared to existing approaches,\nhighlighting the potential of this integrated model in real-world applications.\nThe seamless integration of BGRU and Transformer architectures not only\nenhances system robustness but also opens the road for advanced speech\nprocessing techniques. This research contributes to the ongoing efforts in\nspeech enhancement technology and sets a solid foundation for future\ninvestigations into optimizing model architectures, exploring many application\nscenarios, and advancing the field of speech processing in noisy environments.",
      "generated_abstract": "lity assessment (AQA) is a crucial component in audio system\nperformance evaluation. This study proposes a novel approach for audio\nquality assessment (AQA) using a deep learning-based model that leverages\nspeech-to-audio-to-speech (S2AS) to capture the temporal aspects of speech\ndynamics. The proposed model is based on a multimodal transformer architecture\nthat combines text and audio representations, enabling a comprehensive\nevaluation of audio quality. The model is trained and tested on the\naudio-quality-related dataset AQ-Dataset, which consists of 41,843 audio\nsamples. The proposed methodology is evaluated using the Speech Quality\nIndex (SQI), a standardized metric for AQA, and compared against state-of-the-art\nmodels. The results demonstrate that the proposed methodology achieves",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18584070796460178,
          "p": 0.2876712328767123,
          "f": 0.22580644684414394
        },
        "rouge-2": {
          "r": 0.020134228187919462,
          "p": 0.02912621359223301,
          "f": 0.023809518976128468
        },
        "rouge-l": {
          "r": 0.1415929203539823,
          "p": 0.2191780821917808,
          "f": 0.1720430059839289
        }
      }
    },
    {
      "paper_id": "cs.HC.q-bio/NC/2502.17172v1",
      "true_abstract": "Affective computing has made significant strides in emotion recognition and\ngeneration, yet current approaches mainly focus on short-term pattern\nrecognition and lack a comprehensive framework to guide affective agents toward\nlong-term human well-being. To address this, we propose a teleology-driven\naffective computing framework that unifies major emotion theories (basic\nemotion, appraisal, and constructivist approaches) under the premise that\naffect is an adaptive, goal-directed process that facilitates survival and\ndevelopment. Our framework emphasizes aligning agent responses with both\npersonal/individual and group/collective well-being over extended timescales.\nWe advocate for creating a \"dataverse\" of personal affective events, capturing\nthe interplay between beliefs, goals, actions, and outcomes through real-world\nexperience sampling and immersive virtual reality. By leveraging causal\nmodeling, this \"dataverse\" enables AI systems to infer individuals' unique\naffective concerns and provide tailored interventions for sustained well-being.\nAdditionally, we introduce a meta-reinforcement learning paradigm to train\nagents in simulated environments, allowing them to adapt to evolving affective\nconcerns and balance hierarchical goals - from immediate emotional needs to\nlong-term self-actualization. This framework shifts the focus from statistical\ncorrelations to causal reasoning, enhancing agents' ability to predict and\nrespond proactively to emotional challenges, and offers a foundation for\ndeveloping personalized, ethically aligned affective systems that promote\nmeaningful human-AI interactions and societal well-being.",
      "generated_abstract": "of molecular neuroscience has advanced to the point where it\nhas the potential to revolutionize the study of biological systems. This\nadvance stems from the discovery of the genetic basis of neurological disorders,\nwhich has revealed that many neurological disorders are caused by mutations in\nproteins that are essential for proper brain function. To fully understand\nthese disorders, we must have the ability to identify these proteins in the\nbrain tissue of patients and analyze the effects of these mutations. However,\nthis is a daunting task as it is extremely time-consuming and expensive to\nanalyze all of the brain tissue of patients. To address this challenge, we\npresent a framework for automated proteomic analysis of brain tissue. Our\napproach leverages a large-scale human proteome database, which allows us to\nselectively analyze tissue samples from patients with",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14473684210526316,
          "p": 0.2619047619047619,
          "f": 0.18644067338121242
        },
        "rouge-2": {
          "r": 0.009852216748768473,
          "p": 0.015503875968992248,
          "f": 0.012048188019489462
        },
        "rouge-l": {
          "r": 0.14473684210526316,
          "p": 0.2619047619047619,
          "f": 0.18644067338121242
        }
      }
    },
    {
      "paper_id": "math.PR.nlin/CG/2411.15954v1",
      "true_abstract": "We introduce and study a symmetric, gradient exclusion process, in the class\nof non-cooperative kinetically constrained lattice gases, modelling a\nnon-linear diffusivity where mass transport is constrained by the local density\nnot being too small or too large. Maintaining the gradient property is the main\ntechnical challenge. The resulting model enjoys of properties in common with\nthe Bernstein polynomial basis, and is associated with the diffusion\ncoefficient $D_{n,k}(\\rho)=\\binom{n+k}{k}\\rho^n(1-\\rho)^k$, for $n,k$ arbitrary\nnatural numbers. The dynamics generalizes the Porous Media Model, and we show,\nvia the entropy method, the hydrodynamic limit for the empirical measure\nassociated with a perturbed, irreducible version of the process. The\nhydrodynamic equation is proved to be a Generalized Porous Media Equation.",
      "generated_abstract": "We study a linear stochastic differential equation (SDE) with a drift and\ncumulative diffusion. We prove that the mean-field limit of the system is a\nGaussian SDE with a drift and cumulative diffusion. This limit is obtained by\nsolving the coupled system of ordinary differential equations. We prove that the\nlimit is given by a Gaussian SDE with a drift and cumulative diffusion. We\nshow that the limit is not a Gaussian SDE with a drift, cumulative diffusion\nand a quadratic drift. We show that the limit is not a Gaussian SDE with a\ncumulative diffusion. We show that the limit is not a Gaussian SDE with a\nquadratic drift.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15853658536585366,
          "p": 0.37142857142857144,
          "f": 0.2222222180290745
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.05263157894736842,
          "f": 0.035714281230867916
        },
        "rouge-l": {
          "r": 0.15853658536585366,
          "p": 0.37142857142857144,
          "f": 0.2222222180290745
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/ST/2411.19444v3",
      "true_abstract": "The Capital Asset Pricing Model (CAPM) relates a well-diversified stock\nportfolio to a benchmark portfolio. We insert size effect in CAPM, capturing\nthe observation that small stocks have higher risk and return than large\nstocks, on average. Dividing stock index returns by the Volatility Index makes\nthem independent and normal. In this article, we combine these ideas to create\na new discrete-time model, which includes volatility, relative size, and CAPM.\nWe fit this model using real-world data, prove the long-term stability, and\nconnect this research to Stochastic Portfolio Theory. We fill important gaps in\nour previous article on CAPM with the size factor.",
      "generated_abstract": "er a continuous-time stochastic-volatility framework with a\nvolatility-based capital asset pricing model, where the volatility is\nstructured by a fractional Brownian motion. The pricing problem involves the\nstochastic integral of a function of two variables, where the variable of\ninterest is the stock price process. We study the case of a finite number of\nasset returns. We derive the solution to the stochastic integral and prove\nthat the solution is a solution of a fractional partial differential equation\n(FPE). We also prove that the solution to the FPE is unique. We also prove that\nthe solution to the FPE can be written as a solution to a FPE with a time\ndifference between the diffusion coefficients. We show that the solution to the\nFPE is related to the solution of a system of stochastic differential equations\n(SDEs), and we provide a simple proof that the solution",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1686746987951807,
          "p": 0.19718309859154928,
          "f": 0.18181817684854118
        },
        "rouge-2": {
          "r": 0.00980392156862745,
          "p": 0.00909090909090909,
          "f": 0.009433957271273557
        },
        "rouge-l": {
          "r": 0.13253012048192772,
          "p": 0.15492957746478872,
          "f": 0.1428571378875023
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/LG/2503.10566v1",
      "true_abstract": "Despite their remarkable performance, large language models lack elementary\nsafety features, and this makes them susceptible to numerous malicious attacks.\nIn particular, previous work has identified the absence of an intrinsic\nseparation between instructions and data as a root cause for the success of\nprompt injection attacks. In this work, we propose an architectural change,\nASIDE, that allows the model to clearly separate between instructions and data\nby using separate embeddings for them. Instead of training the embeddings from\nscratch, we propose a method to convert an existing model to ASIDE form by\nusing two copies of the original model's embeddings layer, and applying an\northogonal rotation to one of them. We demonstrate the effectiveness of our\nmethod by showing (1) highly increased instruction-data separation scores\nwithout a loss in model capabilities and (2) competitive results on prompt\ninjection benchmarks, even without dedicated safety training. Additionally, we\nstudy the working mechanism behind our method through an analysis of model\nrepresentations.",
      "generated_abstract": "In this paper, we propose a novel learning-based framework for graph\nlearning with multiple label sparsity, referred to as GraphSAIL. Unlike previous\nlearning-based methods for graph learning, GraphSAIL achieves efficient\nlearning without the need for explicit parameter sharing across multiple\nlabels. Instead, we propose a novel GraphSAIL-Loss that incorporates a\nweighted-average loss function for multiple labels, which is trained with\nGraphSAIL. We show that our GraphSAIL-Loss achieves significant gains in\nperformance compared to GraphSAIL, without compromising learning efficiency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.2857142857142857,
          "f": 0.19999999545000013
        },
        "rouge-2": {
          "r": 0.02,
          "p": 0.0410958904109589,
          "f": 0.026905825192544305
        },
        "rouge-l": {
          "r": 0.14423076923076922,
          "p": 0.26785714285714285,
          "f": 0.1874999954500001
        }
      }
    },
    {
      "paper_id": "physics.atom-ph.physics/atm-clus/2503.08972v1",
      "true_abstract": "Out-of-equilibrium Rydberg gases exhibit emergent many-body phases due to\nmode competition. Sustained limit cycle oscillations (OSC) emerge when driven\nby B-fields at room-temperature, forming robust Rydberg dissipative time\ncrystals (DTC). These driven-dissipative Rydberg DTC have recently been shown\nto develop an effective transition centered at the OSC frequency (-10dB\nbandwidth of ~1.7kHz, centered at 9.8kHz). Weak RF signals injected within this\nemergent transition perturb and emerge on the OSC spectrum, from which\nsensitive and high-resolution sensing of E-fields (~1.6-2.3 uVcm-1Hz-1/2) near\nthe OSC frequencies can be achieved. In this article, it is demonstrated that\nDC and AC Stark fields in the sub-kHz regime can be used effectively to shift\n(DC) or modulate (AC) the OSC frequency of Rydberg DTC at room-temperature. The\nAC-Stark driven modulation of the OSC is shown as an effective technique to\nsense weak AC E-fields in the sub-kHz regime. With a modest setup, a\nsensitivity of ~7.8 uVcm-1Hz-1/2 for AC signals at 300Hz (~8.7x improvement\nover state-of-art Rydberg atom techniques), and high-resolution detection to as\nlow as sub-Hz is demonstrated. This approach enables the development of\nultra-compact, extremely low-frequency E-field detectors for applications in\nremote sensing, communications, navigation, and bio-medical technologies.",
      "generated_abstract": "t discovery of a strong coupling between the two-photon absorption\nand the two-photon scattering of electrons in diamond (D2P) was interpreted as\na signature of a resonant electron-phonon interaction. Here we report a\nconvincing experimental evidence of this interaction for the first time.\nDiamond is an ideal candidate for the realization of a tunable, strongly\ninteracting quantum system. We have performed two-photon absorption spectroscopy\nin a controlled temperature regime and observe a broad resonance in the\nspectrum of the electron. This resonance is strongly affected by the phonons\nwhich can be tuned in the photon energy range of a few hundreds of nanometers.\nWe performed a detailed analysis of the experiment and we conclude that the\nexperimental evidence of the electron-phonon interaction is consistent with a\nspin-independent interaction of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1357142857142857,
          "p": 0.24358974358974358,
          "f": 0.17431192200993192
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.02586206896551724,
          "f": 0.019867544937503867
        },
        "rouge-l": {
          "r": 0.12857142857142856,
          "p": 0.23076923076923078,
          "f": 0.16513761008332645
        }
      }
    },
    {
      "paper_id": "cond-mat.supr-con.cond-mat/supr-con/2503.10040v1",
      "true_abstract": "Delineating the superconducting order parameters is a pivotal task in\ninvestigating superconductivity for probing pairing mechanisms, as well as\ntheir symmetry and topology. Point-contact Andreev reflection (PCAR)\nmeasurement is a simple yet powerful tool for identifying the order parameters.\nThe PCAR spectra exhibit significant variations depending on the type of the\norder parameter in a superconductor, including its magnitude\n($\\mathit{\\Delta}$), as well as temperature, interfacial quality, Fermi\nvelocity mismatch, and other factors. The information on the order parameter\ncan be obtained by finding the combination of these parameters, generating a\ntheoretical spectrum that fits a measured experimental spectrum. However, due\nto the complexity of the spectra and the high dimensionality of parameters,\nextracting the fitting parameters is often time-consuming and labor-intensive.\nIn this study, we employ a convolutional neural network (CNN) algorithm to\ncreate models for rapid and automated analysis of PCAR spectra of various\nsuperconductors with different pairing symmetries (conventional $s$-wave,\nchiral $p_x+ip_y$-wave, and $d_{x^2-y^2}$-wave). The training datasets are\ngenerated based on the Blonder-Tinkham-Klapwijk (BTK) theory and further\nmodified and augmented by selectively incorporating noise and peaks according\nto the bias voltages. This approach not only replicates the experimental\nspectra but also brings the model's attention to important features within the\nspectra. The optimized models provide fitting parameters for experimentally\nmeasured spectra in less than 100 ms per spectrum. Our approaches and findings\npave the way for rapid and automated spectral analysis which will help\naccelerate research on superconductors with complex order parameters.",
      "generated_abstract": "play between electron correlations and spin correlations is an\nsignificant challenge for the realization of high-performance spintronics.\nNear-conducting (NCN) semiconductors, such as iron pnictides, exhibit intriguing\nproperties that have led to their recent applications in spintronics. In\nthese materials, electron correlations are mediated by the spin-orbit\ninteraction, which induces a spin-orbit coupling (SOC) term in the electron\nband structure. In this work, we investigate the effect of SOC on the\nelectron-phonon coupling in NCN semiconductors. We show that SOC\nimproves the electron-phonon coupling by increasing the coupling strength and\nreducing the phonon-assisted energy shifts, thereby enhancing the coupling\nstrength and reducing the phonon-induced",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1320754716981132,
          "p": 0.2916666666666667,
          "f": 0.18181817752740775
        },
        "rouge-2": {
          "r": 0.008968609865470852,
          "p": 0.02127659574468085,
          "f": 0.012618292357970906
        },
        "rouge-l": {
          "r": 0.12578616352201258,
          "p": 0.2777777777777778,
          "f": 0.17316016886939914
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.cond-mat/dis-nn/2503.09518v1",
      "true_abstract": "We generalize the computation of the capacity of exponential Hopfield model\nfrom Lucibello and M\\'ezard (2024) to more generic pattern ensembles, including\nbinary patterns and patterns generated from a hidden manifold model.",
      "generated_abstract": "t a systematic study of the effect of disorder on the ground state\nof the one-dimensional Hubbard model on the triangular lattice. We focus on the\nlowest-energy states, which are obtained by diagonalizing the Hamiltonian in\nthe random-phase approximation (RPA). We study the effect of disorder on the\nground state energy and on the spectral function of the RPA. We find that the\nground state energy exhibits a broad peak at a value that depends on the disorder\nstrength and the distance between the sites, and that the spectral function is\nskewed to higher energies. We find that the effect of disorder on the\nground-state energy is robust with respect to the details of the disorder, and\nit does not depend on the disorder strength. We also show that the disorder\ngenerates a non-trivial spectral function for energies below the Fermi energy\nand that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2692307692307692,
          "p": 0.10144927536231885,
          "f": 0.14736841707700843
        },
        "rouge-2": {
          "r": 0.03225806451612903,
          "p": 0.009345794392523364,
          "f": 0.014492750139677373
        },
        "rouge-l": {
          "r": 0.23076923076923078,
          "p": 0.08695652173913043,
          "f": 0.12631578549806108
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08756v1",
      "true_abstract": "The diagnosis of brain tumours is an extremely sensitive and complex clinical\ntask that must rely upon information gathered through non-invasive techniques.\nOne such technique is magnetic resonance, in the modalities of imaging or\nspectroscopy. The latter provides plenty of metabolic information about the\ntumour tissue, but its high dimensionality makes resorting to pattern\nrecognition techniques advisable. In this brief paper, an international\ndatabase of brain tumours is analyzed resorting to an ad hoc spectral frequency\nselection procedure combined with nonlinear classification.",
      "generated_abstract": "ng need for 3D imaging and analysis of the brain has led to the\ndevelopment of high-resolution magnetic resonance imaging (MRI) systems that\nallow for 3D volumetric scans of the brain. These MRI systems, such as the\nMagnetom Vision 3T scanner, have a resolution of 1.5 mm, with a field of view of\n1.5 x 1.5 x 1.5 cm. This resolution is sufficient for many imaging applications\nin neuroscience, but it may be insufficient for more advanced imaging\napplications. Recently, 3D optical coherence tomography (OCT) has emerged as a\nnew imaging modality that can generate high-resolution images of the brain.\nOCT has the potential to complement 3D MRI scans by providing more\ndetailed anatomic information about the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22388059701492538,
          "p": 0.19480519480519481,
          "f": 0.2083333283574461
        },
        "rouge-2": {
          "r": 0.025974025974025976,
          "p": 0.018518518518518517,
          "f": 0.02162161676201716
        },
        "rouge-l": {
          "r": 0.19402985074626866,
          "p": 0.16883116883116883,
          "f": 0.18055555057966835
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.10489v1",
      "true_abstract": "Molecular pretrained representations (MPR) has emerged as a powerful approach\nfor addressing the challenge of limited supervised data in applications such as\ndrug discovery and material design. While early MPR methods relied on 1D\nsequences and 2D graphs, recent advancements have incorporated 3D\nconformational information to capture rich atomic interactions. However, these\nprior models treat molecules merely as discrete atom sets, overlooking the\nspace surrounding them. We argue from a physical perspective that only modeling\nthese discrete points is insufficient. We first present a simple yet insightful\nobservation: naively adding randomly sampled virtual points beyond atoms can\nsurprisingly enhance MPR performance. In light of this, we propose a principled\nframework that incorporates the entire 3D space spanned by molecules. We\nimplement the framework via a novel Transformer-based architecture, dubbed\nSpaceFormer, with three key components: (1) grid-based space discretization;\n(2) grid sampling/merging; and (3) efficient 3D positional encoding. Extensive\nexperiments show that SpaceFormer significantly outperforms previous 3D MPR\nmodels across various downstream tasks with limited data, validating the\nbenefit of leveraging the additional 3D space beyond atoms in MPR models.",
      "generated_abstract": "We propose a simple yet effective strategy to improve the accuracy of\ncellular automata (CA) simulations. The strategy is to use an iterative\napproach to generate CA configurations from scratch, instead of using an\nexisting CA model. The initial CA model is selected based on its performance\nin simulations. The strategy allows for a more efficient use of available\ncomputational resources and is an alternative to the development of new CA\nmodels. We demonstrate the effectiveness of the strategy in a number of\nsimulations, including those for the three-dimensional (3D) Ising model and\nthe three-dimensional (3D) Heisenberg model, where the CA model is the\nKolmogorov-Arnold-Moser-Mihaly (KAM) model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.28125,
          "f": 0.17821781745319093
        },
        "rouge-2": {
          "r": 0.017142857142857144,
          "p": 0.030303030303030304,
          "f": 0.021897805603655937
        },
        "rouge-l": {
          "r": 0.12318840579710146,
          "p": 0.265625,
          "f": 0.16831682735418108
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.03620v1",
      "true_abstract": "Reconfigurable antennas possess the capability to dynamically adjust their\nfundamental operating characteristics, thereby enhancing system adaptability\nand performance. To fully exploit this flexibility in modern wireless\ncommunication systems, this paper considers a novel tri-hybrid beamforming\narchitecture, which seamlessly integrates pattern-reconfigurable antennas with\nboth analog and digital beamforming. The proposed tri-hybrid architecture\noperates across three layers: (\\textit{i}) a radiation beamformer in the\nelectromagnetic (EM) domain for dynamic pattern alignment, (\\textit{ii}) an\nanalog beamformer in the radio-frequency (RF) domain for array gain\nenhancement, and (\\textit{iii}) a digital beamformer in the baseband (BB)\ndomain for multi-user interference mitigation. To establish a solid theoretical\nfoundation, we first develop a comprehensive mathematical model for the\ntri-hybrid beamforming system and formulate the signal model for a multi-user\nmulti-input single-output (MU-MISO) scenario. The optimization objective is to\nmaximize the sum-rate while satisfying practical constraints. Given the\nchallenges posed by high pilot overhead and computational complexity, we\nintroduce an innovative tri-timescale beamforming framework, wherein the\nradiation beamformer is optimized over a long-timescale, the analog beamformer\nover a medium-timescale, and the digital beamformer over a short-timescale.\nThis hierarchical strategy effectively balances performance and implementation\nfeasibility. Simulation results validate the performance gains of the proposed\ntri-hybrid architecture and demonstrate that the tri-timescale design\nsignificantly reduces pilot overhead and computational complexity, highlighting\nits potential for future wireless communication systems.",
      "generated_abstract": "In this paper, we investigate the design of hybrid multi-input multi-output\n(MIMO) antenna arrays with an RF front-end and an MIMO channel. We consider\nsingle-input single-output (SISO) systems and SISO/MIMO hybrid arrays with a\nsingle MIMO channel. We formulate the problem of designing an RF front-end for\na given MIMO channel as a constrained optimization problem with the constraints\nrelated to the minimum number of MIMO channels and the number of antennas per\nchannel. We propose a generalization of the MINU algorithm to solve this\nproblem. Numerical results show that our approach outperforms existing\nmethods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14388489208633093,
          "p": 0.3333333333333333,
          "f": 0.20100502091361336
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.046511627906976744,
          "f": 0.02836879008701839
        },
        "rouge-l": {
          "r": 0.1223021582733813,
          "p": 0.2833333333333333,
          "f": 0.17085426714476917
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2502.13238v1",
      "true_abstract": "Uncertainty quantification in causal inference settings with random network\ninterference is a challenging open problem. We study the large sample\ndistributional properties of the classical difference-in-means Hajek treatment\neffect estimator, and propose a robust inference procedure for the\n(conditional) direct average treatment effect, allowing for cross-unit\ninterference in both the outcome and treatment equations. Leveraging ideas from\nstatistical physics, we introduce a novel Ising model capturing interference in\nthe treatment assignment, and then obtain three main results. First, we\nestablish a Berry-Esseen distributional approximation pointwise in the degree\nof interference generated by the Ising model. Our distributional approximation\nrecovers known results in the literature under no-interference in treatment\nassignment, and also highlights a fundamental fragility of inference procedures\ndeveloped using such a pointwise approximation. Second, we establish a uniform\ndistributional approximation for the Hajek estimator, and develop robust\ninference procedures that remain valid regardless of the unknown degree of\ninterference in the Ising model. Third, we propose a novel resampling method\nfor implementation of robust inference procedure. A key technical innovation\nunderlying our work is a new \\textit{De-Finetti Machine} that facilitates\nconditional i.i.d. Gaussianization, a technique that may be of independent\ninterest in other settings.",
      "generated_abstract": "r presents a novel, nonparametric Bayesian approach for modeling\nnonlinear time-to-event data with a continuous survival function. The proposed\nmethodology extends the well-known survival analysis framework by leveraging\ninformation from a high-dimensional time series of pre-treatment covariates.\nThis information is incorporated through the use of a latent variable model to\nexplicitly capture the dependencies between covariates and survival. The model\nis then estimated using Bayesian nonparametrics, providing a flexible framework\nfor modeling and inferring survival parameters in time-to-event data. The\nestimated model is then used to construct a novel adaptive kernel smoothing\nestimator for survival analysis. The proposed method is applied to simulated\nand real data, demonstrating its ability to accurately capture the dynamics of\nthe survival function. The effectiveness of the proposed method",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.2,
          "f": 0.15789473206371207
        },
        "rouge-2": {
          "r": 0.011764705882352941,
          "p": 0.017857142857142856,
          "f": 0.014184392374630659
        },
        "rouge-l": {
          "r": 0.13043478260869565,
          "p": 0.2,
          "f": 0.15789473206371207
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2503.03783v2",
      "true_abstract": "Resting heart rate (RHR) is an important biomarker of cardiovascular health\nand mortality, but tracking it longitudinally generally requires a wearable\ndevice, limiting its availability. We present PHRM, a deep learning system for\npassive heart rate (HR) and RHR measurements during everyday smartphone use,\nusing facial video-based photoplethysmography. Our system was developed using\n225,773 videos from 495 participants and validated on 185,970 videos from 205\nparticipants in laboratory and free-living conditions, representing the largest\nvalidation study of its kind. Compared to reference electrocardiogram, PHRM\nachieved a mean absolute percentage error (MAPE) < 10% for HR measurements\nacross three skin tone groups of light, medium and dark pigmentation; MAPE for\neach skin tone group was non-inferior versus the others. Daily RHR measured by\nPHRM had a mean absolute error < 5 bpm compared to a wearable HR tracker, and\nwas associated with known risk factors. These results highlight the potential\nof smartphones to enable passive and equitable heart health monitoring.",
      "generated_abstract": "ession profiles (GEPs) are complex and nonlinear systems, characterized\nby significant interactions between genes and their environment. In this paper,\nwe introduce a novel approach to GEP modeling, termed GEP-GMM, which\nintroduces the Generalized Multilayer Perceptron (GMP) as a generalization of\nthe Multilayer Perceptron (MLP) model, which has been widely used for GEP\nmodeling. GEP-GMM is a hybrid approach that combines the GMP with a\ngenerative model, enabling the model to learn complex interactions between\ngenes and their environment. The GMP model is used to learn the nonlinear\ninteractions between genes and their environment, while the generative model\nis used to generate synthetic gene expression profiles that mimic the\ninteractions between genes and their environment. By integrating the\nnonlinear interactions between",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07758620689655173,
          "p": 0.1323529411764706,
          "f": 0.09782608229678662
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.06896551724137931,
          "p": 0.11764705882352941,
          "f": 0.08695651707939533
        }
      }
    },
    {
      "paper_id": "physics.flu-dyn.nlin/CG/2502.16568v1",
      "true_abstract": "Quantum computing holds great promise to accelerate scientific computations\nin fluid dynamics and other classical physical systems. While various quantum\nalgorithms have been proposed for linear flows, developing quantum algorithms\nfor nonlinear problems remains a significant challenge. We introduce a novel\nnode-level ensemble description of lattice gas for simulating nonlinear fluid\ndynamics on a quantum computer. This approach combines the advantages of the\nlattice Boltzmann method, which offers low-dimensional representation, and\nlattice gas cellular automata, which provide linear collision treatment.\nBuilding on this framework, we propose a quantum lattice Boltzmann method that\nrelies on linear operations with medium dimensionality. We validated the\nalgorithm through comprehensive simulations of benchmark cases, including\nvortex-pair merging and decaying turbulence on $2048^2$ computational grid\npoints. The results demonstrate remarkable agreement with direct numerical\nsimulation, effectively capturing the essential nonlinear mechanisms of fluid\ndynamics. This work offers valuable insights into developing quantum algorithms\nfor other nonlinear problems, and potentially advances the application of\nquantum computing across various transport phenomena in engineering.",
      "generated_abstract": "We investigate the effects of the initial condition on the numerical\nresults of the mean squared displacement of a fluid particle. The initial\nconditions are taken from a one-dimensional compressible Navier-Stokes\nequation, and we compare the results of three numerical schemes:\nconservative-flux, conservative-centered, and conservative-centered-momentum\nfor the conservative-flux scheme, and conservative-centered-momentum and\nconservative-centered-momentum-momentum for the conservative-centered scheme.\nThe numerical results show that the initial conditions have a significant\neffect on the mean squared displacement of the fluid particle. We also\ndiscuss the effect of the initial conditions on the mean squared displacement of\nthe fluid particle.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13157894736842105,
          "p": 0.32608695652173914,
          "f": 0.1874999959031251
        },
        "rouge-2": {
          "r": 0.012903225806451613,
          "p": 0.02857142857142857,
          "f": 0.017777773491359058
        },
        "rouge-l": {
          "r": 0.12280701754385964,
          "p": 0.30434782608695654,
          "f": 0.1749999959031251
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/GR/2503.10624v1",
      "true_abstract": "Fitting a body to a 3D clothed human point cloud is a common yet challenging\ntask. Traditional optimization-based approaches use multi-stage pipelines that\nare sensitive to pose initialization, while recent learning-based methods often\nstruggle with generalization across diverse poses and garment types. We propose\nEquivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline\nthat estimates cloth-to-body surface mapping through locally approximate SE(3)\nequivariance, encoding tightness as displacement vectors from the cloth surface\nto the underlying body. Following this mapping, pose-invariant body features\nregress sparse body markers, simplifying clothed human fitting into an\ninner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show\nthat ETCH significantly outperforms state-of-the-art methods -- both\ntightness-agnostic and tightness-aware -- in body fitting accuracy on loose\nclothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant\ntightness design can even reduce directional errors by (67.2% ~ 89.8%) in\none-shot (or out-of-distribution) settings. Qualitative results demonstrate\nstrong generalization of ETCH, regardless of challenging poses, unseen shapes,\nloose clothing, and non-rigid dynamics. We will release the code and models\nsoon for research purposes at https://boqian-li.github.io/ETCH/.",
      "generated_abstract": "lity Assessment (VQA) is a challenging task that requires evaluating\nvideo quality by accurately classifying the visual content of the video into\nfour categories: distortion, motion blur, noise, and background clutter. The\ncurrent state-of-the-art (SOTA) VQA models often rely on complex\nmulti-modal fusion approaches to generate a single visual representation for\nclassification. While these methods have demonstrated superior performance,\nthese approaches often require complex multi-modal fusion and often lead to\nsuboptimal results. To address this issue, we propose a novel multi-modal\nfusion method for VQA. Our approach leverages the fusion capability of the\nVQA model to enhance the fusion capability of the multi-modal fusion model,\nresulting in a more efficient and effective multi-modal fusion. This approach\nenables the model to effectively classify the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1610738255033557,
          "p": 0.2891566265060241,
          "f": 0.20689654712879024
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.02702702702702703,
          "f": 0.0202020155208663
        },
        "rouge-l": {
          "r": 0.1476510067114094,
          "p": 0.26506024096385544,
          "f": 0.18965516781844544
        }
      }
    },
    {
      "paper_id": "q-bio.SC.q-bio/SC/2407.18237v2",
      "true_abstract": "Transport of dense core vesicles (DCVs) in neurons is crucial for\ndistributing molecules like neuropeptides and growth factors. We studied the\nexperimental trajectories of dynein-driven directed movement of DCVs in the ALA\nneuron C. elegans over a duration of up to 6 seconds. We analysed the DCV\nmovement in three strains of C. elegans: 1) with normal kinesin-1 function, 2)\nwith reduced function in kinesin light chain 2 (KLC-2), and 3) a null mutation\nin kinesin light chain 1 (KLC-1). We find that DCVs move superdiffusively with\ndisplacement variance $var(x) \\sim t^2$ in all three strains with low reversal\nrates and frequent immobilization of DCVs. The distribution of DCV\ndisplacements fits a beta-binomial distribution with the mean and the variance\nfollowing linear and quadratic growth patterns, respectively. We propose a\nsimple heterogeneous random walk model to explain the observed superdiffusive\nretrograde transport behaviour of DCV movement. This model involves a random\nprobability with the beta density for a DCV to resume its movement or remain in\nthe same position.",
      "generated_abstract": "ng global pandemic has brought new insights into the pathogenesis\nof infectious diseases. In particular, the interplay between viruses and their\nhost cells has been highlighted. However, the underlying mechanisms remain\nunderstudied. In this paper, we address this gap by providing a systematic\nreview and integrative analysis of the viral mechanisms affecting host\ncells. We focus on two key aspects: (1) how viruses alter host cellular\nfunctions, and (2) how host cellular functions are altered by viral infections.\nWe aim to provide an in-depth understanding of the host-virus interaction\nmechanisms, and identify the key underlying mechanisms. We discuss the\nimportant roles of the host cellular components, such as the endoplasmic\nreticulum, and the viral proteins. We also highlight the potential\ninteractions between viral and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06363636363636363,
          "p": 0.08536585365853659,
          "f": 0.07291666177300381
        },
        "rouge-2": {
          "r": 0.006289308176100629,
          "p": 0.008620689655172414,
          "f": 0.0072727223949784784
        },
        "rouge-l": {
          "r": 0.06363636363636363,
          "p": 0.08536585365853659,
          "f": 0.07291666177300381
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.09934v1",
      "true_abstract": "It is time to move on from attempts to make the pharmacy benefit manager\n(PBM) reseller business model more transparent. Time and time again the Big 3\nPBMs have developed opaque alternatives to piece-meal 100% pass-through\nmandates. Time and time again PBMs have demonstrated expertise in finding\nloopholes in state government disclosure laws. The purpose of this paper is to\nprovide quantitative estimates of two transparent insurance business models as\na solution to the PBM agency issue. The key parameter used is an 8% gross\nprofit margin figure disclosed by the Big 3 PBMs themselves. Based on reported\ndrug trend delivered to plans, we use a $1,200 to $1,500 per member per year\n(PMPY) as the range for this key performance indicator (KPI). We propose that\ndiscussions of PBM insurance business models start with the following figures:\n(1) a fixed premium model with medical loss ratio ranging from 92% to 85%; (2)\na fee-for-service model ranging from $96 to $180 PMPY with risk sharing of\ndeviations from a contracted PMPY delivered drug spend.",
      "generated_abstract": "This paper examines the interplay between climate and the labor market in\nthe United States. We use a dynamic stochastic general equilibrium model to\nanalyze the impact of a 0.5$^\\circ$C temperature target on the employment\nresponse to climate change. Our results reveal that a temperature target can\nreduce unemployment by approximately 20% and increase labor supply by 15% on\naverage. Moreover, the impact is larger for younger workers and women. These\nfindings suggest that a temperature target can help mitigate the economic\nconsequences of climate change and enhance the sustainability of the economy.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1282051282051282,
          "p": 0.234375,
          "f": 0.16574585178230225
        },
        "rouge-2": {
          "r": 0.006172839506172839,
          "p": 0.011627906976744186,
          "f": 0.008064511598597756
        },
        "rouge-l": {
          "r": 0.11965811965811966,
          "p": 0.21875,
          "f": 0.15469612802539617
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.04100v1",
      "true_abstract": "We show how to improve the discrepancy of an iid sample by moving only a few\npoints. Specifically, modifying \\( O(m) \\) sample points on average reduces the\nKolmogorov-Smirnov distance to the population distribution to \\(1/m\\).",
      "generated_abstract": "We consider a class of random variables with an exponentially distributed\nrandom support. We show that if the support has a large enough mean, then the\nexpected value of the variable is almost surely finite. The proof relies on a\nconcentration inequality for the exponential random variable, which is a key\ningredient in the proof. We show that this inequality can be used to prove that\nthe random variable is almost surely finite if the support has a large enough\nmean. We further show that this concentration inequality is valid for a wide\nrange of distributions, not just the exponential. The proof of the concentration\ninequality is a key ingredient in the proof of the main result of the paper.\nFinally, we use this result to show that a class of random variables with an\nexponentially distributed random support has a large enough mean and can be\nconsidered as a random variable in a more general setting.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26666666666666666,
          "p": 0.11940298507462686,
          "f": 0.1649484493357425
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.009174311926605505,
          "f": 0.013888885209298815
        },
        "rouge-l": {
          "r": 0.26666666666666666,
          "p": 0.11940298507462686,
          "f": 0.1649484493357425
        }
      }
    },
    {
      "paper_id": "math.AG.math/KT/2503.09928v1",
      "true_abstract": "Given a compact Lie group $G$ acting on a space $X$, the classical\nAtiyah-Segal completion theorem identifies topological $K$-theory of the\nhomotopy quotient $X/G$ with an explicit completion of $G$-equivariant\ntopological $K$-theory of $X$. We prove an analog of this result for algebraic\n$K$-theory over a field of characteristic 0. In our setting $G$ is a reductive\ngroup that acts on a derived algebraic space $X$ with the assumption that all\nstabilizer groups are nice (in the sense of Alper). Our main result identifies\nthe value $R^{\\mathrm{dAff}}K([X/G])$ of right Kan extension of the $K$-theory\nfunctor from schemes to stacks with the completion of $K$-theory of the\ncategory $\\mathrm{Perf}([X/G])$ at the augmentation ideal of\n$K_0(\\mathrm{Rep}(G))$. The main novelty of our results is that $X$ is allowed\nto be singular or even derived. This generality is achieved by employing and\nimproving analogous versions of completion theorem for negative cyclic homology\n(after Ben-Zvi--Nadler and Chen) and for homotopy $K$-theory (after van den\nBergh--Tabuada). We also show that in the singular setting the completion\ntheorem does not necessarily hold without the nice stabilizer assumption. We\nview our results as a part of the general paradigm of extending the motivic\nfiltration on algebraic $K$-theory of schemes to algebraic $K$-theory of\nstacks.",
      "generated_abstract": "We prove a result about a family of sub-Riemannian metric spaces that\nintroduces a concept of local weak convergence of Riemannian metrics. We use\nthis result to study a certain sub-Riemannian manifold that is a family of\nRiemannian manifolds with a common base. We establish some local weak\nconvergence results for the metrics on the base, which we use to study the\ngeodesic flow on this manifold. We also use this to show that if the base has\nuniformly convex boundary, then the flow has uniformly convex boundary. We use\nthis to obtain a result about the geometry of the base.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1391304347826087,
          "p": 0.3137254901960784,
          "f": 0.19277108008056332
        },
        "rouge-2": {
          "r": 0.03208556149732621,
          "p": 0.07058823529411765,
          "f": 0.044117642761948954
        },
        "rouge-l": {
          "r": 0.1391304347826087,
          "p": 0.3137254901960784,
          "f": 0.19277108008056332
        }
      }
    },
    {
      "paper_id": "math.QA.math/CT/2503.06280v1",
      "true_abstract": "Hopf braces are the quantum analogues of skew braces and, as such, their\ncocommutative counterparts provide solutions to the quantum Yang-Baxter\nequation. We investigate various properties of categories related to Hopf\nbraces. In particular, we prove that the category of Hopf braces is accessible\nwhile the category of cocommutative Hopf braces is even locally presentable. We\nalso show that functors forgetting multiple antipodes and/or multiplications\ndown to coalgebras are monadic. Colimits in the category of cocommutative Hopf\nbraces are described explicitly and a free cocommutative Hopf brace on an\narbitrary cocommutative Hopf algebra is constructed.",
      "generated_abstract": "We prove a result that characterizes the maximal height of a graph with\nno isolated vertices. This result generalizes a result of Erd\\H{o}s and\nR\\'{e}nyi, which was previously used to prove that the maximal height of a\ngraph with no isolated vertices is bounded below by the chromatic number of\nthe graph. Our proof uses a method developed by Bader and Sch\\\"afke, who used it\nto prove the maximal height of a graph with no isolated vertices is bounded\nabove by the minimum of the chromatic numbers of its bipartite subgraphs.\nMoreover, we give a new proof of Erd\\H{o}s and R\\'{e}nyi's result.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16393442622950818,
          "p": 0.19607843137254902,
          "f": 0.1785714236112884
        },
        "rouge-2": {
          "r": 0.02531645569620253,
          "p": 0.02702702702702703,
          "f": 0.026143785855013982
        },
        "rouge-l": {
          "r": 0.16393442622950818,
          "p": 0.19607843137254902,
          "f": 0.1785714236112884
        }
      }
    },
    {
      "paper_id": "gr-qc.gr-qc/2503.09731v1",
      "true_abstract": "We conduct two searches for continuous, nearly monochromatic gravitational\nwaves originating from the central compact objects in the supernova remnants\nCassiopeia A and Vela Jr. using public LIGO data. The search for Cassiopeia A\ntargets signal frequencies between 20 Hz and 400 Hz; the Vela Jr. search\nbetween 400 Hz and 1700 Hz, and both investigate the broadest set of waveforms\never considered with highly sensitive deterministic search methods. Above 1500\nHz the Vela Jr. search is the most sensitive carried out thus far, improving on\nprevious results by over 300\\%. Above 976 Hz these results improve on existing\nones by 50\\%. In all we investigate over $10^{18}$ waveforms, leveraging the\ncomputational power donated by thousands of Einstein@Home volunteers. We\nperform a 4-stage follow-up on more than 6 million waveforms. None of the\nconsidered waveforms survives the follow-up scrutiny, indicating no significate\ndetection candidate. Our null results constrain the maximum amplitude of\ncontinuous signals as a function of signal frequency from the targets. The most\nstringent 90\\% confidence upper limit for Cas A is $h_0^{90 \\%}\\approx\n7.3\\times10^{-26}$ near 200 Hz, and for Vela Jr. it is $h_0^{90 \\%}\\approx\n8.9\\times10^{-26}$ near 400 Hz. Translated into upper limits on the ellipticity\nand r-mode amplitude, our results probe physically interesting regions: for\nexample the ellipticity of Vela Jr. is constrained to be smaller than $10^{-7}$\nacross the frequency band, with a tighter constraint of less than\n$2\\times10^{-8}$ at the highest frequencies.",
      "generated_abstract": "the spectral properties of the BH black hole with a non-vanishing\nenergy-momentum tensor. It is shown that the energy-momentum tensor can\nrepresent the source of a non-vanishing gravitational wave. It is also shown\nthat the metric perturbation with the BH background can be reduced to the\nEinstein-Maxwell equation in the limit of vanishing energy-momentum tensor.\n  We use the conformal transformation to study the spectrum of the metric\nperturbation in the limit of vanishing energy-momentum tensor. It is found that\nthe spectrum of the metric perturbation in the limit of vanishing energy-momentum\ntensor is a combination of the energy-momentum tensor spectrum and the\nSchwarzschild spectrum. We also show that the metric perturbation with the\nBH background can be reduced to the Maxwell equation in the limit of\nvan",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07894736842105263,
          "p": 0.2608695652173913,
          "f": 0.12121211764513837
        },
        "rouge-2": {
          "r": 0.013333333333333334,
          "p": 0.039473684210526314,
          "f": 0.019933551042483687
        },
        "rouge-l": {
          "r": 0.06578947368421052,
          "p": 0.21739130434782608,
          "f": 0.10101009744311817
        }
      }
    },
    {
      "paper_id": "math.AP.math/FA/2503.08309v1",
      "true_abstract": "We investigate the asymptotic behavior as $\\varepsilon \\to 0$ of singularly\nperturbed phase transition models of order $n \\geq 2$, given by \\begin{align}\n  G_\\varepsilon^{\\lambda,n}[u] := \\int_I \\frac 1\\varepsilon W(u)\n-\\lambda\\varepsilon^{2n-3} (u^{(n-1)})^2 + \\varepsilon^{2n-1} (u^{(n)})^2 \\ dx,\n\\quad u \\in W^{n,2}(I), \\end{align}\n  where $\\lambda >0$ is fixed, $I \\subset \\mathbb{R}$ is an open bounded\ninterval, and $W \\in C^0(\\mathbb{R})$ is a suitable double-well potential. We\nfind that there exists a positive critical parameter depending on $W$ and $n$,\nsuch that the $\\Gamma$-limit of $G_\\varepsilon^{\\lambda,n}$ with respect to the\n$L^1$-topology is given by a sharp interface functional in the subcritical\nregime. The cornerstone for the corresponding compactness property is a novel\nnonlinear interpolation inequality involving higher-order derivatives, which is\nbased on Gagliardo-Nirenberg type inequalities.",
      "generated_abstract": "the problem of finding a finite-dimensional subspace $V\\subset\nL^2(\\Omega)$ such that, for every $f\\in L^2(\\Omega)$ and every $A\\in\nL^\\infty(\\Omega)$, the functional $f+A\\cdot V$ attains a minimum on $V$. We\nshow that this is equivalent to finding a finite-dimensional subspace $W\\subset\nL^2(\\Omega)$ such that, for every $f\\in L^2(\\Omega)$ and every $A\\in\nL^\\infty(\\Omega)$, the functional $f+A\\cdot W$ attains a minimum on $W$. We\nshow that $W$ must be finite-dimensional, and we give conditions under which\n$W$ is a finite-dimensional subspace of $L^2(\\Omega)$ and the dimension of\n$W$ is equal to the dimension of $V$. We prove that if",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13861386138613863,
          "p": 0.2978723404255319,
          "f": 0.18918918485482114
        },
        "rouge-2": {
          "r": 0.025210084033613446,
          "p": 0.04411764705882353,
          "f": 0.0320855568692277
        },
        "rouge-l": {
          "r": 0.1188118811881188,
          "p": 0.2553191489361702,
          "f": 0.1621621578277941
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.astro-ph/EP/2503.09137v1",
      "true_abstract": "Loeb & Cloete (2025) intriguingly suggest that the near-Earth object 2005\nVL$_1$ could be the lost Soviet probe Venera 2. Here I evaluate the\nplausibility of such a claim against the available data. I have re-determined\nthe orbit of 2005 VL$_1$ (including a non-gravitational acceleration component)\nusing the astrometric observations retrieved from the Minor Planet Center (MPC)\ndatabase. By propagating the orbit of 2005 VL$_1$ over the period of the Venera\n2 mission, I compare this object's distance from the Earth and from Venus at\nthe times of the probe's launch and flyby with Venus, respectively. My\nanalysis, which takes into account realistic uncertainties on both the orbit of\n2005 VL1 and the position of Venera 2, decisively rules out the proposed\nidentification. My approach relies entirely on open-source software and\npublicly available data, and could represent a viable method to assess similar\nclaims in the future.",
      "generated_abstract": "t a novel method to constrain the properties of the circumstellar\ndust disk around a young star, by measuring the radial profile of the\npolarization of the starlight. The method uses the polarization of the starlight\nto infer the disk inclination and the dust grain size distribution. The method\nis applied to a sample of 108 young stars, spanning a wide range of spectral\ntypes and ages. The sample includes both disk-free and disk-obscured systems.\nWe use the disk-free sample to constrain the disk inclination. We find that\n$i_{disk} = 78^{+17}_{-23}\\degree$ is the best fit value. We also measure the\npolarization fraction ($P/I$) of the starlight and derive the dust grain size\ndistribution. We find a log-normal distribution with a median of $\\log(n_g",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0891089108910891,
          "p": 0.140625,
          "f": 0.10909090434233262
        },
        "rouge-2": {
          "r": 0.022222222222222223,
          "p": 0.02912621359223301,
          "f": 0.025210079124003495
        },
        "rouge-l": {
          "r": 0.0891089108910891,
          "p": 0.140625,
          "f": 0.10909090434233262
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/PM/2503.04662v1",
      "true_abstract": "We investigate portfolio optimization in financial markets from a trading and\nrisk management perspective. We term this task Risk-Aware Trading Portfolio\nOptimization (RATPO), formulate the corresponding optimization problem, and\npropose an efficient Risk-Aware Trading Swarm (RATS) algorithm to solve it. The\nkey elements of RATPO are a generic initial portfolio P, a specific set of\nUnique Eligible Instruments (UEIs), their combination into an Eligible\nOptimization Strategy (EOS), an objective function, and a set of constraints.\nRATS searches for an optimal EOS that, added to P, improves the objective\nfunction repecting the constraints.\n  RATS is a specialized Particle Swarm Optimization method that leverages the\nparameterization of P in terms of UEIs, enables parallel computation with a\nlarge number of particles, and is fully general with respect to specific\nchoices of the key elements, which can be customized to encode financial\nknowledge and needs of traders and risk managers.\n  We showcase two RATPO applications involving a real trading portfolio made of\nhundreds of different financial instruments, an objective function combining\nboth market risk (VaR) and profit&loss measures, constrains on market\nsensitivities and UEIs trading costs. In the case of small-sized EOS, RATS\nsuccessfully identifies the optimal solution and demonstrates robustness with\nrespect to hyper-parameters tuning. In the case of large-sized EOS, RATS\nmarkedly improves the portfolio objective value, optimizing risk and capital\ncharge while respecting risk limits and preserving expected profits.\n  Our work bridges the gap between the implementation of effective trading\nstrategies and compliance with stringent regulatory and economic capital\nrequirements, allowing a better alignment of business and risk management\nobjectives.",
      "generated_abstract": "the problem of optimal investment under the risk-neutral\nprobability space $({\\cal P}, \\mu)$ with an underlying continuous-time Markov\nchain $({\\cal Z}, {\\cal P}({\\cal Z}), \\mu)$. This is the first part of a\nproposed approach to the problem of optimal investment in a risky financial\nportfolio, which we call the Infinite-Horizon Model. The problem is to\nminimize the expected loss of the portfolio, under a given set of constraints\non the investment budget, and the investor's risk aversion $\\alpha$.\nConcretely, we derive a set of optimal investment policies, and show that they\ncan be expressed in terms of a class of stochastic control problems. We also\npropose a policy gradient method to solve the problem, and show that it\nprovides a first-order approximation to the optimal policy. In addition, we",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18125,
          "p": 0.35802469135802467,
          "f": 0.24066389595220472
        },
        "rouge-2": {
          "r": 0.040983606557377046,
          "p": 0.08547008547008547,
          "f": 0.05540165766868
        },
        "rouge-l": {
          "r": 0.14375,
          "p": 0.2839506172839506,
          "f": 0.19087136483187284
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.02058v1",
      "true_abstract": "Ribonucleic acid (RNA) plays fundamental roles in biological systems, from\ncarrying genetic information to performing enzymatic function. Understanding\nand designing RNA can enable novel therapeutic application and biotechnological\ninnovation. To enhance RNA design, in this paper we introduce RiboGen, the\nfirst deep learning model to simultaneously generate RNA sequence and all-atom\n3D structure. RiboGen leverages the standard Flow Matching with Discrete Flow\nMatching in a multimodal data representation. RiboGen is based on Euclidean\nEquivariant neural networks for efficiently processing and learning\nthree-dimensional geometry. Our experiments show that RiboGen can efficiently\ngenerate chemically plausible and self-consistent RNA samples. Our results\nsuggest that co-generation of sequence and structure is a competitive approach\nfor modeling RNA.",
      "generated_abstract": "We present a novel methodology for modeling the gene regulatory network (GRN)\nat the level of genes and transcription factors (TFs). Our approach is\nbased on the concept of the \"embedding\" of the GRN, i.e. the generation of a\nGibbs sampler that jointly models the GRN with its TFs and the state of the\nsystem. The sampling process is performed in a sequential way, starting with\nthe TFs, and then with the state of the system. The embedding methodology is\napplied to the yeast GRN and to its translation regulatory network. The\ncomputational results show that the embedding methodology can reduce the\ncomputational cost of the Gibbs sampler by a factor of 20-50 compared to the\nexisting direct sampling methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2235294117647059,
          "p": 0.2714285714285714,
          "f": 0.24516128536940696
        },
        "rouge-2": {
          "r": 0.045454545454545456,
          "p": 0.04672897196261682,
          "f": 0.046082944309711944
        },
        "rouge-l": {
          "r": 0.21176470588235294,
          "p": 0.2571428571428571,
          "f": 0.23225805956295534
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.03520v1",
      "true_abstract": "In our previous paper an effective algorithm for inverting polynomial\nautomorphisms was proposed. We extend its application to the case of formal\npower series over a field of arbitrary characteristic and illustrate the\nproposed approach with some examples.",
      "generated_abstract": "We introduce the notion of an iterated Banach space, which generalizes\nthe notion of a Banach space. The iteration process is defined inductively.\nWe prove that an iterated Banach space is isomorphic to the direct sum of its\niterates. This result extends to arbitrary Banach spaces the fact that the\ncone of iterated Banach spaces is the set of all iterated Banach spaces. In\naddition, we show that the iterated Banach space of the Banach space of all\nbounded sequences is the Banach space of all bounded sequences. We also show\nthat if the Banach space is reflexive, then the iterated Banach space is\nreflexive.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2571428571428571,
          "p": 0.19148936170212766,
          "f": 0.21951219022903046
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.013513513513513514,
          "f": 0.01801801357357467
        },
        "rouge-l": {
          "r": 0.17142857142857143,
          "p": 0.1276595744680851,
          "f": 0.1463414585217134
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/PL/2503.03698v1",
      "true_abstract": "Programs written in unsafe languages such as C are prone to memory safety\nerrors, which can lead to program compromises and serious real-world security\nconsequences. Recently, Memory-Safe WebAssembly (MSWASM) is introduced as a\ngeneral-purpose intermediate bytecode with built-in memory safety semantics.\nPrograms written in C can be compiled into MSWASM to get complete memory safety\nprotection. In this paper, we present our extensions on MSWASM, which improve\nits semantics and practicality. First, we formalize MSWASM semantics in\nCoq/Iris, extending it with inter-module interaction, showing that MSWASM\nprovides fine-grained isolation guarantees analogous to WASM's coarse-grained\nisolation via linear memory. Second, we present Aegis, a system to adopt the\nmemory safety of MSWASM for C programs in an interoperable way. Aegis pipeline\ngenerates Checked C source code from MSWASM modules to enforce spatial memory\nsafety. Checked C is a recent binary-compatible extension of C which can\nprovide guaranteed spatial safety. Our design allows Aegis to protect C\nprograms that depend on legacy C libraries with no extra dependency and with\nlow overhead. Aegis pipeline incurs 67% runtime overhead and near-zero memory\noverhead on PolyBenchC programs compared to native.",
      "generated_abstract": "r introduces a novel approach for generating program transformations\nin which the generated code is directly based on the original program,\nwithout any intermediate representation. The approach is based on the\nsemantic equivalence between programs and their corresponding abstract syntax\ntrees. We present a novel algorithm for transforming a program into its\nabstract syntax tree (AST), and we show that the resulting AST is equivalent to\nthe original program. We then show how to use this equivalence to generate\ntransformations that reduce the size of the original program by a fixed amount\nwithout changing the semantics of the program. The generated transformations\ncan be executed in the original program using an existing interpreter.\nAdditionally, we present an implementation of our approach in the Julia\nprogramming language, and we demonstrate the effectiveness of our approach by\ntransforming several real-world programs, including the C programming language,\nthe Julia programming language, and the LLVM programming language. Our approach\nreduces the size of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23529411764705882,
          "p": 0.3218390804597701,
          "f": 0.2718446553148271
        },
        "rouge-2": {
          "r": 0.011494252873563218,
          "p": 0.014598540145985401,
          "f": 0.0128617314051776
        },
        "rouge-l": {
          "r": 0.23529411764705882,
          "p": 0.3218390804597701,
          "f": 0.2718446553148271
        }
      }
    },
    {
      "paper_id": "q-fin.GN.econ/EM/2501.01763v1",
      "true_abstract": "Following an analysis of existing AI-related exchange-traded funds (ETFs), we\nreveal the selection criteria for determining which stocks qualify as\nAI-related are often opaque and rely on vague phrases and subjective judgments.\nThis paper proposes a new, objective, data-driven approach using natural\nlanguage processing (NLP) techniques to classify AI stocks by analyzing annual\n10-K filings from 3,395 NASDAQ-listed firms between 2011 and 2023. This\nanalysis quantifies each company's engagement with AI through binary indicators\nand weighted AI scores based on the frequency and context of AI-related terms.\nUsing these metrics, we construct four AI stock indices-the Equally Weighted AI\nIndex (AII), the Size-Weighted AI Index (SAII), and two Time-Discounted AI\nIndices (TAII05 and TAII5X)-offering different perspectives on AI investment.\nWe validate our methodology through an event study on the launch of OpenAI's\nChatGPT, demonstrating that companies with higher AI engagement saw\nsignificantly greater positive abnormal returns, with analyses supporting the\npredictive power of our AI measures. Our indices perform on par with or surpass\n14 existing AI-themed ETFs and the Nasdaq Composite Index in risk-return\nprofiles, market responsiveness, and overall performance, achieving higher\naverage daily returns and risk-adjusted metrics without increased volatility.\nThese results suggest our NLP-based approach offers a reliable,\nmarket-responsive, and cost-effective alternative to existing AI-related ETF\nproducts. Our innovative methodology can also guide investors, asset managers,\nand policymakers in using corporate data to construct other thematic\nportfolios, contributing to a more transparent, data-driven, and competitive\napproach.",
      "generated_abstract": "aper, we propose a novel approach to the estimation of heterogeneous\nand non-linear stochastic volatility models using a machine learning framework.\nThe proposed methodology employs a multilayer perceptron architecture,\nincluding two input layers and two output layers, to learn the stochastic\nvolatility parameters. The input layers receive data from a set of asset\nindices and the output layers predict the stochastic volatility of the\nindices. The model is trained on daily data spanning from 2000 to 2025 using\nthe financial time series dataset from the SMART-3 database. The proposed\nmethodology is evaluated through two case studies: (i) a study of the\ninfluence of the market risk premium on the stochastic volatility, and (ii) a\nstudy of the impact of the level of market risk on the volatility of the\nfin",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11494252873563218,
          "p": 0.28169014084507044,
          "f": 0.1632653020061642
        },
        "rouge-2": {
          "r": 0.01276595744680851,
          "p": 0.026785714285714284,
          "f": 0.017291061910655624
        },
        "rouge-l": {
          "r": 0.10919540229885058,
          "p": 0.2676056338028169,
          "f": 0.15510203670004175
        }
      }
    },
    {
      "paper_id": "physics.hist-ph.physics/hist-ph/2503.01617v1",
      "true_abstract": "Although several accounts of scientific understanding exist, the concept of\nunderstanding in relation to technology remains underexplored. This paper\naddresses this gap by proposing a philosophical account of technological\nunderstanding - the type of understanding that is required for and reflected by\nsuccessfully designing and using technological artefacts. We develop this\nnotion by building on the concept of scientific understanding. Drawing on\nparallels between science and technology, and specifically between scientific\ntheories and technological artefacts, we extend the idea of scientific\nunderstanding into the realm of technology. We argue that, just as scientific\nunderstanding involves the ability to explain a phenomenon using a theory,\ntechnological understanding involves the ability to use a technological\nartefact to realise a practical aim. Technological understanding can thus be\nconsidered a specific application of knowledge: it encompasses the cognitive\nskill of recognising how a practical aim can be achieved by using a\ntechnological artefact. In a context of design, this general notion of\ntechnological understanding is specified as the ability to design an artefact\nthat, by producing a phenomenon through its physical structure, achieves the\nintended aim. We illustrate our concept of technological understanding through\ntwo running examples: magnetic resonance imaging (MRI) and superconducting\nquantum computers. Our account highlights the epistemic dimension of engaging\nwith technology and, by allowing for context-dependent specifications, provides\nguidance for testing and improving technological understanding in specific\ncontexts.",
      "generated_abstract": "y of the quantum theory of gravity is a promising approach to the\nsolution of the so-called ``hierarchy problem'' and the problem of the\nrelationship between gravity and quantum mechanics. However, there is a\nsuspicion that the quantum theory of gravity may not be able to describe the\nactual gravitational interaction correctly. In particular, the quantum theory\nof gravity may be unable to predict the correct quantum mechanical properties\nof the gravitational field. The purpose of this paper is to examine this\nsuspicion by examining the quantum theory of gravity in the framework of\nquantum field theory. The quantum theory of gravity is a theory based on the\nfield theory of quantum field theory. The quantum field theory is the theory of\nthe interaction between the gravitational field and the particles of the\ninterest. In the quantum theory of gravity, the gravitational field is\ndescribed by a field. The quantum theory of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12698412698412698,
          "p": 0.2857142857142857,
          "f": 0.17582417156382088
        },
        "rouge-2": {
          "r": 0.005025125628140704,
          "p": 0.009523809523809525,
          "f": 0.006578942846479909
        },
        "rouge-l": {
          "r": 0.11904761904761904,
          "p": 0.26785714285714285,
          "f": 0.16483516057480993
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.12026v1",
      "true_abstract": "In this paper, we consider the impact of the order flow auction (OFA) in the\ncontext of the proposer-builder separation (PBS) mechanism through a\ngame-theoretic perspective. The OFA is designed to improve user welfare by\nredistributing maximal extractable value (MEV) to the users, in which two\nauctions take place: the order flow auction and the block-building auction. We\nformulate the OFA as a multiplayer game, and focus our analyses on the case of\ntwo competing players (builders). We prove the existence and uniqueness of a\nNash equilibrium for the two-player game, and derive a closed-form solution by\nsolving a quartic equation. Our result shows that the builder with a\ncompetitive advantage pays a relatively lower cost, leading to centralization\nin the builder space. In contrast, the proposer's shares evolve as a martingale\nprocess, which implies decentralization in the proposer (or, validator) space.\nOur analyses rely on various tools from stochastic processes, convex\noptimization, and polynomial equations. We also conduct numerical studies to\ncorroborate our findings, and explore other features of the OFA under the PBS\nmechanism.",
      "generated_abstract": "We study the trade-off between the social welfare achieved by an\noptimizer, $V_{\\text{opt}}$, and the social welfare achieved by the worst\noptimal algorithm, $V_{\\text{worst}}$. We prove that the worst-case\ntrade-off is at most $\\frac{1}{2}$. Our results generalize the results of\nVanhaverbeke and Vandewalle (2019) and Tian and Zhang (2023) and show that the\nworst-case trade-off is not always at most $\\frac{1}{2}$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08547008547008547,
          "p": 0.2702702702702703,
          "f": 0.12987012621943003
        },
        "rouge-2": {
          "r": 0.018292682926829267,
          "p": 0.06,
          "f": 0.028037379596471766
        },
        "rouge-l": {
          "r": 0.06837606837606838,
          "p": 0.21621621621621623,
          "f": 0.1038961002454041
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2503.06822v1",
      "true_abstract": "Clustering is a fundamental task in network analysis, essential for\nuncovering hidden structures within complex systems. Edge clustering, which\nfocuses on relationships between nodes rather than the nodes themselves, has\ngained increased attention in recent years. However, existing edge clustering\nalgorithms often overlook the significance of edge weights, which can represent\nthe strength or capacity of connections, and fail to account for noisy\nedges--connections that obscure the true structure of the network. To address\nthese challenges, the Weighted Edge Clustering Adjusting for Noise (WECAN)\nmodel is introduced. This novel algorithm integrates edge weights into the\nclustering process and includes a noise component that filters out spurious\nedges. WECAN offers a data-driven approach to distinguishing between meaningful\nand noisy edges, avoiding the arbitrary thresholding commonly used in network\nanalysis. Its effectiveness is demonstrated through simulation studies and\napplications to real-world datasets, showing significant improvements over\ntraditional clustering methods. Additionally, the R package ``WECAN'' has been\ndeveloped to facilitate its practical implementation.",
      "generated_abstract": "A statistical test is an evaluation of the null hypothesis by comparing the\ntest statistic with a known null distribution. We propose a method to determine\nwhether a null distribution exists for a test statistic in a given population\nand whether a test statistic has a specific value for a given population. We\nalso discuss the use of the test statistic in the context of hypothesis\ntesting and show how the test statistic can be used as an approximation to a\nstatistical test. We provide a simple proof of the existence of a null\ndistribution for the test statistic and derive a simple formula for the\nprobability of finding a value of the test statistic at the right endpoint of\nthe null distribution. We also derive a formula for the probability of\nfinding a value of the test statistic at the left endpoint of the null\ndistribution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09016393442622951,
          "p": 0.19642857142857142,
          "f": 0.12359550130539088
        },
        "rouge-2": {
          "r": 0.006329113924050633,
          "p": 0.01020408163265306,
          "f": 0.00781249527466106
        },
        "rouge-l": {
          "r": 0.07377049180327869,
          "p": 0.16071428571428573,
          "f": 0.10112359119303137
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.07978v4",
      "true_abstract": "This note introduces a doubly robust (DR) estimator for regression\ndiscontinuity (RD) designs. RD designs provide a quasi-experimental framework\nfor estimating treatment effects, where treatment assignment depends on whether\na running variable surpasses a predefined cutoff. A common approach in RD\nestimation is the use of nonparametric regression methods, such as local linear\nregression. However, the validity of these methods still relies on the\nconsistency of the nonparametric estimators. In this study, we propose the\nDR-RD estimator, which combines two distinct estimators for the conditional\nexpected outcomes. The primary advantage of the DR-RD estimator lies in its\nability to ensure the consistency of the treatment effect estimation as long as\nat least one of the two estimators is consistent. Consequently, our DR-RD\nestimator enhances robustness of treatment effect estimators in RD designs.",
      "generated_abstract": "We develop a novel methodology for identifying the unique intertemporal\nintervention of a government on the probability of a stock market crash. We\nexploit a randomized controlled experiment that we conducted in the United\nStates in 2014, in which we randomly assigned a stock market crash. We find\nthat the intervention reduces the probability of a crash by 10.4% compared to\nthe control group, which has a 10.7% probability of a crash. This reduction in\ncrash probability is driven by a 10.6% reduction in the probability of a\ncrash that occurred during the intervention. Using a difference-in-differences\nframework, we demonstrate that the intervention had a statistically significant\neffect on the probability of a crash and on the size of the crash.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1411764705882353,
          "p": 0.2,
          "f": 0.16551723652794306
        },
        "rouge-2": {
          "r": 0.01652892561983471,
          "p": 0.020833333333333332,
          "f": 0.01843317478986732
        },
        "rouge-l": {
          "r": 0.1411764705882353,
          "p": 0.2,
          "f": 0.16551723652794306
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/CB/2501.08356v1",
      "true_abstract": "Recent high-throughput experiments unveil substantial electrophysiological\ndiversity among uncoupled healthy myocytes under identical conditions. To\nquantify inter-cell variability, the values of a subset of the parameters in a\nwell-regarded mathematical model of the action potential of rabbit ventricular\nmyocytes are estimated from fluorescence voltage measurements of a large number\nof cells. Statistical inference yields a population of nearly 1200\ncell-specific model variants that, on a population-level replicate\nexperimentally measured biomarker ranges and distributions, and in contrast to\nearlier studies, also match experimental biomarker values on a cell-by-cell\nbasis. This model population may be regarded as a random sample from the\nphenotype of healthy rabbit ventricular myocytes. Uni-variate and bi-variate\njoint marginal distributions of the estimated parameters are presented, and the\nparameter dependencies of several commonly utilised electrophysiological\nbiomarkers are revealed. Parameter values are weakly correlated, while summary\nmetrics such as the action potential duration are not strongly dependent on any\nsingle electrophysiological characteristic of the myocyte. Our results\ndemonstrate the feasibility of accurately and efficiently fitting entire action\npotential waveforms at scale.\n  Keywords: cellular excitability, rabbit ventricular myocytes, fluorescence\nvoltage measurements, action potential waveform, parameter estimation in\ndifferential equations, noisy time series",
      "generated_abstract": "scientists at the University of California, Davis reported the\n discovery of a novel, self-assembling peptide that they named SARS-CoV-2\n ACE2-targeting peptide (SARS-CoV-2-ATP). Although the structure of this peptide\n has been determined, its function remains unknown. In this study, we present\n the first structure determination of a SARS-CoV-2-ATP-like peptide,\n SARS-CoV-2-ATP-C3. Using a combination of advanced structural and biochemical\n techniques, we determined the structure of SARS-CoV-2-ATP-C3 in complex with\n the ACE2 protein, the receptor that the virus uses to enter cells. This study\n reveals that SARS-CoV-2-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07086614173228346,
          "p": 0.14754098360655737,
          "f": 0.0957446764672931
        },
        "rouge-2": {
          "r": 0.0056179775280898875,
          "p": 0.0125,
          "f": 0.007751933705909458
        },
        "rouge-l": {
          "r": 0.06299212598425197,
          "p": 0.13114754098360656,
          "f": 0.08510637859495271
        }
      }
    },
    {
      "paper_id": "nlin.CG.nlin/CG/2502.13735v1",
      "true_abstract": "Two kinetic exchange models are proposed to explore the dynamics of closed\neconomic markets characterized by random exchanges, saving propensities, and\ncollective transactions. Model I simulates a system where individual\ntransactions occur among agents with saving tendencies, along with collective\ntransactions between groups. Model II restricts individual transactions to\nagents within the same group, but allows for collective transactions between\ngroups. A three-step trading process--comprising intergroup transactions,\nintragroup redistribution, and individual exchanges--is developed to capture\nthe dual-layered market dynamics. The saving propensity is incorporated using\nthe Chakraborti-Chakrabarti model, applied to both individual and collective\ntransactions. Results reveal that collective transactions increase wealth\ninequality by concentrating wealth within groups, as indicated by higher Gini\ncoefficients and Kolkata indices. In contrast, individual transactions across\ngroups mitigate inequality through more uniform wealth redistribution. The\ninterplay between saving propensities and collective transactions governs\ndeviation degree and entropy, which display inverse trends. Higher saving\npropensities lead to deviations from the Boltzmann-Gibbs equilibrium, whereas\nspecific thresholds result in collective transaction dominance, producing\nnotable peaks or troughs in these metrics. These findings underscore the\ncritical influence of dual-layered market interactions on wealth distribution\nand economic dynamics.",
      "generated_abstract": "igate the stability of the equilibrium points of the\nsystem\n\\begin{equation}\\label{eq:system}\n  \\begin{cases}\n  \\dot x_1=x_2+x_3,\\\\\n  \\dot x_2=-x_3+x_1,\\\\\n  \\dot x_3=-x_1-x_2.\n  \\end{cases}\n\\end{equation}\nThe parameter $\\alpha$ determines the strength of the bifurcation. We\nexamine the stability of the three equilibrium points. For $\\alpha=0$ we\nobtain a Hopf bifurcation, while for $\\alpha>0$ we observe a Hopf, Saddle,\nCritical (HSC), and Saddle-Critical (SC) bifurcation. We study the effects of\nthe parameter $\\beta$ on the stability of the equilibrium points.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06201550387596899,
          "p": 0.17391304347826086,
          "f": 0.09142856755330628
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.06201550387596899,
          "p": 0.17391304347826086,
          "f": 0.09142856755330628
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.11604v1",
      "true_abstract": "In this work, we revisit the estimation of the model parameters of a Weibull\ndistribution based on iid observations, using the maximum likelihood estimation\n(MLE) method which does not yield closed expressions of the estimators. Among\nother results, it has been shown analytically that the MLEs obtained by solving\nthe highly non-linear equations do exist (i.e., finite), and are unique. We\nthen proceed to study the sampling distributions of the MLEs through both\ntheoretical as well as computational means. It has been shown that the sampling\ndistributions of the two model parameters' MLEs can be approximated fairly well\nby suitable Weibull distributions too. Results of our comprehensive simulation\nstudy corroborate some recent results on the first-order bias and first-order\nmean squared error (MSE) expressions of the MLEs.",
      "generated_abstract": "uce the method of the mean squared error of the logarithmic\nregression (MSELR), which provides a new method for estimating the regression\ncoefficients of a logistic regression model. The MSELR is an extension of the\nordinary least squares estimator for regression, which can be used for a wide\nrange of models, including logistic regression models. In this paper, we\ninvestigate the properties of the MSELR. First, we prove that the MSELR is a\nconvex estimator. Second, we show that the MSELR is unbiased and asymptotically\nnormal. Third, we establish the consistency of the MSELR, and the asymptotic\nnormality of the MSELR, and the asymptotic covariance of the MSELR. Finally, we\nestablish the asymptotic normality of the MSELR. The asymptotic normality of the\nM",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17582417582417584,
          "p": 0.25806451612903225,
          "f": 0.2091503219770175
        },
        "rouge-2": {
          "r": 0.06086956521739131,
          "p": 0.07368421052631578,
          "f": 0.0666666617120185
        },
        "rouge-l": {
          "r": 0.16483516483516483,
          "p": 0.24193548387096775,
          "f": 0.1960784265521809
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.physics/space-ph/2503.05358v1",
      "true_abstract": "Comets are the most pristine planetesimals left from the formation of the\nSolar System. They carry unique information on the materials and the physical\nprocesses which led to the presence of planets and moons. Many important\nquestions about cometary physics, such as origin, constituents and mechanism of\ncometary activity, remain unanswered. The next perihelion of comet 1P/Halley,\nin 2061, is an excellent opportunity to revisit this object of outstanding\nscientific and cultural relevance. In 1986, during its latest approach to the\nSun, several flyby targeted Halley's comet to observe its nucleus and shed\nlight on its properties, origin, and evolution. However, due to its retrograde\norbit and high ecliptic inclination, the quality of data was limited by the\nlarge relative velocity and short time spent by the spacecraft inside the coma\nof the comet. A rendezvous mission like ESA/Rosetta would overcome such\nlimitations, but the trajectory design is extremely challenging due to the\nshortcomings of current propulsion technology. Given the considerable lead\ntimes of spacecraft development and the long duration of the interplanetary\ntransfer required to reach the comet, it is imperative to start mission\nplanning several decades in advance. This study presents a low-thrust\nrendezvous strategy to reach the comet before the phase of intense activity\nduring the close approach to the Sun. The trajectory design combines a\ngravity-assist maneuver with electric propulsion arcs to maximize scientific\npayload mass while constraining transfer duration. A propulsive plane change\nmaneuver would be prohibitive. To keep the propellant budget within reasonable\nlimits, most of the plane change maneuver is achieved via either a Jupiter or a\nSaturn flyby. The interplanetary low-thrust gravity-assisted trajectory design\nstrategy is described, followed by the presentation of multiple\nproof-of-concept solutions.",
      "generated_abstract": "-light spectroscopic observations of the young star cluster\ncl05-202 are presented, as part of the Southern Sky Survey (S3S). The\nobservations were carried out with the 1.5 m MATKA telescope at the La Palma\nObservatory (Canary Islands, Spain) in March 2024 and April 2025. We have\nobtained 72 spectra of the cluster members, with a resolution of R = 22,000\n(FWHM = 5.64 km s-1). We have identified 23 objects, 18 of which are new\ndetected members of the cluster. The spectrum of the youngest star,\ncl05-202-001, is shown in Fig. 1. The spectrum of cl05-202-001 is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06111111111111111,
          "p": 0.15492957746478872,
          "f": 0.08764939833335997
        },
        "rouge-2": {
          "r": 0.003787878787878788,
          "p": 0.011111111111111112,
          "f": 0.00564971372211305
        },
        "rouge-l": {
          "r": 0.06111111111111111,
          "p": 0.15492957746478872,
          "f": 0.08764939833335997
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2412.15225v1",
      "true_abstract": "Biochemical reaction networks are typically modeled by $\\dfrac{dx}{dt}=N\\cdot\nK(x)=Y\\cdot I_a\\cdot K(x)$, with $x$ and $K(x)$ as the concentration and rate\nvectors, respectively, and $N$, $Y$, and $I_a$ as the stoichiometric,\nmolecularity, and incidence matrices, respectively. Steady states, which\ndescribe their long-term behaviors, are determined by solving $N\\cdot K(x)=0$,\nwhile complex balanced steady states are found by solving $I_a \\cdot K(x)=0$.\nTo investigate these complex networks, decomposition techniques are important,\nin particular, for computing steady states. Previously, we identified a\nwidespread property across many networks: the existence of independent and\nincidence-independent decompositions, characterized by the ability to directly\nsum the stoichiometric and incidence matrices of the subnetworks, respectively,\nto match those of the entire network. Here, we discover the ubiquitous property\nthat we call the Finest Decomposition Coarsening (FDC), where the finest\nindependent decomposition (FID) is a coarsening of the finest\nincidence-independent decomposition (FIID). To support the analysis of this\nproperty, we introduce a MATLAB package designed to compute both these\ndecompositions. We then characterize the FDC property and its relationship to\nstructural factors such as the invertibility of the molecularity matrix. We\nalso introduce and characterize the Finest Decompositions Equality (FDE)\nproperty, where FIID equals FID. Notably, we show that all deficiency zero\nnetworks exhibit the FDE property. Furthermore, we establish important\nrelationships of the FID and FIID with decomposition of the network into its\nconnected components. Our results highlight the prevalence of the coarsening\nproperty in reaction networks and deepens the understanding of the algebraic\nstructure and dynamics of biochemical networks.",
      "generated_abstract": "f machine learning and artificial intelligence (AI) has revolutionized\nthe practice of molecular biology and genomics. However, the rapid advancement\nof these technologies has also highlighted the need for robust and scalable\ndata processing methods. This paper explores the use of a novel, scalable\nframework for large-scale data analysis, the Ripple Model. The Ripple Model\nprovides a framework for analyzing large data sets, enabling efficient and\nscalable computational operations while maintaining high accuracy and\nreproducibility. The Ripple Model combines the power of GPUs with the\nflexibility of Python to provide a robust and scalable solution for large-scale\ndata analysis. The Ripple Model offers a practical and efficient way to handle\nlarge-scale data analysis, providing a flexible and scalable solution for\nanalyzing large datasets. The Ripple Model offers a practical and efficient",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07096774193548387,
          "p": 0.15492957746478872,
          "f": 0.09734512843409841
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.05806451612903226,
          "p": 0.1267605633802817,
          "f": 0.07964601338985065
        }
      }
    },
    {
      "paper_id": "math.CA.math/CA/2503.04604v1",
      "true_abstract": "In this paper, we explore the relationship between the operators mapping\natoms to molecules in local Hardy spaces $h^p(\\mathbb{R}^n)$ and the size\nconditions of its kernel. In particular, we show that if the kernel a\nCalder\\'on--Zygmund-type operator satisfies an integral-type size condition and\na $T^*-$type cancellation, then the operator maps $h^p(\\mathbb{R}^n)$ atoms to\nmolecules. On the other hand, assuming that $T$ is an integral type operator\nbounded on $L^2(\\mathbb{R}^n)$ that maps atoms to molecules in\n$h^p(\\mathbb{R}^n)$, then the kernel of such operator satisfies the same\nintegral-type size conditions. We also provide the $L^1(\\mathbb{R}^n)$ to\n$L^{1,\\infty}(\\mathbb{R}^n)$ boundedness for such operators connecting our\nintegral-type size conditions on the kernel with others presented in the\nliterature.",
      "generated_abstract": "We prove that the local entropy of the Markov chain $X_n$ converges to\nthe local entropy of the Markov chain $X_n^c$ as $n\\to\\infty$. We show that\nthe local entropy of $X_n^c$ converges to the Shannon entropy of a\nrandom variable, which is a convex combination of the local entropy of\n$X_n$ and that of $X_n^c$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15151515151515152,
          "p": 0.4,
          "f": 0.21978021579519388
        },
        "rouge-2": {
          "r": 0.010101010101010102,
          "p": 0.027777777777777776,
          "f": 0.014814810903704737
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.36,
          "f": 0.19780219381717193
        }
      }
    },
    {
      "paper_id": "math.NT.cs/FL/2503.00959v2",
      "true_abstract": "The Riemann zeta function, and more generally the L-functions of Dirichlet\ncharacters, are among the central objects of study in number theory. We report\non a project to formalize the theory of these objects in Lean's \"Mathlib\"\nlibrary, including a proof of Dirichlet's theorem on primes in arithmetic\nprogressions and a formal statement of the Riemann hypothesis",
      "generated_abstract": "aper, we introduce a new model for the study of finite automata with\nthe minimal number of states, namely the minimal finite automaton (MFA). This\nmodel, which we call the minimal MFA model, is based on a generalized\nconfiguration space and is designed to allow for a more concise presentation of\nthe behavior of a finite automaton than the usual configuration space. In this\npaper, we first present a complete characterization of the behavior of a finite\nautomaton under the MFA model, which is a refinement of the characterization\npresented in the previous work by the first author. We also present a number of\nequivalences between the MFA model and other known models of finite automata. We\nthen investigate a number of properties of the MFA model and its variants.\nFinally, we use the MFA model to prove the correctness of a new deterministic\nalgorithm for the generation of the finite autom",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2682926829268293,
          "p": 0.14666666666666667,
          "f": 0.18965516784334138
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.01652892561983471,
          "f": 0.022598865730793383
        },
        "rouge-l": {
          "r": 0.24390243902439024,
          "p": 0.13333333333333333,
          "f": 0.17241378853299655
        }
      }
    },
    {
      "paper_id": "math.AG.math/AG/2503.09133v1",
      "true_abstract": "The usual approach to tropical geometry is via degeneration of amoebas of\nalgebraic subvarieties of an algebraic torus $(\\mathbb{C}^*)^n$. An amoeba is\nlogarithmic projection of the variety forgetting the angular part of\ncoordinates, called the phase. Similar degeneration can be performed without\nignoring the phase. The limit then is called phase tropical variety, and it is\na powerful tool in numerous areas. In the article is described a\nnon-commutative version of phase tropicalization in the simplest case of the\nmatrix group $PSL_2(\\mathbb{C})$, replacing here $(\\mathbb{C}^*)^n$ in the\nclassical approach.",
      "generated_abstract": "This paper presents a new proof of the existence of a weak solution to a\nnonlinear fluid-structure model. We consider a fluid-structure model (FEM)\nformulation of the wave equation with an inhomogeneous viscosity that depends\non the deformation gradient. We introduce a regularized functional, and prove\nthat the minimizer of this functional is a weak solution to the considered\nproblem. The regularity of the minimizer is then established. The proof uses\nthe trace inequality, the Harnack inequality, and a regularity result for the\nwave equation with a nonhomogeneous nonlinearity. This result is applied to\nshow the existence of a weak solution to the considered problem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15254237288135594,
          "p": 0.16363636363636364,
          "f": 0.15789473184826117
        },
        "rouge-2": {
          "r": 0.023529411764705882,
          "p": 0.023529411764705882,
          "f": 0.023529406764706943
        },
        "rouge-l": {
          "r": 0.11864406779661017,
          "p": 0.12727272727272726,
          "f": 0.12280701255001558
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/MF/2503.08833v1",
      "true_abstract": "We study optimal execution in markets with transient price impact in a\ncompetitive setting with $N$ traders. Motivated by prior negative results on\nthe existence of pure Nash equilibria, we consider randomized strategies for\nthe traders and whether allowing such strategies can restore the existence of\nequilibria. We show that given a randomized strategy, there is a non-randomized\nstrategy with strictly lower expected execution cost, and moreover this\nde-randomization can be achieved by a simple averaging procedure. As a\nconsequence, Nash equilibria cannot contain randomized strategies, and\nnon-existence of pure equilibria implies non-existence of randomized\nequilibria. Separately, we also establish uniqueness of equilibria. Both\nresults hold in a general transaction cost model given by a strictly positive\ndefinite impact decay kernel and a convex trading cost.",
      "generated_abstract": "uce a novel method for portfolio selection based on the\nprincipal-agent problem, where a portfolio manager manages a set of assets\nunder the constraints of a limited budget. The portfolio manager aims to\noptimize the portfolio's risk-return characteristics with respect to the\nbudget constraint. We develop a closed-form solution for the optimal\nportfolio, as well as a practical algorithm for computing the optimal\nportfolio. Our results demonstrate that the portfolio manager's budget can be\noptimized through a convex optimization problem, while the optimal portfolio\nis a non-convex mixture of optimal investments. The portfolio manager's\nobjective is a linear function of the portfolio's realized risk and return. We\ninvestigate the performance of the portfolio manager's optimal portfolio\nthrough a simulation study, where we compare the performance of the optimal\nportfolio with that of an efficient market hypothesis",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1951219512195122,
          "p": 0.21052631578947367,
          "f": 0.20253164057683076
        },
        "rouge-2": {
          "r": 0.0423728813559322,
          "p": 0.042735042735042736,
          "f": 0.04255318648945283
        },
        "rouge-l": {
          "r": 0.1951219512195122,
          "p": 0.21052631578947367,
          "f": 0.20253164057683076
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.q-bio/PE/2503.07083v2",
      "true_abstract": "Motivated by the paradigm of a super-Maltusian population catastrophe, we\nstudy a simple stochastic population model which exhibits a finite-time blowup\nof the population size and is strongly affected by intrinsic noise. We focus on\nthe fluctuations of the blowup time $T$ in the asexual binary reproduction\nmodel $2A \\to 3A$, where two identical individuals give birth to a third one.\nWe determine exactly the average blowup time as well as the probability\ndistribution $\\mathcal{P}(T)$ of the blowup time and its moments. In\nparticular, we show that the long-time tail $\\mathcal{P}(T\\to \\infty)$ is\npurely exponential. The short-time tail $\\mathcal{P}(T\\to 0)$ exhibits an\nessential singularity at $T=0$, and it is dominated by a single (the most\nlikely) population trajectory which we determine analytically.",
      "generated_abstract": "of the phase diagram of the quantum Ising model in the presence of\nthe magnetic field provides valuable insights into the interaction of the\nmagnetic field with the quantum system, and its role in shaping the phase\ndiagram. However, the study of the phase diagram in the presence of the\nmagnetic field has received less attention. In this paper, we study the phase\ndiagram of the quantum Ising model in the presence of the magnetic field for\nthe case of a single ion and for a system with a finite number of ions,\ndemonstrating that the magnetic field can play a significant role in shaping\nthe phase diagram. We focus on the two-dimensional case, demonstrating the\nexistence of a critical point at finite values of the magnetic field and\ndiscovering a rich phase diagram, which includes a ferromagnetic phase, a\nparamagnetic phase, and a phase transition to a non-mag",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21686746987951808,
          "p": 0.26865671641791045,
          "f": 0.239999995056889
        },
        "rouge-2": {
          "r": 0.09565217391304348,
          "p": 0.10377358490566038,
          "f": 0.09954750632050965
        },
        "rouge-l": {
          "r": 0.21686746987951808,
          "p": 0.26865671641791045,
          "f": 0.239999995056889
        }
      }
    },
    {
      "paper_id": "physics.ed-ph.physics/ed-ph/2502.13774v1",
      "true_abstract": "In this work, we present a teaching strategy implemented in Introduction to\nPhysics, corresponding to the first year of the Physics Teacher Degree at the\nNational University of Rosario, whose main purpose is to provide students with\ntools to understand problem statements and exercises and to incorporate habits\nthat favor their resolution and communication. For this purpose, we implemented\nthe use of certain problemsolving algorithms, which we call\nHopscotch-Algorithms, appealing to the image of this popular game in which\nsteps can be skipped, rearranged or simultaneously executed. The aim is to\nstimulate a work method in a critical and problematized way, avoiding rigid or\ndogmatic applications of resolution steps. The testimonies collected indicate\nthat the strategy was positively valued by the students.",
      "generated_abstract": "r presents an introduction to the fundamental principles of quantum\nphysics, using the theory of quantum many-body systems. Topics covered include\nthe nature of quantum systems, the Schr\\\"odinger equation, the wave function\nand energy spectrum, and the Heisenberg uncertainty principle. The paper\nexplains the relationship between quantum physics and classical physics, and\nhighlights the differences between quantum and classical systems. The\nprinciples of quantum mechanics are explored through the use of example\nproblems, including the Schr\\\"odinger equation and the uncertainty\nprinciple. The book is intended for students and educators who are unfamiliar\nwith quantum physics. The book is written in a clear and accessible style, and\nuses examples and visual aids to make the concepts more accessible to\nreaders. The book also includes a companion website, which includes\ninteractive and quiz-based exercises to reinforce the concepts discussed in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15294117647058825,
          "p": 0.16049382716049382,
          "f": 0.15662650102699974
        },
        "rouge-2": {
          "r": 0.03361344537815126,
          "p": 0.03278688524590164,
          "f": 0.0331950157476635
        },
        "rouge-l": {
          "r": 0.1411764705882353,
          "p": 0.14814814814814814,
          "f": 0.1445783082559154
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/RM/2410.11849v1",
      "true_abstract": "In this paper, we investigate a complex variation of the standard joint life\nannuity policy by introducing three distinct contingent benefits for the\nsurviving member(s) of a couple, along with a contingent benefit for their\nbeneficiaries if both members pass away. Our objective is to price this\ninnovative insurance policy and analyse its sensitivity to key model\nparameters, particularly those related to the joint mortality framework. We\nemploy the $QP$-rule (described in Section \\ref{secgenset}), which combines the\nreal-world probability measure $P$ for mortality risk with risk-neutral\nvaluation under $Q$ for financial market risks. The model enables explicit\npricing expressions, computed using efficient numerical methods. Our results\nhighlight the interdependent risks faced by couples, such as broken-heart\nsyndrome, providing valuable insights for insurers and policyholders regarding\nthe pricing influences of these factors.",
      "generated_abstract": "We propose a novel stochastic volatility model for the term structure of\ntraditional asset prices, focusing on the high-frequency trading regime. The\nmodel is based on the generalized It\\^{o} formula and is characterized by a\nnon-linear and non-Gaussian diffusion term. We apply the model to the S\\&P500\nand the F\\&O index, and demonstrate its good performance in predicting the\nterminal price of these two indices. The results indicate that the stochastic\nvolatility model can effectively capture the high-frequency trading dynamics of\nthese two indices, which is consistent with the findings of previous research.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17475728155339806,
          "p": 0.2903225806451613,
          "f": 0.21818181349054191
        },
        "rouge-2": {
          "r": 0.03076923076923077,
          "p": 0.047058823529411764,
          "f": 0.03720929754461933
        },
        "rouge-l": {
          "r": 0.14563106796116504,
          "p": 0.24193548387096775,
          "f": 0.1818181771269055
        }
      }
    },
    {
      "paper_id": "quant-ph.physics/atom-ph/2503.05664v1",
      "true_abstract": "We experimentally and theoretically study collective emission of a dense\natomic ensemble coupled to a whispering-gallery-mode (WGM) in a nanophotonic\nmicroring resonator. Due to many cold atoms localized in a small volume, these\ntrapped atoms collectively couple not only to the WGM and but also to the\nnon-guided modes in free space. Through tuning the atom-WGM coupling and by\nadjusting the number of trapped atoms, we demonstrate superradiant emission to\nthe WGM. For photon emission via the non-guided modes, our study reveals\nsignatures of subradiance and superradiance when the system is driven to the\nsteady-state states and the timed-Dicke states, respectively. Our experimental\nplatform thus presents the first atom-light interface with selective collective\nemission behavior into a guided mode and the environment, respectively. Our\nobservation and methodology could shed light on future explorations of\ncollective emission with densely packed quantum emitters coupled to\nnanophotonic light-matter interfaces.",
      "generated_abstract": "We propose a new method for the generation of coherent laser fields with\nno external driving, which allows for a large range of applications in\nquantum optics. Our method relies on the nonlinear Schr\\\"odinger equation\nsolved via the Tikhonov regularization. We derive the solution for a general\nnonlinear Schr\\\"odinger equation with an arbitrary number of degrees of\nfreedom, without the need to assume the existence of a particular form for the\nsolution. We demonstrate our method by applying it to the Schr\\\"odinger equation\nfor the harmonic oscillator and to the Schr\\\"odinger equation for a\nquadratic-harmonic oscillator. In both cases, we find that the solution can be\nfound analytically.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1717171717171717,
          "p": 0.25,
          "f": 0.20359280954354775
        },
        "rouge-2": {
          "r": 0.029850746268656716,
          "p": 0.0425531914893617,
          "f": 0.0350877144521398
        },
        "rouge-l": {
          "r": 0.15151515151515152,
          "p": 0.22058823529411764,
          "f": 0.17964071373516455
        }
      }
    },
    {
      "paper_id": "math.QA.math/QA/2503.05960v2",
      "true_abstract": "A parametrized Yang-Baxter equation is a map from a group to a set of\nR-matrices, satisfying the Yang-Baxter commutation relation. For the six-vertex\nmodel, there are two main regimes of the Yang-Baxter equation: the\nfree-fermionic point, and everything else. For the free-fermionic point, there\nexists a parametrized Yang-Baxter equation with a large parameter group\nGL(2)xGL(1). For non-free-fermionic six-vertex matrices, there are also\nparametrized Yang-Baxter equations, but these do not account for all possible\ninteractions. Instead we will construct a groupoid parametrized Yang-Baxter\nequation that does reflect all possible Yang-Baxter equations in the six-vertex\nmodel.",
      "generated_abstract": "We construct a new class of modular forms on the modular group $SL_2(\\mathbb{Z})$\nwhich are not in the category of modular forms of weight $k$ and level $N$. We\nprove that such forms are in fact modular forms of weight $-2k+1$ and level\n$N^{\\frac{k+1}{2}}$. Our construction also provides a new class of\n$SL_2(\\mathbb{Z})$-modular forms for which we obtain an explicit formula for the\n$k$-th Fourier coefficients. As a by-product, we prove that the Fourier\ncoefficients of the new modular forms are in bijection with certain Young\ndiagrams, a result which was conjectured by Bader and H\\\"ull.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23333333333333334,
          "p": 0.2545454545454545,
          "f": 0.2434782558790171
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.037037037037037035,
          "f": 0.03703703203703771
        },
        "rouge-l": {
          "r": 0.21666666666666667,
          "p": 0.23636363636363636,
          "f": 0.22608695153119107
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.cond-mat/soft/2503.09056v1",
      "true_abstract": "Long range order and symmetry in heterogeneous materials architected on\ncrystal lattices lead to elastic and inelastic anisotropies and thus limit\nmechanical functionalities in particular crystallographic directions. Here, we\npresent a facile approach for designing heterogeneous disordered materials that\nexhibit nearly isotropic mechanical resilience and energy dissipation\ncapabilities. We demonstrate, through experiments and numerical simulations on\n3D-printed prototypes, that near-complete isotropy can be attained in the\nproposed heterogeneous materials with a small, finite number of random spatial\npoints. We also show that adding connectivity between random subdomains leads\nto much enhanced elastic stiffness, plastic strength, energy dissipation, shape\nrecovery, structural stability and reusability in our new heterogeneous\nmaterials. Overall, our study opens avenues for the rational design of a new\nclass of heterogeneous materials with isotropic mechanical functionalities for\nwhich the engineered disorder throughout the subdomains plays a crucial role.",
      "generated_abstract": "In this paper, we investigate the effects of the surface traction on the\npercolation of a random array of elastic rods. We find that the presence of\nsurface tractions can lead to a change in the critical behavior of the percolation\nprobability. For a fixed value of the percolation threshold, the critical\nexponent $\\alpha$ is modified by the presence of the surface tractions. In\nparticular, for the Kaup-Thomson model, where the percolation threshold is\nproportional to $\\rho$, we find that the critical exponent $\\alpha$ is\nsignificantly modified by the surface tractions. In contrast, for the\nKruskal-Narasimhan-Seshadri model, where the percolation threshold is\nproportional to $\\rho^2$, the critical exponent $\\alpha$ remains constant for\nall values of the surface tractions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.2545454545454545,
          "f": 0.18300653134264608
        },
        "rouge-2": {
          "r": 0.03007518796992481,
          "p": 0.04938271604938271,
          "f": 0.037383172865316346
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.2545454545454545,
          "f": 0.18300653134264608
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.15718v1",
      "true_abstract": "It is widely assumed that increases in economic productivity necessarily lead\nto economic growth. In this paper, it is shown that this is not always the\ncase. An idealized model of an economy is presented in which a new technology\nallows capital to be utilized autonomously without labor input. This is\nmotivated by the possibility that advances in artificial intelligence (AI) will\ngive rise to AI agents that act autonomously in the economy. The economic model\ninvolves a single profit-maximizing firm which is a monopolist in the product\nmarket and a monopsonist in the labor market. The new automation technology\ncauses the firm to replace labor with capital in such a way that its profit\nincreases while total production decreases. The model is not intended to\ncapture the structure of a real economy, but rather to illustrate how basic\neconomic mechanisms can give rise to counterintuitive and undesirable outcomes.",
      "generated_abstract": "l economy is increasingly reliant on artificial intelligence (AI)\nfor innovation and productivity growth. However, the impact of AI on labor\nmarket outcomes remains poorly understood. This paper examines the impact of\nAI on wage inequality in the United States, focusing on the labor market for\nthe skilled workforce. Using a difference-in-differences (DiD) approach, we\nfind that AI-driven technological change has led to a significant increase in\nwage inequality among skilled workers. The DiD effect is driven by a wage\nreduction for those who are displaced by AI, which is particularly pronounced\namong those with low levels of education and low skill levels. These findings\nhighlight the importance of employers' actions in addressing AI-driven wage\ngrowth, as they can exacerbate existing wage inequality. The find",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2608695652173913,
          "p": 0.27906976744186046,
          "f": 0.26966291635399575
        },
        "rouge-2": {
          "r": 0.04895104895104895,
          "p": 0.05982905982905983,
          "f": 0.05384614889615431
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.26744186046511625,
          "f": 0.25842696129781595
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/TO/2502.08062v1",
      "true_abstract": "We present a machine learning approach for predicting the organisation of\ncorneal, glial and fibroblast cells in 3D cultures used for tissue engineering.\nOur machine-learning-based method uses a powerful generative adversarial\nnetwork architecture called pix2pix, which we train using results from\nbiophysical contractile network dipole orientation (CONDOR) simulations. In the\nfollowing, we refer to the machine learning method as the RAPTOR (RApid\nPrediction of Tissue ORganisation) approach. A training data set containing a\nrange of CONDOR simulations is created, covering a range of underlying model\nparameters. Validation of the trained neural network is carried out by\ncomparing predictions with cultured glial, corneal, and fibroblast tissues,\nwith good agreements for both CONDOR and RAPTOR approaches. An approach is\ndeveloped to determine CONDOR model parameters for specific tissues using a fit\nto tissue properties. RAPTOR outputs a variety of tissue properties, including\ncell densities of cell alignments and tension. Since it is fast, it could be\nvaluable for the design of tethered moulds for tissue growth.",
      "generated_abstract": "very of the biological importance of the RNA world, and the\nformation of ribosomes, is often described in terms of a sequence of\nrandom-walk-like events. However, the mathematical description of the\nbiological process is not simple and straightforward. This article is an\nintroduction to the mathematical framework and tools used to study the\nbiological process of ribosome formation. It starts with a review of\nmolecular dynamics simulations of the ribosome and the mechanism of\nself-assembly of the ribosome. The importance of the interaction between\nRNA and RNA in the formation of the ribosome is also discussed. Next, the\nmathematical model is extended to a biological model of ribosome formation in\nthe cell. It is shown that the biological process can be reduced to the\nequilibrium statistical mechanics of the model. The final section of the\narticle introduces some aspects of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11926605504587157,
          "p": 0.19402985074626866,
          "f": 0.14772726801200944
        },
        "rouge-2": {
          "r": 0.012738853503184714,
          "p": 0.017543859649122806,
          "f": 0.014760142727360984
        },
        "rouge-l": {
          "r": 0.11009174311926606,
          "p": 0.1791044776119403,
          "f": 0.13636363164837312
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/TO/2410.06002v1",
      "true_abstract": "The understanding of the mechanisms driving vascular development is still\nlimited. Techniques to generate vascular trees synthetically have been\ndeveloped to tackle this problem. However, most algorithms are limited to\nsingle trees inside convex perfusion volumes. We introduce a new framework for\ngenerating multiple trees inside general nonconvex perfusion volumes. Our\nframework combines topology optimization and global geometry optimization into\na single algorithmic approach. Our first contribution is defining a baseline\nproblem based on Murray's original formulation, which accommodates efficient\nsolution algorithms. The problem of finding the global minimum is cast into a\nnonlinear optimization problem (NLP) with merely super-linear solution effort.\nOur second contribution extends the NLP to constrain multiple vascular trees\ninside any nonconvex boundary while avoiding intersections. We test our\nframework against a benchmark of an anatomic region of brain tissue and a\nvasculature of the human liver. In all cases, the total tree energy is improved\nsignificantly compared to local approaches. By avoiding intersections globally,\nwe can reproduce key physiological features such as parallel running inflow\nvessels and tortuous vessels. The ability to generate non-intersecting vascular\ntrees inside nonconvex organs can improve the functional assessment of organs.",
      "generated_abstract": "hat the motion of the Sun in its orbit around the Galactic Center is\ndriven by the dynamical friction of the Galactic Center's central black hole,\nwhich is a 500 times more massive than the Sun. The dynamical friction is\nexpressed as a torque and the resulting torque on the Sun is calculated using\nthe gravitational potential of the Galactic Center. We show that the Sun\napproaches the Galactic Center at a rate that is several orders of magnitude\nhigher than the observed rate of the Sun's perihelion advance. We show that\nthe Sun's orbit around the Galactic Center is highly eccentric, with a mean\neccentricity of 0.03, and the orbit is tilted with respect to the Galactic\nplane by 20 degrees. The Sun's mean motion is 2.36 days, and the mean",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10317460317460317,
          "p": 0.19117647058823528,
          "f": 0.1340206140036137
        },
        "rouge-2": {
          "r": 0.005555555555555556,
          "p": 0.009259259259259259,
          "f": 0.006944439756947608
        },
        "rouge-l": {
          "r": 0.10317460317460317,
          "p": 0.19117647058823528,
          "f": 0.1340206140036137
        }
      }
    },
    {
      "paper_id": "hep-th.hep-th/2503.10584v1",
      "true_abstract": "We investigate the shear viscosity and butterfly velocity of a magnetic\nfield-induced quantum phase transition in five dimensional\nEinstein-Maxwell-Chern-Simons theory, which is holographically dual to a class\nof strongly coupled quantum field theories with chiral anomalies. Our analysis\nreveals that the ratio of longitudinal shear viscosity to entropy density\n$\\eta_\\parallel/s$ exhibits a pronounced non-monotonic dependence on\ntemperature $T$ when the magnetic field $B$ is slightly below the critical\nvalue $B_c$ of the quantum phase transition. In particular, it can develop a\ndistinct minimum at an intermediate temperature. This contrasts sharply with\nthe monotonic temperature scaling observed at and above $B_c$, where\n$\\eta_\\parallel/s$ follows the scaling $T^{2/3}$ at $B=B_c$ and transitions to\n$T^2$ for $B>B_c$ as $T\\to0$. The non-vanishing of $\\eta_\\parallel/s$ for\n$B<B_c$ in the zero temperature limit suggests that it could serve as a good\norder parameter of the quantum phase transition. We also find that all\nbutterfly velocities change dramatically near the quantum phase transition, and\nthus their derivatives with respect to $B$ can be independently used to detect\nthe quantum critical point.",
      "generated_abstract": "t a novel method to construct solutions for the Einstein-Hilbert\naction in the presence of a non-trivial background scalar field. The method\nrelies on the use of the de Sitter invariant entropy function for the\ndetermination of the null energy condition (NEC) of the metric. This\ndetermination is performed in a way that allows us to directly compare the\nresulting entropy function to that of the Schwarzschild-de Sitter black hole.\nThe method is illustrated through the study of the Einstein-Hilbert action\nwith a non-trivial background scalar field. It is shown that the entropy\nfunction corresponding to the Schwarzschild-de Sitter black hole can be\nreplaced by the de Sitter invariant entropy function. Furthermore, it is shown\nthat the entropy function corresponding to a black string can be replaced by\nthe entropy function corresponding to a Schwarzschild black hole.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15178571428571427,
          "p": 0.2786885245901639,
          "f": 0.1965317873420429
        },
        "rouge-2": {
          "r": 0.03680981595092025,
          "p": 0.0625,
          "f": 0.046332041666642294
        },
        "rouge-l": {
          "r": 0.13392857142857142,
          "p": 0.2459016393442623,
          "f": 0.17341040005880595
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2503.06348v1",
      "true_abstract": "Real-time computer-based accompaniment for human musical performances entails\nthree critical tasks: identifying what the performer is playing, locating their\nposition within the score, and synchronously playing the accompanying parts.\nAmong these, the second task (score following) has been addressed through\nmethods such as dynamic programming on string sequences, Hidden Markov Models\n(HMMs), and Online Time Warping (OLTW). Yet, the remarkably successful\ntechniques of Deep Learning (DL) have not been directly applied to this\nproblem.\n  Therefore, we introduce HeurMiT, a novel DL-based score-following framework,\nutilizing a neural architecture designed to learn compressed latent\nrepresentations that enables precise performer tracking despite deviations from\nthe score. Parallelly, we implement a real-time MIDI data augmentation toolkit,\naimed at enhancing the robustness of these learned representations.\nAdditionally, we integrate the overall system with simple heuristic rules to\ncreate a comprehensive framework that can interface seamlessly with existing\ntranscription and accompaniment technologies.\n  However, thorough experimentation reveals that despite its impressive\ncomputational efficiency, HeurMiT's underlying limitations prevent it from\nbeing practical in real-world score following scenarios. Consequently, we\npresent our work as an introductory exploration into the world of DL-based\nscore followers, while highlighting some promising avenues to encourage future\nresearch towards robust, state-of-the-art neural score following systems.",
      "generated_abstract": "text (A2T) models have achieved significant progress in speech\ntext\ntranslation (STT). However, they are limited by the high cost of text generation\nand the lack of attention mechanisms to preserve speaker identity. In this\npaper, we propose SPECTA, a transformer-based model for A2T that enhances\nspeaker-informed attention through a dual-attention module and\nself-attention-based attention mechanism. Specifically, the dual-attention\nmodule uses a multi-head self-attention mechanism to integrate the self-text\nand speaker-text information, and the self-attention-based attention module\nemploys a multi-head self-attention mechanism to capture the speaker-text\ninteractions. Experimental results on STT benchmarks demonstrate that SPECTA\nachieves state-of-the-art performance across different metrics, particularly in\npreserving",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0975609756097561,
          "p": 0.21621621621621623,
          "f": 0.13445377722759708
        },
        "rouge-2": {
          "r": 0.005025125628140704,
          "p": 0.010416666666666666,
          "f": 0.0067796566264894855
        },
        "rouge-l": {
          "r": 0.09146341463414634,
          "p": 0.20270270270270271,
          "f": 0.12605041588305926
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.08587v1",
      "true_abstract": "The increasing use of children's automatic speech recognition (ASR) systems\nhas spurred research efforts to improve the accuracy of models designed for\nchildren's speech in recent years. The current approach utilizes either\nopen-source speech foundation models (SFMs) directly or fine-tuning them with\nchildren's speech data. These SFMs, whether open-source or fine-tuned for\nchildren, often exhibit higher word error rates (WERs) compared to adult\nspeech. However, there is a lack of systemic analysis of the cause of this\ndegraded performance of SFMs. Understanding and addressing the reasons behind\nthis performance disparity is crucial for improving the accuracy of SFMs for\nchildren's speech. Our study addresses this gap by investigating the causes of\naccuracy degradation and the primary contributors to WER in children's speech.\nIn the first part of the study, we conduct a comprehensive benchmarking study\non two self-supervised SFMs (Wav2Vec2.0 and Hubert) and two weakly supervised\nSFMs (Whisper and MMS) across various age groups on two children speech\ncorpora, establishing the raw data for the causal inference analysis in the\nsecond part. In the second part of the study, we analyze the impact of\nphysiological factors (age, gender), cognitive factors (pronunciation ability),\nand external factors (vocabulary difficulty, background noise, and word count)\non SFM accuracy in children's speech using causal inference. The results\nindicate that physiology (age) and particular external factor (number of words\nin audio) have the highest impact on accuracy, followed by background noise and\npronunciation ability. Fine-tuning SFMs on children's speech reduces\nsensitivity to physiological and cognitive factors, while sensitivity to the\nnumber of words in audio persists.\n  Keywords: Children's ASR, Speech Foundational Models, Causal Inference,\nPhysiology, Cognition, Pronunciation",
      "generated_abstract": "r presents a novel deep learning model for the automatic generation\nof audio-visual summaries for music. The proposed approach, dubbed Audio-Visual\nMusic Summarization (AVMS), consists of two key modules: an audio feature\nextraction model and a music video model. The audio feature extractor is a\ntwo-stage model comprising an audio energy model and an audio feature\nextraction model. The audio energy model is designed to capture the\ninformation-content of audio samples and extract the audio energy features.\nThe second stage of the extractor is a music video feature model, which\nconstructs music video features by processing the audio energy features from\nthe first stage. To enhance the robustness of the proposed model, a\nmultimodal fusion method is employed. The fusion method uses the music video\nfeatures to refine the audio features, which is then used to train the music\nvideo model. The model is further improved through the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09146341463414634,
          "p": 0.21428571428571427,
          "f": 0.12820512401198056
        },
        "rouge-2": {
          "r": 0.01606425702811245,
          "p": 0.03418803418803419,
          "f": 0.02185791914763143
        },
        "rouge-l": {
          "r": 0.07317073170731707,
          "p": 0.17142857142857143,
          "f": 0.10256409837095495
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.09775v1",
      "true_abstract": "This paper introduces a data-driven graphical framework for the real-time\nsearch of risky cascading fault chains (FCs) in power-grids, crucial for\nenhancing grid resiliency in the face of climate change. As extreme weather\nevents driven by climate change increase, identifying risky FCs becomes crucial\nfor mitigating cascading failures and ensuring grid stability. However, the\ncomplexity of the spatio-temporal dependencies among grid components and the\nexponential growth of the search space with system size pose significant\nchallenges to modeling and risky FC search. To tackle this, we model the search\nprocess as a partially observable Markov decision process (POMDP), which is\nsubsequently solved via a time-varying graph recurrent neural network (GRNN).\nThis approach captures the spatial and temporal structure induced by the\nsystem's topology and dynamics, while efficiently summarizing the system's\nhistory in the GRNN's latent space, enabling scalable and effective\nidentification of risky FCs.",
      "generated_abstract": "r explores the potential of applying reinforcement learning (RL)\nto autonomous driving by integrating the RL approach into the Traffic\nLight Control (TLC) system. A novel reinforcement learning-based TLC policy\noptimization framework is proposed to control the traffic light system\nefficiently and effectively, enhancing the safety, efficiency, and environmental\nfriendliness of autonomous driving. First, a traffic light system is modeled\nand simulated using the Highway Environmental Simulator (HES) to simulate\ntraffic conditions, traffic light states, and the environment. Then, a\nreinforcement learning-based TLC policy optimization framework is proposed to\noptimize the traffic light system. This framework includes a traffic light\nstate prediction module, a traffic light control module, and a policy\noptimization module. The traffic light state prediction module uses a deep\nlearning-based traffic light prediction network to predict traffic light",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13725490196078433,
          "p": 0.20588235294117646,
          "f": 0.16470587755294133
        },
        "rouge-2": {
          "r": 0.007407407407407408,
          "p": 0.009523809523809525,
          "f": 0.00833332841146124
        },
        "rouge-l": {
          "r": 0.12745098039215685,
          "p": 0.19117647058823528,
          "f": 0.15294117167058838
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2503.07798v1",
      "true_abstract": "Understanding the risk and protective factors associated with Parkinsons\ndisease (PD) is crucial for improving outcomes for patients, individuals at\nrisk, healthcare providers, and healthcare systems. Studying these factors not\nonly enhances our knowledge of the disease but also aids in developing\neffective prevention, management, and treatment strategies. This paper reviews\nthe key risk and protective factors associated with PD, with a particular focus\non the biological mechanisms underlying these factors. Risk factors include\ngenetic mutations, racial predispositions, and environmental exposures, all of\nwhich contribute to an increased likelihood of developing PD or accelerating\ndisease progression. Conversely, protective factors such as regular physical\nexercise, adherence to a Mediterranean diet, and higher urate levels have\ndemonstrated potential to reduce inflammation and support mitochondrial\nfunction, thereby mitigating disease risk. However, identifying and validating\nthese factors presents significant challenges. To overcome challenges, we\npropose several solutions and recommendations. Future research should\nprioritize the development of standardized biomarkers for early diagnosis,\ninvestigate gene-environment interactions in greater depth, and refine animal\nmodels to better mimic human PD pathology. Additionally, we offer actionable\nrecommendations for PD prevention and management, tailored to healthy\nindividuals, patients diagnosed with PD, and healthcare systems. These\nstrategies aim to improve clinical outcomes, enhance quality of life, and\noptimize healthcare delivery for PD.",
      "generated_abstract": "t the CARL model, a novel population dynamics model based on\nthe Carrousel-Arrondissements-Limites (CARL) approach, which aims to capture the\nevolution of spatial populations in a city. The CARL model is an extension of\nthe spatial dynamics model based on the Carrousel-Arrondissements (CA)\nframework. The CA model has been widely used in spatial epidemiology to study\nthe spatial spread of infectious diseases. The CARL model extends this model by\nincluding spatial interactions between individuals in the city, enabling the\nstudy of interactions between spatially disjoint populations, such as households\nor neighborhoods. The CARL model is based on a two-dimensional spatial\narchitecture, where each cell is a discrete population. The model assumes that\nthe population dynamics are driven by the interactions between individuals\nthrough an interaction matrix. The interactions between individuals",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0945945945945946,
          "p": 0.19718309859154928,
          "f": 0.12785387689664535
        },
        "rouge-2": {
          "r": 0.015151515151515152,
          "p": 0.026785714285714284,
          "f": 0.019354834094486007
        },
        "rouge-l": {
          "r": 0.0945945945945946,
          "p": 0.19718309859154928,
          "f": 0.12785387689664535
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2503.01761v1",
      "true_abstract": "The claustrum is a thin gray matter structure in each brain hemisphere,\ncharacterized by exceptionally high connectivity with nearly all brain regions.\nDespite extensive animal studies on its anatomy and function and growing\nevidence of claustral deficits in neuropsychiatric disorders, its specific\nroles in normal and abnormal human brain function remain largely unknown. This\nis primarily due to its thin and complex morphology, which limits accurate\nanatomical delineation and neural activity isolation in conventional in vivo\nneuroimaging. To facilitate future neuroimaging studies, we developed a\ncomprehensive and reliable manual segmentation protocol based on a\ncellular-resolution brain atlas and high-resolution (0.7^3 mm) MRI data. The\nprotocols involve detailed guidelines to delineate the entire claustrum,\nincluding the inferior parts that have not been clearly described in earlier\nMRI studies. Additionally, we propose a geometric method to parcellate the\nclaustrum into three subregions (the dorsal, ventral, and temporal claustrum)\nalong the superior-to-inferior axis. The mean bilateral claustrum volume in 10\nyoung adults was 3307.5 mm^3, approximately 0.21% of total intracranial volume.\nOur segmentation protocol demonstrated high inter- and intra-rater reliability\n(ICC > 0.89, DSC > 0.85), confirming its replicability. This comprehensive and\nreliable claustrum segmentation protocols will provide a cornerstone for future\nneuroimaging studies of systematic, large-scale investigations of the anatomy\nand the functions of the human claustrum in normal and pathological\npopulations.",
      "generated_abstract": "Quantitative Genetics (QG) is the branch of genetics that integrates\nquantitative methods (e.g., statistical and computational modeling) with\ngenetics. QG has developed rapidly in the last decade, making it a prominent\nfield in modern genetics. The goal of this review is to summarize the\nfoundations of QG, and to describe some of its most prominent applications in\nmodern genetics. We cover both classical and modern approaches, and we discuss\nthe importance of QG for understanding complex biological phenomena. We also\ndiscuss the potential of QG in advancing our understanding of genetic\ndiseases, with a particular focus on neurogenetics and other complex\ndiseases.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10457516339869281,
          "p": 0.22535211267605634,
          "f": 0.14285713852718446
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0915032679738562,
          "p": 0.19718309859154928,
          "f": 0.12499999567004161
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.17059v1",
      "true_abstract": "This study examines the determinants of the spousal age gap (SAG) in India,\nutilizing data from the 61st and 68th rounds of the National Sample Survey\n(NSSO). We employ regression analysis, including instrumental variables, to\naddress selection bias and account for unobservable factors. We hypothesize an\ninverted U-shaped relationship between educational assortative mating and SAG,\nwhere, keeping the husband's education constant at the graduation level, the\nSAG first widens and then narrows as the wife's education level increases from\nprimary to postgraduate. This pattern is shaped by distinct socio-economic\nfactors across rural and urban settings. In rural India, increasing prosperity,\nchanges in family structure, and educational hypergamy contribute to a wider\nage gap, with the influence of bride squeeze further exacerbating this\ndisparity. Conversely, in urban areas, while the growth of white-collar jobs\ninitially contributed to a narrowing of the SAG in 2004-05, this trend did not\npersist by 2011-12. Specifically, the influence of income on SAG becomes\nnonlinear, showing declining trends beyond the 7th income quantile, reflecting\nlimited marriage mobility opportunities for females and hinting at a possible\nthreat to the institution of marriage among the urban upper class. To the best\nof our knowledge, this is the first study to provide empirical evidence on how\nspecific social, economic, and cultural dynamics influence the spousal age gap\nin Indian society. This increasing and persistent spousal age gap has\nsignificant implications for the treatment of women, power dynamics, and\nviolence within marriage.",
      "generated_abstract": "y examines the impact of the COVID-19 pandemic on the use of\nnon-motorized transport (NMT) modes in China, focusing on the use of bicycles\nand e-scooters. We analyze data from the 2021 China Travel Survey (CCTS) to\nquantify the adoption of NMT modes during the pandemic and compare these\nchanges to the pre-pandemic trends. The results show that the adoption of NMT\nmodes has increased during the pandemic, particularly for e-scooters, which\nexhibited the highest increase. The findings also indicate that the use of\nbicycles decreased during the pandemic. However, the study finds that e-scooters\nand e-bikes experienced a significant increase in use during the pandemic.\nThese findings highlight the importance of incorporating NMT modes",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1125,
          "p": 0.26865671641791045,
          "f": 0.15859030420928033
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.05154639175257732,
          "f": 0.03058103558248989
        },
        "rouge-l": {
          "r": 0.09375,
          "p": 0.22388059701492538,
          "f": 0.13215858614760637
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.06379v1",
      "true_abstract": "Let $G$ be a finite group and $p$ be a prime. We denote by $C_p(G)$ the poset\nof all cosets of $p$-subgroups of $G$. We characterize the homotopy type of the\ngeometric realization $|\\Delta C_p(G)|$ for $p$-closed groups $G$, which is\nmotivated by K.S.Brown's Question. We will further demonstrate that\n$\\chi(C_{p}(G)) \\equiv |G|_{p'} (\\text{mod} p)$ for any finite group $G$ and\nany prime $p$.",
      "generated_abstract": "In this paper, we investigate the asymptotic behavior of the solution of the\ncubic nonlinear Schr\\\"odinger equation (NLS) in a cylinder with an external\nmagnetic field. We show that the solution converges to the classical\nsolution of the NLS in the limit of infinitely large magnetic field. We also\nprove that the asymptotic behavior of the magnetic field is the same as the\nasymptotic behavior of the energy. This result is obtained by a variational\nmethod.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14893617021276595,
          "p": 0.15555555555555556,
          "f": 0.15217390804584138
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.01694915254237288,
          "f": 0.01639343762832724
        },
        "rouge-l": {
          "r": 0.14893617021276595,
          "p": 0.15555555555555556,
          "f": 0.15217390804584138
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/CO/2502.03439v1",
      "true_abstract": "The pyLOT library offers a Python implementation of linearized optimal\ntransport (LOT) techniques and methods to use in downstream tasks. The pipeline\nembeds probability distributions into a Hilbert space via the Optimal Transport\nmaps from a fixed reference distribution, and this linearization allows\ndownstream tasks to be completed using off the shelf (linear) machine learning\nalgorithms. We provide a case study of performing ML on 3D scans of lemur\nteeth, where the original questions of classification, clustering, dimension\nreduction, and data generation reduce to simple linear operations performed on\nthe LOT embedded representations.",
      "generated_abstract": "er the problem of predicting the time until a patient's disease\nremains untreatable. While we have access to many variables (clinical and\nmolecular features), including genetic and environmental factors, we only\nobserve the first-moment of these variables. We are interested in developing\nmodels that can make robust predictions for the time until a disease becomes\ntreatable. To address this problem, we propose an approach that leverages the\njoint latent structure of the variables to learn a joint Gaussian Process\nregression model. This approach combines the advantages of GP regression,\nwhich is typically sensitive to outliers, and the flexibility of Gaussian\nProcesses, which can handle high-dimensional inputs. The method is based on\ntransforming the data into a latent space, where each variable is mapped to a\nGaussian Process density. The resulting latent space is then used to learn a\nGaussian Process regression model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18421052631578946,
          "p": 0.15053763440860216,
          "f": 0.16568046842337467
        },
        "rouge-2": {
          "r": 0.01098901098901099,
          "p": 0.007633587786259542,
          "f": 0.009009004171336092
        },
        "rouge-l": {
          "r": 0.14473684210526316,
          "p": 0.11827956989247312,
          "f": 0.13017750984349308
        }
      }
    },
    {
      "paper_id": "hep-th.math-ph/2503.09997v1",
      "true_abstract": "This review explores recent advances in the theory of $T\\bar{T}$ deformation,\nan irrelevant yet solvable deformation of quantum field theories defined via\nthe quadratic form of the energy-momentum tensor. It addresses classical and\nquantum aspects, highlighting significant developments across various fields,\nincluding field theory, holography, and string theory. Classically, $T\\bar{T}$\ndeformation manifests through multiple geometric interpretations, notably\nrandom geometry, Jackiw-Teitelboim-like gravity, and uniform light-cone gauge\nframeworks. For quantum aspects, the deformation introduces notable features\nsuch as non-locality, UV-IR mixing, solvable renormalization structures, and\nintriguing modifications to correlation functions and entanglement properties.\nFurthermore, the paper examines the profound relationship between $T\\bar{T}$\ndeformation and holography, particularly within the mixed boundary\nconditions/cutoff AdS holography proposal and holographic entanglement entropy.\nConnections to string theory through single-trace deformations and their\nholographic duals further reveal the deformed structure of the worldsheet. This\nreview synthesizes recent developments and outlines potential directions for\nfuture research in the study of $T\\bar{T}$-like deformation.",
      "generated_abstract": "We study the stability of quantum states in the presence of local quantum\ndisorder. We provide a precise characterization of the spectrum of the\ndisorder-induced decoherence operator, and we establish the necessary and\nsufficient conditions for the stability of any given quantum state against\ndecoherence. These conditions are based on the so-called Lindblad equation,\nwhich provides an exact description of the decoherence process. Our results\nprovide a unified framework for studying quantum decoherence, and they are\napplicable to both single-particle and many-body systems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08333333333333333,
          "p": 0.16071428571428573,
          "f": 0.10975609306365278
        },
        "rouge-2": {
          "r": 0.02054794520547945,
          "p": 0.03896103896103896,
          "f": 0.026905825075107838
        },
        "rouge-l": {
          "r": 0.07407407407407407,
          "p": 0.14285714285714285,
          "f": 0.09756097111243328
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.04325v2",
      "true_abstract": "Gliomas are brain tumours that stand out for their highly lethal and\naggressive nature, which demands a precise approach in their diagnosis. Medical\nimage segmentation plays a crucial role in the evaluation and follow-up of\nthese tumours, allowing specialists to analyse their morphology. However,\nexisting methods for automatic glioma segmentation often lack generalization\ncapability across other brain tumour domains, require extensive computational\nresources, or fail to fully utilize the multi-parametric MRI (mp-MRI) data used\nto delineate them. In this work, we introduce GBT-SAM, a novel Generalizable\nBrain Tumour (GBT) framework that extends the Segment Anything Model (SAM) to\nbrain tumour segmentation tasks. Our method employs a two-step training\nprotocol: first, fine-tuning the patch embedding layer to process the entire\nmp-MRI modalities, and second, incorporating parameter-efficient LoRA blocks\nand a Depth-Condition block into the Vision Transformer (ViT) to capture\ninter-slice correlations. GBT-SAM achieves state-of-the-art performance on the\nAdult Glioma dataset (Dice Score of $93.54$) while demonstrating robust\ngeneralization across Meningioma, Pediatric Glioma, and Sub-Saharan Glioma\ndatasets. Furthermore, GBT-SAM uses less than 6.5M trainable parameters, thus\noffering an efficient solution for brain tumour segmentation. \\\\ Our code and\nmodels are available at https://github.com/vpulab/med-sam-brain .",
      "generated_abstract": "r introduces a novel framework for the automatic detection of\ndrug-related side effects in clinical trial reports. We focus on the\nadverse-event (AE) category of the Common Terminology Criteria for Adverse\nEvents (CTCAE) version 4.0, which contains detailed descriptions of clinical\nevents and their causal relationships. Using a machine-learning approach, we\ndeveloped a model that predicts whether the AE is due to a drug or a\nnon-drug-related event. The model was trained and validated on a dataset of\nreports from the United States Food and Drug Administration (FDA) Adverse Event\nReporting System (FAERS) and a larger dataset of clinical trials. The model\nachieved an overall accuracy of 81.6% and an AUC of 0.782. It also showed\nsignificant performance improvements when trained",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12418300653594772,
          "p": 0.20652173913043478,
          "f": 0.15510203612628085
        },
        "rouge-2": {
          "r": 0.015789473684210527,
          "p": 0.025,
          "f": 0.01935483396462135
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.18478260869565216,
          "f": 0.13877550551403597
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/AP/2502.17371v2",
      "true_abstract": "The integration of photovoltaic (PV) systems into greenhouses not only\noptimizes land use but also enhances sustainable agricultural practices by\nenabling dual benefits of food production and renewable energy generation.\nHowever, accurate prediction of internal environmental conditions is crucial to\nensure optimal crop growth while maximizing energy production. This study\nintroduces a novel application of Spatio-Temporal Graph Neural Networks\n(STGNNs) to greenhouse microclimate modeling, comparing their performance with\ntraditional Recurrent Neural Networks (RNNs). While RNNs excel at temporal\npattern recognition, they cannot explicitly model the directional relationships\nbetween environmental variables. Our STGNN approach addresses this limitation\nby representing these relationships as directed graphs, enabling the model to\ncapture both spatial dependencies and their directionality. Using\nhigh-frequency data collected at 15-minute intervals from a greenhouse in\nVolos, Greece, we demonstrate that RNNs achieve exceptional accuracy in winter\nconditions (R^2 = 0.985) but show limitations during summer cooling system\noperation. Though STGNNs currently show lower performance (winter R^2 = 0.947),\ntheir architecture offers greater potential for integrating additional\nvariables such as PV generation and crop growth indicators.",
      "generated_abstract": "presents a novel statistical methodology for the assessment of\nquantitative risk factors and the determination of their individual and combined\nsignificance in the context of the analysis of multivariate data. The\napproach is based on the application of the multivariate quantile regression\nmodel, which allows for the simultaneous estimation of the quantile values of\nquantitative risk factors and their interrelationship. This model allows for\nthe simultaneous estimation of quantile values of the risk factors, as well as\ntheir interrelationship. The proposed methodology is based on the\ncorrelation-based multivariate quantile regression model, which allows for the\nsimultaneous estimation of the quantile values of the risk factors and their\ninterrelationship. The proposed methodology is based on the correlation-based\nmultivariate quantile regression model, which allows for the simultaneous\nestimation of the quantile values of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11267605633802817,
          "p": 0.35555555555555557,
          "f": 0.17112299099774095
        },
        "rouge-2": {
          "r": 0.017341040462427744,
          "p": 0.04411764705882353,
          "f": 0.024896261509272193
        },
        "rouge-l": {
          "r": 0.1056338028169014,
          "p": 0.3333333333333333,
          "f": 0.1604278038319655
        }
      }
    },
    {
      "paper_id": "physics.class-ph.physics/class-ph/2503.00432v1",
      "true_abstract": "A specialized high-precision numerical search for equal-mass collisionless\nthree-body periodic free-fall orbits with central symmetry is conducted. The\nsearch is based on Newton's method with initial approximations obtained by the\ngrid-search method. Instead of solving the standard periodicity equation on the\nentire period a quarter-period equation that also characterizes the periodic\norbits is solved. The number of the known orbits from the class is\nsignificantly enlarged. The linear stability of the orbits is also\ninvestigated. All of them are unstable. A discussion in relation to the\nefficiency of Newton's method applied with grid-search initial approximations\nis held.",
      "generated_abstract": "f this paper is to develop a novel classifier that predicts the\nenergy loss of a particle in a hadronic environment. We propose a method based\non the use of the energy-momentum tensor (EMT) of the medium, whose components\nare obtained from the Monte Carlo (MC) simulations of the hadronic medium\nproduction. This methodology is based on the analysis of the correlation\nbetween the transverse energy of the particle and the temperature of the\nmedium. The EMT components are obtained by computing the longitudinal and\ntransverse energy flow in the medium. We use the MC simulation of the\ncollision of a $p+p$ and $pp$ proton beams at the CERN-SPS facility, where the\nhadronic interaction is described by the transverse parton distribution functions\n(PDFs). The EMT components are extracted by analyzing the correlation between\nthe energy loss of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24193548387096775,
          "p": 0.2054794520547945,
          "f": 0.2222222172554185
        },
        "rouge-2": {
          "r": 0.07608695652173914,
          "p": 0.0625,
          "f": 0.06862744602845096
        },
        "rouge-l": {
          "r": 0.22580645161290322,
          "p": 0.1917808219178082,
          "f": 0.20740740244060368
        }
      }
    },
    {
      "paper_id": "math.GT.math/GT/2503.08268v1",
      "true_abstract": "We generalise the finite biquandle colouring invariant to a polynomial\ninvariant based on labelling a knot diagram with a finite birack that reduces\nto the biquandle colouring invariant in that case. The polynomial is an\ninvariant of a class of knot theories amenable to a generalisation of theorem\nof Trace on regular homotopy. We take the opportunity to reprise the relevant\ngeneralised knot theory and the theory of generalised biracks in the light of\nthis work and recent developments.",
      "generated_abstract": "hbb{F}$ be a field. In this paper, we study the topological\nalgebra $H^*(K(\\mathbb{F}), \\mathbb{F})$ where $K(\\mathbb{F})$ is the\nCayley complex of $\\mathbb{F}$ and $\\mathbb{F}$ is an arbitrary field. We\ndefine an \"infinite filtration\" on $H^*(K(\\mathbb{F}), \\mathbb{F})$ as a\nfiltration by non-negative integers on $H^*(K(\\mathbb{F}), \\mathbb{F})$ such\nthat each component of the filtration is a finite direct sum of copies of\n$\\mathbb{F}$. We show that the infinite filtration on $H^*(K(\\mathbb{F}),\n\\mathbb{F})$ is compatible with the action of the fundamental group of\n$K(\\mathbb{F})$. We also show that the infinite filtration on $H",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2608695652173913,
          "p": 0.23529411764705882,
          "f": 0.24742267542565638
        },
        "rouge-2": {
          "r": 0.02666666666666667,
          "p": 0.025974025974025976,
          "f": 0.026315784474550814
        },
        "rouge-l": {
          "r": 0.21739130434782608,
          "p": 0.19607843137254902,
          "f": 0.20618556202359456
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.13556v1",
      "true_abstract": "This study examines the relationship between the concept of serious leisure\nand user innovation. We adopted the characteristics of innovative consumers\nidentified by Luthje (2004)-product use experience, information exchange, and\nnew product adoption speed-to analyze their correlation with serious leisure\nengagement. The analysis utilized consumer behavior survey data from the\n\"Marketing Analysis Contest 2023\" sponsored by Nomura Research Institute,\nexamining the relationship between innovative consumer characteristics and the\ndegree of serious leisure (Serious Leisure Inventory and Measure: SLIM). Since\nthe contest data did not directly measure innovative consumer characteristics\nor serious leisure engagement, we established alternative variables for\nquantitative analysis. The results showed that the SLIM alternative variable\nhad positive correlations with diverse product experiences and early adoption\nof new products. However, no clear relationship was found with information\nexchange among consumers. These findings suggest that serious leisure practice\nmay serve as a potential antecedent to user innovation. The leisure career\nperspective of the serious leisure concept may capture the motivations of user\ninnovators that Okada and Nishikawa (2019) identified.",
      "generated_abstract": "In this paper, we develop a novel semiparametric modeling framework for\ndynamics of financial returns, leveraging the joint conditional dependence\nstructure of returns and market variables. The proposed model incorporates\nmarket variables as potential confounders, which enables a more flexible\ndescription of market variables that may be relevant for financial returns. We\nemploy an estimated generalized additive model for the conditional mean of\nreturns over time, and show that the model is flexible enough to capture the\nsignals of key market variables and to capture the dynamics of financial\nreturns. We also explore the robustness of the proposed model by conducting\nsimulation studies to examine its ability to capture the joint dependence\nstructure of returns and market variables. Our empirical application,\ncomparing the returns of US equities before and after the COVID-19 crisis,\nsuggests that the proposed model captures the market variables that are\nimportant for financial returns.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14545454545454545,
          "p": 0.19753086419753085,
          "f": 0.16753926213097242
        },
        "rouge-2": {
          "r": 0.0189873417721519,
          "p": 0.025,
          "f": 0.021582728906372424
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.18518518518518517,
          "f": 0.15706805794249076
        }
      }
    },
    {
      "paper_id": "physics.soc-ph.q-bio/PE/2502.02713v1",
      "true_abstract": "The spread of disinformation poses a significant threat to societal\nwell-being. We analyze this phenomenon using an evolutionary game theory model\nof the sender-receiver game, where senders aim to mislead receivers and\nreceivers aim to discern the truth. Using a combination of replicator\nequations, finite-size scaling analysis, and extensive Monte Carlo simulations,\nwe investigate the long-term evolutionary dynamics of this game. Our central\nfinding is a counterintuitive threshold phenomenon: the role (sender or\nreceiver) with the larger difference in payoffs between successful and\nunsuccessful interactions is surprisingly more likely to lose in the long run.\nWe show that this effect is robust across different parameter values and arises\nfrom the interplay between the relative speeds of evolution of the two roles\nand the ability of the slower evolving role to exploit the fixed strategy of\nthe faster evolving role. Moreover, for finite populations we find that the\ninitially less frequent strategy of the slower role is more likely to fixate in\nthe population. The initially rarer strategy in the less-rewarded role is,\nparadoxically, more likely to prevail.",
      "generated_abstract": "r presents a methodology for the rapid analysis of the evolution\nof bacterial populations. We propose a method for the detection of bacterial\nvirulence factors (BVFs) within a population by the use of a two-step\ndetection algorithm. In the first step, we use a genetic algorithm (GA) to\nidentify the BVFs present in a given population of bacteria. This step is\ncritical as it provides a list of BVFs that can be used to identify the\npopulation in question. In the second step, we perform a comparison between\nthe presence of these BVFs in the population and the presence of a reference\npopulation. The comparison is performed using a statistical analysis and a\ncomprehensive evaluation of the sensitivity, specificity, and accuracy of the\ndetection algorithm. This methodology has been applied to a bacterial community\ninvolved in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15315315315315314,
          "p": 0.2328767123287671,
          "f": 0.18478260390890844
        },
        "rouge-2": {
          "r": 0.043209876543209874,
          "p": 0.05785123966942149,
          "f": 0.049469959769257194
        },
        "rouge-l": {
          "r": 0.12612612612612611,
          "p": 0.1917808219178082,
          "f": 0.15217390825673455
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/TR/2412.14361v2",
      "true_abstract": "We revisit the long-only trend-following strategy presented in A Century of\nProfitable Industry Trends by Zarattini and Antonacci, which achieved\nexceptional historical performance with an 18.2% annualized return and a Sharpe\nRatio of 1.39. While the results outperformed benchmarks, practical\nimplementation raises concerns about robustness and evolving market conditions.\nThis study explores modifications addressing reliance on T-bills, alternative\nfallback allocations, and industry exclusions. Despite attempts to enhance\nadaptability through momentum signals, parameter optimization, and Walk-Forward\nAnalysis, results reveal persistent challenges. The results highlight\nchallenges in adapting historical strategies to modern markets and offer\ninsights for future trend-following frameworks.",
      "generated_abstract": "This paper presents a novel methodology for the pricing of equity options\nwith volatility structures that depend on the market risk premium. The\nmethodology leverages a stochastic volatility framework to estimate the\nvolatility surface based on historical volatility data and option prices. The\npricing methodology is applied to the pricing of standard options, including\nputs and calls, as well as the pricing of straddle, strangle, and out-of-the-money\noptions. The results demonstrate the effectiveness of the proposed pricing\nmethodology, as the model-based pricing of options provides more accurate\nprices compared to the traditional pricing methods, such as the Black-Scholes\nmodel and the Monte-Carlo simulation approach.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1511627906976744,
          "p": 0.203125,
          "f": 0.17333332844088903
        },
        "rouge-2": {
          "r": 0.010101010101010102,
          "p": 0.010526315789473684,
          "f": 0.01030927335264351
        },
        "rouge-l": {
          "r": 0.1511627906976744,
          "p": 0.203125,
          "f": 0.17333332844088903
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.07189v1",
      "true_abstract": "Reconfigurable intelligent surface (RIS)-aided cell-free (CF) massive\nmultiple-input multiple-output (mMIMO) is a promising architecture for further\nimproving spectral efficiency (SE) with low cost and power consumption.\nHowever, conventional RIS has inevitable limitations due to its capability of\nonly reflecting signals. In contrast, beyond-diagonal RIS (BD-RIS), with its\nability to both reflect and transmit signals, has gained great attention. This\ncorrespondence focuses on using BD-RIS to improve the sum SE of CF mMIMO\nsystems. This requires completing the beamforming design under the transmit\npower constraints and unitary constraints of the BD-RIS, by optimizing active\nand passive beamformer simultaneously. To tackle this issue, we introduce an\nalternating optimization algorithm that decomposes it using fractional\nprogramming and solves the subproblems alternatively. Moreover, to address the\nchallenge introduced by the unitary constraint on the beamforming matrix of the\nBD-RIS, a manifold optimization algorithm is proposed to solve the problem\noptimally. Simulation results show that BD-RISs outperform RISs\ncomprehensively, especially in the case of the full connected architecture\nwhich achieves the best performance, enhancing the sum SE by around 40%\ncompared to ideal RISs.",
      "generated_abstract": "This paper presents a novel methodology for real-time low-latency control\nof nonlinear systems using a novel approach based on the use of artificial\nneural networks (ANNs) in a multi-agent framework. The proposed framework\nincludes a central controller and a set of agents that communicate through a\ntime-varying communication channel. The central controller determines the\ncontrol inputs, which are then sent to the agents. The agents in the\nmulti-agent system communicate with each other and with the central controller\nvia a time-varying communication channel. The main objective of the proposed\nframework is to minimize the overall control cost in the presence of time\nvariation in the communication channel. The performance of the proposed\nframework is evaluated using both simulated and real data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13953488372093023,
          "p": 0.26865671641791045,
          "f": 0.18367346488806757
        },
        "rouge-2": {
          "r": 0.017543859649122806,
          "p": 0.028846153846153848,
          "f": 0.02181817711497622
        },
        "rouge-l": {
          "r": 0.12403100775193798,
          "p": 0.23880597014925373,
          "f": 0.16326530162276146
        }
      }
    },
    {
      "paper_id": "q-bio.QM.stat/AP/2502.19206v2",
      "true_abstract": "Understanding the role of different age groups in disease transmission is\ncrucial for designing effective intervention strategies. A key parameter in\nage-structured epidemic models is the contact matrix, which defines the\ninteraction structure between age groups. However, accurately estimating\ncontact matrices is challenging, as different age groups respond differently to\nsurveys and are accessible through different channels. This variability\nintroduces significant epistemic uncertainty in epidemic models.\n  In this study, we introduce the Age Group Sensitivity Analysis (AGSA) method,\na novel framework for assessing the impact of age-structured contact patterns\non epidemic outcomes. Our approach integrates age-stratified epidemic models\nwith Latin Hypercube Sampling (LHS) and the Partial Rank Correlation\nCoefficient (PRCC) method, enabling a systematic sensitivity analysis of\nage-specific interactions. Additionally, we propose a new sensitivity\naggregation technique that quantifies the contribution of each age group to key\nepidemic parameters.\n  By identifying the age groups to which the model is most sensitive, AGSA\nhelps pinpoint those that introduce the greatest epistemic uncertainty. This\nallows for targeted data collection efforts, focusing surveys and empirical\nstudies on the most influential age groups to improve model accuracy. As a\nresult, AGSA can enhance epidemic forecasting and inform the design of more\neffective and efficient public health interventions.",
      "generated_abstract": "ification of causal relationships in complex biological systems is\na fundamental challenge in scientific research. In this paper, we present a\nnovel framework for the identification of causal relationships in biological\nsystems using a general statistical framework based on the concept of causal\ngraphical model, called the causal graphical modeling framework. The framework\nprovides a unified approach for identifying causal relationships in various\nbiological systems, including genetic networks, metabolic networks, cellular\nnetworks, and the network of gene expression. The framework is based on the\nframework of causal graphical models, which is a general framework for\nrepresenting complex causal relationships. The framework also provides a\nunified approach for identifying causal relationships in various biological\nsystems, including genetic networks, metabolic networks, cellular networks,\nand the network of gene expression. The framework is based on the framework of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11940298507462686,
          "p": 0.2962962962962963,
          "f": 0.17021276186283396
        },
        "rouge-2": {
          "r": 0.031413612565445025,
          "p": 0.07142857142857142,
          "f": 0.04363635939332273
        },
        "rouge-l": {
          "r": 0.11940298507462686,
          "p": 0.2962962962962963,
          "f": 0.17021276186283396
        }
      }
    },
    {
      "paper_id": "eess.IV.q-bio/TO/2502.14707v1",
      "true_abstract": "While deep learning methods have shown great promise in improving the\neffectiveness of prostate cancer (PCa) diagnosis by detecting suspicious\nlesions from trans-rectal ultrasound (TRUS), they must overcome multiple\nsimultaneous challenges. There is high heterogeneity in tissue appearance,\nsignificant class imbalance in favor of benign examples, and scarcity in the\nnumber and quality of ground truth annotations available to train models.\nFailure to address even a single one of these problems can result in\nunacceptable clinical outcomes.We propose TRUSWorthy, a carefully designed,\ntuned, and integrated system for reliable PCa detection. Our pipeline\nintegrates self-supervised learning, multiple-instance learning aggregation\nusing transformers, random-undersampled boosting and ensembling: these address\nlabel scarcity, weak labels, class imbalance, and overconfidence, respectively.\nWe train and rigorously evaluate our method using a large, multi-center dataset\nof micro-ultrasound data. Our method outperforms previous state-of-the-art deep\nlearning methods in terms of accuracy and uncertainty calibration, with AUROC\nand balanced accuracy scores of 79.9% and 71.5%, respectively. On the top 20%\nof predictions with the highest confidence, we can achieve a balanced accuracy\nof up to 91%. The success of TRUSWorthy demonstrates the potential of\nintegrated deep learning solutions to meet clinical needs in a highly\nchallenging deployment setting, and is a significant step towards creating a\ntrustworthy system for computer-assisted PCa diagnosis.",
      "generated_abstract": "of Computational Pathology has made significant progress in\ntackling challenges in the field of image analysis in the last decade. However,\ncomputational methods still have limitations in terms of their scalability,\nefficiency and precision, especially when dealing with multi-organ images and\nlarge datasets. This study proposes a new approach to image segmentation,\nbased on the use of a graph neural network, for the segmentation of brain\ntissue regions in multi-organ microscopy images. The method is based on the\nintroduction of a new layer that contains information about the number of\nvoxels belonging to each organ, allowing the model to learn the optimal\nrepresentation of the data. This representation is then used to predict the\norgan of interest in each voxel. This method is evaluated on 28 microscopy\nimages of different organs (brain, liver, pancreas, lung, s",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12751677852348994,
          "p": 0.2087912087912088,
          "f": 0.15833332862534735
        },
        "rouge-2": {
          "r": 0.019138755980861243,
          "p": 0.03125,
          "f": 0.023738867692416996
        },
        "rouge-l": {
          "r": 0.11409395973154363,
          "p": 0.18681318681318682,
          "f": 0.14166666195868072
        }
      }
    },
    {
      "paper_id": "cond-mat.quant-gas.nlin/PS/2503.10519v1",
      "true_abstract": "Driven systems are of fundamental scientific interest, as they can exhibit\nproperties that are radically different from the same system at equilibrium. In\ncertain cases, long-lived states of driven matter can emerge, which exhibit new\nmaterial properties. In this work, we probe the excitation spectrum of an\nemergent patterned state in a driven superfluid, finding that its response is\nidentical to that of a one-dimensional supersolid. In order to extract physical\nquantities that parametrize the observed sound modes, we apply an effective\nhydrodynamic theory of superfluid smectics, which is agnostic to microscopic\nprocesses. We therefore use the conceptual framework of supersolids to\ncharacterize an otherwise dynamic and far-from-equilibrium state.",
      "generated_abstract": "t a theoretical analysis of the quantum phase transition in a\nsystem of interacting bosons confined in a one-dimensional box. By using a\nGaussian ansatz for the density wave, we show that the system is in a\ndynamical critical phase with a finite energy gap between the ground state and\nthe thermodynamic limit. We find that the ground state exhibits a Bose-Einstein\ncondensate at the origin, which we interpret as a bosonic Bose-Einstein\ncondensate in the thermodynamic limit. We show that the phase transition can\nbe described by a Binder cumulant, which is a measure of the energy-dependent\ncorrelation function. We further derive a simple formula for the Binder\ncumulant in the thermodynamic limit, which is valid for any finite correlation\nlength. Finally, we show that the phase transition is sensitive to the choice\nof the Gaussian",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20987654320987653,
          "p": 0.2328767123287671,
          "f": 0.2207792157927139
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.02654867256637168,
          "f": 0.02714931626952857
        },
        "rouge-l": {
          "r": 0.18518518518518517,
          "p": 0.2054794520547945,
          "f": 0.19480518981868794
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/QM/2503.03485v1",
      "true_abstract": "Understanding the biological mechanism of disease is critical for medicine,\nand in particular drug discovery. AI-powered analysis of genome-scale\nbiological data hold great potential in this regard. The increasing\navailability of single-cell RNA sequencing data has enabled the development of\nlarge foundation models for disease biology. However, existing foundation\nmodels either do not improve or only modestly improve over task-specific models\nin downstream applications. Here, we explored two avenues for improving the\nstate-of-the-art. First, we scaled the pre-training dataset to 116 million\ncells, which is larger than those used by previous models. Second, we leveraged\nthe availability of large-scale biological annotations as a form of supervision\nduring pre-training. We trained the TEDDY family of models comprising six\ntransformer-based state-of-the-art single-cell foundation models with 70\nmillion, 160 million, and 400 million parameters. We vetted our models on two\ndownstream evaluation tasks -- identifying the underlying disease state of\nheld-out donors not seen during training and distinguishing healthy cells from\ndiseased ones for disease conditions and donors not seen during training.\nScaling experiments showed that performance improved predictably with both data\nvolume and parameter count. Our models showed substantial improvement over\nexisting work on the first task and more muted improvements on the second.",
      "generated_abstract": "pt of time-varying information flow (TVIF) has emerged as a\nnew paradigm for information flow (IF) analysis. It considers the evolution of\ninformation over time, distinguishing between ``naturally evolving'' and\n``non-naturally evolving'' flows. Non-naturally evolving flows are\ndeterministic, while naturally evolving flows can be either. We introduce a\nnovel methodology to quantify the evolution of information over time for\ndifferent types of information. Our approach integrates various information\nflow metrics, including IF, time-varying IF (TVIF), and time-varying information\nflow (TVIF). We show that the evolution of information over time can be\ndescribed by a set of nonlinear differential equations. These equations can be\nsolved analytically for a wide range of systems, allowing us to identify the\nevolution of information",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11851851851851852,
          "p": 0.2222222222222222,
          "f": 0.15458936744381446
        },
        "rouge-2": {
          "r": 0.005208333333333333,
          "p": 0.010309278350515464,
          "f": 0.00692041076519965
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.20833333333333334,
          "f": 0.1449275316950222
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/LG/2503.10636v1",
      "true_abstract": "Minibatch optimal transport coupling straightens paths in unconditional flow\nmatching. This leads to computationally less demanding inference as fewer\nintegration steps and less complex numerical solvers can be employed when\nnumerically solving an ordinary differential equation at test time. However, in\nthe conditional setting, minibatch optimal transport falls short. This is\nbecause the default optimal transport mapping disregards conditions, resulting\nin a conditionally skewed prior distribution during training. In contrast, at\ntest time, we have no access to the skewed prior, and instead sample from the\nfull, unbiased prior distribution. This gap between training and testing leads\nto a subpar performance. To bridge this gap, we propose conditional optimal\ntransport C^2OT that adds a conditional weighting term in the cost matrix when\ncomputing the optimal transport assignment. Experiments demonstrate that this\nsimple fix works with both discrete and continuous conditions in\n8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method\nperforms better overall compared to the existing baselines across different\nfunction evaluation budgets. Code is available at\nhttps://hkchengrex.github.io/C2OT",
      "generated_abstract": "In this paper, we propose a novel framework for building a\nfederated learning (FL) model on a mobile edge computing (MEC) platform.\nSpecifically, we design an FL model that can be trained offline on a\ndedicated edge device and deployed on the MEC platform for real-time\ndeployment. To this end, we propose a novel FL model framework for MEC\ndeployments, which includes a training stage on the edge device and a\ndeployment stage on the MEC platform. The training stage uses a lightweight\nlightweight model for efficient model training, and the deployment stage\noptimizes the deployed model based on the training results. We conduct\nexperimental evaluations on two public datasets to demonstrate the effectiveness\nof our proposed model framework. The experimental results show that the FL\nmodel framework can effectively improve the performance of FL models on\nMEC platforms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13821138211382114,
          "p": 0.23943661971830985,
          "f": 0.17525772731799355
        },
        "rouge-2": {
          "r": 0.012658227848101266,
          "p": 0.01680672268907563,
          "f": 0.014440428312113118
        },
        "rouge-l": {
          "r": 0.13008130081300814,
          "p": 0.22535211267605634,
          "f": 0.1649484489674781
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/OT/2407.18835v3",
      "true_abstract": "Polychoric correlation is often an important building block in the analysis\nof rating data, particularly for structural equation models. However, the\ncommonly employed maximum likelihood (ML) estimator is highly susceptible to\nmisspecification of the polychoric correlation model, for instance through\nviolations of latent normality assumptions. We propose a novel estimator that\nis designed to be robust to partial misspecification of the polychoric model,\nthat is, the model is only misspecified for an unknown fraction of\nobservations, for instance (but not limited to) careless respondents. In\ncontrast to existing literature, our estimator makes no assumption on the type\nor degree of model misspecification. It furthermore generalizes ML estimation,\nis consistent as well as asymptotically normally distributed, and comes at no\nadditional computational cost. We demonstrate the robustness and practical\nusefulness of our estimator in simulation studies and an empirical application\non a Big Five administration. In the latter, the polychoric correlation\nestimates of our estimator and ML differ substantially, which, after further\ninspection, is likely due to the presence of careless respondents that the\nestimator helps identify.",
      "generated_abstract": "We propose a novel approach to Bayesian hierarchical modeling using a\nmodel-based Gibbs sampler. The proposed algorithm is general and can be\napplied to any multivariate normal model with known and unknown covariance\nstructures. The model-based Gibbs sampler is shown to be well-conditioned and\ncan be easily parallelized. We demonstrate the utility of the proposed model\n-based Gibbs sampler through two application examples.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13274336283185842,
          "p": 0.3409090909090909,
          "f": 0.19108279851353005
        },
        "rouge-2": {
          "r": 0.04242424242424243,
          "p": 0.12280701754385964,
          "f": 0.06306305924640881
        },
        "rouge-l": {
          "r": 0.13274336283185842,
          "p": 0.3409090909090909,
          "f": 0.19108279851353005
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.00586v1",
      "true_abstract": "Early diagnosis of Alzheimer's disease (AD) is critical for intervention\nbefore irreversible neurodegeneration occurs. Structural MRI (sMRI) is widely\nused for AD diagnosis, but conventional deep learning approaches primarily rely\non intensity-based features, which require large datasets to capture subtle\nstructural changes. Jacobian determinant maps (JSM) provide complementary\ninformation by encoding localized brain deformations, yet existing multimodal\nfusion strategies fail to fully integrate these features with sMRI. We propose\na cross-attention fusion framework to model the intrinsic relationship between\nsMRI intensity and JSM-derived deformations for AD classification. Using the\nAlzheimer's Disease Neuroimaging Initiative (ADNI) dataset, we compare\ncross-attention, pairwise self-attention, and bottleneck attention with four\npre-trained 3D image encoders. Cross-attention fusion achieves superior\nperformance, with mean ROC-AUC scores of 0.903 (+/-0.033) for AD vs.\ncognitively normal (CN) and 0.692 (+/-0.061) for mild cognitive impairment\n(MCI) vs. CN. Despite its strong performance, our model remains highly\nefficient, with only 1.56 million parameters--over 40 times fewer than\nResNet-34 (63M) and Swin UNETR (61.98M). These findings demonstrate the\npotential of cross-attention fusion for improving AD diagnosis while\nmaintaining computational efficiency.",
      "generated_abstract": "In many applications, the distribution of the data is non-Gaussian, and the\ntypical distribution is modeled by a multivariate Gaussian distribution. When\nthe multivariate Gaussian distribution is a mixture of Gaussians, the\ninverse problem of reconstructing the mixture distribution from data is\nattractive. In this paper, we propose a novel method for recovering the\nmixture distribution using data. We propose a new Gaussian mixture model\nreconstruction algorithm based on the Maximum Likelihood Estimation (MLE).\nThe MLE algorithm is efficient and has a closed-form solution. We also propose a\nnovel regularization term that is incorporated in the MLE algorithm to\nenhance the reconstruction accuracy. Theoretical analysis shows that the\nproposed method is robust and performs well in different scenarios. The\nnumerical experiments demonstrate the effectiveness of the proposed method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09333333333333334,
          "p": 0.19444444444444445,
          "f": 0.12612612174336513
        },
        "rouge-2": {
          "r": 0.01675977653631285,
          "p": 0.02631578947368421,
          "f": 0.0204778109457314
        },
        "rouge-l": {
          "r": 0.09333333333333334,
          "p": 0.19444444444444445,
          "f": 0.12612612174336513
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09795v1",
      "true_abstract": "An isolating set of a graph is a set of vertices $S$ such that, if $S$ and\nits neighborhood is removed, only isolated vertices remain; and the isolation\nnumber is the minimum size of such a set. It is known that for every connected\ngraph apart from $K_2$ and $C_5$, the isolation number is at most one-third the\norder and indeed such a graph has three disjoint isolating sets. In this paper\nwe consider isolating sets where $S$ is required to be an independent set and\ncall the minimum size thereof the independent isolation number. While for\ngeneral graphs of order $n$ the independent isolation number can be arbitrarily\nclose to $n/2$, we show that in bipartite graphs the vertex set can be\npartitioned into three disjoint independent isolating sets, whence the\nindependent isolation number is at most $n/3$; while for $3$-colorable graphs\nthe maximum value of the independent isolation number is $(n+1)/3$. We also\nprovide a bound for $k$-colorable graphs.",
      "generated_abstract": "We study the problem of computing the largest integer $n$ such that there are\n$m$ positive integers $a_1,a_2,\\dots,a_m$ with $a_i\\ge n$ such that\n$a_1+a_2+\\dots+a_m=n$. The problem is known to be NP-hard and, to the best of\nour knowledge, it is still open to determine the values of $n$ for which this\nproblem is solvable. We provide an upper bound of $\\frac{n}{\\log n}$ for this\nproblem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18072289156626506,
          "p": 0.32608695652173914,
          "f": 0.23255813494621722
        },
        "rouge-2": {
          "r": 0.015037593984962405,
          "p": 0.03278688524590164,
          "f": 0.020618552389734136
        },
        "rouge-l": {
          "r": 0.14457831325301204,
          "p": 0.2608695652173913,
          "f": 0.18604650703924053
        }
      }
    },
    {
      "paper_id": "cs.CR.eess/IV/2503.09302v1",
      "true_abstract": "This paper investigates the critical issue of data poisoning attacks on AI\nmodels, a growing concern in the ever-evolving landscape of artificial\nintelligence and cybersecurity. As advanced technology systems become\nincreasingly prevalent across various sectors, the need for robust defence\nmechanisms against adversarial attacks becomes paramount. The study aims to\ndevelop and evaluate novel techniques for detecting and preventing data\npoisoning attacks, focusing on both theoretical frameworks and practical\napplications. Through a comprehensive literature review, experimental\nvalidation using the CIFAR-10 and Insurance Claims datasets, and the\ndevelopment of innovative algorithms, this paper seeks to enhance the\nresilience of AI models against malicious data manipulation. The study explores\nvarious methods, including anomaly detection, robust optimization strategies,\nand ensemble learning, to identify and mitigate the effects of poisoned data\nduring model training. Experimental results indicate that data poisoning\nsignificantly degrades model performance, reducing classification accuracy by\nup to 27% in image recognition tasks (CIFAR-10) and 22% in fraud detection\nmodels (Insurance Claims dataset). The proposed defence mechanisms, including\nstatistical anomaly detection and adversarial training, successfully mitigated\npoisoning effects, improving model robustness and restoring accuracy levels by\nan average of 15-20%. The findings further demonstrate that ensemble learning\ntechniques provide an additional layer of resilience, reducing false positives\nand false negatives caused by adversarial data injections.",
      "generated_abstract": "The recent advancement of deep learning-based speaker verification systems\n(SVS) has led to breakthroughs in automated speaker recognition. However,\ncurrently available models often lack the ability to effectively detect\nspeaker characteristics such as gender and accent. This paper proposes a\nspeaker-specific gender and accent discrimination module for SVS. The proposed\nmodule uses a pre-trained speaker recognition model as a baseline, and then\ntakes the gender and accent characteristics of the speaker as input to\ndiscriminate the original speaker. The gender and accent discrimination module\nis trained using a single-shot male-female speaker dataset. Experimental results\nshow that the gender and accent discrimination module can effectively identify\ngender and accent information, with a 1.48% and 3.16% accuracy improvement\nrespectively over the baseline model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14482758620689656,
          "p": 0.28,
          "f": 0.19090908641528936
        },
        "rouge-2": {
          "r": 0.014354066985645933,
          "p": 0.02830188679245283,
          "f": 0.019047614582213192
        },
        "rouge-l": {
          "r": 0.1310344827586207,
          "p": 0.25333333333333335,
          "f": 0.17272726823347123
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/CV/2503.10639v1",
      "true_abstract": "Current image generation and editing methods primarily process textual\nprompts as direct inputs without reasoning about visual composition and\nexplicit operations. We present Generation Chain-of-Thought (GoT), a novel\nparadigm that enables generation and editing through an explicit language\nreasoning process before outputting images. This approach transforms\nconventional text-to-image generation and editing into a reasoning-guided\nframework that analyzes semantic relationships and spatial arrangements. We\ndefine the formulation of GoT and construct large-scale GoT datasets containing\nover 9M samples with detailed reasoning chains capturing semantic-spatial\nrelationships. To leverage the advantages of GoT, we implement a unified\nframework that integrates Qwen2.5-VL for reasoning chain generation with an\nend-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance\nModule. Experiments show our GoT framework achieves excellent performance on\nboth generation and editing tasks, with significant improvements over\nbaselines. Additionally, our approach enables interactive visual generation,\nallowing users to explicitly modify reasoning steps for precise image\nadjustments. GoT pioneers a new direction for reasoning-driven visual\ngeneration and editing, producing images that better align with human intent.\nTo facilitate future research, we make our datasets, code, and pretrained\nmodels publicly available at https://github.com/rongyaofang/GoT.",
      "generated_abstract": "years, computer vision has been transformed by deep learning\nnovelties. The emergence of large language models (LLMs) has revolutionized\ncomputer vision by enabling unsupervised learning of visual representations,\nenabling powerful object detection and instance segmentation. However, the\npowerful features of LLMs have led to the emergence of a new generation of\ncomputer vision models that use LLMs as their backbone, such as DALL-E 2,\nSwipe, and CARLA. This paper provides a comprehensive survey of the state of\nthe art in LLM-based computer vision models, highlighting their advantages and\nchallenges. We provide a detailed comparison of these models, including their\narchitecture, training methodology, and application domains. We also explore\nthe potential of LLM-based models in future research areas, such as language\nmodeling, image generation, and scene understanding.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15151515151515152,
          "p": 0.24096385542168675,
          "f": 0.18604650688761507
        },
        "rouge-2": {
          "r": 0.00558659217877095,
          "p": 0.008403361344537815,
          "f": 0.006711404598670157
        },
        "rouge-l": {
          "r": 0.12878787878787878,
          "p": 0.20481927710843373,
          "f": 0.15813953014342902
        }
      }
    },
    {
      "paper_id": "cs.CL.q-fin/CP/2502.02199v1",
      "true_abstract": "Large language models (LLMs) have shown remarkable success in language\nmodelling due to scaling laws found in model size and the hidden dimension of\nthe model's text representation. Yet, we demonstrate that compressed\nrepresentations of text can yield better performance in LLM-based regression\ntasks. In this paper, we compare the relative performance of embedding\ncompression in three different signal-to-noise contexts: financial return\nprediction, writing quality assessment and review scoring. Our results show\nthat compressing embeddings, in a minimally supervised manner using an\nautoencoder's hidden representation, can mitigate overfitting and improve\nperformance on noisy tasks, such as financial return prediction; but that\ncompression reduces performance on tasks that have high causal dependencies\nbetween the input and target data. Our results suggest that the success of\ninterpretable compressed representations such as sentiment may be due to a\nregularising effect.",
      "generated_abstract": "guage Models (LLMs) have achieved remarkable performance in financial\nliterature, yet their ability to understand and extract relevant information from\ncomplex financial documents remains limited. In this paper, we present a\nmulti-agent approach that integrates a transformer-based language model with a\nknowledge-based agent to analyze financial documents. The transformer-based\nlanguage model captures the overall structure of financial documents, while the\nknowledge-based agent utilizes structured knowledge to guide the language model\nin generating relevant responses. The integration of the transformer-based\nlanguage model and knowledge-based agent improves the LLM's ability to extract\nmeaningful financial information and generate more precise responses. Our\nevaluation on two real-world datasets shows that the proposed approach outperforms\nstate-of-the-art methods, achieving an average F1-score of 0.966 and 0.96",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22105263157894736,
          "p": 0.2625,
          "f": 0.2399999950367348
        },
        "rouge-2": {
          "r": 0.05384615384615385,
          "p": 0.0625,
          "f": 0.05785123469708396
        },
        "rouge-l": {
          "r": 0.21052631578947367,
          "p": 0.25,
          "f": 0.22857142360816338
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.07343v1",
      "true_abstract": "Seismic fragility curves express the probability of failure of a mechanical\nequipment conditional to an intensity measure derived from a seismic signal.\nAlthough based on a strong assumption, the probit-lognormal model is very\npopular among practitioners for estimating such curves, judging by its abundant\nuse in the literature. However, as this model is likely to lead to biased\nestimates, its use should be limited to cases for which only few data are\navailable. In practice, this involves having to resort to binary data which\nindicate the state of the structure when it has been subjected to a seismic\nloading, namely failure or non-failure. The question then arises of the choice\nof data that must be used to obtain an optimal estimate, that is to say the\nmost precise possible with the minimum of data. To answer this question, we\npropose a methodology for design of experiments in a Bayesian framework based\non the reference prior theory. This theory aims to define a so-called objective\nprior that favors data learning, which is slighty constrained in this work in\norder tackle the problems of likelihood degeneracy that are ubiquitous with\nsmall data sets. The novelty of our work is then twofold. First, we rigorously\npresent the problem of likelihood degeneracy which hampers frequentist\napproaches such as the maximum likelihood estimation. Then, we propose our\nstrategy inherited from the reference prior theory to build the data set. This\nstrategy aims to maximize the impact of the data on the posterior distribution\nof the fragility curve. Our method is applied to a case study of the nuclear\nindustry. The results demonstrate its ability to efficiently and robustly\nestimate the fragility curve, and to avoid degeneracy even with a limited\nnumber of experiments. Additionally, we demonstrate that the estimates quickly\nreach the model bias induced by the probit-lognormal modeling.",
      "generated_abstract": "opment of big data and artificial intelligence (AI) has enabled\nmore effective and efficient data analysis and decision-making. The increasing\npopulation of big data and the increasingly complex nature of the data has\nintroduced new challenges. The use of artificial intelligence (AI) in\nunderstanding and interpreting data is essential for developing effective\ndata-driven models. This paper explores the use of artificial intelligence to\nimprove the accuracy of decision-making. The paper introduces a methodology for\ndeveloping a machine learning model that uses Artificial Intelligence (AI) to\npredict the probability of an insurance claim occurring in a given year. The\nAI model uses data from the Australian Bureau of Statistics (ABS) to predict\nthe probability of a claim occurring in the future. The model is trained and\ntested on data from 2008 to 2021. The model was evaluated using",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11560693641618497,
          "p": 0.2702702702702703,
          "f": 0.16194331564129893
        },
        "rouge-2": {
          "r": 0.03900709219858156,
          "p": 0.09322033898305085,
          "f": 0.05499999584050032
        },
        "rouge-l": {
          "r": 0.11560693641618497,
          "p": 0.2702702702702703,
          "f": 0.16194331564129893
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.12393v1",
      "true_abstract": "This study investigates the emergence of power-law and other concentrated\ndistributions through a feedback loop model in crowd interactions. Agents act\nby their response functions to observations and external forces, while\nobservations change by the aggregated actions of all agents, weighted by their\nrespective influence, i.e. power or wealth. Agents wealth dynamically adjust\nbased on the alignment between an agents actions and observation outcomes:\nagents gain wealth when their actions align with observed trends and lose\nwealth otherwise. A reward function, that describes the change of agents wealth\nat each time step, manifests the differences of response functions of agents to\nobservations. When all agents responses are set to zero and feedback loop is\nbroken, agents wealth follow a normal or lognormal distribution. Otherwise,\nthis response-reward iterative feedback mechanism results in concentrated\nwealth distributions, characterized by a small number of dominant agents and\nthe marginalization of the majority. Contrasted to past studies, such\nconcentration is not limited only to asymptotic behavior at the upper tail for\nlarge variables, nor does it require the reward function to be linear to agents\nprevious wealth as formulated in random growth model and network preferential\nattachment. Probability density functions for various distributions are more\nvisually distinguishable for small values at the lower tail. In application of\nthis model, key differences in income and wealth distributions in the US vs\nJapan are attributed to different response functions of agents in the two\ncountries. The model applicability extends beyond social systems to other\nmany-body systems with analogous feedback mechanisms, where power-law\ndistributions represent a rare subset of general concentrated outcomes.",
      "generated_abstract": "y aims to assess the role of digitalization and the changing\ntechnological environment in the evolution of the workforce in the Republic of\nKosovo. We use quantitative and qualitative methods to analyze the data\ncollected from a survey conducted among the general public. The results show\nthat the labor market is characterized by an increased demand for digital\nskills, with the most significant increase in the demand for IT specialists and\ndata scientists. The results also show that the increase in demand for\ndigital-skills related occupations has led to a decrease in the demand for\ntraditional occupations, with the greatest decrease in the demand for\ntraditional occupations such as clerical and sales occupations. The results\nalso show that the labor market is characterized by a shift in the demand\nrelationship between traditional and digital occupations. Traditional occupations\nare increasingly demanding digital skills, while digital skills are increasing",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.2692307692307692,
          "f": 0.17573221317624002
        },
        "rouge-2": {
          "r": 0.019762845849802372,
          "p": 0.04310344827586207,
          "f": 0.027100266691931565
        },
        "rouge-l": {
          "r": 0.11801242236024845,
          "p": 0.24358974358974358,
          "f": 0.15899581150259984
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2503.08674v1",
      "true_abstract": "Machine Learning Force Fields (MLFFs) are a promising alternative to\nexpensive ab initio quantum mechanical molecular simulations. Given the\ndiversity of chemical spaces that are of interest and the cost of generating\nnew data, it is important to understand how MLFFs generalize beyond their\ntraining distributions. In order to characterize and better understand\ndistribution shifts in MLFFs, we conduct diagnostic experiments on chemical\ndatasets, revealing common shifts that pose significant challenges, even for\nlarge foundation models trained on extensive data. Based on these observations,\nwe hypothesize that current supervised training methods inadequately regularize\nMLFFs, resulting in overfitting and learning poor representations of\nout-of-distribution systems. We then propose two new methods as initial steps\nfor mitigating distribution shifts for MLFFs. Our methods focus on test-time\nrefinement strategies that incur minimal computational cost and do not use\nexpensive ab initio reference labels. The first strategy, based on spectral\ngraph theory, modifies the edges of test graphs to align with graph structures\nseen during training. Our second strategy improves representations for\nout-of-distribution systems at test-time by taking gradient steps using an\nauxiliary objective, such as a cheap physical prior. Our test-time refinement\nstrategies significantly reduce errors on out-of-distribution systems,\nsuggesting that MLFFs are capable of and can move towards modeling diverse\nchemical spaces, but are not being effectively trained to do so. Our\nexperiments establish clear benchmarks for evaluating the generalization\ncapabilities of the next generation of MLFFs. Our code is available at\nhttps://tkreiman.github.io/projects/mlff_distribution_shifts/.",
      "generated_abstract": "development of next-generation sequencing technologies has\npromoted the study of genetic diversity and evolution in biological systems.\nHowever, the complexity of modern genomes and the abundance of polymorphisms\ninvolve a significant challenge in understanding the genetic diversity and\nevolutionary processes in complex biological systems. To address this challenge,\nwe propose a novel approach called Genetic Diversity and Evolution in Complex\nBiological Systems (GenDECS). GenDECS employs a novel framework called\nGenetic Diversity Graph (GDG) to represent genetic diversity and evolutionary\nprocesses in complex biological systems, including genomes, transcriptomes,\nmetabolomes, and proteomes. In particular, GDG captures the underlying\ngenetic diversity and evolutionary processes in complex biological systems by\nconnecting genetic variation and evolutionary processes through genetic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08484848484848485,
          "p": 0.20588235294117646,
          "f": 0.1201716696863086
        },
        "rouge-2": {
          "r": 0.00425531914893617,
          "p": 0.011111111111111112,
          "f": 0.006153842149115032
        },
        "rouge-l": {
          "r": 0.08484848484848485,
          "p": 0.20588235294117646,
          "f": 0.1201716696863086
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.18008v4",
      "true_abstract": "We introduce NotaGen, a symbolic music generation model aiming to explore the\npotential of producing high-quality classical sheet music. Inspired by the\nsuccess of Large Language Models (LLMs), NotaGen adopts pre-training,\nfine-tuning, and reinforcement learning paradigms (henceforth referred to as\nthe LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC\nnotation, and then fine-tuned on approximately 9K high-quality classical\ncompositions conditioned on \"period-composer-instrumentation\" prompts. For\nreinforcement learning, we propose the CLaMP-DPO method, which further enhances\ngeneration quality and controllability without requiring human annotations or\npredefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in\nsymbolic music generation models with different architectures and encoding\nschemes. Furthermore, subjective A/B tests show that NotaGen outperforms\nbaseline models against human compositions, greatly advancing musical\naesthetics in symbolic music generation.",
      "generated_abstract": "r presents a novel audio-to-text model that can generate\ntext from audio samples in a self-supervised manner. Our model,\nA2T-Self-Supervised, is based on an audio-to-text model called\nA2T-Audio. This model employs a hierarchical attention mechanism to\ngenerate text from audio samples using self-supervised learning. A2T-Self-Supervised\nachieves state-of-the-art results on the LibriSpeech dataset, achieving\nF1-score of 0.869 and BLEU-4 score of 0.952. The model also achieves\nstate-of-the-art results on the CleanSpeech dataset, achieving F1-score of\n0.767 and BLEU-4 score of 0.858. Additionally, our model outperforms\nstate-of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13131313131313133,
          "p": 0.22413793103448276,
          "f": 0.16560509088238887
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12121212121212122,
          "p": 0.20689655172413793,
          "f": 0.15286623737920418
        }
      }
    },
    {
      "paper_id": "physics.geo-ph.stat/AP/2502.19549v1",
      "true_abstract": "This paper presents a comparative analysis of structural seismic responses\nunder two types of ground motion inputs: (i) synthetic motions generated by\nstochastic ground motion models and (ii) recorded motions from an earthquake\ndatabase. Five key seismic response metrics - probability distributions,\nstatistical moments, correlations, tail indices, and variance-based global\nsensitivity indices - are systematically evaluated for two archetypal\nstructures: a 12-story medium-period building and a high-rise long-period\ntower. Both ground motion datasets are calibrated to a shared response\nspectrum, ensuring consistency in spectral characteristics, including spectral\nmedian, variance, and correlation structure. The analysis incorporates both\naleatory uncertainties from ground motion variability and epistemic\nuncertainties associated with structural parameters, providing a comprehensive\ncomparison of seismic responses. The results demonstrate close agreement in\nglobal response characteristics, including distributions, correlations, and\nsensitivity indices, between synthetic and recorded motions, with differences\ntypically within 15\\%. However, significant discrepancies are observed under\nextreme conditions, particularly in tail behavior, higher-order moments, and\ndrift responses of long-period structures, with differences exceeding 50\\%.\nThese discrepancies are attributed to the non-Gaussian features and complex\ncharacteristics inherent in recorded motions, which are less pronounced in\nsynthetic datasets. The findings support the use of synthetic ground motions\nfor evaluating global seismic response characteristics, while highlighting\ntheir limitations in capturing rare-event behavior and long-period structural\ndynamics.",
      "generated_abstract": "We propose a novel Bayesian framework for modeling geologic faults, based on\nthe idea of a fractal domain. The fractal domain model is defined by a\ngeometric object with a finite number of elements, with the elements being\nfractal domains. The fractal domain model is used as a prior distribution on\nthe fault model, which is then combined with a Gaussian process prior. The\nresults of the Bayesian inference are then used to construct a fractal domain\nmodel of the fault. In this way, we can use fractal domain models to model\nfaults. We demonstrate that the fractal domain model is able to reproduce\nseveral prominent faults. We further demonstrate that the model can be used to\nmodel faults with different geometry and geometry of the fault plane.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11278195488721804,
          "p": 0.22388059701492538,
          "f": 0.1499999955445001
        },
        "rouge-2": {
          "r": 0.0049261083743842365,
          "p": 0.009708737864077669,
          "f": 0.006535943246404008
        },
        "rouge-l": {
          "r": 0.09774436090225563,
          "p": 0.19402985074626866,
          "f": 0.12999999554450015
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.02292v1",
      "true_abstract": "Selecting the right monitoring level in Remote Patient Monitoring (RPM)\nsystems for e-healthcare is crucial for balancing patient outcomes, various\nresources, and patient's quality of life. A prior work has used one-dimensional\nhealth representations, but patient health is inherently multidimensional and\ntypically consists of many measurable physiological factors. In this paper, we\nintroduce a multidimensional health state model within the RPM framework and\nuse dynamic programming to study optimal monitoring strategies. Our analysis\nreveals that the optimal control is characterized by switching curves (for\ntwo-dimensional health states) or switching hyper-surfaces (in general):\npatients switch to intensive monitoring when health measurements cross a\nspecific multidimensional surface. We further study how the optimal switching\ncurve varies for different medical conditions and model parameters. This\nfinding of the optimal control structure provides actionable insights for\nclinicians and aids in resource planning. The tunable modeling framework\nenhances the applicability and effectiveness of RPM services across various\nmedical conditions.",
      "generated_abstract": "r presents a novel multi-agent control strategy for an autonomous\ntransportation system that includes both humans and autonomous vehicles. The\nagents' objectives are to achieve the lowest total travel time for all\nvehicles and for all passengers. A multi-agent network is formed with each\nagent representing a passenger, and a multi-agent reinforcement learning\n(MARL) framework is used to optimize the network's structure and control\npolicies. The MARL agent selects the optimal route, which is determined by\noptimizing the travel time of passengers with different destinations. The\nindividual agents then use the optimal route to control their vehicles. The\nnetwork structure is optimized based on the objective function, including the\ntotal travel time for all vehicles and for all passengers. The results\ndemonstrate that the proposed MARL strategy can optimize the network's\nstructure and control policies, resulting in reduced travel time for all\nve",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17117117117117117,
          "p": 0.2345679012345679,
          "f": 0.1979166617887371
        },
        "rouge-2": {
          "r": 0.013422818791946308,
          "p": 0.01652892561983471,
          "f": 0.014814809868588755
        },
        "rouge-l": {
          "r": 0.16216216216216217,
          "p": 0.2222222222222222,
          "f": 0.18749999512207044
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/TH/2502.10317v1",
      "true_abstract": "Understanding directionality is crucial for identifying causal structures\nfrom observational data. A key challenge lies in detecting collider structures,\nwhere a $V$--structure is formed between a child node $Z$ receiving directed\nedges from parents $X$ and $Y$, denoted by $X \\rightarrow Z \\leftarrow Y$.\nTraditional causal discovery approaches, such as constraint-based and\nscore-based structure learning algorithms, do not provide statistical inference\non estimated pathways and are often sensitive to latent confounding. To\novercome these issues, we introduce methodology to quantify directionality in\ncollider structures using a pair of conditional asymmetry coefficients to\nsimultaneously examine validity of the pathways $Y \\rightarrow Z$ and $X\n\\rightarrow Z$ in the collider structure. These coefficients are based on\nShannon's differential entropy. Leveraging kernel-based conditional density\nestimation and a nonparametric smoothing technique, we utilise our proposed\nmethod to estimate collider structures and provide uncertainty quantification.\n  Simulation studies demonstrate that our method outperforms existing structure\nlearning algorithms in accurately identifying collider structures. We further\napply our approach to investigate the role of blood pressure as a collider in\nepigenetic DNA methylation, uncovering novel insights into the genetic\nregulation of blood pressure. This framework represents a significant\nadvancement in causal structure learning, offering a robust, nonparametric\nmethod for collider detection with practical applications in biostatistics and\nepidemiology.",
      "generated_abstract": "eal-world problems, we are often interested in inferring a\nparameter or a function of interest from data collected from a limited number\nof samples. In such settings, the number of samples available is usually\nlimited, and the number of samples needed to obtain a sufficient amount of\ninformative data is not known. We introduce a novel statistical framework for\ninferring functions and parameters from data that combines the powerful\nstatistical tools of Bayesian inference with a novel approach to selecting the\nnumber of samples required to obtain a sufficient amount of data. Our\nframework is based on a novel way to select the number of samples that is\nbased on a theoretical analysis of the expected value of the error. Our theory\nalso provides a way to select the number of samples that is based on the\nexpected value of the error, and provides a way to select the number of\nsamples that is based on the expected value of the error, and provides a way",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.37681159420289856,
          "f": 0.24528301447712716
        },
        "rouge-2": {
          "r": 0.019801980198019802,
          "p": 0.03773584905660377,
          "f": 0.025974021459774777
        },
        "rouge-l": {
          "r": 0.16083916083916083,
          "p": 0.3333333333333333,
          "f": 0.21698112768467434
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2502.20943v1",
      "true_abstract": "Reference-based image super-resolution (RefSR) represents a promising\nadvancement in super-resolution (SR). In contrast to single-image\nsuper-resolution (SISR), RefSR leverages an additional reference image to help\nrecover high-frequency details, yet its vulnerability to backdoor attacks has\nnot been explored. To fill this research gap, we propose a novel attack\nframework called BadRefSR, which embeds backdoors in the RefSR model by adding\ntriggers to the reference images and training with a mixed loss function.\nExtensive experiments across various backdoor attack settings demonstrate the\neffectiveness of BadRefSR. The compromised RefSR network performs normally on\nclean input images, while outputting attacker-specified target images on\ntriggered input images. Our study aims to alert researchers to the potential\nbackdoor risks in RefSR. Codes are available at\nhttps://github.com/xuefusiji/BadRefSR.",
      "generated_abstract": "tent is a critical component of modern digital entertainment,\nextending beyond the traditional use cases of media and gaming. However, the\nemergence of deepfakes, a rapidly evolving form of digital manipulation, has\ncaused significant concerns. Existing approaches for detecting deepfakes in\nvideo content often rely on supervised learning techniques, which are limited\nto a small number of deepfake datasets and are prone to overfitting. In this\npaper, we propose a novel approach for deepfake detection in video content using\na transformer-based architecture. Our method employs a novel data augmentation\nmethod that leverages high-quality video-text pairs to generate diverse\nsample-level images. These images are then used to train a transformer-based\nmodel that classifies the sample images as deepfake or normal video. To\ncombat overfitting, we propose a novel loss function that leverages",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22916666666666666,
          "p": 0.23655913978494625,
          "f": 0.23280422780549268
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.03278688524590164,
          "f": 0.033057846240011686
        },
        "rouge-l": {
          "r": 0.21875,
          "p": 0.22580645161290322,
          "f": 0.22222221722348207
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.07359v1",
      "true_abstract": "In this paper, we present an advanced wind turbine control scheme for power\nmaximization as well as for active power control, which is designed using\n$\\mathcal{H}_\\infty$ loop-shaping. Our approach involves the synthesis of two\nseparate controllers for two different operating modes. To ensure smooth\ntransitions between these modes, we implement a bumpless transfer strategy that\nreduces transient effects. A comprehensive case study demonstrates the efficacy\nof our control scheme, showing significant improvements in power tracking\naccuracy and a reduction in mechanical wear. Moreover, our control strategy\ncomes with robust stability guarantees.",
      "generated_abstract": "This paper proposes a novel nonlinear iterative scheme for the control of\nnonlinear systems with nonlinear dynamic feedback control. The proposed\nproposed methodology utilizes the nonlinear dynamics of the control system to\ncontrol the system dynamics. It uses a Lyapunov function to compute a\nnonlinear approximation of the system dynamics, and an iterative scheme to\nupdate the approximation. This iterative scheme is based on the Newton's\nmethod and a semi-definite relaxation algorithm. The iterative scheme is\nvalidated using numerical examples for both linear and nonlinear systems.\nFurthermore, the effectiveness of the proposed method is validated by\ncomparing the results with a Newton's method using a constant control parameter.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14473684210526316,
          "p": 0.2037037037037037,
          "f": 0.16923076437396462
        },
        "rouge-2": {
          "r": 0.02247191011235955,
          "p": 0.020833333333333332,
          "f": 0.021621616628781284
        },
        "rouge-l": {
          "r": 0.14473684210526316,
          "p": 0.2037037037037037,
          "f": 0.16923076437396462
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.q-bio/MN/2409.05827v1",
      "true_abstract": "Many biological decision-making processes can be viewed as performing a\nclassification task over a set of inputs, using various chemical and physical\nprocesses as \"biological hardware.\" In this context, it is important to\nunderstand the inherent limitations on the computational expressivity of\nclassification functions instantiated in biophysical media. Here, we model\nbiochemical networks as Markov jump processes and train them to perform\nclassification tasks, allowing us to investigate their computational\nexpressivity. We reveal several unanticipated limitations on the input-output\nfunctions of these systems, which we further show can be lifted using\nbiochemical mechanisms like promiscuous binding. We analyze the flexibility and\nsharpness of decision boundaries as well as the classification capacity of\nthese networks. Additionally, we identify distinctive signatures of networks\ntrained for classification, including the emergence of correlated subsets of\nspanning trees and a creased \"energy landscape\" with multiple basins. Our\nfindings have implications for understanding and designing physical computing\nsystems in both biological and synthetic chemical settings.",
      "generated_abstract": "t an integrated theoretical framework for understanding the\nphysics of the collective dynamics of a single-cell organism. Our framework\nintegrates the dynamics of single-cell populations with the dynamics of a\nhigh-dimensional population of collective cells. We demonstrate that the\ncollective dynamics of a single-cell population can be expressed as the\nconvolution of a deterministic spatially-varying field and a stochastic\ntemporal-varying field. We propose a general theoretical framework to model\nthe dynamics of a population of single-cells undergoing spatially-varying\ndynamics. This framework includes the dynamics of the population of cells in\ntheir native environment, the dynamics of the population of cells in the\nextracellular space, and the dynamics of the population of cells in the\nintracellular space. We show that this framework provides a unified framework\nfor understanding the collective dynamics of a population",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16363636363636364,
          "p": 0.32142857142857145,
          "f": 0.21686746540862253
        },
        "rouge-2": {
          "r": 0.025974025974025976,
          "p": 0.044444444444444446,
          "f": 0.03278688058989586
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.26785714285714285,
          "f": 0.18072288709536954
        }
      }
    },
    {
      "paper_id": "cs.LG.q-fin/GN/2411.10325v1",
      "true_abstract": "Bitcoin, launched in 2008 by Satoshi Nakamoto, established a new digital\neconomy where value can be stored and transferred in a fully decentralized\nmanner - alleviating the need for a central authority. This paper introduces a\nlarge scale dataset in the form of a transactions graph representing\ntransactions between Bitcoin users along with a set of tasks and baselines. The\ngraph includes 252 million nodes and 785 million edges, covering a time span of\nnearly 13 years of and 670 million transactions. Each node and edge is\ntimestamped. As for supervised tasks we provide two labeled sets i. a 33,000\nnodes based on entity type and ii. nearly 100,000 Bitcoin addresses labeled\nwith an entity name and an entity type. This is the largest publicly available\ndata set of bitcoin transactions designed to facilitate advanced research and\nexploration in this domain, overcoming the limitations of existing datasets.\nVarious graph neural network models are trained to predict node labels,\nestablishing a baseline for future research. In addition, several use cases are\npresented to demonstrate the dataset's applicability beyond Bitcoin analysis.\nFinally, all data and source code is made publicly available to enable\nreproducibility of the results.",
      "generated_abstract": "In this paper, we introduce a novel algorithm for the estimation of\nthe mean of a random variable. Our method is based on a decomposition of the\nrandom variable into a sum of several Gamma random variables. The Gamma random\nvariables are distributed according to a Dirichlet process. To estimate the\nmean of the sum of Gamma random variables, we propose a method that allows for\na more flexible modeling of the distribution of the sum of Gamma random\nvariables. To illustrate our method, we consider the estimation of the mean of\na logarithmic time series in a stock market environment. In this setting, we\nestimate the mean of the logarithmic time series by combining our method with\na Gaussian process regression.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13636363636363635,
          "p": 0.3,
          "f": 0.1874999957031251
        },
        "rouge-2": {
          "r": 0.031578947368421054,
          "p": 0.06521739130434782,
          "f": 0.042553187093205026
        },
        "rouge-l": {
          "r": 0.11363636363636363,
          "p": 0.25,
          "f": 0.15624999570312512
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/PR/2403.15810v1",
      "true_abstract": "National football teams increasingly issue tradeable blockchain-based fan\ntokens to strategically enhance fan engagement. This study investigates the\nimpact of 2022 World Cup matches on the dynamic performance of each team's fan\ntoken. The event study uncovers fan token returns surged six months before the\nWorld Cup, driven by positive anticipation effects. However, intraday analysis\nreveals a reversal of fan token returns consistently declining and trading\nvolumes rising as matches unfold. To explain findings, we uncover asymmetries\nwhereby defeats in high-stake matches caused a plunge in fan token returns,\ncompared to low-stake matches, intensifying in magnitude for knockout matches.\nContrarily, victories enhance trading volumes, reflecting increased market\nactivity without a corresponding positive effect on returns. We align findings\nwith the classic market adage \"buy the rumor, sell the news,\" unveiling\ncognitive biases and nuances in investor sentiment, cautioning the dichotomy of\npre-event optimism and subsequent performance declines.",
      "generated_abstract": "r investigates the impact of macroeconomic variables on the stock\nmarket through a cointegration framework. The study employs the monthly data\nfrom 1990 to 2019 of the S\\&P 500 index to analyze the cointegration relationship\nbetween the U.S. macroeconomy and the stock market. A CAR model is constructed\nto analyze the cointegration relationship. The results show that the\ncointegration relationship between the U.S. macroeconomy and the stock market\nis stable, which indicates that the U.S. macroeconomic factors are stable and\ndo not affect the stock market. In addition, we also analyze the cointegration\nrelationship between the U.S. macroeconomy and the major stock market\nindices. The results show that the cointegration relationship between the U.S.\nmacroeconomy and the major stock market indices",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11009174311926606,
          "p": 0.21428571428571427,
          "f": 0.14545454097043173
        },
        "rouge-2": {
          "r": 0.028169014084507043,
          "p": 0.05333333333333334,
          "f": 0.03686635492365577
        },
        "rouge-l": {
          "r": 0.11009174311926606,
          "p": 0.21428571428571427,
          "f": 0.14545454097043173
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.00492v1",
      "true_abstract": "We introduce a nonparametric spectral density estimator for fully irregularly\nsampled points in one or more dimensions, constructed using a weighted\nnonuniform Fourier sum whose weights yield a high-accuracy quadrature rule for\na user-specified window function. The resulting estimator significantly reduces\nthe aliasing seen in periodogram approaches and least squares spectral\nanalysis, sidesteps the dangers of ill-conditioning of the nonuniform Fourier\ninverse problem, and can be adapted to a wide variety of irregular sampling\nsettings. After a discussion of methods for computing the necessary weights and\na theoretical analysis of sources of bias, we close with demonstrations of the\nmethod's efficacy, including for processes that exhibit very slow spectral\ndecay and for processes in multiple dimensions.",
      "generated_abstract": "deals with the problem of evaluating the accuracy of the\n(non-parametric) quantile regression model. The main focus is on the\napplication of the quantile regression model to the analysis of time series\ndata, and its comparison with the classical quantile regression model (QRM).\nThe proposed approach is based on the use of the quantile bootstrap. The\nquantile bootstrap allows for the calculation of quantile forecasts, which are\nused for the assessment of the accuracy of the quantile regression model. The\nmain advantage of the quantile bootstrap is that it provides a reliable\nestimation of the prediction interval, which is particularly important in the\nevaluation of the accuracy of the quantile regression model. In addition, the\nquantile bootstrap allows for the calculation of the quantile residuals, which\nare used for the assessment of the accuracy of the quantile regression model.\nThe quantile residuals are particularly",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13095238095238096,
          "p": 0.19298245614035087,
          "f": 0.15602836397766726
        },
        "rouge-2": {
          "r": 0.017857142857142856,
          "p": 0.022727272727272728,
          "f": 0.019999995072001214
        },
        "rouge-l": {
          "r": 0.10714285714285714,
          "p": 0.15789473684210525,
          "f": 0.12765956965142616
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.15275v1",
      "true_abstract": "Factor-based forecasting using Principal Component Analysis (PCA) is an\neffective machine learning tool for dimension reduction with many applications\nin statistics, economics, and finance. This paper introduces a Supervised\nScreening and Regularized Factor-based (SSRF) framework that systematically\naddresses high-dimensional predictor sets through a structured four-step\nprocedure integrating both static and dynamic forecasting mechanisms. The\nstatic approach selects predictors via marginal correlation screening and\nscales them using univariate predictive slopes, while the dynamic method\nscreens and scales predictors based on time series regression incorporating\nlagged predictors. PCA then extracts latent factors from the scaled predictors,\nfollowed by LASSO regularization to refine predictive accuracy. In the\nsimulation study, we validate the effectiveness of SSRF and identify its\nparameter adjustment strategies in high-dimensional data settings. An empirical\nanalysis of macroeconomic indices in China demonstrates that the SSRF method\ngenerally outperforms several commonly used forecasting techniques in\nout-of-sample predictions.",
      "generated_abstract": "aper, we study the estimation and inference of a general\nregression discontinuity (GRD) model using a novel framework. We propose a\nnovel estimator that takes advantage of the randomization structure of the\nGRD model. This estimator is a randomized version of the standard estimator\nbased on the observed treatment effects. Our estimator is an extension of the\nlabeled-data estimator that was proposed by Shi and Yuan (2023) and uses a\ncombination of the labeled data and a propensity score-based method to estimate\nthe treatment effect. We show that our estimator has a closed-form expression\nand is consistent. We also provide theoretical results on the asymptotic\nnormality and the asymptotic power of our estimator. We conduct an\nappropriate simulation study to validate the finite-sample performance of our\nestimator. Finally, we apply our estimator",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17094017094017094,
          "p": 0.2597402597402597,
          "f": 0.20618556222287185
        },
        "rouge-2": {
          "r": 0.02097902097902098,
          "p": 0.025423728813559324,
          "f": 0.02298850079300176
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.23376623376623376,
          "f": 0.18556700552184094
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.09530v1",
      "true_abstract": "In this paper, we present a comprehensive review of the analysis of the\nwell-known $1 - 1/e$ upper bound on the competitiveness that any online\nalgorithm can achieve, as established in the classical paper by Karp, Vazirani,\nand Vazirani (STOC 1990). We discuss in detail all the minor and major\ntechnical issues in their approach and present a \\emph{simple yet rigorous}\nmethod to address them. Specifically, we show that the upper bound of $n(1 -\n1/e) + o(n)$ on the performance of any online algorithm, as shown in the paper,\ncan be replaced by $\\lceil n \\cdot (1 - 1/e) + 2 - 1/e \\rceil$. Our approach is\nnotable for its simplicity and is significantly less technically involved than\nexisting ones.",
      "generated_abstract": "The problem of computing the optimal solution to a distributed nonlinear\noptimal control problem is known to be NP-hard. In this paper, we propose an\niterative heuristic that outperforms existing approaches for this problem.\nThe key innovation of our approach is to use a low-rank approximation of the\nHamiltonian matrix, which is a key component of the optimal control problem.\nThis allows us to perform a few rounds of gradient descent on the approximation\nto update the optimal control, resulting in a computationally efficient\nalgorithm. We compare our approach with existing methods, and demonstrate its\neffectiveness in the context of nonlinear optimization.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23529411764705882,
          "p": 0.30303030303030304,
          "f": 0.2649006573308189
        },
        "rouge-2": {
          "r": 0.0625,
          "p": 0.07368421052631578,
          "f": 0.06763284527526933
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.25757575757575757,
          "f": 0.2251655579930706
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/SR/2503.10456v1",
      "true_abstract": "Aims. In the low-collisional, partially ionized plasma (PIP) of solar\nprominences, uncharged emitters might show different signatures of magnetic\nline broadening than charged emitters. We investigate if the widths of weak\nmetall emissions in prominences exceed the thermal line broadening by a\ndifferent amount for charged and for uncharged emitters.\n  Methods. We simultaneously observe five optically thin, weak metall lines in\nthe brightness center of a quiescent prominence and compare their observed\nwidths with the thermal broadening.\n  Results. The inferred non-thermal broadening of the metall lines does not\nindicate systematic differences between the uncharged Mg b2 and Na D1 and the\ncharged Fe II emitters, only Sr II is broader.\n  Conclusions. The additional line broadening of charged emitters is reasonably\nattributed to magnetic forces. That of uncharged emitters can then come from\ntheir temporary state as ion before recombination. Magnetically induced\nvelocities will retain some time after recombination. Modelling partially\nionized plasmas then requires consideration of a memory of previous ionization\nstates.",
      "generated_abstract": "t the results of a 10-year-long survey of the Large Magellanic\nCloud (LMC) using the VLT Survey Telescope (VST), the Subaru Telescope (ST),\nand the Magellan Telescopes of the Las Campanas Observatory (MtLC). This\nsurvey, named VST-MtLC LMC Survey (VMLCS), is designed to study the LMC\ngalaxy population, its star formation history, and the evolution of its stellar\npopulations. Our analysis includes the discovery of 107 new stars in the LMC,\nincluding 37 new members of the Magellanic Stream. We report on the first\nidentification of a new member of the M13 globular cluster system, providing\nevidence for an ongoing star formation episode. We also detect 10 new members of\nthe Magellanic Clouds, which",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08411214953271028,
          "p": 0.11538461538461539,
          "f": 0.09729729242016094
        },
        "rouge-2": {
          "r": 0.02666666666666667,
          "p": 0.038834951456310676,
          "f": 0.03162054853223839
        },
        "rouge-l": {
          "r": 0.08411214953271028,
          "p": 0.11538461538461539,
          "f": 0.09729729242016094
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.10511v1",
      "true_abstract": "One of the main challenges in children's speaker verification (C-SV) is the\nsignificant change in children's voices as they grow. In this paper, we propose\ntwo approaches to improve age-related robustness in C-SV. We first introduce a\nFeature Transform Adapter (FTA) module that integrates local patterns into\nhigher-level global representations, reducing overfitting to specific local\nfeatures and improving the inter-year SV performance of the system. We then\nemploy Synthetic Audio Augmentation (SAA) to increase data diversity and size,\nthereby improving robustness against age-related changes. Since the lack of\nlongitudinal speech datasets makes it difficult to measure age-related\nrobustness of C-SV systems, we introduce a longitudinal dataset to assess\ninter-year verification robustness of C-SV systems. By integrating both of our\nproposed methods, the average equal error rate was reduced by 19.4%, 13.0%, and\n6.1% in the one-year, two-year, and three-year gap inter-year evaluation sets,\nrespectively, compared to the baseline.",
      "generated_abstract": "In this paper, we introduce a novel approach to 3D Speaker Verification\n(SV) using a Convolutional Neural Network (CNN) architecture. Our method\nintegrates both speaker embedding and speaker identity embedding into a single\nmulti-task model, enabling the task of 3D speaker verification. We employ\nthree different speaker embedding techniques, namely, LDA, Latent Dirichlet\nAllocation (LDA), and Independent Component Analysis (ICA), to generate speaker\nembeddings for both the speaker identity and speaker embedding tasks. We also\nintroduce an attention mechanism to enhance the fusion of the speaker embeddings\nacross the three tasks. Our approach is evaluated on the CLEF 3D Speaker\nVerification dataset, which includes 3D speaker verification and 3D speaker\nidentification, and the results demonstrate that our model outperforms\nexisting methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.2564102564102564,
          "f": 0.21276595259167055
        },
        "rouge-2": {
          "r": 0.04861111111111111,
          "p": 0.06422018348623854,
          "f": 0.05533596347513675
        },
        "rouge-l": {
          "r": 0.14545454545454545,
          "p": 0.20512820512820512,
          "f": 0.17021276110230887
        }
      }
    },
    {
      "paper_id": "q-bio.SC.q-bio/SC/2410.20644v1",
      "true_abstract": "Fungal keratitis is a severe vision-threatening corneal infection with a\nprognosis influenced by fungal virulence and the host's immune defense\nmechanisms. The immune system, through its regulation of the inflammatory\nresponse, ensures cells and tissues can effectively activate defense mechanisms\nin response to infection and injury. However, there is still a lack of\neffective drugs that attenuate fungal virulence while relieving the\ninflammatory response caused by fungal keratitis. Therefore, finding effective\ntreatments to solve these problems is particularly important.\n  We synthesized ZIF-90 by water-based synthesis and characterized by SEM, XRD\netc. In vitro experiments included CCK-8 and ELISA. These evaluations verified\nthe disruptive effects of ZIF-90 on Aspergillus. fumigatus spore adhesion,\nmorphology, cell membrane, and the effect of ZIF-90 on apoptosis. In addition,\nto investigate whether the metal-ligand zinc and the organic ligand imidazole\nact as essential factors in ZIF-90, we investigated the in vitro antimicrobial\nand anti-inflammatory effects of ZIF-8, ZIF-67, and MOF-74 (Zn) by MIC and\nELISA experiments.\n  ZIF-90 has therapeutic effects on fungal keratitis, which could break the\nprotective organelles of Aspergillus. fumigatus, such as the cell wall. In\naddition, ZIF-90 can avoid excessive inflammatory response by promoting\napoptosis of inflammatory cells. The results demonstrated that both zinc ions\nand imidazole possessed antimicrobial and anti-inflammatory effects. In\naddition, ZIF-90 exhibited better biocompatibility compared to ZIF-8, ZIF-67,\nand MOF-74 (Zn).\n  ZIF-90 has anti-inflammatory and antifungal effects and preferable\nbiocompatibility, and has great potential for the treatment of fungal\nkeratitis.",
      "generated_abstract": "e a simple method to perform the first direct measurements of\nthe in vivo activity of a small molecule in the human brain. The method\ninvolves the injection of the small molecule into the human brain and the\nmeasurement of its concentration in the cerebrospinal fluid (CSF) using a\nmagnetic resonance imaging (MRI) probe. The method is applicable to a wide\nrange of small molecules and has the potential to provide critical insights\ninto the therapeutic potential of small molecules. We have applied the method\nto the cocaine metabolite 3,4-methylenedioxymethamphetamine (MDMA), a\nclassical psychoactive drug, and to the non-competitive N-methyl-D-aspartate\n(NMDA) receptor antagonist, ket",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09352517985611511,
          "p": 0.20967741935483872,
          "f": 0.12935322956461487
        },
        "rouge-2": {
          "r": 0.018433179723502304,
          "p": 0.043478260869565216,
          "f": 0.025889963455766742
        },
        "rouge-l": {
          "r": 0.07913669064748201,
          "p": 0.1774193548387097,
          "f": 0.10945273205217709
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2410.23297v1",
      "true_abstract": "We propose a new way of building portfolios of cryptocurrencies that provide\ngood diversification properties to investors. First, we seek to filter these\ndigital assets by creating some clusters based on their path signature. The\ngoal is to identify similar patterns in the behavior of these highly volatile\nassets. Once such clusters have been built, we propose \"optimal\" portfolios by\ncomparing the performances of such portfolios to a universe of unfiltered\ndigital assets. Our intuition is that clustering based on path signatures will\nmake it easier to capture the main trends and features of a group of\ncryptocurrencies, and allow parsimonious portfolios that reduce excessive\ntransaction fees. Empirically, our assumptions seem to be satisfied.",
      "generated_abstract": "ntext of the financial crisis, the possibility of a contagion\nbetween banks was considered as one of the main risks. This paper aims to\ninvestigate the impact of contagion risk on the financial system and to study\nthe evolution of the contagion risk. The results show that the contagion risk\nis a source of uncertainty and volatility in the financial system. The\ncontagion risk is considered as a driver of financial instability, which is\ncaused by the disruption of the financial system. The contagion risk is\ncharacterized by a complex and nonlinear dynamic system, which leads to a\ndifferent evolution of the system depending on the time horizon. The results\nsuggest that the contagion risk is more likely to increase when the system is\nin an unstable phase and when the system is more vulnerable to shocks. The\ncontagion risk is also considered as a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15,
          "p": 0.1875,
          "f": 0.16666666172839517
        },
        "rouge-2": {
          "r": 0.036036036036036036,
          "p": 0.037037037037037035,
          "f": 0.036529675366235755
        },
        "rouge-l": {
          "r": 0.1375,
          "p": 0.171875,
          "f": 0.15277777283950636
        }
      }
    },
    {
      "paper_id": "cs.SC.cs/SC/2502.11787v1",
      "true_abstract": "We propose a version of the classical shape lemma for zero-dimensional ideals\nof a commutative multivariate polynomial ring to the noncommutative setting of\nzero-dimensional ideals in an algebra of differential operators.",
      "generated_abstract": "r presents the first empirical study of the use of Squiggle as an\nacquisition tool for a multi-agent system with bounded rationality. We use the\nwell-known Squiggle-based game to demonstrate the value of Squiggle in\nreinforcement learning. We propose two different strategies to use Squiggle as\nan acquisition tool. The first strategy is based on the assumption that Squiggle\ncan be seen as an agent with limited rationality. The second strategy assumes\nthat Squiggle's acquisition behavior is governed by the intrinsic reward\nfunction of the underlying game. We evaluate the performance of these two\nstrategies through simulations and we also analyze the impact of Squiggle's\nacquisition behavior on the convergence of the multi-agent system. The results\nshow that the second strategy achieves a higher convergence rate and provides\nbetter results in terms",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.375,
          "p": 0.12,
          "f": 0.1818181781450873
        },
        "rouge-2": {
          "r": 0.06896551724137931,
          "p": 0.01694915254237288,
          "f": 0.027210881186543013
        },
        "rouge-l": {
          "r": 0.3333333333333333,
          "p": 0.10666666666666667,
          "f": 0.1616161579430671
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.12211v1",
      "true_abstract": "This study presents a comprehensive techno-economic analysis of gray, blue,\nand green hydrogen production pathways, evaluating their cost structures,\ninvestment feasibility, infrastructure challenges, and policy-driven cost\nreductions. The findings confirm that gray hydrogen (1.50-2.50/kg) remains the\nmost cost-effective today but is increasingly constrained by carbon pricing.\nBlue hydrogen (2.00-3.50/kg) offers a transitional pathway but depends on CCS\ncosts, natural gas price volatility, and regulatory support. Green hydrogen\n(3.50-6.00/kg) is currently the most expensive but benefits from declining\nrenewable electricity costs, electrolyzer efficiency improvements, and\ngovernment incentives such as the Inflation Reduction Act (IRA), which provides\ntax credits of up to 3.00/kg. The analysis shows that renewable electricity\ncosts below 20-30/MWh are essential for green hydrogen to achieve cost parity\nwith fossil-based hydrogen. The DOE's Hydrogen Shot Initiative aims to lower\ngreen hydrogen costs to 1.00/kg by 2031, emphasizing the need for CAPEX\nreductions, economies of scale, and improved electrolyzer efficiency.\nInfrastructure remains a critical challenge, with pipeline retrofitting\nreducing transport costs by 50-70%, though liquefied hydrogen and chemical\ncarriers remain costly due to energy losses and reconversion expenses.\nInvestment trends indicate a shift toward green hydrogen, with over 250 billion\nprojected by 2035, surpassing blue hydrogen's expected 100 billion. Carbon\npricing above $100/ton CO2 will likely make gray hydrogen uncompetitive by\n2030, accelerating the shift to low-carbon hydrogen. Hydrogen's long-term\nviability depends on continued cost reductions, policy incentives, and\ninfrastructure expansion, with green hydrogen positioned as a cornerstone of\nthe net-zero energy transition by 2035.",
      "generated_abstract": "The paper examines the role of the financial sector in developing countries,\nby analyzing the financial sector and financial integration in 158 developing\ncountries, spanning 1960-2025. We employ a dynamic panel model with endogenous\nfinancial shocks, including a comprehensive financial sector framework, to\nexplore the role of financial intermediaries in supporting development. Our\nfindings reveal that financial integration is positively associated with\neconomic growth, and that financial sector strength is particularly\ninfluential for countries with higher development levels. We also observe that\nfinancial intermediaries play a critical role in supporting development,\nhighlighting the importance of strengthening financial systems. These findings\ncontribute to the literature on financial integration, development, and the\nfinancial sector, offering valuable insights into the role of the financial\nsector in economic growth.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08571428571428572,
          "p": 0.2112676056338028,
          "f": 0.12195121540584324
        },
        "rouge-2": {
          "r": 0.012295081967213115,
          "p": 0.02857142857142857,
          "f": 0.017191972870502916
        },
        "rouge-l": {
          "r": 0.08571428571428572,
          "p": 0.2112676056338028,
          "f": 0.12195121540584324
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08712v1",
      "true_abstract": "This study introduces the SHAP-integrated convolutional diagnostic network\n(SICDN), an interpretable feature selection method designed for limited\ndatasets, to address the challenge posed by data privacy regulations that\nrestrict access to medical datasets. The SICDN model was tested on\nclassification tasks using pneumonia and breast cancer datasets, demonstrating\nover 97% accuracy and surpassing four popular CNN models. We also integrated a\nhistorical weighted moving average technique to enhance feature selection. The\nSICDN shows potential in medical image prediction, with the code available on\nhttps://github.com/AIPMLab/SICDN.",
      "generated_abstract": "r presents a novel approach to 3D ultrasound image registration using\na graph neural network (GNN). Traditional methods for 3D ultrasound image\nregistration rely on the use of graph convolutional networks (GCNs) to model\nspatial relationships within the data, but the spatial relationships that\nunderlie 3D ultrasound images are complex and non-local, making traditional\nGCNs inappropriate. To address this, we propose a novel GNN architecture that\nuses a diffusion operator to model the non-local relationships between\nvoxels, enabling the model to capture spatial relationships in the data.\nAdditionally, we introduce a novel loss function that leverages a\ncontrastive learning framework to improve the model's ability to capture\nnon-local relationships in the data. This enables the model to better\ndistinguish between healthy and abnormal tissue and improve the accuracy of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2328767123287671,
          "p": 0.21518987341772153,
          "f": 0.22368420553410678
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1780821917808219,
          "p": 0.16455696202531644,
          "f": 0.17105262658673837
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.09508v2",
      "true_abstract": "The online randomized primal-dual method has widespread applications in\nonline algorithm design and analysis. A key challenge is identifying an\nappropriate function space, $F$, in which we search for an optimal updating\nfunction $f \\in F$ that yields the best possible lower bound on the\ncompetitiveness of a given algorithm. The choice of $F$ must balance two\ncompeting objectives: on one hand, it should impose sufficient simplifying\nconditions on $f$ to facilitate worst-case analysis and establish a valid lower\nbound; on the other hand, it should remain general enough to offer a broad\nselection of candidate functions. The tradeoff is that any additional\nconstraints on $f$ that can facilitate competitive analysis may also lead to a\nsuboptimal choice, weakening the resulting lower bound.\n  To address this challenge, we propose an auxiliary-LP-based framework capable\nof effectively approximating the best possible competitiveness achievable when\napplying the randomized primal-dual method to different function spaces.\nSpecifically, we examine the framework introduced by Huang and Zhang (STOC\n2020), which analyzes Stochastic Balance for vertex-weighted online matching\nwith stochastic rewards. Our approach yields both lower and upper bounds on the\nbest possible competitiveness attainable using the randomized primal-dual\nmethod for different choices of ${F}$. Notably, we establish that Stochastic\nBalance achieves a competitiveness of at least $0.5796$ for the problem (under\nequal vanishing probabilities), improving upon the previous bound of $0.576$ by\nHuang and Zhang (STOC 2020). Meanwhile, our analysis yields an upper bound of\n$0.5810$ for a function space strictly larger than that considered in Huang and\nZhang (STOC 2020).",
      "generated_abstract": "In this paper, we present a novel method for the computation of the\nminimum number of rounds required to reach a consensus on a given finite\nmessage-carrying chain in the presence of noise and faults. Our approach\nconsiders a multi-agent system where each agent is equipped with a finite\nmessage-carrying chain and a finite set of messages. Each agent maintains its\nlocal state and performs local computation on its message-carrying chain and the\nlocal messages received from its neighbors. To ensure the existence of a\nconsensus, we propose a novel consensus algorithm that is designed to be\nrobust to the presence of faults and noise. We also provide a theoretical\njustification for our proposed consensus algorithm based on a result by\nBaik and Vu [Baik and Vu, 2019",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1513157894736842,
          "p": 0.3026315789473684,
          "f": 0.20175438152046793
        },
        "rouge-2": {
          "r": 0.026200873362445413,
          "p": 0.05128205128205128,
          "f": 0.03468207644876265
        },
        "rouge-l": {
          "r": 0.1513157894736842,
          "p": 0.3026315789473684,
          "f": 0.20175438152046793
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.00233v1",
      "true_abstract": "This study employs a Bayesian Probit model to empirically analyze peer\neffects and herd behavior among consumers during the \"Double 11\" shopping\nfestival, using data collected through a questionnaire survey. The results\ndemonstrate that peer effects significantly influence consumer decision-making,\nwith the probability of participation in the shopping event increasing notably\nwhen roommates are involved. Additionally, factors such as gender, online\nshopping experience, and fashion consciousness significantly impact consumers'\nherd behavior. This research not only enhances the understanding of online\nshopping behavior among college students but also provides empirical evidence\nfor e-commerce platforms to formulate targeted marketing strategies. Finally,\nthe study discusses the fragility of online consumption activities, the need\nfor adjustments in corporate marketing strategies, and the importance of\npromoting a healthy online culture.",
      "generated_abstract": "the problem of maximizing the aggregate revenue of a two-sided\nmarket with imperfectly competitive sellers, where buyers are endogenous.\nEndogenous buyers are characterized by their preference data, which is\nexogenously fixed at the beginning of the auction. We consider two scenarios:\n(i) buyers have no preference data, (ii) they have a limited number of\npreference parameters, and (iii) they have a large number of preference\nparameters. In scenario (i), we show that the optimal auction design depends on\nthe number of preference parameters. In scenario (ii), we characterize the\noptimal auction design and its equivalence with the one-sided auction design.\nIn scenario (iii), we show that the optimal auction design depends on the\nnumber of preference parameters and the number of sellers. We provide an\nalgorithm to compute the optimal auction design",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08695652173913043,
          "p": 0.11764705882352941,
          "f": 0.09999999511250024
        },
        "rouge-2": {
          "r": 0.01680672268907563,
          "p": 0.020618556701030927,
          "f": 0.018518513570388837
        },
        "rouge-l": {
          "r": 0.07608695652173914,
          "p": 0.10294117647058823,
          "f": 0.08749999511250027
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.cond-mat/quant-gas/2503.09275v1",
      "true_abstract": "The present paper is devoted to comprehensive theoretical studies of\nexction-polariton quantum fluids specificities in the optics of their\nutilization for quantum turbulence research. We show that a non-trivial\nimplementation of time-varying potential for excitation of quantum fluid\n(injection of quantized vortices) via the stirring procedure can be efficiently\nsubstituted with resonant excitation-based phase-imprinting techniques. The\nmost efficient phase pattern corresponds to imprinting of tiles with randomly\noriented plane waves in each. The resulting turbulent flows, spatial vortex\ndistributions, and clustering statistics resemble those for the case of a\nconventional spoon-stirring scheme. We quantify the limitations on the lifetime\nand density depletion for the development and sustainability of quantum\nturbulence. The yield is the necessity to prevent the density depletion for\nmore than one order of magnitude. Finally, we demonstrate that turbulence is\nrobust with respect to alternating gain and loss at a certain range of\nmodulation parameters, which corresponds to laser operating above and below\ncondensation threshold.",
      "generated_abstract": "igate the ground state of the 2D Hubbard model on a square lattice\nwith a short-ranged onsite interaction $U$. We show that for $U/t < 0.25$ the\nground state is a superposition of two distinct Fermi seas, one of which has\nnegative energy, while the other has positive energy. We use density matrix\nrenormalization group to compute the energy of each Fermi sea, and find that\nthe ground state is stable for $U/t < 0.25$ with a gap of about $0.2 t$. We\nthen use numerical renormalization group and quantum Monte Carlo simulations\nto calculate the energy of the ground state for $U/t > 0.25$. We find that the\nground state is stable for $U/t > 0.25$ with a gap of about $0.4 t$. Our results",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15454545454545454,
          "p": 0.25,
          "f": 0.19101123123343025
        },
        "rouge-2": {
          "r": 0.013245033112582781,
          "p": 0.020833333333333332,
          "f": 0.016194327231721303
        },
        "rouge-l": {
          "r": 0.14545454545454545,
          "p": 0.23529411764705882,
          "f": 0.17977527617725048
        }
      }
    },
    {
      "paper_id": "hep-th.nlin/SI/2503.05890v1",
      "true_abstract": "This review paper explores the Riccati-type pseudo-potential formulation\napplied to the quasi-integrable sine-Gordon, KdV, and NLS models. The proposed\nframework provides a unified methodology for analyzing quasi-integrability\nproperties across various integrable systems, including deformations of the\nsine-Gordon, Bullough-Dodd, Toda, KdV, pKdV, NLS and SUSY sine-Gordon models.\nKey findings include the emergence of infinite towers of anomalous conservation\nlaws within the Riccati-type approach and the identification of exact non-local\nconservation laws in the linear formulations of deformed models. As modified\nintegrable models play a crucial role in diverse fields of non-linear\nphysics-such as Bose-Einstein condensation, superconductivity, gravity models,\noptics, and soliton turbulence-these results may have far-reaching\napplications.",
      "generated_abstract": "In this paper, we provide a self-consistent quantum mechanical solution\nfor the Schr\\\"odinger equation in the context of non-equilibrium quantum\nmechanics in strongly coupled conformal field theories. We derive a set of\ndifferential equations which describe the evolution of the quantum state in the\npresence of an external field. The solution is found to be a generalization of\nthe Landau damping theory. We show that the quantum state is characterized by\ntwo non-commuting observables, the ``damping operator'' and the ``stiffness\noperator''. We also analyze the case of a non-rotating external field, where\nthe solutions are given by the Wigner function of the classical field.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1,
          "p": 0.11764705882352941,
          "f": 0.10810810314097904
        },
        "rouge-2": {
          "r": 0.02912621359223301,
          "p": 0.030612244897959183,
          "f": 0.029850741271751535
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.11764705882352941,
          "f": 0.10810810314097904
        }
      }
    },
    {
      "paper_id": "hep-ph.nucl-th/2503.09686v1",
      "true_abstract": "We explore the confining pressure inside the nucleon and the related\ngravitational form factor referred to as the D-term, using the skyrmion\napproach based on the scale-invariant chiral perturbation theory, where the\nskyrmion is described as the nucleon and a scalar meson couples to the scale\nanomaly through the low energy theorem. Within this model framework, the\ncurrent quark mass and gluonic quantum contributions to the scale anomaly can\nbe described by the pion and scalar meson masses, respectively, through\nmatching with the underlying QCD. By considering the decomposition of the\nenergy momentum tensor of nucleon, we examine the role of the scale anomaly\ncontributions in the pressure inside the nucleon. As a result, the gluonic\nscale anomaly is found to dominate the confining pressure. Compared to the\nresult based on the conventional chiral perturbation theory in the chiral\nlimit, our result for the total pressure is capable of qualitatively improving\nthe alignment with lattice QCD observations. Moreover, the pressure from the\ngluonic scale anomaly is widely distributed in position space, leading to its\nsubstantial contribution to the D-term.",
      "generated_abstract": "We study the behavior of the electroweak baryon octet under the influence of\nthe axial vector current of the weak interaction. We show that the octet is\nprotected by the chiral symmetry of the QCD Lagrangian. In addition, we show\nthat the octet can be stable against the axial vector current, when the\nelectroweak coupling constant $g$ is sufficiently small. Our study provides a\nnew perspective for the stability of the octet baryons.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1326530612244898,
          "p": 0.2826086956521739,
          "f": 0.18055555120756184
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.08196721311475409,
          "f": 0.0473933608184905
        },
        "rouge-l": {
          "r": 0.12244897959183673,
          "p": 0.2608695652173913,
          "f": 0.16666666231867294
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2501.12837v1",
      "true_abstract": "BRBVS is a publicly available \\texttt{R} package on CRAN that implements the\nalgorithm proposed in Petti et al.(2024a). The algorithm was developed as the\nfirst proposal of variable selection for the class of Bivariate Survival Copula\nModels originally proposed in Marra & Radice (2020) and implemented in the\n\\texttt{GJRM} package. The core of the \\texttt{BRBVS} package is to implement\nand make available to practitioners variable selection algorithms for bivariate\nsurvival data affected by censoring, providing easy-to-use functions and\ngraphical outputs. The idea behind the algorithm is almost general and may also\nbe extended to different class of models.",
      "generated_abstract": "We consider the estimation of the probability distribution of a random\nsequential event in a random variable. We show that the distribution of the\nsequence of random variables $Y_n = X_1 + \\cdots + X_n$ satisfies a\ndistributional independence condition if and only if the distribution of the\nsequence of random variables $Y_n - Y_{n-1}$ satisfies a distributional\nindependence condition. We give a simple algorithm for constructing estimators\nfor the conditional probability distribution of the random variable $Y_n - Y_{n-1}$\nfrom the conditional probability distribution of $Y_n$. We also show that the\nestimators obtained from our algorithm are consistent and asymptotically\nnormal.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.20833333333333334,
          "f": 0.1694915205975296
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.02702702702702703,
          "f": 0.023809518880386505
        },
        "rouge-l": {
          "r": 0.11428571428571428,
          "p": 0.16666666666666666,
          "f": 0.13559321551278383
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.05025v1",
      "true_abstract": "We develop ProtComposer to generate protein structures conditioned on spatial\nprotein layouts that are specified via a set of 3D ellipsoids capturing\nsubstructure shapes and semantics. At inference time, we condition on\nellipsoids that are hand-constructed, extracted from existing proteins, or from\na statistical model, with each option unlocking new capabilities.\nHand-specifying ellipsoids enables users to control the location, size,\norientation, secondary structure, and approximate shape of protein\nsubstructures. Conditioning on ellipsoids of existing proteins enables\nredesigning their substructure's connectivity or editing substructure\nproperties. By conditioning on novel and diverse ellipsoid layouts from a\nsimple statistical model, we improve protein generation with expanded Pareto\nfrontiers between designability, novelty, and diversity. Further, this enables\nsampling designable proteins with a helix-fraction that matches PDB proteins,\nunlike existing generative models that commonly oversample conceptually simple\nhelix bundles. Code is available at https://github.com/NVlabs/protcomposer.",
      "generated_abstract": "tion of a population is often driven by competition. This\ndiscipline has been the subject of intense research since the 1950s, when\nevolutionary biologists started to develop statistical models to explain the\norigin of new species. However, the emergence of the Internet has allowed the\ncomprehensive study of the evolution of the Internet, which is an extremely\ncomplex system. In particular, the evolution of the Internet has been studied\nfrom the perspective of competition, as it is a paradigmatic example of a\ncomplex system that exhibits a multitude of evolutionary mechanisms. In this\npaper, we investigate the evolution of the Internet from the perspective of\ncomparative methodology. Specifically, we focus on two aspects of the evolution\nof the Internet: (i) the emergence of new technologies and (ii) the emergence\nof new Internet services. We discuss the evolution of new technologies",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15841584158415842,
          "p": 0.2077922077922078,
          "f": 0.1797752759897742
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.15841584158415842,
          "p": 0.2077922077922078,
          "f": 0.1797752759897742
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/EM/2501.06270v1",
      "true_abstract": "The long-term estimation of the Marxist average rate of profit does not\nadhere to a theoretically grounded standard regarding which economic activities\nshould or should not be included for such purposes, which is relevant because\nmethodological non-uniformity can be a significant source of overestimation or\nunderestimation, generating a less accurate reflection of the capital\naccumulation dynamics. This research aims to provide a standard Marxist\ndecision criterion regarding the inclusion and exclusion of economic activities\nfor the calculation of the Marxist average profit rate for the case of United\nStates economic sectors from 1960 to 2020, based on the Marxist definition of\nproductive labor, its location in the circuit of capital, and its relationship\nwith the production of surplus value. Using wavelet-transformed Daubechies\nfilters with increased symmetry, empirical mode decomposition, Hodrick-Prescott\nfilter embedded in unobserved components model, and a wide variety of unit root\ntests the internal theoretical consistency of the presented criteria is\nevaluated. Also, the objective consistency of the theory is evaluated by a\ndynamic factor auto-regressive model, Principal Component Analysis, Singular\nValue Decomposition and Backward Elimination with Linear and Generalized Linear\nModels. The results are consistent both theoretically and econometrically with\nthe logic of Marx's political economy.",
      "generated_abstract": "r investigates the effects of COVID-19 on workplace segregation,\nusing data from the 2017-2021 American Community Survey (ACS). We estimate the\neffect of COVID-19 on segregation using a difference-in-differences (DiD)\nframework. The results show that COVID-19 had a significant impact on segregation\nin the United States, with a decrease in the average segregation coefficient\nfrom 0.29 to -0.21. The DiD estimates suggest that the pandemic had a larger\neffect on segregation in cities with higher levels of income inequality and\nurban density. The results also indicate that the pandemic had a greater\nimpact on segregation in the Midwest and South than in the Northeast and West.\nOur findings suggest that COVID-19 had a significant impact on segregation in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1076923076923077,
          "p": 0.208955223880597,
          "f": 0.14213197520678209
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.03225806451612903,
          "f": 0.02150537189964249
        },
        "rouge-l": {
          "r": 0.08461538461538462,
          "p": 0.16417910447761194,
          "f": 0.11167512241490395
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2501.03658v2",
      "true_abstract": "We characterise the solutions to a continuous-time optimal liquidity\nprovision problem in a market populated by informed and uninformed traders. In\nour model, the asset price exhibits fads -- these are short-term deviations\nfrom the fundamental value of the asset. Conditional on the value of the fad,\nwe model how informed traders and uninformed traders arrive in the market. The\nmarket maker knows of the two groups of traders but only observes the anonymous\norder arrivals. We study both, the complete information and the partial\ninformation versions of the control problem faced by the market maker. In such\nframeworks, we characterise the value of information, and we find the price of\nliquidity as a function of the proportion of informed traders in the market.\nLastly, for the partial information setup, we explore how to go beyond the\nKalman-Bucy filter to extract information about the fad from the market\narrivals.",
      "generated_abstract": "y investigates the impact of the COVID-19 pandemic on the\nmarket for crypto assets. To analyze the impact of the pandemic on the\nmarket, we used a time series analysis of Bitcoin (BTC), Ethereum (ETH), and\nLitecoin (LTC) using 2206 observations from June 1, 2020 to March 31, 2022.\nOur findings indicate that the pandemic had a significant impact on the\nmarket, with an increase in volatility and decreased market liquidity.\nSpecifically, the correlation between BTC and ETH fluctuated between 0.33 and\n0.66, while the correlation between LTC and BTC fluctuated between 0.11 and\n0.30. These findings suggest that the pandemic had a significant impact on the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1518987341772152,
          "p": 0.1791044776119403,
          "f": 0.1643835566776132
        },
        "rouge-2": {
          "r": 0.0234375,
          "p": 0.033707865168539325,
          "f": 0.02764976474675699
        },
        "rouge-l": {
          "r": 0.10126582278481013,
          "p": 0.11940298507462686,
          "f": 0.10958903612966807
        }
      }
    },
    {
      "paper_id": "math.MG.math/MG/2503.05435v1",
      "true_abstract": "We show that the centers of the excircles of a bicentric polygon $B$ are\nconcyclic on a circle $E$. The center of the circumscribed circle $K$ of $B$ is\nthe midpoint of the center of $E$ and the center of the inscribed circle $C$ of\n$B$. The radius of $E$ is given by a simple formula in terms of the radii of\n$C$ and $K$ and the distance between their centers.",
      "generated_abstract": "uce a novel method for constructing a family of $G$-equivariant\nstructures on a smooth complex manifold $(M, \\omega)$. In particular, this\nmethod is applicable to the case of smooth projective bundles over a smooth\nbase. We construct an equivariant bundle over the base, which is a smooth\nprojective bundle with an action of a reductive group. Our construction of the\nequivariant bundle is based on a recent result of the first author on\nequivariant vector bundles on the symmetric space of a reductive group. We\nthen apply this method to construct a family of $G$-equivariant bundles over\n$M$, where $G$ is a reductive group acting on itself by conjugation. We show\nthat these bundles are equivariant bundles over the base, and that they are\nassociated to $G$-equivariant bundles on the symmetric space of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3055555555555556,
          "p": 0.16923076923076924,
          "f": 0.21782177759043242
        },
        "rouge-2": {
          "r": 0.08333333333333333,
          "p": 0.04716981132075472,
          "f": 0.06024095923936747
        },
        "rouge-l": {
          "r": 0.2777777777777778,
          "p": 0.15384615384615385,
          "f": 0.1980197973924126
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2503.01226v1",
      "true_abstract": "Dementia, a progressive neurodegenerative disorder, affects memory,\nreasoning, and daily functioning, creating challenges for individuals and\nhealthcare systems. Early detection is crucial for timely interventions that\nmay slow disease progression. Large pre-trained models (LPMs) for text and\naudio, such as Generative Pre-trained Transformer (GPT), Bidirectional Encoder\nRepresentations from Transformers (BERT), and Contrastive Language-Audio\nPretraining (CLAP), have shown promise in identifying cognitive impairments.\nHowever, existing studies generally rely heavily on expert-annotated datasets\nand unimodal approaches, limiting robustness and scalability. This study\nproposes a context-based multimodal method, integrating both text and audio\ndata using the best-performing LPMs in each modality. By incorporating\ncontextual embeddings, our method improves dementia detection performance.\nAdditionally, motivated by the effectiveness of contextual embeddings, we\nfurther experimented with a context-based In-Context Learning (ICL) as a\ncomplementary technique. Results show that GPT-based embeddings, particularly\nwhen fused with CLAP audio features, achieve an F1-score of $83.33\\%$,\nsurpassing state-of-the-art dementia detection models. Furthermore, raw text\ndata outperforms expert-annotated datasets, demonstrating that LPMs can extract\nmeaningful linguistic and acoustic patterns without extensive manual labeling.\nThese findings highlight the potential for scalable, non-invasive diagnostic\ntools that reduce reliance on costly annotations while maintaining high\naccuracy. By integrating multimodal learning with contextual embeddings, this\nwork lays the foundation for future advancements in personalized dementia\ndetection and cognitive health research.",
      "generated_abstract": "r introduces a novel approach to quantifying the evolutionary\nspeed of species diversification in networks. We first develop a\nstatistical framework to measure the divergence rate of species-species\ninteractions. By incorporating the concept of species-species interaction\ndiversity, we define the diversity of species-species interactions. We then\npropose a novel measure of diversity of species-species interactions, based on\nthe ratio of the species-species interaction diversity to the species-species\ndiversity of species. Finally, we propose a novel measure of diversity of\nspecies diversification, based on the ratio of the species-species diversity to\nthe species-species diversity of species. The proposed measures are tested on\n14 real-world network datasets of species diversification. The proposed\nmeasures of diversity of species diversification are found to be correlated\nwith the measured diversity of species diversification. We also discuss",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.060240963855421686,
          "p": 0.17543859649122806,
          "f": 0.08968609484928328
        },
        "rouge-2": {
          "r": 0.004784688995215311,
          "p": 0.011235955056179775,
          "f": 0.006711405206750057
        },
        "rouge-l": {
          "r": 0.060240963855421686,
          "p": 0.17543859649122806,
          "f": 0.08968609484928328
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/MS/2502.03402v2",
      "true_abstract": "This paper introduces a new mathematical framework for analysis and\noptimization of tensor expressions within an enclosing loop. Tensors are\nmulti-dimensional arrays of values. They are common in high performance\ncomputing (HPC) and machine learning domains. Our framework extends Scalar\nEvolution - an important optimization pass implemented in both LLVM and GCC -\nto tensors. Scalar Evolution (SCEV) relies on the theory of `Chain of\nRecurrences' for its mathematical underpinnings. We use the same theory for\nTensor Evolution (TeV). While some concepts from SCEV map easily to TeV -- e.g.\nelement-wise operations; tensors introduce new operations such as\nconcatenation, slicing, broadcast, reduction, and reshape which have no\nequivalent in scalars and SCEV. Not all computations are amenable to TeV\nanalysis but it can play a part in the optimization and analysis parts of ML\nand HPC compilers. Also, for many mathematical/compiler ideas, applications may\ngo beyond what was initially envisioned, once others build on it and take it\nfurther. We hope for a similar trajectory for the tensor-evolution concept.",
      "generated_abstract": "In this paper, we propose a novel framework for solving the multi-agent\nsystems (MAS) problem of the multi-objective optimization (MOO). The proposed\nframework is based on the multi-agent strategy (MAS) problem, which is a\nproblem of maximizing a single objective function. The proposed framework\nprovides a unified approach for solving MAS problems of arbitrary number of\nagents and objectives, and also provides a unified approach for solving the\nmulti-objective optimization (MOO) problems of arbitrary number of agents and\nobjectives. We compare the proposed framework with the existing approaches. We\nconclude that the proposed framework is more effective than the existing\napproaches in solving the MAS problem of arbitrary number of agents and\nobjectives.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09016393442622951,
          "p": 0.2037037037037037,
          "f": 0.12499999574638444
        },
        "rouge-2": {
          "r": 0.011976047904191617,
          "p": 0.02564102564102564,
          "f": 0.016326526272054463
        },
        "rouge-l": {
          "r": 0.09016393442622951,
          "p": 0.2037037037037037,
          "f": 0.12499999574638444
        }
      }
    },
    {
      "paper_id": "stat.ME.q-bio/MN/2503.05448v1",
      "true_abstract": "Graphical modeling is a widely used tool for analyzing conditional\ndependencies between variables and traditional methods may struggle to capture\nshared and distinct structures in multi-group or multi-condition settings.\nJoint graphical modeling (JGM) extends this framework by simultaneously\nestimating network structures across multiple related datasets, allowing for a\ndeeper understanding of commonalities and differences. This capability is\nparticularly valuable in fields such as genomics and neuroscience, where\nidentifying variations in network topology can provide critical biological\ninsights. Existing JGM methodologies largely fall into two categories:\nregularization-based approaches, which introduce additional penalties to\nenforce structured sparsity, and Bayesian frameworks, which incorporate prior\nknowledge to improve network inference. In this study, we explore an\nalternative method based on two-target linear covariance matrix shrinkage.\nFormula for optimal shrinkage intensities is proposed which leads to the\ndevelopment of JointStein framework. Performance of JointStein framework is\nproposed through simulation benchmarking which demonstrates its effectiveness\nfor large-scale single-cell RNA sequencing (scRNA-seq) data analysis. Finally,\nwe apply our approach to glioblastoma scRNA-seq data, uncovering dynamic shifts\nin T cell network structures across disease progression stages. The result\nhighlights potential of JointStein framework in extracting biologically\nmeaningful insights from high-dimensional data.",
      "generated_abstract": "e a framework for inferring the number of allelic variants from\n(i) the number of transcripts and (ii) the number of SNPs per transcript. In\nthe first step, we derive an exact formula for the number of transcripts and\nSNPs per transcript that depends only on the transcript length. In the second\nstep, we estimate the number of transcripts and SNPs per transcript using a\nlinear regression model that allows for random effects. We apply the method\nto a dataset of single nucleotide polymorphism (SNP) data from the Human\nImmunodeficiency Virus (HIV) and a dataset of transcriptome data from\nhuman-induced pluripotent stem cells (hiPSCs). We find that the method can\nreliably infer the number of transcripts and SNPs per transcript, even in\ncases where the number",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12666666666666668,
          "p": 0.2753623188405797,
          "f": 0.17351597741915317
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11333333333333333,
          "p": 0.2463768115942029,
          "f": 0.15525113723650477
        }
      }
    },
    {
      "paper_id": "math.OA.math/OA/2503.07398v1",
      "true_abstract": "We demonstrate that any full and faithful $*$-functor between approximable\ncategories of locally finite coarse spaces induces a coarse embedding between\nthe underlying spaces. Furthermore, we establish a general characterisation of\nsuch $*$-functors between approximable categories and prove that the functor\nassociating each locally finite coarse space with its approximable category is\nfull and faithful.",
      "generated_abstract": "We prove a generalization of a result of Farkas and Opper, which states that\nfor any non-trivial quadratic form $Q$ on $\\mathbb{R}^n$, there exists a\nnon-trivial quadratic form $Q_1$ on $\\mathbb{R}^n$ such that $Q_1$ is\n$\\epsilon$-close to $Q$ for some $\\epsilon>0$. We show that if $Q$ is\nnonsingular, then the desired quadratic form $Q_1$ is unique. We also prove\nthat if $Q$ is singular, then the desired quadratic form $Q_1$ is singular.\nFinally, we prove that if $Q$ is positive definite, then $Q_1$ is positive\ndefinite.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2972972972972973,
          "p": 0.24444444444444444,
          "f": 0.2682926779744201
        },
        "rouge-2": {
          "r": 0.020833333333333332,
          "p": 0.015384615384615385,
          "f": 0.017699110157413833
        },
        "rouge-l": {
          "r": 0.2702702702702703,
          "p": 0.2222222222222222,
          "f": 0.24390243407198106
        }
      }
    },
    {
      "paper_id": "cs.CY.cs/CY/2503.09276v1",
      "true_abstract": "Effective lesson planning is crucial in education process, serving as the\ncornerstone for high-quality teaching and the cultivation of a conducive\nlearning atmosphere. This study investigates how large language models (LLMs)\ncan enhance teacher preparation by incorporating them with Gagne's Nine Events\nof Instruction, especially in the field of mathematics education in compulsory\neducation. It investigates two distinct methodologies: the development of Chain\nof Thought (CoT) prompts to direct LLMs in generating content that aligns with\ninstructional events, and the application of fine-tuning approaches like\nLow-Rank Adaptation (LoRA) to enhance model performance. This research starts\nwith creating a comprehensive dataset based on math curriculum standards and\nGagne's instructional events. The first method involves crafting CoT-optimized\nprompts to generate detailed, logically coherent responses from LLMs, improving\ntheir ability to create educationally relevant content. The second method uses\nspecialized datasets to fine-tune open-source models, enhancing their\neducational content generation and analysis capabilities. This study\ncontributes to the evolving dialogue on the integration of AI in education,\nillustrating innovative strategies for leveraging LLMs to bolster teaching and\nlearning processes.",
      "generated_abstract": "This paper presents a novel approach to data-driven decision-making in\nrisk management. Our framework is based on the concept of decision-making\ncoalition, a novel approach for stakeholder engagement in risk management\ndecision-making. The proposed framework consists of three key components:\ndecision-making coalition formation, risk management strategy formation, and\nrisk management process design. These components are interlinked through a\ndynamic decision-making process. We demonstrate the effectiveness of the\nframework through an application to a real-world example of a risk management\nsystem in a retail bank. Our results show that the proposed framework can\neffectively facilitate stakeholder engagement and enhance the effectiveness of\nrisk management processes.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13385826771653545,
          "p": 0.2833333333333333,
          "f": 0.18181817746003617
        },
        "rouge-2": {
          "r": 0.017543859649122806,
          "p": 0.03296703296703297,
          "f": 0.02290075882495282
        },
        "rouge-l": {
          "r": 0.13385826771653545,
          "p": 0.2833333333333333,
          "f": 0.18181817746003617
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2503.05128v1",
      "true_abstract": "We developed a theory showing that under appropriate normalizations and\nrescalings, temperature response curves show a remarkably regular behavior and\nfollow a general, universal law. The impressive universality of temperature\nresponse curves remained hidden due to various curve-fitting models not\nwell-grounded in first principles. In addition, this framework has the\npotential to explain the origin of different scaling relationships in thermal\nperformance in biology, from molecules to ecosystems. Here, we summarize the\nbackground, principles and assumptions, predictions, implications, and possible\nextensions of this theory.",
      "generated_abstract": "-10-10 model is a simple, widely used, and effective model for\nmodeling the ecological interactions between individual species in a\nnon-linear, non-equilibrium system. The model assumes that the number of\nindividuals of each species in the system follows a power-law distribution\nwith a mean number of individuals equal to 10, a constant, and a constant\ndecay rate of 10-10. The model has been used to study the dynamics of\nspecies-specific interactions, population growth, and spatial dynamics. In this\npaper, we propose a simple method to model the 10-10-10-10 model using\nspatially-varying population sizes. Our method involves using a\nnon-linear-differential equation to approximate the population growth rate\ndepending on the population sizes. We show that the model can be",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2153846153846154,
          "p": 0.1891891891891892,
          "f": 0.2014388439418251
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.2153846153846154,
          "p": 0.1891891891891892,
          "f": 0.2014388439418251
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2502.14012v1",
      "true_abstract": "Considering that the physical design of printed circuit board (PCB) follows\nthe principle of modularized design, this paper proposes an automatic placement\nalgorithm for functional modules. We first model the placement problem as a\nmixed-variable optimization problem, and then, developed tailored algorithms of\nglobal placement and legalization for the top-layer centralized placement\nsubproblem and the bottom-layer pin-oriented placement subproblem. Numerical\ncomparison demonstrates that the proposed mixed-variable optimization scheme\ncan get optimized total wirelength of placement. Meanwhile, experimental\nresults on several industrial PCB cases show that the developed centralized\nstrategies can well accommodate the requirement of top-layer placement, and the\npin-oriented global placement based on bin clustering contributes to optimized\nplacement results meeting the requirement of pin-oriented design.",
      "generated_abstract": "uce the Precise-RNN, a new generative model that combines RNNs with\nprior-based, low-rank modeling to produce high-quality, low-latency\nrepresentations. Our approach, which is trained end-to-end, employs a\nlow-rank-regularized RNN that learns to map inputs to latent vectors that are\ncomposed of a small number of low-rank components. We demonstrate that our\napproach outperforms prior generative models in various metrics, including\naccuracy, naturalness, and style preservation. In particular, our model\noutperforms existing approaches by 20%-60% in both naturalness and style\npreservation, and by 20%-30% in accuracy. Our model is also trained and\ntrained-evaluated in a single GPU, making it scalable to large datasets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10666666666666667,
          "p": 0.10666666666666667,
          "f": 0.10666666166666691
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10666666666666667,
          "p": 0.10666666666666667,
          "f": 0.10666666166666691
        }
      }
    },
    {
      "paper_id": "cs.CY.econ/GN/2503.05754v1",
      "true_abstract": "The air transportation local share, defined as the proportion of local\npassengers relative to total passengers, serves as a critical metric reflecting\nhow economic growth, carrier strategies, and market forces jointly influence\ndemand composition. This metric is particularly useful for examining industry\nstructure changes and large-scale disruptive events such as the COVID-19\npandemic. This research offers an in-depth analysis of local share patterns on\nmore than 3900 Origin and Destination (O&D) pairs across the U.S. air\ntransportation system, revealing how economic expansion, the emergence of\nlow-cost carriers (LCCs), and strategic shifts by legacy carriers have\ncollectively elevated local share. To efficiently identify the local share\ncharacteristics of thousands of O&Ds and to categorize the O&Ds that have the\nsame behavior, a range of time series clustering methods were used. Evaluation\nusing visualization, performance metrics, and case-based examination\nhighlighted distinct patterns and trends, from magnitude-based stratification\nto trend-based groupings. The analysis also identified pattern commonalities\nwithin O&D pairs, suggesting that macro-level forces (e.g., economic cycles,\nchanging demographics, or disruptions such as COVID-19) can synchronize changes\nbetween disparate markets. These insights set the stage for predictive modeling\nof local share, guiding airline network planning and infrastructure\ninvestments. This study combines quantitative analysis with flexible clustering\nto help stakeholders anticipate market shifts, optimize resource allocation\nstrategies, and strengthen the air transportation system's resilience and\ncompetitiveness.",
      "generated_abstract": "This paper investigates the impact of algorithmic trading on market\ncapitalization dynamics using the daily trading volume data of 4000 companies\nfrom the Nasdaq, NYSE, and S&P 500 from 1995 to 2022. The results show that\nalgorithmic trading significantly enhanced the market capitalization growth\nrate. The results indicate that algorithmic trading can lead to a significant\nincrease in the market capitalization growth rate of companies. The findings\nhighlight the need for further research on algorithmic trading and its\nimpact on market capitalization dynamics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09259259259259259,
          "p": 0.30612244897959184,
          "f": 0.1421800912207723
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09259259259259259,
          "p": 0.30612244897959184,
          "f": 0.1421800912207723
        }
      }
    },
    {
      "paper_id": "cs.PF.cs/PF/2503.09650v1",
      "true_abstract": "With the advancement of Large Language Models (LLMs), the importance of\naccelerators that efficiently process LLM computations has been increasing.\nThis paper discusses the necessity of LLM accelerators and provides a\ncomprehensive analysis of the hardware and software characteristics of the main\ncommercial LLM accelerators. Based on this analysis, we propose considerations\nfor the development of next-generation LLM accelerators and suggest future\nresearch directions.",
      "generated_abstract": "The C++ language supports an object-oriented style of programming that\nhas been successfully used in the programming of many systems, including\noperating systems and middleware. However, in the context of distributed\nsystems, the object-oriented paradigm is often inappropriate. This paper\nproposes an alternative approach, based on the use of a functional style of\nprogramming, which we call functional programming. The functional style of\nprogramming emphasizes abstraction, making it suitable for the design of\ndistributed systems. We illustrate the use of functional programming in a\ndistributed system by presenting a distributed message passing system that\nuses functional programming to model the communication and computation\nprocesses of the system. We show that the use of functional programming in this\nsystem leads to significant improvements in the scalability and performance of\nthe system compared to a system based on the object-oriented paradigm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2826086956521739,
          "p": 0.18055555555555555,
          "f": 0.22033897829359392
        },
        "rouge-2": {
          "r": 0.06779661016949153,
          "p": 0.034782608695652174,
          "f": 0.045977007012155284
        },
        "rouge-l": {
          "r": 0.2391304347826087,
          "p": 0.1527777777777778,
          "f": 0.18644067320884816
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.07231v1",
      "true_abstract": "In today's globalised trade, supply chains form complex networks spanning\nmultiple organisations and even countries, making them highly vulnerable to\ndisruptions. These vulnerabilities, highlighted by recent global crises,\nunderscore the urgent need for improved visibility and resilience of the supply\nchain. However, data-sharing limitations often hinder the achievement of\ncomprehensive visibility between organisations or countries due to privacy,\nsecurity, and regulatory concerns. Moreover, most existing research studies\nfocused on individual firm- or product-level networks, overlooking the\nmultifaceted interactions among diverse entities that characterise real-world\nsupply chains, thus limiting a holistic understanding of supply chain dynamics.\nTo address these challenges, we propose a novel approach that integrates\nFederated Learning (FL) and Graph Convolutional Neural Networks (GCNs) to\nenhance supply chain visibility through relationship prediction in supply chain\nknowledge graphs. FL enables collaborative model training across countries by\nfacilitating information sharing without requiring raw data exchange, ensuring\ncompliance with privacy regulations and maintaining data security. GCNs empower\nthe framework to capture intricate relational patterns within knowledge graphs,\nenabling accurate link prediction to uncover hidden connections and provide\ncomprehensive insights into supply chain networks. Experimental results\nvalidate the effectiveness of the proposed approach, demonstrating its ability\nto accurately predict relationships within country-level supply chain knowledge\ngraphs. This enhanced visibility supports actionable insights, facilitates\nproactive risk management, and contributes to the development of resilient and\nadaptive supply chain strategies, ensuring that supply chains are better\nequipped to navigate the complexities of the global economy.",
      "generated_abstract": "r presents a novel, end-to-end, autonomous, and scalable approach\nfor the development of robust and efficient embedded software systems. The\napproach uses a novel data-driven framework to design and develop a complete\nsoftware system from the ground up, leveraging a deep generative model to\ngenerate code for both hardware and software components. This approach\ndemonstrates that it is possible to develop embedded software systems that\nenhance safety, reliability, and robustness while reducing the total cost of\nownership (TCO). The approach combines a data-driven approach for the\ngeneration of software components with a novel deep generative model, which\ngenerates realistic and robust code for both hardware and software components.\nThe results demonstrate that it is possible to generate code that meets\nsafety, reliability, and robustness requirements while reducing the total\ncost of ownership (TCO). The approach can be",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09090909090909091,
          "p": 0.2318840579710145,
          "f": 0.13061224085164527
        },
        "rouge-2": {
          "r": 0.017543859649122806,
          "p": 0.038461538461538464,
          "f": 0.02409638123965819
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.2318840579710145,
          "f": 0.13061224085164527
        }
      }
    },
    {
      "paper_id": "cs.NI.cs/NI/2503.08123v1",
      "true_abstract": "With the advent of 6G systems, emerging hyper-connected ecosystems\nnecessitate agile and adaptive medium access control (MAC) protocols to contend\nwith network dynamics and diverse service requirements. We propose LLM4MAC, a\nnovel framework that harnesses large language models (LLMs) within a\nreinforcement learning paradigm to drive MAC protocol emergence. By\nreformulating uplink data transmission scheduling as a semantics-generalized\npartially observable Markov game (POMG), LLM4MAC encodes network operations in\nnatural language, while proximal policy optimization (PPO) ensures continuous\nalignment with the evolving network dynamics. A structured identity embedding\n(SIE) mechanism further enables robust coordination among heterogeneous agents.\nExtensive simulations demonstrate that on top of a compact LLM, which is\npurposefully selected to balance performance with resource efficiency, the\nprotocol emerging from LLM4MAC outperforms comparative baselines in throughput\nand generalization.",
      "generated_abstract": "mber of IoT devices continues to grow, the complexity of managing\nthe data generated by these devices increases. Traditional approaches for\nmanaging IoT data, such as data warehouses, are inefficient and difficult to\nscale. In this paper, we propose a novel approach to managing IoT data that\nrelies on data mining to identify critical IoT data, and then automating the\nanalysis of these critical IoT data to derive insights. We demonstrate the\neffectiveness of our approach on the IoT data from the DIGITS dataset, which\nincludes 2600 IoT devices, 1000 hours of data, and 300 million records. Our\nanalysis revealed critical IoT data, which includes the root cause analysis\nof the IoT devices, and the performance of the IoT devices. Our analysis\ndemonstrated that the critical IoT data can be extracted using",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.2222222222222222,
          "f": 0.1777777729777779
        },
        "rouge-2": {
          "r": 0.007936507936507936,
          "p": 0.008849557522123894,
          "f": 0.008368195851616212
        },
        "rouge-l": {
          "r": 0.1388888888888889,
          "p": 0.20833333333333334,
          "f": 0.16666666186666684
        }
      }
    },
    {
      "paper_id": "math.AG.math/AG/2503.09195v1",
      "true_abstract": "Refined algebraic domains are regions in the plane surrounded by finitely\nmany non-singular real algebraic curves which may intersect with normal\ncrossing. We are interested in shapes of such regions with surrounding real\nalgebraic curves. Poincar'e-Reeb Graphs of them are graphs the regions\nnaturally collapse to respecting the projection to a straight line. Such graphs\nwere first formulated by Sorea, for example, around 2020, and regions\nsurrounded by mutually disjoint non-singular real algebraic curves were mainly\nconsidered. The author has generalized the studies to several general\nsituations.\n  We find classes of such objects defined inductively by adding curves. We\nrespect characteristic finite sets in the curves. We consider regions\nsurrounded by the curves and of a new type. We investigate geometric properties\nand combinatorial ones of them and discuss important examples. We also\npreviously studied explicit classes defined inductively in this way and review\nthem.",
      "generated_abstract": "r provides a complete solution to the first-order differential\nequations arising in the study of geometric structures on arbitrary spaces\nwith continuous actions of a group $G$. The solution involves the study of\n$G$-equivariant linear differential operators acting on the space of\n$G$-invariant functions on a given space, with the space of $G$-invariant\nfunctions being endowed with a suitable metric. The key feature of the solution\nis the introduction of a new metric on the space of $G$-invariant functions,\nwhich we call the $G$-equivariant metric. We show that the $G$-equivariant\nmetric is a generalization of the metric on the space of $G$-invariant\nfunctions on the base space, with the group-action as the metric. We also\nstudy the properties of the $G$-equivariant metric, including the properties of\nits norm and the associated $L^",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15730337078651685,
          "p": 0.21875,
          "f": 0.1830065310812082
        },
        "rouge-2": {
          "r": 0.03076923076923077,
          "p": 0.039603960396039604,
          "f": 0.0346320297108382
        },
        "rouge-l": {
          "r": 0.15730337078651685,
          "p": 0.21875,
          "f": 0.1830065310812082
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/CB/2501.08714v1",
      "true_abstract": "Neuroblastoma, is a highly heterogeneous pediatric tumour and is responsible\nfor 15% of pediatric cancer-related deaths. The clinical outcomes can vary from\nspontaneous regression to high metastatic disease. This extracranial tumour\narises from a neural crest-derived cell and can harbor different phenotypes.\nIts heterogeneity may result from variations in differentiation states\ninfluenced by genetic and epigenetic factors and individual patient\ncharacteristics. This leads downstream to disruption of homeostasis and a\nmetabolic shift in response to the tumour needs. Nutrition can play a key role\nin influencing various aspects of a tumour behaviour. This review provides an\nin-depth exploration of the aetiology of neuroblastoma and the different\navenues of disease progression, which can be targeted with individualized\nnutrition intervention strategies to improve the well-being of children and\noptimize clinical outcomes.",
      "generated_abstract": "of recent studies have suggested that the molecular structures of\nunfolded proteins are a source of structural heterogeneity, and the\ncomposition of unfolded protein complexes (UPCs) is a source of structural\npolymorphism. We report here the first observation of a correlation between\nheterogeneity of the unfolded protein content of an individual UPC and its\ntopological properties. In particular, we find that the average number of\nlinked domains of the UPCs, and the average number of linkers per domain,\nincrease with the amount of the unfolded protein. We also report a correlation\nbetween the total number of linked domains and the number of linked domains in\nwhich the unlinked domains are linked by a linker. These results suggest that\nthe topological properties of UPCs may be controlled by the molecular\nstructures of the unfolded protein. Our findings provide new insights",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15730337078651685,
          "p": 0.19444444444444445,
          "f": 0.1739130385340073
        },
        "rouge-2": {
          "r": 0.031496062992125984,
          "p": 0.037383177570093455,
          "f": 0.03418802922456059
        },
        "rouge-l": {
          "r": 0.14606741573033707,
          "p": 0.18055555555555555,
          "f": 0.16149067828556013
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.10062v1",
      "true_abstract": "This paper addresses the one-bit consensus of controllable linear multi-agent\nsystems (MASs) with communication noises. A consensus algorithm consisting of a\ncommunication protocol and a consensus controller is designed. The\ncommunication protocol introduces a linear compression encoding function to\nachieve a one-bit data rate, thereby saving communication costs. The consensus\ncontroller with a stabilization term and a consensus term is proposed to ensure\nthe consensus of a potentially unstable but controllable MAS. Specifically, in\nthe consensus term, we adopt an estimation method to overcome the information\nloss caused by one-bit communications and a decay step to attenuate the effect\nof communication noise. Two combined Lyapunov functions are constructed to\novercome the difficulty arising from the coupling of the control and\nestimation. By establishing similar iterative structures of these two\nfunctions, this paper shows that the MAS can achieve consensus in the mean\nsquare sense at the rate of the reciprocal of the iteration number under the\ncase with a connected fixed topology. Moreover, the theoretical results are\ngeneralized to the case with jointly connected Markovian switching topologies\nby establishing a certain equivalence relationship between the Markovian\nswitching topologies and a fixed topology. Two simulation examples are given to\nvalidate the algorithm.",
      "generated_abstract": "r investigates the performance of distributed optimization (DO)\nmethods for multi-agent systems (MASs) with heterogeneous agents and nonlinear\nobjectives. The system is modeled as a stochastic control problem, where the\nagents are subject to uncertainty and nonlinear constraints. A DO framework is\nproposed that optimizes the system's state and controls simultaneously using\nlocal models. The proposed framework leverages the local models to estimate the\nuncertainty and nonlinear constraints, enabling the agents to achieve a\nlocal optimal solution that is more robust to uncertainty and constraints. The\nproposed framework is tested through an example in which the agents are\nstationary obstacles in a complex environment. The proposed framework\nsignificantly improves the convergence rate of the global optimization while\nremaining computationally efficient. Furthermore, the proposed framework\neffectively captures the complex dynamics of the agents and the nonlinear\nobjectives, enabling more",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.25,
          "f": 0.20512820028928347
        },
        "rouge-2": {
          "r": 0.044444444444444446,
          "p": 0.06722689075630252,
          "f": 0.05351170089372645
        },
        "rouge-l": {
          "r": 0.16521739130434782,
          "p": 0.2375,
          "f": 0.19487179003287322
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.14447v1",
      "true_abstract": "This paper introduces the two-way common causal covariates (CCC) assumption,\nwhich is necessary to get an unbiased estimate of the ATT when using\ntime-varying covariates in existing Difference-in-Differences methods. The\ntwo-way CCC assumption implies that the effect of the covariates remain the\nsame between groups and across time periods. This assumption has been implied\nin previous literature, but has not been explicitly addressed. Through\ntheoretical proofs and a Monte Carlo simulation study, we show that the\nstandard TWFE and the CS-DID estimators are biased when the two-way CCC\nassumption is violated. We propose a new estimator called the Intersection\nDifference-in-differences (DID-INT) which can provide an unbiased estimate of\nthe ATT under two-way CCC violations. DID-INT can also identify the ATT under\nheterogeneous treatment effects and with staggered treatment rollout. The\nestimator relies on parallel trends of the residuals of the outcome variable,\nafter appropriately adjusting for covariates. This covariate residualization\ncan recover parallel trends that are hidden with conventional estimators.",
      "generated_abstract": "This paper studies the problem of estimating the mean and covariance matrix of\na multivariate normal distribution from a finite number of samples. We propose\nan algorithm that uses a random permutation of the sample set and a novel\nalgorithm for computing the covariance matrix. We also propose a method for\nestimating the mean of a multivariate normal distribution from a finite number\nof samples that is more efficient than the method based on the Lasso. The\nmethods are applied to the problem of estimating the mean and covariance matrix\nof a multivariate normal distribution from a finite number of samples. We\nprovide simulation studies and empirical applications in econometrics and\nstatistics to illustrate the efficacy of the proposed methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20192307692307693,
          "p": 0.375,
          "f": 0.26249999545000013
        },
        "rouge-2": {
          "r": 0.03496503496503497,
          "p": 0.058823529411764705,
          "f": 0.04385964444636862
        },
        "rouge-l": {
          "r": 0.18269230769230768,
          "p": 0.3392857142857143,
          "f": 0.23749999545000006
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2502.16246v1",
      "true_abstract": "Understanding the impact of trades on prices is a crucial question for both\nacademic research and industry practice. It is well established that impact\nfollows a square-root impact as a function of traded volume. However, the\nmicroscopic origin of such a law remains elusive: empirical studies are\nparticularly challenging due to the anonymity of orders in public data. Indeed,\nthere is ongoing debate about whether price impact has a mechanical origin or\nwhether it is primarily driven by information, as suggested by many economic\ntheories. In this paper, we revisit this question using a very detailed dataset\nprovided by the Japanese stock exchange, containing the trader IDs for all\norders sent to the exchange between 2012 and 2018. Our central result is that\nsuch a law has in fact microscopic roots and applies already at the level of\nsingle child orders, provided one waits long enough for the market to \"digest\"\nthem. The mesoscopic impact of metaorders arises from a \"double\" square-root\neffect: square-root in volume of individual impact, followed by an inverse\nsquare-root decay as a function of time. Since market orders are anonymous, we\nexpect and indeed find that these results apply to any market orders, and the\nimpact of synthetic metaorders, reconstructed by scrambling the identity of the\nissuers, is described by the very same square-root impact law. We conclude that\nprice impact is essentially mechanical, at odds with theories that emphasize\nthe information content of such trades to explain the square-root impact law.",
      "generated_abstract": "ntext of the increasingly complex financial landscape, the need for\nefficient and accurate risk management solutions is paramount. Traditional\nmodels, while effective for certain purposes, often struggle to adapt to\nchanging market conditions, often requiring manual adjustments or\nimplementing additional models to ensure continuous monitoring. This paper\nintroduces a novel methodology that leverages deep learning to address these\nchallenges. Specifically, we propose a two-stage approach for financial risk\nmanagement. The first stage uses a transformer-based neural network to analyze\nhistorical market data and predict future market returns. The second stage\nutilizes a neural network to predict risk levels and associated probabilities,\nenabling a more efficient and effective risk management process. By integrating\nneural networks with deep learning, we are able to leverage the powerful\ncapabilities of these models to improve financial risk management. By\nintroducing this methodology, we aim to provide",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11486486486486487,
          "p": 0.17894736842105263,
          "f": 0.13991769071110452
        },
        "rouge-2": {
          "r": 0.004329004329004329,
          "p": 0.007575757575757576,
          "f": 0.005509637245182952
        },
        "rouge-l": {
          "r": 0.10810810810810811,
          "p": 0.16842105263157894,
          "f": 0.1316872380362074
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/CB/2408.05119v1",
      "true_abstract": "Stress generation by the actin cytoskeleton shapes cells and tissues. Despite\nimpressive progress in live imaging and quantitative physical descriptions of\ncytoskeletal network dynamics, the connection between processes at molecular\nscales and cell-scale spatio-temporal patterns is still unclear. Here we review\nstudies reporting acto-myosin clusters of micrometer size and with lifetimes of\nseveral minutes in a large number of organisms ranging from fission yeast to\nhumans. Such structures have also been found in reconstituted systems in vitro\nand in theoretical analysis of cytoskeletal dynamics. We propose that tracking\nthese clusters can serve as a simple readout for characterising living matter.\nSpatio-temporal patterns of clusters could serve as determinants of\nmorphogenetic processes that play similar roles in diverse organisms.",
      "generated_abstract": "of this study is to investigate the effect of nutrient supply on\nthe development of a plant disease, black spot, in the context of\ngene-diet-environment interactions. The model was developed using the\nEnsemble Kalman Filter (EnKF) and the R package \"Climate Modeling\" (CM) for\nthe evaluation of the fit of the model and to determine the optimal parameters.\nThe model was applied to the data obtained in a field experiment conducted in\n2018-2019 on the black spot disease of tomato in the region of Krasnodar\n(Russia). The model was calibrated and validated by the Akaike information\ncriterion (AIC) and the Bayesian information criterion (BIC). The EnKF\nalgorithm was used to predict the evolution of the disease status of tomato\ncrops over the course of the year.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09782608695652174,
          "p": 0.12162162162162163,
          "f": 0.10843372999854864
        },
        "rouge-2": {
          "r": 0.017391304347826087,
          "p": 0.017699115044247787,
          "f": 0.017543854649508965
        },
        "rouge-l": {
          "r": 0.09782608695652174,
          "p": 0.12162162162162163,
          "f": 0.10843372999854864
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.21181v1",
      "true_abstract": "It's not unreasonable to think that in-game sporting performance can be\naffected partly by what takes place off the court. We can't observe what\nhappens between games directly. Instead, we proxy for the possibility of\nathletes partying by looking at play following games in party cities. We are\ninterested to see if teams exhibit a decline in performance the day following a\ngame in a city with active nightlife; we call this a \"hangover effect\". Part of\nthe question is determining a reasonable way to measure levels of nightlife,\nand correspondingly which cities are notorious for it; we colloquially refer to\nsuch cities as \"party cities\". To carry out this study, we exploit data on\nbookmaker spreads: the expected score differential between two teams after\nconditioning on observable performance in past games and expectations about the\nupcoming game. We expect a team to meet the spread half the time, since this is\none of the easiest ways for bookmakers to guarantee a profit. We construct a\nmodel which attempts to estimate the causal effect of visiting a \"party city\"\non subsequent day performance as measured by the odds of beating the spread. In\nparticular, we only consider the hangover effect on games played back-to-back\nwithin 24 hours of each other. To the extent that odds of beating the spread\nagainst next day opponent is uncorrelated with playing in a party city the day\nbefore, which should be the case under an efficient betting market, we have\nidentification in our variable of interest. We find that visiting a city with\nactive nightlife the day prior to a game does have a statistically significant\nnegative effect on a team's likelihood of meeting bookmakers' expectations for\nboth NBA and MLB.",
      "generated_abstract": "r introduces a novel method to estimate the mean-reverting volatility\nmodel for stock returns using a stochastic volatility (SV) model with a\nlong-memory (LM) structure. This model is motivated by the observation that\nhistorical stock returns exhibit a long-term memory effect, where past returns\nare more likely to repeat themselves in the future than they are to differ\nsubstantially from their mean. We propose a stochastic volatility model with\na LM structure that captures this memory effect. The model accounts for\nseasonality, which is a key feature of real data, and accounts for the\nnon-linear relationship between past returns and future returns. We show that\nthis model can be estimated using the maximum likelihood method. We then\nestimate the mean-reverting volatility model for stock returns using a\nsemiparametric approach that uses a weighted mean of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1411764705882353,
          "p": 0.3076923076923077,
          "f": 0.19354838278485964
        },
        "rouge-2": {
          "r": 0.02214022140221402,
          "p": 0.05128205128205128,
          "f": 0.030927830839223663
        },
        "rouge-l": {
          "r": 0.1411764705882353,
          "p": 0.3076923076923077,
          "f": 0.19354838278485964
        }
      }
    },
    {
      "paper_id": "cs.CY.cs/CY/2503.10458v1",
      "true_abstract": "Social media platforms have been accused of causing a range of harms,\nresulting in dozens of lawsuits across jurisdictions. These lawsuits are\nsituated within the context of a long history of American product safety\nlitigation, suggesting opportunities for remediation outside of financial\ncompensation. Anticipating that at least some of these cases may be successful\nand/or lead to settlements, this article outlines an implementable mechanism\nfor an abatement and/or settlement plan capable of mitigating abuse. The paper\ndescribes the requirements of such a mechanism, implications for privacy and\noversight, and tradeoffs that such a procedure would entail. The mechanism is\nframed to operate at the intersection of legal procedure, standards for\ntransparent public health assessment, and the practical requirements of modern\ntechnology products.",
      "generated_abstract": "the design of a low-complexity protocol for private aggregation\nof binary vectors using the so-called ``independent adders'' method, where\neach participant adds its corresponding entry to a common adder. The key\nobstacle is that the adders are not symmetric, and thus it is not clear if a\npublicly known permutation of the adders will be a private permutation.\n  We introduce a new class of permutations called ``partial permutations'' and\nshow that a given permutation is a private permutation if and only if it is a\npartial permutation. We then show that the problem of finding a private\npermutation is NP-hard and gives a polynomial-time algorithm for finding\npartial permutations. We then show that finding a private permutation is\nNP-hard and gives a polynomial-time algorithm for finding partial permutations.\n  We then study the problem of finding a private permut",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12087912087912088,
          "p": 0.16666666666666666,
          "f": 0.14012738366181202
        },
        "rouge-2": {
          "r": 0.008403361344537815,
          "p": 0.00980392156862745,
          "f": 0.009049768785244636
        },
        "rouge-l": {
          "r": 0.0989010989010989,
          "p": 0.13636363636363635,
          "f": 0.11464967665544262
        }
      }
    },
    {
      "paper_id": "math.QA.math/QA/2503.05553v1",
      "true_abstract": "For a simple, self-dual, strong CFT-type vertex operator algebra (VOA) of\ncentral charge $c$, we describe the Virasoro $n$-point correlation function on\na genus $g$ marked Riemann surface in the Schottky uniformisation. We show that\nthis $n$-point function determines the correlation functions for all Virasoro\nvacuum descendants. Using our recent work on genus $g$ Zhu recursion, we show\nthat the Virasoro $n$-point function is determined by a differential operator\n$\\mathcal{D}_{n}$ acting on the genus $g$ VOA partition function normalised by\nthe Heisenberg partition function to the power of $c$. We express\n$\\mathcal{D}_{n}$ as the sum of weights over certain Virasoro graphs where the\nweights explicitly depend on $c$, the classical bidifferential of the second\nkind, the projective connection, holomorphic 1-forms and derivatives with\nrespect to any $3g-3$ locally independent period matrix elements. We also\ndescribe the modular properties of $\\mathcal{D}_{n}$ under a homology base\nchange.",
      "generated_abstract": "We prove a version of the Leray-Hopf theorem for groups with finite\nconjugacy classes that does not rely on the existence of an ideal. This\ngeneralization of the theorem is useful for proving the existence of a finite\n$\\mathbb{Z}/k\\mathbb{Z}$-graded Lie algebra $L$ with finite graded dimension and\nwith a maximal ideal $\\mathcal{M}$ such that $L/\\mathcal{M}$ is finite\n$\\mathbb{Z}/k\\mathbb{Z}$-graded. We also prove a generalization of the Leray-Hopf\ntheorem for finite dimensional Lie algebras.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12631578947368421,
          "p": 0.2926829268292683,
          "f": 0.17647058402357274
        },
        "rouge-2": {
          "r": 0.022058823529411766,
          "p": 0.05,
          "f": 0.030612240649729873
        },
        "rouge-l": {
          "r": 0.10526315789473684,
          "p": 0.24390243902439024,
          "f": 0.14705881931769044
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.cond-mat/mes-hall/2503.09970v1",
      "true_abstract": "We introduce new classes of gapped topological phases characterized by\nquantized crystalline-electromagnetic responses, termed \"multipolar Chern\ninsulators\". These systems are characterized by nonsymmorphic momentum-space\nsymmetries and mirror symmetries, leading to quantization of momentum-weighted\nBerry curvature multipole moments. We construct lattice models for such phases\nand confirm their quantized responses through numerical calculations. These\nsystems exhibit bound charge and momentum densities at lattice and magnetic\ndefects, and currents induced by electric or time-varying strain fields. Our\nwork extends the classification of topological matter by uncovering novel\nsymmetry-protected topological phases with quantized responses.",
      "generated_abstract": "opment of spintronic devices requires an understanding of the\nspin-orbit interaction (SOI) that is present in bulk materials and in\nexfoliated materials such as graphene. The SOI is a significant effect that\nintroduces a non-zero magnetic moment to the electrons and a non-zero energy\nshift to the spin states of the electrons. The SOI is also a consequence of the\nspin-orbit coupling (SOC) that arises from the coupling between the spin and\nthe orbital angular momentum. In exfoliated materials, the SOC is a consequence\nof the crystal lattice structure, and SOC arises from the crystal field\npotential. In this review, we discuss the SOC and SOI in exfoliated materials\nand the effect of SOC and SOI on the spin-orbit interaction (SOI). We will\nexplore the effects of SOC",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12857142857142856,
          "p": 0.140625,
          "f": 0.1343283532189799
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11428571428571428,
          "p": 0.125,
          "f": 0.11940298008465158
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.15761v1",
      "true_abstract": "We propose a factor model and an estimator of the factors and loadings that\nare robust to weak factors. The factors can have an arbitrarily weak influence\non the mean or quantile of the outcome variable at most quantile levels; each\nfactor only needs to have a strong impact on the outcome's quantile near one\nunknown quantile level. The estimator for every factor, loading, and common\ncomponent is asymptotically normal at the $\\sqrt{N}$ or $\\sqrt{T}$ rate. It\ndoes not require the knowledge of whether the factors are weak and how weak\nthey are. We also develop a weak-factor-robust estimator of the number of\nfactors and a consistent selectors of factors of any desired strength of\ninfluence on the quantile or mean of the outcome variable. Monte Carlo\nsimulations demonstrate the effectiveness of our methods.",
      "generated_abstract": "the identification of a dynamic stochastic general equilibrium model\nwith a two-dimensional state space. The model is described by a vector of\ninteraction terms with a common state, a state-dependent treatment effect, and\nan exogenous state. We use a Bayesian approach to infer the parameters. The\nmodel is nonlinear and nonstationary. In this paper, we assume the state space\nis separable and the state-dependent treatment effect is additive. We derive\nthe maximum likelihood estimator and test statistics. We show that the\nidentification of the state-dependent treatment effect is weak, and that the\nidentification of the exogenous state is strong. We show that the state space\nis separable when the exogenous state is constant over time, and that the\nseparability is preserved under general conditions. We provide a new method to\nidentify the parameters of a dynamic stochastic general equilibrium model with\na two",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.20588235294117646,
          "f": 0.19310344329512497
        },
        "rouge-2": {
          "r": 0.01652892561983471,
          "p": 0.01904761904761905,
          "f": 0.01769911006930988
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.16176470588235295,
          "f": 0.15172413295029744
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/LG/2503.10635v1",
      "true_abstract": "Despite promising performance on open-source large vision-language models\n(LVLMs), transfer-based targeted attacks often fail against black-box\ncommercial LVLMs. Analyzing failed adversarial perturbations reveals that the\nlearned perturbations typically originate from a uniform distribution and lack\nclear semantic details, resulting in unintended responses. This critical\nabsence of semantic information leads commercial LVLMs to either ignore the\nperturbation entirely or misinterpret its embedded semantics, thereby causing\nthe attack to fail. To overcome these issues, we notice that identifying core\nsemantic objects is a key objective for models trained with various datasets\nand methodologies. This insight motivates our approach that refines semantic\nclarity by encoding explicit semantic details within local regions, thus\nensuring interoperability and capturing finer-grained features, and by\nconcentrating modifications on semantically rich areas rather than applying\nthem uniformly. To achieve this, we propose a simple yet highly effective\nsolution: at each optimization step, the adversarial image is cropped randomly\nby a controlled aspect ratio and scale, resized, and then aligned with the\ntarget image in the embedding space. Experimental results confirm our\nhypothesis. Our adversarial examples crafted with local-aggregated\nperturbations focused on crucial regions exhibit surprisingly good\ntransferability to commercial LVLMs, including GPT-4.5, GPT-4o,\nGemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning\nmodels like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach\nachieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly\noutperforming all prior state-of-the-art attack methods. Our optimized\nadversarial examples under different configurations and training code are\navailable at https://github.com/VILA-Lab/M-Attack.",
      "generated_abstract": "In this paper, we present a novel method for generating 3D facial\nscenes using a single RGB-D image. Our method leverages a large-scale\npre-trained model to generate facial meshes from a single RGB-D image,\nproviding an alternative to current approaches that require multiple RGB\nimages. The generated mesh is used to generate a 3D model of the face. We\nevaluate our method on a dataset of 150 subjects, including 137 adults and\n13 children. Our results demonstrate that the generated 3D face model is\naccurate and realistic, with a mean error of 0.015 mm across all subjects.\nOur method is also compatible with both single-camera and multi-camera setups,\nenabling generation of realistic 3D facial scenes in a single image.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10270270270270271,
          "p": 0.2345679012345679,
          "f": 0.1428571386214598
        },
        "rouge-2": {
          "r": 0.00823045267489712,
          "p": 0.017857142857142856,
          "f": 0.011267601314661445
        },
        "rouge-l": {
          "r": 0.10270270270270271,
          "p": 0.2345679012345679,
          "f": 0.1428571386214598
        }
      }
    },
    {
      "paper_id": "cs.NI.eess/SP/2503.05429v1",
      "true_abstract": "Cross-Technology Interference (CTI) poses challenges for the performance and\nrobustness of wireless networks. There are opportunities for better cooperation\nif the spectral occupation and technology of the interference can be detected.\nNamely, this information can help the Orthogonal Frequency Division Multiple\nAccess (OFDMA) scheduler in IEEE 802.11ax (Wi-Fi 6) to efficiently allocate\nresources to multiple users inthe frequency domain. This work shows that a\nsingle Channel State Information (CSI) snapshot, which is used for packet\ndemodulation in the receiver, is enough to detect and classify the type of CTI\non low-cost Wi-Fi 6 hardware. We show the classification accuracy of a small\nConvolutional Neural Network (CNN) for different Signal-to-Noise Ratio (SNR)\nand Signal-to-Interference Ratio (SIR) with simulated data, as well as using a\nwired and over-the-air test with a professional wireless connectivity tester,\nwhile running the inference on the low-cost device. Furthermore, we use\nopenwifi, a full-stack Wi-Fi transceiver running on software-defined radio\n(SDR) available in the w-iLab.t testbed, as Access Point (AP) to implement a\nCTI-aware multi-user OFDMA scheduler when the clients send CTI detection\nfeedback to the AP. We show experimentally that it can fully mitigate the 35%\nthroughput loss caused by CTI when the AP applies the appropriate scheduling.",
      "generated_abstract": "llenge in the design of wireless networks is to balance network\nperformance with resource consumption, particularly energy consumption. In this\nwork, we investigate the problem of optimizing network energy efficiency in\nwireless networks by using a multi-objective optimization framework. The\noptimization problem is formulated as a mixed-integer nonlinear programming\n(MINLP) problem, which can be solved using standard commercial solvers. We\npropose a novel heuristic algorithm that uses a local search technique to\nfind local optimal solutions. Our experimental results demonstrate that our\nmethod achieves superior performance compared to existing algorithms. The\nperformance of our algorithm is also compared to that of a state-of-the-art\nheuristic algorithm, which uses a genetic algorithm. We also show that our\nmethod outperforms the genetic algorithm in terms of the average number of\niterations needed to find a solution and the number of iterations needed to\nfind a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14965986394557823,
          "p": 0.2619047619047619,
          "f": 0.19047618584809142
        },
        "rouge-2": {
          "r": 0.035175879396984924,
          "p": 0.0546875,
          "f": 0.04281345089321005
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.25,
          "f": 0.18181817719008275
        }
      }
    },
    {
      "paper_id": "math.OA.math/LO/2503.10505v1",
      "true_abstract": "We compute the $K_1$-group of ultraproducts of unital, simple $C^*$-algebras\nwith unique trace and strict comparison. As an application, we prove that the\nreduced free group $C^*$-algebras $C^*_r(F_m)$ and $C^*_r(F_n)$ are\nelementarily equivalent (i.e., have isomorphic ultrapowers) if and only if $m =\nn$. This settles in the negative the $C^*$-algebraic analogue of Tarski's 1945\nproblem for groups.",
      "generated_abstract": "aper, we study the existence and the asymptotic behavior of the\nvariational solutions of the class of elliptic partial differential equations\nwith Dirichlet or Neumann type boundary conditions in the case of the\nGinzburg-Landau (GL) model, where the GL parameter is a function of the\nboundary coordinates. In addition, we consider the case of the GL model with a\nnonlinearity which is a polynomial of the GL parameter. We prove that the\nvariational solutions are of the form $u(x,y,z)=\\Psi(x,y,z)v(z)$, where\n$\\Psi(x,y,z)$ is a certain function depending on the boundary coordinates and\n$v(z)$ is a solution of a linear Dirichlet problem of the Neumann type with\nDirichlet condition. As an application, we obtain a complete classification of\nthe variational solutions of the class of elliptic partial",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27450980392156865,
          "p": 0.2222222222222222,
          "f": 0.24561403014312105
        },
        "rouge-2": {
          "r": 0.1016949152542373,
          "p": 0.06382978723404255,
          "f": 0.0784313678106714
        },
        "rouge-l": {
          "r": 0.27450980392156865,
          "p": 0.2222222222222222,
          "f": 0.24561403014312105
        }
      }
    },
    {
      "paper_id": "math.MG.math/FA/2503.07287v1",
      "true_abstract": "A functional analog of the Klain-Schneider theorem for vector-valued\nvaluations on convex functions is established, providing a classification of\ncontinuous, translation covariant, simple valuations. Under additional rotation\nequivariance assumptions, an analytic counterpart of the moment vector is\ncharacterized alongside a new epi-translation invariant valuation. The former\narises as the top-degree operator in a family of functional intrinsic moments,\nwhich are linked to functional intrinsic volumes through translations. The\nlatter represents the top-degree operator in a class of Minkowski vectors,\nwhich are introduced in this article and which lack classical counterparts on\nconvex bodies, as they vanish due to the Minkowski relations. Additional\nclassification results are obtained for homogeneous valuations of extremal\ndegrees.",
      "generated_abstract": "Let $K$ be a compact K\\\"ahler manifold with $b_1(K) = 0$. In this paper, we\ndevelop a new method to construct $K$-theoretical cocycles in $K$-theory by\nusing the Bott-Samelson theorem. We prove that for any $K$-theoretical cocycle\n$\\alpha$ on $K$-theory $K(K)$, the Bott-Samelson property holds for the\n$K(K)$-module $\\ker \\alpha$. This property implies that the image of\n$\\alpha$ is a $K$-theoretical submodule of the $K(K)$-module $\\mathrm{H}^1(K)$.\nIn particular, we obtain a new description of the $K(K)$-module $\\mathrm{H}^1(K)$\nfrom the $K(K)$-module $\\ker \\alpha$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14102564102564102,
          "p": 0.2037037037037037,
          "f": 0.16666666183195608
        },
        "rouge-2": {
          "r": 0.019417475728155338,
          "p": 0.027777777777777776,
          "f": 0.02285713801404184
        },
        "rouge-l": {
          "r": 0.1282051282051282,
          "p": 0.18518518518518517,
          "f": 0.15151514668044092
        }
      }
    },
    {
      "paper_id": "math.PR.econ/TH/2411.15401v4",
      "true_abstract": "Given two random variables taking values in a bounded interval, we study\nwhether one dominates the other in higher-order stochastic dominance depends on\nthe reference interval in the model setting. We obtain two results. First, the\nstochastic dominance relations get strictly stronger when the reference\ninterval shrinks if and only if the order of stochastic dominance is larger\nthan three. Second, for mean-preserving stochastic dominance relations, the\nreference interval is irrelevant if and only if the difference between the\ndegree of the stochastic dominance and the number of moments is no larger than\nthree. These results highlight complications arising from using higher-order\nstochastic dominance in economic applications.",
      "generated_abstract": "We consider the problem of matching games with two agents, one of whom is\nattracted by a non-negative utility function and the other is attracted by a\nnon-positive utility function. We show that this problem admits a\nrepresentative equilibrium with respect to a large class of utility functions.\nMoreover, we show that the equilibrium of this problem is a quasi-equilibrium\nwhenever the non-negative utility function is strictly convex. We also show that\nthe equilibrium of this problem is a quasi-equilibrium whenever the non-positive\nutility function is strictly convex.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.171875,
          "p": 0.275,
          "f": 0.21153845680473385
        },
        "rouge-2": {
          "r": 0.02247191011235955,
          "p": 0.031746031746031744,
          "f": 0.026315784619980118
        },
        "rouge-l": {
          "r": 0.15625,
          "p": 0.25,
          "f": 0.1923076875739646
        }
      }
    },
    {
      "paper_id": "q-fin.PM.econ/EM/2410.16333v2",
      "true_abstract": "This study examines portfolio selection using predictive models for portfolio\nreturns. Portfolio selection is a fundamental task in finance, and a variety of\nmethods have been developed to achieve this goal. For instance, the\nmean-variance approach constructs portfolios by balancing the trade-off between\nthe mean and variance of asset returns, while the quantile-based approach\noptimizes portfolios by considering tail risk. These methods often depend on\ndistributional information estimated from historical data using predictive\nmodels, each of which carries its own uncertainty. To address this, we propose\na framework for predictive portfolio selection via conformal prediction ,\ncalled \\emph{Conformal Predictive Portfolio Selection} (CPPS). Our approach\nforecasts future portfolio returns, computes the corresponding prediction\nintervals, and selects the portfolio of interest based on these intervals. The\nframework is flexible and can accommodate a wide range of predictive models,\nincluding autoregressive (AR) models, random forests, and neural networks. We\ndemonstrate the effectiveness of the CPPS framework by applying it to an AR\nmodel and validate its performance through empirical studies, showing that it\ndelivers superior returns compared to simpler strategies.",
      "generated_abstract": "y examines the performance of the state-of-the-art deep learning\nmodels in predicting the volatility of the S&P 500 index (SPX), a widely\nadopted and valuable financial instrument. Using a large dataset of historical\nvolatility data, we compare the performance of the most popular deep learning\nmodels--including XGBoost, LightGBM, and Dask LightGBM--with traditional\nstatistical methods, such as the CUSUM and the CUSUMCT test. We find that the\nmost powerful model is the Dask LightGBM ensemble model, which outperforms the\nother models across all statistical tests. This ensemble model combines the\npredictive power of individual models, achieving superior performance across\nall statistical tests, including the CUSUM and the CUSUMCT test. The findings\nof this study highlight the power of deep learning models in predicting",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18253968253968253,
          "p": 0.30666666666666664,
          "f": 0.22885571671493285
        },
        "rouge-2": {
          "r": 0.005813953488372093,
          "p": 0.009708737864077669,
          "f": 0.00727272258750715
        },
        "rouge-l": {
          "r": 0.15079365079365079,
          "p": 0.25333333333333335,
          "f": 0.1890547216900573
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/GA/2503.10415v1",
      "true_abstract": "This article focuses on NGC7538 IRS1, one of the most luminous and studied HC\nHII regions in the northern hemisphere. Our aim is to identify the young\nstellar objects (YSOs) embedded within the ionized gas and study their\nkinematic structures. This work expands on a recent survey called \"Protostellar\nOutflows at the EarliesT Stages\" (POETS), which has been devoted to studying\nyoung outflow emission on scales of 10-100 au near luminous YSOs, before they\nstart photoionizing the surrounding medium. We carried out multi-epoch Very\nLong Baseline Array observations of the 22 GHz water masers toward NGC7538 IRS1\nto measure the maser 3D velocities, which, following POETS' findings, are\nreliable tracers of the protostellar winds. Recently, we reobserved the water\nmasers in NGC7538 IRS1 with sensitive global very long baseline interferometry\n(VLBI) observations to map weaker maser emission. Our study confirms the\npresence of two embedded YSOs, IRS1a and IRS1b, at the center of the two linear\ndistributions of 6.7 GHz methanol masers observed in the southern and northern\ncores of the HC HII region, which have been previously interpreted in terms of\nedge-on rotating disks. The water masers trace an extended (~200 au) stationary\nshock front adjacent to the inner portion of the disk around IRS1a. This shock\nfront corresponds to the edge of the southern tip of the ionized core and might\nbe produced by the interaction of the disk wind ejected from IRS1a with the\ninfalling envelope. The water masers closer to IRS1b follow the same LSR\nvelocity (Vlsr) pattern of the 6.7~GHz masers rotating in the disk, but the\ndirection and amplitude of the water maser proper motions are inconsistent with\nrotation. We propose that these water masers are tracing a photo-evaporated\ndisk wind, where the maser Vlsr traces mainly the disk rotation and the proper\nmotions the poloidal velocity of the wind.",
      "generated_abstract": "The Large Synoptic Survey Telescope (LSST) will observe 100 billion stars\nover a 10-year period, yielding a catalog of billions of stars and\ngalaxies. The LSST Data Management System (LSDMS) is an open-source\nimplementation of the Data Management System (DMS) that provides a\nplatform for data processing and management for LSST. This paper describes the\ndevelopment of a Jupyter Notebook for the LSDMS, which provides a\nuser-friendly interface for interacting with LSDMS's data processing\npipelines. The Jupyter Notebook is the first step in an effort to make LSDMS\npipelines available to the broader community, and it is available at\nhttps://github.com/LSST-Observations/LSDMS-notebooks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07567567567567568,
          "p": 0.208955223880597,
          "f": 0.11111110720742015
        },
        "rouge-2": {
          "r": 0.007220216606498195,
          "p": 0.021052631578947368,
          "f": 0.010752684368859016
        },
        "rouge-l": {
          "r": 0.07567567567567568,
          "p": 0.208955223880597,
          "f": 0.11111110720742015
        }
      }
    },
    {
      "paper_id": "math.OC.math/OC/2503.10572v1",
      "true_abstract": "The objective of this paper is to investigate the connection between penalty\nfunctions from stochastic optimal control, convex semigroups from analysis and\nconvex expectations from probability theory. Our main result provides a\none-to-one relation between these objects. As an application, we use the\nrepresentation via penality functions and duality arguments to show that convex\nexpectations are determined by their finite dimensional distributions. To\nillustrate this structural result, we show that Hu and Peng's axiomatic\ndescription of $G$-L\\'evy processes in terms of finite dimensional\ndistributions extends uniquely to the control approach introduced by Neufeld\nand Nutz. Finally, we show that convex expectations with a Markovian structure\nare fully determined by their one-dimensional distributions, which give rise to\na classical semigroup on the state space.",
      "generated_abstract": "st decades, the study of linear operators has been a major field\nin the theory of partial differential equations. The study of linear operators\non Hilbert spaces has been the subject of many fundamental papers and the\nstudy of linear operators on Banach spaces has been the subject of many\nimportant papers. This paper is devoted to study the class of linear operators\non Hilbert space that are of the form $A=\\left(  a_{ij}\\right)  $ where $a_{ij}\n$ are symmetric and positive semi-definite. We prove that the operator $A$ is\npositive and that the operator $A^{-1}$ is positive. We also study the case of\nthe operator $A$ that is a diagonal operator with positive entries. We prove\nthat the operator $A$ is positive if and only if the matrix $A$ is positive\nsemi-definite. This paper is dedicated to the study of positive",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1744186046511628,
          "p": 0.25,
          "f": 0.20547944721336098
        },
        "rouge-2": {
          "r": 0.017857142857142856,
          "p": 0.02,
          "f": 0.01886791954432314
        },
        "rouge-l": {
          "r": 0.16279069767441862,
          "p": 0.23333333333333334,
          "f": 0.1917808170763747
        }
      }
    },
    {
      "paper_id": "q-bio.PE.econ/TH/2502.18488v1",
      "true_abstract": "Aquaculture has been the fastest growing food production sector globally due\nto its potential to improve food security, stimulate economic growth, and\nreduce poverty. Its rapid development has been linked to sustainability\nchallenges, many of which are still unresolved and poorly understood.\nSmall-scale producers account for an increasing fraction of aquacultural\noutput. At the same time, many of these producers experience poverty, food\ninsecurity, and rely on unimproved production practices. We develop a stylized\nmathematical model to explore the effects of ecological, social, and economic\nfactors on the dynamics of a small-scale pond aquaculture system. Using\nanalytical and numerical methods, we explore the stability, asymptotic\ndynamics, and bifurcations of the model. Depending on the characteristics of\nthe system, the model exhibits one of three distinct configurations:\nmonostability with a global poverty trap in a nutrient-dominated or\nfish-dominated system; bistability with poverty trap and well-being attractors;\nmultistability with poverty trap and two well-being attractors with different\ncharacteristics. The model results show that intensification can be sustainable\nonly if it takes into account the local social-ecological context. In addition,\nthe heterogeneity of small-scale aquaculture producers matters, as the effects\nof intensification can be unevenly distributed among them. Finally, more is not\nalways better because too high nutrient input or productivity can lead to a\nsuboptimal attractor or system collapse.",
      "generated_abstract": "r provides a novel framework for modeling the dynamics of\nresearch and innovation (R&I) in the context of a heterogeneous agent model.\nWe focus on the interplay between researchers and firms, highlighting the\nimportance of firm-specific technological capabilities, productivity, and\nmarket structures in shaping R&I investment decisions. We develop a\ngeneral-equilibrium model that captures the interactions between firms,\nresearchers, and agents. Our model highlights the crucial role of productivity\nin shaping R&I investment patterns, emphasizing the importance of firm-specific\nproductivity differences in driving innovation. We show that productivity\ndifferences between firms can influence R&I investment decisions through the\ninfluence of R&I on firm productivity, as well as the level of productivity\ndifferences between firms. Additionally, we",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1232876712328767,
          "p": 0.27692307692307694,
          "f": 0.1706161094809192
        },
        "rouge-2": {
          "r": 0.039603960396039604,
          "p": 0.0784313725490196,
          "f": 0.052631574488400654
        },
        "rouge-l": {
          "r": 0.11643835616438356,
          "p": 0.26153846153846155,
          "f": 0.16113743649513726
        }
      }
    },
    {
      "paper_id": "astro-ph.HE.astro-ph/HE/2503.10540v1",
      "true_abstract": "Context. The nearby middle-aged gamma-ray pulsar J1741-2054 and its pulsar\nwind nebula (PWN) have been studied in X-rays, and its bow-shock nebula (BSN)\nhas been investigated in the Balmer lines, but they have never been observed in\nfar ultraviolet (FUV). Aims. To further study the thermal and magnetospheric\nemission from PSR J1741-2054 and the BSN properties, we observed them in the\nFUV range with the Hubble Space Telescope (HST). Methods. We imaged the target\nin two FUV filters of the HST's ACS/SBC detector. We also re-analyzed previous\noptical observations of the pulsar and its BSN. We fit the pulsar's FUV-optical\nspectrum separately and together with its X-ray spectrum. Results. We found\nthat the pulsar's FUV-optical spectrum consists of a thermal and nonthermal\ncomponents. A joint fit of the FUV-optical and X-ray spectra with combinations\nof a nonthermal and thermal components showed a hard optical nonthermal\nspectrum with a photon index $\\Gamma_{opt} \\approx 1.0-1.2$ and a softer X-ray\ncomponent, $\\Gamma_X \\approx 2.6-2.7$. The thermal emission is dominated by the\ncold component with the temperature $kT_{cold}\\approx 40-50$ eV and emitting\nsphere radius $R_{cold}\\approx 8-15$ km, at $d=270$ pc. An additional hot\nthermal component, with $kT_{hot}\\sim 80$ eV and $R_{hot}\\sim 1$ km, is also\npossible. Such a spectrum resembles the spectra of other middle-aged pulsars,\nbut it shows a harder (softer) optical (X-ray) nonthermal spectrum. We detected\nthe FUV BSN, the first one associated with a middle-aged pulsar. Its\nclosed-shell morphology is similar to the H$\\alpha$ BSN morphology, while its\nFUV flux, $\\sim10^{-13}$ erg cm$^{-2}$ s$^{-1}$, is a factor of $\\sim 4$ higher\nthan the H$\\alpha$ flux. This FUV BSN has a higher surface brightness than the\ntwo previously known ones.",
      "generated_abstract": "h for gravitational waves from neutron star-black hole (NSBH)\nsystems is still in its infancy, despite decades of efforts. The\nCharmplanet experiment aims to detect gravitational waves from NSBH\nmergers with a detector located in the Pacific Ocean. To reduce the\nbackground from terrestrial sources, the experiment employs a\nhigh-efficiency gravitational-wave instrument (GEI) designed to detect\nlow-frequency gravitational waves with a minimum sensitivity of 1.0\nnano-Newton (nN). To achieve this, the GEI employs a hybrid-electromagnetic\n(HEM) and electromagnetic (EM) calibration technique, with the HEM calibration\nbeing performed by a reference coil. In this paper, the GEI calibration\nprocess is described, with an emphasis on the HEM",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09146341463414634,
          "p": 0.19736842105263158,
          "f": 0.12499999567222235
        },
        "rouge-2": {
          "r": 0.011406844106463879,
          "p": 0.030612244897959183,
          "f": 0.016620494659495572
        },
        "rouge-l": {
          "r": 0.07926829268292683,
          "p": 0.17105263157894737,
          "f": 0.10833332900555574
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2406.02623v2",
      "true_abstract": "Background: Limited universally-adopted data standards in veterinary medicine\nhinder data interoperability and therefore integration and comparison; this\nultimately impedes the application of existing information-based tools to\nsupport advancement in diagnostics, treatments, and precision medicine.\n  Objectives: A single, coherent, logic-based standard for documenting breed\nnames in health, production, and research-related records will improve data use\ncapabilities in veterinary and comparative medicine. Methods: The Vertebrate\nBreed Ontology (VBO) was created from breed names and related information\ncompiled from the Food and Agriculture Organization of the United Nations,\nbreed registries, communities, and experts, using manual and computational\napproaches. Each breed is represented by a VBO term that includes breed\ninformation and provenance as metadata. VBO terms are classified using\ndescription logic to allow computational applications and Artificial\nIntelligence-readiness.\n  Results: VBO is an open, community-driven ontology representing over 19,500\nlivestock and companion animal breed concepts covering 49 species. Breeds are\nclassified based on community and expert conventions (e.g., cattle breed) and\nsupported by relations to the breed's genus and species indicated by National\nCenter for Biotechnology Information (NCBI) Taxonomy terms. Relationships\nbetween VBO terms (e.g., relating breeds to their foundation stock) provide\nadditional context to support advanced data analytics. VBO term metadata\nincludes synonyms, breed identifiers/codes, and attributed cross-references to\nother databases.\n  Conclusion and clinical importance: The adoption of VBO as a source of\nstandard breed names in databases and veterinary electronic health records can\nenhance veterinary data interoperability and computability.",
      "generated_abstract": "of cellular systems is often reduced to the study of the\ncells, with the assumption that the properties of the cells are the same in all\ncells. This assumption is often incorrect, and the properties of the cells are\nnot universal across all cells. In this paper, we demonstrate that the\nproperties of the cell are not universal, but instead depend on the environment\nin which the cell is living. We demonstrate this using a simple model of a\ncell, which we refer to as the cellular automaton (CA). We show that the\nproperties of the cell can vary depending on the environment in which the cell\nlives, such as temperature, chemicals, and light. Additionally, we show that the\nproperties of the cell can vary depending on the state of the cell, such as\nwhether the cell is alive or dead. These findings suggest that the properties\nof the cell are not universal, but instead depend on the environment in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08860759493670886,
          "p": 0.208955223880597,
          "f": 0.12444444026232114
        },
        "rouge-2": {
          "r": 0.008771929824561403,
          "p": 0.019230769230769232,
          "f": 0.012048188468574623
        },
        "rouge-l": {
          "r": 0.08860759493670886,
          "p": 0.208955223880597,
          "f": 0.12444444026232114
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.03567v1",
      "true_abstract": "We propose a new statistical hypothesis testing framework which decides\nvisually, using confidence intervals, whether the means of two samples are\nequal or if one is larger than the other. With our method, the user can at the\nsame time visualize the confidence region of the means and do a test to decide\nif the means of the two populations are significantly different or not by\nlooking whether the two confidence intervals overlap. To design this test we\nuse confidence intervals constructed using e-variables, which provide a measure\nof evidence in hypothesis testing. We propose both a sequential test and a\nnon-sequential test based on the overlap of confidence intervals and for each\nof these tests we give finite-time error bounds on the probabilities of error.\nWe also illustrate the practicality of our method by applying it to the\ncomparison of sequential learning algorithms.",
      "generated_abstract": "aper, we consider the problem of recovering the unknown parameters of\na parametric model from a noisy observation of the model parameters. This\nproblem arises in a variety of applications, such as in signal detection,\nrobust statistics, and other fields. In this paper, we focus on the case where\nthe model parameters are generated by a random walk, which is a stochastic\nprocess that follows a Markov chain. We propose a statistical test for\nassessing the difference between two random walks, and show that this test\nyields a minimax optimal rate of convergence. Furthermore, we propose a\nsemiparametric estimator for the parameters of the random walk, and show that\nthis estimator achieves the minimax optimal rate of convergence as well. Our\nestimator can be implemented using a two-stage semi-parametric estimation\nprocedure. The first stage is based on the statistical test, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2558139534883721,
          "p": 0.2619047619047619,
          "f": 0.2588235244124569
        },
        "rouge-2": {
          "r": 0.045454545454545456,
          "p": 0.047619047619047616,
          "f": 0.04651162290968145
        },
        "rouge-l": {
          "r": 0.2558139534883721,
          "p": 0.2619047619047619,
          "f": 0.2588235244124569
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.10845v1",
      "true_abstract": "Optimal experimental design (OED) is a framework that leverages a\nmathematical model of the experiment to identify optimal conditions for\nconducting the experiment. Under a Bayesian approach, the design objective\nfunction is typically chosen to be the expected information gain (EIG).\nHowever, EIG is intractable for nonlinear models and must be estimated\nnumerically. Estimating the EIG generally entails some variant of Monte Carlo\nsampling, requiring repeated data model and likelihood evaluations\n$\\unicode{x2013}$ each involving solving the governing equations of the\nexperimental physics $\\unicode{x2013}$ under different sample realizations.\nThis computation becomes impractical for high-fidelity models.\n  We introduce a novel multi-fidelity EIG (MF-EIG) estimator under the\napproximate control variate (ACV) framework. This estimator is unbiased with\nrespect to the high-fidelity mean, and minimizes variance under a given\ncomputational budget. We achieve this by first reparameterizing the EIG so that\nits expectations are independent of the data models, a requirement for\ncompatibility with ACV. We then provide specific examples under different data\nmodel forms, as well as practical enhancements of sample size optimization and\nsample reuse techniques. We demonstrate the MF-EIG estimator in two numerical\nexamples: a nonlinear benchmark and a turbulent flow problem involving the\ncalibration of shear-stress transport turbulence closure model parameters\nwithin the Reynolds-averaged Navier-Stokes model. We validate the estimator's\nunbiasedness and observe one- to two-orders-of-magnitude variance reduction\ncompared to existing single-fidelity EIG estimators.",
      "generated_abstract": "ng complexity of genomic data and the need for comprehensive\nanalysis of large-scale data sets have driven the development of advanced\nmethods for modeling and analyzing gene expression data. The emergence of\nhigh-throughput technologies has also resulted in the generation of large\ndatabases of transcriptome profiles. In this paper, we present a novel\napproach for modeling and analyzing these large-scale gene expression\ndatabases. The method is based on a statistical framework that combines the\ngeneralized method of moments (GMM) with the likelihood ratio test (LRT). This\napproach allows for flexible modeling of the data and a more efficient use of\ncomputational resources, thereby improving the model fit. We apply our method to\ndata from the Gene Expression Omnibus (GEO) database. In particular, we focus on\nthe GSE13565 dataset, which contains gene expression measurements for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13013698630136986,
          "p": 0.21348314606741572,
          "f": 0.16170212295373487
        },
        "rouge-2": {
          "r": 0.02304147465437788,
          "p": 0.03937007874015748,
          "f": 0.02906976278410702
        },
        "rouge-l": {
          "r": 0.1232876712328767,
          "p": 0.20224719101123595,
          "f": 0.15319148465586252
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.cond-mat/stat-mech/2503.09883v1",
      "true_abstract": "Van der Waals \"sliding\" ferroelectric bilayers, whose electric polarization\nis locked to the interlayer alignment, show promise for future non-volatile\nmemory and other nanoelectronic devices. These applications require a fuller\nunderstanding of the polarization stability and switching properties, which\npresent models have described in terms of an Ising-like binary polarization.\nHowever, it is a much larger translation symmetry that is broken in the polar\nstate. Here we introduce a discrete statistical-mechanical model that\nemphasizes the effect of this larger symmetry. Through Monte-Carlo numerics we\nshow this model possesses a richer phase diagram, including an intermediate\ncritical phase of algebraically-correlated polarization. A low energy effective\ntheory allows us to connect the ferroelectric-paraelectric transition to the\nBerezinskii-Kosterlitz-Thouless class, driven by excitations not available in\nIsing-like models. Our results indicate the need for theoretical models of this\nferroelectric system to account for the larger symmetry.",
      "generated_abstract": "We study the dynamical evolution of a random walker in a random environment\nand show that the time to reach a particular final position can be predicted\nfrom the initial conditions. This prediction is given by the solution of the\nHamilton-Jacobi equation with an interaction potential that is symmetric under\nrotation, which generalizes the classical example of the Ising model. The\ninteraction potential is characterized by a Gaussian random field that is\ncorrelated with the environment and can be described by a random field\ngenerating function. The results are general, and they can be obtained by\nanalyzing the large deviation principle for the walker's energy. We demonstrate\nthe application of the results to the dynamics of a random walker in a\nfrustrated magnet.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1717171717171717,
          "p": 0.2361111111111111,
          "f": 0.19883040448137898
        },
        "rouge-2": {
          "r": 0.028985507246376812,
          "p": 0.03773584905660377,
          "f": 0.03278688033190077
        },
        "rouge-l": {
          "r": 0.15151515151515152,
          "p": 0.20833333333333334,
          "f": 0.17543859161588196
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/MN/2409.19294v3",
      "true_abstract": "Agent-based models capture heterogeneity among individuals in a population\nand are widely used in studies of multi-cellular systems, disease, epidemics\nand demography to name a few. However, existing frameworks consider discrete\ntime-step simulation or assume that agents' states only change as a result of\ndiscrete events. In this note, we present AgentBasedModeling$.$jl, a Julia\npackage for simulating stochastic agent-based population models in continuous\ntime. The tool allows to easily specify and simulate agents evolving through\ngeneric continuous-time jump-diffusions and interacting via continuous-rate\nprocesses. AgentBasedModeling$.$jl provides a powerful methodology for studying\nthe effects of stochasticity on structured population dynamics.",
      "generated_abstract": "This paper introduces a new approach to the study of the dynamics of\nphenotypic diversity in populations of bacteria. We propose a multiscale\nframework that incorporates the effect of spatial scale on the dynamics of\ndiversity and the impact of spatial heterogeneity on this effect. We describe\nthe underlying mechanisms through an elementary model of a population of\nbacteria, and we show how to estimate the parameters of this model in the\npresence of experimental and field data. Our analysis reveals the\ninterplay between spatial scale and heterogeneity, providing a unifying\nframework for understanding the effects of both on bacterial diversity.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1951219512195122,
          "p": 0.26229508196721313,
          "f": 0.22377621888405314
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.03125,
          "f": 0.030769225770415014
        },
        "rouge-l": {
          "r": 0.17073170731707318,
          "p": 0.22950819672131148,
          "f": 0.19580419091202517
        }
      }
    },
    {
      "paper_id": "cs.DS.stat/TH/2503.06464v1",
      "true_abstract": "Consider a pair of sparse correlated stochastic block models $\\mathcal\nS(n,\\tfrac{\\lambda}{n},\\epsilon;s)$ subsampled from a common parent stochastic\nblock model with two symmetric communities, average degree $\\lambda=O(1)$ and\ndivergence parameter $\\epsilon \\in (0,1)$. For all $\\epsilon\\in(0,1)$, we\nconstruct a statistic based on the combination of two low-degree polynomials\nand show that there exists a sufficiently small constant\n$\\delta=\\delta(\\epsilon)>0$ and a sufficiently large constant\n$\\Delta=\\Delta(\\epsilon,\\delta)$ such that when $\\lambda>\\Delta$ and\n$s>\\sqrt{\\alpha}-\\delta$ where $\\alpha\\approx 0.338$ is Otter's constant, this\nstatistic can distinguish this model and a pair of independent stochastic block\nmodels $\\mathcal S(n,\\tfrac{\\lambda s}{n},\\epsilon)$ with probability $1-o(1)$.\nWe also provide an efficient algorithm that approximates this statistic in\npolynomial time. The crux of our statistic's construction lies in a carefully\ncurated family of multigraphs called \\emph{decorated trees}, which enables\neffective aggregation of the community signal and graph correlation from the\ncounts of the same decorated tree while suppressing the undesirable\ncorrelations among counts of different decorated trees.",
      "generated_abstract": "er the problem of estimating the mean and covariance of a random\ndistribution. We study a generalization of the Expectation-Maximization (EM)\nalgorithm for estimating the mean of a random distribution that has a\ncorresponding Gaussian process (GP) representation. The GP representation is\nobtained using a kernel function and a kernel-based estimator. We propose a\nnew algorithm that uses a mixture of kernels and kernel-based estimators to\nimprove the convergence rate and efficiency of the EM algorithm. This new\nalgorithm is called GP-EM. We prove that the GP-EM algorithm achieves a\npolynomial convergence rate. We also prove that the GP-EM algorithm is more\nefficient than the EM algorithm. We also prove that GP-EM is more efficient than\nthe Gaussian-Process-Based-Estimator (GPBE) algorithm. Finally, we provide an\nexample of a distribution",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.25396825396825395,
          "f": 0.18285713824914296
        },
        "rouge-2": {
          "r": 0.034722222222222224,
          "p": 0.04807692307692308,
          "f": 0.04032257577523472
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.25396825396825395,
          "f": 0.18285713824914296
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.07795v1",
      "true_abstract": "Black-box optimization is often encountered for decision-making in complex\nsystems management, where the knowledge of system is limited. Under these\ncircumstances, it is essential to balance the utilization of new information\nwith computational efficiency. In practice, decision-makers often face the dual\ntasks of optimization and statistical inference for the optimal performance, in\norder to achieve it with a high reliability. Our goal is to address the dual\ntasks in an online fashion. Wu et al (2022) [arXiv preprint: 2210.06737] point\nout that the sample average of performance estimates generated by the\noptimization algorithm needs not to admit a central limit theorem. We propose\nan algorithm that not only tackles this issue, but also provides an online\nconsistent estimator for the variance of the performance. Furthermore, we\ncharacterize the convergence rate of the coverage probabilities of the\nasymptotic confidence intervals.",
      "generated_abstract": "The recent development of deep learning has revolutionized various fields in\ncomputational science, including the analysis of large-scale data. However,\nthese advances have been hindered by the lack of robust, interpretable\nmodels that capture the complex relationships between the data and the model\nparameters. This paper introduces a novel framework for deep learning-based\nstatistical modeling that addresses this limitation by integrating\ninterpretability into the training process. By incorporating a\nrepresentation-learning objective that encourages the model to focus on the\nrelevant features, we introduce a method that enables the model to learn\nrelationships between the data and the parameters while preserving interpretability.\n  We demonstrate the effectiveness of our approach on various data sets in\nseveral applications, including image classification, recommendation systems,\nand time series forecasting.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16326530612244897,
          "p": 0.1839080459770115,
          "f": 0.17297296799065026
        },
        "rouge-2": {
          "r": 0.007518796992481203,
          "p": 0.008695652173913044,
          "f": 0.008064511155375076
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.16091954022988506,
          "f": 0.15135134636902867
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2412.03618v3",
      "true_abstract": "In today's complex and volatile financial market environment, risk management\nof multi-asset portfolios faces significant challenges. Traditional risk\nassessment methods, due to their limited ability to capture complex\ncorrelations between assets, find it difficult to effectively cope with dynamic\nmarket changes. This paper proposes a multi-asset portfolio risk prediction\nmodel based on Convolutional Neural Networks (CNN). By utilizing image\nprocessing techniques, financial time series data are converted into\ntwo-dimensional images to extract high-order features and enhance the accuracy\nof risk prediction. Through empirical analysis of data from multiple asset\nclasses such as stocks, bonds, commodities, and foreign exchange, the results\nshow that the proposed CNN model significantly outperforms traditional models\nin terms of prediction accuracy and robustness, especially under extreme market\nconditions. This research provides a new method for financial risk management,\nwith important theoretical significance and practical value.",
      "generated_abstract": "y develops a novel approach to market structure analysis of\nmarket-making platforms, leveraging the data from the most recent month of\ntrading volume and bid-ask spreads on a daily basis. The proposed model\nintegrates the fundamental data from the trading volume and bid-ask spreads\nwith the high-frequency data from the market makers' orders. The analysis\ndemonstrates that the market structure is not uniform across different\nplatforms. The results show that the bid-ask spreads and the volume data\ncontain a high correlation, but the market makers' orders exhibit a lower\ncorrelation. The results also show that the number of market makers on the\nplatform significantly affects the spread and volume data, while the volume\ndata exhibits a positive correlation with the number of market makers. The\nanalysis reveals that the volume data exhibits a positive correlation with the\nnumber of market makers on",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1559633027522936,
          "p": 0.26153846153846155,
          "f": 0.19540229417030003
        },
        "rouge-2": {
          "r": 0.0364963503649635,
          "p": 0.04950495049504951,
          "f": 0.04201680183708835
        },
        "rouge-l": {
          "r": 0.13761467889908258,
          "p": 0.23076923076923078,
          "f": 0.1724137884231736
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.10838v1",
      "true_abstract": "Generalizable deepfake detection can be formulated as a detection problem\nwhere labels (bonafide and fake) are fixed but distributional drift affects the\ndeepfake set. We can always train our detector with one-selected attacks and\nbonafide data, but an attacker can generate new attacks by just retraining his\ngenerator with a different seed. One reasonable approach is to simply pool all\ndifferent attack types available in training time. Our proposed approach is to\nutilize meta-learning in combination with LoRA adapters to learn the structure\nin the training data that is common to all attack types.",
      "generated_abstract": "ntext of automated speech recognition (ASR), training large\nmodels on large corpora can be expensive. In this work, we propose a\nmodified version of the ASR model developed by the Google Translate team. We\nadopt a similar approach to that used by the Google Translate team, and\nintroduce a new data augmentation method that allows us to reduce the\nrequirement of large corpora to a more manageable level. We introduce the\nTranslate-Augmented Large Transformer (TALT) model, which is trained on a\nsmaller dataset of 100 hours of speech. The model achieves comparable\nperformance to the Translate model on the same dataset. We also introduce a\nfine-tuning method for the Translate-Augmented Large Transformer (TALT-FS),\nwhich enables us to fine-tune the model on smaller corpor",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2028985507246377,
          "p": 0.18666666666666668,
          "f": 0.19444443945312517
        },
        "rouge-2": {
          "r": 0.011111111111111112,
          "p": 0.008928571428571428,
          "f": 0.009900985158320278
        },
        "rouge-l": {
          "r": 0.18840579710144928,
          "p": 0.17333333333333334,
          "f": 0.18055555056423628
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.01315v1",
      "true_abstract": "Harsanyi (1955) showed that the only way to aggregate individual preferences\ninto a social preference which satisfies certain desirable properties is\n``utilitarianism'', whereby the social utility function is a weighted average\nof individual utilities. This representation forms the basis for welfare\nanalysis in most applied work. We argue, however, that welfare analysis based\non Harsanyi's version of utilitarianism may overlook important distributional\nconsiderations. We therefore introduce a notion of utilitarianism for\ndiscrete-choice settings which applies to \\textit{social choice functions},\nwhich describe the actions of society, rather than social welfare functions\nwhich describe society's preferences (as in Harsanyi). We characterize a\nrepresentation of utilitarian social choice, and show that it provides a\nfoundation for a family of \\textit{distributional welfare measures} based on\nquantiles of the distribution of individual welfare effects, rather than\naverages.",
      "generated_abstract": "We study a market for a set of identical goods with a single buyer and\nmultiple sellers. The buyer seeks to maximize his utility by selecting a\ncombination of goods that maximizes the expected value of his payoff. We\ncharacterize the buyer's optimal choice and show that the optimal choice is\nindependent of the number of sellers. This result extends a result of\nKleinberg (2010) to the setting of a single buyer and multiple sellers.\nFurthermore, we show that the number of sellers is optimal for the buyer's\nutility, regardless of the number of buyers. This result extends a result of\nKleinberg (2010) to the setting of a single buyer and multiple sellers.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16279069767441862,
          "p": 0.2857142857142857,
          "f": 0.2074074027829905
        },
        "rouge-2": {
          "r": 0.048,
          "p": 0.075,
          "f": 0.05853658060678207
        },
        "rouge-l": {
          "r": 0.16279069767441862,
          "p": 0.2857142857142857,
          "f": 0.2074074027829905
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/SE/2503.09510v1",
      "true_abstract": "Code Review consists in assessing the code written by teammates with the goal\nof increasing code quality. Empirical studies documented the benefits brought\nby such a practice that, however, has its cost to pay in terms of developers'\ntime. For this reason, researchers have proposed techniques and tools to\nautomate code review tasks such as the reviewers selection (i.e., identifying\nsuitable reviewers for a given code change) or the actual review of a given\nchange (i.e., recommending improvements to the contributor as a human reviewer\nwould do). Given the substantial amount of papers recently published on the\ntopic, it may be challenging for researchers and practitioners to get a\ncomplete overview of the state-of-the-art.\n  We present a systematic literature review (SLR) featuring 119 papers\nconcerning the automation of code review tasks. We provide: (i) a\ncategorization of the code review tasks automated in the literature; (ii) an\noverview of the under-the-hood techniques used for the automation, including\nthe datasets used for training data-driven techniques; (iii) publicly available\ntechniques and datasets used for their evaluation, with a description of the\nevaluation metrics usually adopted for each task.\n  The SLR is concluded by a discussion of the current limitations of the\nstate-of-the-art, with insights for future research directions.",
      "generated_abstract": "r introduces an approach for evaluating the performance of\nEthereum-based smart contracts, leveraging the concept of \"Gold Standard\"\ncontracts. This approach is based on the concept of \"Gold Standard\" benchmarks,\nwhich are based on the concept of \"Gold Standard\" testing for software\nsoftware. This approach is based on the concept of \"Gold Standard\" benchmarks,\nwhich are based on the concept of \"Gold Standard\" testing for software,\nwhereas \"Gold Standard\" testing is a testing methodology where the goal is to\nmeasure the performance of the test tool and the system under test in the same\nway. This approach is based on the concept of \"Gold Standard\" benchmarks, which\nare based on the concept of \"Gold Standard\" testing for software, whereas \"Gold\nStandard\" testing is a testing methodology where the goal is to measure the\nperformance",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08396946564885496,
          "p": 0.2619047619047619,
          "f": 0.12716762638110207
        },
        "rouge-2": {
          "r": 0.020942408376963352,
          "p": 0.06666666666666667,
          "f": 0.03187250632212229
        },
        "rouge-l": {
          "r": 0.06870229007633588,
          "p": 0.21428571428571427,
          "f": 0.10404623909786506
        }
      }
    },
    {
      "paper_id": "physics.hist-ph.physics/hist-ph/2503.05668v1",
      "true_abstract": "This paper concerns the question of which collections of general relativistic\nspacetimes are deterministic relative to which definitions. We begin by\nconsidering a series of three definitions of increasing strength due to Belot\n(1995). The strongest of these definitions is particularly interesting for\nspacetime theories because it involves an asymmetry condition called\n``rigidity'' that has been studied previously in a different context (Geroch\n1969; Halvorson and Manchak 2022; Dewar 2024). We go on to explore other\n(stronger) asymmetry conditions that give rise to other (stronger) forms of\ndeterminism. We introduce a number of definitions of this type and clarify the\nrelationships between them and the three considered by Belot. We go on to show\nthat there are collections of general relativistic spacetimes that satisfy much\nstronger forms of determinism than previously known. We also highlight a number\nof open questions.",
      "generated_abstract": "In this work, we propose a novel methodology to interpret the history of\nthings. Our methodology is based on the assumption that the past is a\n``blackboard'' for the present, a time-travelling ``pencil board''. Our\napproach relies on the concept of ``history as a language'' and the notion of\n``linguistic relativity''. By applying this approach to the history of the\nuniverse, we demonstrate that the universe's present is not only a ``blackboard\nfor the future'' but also a ``pencil board for the past''. We conclude that\nthe present is the language in which the universe's future is written.\nAdditionally, we demonstrate that the present is the language in which the\nuniverse's past is written.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.23728813559322035,
          "f": 0.18666666189422235
        },
        "rouge-2": {
          "r": 0.007936507936507936,
          "p": 0.01098901098901099,
          "f": 0.009216584991826563
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.23728813559322035,
          "f": 0.18666666189422235
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.02344v1",
      "true_abstract": "The recently emerged movable antenna (MA) shows great promise in leveraging\nspatial degrees of freedom to enhance the performance of wireless systems.\nHowever, resource allocation in MA-aided systems faces challenges due to the\nnonconvex and coupled constraints on antenna positions. This paper\nsystematically reveals the challenges posed by the minimum antenna separation\ndistance constraints. Furthermore, we propose a penalty optimization framework\nfor resource allocation under such new constraints for MA-aided systems.\nSpecifically, the proposed framework separates the non-convex and coupled\nantenna distance constraints from the movable region constraints by introducing\nauxiliary variables. Subsequently, the resulting problem is efficiently solved\nby alternating optimization, where the optimization of the original variables\nresembles that in conventional resource allocation problem while the\noptimization with respect to the auxiliary variables is achieved in closedform\nsolutions. To illustrate the effectiveness of the proposed framework, we\npresent three case studies: capacity maximization, latency minimization, and\nregularized zero-forcing precoding. Simulation results demonstrate that the\nproposed optimization framework consistently outperforms state-of-the-art\nschemes.",
      "generated_abstract": "r presents a novel 3D beamforming system for the 3D multi-user\nmultiband interference mitigation (3D-MUMI) problem. The proposed method\nintroduces a new beamforming vector (BV) to exploit the 3D cross-talk among\nthe users. Additionally, the proposed method dynamically adjusts the BV to\nreduce the cross-talk and the inter-cell interference (ICI) between users.\nFurthermore, we extend the proposed method to the 3D multi-user interference\nmitigation (3D-MUMI) problem with time-varying interference. In this case, the\nproposed method dynamically adjusts the BVs to mitigate the inter-cell ICI\namong the users. Simulation results show that the proposed method significantly\nimproves the 3D MUMI",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14423076923076922,
          "p": 0.2777777777777778,
          "f": 0.1898734132222401
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.0641025641025641,
          "f": 0.043859644621422444
        },
        "rouge-l": {
          "r": 0.14423076923076922,
          "p": 0.2777777777777778,
          "f": 0.1898734132222401
        }
      }
    },
    {
      "paper_id": "math.PR.stat/OT/2411.03955v1",
      "true_abstract": "We provide bounds on the tail probabilities for simple procedures that\ngenerate random samples _without replacement_, when the probabilities of being\nselected need not be equal.",
      "generated_abstract": "We propose a method for generating random variables with arbitrary support\nin finite-dimensional Hilbert spaces. This method, called random variable\nsampling with support, is based on the construction of a Markov chain that\ngenerates random variables with the given support. We prove that this Markov\nchain has a unique stationary distribution and that the distribution of each\nrandom variable is obtained by applying a suitable projection operator to the\nstationary distribution. We prove that the stationary distribution is unique and\nthat it coincides with the support of the generating Markov chain. We prove that\nthe distribution of each random variable is the same as the distribution of the\ncorresponding variable under the stationary distribution. We prove that the\ndistribution of each random variable is the same as the distribution of the\ncorresponding variable under the stationary distribution if the support is the\nunion of finitely many sets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2916666666666667,
          "p": 0.1206896551724138,
          "f": 0.17073170317668065
        },
        "rouge-2": {
          "r": 0.04,
          "p": 0.010638297872340425,
          "f": 0.016806719370101635
        },
        "rouge-l": {
          "r": 0.2916666666666667,
          "p": 0.1206896551724138,
          "f": 0.17073170317668065
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.03554v1",
      "true_abstract": "We model stochastic choices with categorization, resulting from the\npreliminary step of grouping alternatives in homogenous disjoint classes. The\nagent randomly chooses one class among those available, then randomly picks an\nitem within the selected class. We give a formal definition of a choice\ngenerated by this procedure, and provide a characterization. The characterizing\nproperties allow an external observer to elicit that categorization is applied.\nIn a more general interpretation, the model allows to describe the observed\nchoice as the composition of independent subchoices. This composition preserves\nrationalizability by random utility maximization. A generalization of the model\nsubsumes Luce model and Nested Logit.",
      "generated_abstract": "r introduces a novel approach to analyzing the impact of climate\nchange on energy systems and economic activity. We model the energy system\nwith a continuous-time Markov chain, where each state represents the amount of\nenergy in use at a given time. The system is subject to a continuous-time\nMarkovian transition process, which generates a discrete-time sequence of\nstates. We show that this process can be viewed as a renewal process that\nrecurs at a constant rate, with the renewal process being driven by a\nnegative-feedback, time-inhomogeneous forcing term. We then apply this\nperspective to the economic sector, analyzing its impact on economic activity\nand energy use. Our analysis reveals that the economy exhibits two distinct\nregimes: a slow, stochastic regime, in which economic activity is constrained\nby resource availability, and a fast, deterministic regime, in which economic\nactivity is largely",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21518987341772153,
          "p": 0.19540229885057472,
          "f": 0.20481927212004658
        },
        "rouge-2": {
          "r": 0.009900990099009901,
          "p": 0.007692307692307693,
          "f": 0.008658003736814326
        },
        "rouge-l": {
          "r": 0.20253164556962025,
          "p": 0.1839080459770115,
          "f": 0.19277107934896226
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2408.01470v1",
      "true_abstract": "In order to overcome the drawbacks of assuming deterministic volatility\ncoefficients in the standard LIBOR market models to capture volatility smiles\nand skews in real markets, several extensions of LIBOR models to incorporate\nstochastic volatilities have been proposed. The efficient calibration to market\ndata of these more complex models becomes a relevant target in practice. The\nmain objective of the present work is to efficiently calibrate some recent\nSABR/LIBOR market models to real market prices of caplets and swaptions. For\nthe calibration we propose a parallelized version of the simulated annealing\nalgorithm for multi-GPUs. The numerical results clearly illustrate the\nadvantages of using the proposed multi-GPUs tools when applied to real market\ndata and popular SABR/LIBOR models.",
      "generated_abstract": "e a novel approach to the estimation of the forward-looking\ncontingent claims on a defaulting firm under the Generalized Method of Moments\n(GMM) framework. Our approach combines the estimation of the firm's cash\nflows with the construction of a parametric model for the firm's contingent\nclaims. The estimation of the contingent claims on a defaulting firm is\nconventional within the framework of GMM. However, the estimation of these\nclaims involves a number of challenges that are not fully addressed in the\nliterature. First, the firm's cash flows are non-stationary and have a\nnon-linear dependence on the firm's past performance. Second, the firm's\nclaims are non-linear in the contingent event and are not fully\nspecifiable. Third, the contingent claims are dependent on both the firm'",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14666666666666667,
          "p": 0.1774193548387097,
          "f": 0.16058393665086063
        },
        "rouge-2": {
          "r": 0.027522935779816515,
          "p": 0.030927835051546393,
          "f": 0.02912620860920059
        },
        "rouge-l": {
          "r": 0.14666666666666667,
          "p": 0.1774193548387097,
          "f": 0.16058393665086063
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2502.13344v1",
      "true_abstract": "Drug discovery is a complex and time-intensive process that requires\nidentifying and validating new therapeutic candidates. Computational approaches\nusing large-scale biomedical knowledge graphs (KGs) offer a promising solution\nto accelerate this process. However, extracting meaningful insights from\nlarge-scale KGs remains challenging due to the complexity of graph traversal.\nExisting subgraph-based methods are tailored to graph neural networks (GNNs),\nmaking them incompatible with other models, such as large language models\n(LLMs). We introduce K-Paths, a retrieval framework that extracts structured,\ndiverse, and biologically meaningful paths from KGs. Integrating these paths\nenables LLMs and GNNs to effectively predict unobserved drug-drug and\ndrug-disease interactions. Unlike traditional path-ranking approaches, K-Paths\nretrieves and transforms paths into a structured format that LLMs can directly\nprocess, facilitating explainable reasoning. K-Paths employs a diversity-aware\nadaptation of Yen's algorithm to retrieve the K shortest loopless paths between\nentities in an interaction query, prioritizing biologically relevant and\ndiverse relationships. Our experiments on benchmark datasets show that K-Paths\nimproves the zero-shot performance of Llama 8.1B's F1-score by 12.45 points on\ndrug repurposing and 13.42 points on interaction severity prediction. We also\nshow that Llama 70B achieves F1-score gains of 6.18 and 8.46 points,\nrespectively. K-Paths also improves the supervised training efficiency of\nEmerGNN, a state-of-the-art GNN, by reducing KG size by 90% while maintaining\nstrong predictive performance. Beyond its scalability and efficiency, K-Paths\nuniquely bridges the gap between KGs and LLMs, providing explainable rationales\nfor predicted interactions. These capabilities show that K-Paths is a valuable\ntool for efficient data-driven drug discovery.",
      "generated_abstract": "t a novel methodology for analyzing long-range interactions in\nnetworks. We propose a framework that combines network topology and\nlong-range interactions to model complex systems. By integrating\nlong-range interactions into the network, our approach enables us to better\ncapture the dynamic nature of these systems. In particular, we leverage this\nframework to analyze the role of long-range interactions in regulating gene\nexpression and disease susceptibility in the human genome. We use our framework\nto analyze the genetic regulatory network of the human genome, showing how\nlong-range interactions can alter gene expression by regulating gene\ntranscription. Our results highlight the importance of long-range interactions\nin modulating gene expression, particularly in complex biological systems.\nAdditionally, we analyze the role of long-range interactions in disease\nsusceptibility, showing how these interactions can influence gene expression\nand disease susceptibility. Our",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1092896174863388,
          "p": 0.2777777777777778,
          "f": 0.15686274104544415
        },
        "rouge-2": {
          "r": 0.004032258064516129,
          "p": 0.009259259259259259,
          "f": 0.00561797330135402
        },
        "rouge-l": {
          "r": 0.1092896174863388,
          "p": 0.2777777777777778,
          "f": 0.15686274104544415
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2411.09657v1",
      "true_abstract": "We study the tail asymptotics of the sum of two heavy-tailed random\nvariables. The dependence structure is modeled by copulas with the so-called\ntail order property. Examples are presented to illustrate the approach. Further\nfor each example we apply the main results to obtain the asymptotic expansions\nfor Value-at-Risk of aggregate risk.",
      "generated_abstract": "The use of quantile regression models for forecasting option prices in\ntraditional options pricing models has been extensively studied in the past.\nHowever, the use of the quantile regression model in the context of Black-Scholes\npricing has not been explored. This paper examines the use of quantile\nregression in the Black-Scholes model to derive new risk-neutral pricing\nformulae. In particular, we propose risk-neutral pricing formulae for the\nvolatility and the dividend. We also derive the corresponding price formulae\nfor the options. We compare our proposed risk-neutral pricing formulae with\nexisting ones in the literature.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19047619047619047,
          "p": 0.15384615384615385,
          "f": 0.17021276101403365
        },
        "rouge-2": {
          "r": 0.0196078431372549,
          "p": 0.01282051282051282,
          "f": 0.01550387118803104
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.1346153846153846,
          "f": 0.1489361652693528
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.10145v1",
      "true_abstract": "We discuss the qualitatively new properties of random walks on groups that\narise in the situation when the entropy of the step distribution is infinite.",
      "generated_abstract": "In this paper, we introduce a new framework for analyzing and quantifying\nthe influence of geometric factors in the problem of finding optimal trajectories\nin the constrained space of time and position. This framework unifies the\nanalysis of the classical approaches, such as the Euler-Maruyama scheme,\nsemi-implicit schemes, and the Langevin diffusion, and provides a more\ngeneral and unified approach to the problem of finding optimal trajectories in\nthe constrained space of time and position. The new approach allows for the\nanalysis of the influence of geometric factors on the behavior of optimal\ntrajectories in the constrained space of time and position, and provides a\nframework for analyzing the influence of geometric factors on the behavior of\nthe optimal trajectories in the constrained space of time and position in more\ngeneral situations. We also discuss some of the applications of the new\nframework to problems in financial mathematics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3333333333333333,
          "p": 0.11864406779661017,
          "f": 0.17499999612812508
        },
        "rouge-2": {
          "r": 0.08333333333333333,
          "p": 0.021739130434782608,
          "f": 0.03448275533888259
        },
        "rouge-l": {
          "r": 0.3333333333333333,
          "p": 0.11864406779661017,
          "f": 0.17499999612812508
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SP/2503.04278v1",
      "true_abstract": "This study addresses the challenge of access point (AP) and user equipment\n(UE) association in cell-free massive MIMO networks. It introduces a deep\nlearning algorithm leveraging Bidirectional Long Short-Term Memory cells and a\nhybrid probabilistic methodology for weight updating. This approach enhances\nscalability by adapting to variations in the number of UEs without requiring\nretraining. Additionally, the study presents a training methodology that\nimproves scalability not only with respect to the number of UEs but also to the\nnumber of APs. Furthermore, a variant of the proposed AP-UE algorithm ensures\nrobustness against pilot contamination effects, a critical issue arising from\npilot reuse in channel estimation. Extensive numerical results validate the\neffectiveness and adaptability of the proposed methods, demonstrating their\nsuperiority over widely used heuristic alternatives.",
      "generated_abstract": "the design of robust communication networks for multi-agent systems\nwith unknown or random communication graph structure. Our primary focus is on\nthe case of multiple vehicles sharing a single radio frequency channel. We\ndevelop an information theoretic framework to characterize the achievable\nrate-distortion trade-off for such networks. Our key insight is that the\ncommunication scheme has to be designed in such a way that the channel is\nefficiently shared by all vehicles. In particular, we show that the minimum\nrate achieved by any scheme can be achieved by a scheme that shares the\ncommunication channel only among the vehicles that can exploit it. This is a\nfundamental result that gives rise to a new class of communication strategies\nthat are inherently designed for the communication network structure. We then\napply our results to a scenario of a fleet of vehicles sharing a single radio\nfrequency channel. In particular, we analyze the problem of jointly scheduling",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15053763440860216,
          "p": 0.15730337078651685,
          "f": 0.15384614884856918
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.1348314606741573,
          "f": 0.13186812687054722
        }
      }
    },
    {
      "paper_id": "cs.SI.stat/ML/2503.02271v1",
      "true_abstract": "Experiments in online platforms frequently suffer from network interference,\nin which a treatment applied to a given unit affects outcomes for other units\nconnected via the platform. This SUTVA violation biases naive approaches to\nexperiment design and estimation. A common solution is to reduce interference\nby clustering connected units, and randomizing treatments at the cluster level,\ntypically followed by estimation using one of two extremes: either a simple\ndifference-in-means (DM) estimator, which ignores remaining interference; or an\nunbiased Horvitz-Thompson (HT) estimator, which eliminates interference at\ngreat cost in variance. Even combined with clustered designs, this presents a\nlimited set of achievable bias variance tradeoffs. We propose a new estimator,\ndubbed Differences-in-Neighbors (DN), designed explicitly to mitigate network\ninterference. Compared to DM estimators, DN achieves bias second order in the\nmagnitude of the interference effect, while its variance is exponentially\nsmaller than that of HT estimators. When combined with clustered designs, DN\noffers improved bias-variance tradeoffs not achievable by existing approaches.\nEmpirical evaluations on a large-scale social network and a city-level\nride-sharing simulator demonstrate the superior performance of DN in\nexperiments at practical scale.",
      "generated_abstract": "t a novel method for the generation of synthetic data for\nstudying the behavior of large-scale systems. The method uses a\nmulti-stage process that generates data from a set of predefined distributions\nwhile maintaining a high degree of realism. The method is applicable to\nnumerous scenarios, including analyzing the dynamics of complex systems such\nas the internet or social networks, as well as simulating the behavior of\nspecific algorithms in real-time. The method is based on a novel method for\nsampling from a broad family of distributions, which is a generalization of the\nmethod used by Kolmogorov and Zaremba in their recent paper. The method\nprovides a powerful tool for studying the behavior of large-scale systems,\nenabling researchers to obtain data that closely approximates the behavior of\nthe real-world system. The method is easy to implement and can be applied to\nvarious scenarios, making it",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1450381679389313,
          "p": 0.2235294117647059,
          "f": 0.17592592115269215
        },
        "rouge-2": {
          "r": 0.02247191011235955,
          "p": 0.03225806451612903,
          "f": 0.026490061385027856
        },
        "rouge-l": {
          "r": 0.1450381679389313,
          "p": 0.2235294117647059,
          "f": 0.17592592115269215
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.01075v1",
      "true_abstract": "Hallucinations are spurious structures not present in the ground truth,\nposing a critical challenge in medical image reconstruction, especially for\ndata-driven conditional models. We hypothesize that combining an unconditional\ndiffusion model with data consistency, trained on a diverse dataset, can reduce\nthese hallucinations. Based on this, we propose DynamicDPS, a diffusion-based\nframework that integrates conditional and unconditional diffusion models to\nenhance low-quality medical images while systematically reducing\nhallucinations. Our approach first generates an initial reconstruction using a\nconditional model, then refines it with an adaptive diffusion-based inverse\nproblem solver. DynamicDPS skips early stage in the reverse process by\nselecting an optimal starting time point per sample and applies Wolfe's line\nsearch for adaptive step sizes, improving both efficiency and image fidelity.\nUsing diffusion priors and data consistency, our method effectively reduces\nhallucinations from any conditional model output. We validate its effectiveness\nin Image Quality Transfer for low-field MRI enhancement. Extensive evaluations\non synthetic and real MR scans, including a downstream task for tissue volume\nestimation, show that DynamicDPS reduces hallucinations, improving relative\nvolume estimation by over 15% for critical tissues while using only 5% of the\nsampling steps required by baseline diffusion models. As a model-agnostic and\nfine-tuning-free approach, DynamicDPS offers a robust solution for\nhallucination reduction in medical imaging. The code will be made publicly\navailable upon publication.",
      "generated_abstract": "mber of data sources continues to grow, the need for efficient\ndata integration becomes increasingly critical. This paper proposes a\nframework for integrating and analyzing heterogeneous data, leveraging the\ninterpretability of convolutional neural networks (CNNs) to identify key\nfeatures. Our method is based on the idea of \"fusing information from\ndifferent sources\" and is particularly suited to analyze multi-source data,\nwhich is commonly encountered in clinical research. The framework consists of a\ndata preprocessing step that converts raw data into feature vectors, followed\nby an ensemble of CNNs that analyze and extract features from each data source.\nThe feature vectors are then fused using a weighted average, and the\nensemble's predictions are aggregated to produce a final output. The\nperformance of our method was evaluated through extensive experiments,\nincluding a simulation study and two real-world datasets. The results show that\nour approach",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16981132075471697,
          "p": 0.2647058823529412,
          "f": 0.20689654696261076
        },
        "rouge-2": {
          "r": 0.018779342723004695,
          "p": 0.028368794326241134,
          "f": 0.022598865263335308
        },
        "rouge-l": {
          "r": 0.1509433962264151,
          "p": 0.23529411764705882,
          "f": 0.18390804121548432
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2412.03305v1",
      "true_abstract": "An investment portfolio consists of $n$ algorithmic trading strategies, which\ngenerate vectors of positions in trading assets. Sign opposite trades\n(buy/sell) cross each other as strategies are combined in a portfolio. Then\nportfolio turnover becomes a non linear function of strategies turnover. It\nrises a problem of effective (quick and precise) portfolio turnover estimation.\nKakushadze and Liew (2014) shows how to estimate turnover via covariance matrix\nof returns. We build a mathematical model for such estimations; prove a theorem\nwhich gives a necessary condition for model applicability; suggest new turnover\nestimations; check numerically the preciseness of turnover estimations for\nalgorithmic strategies on USA equity market.",
      "generated_abstract": "This paper presents a novel approach to the problem of modeling financial\npuzzles, which are puzzling puzzles in financial markets. The puzzle consists of\ntwo assets with a common market maker and a third asset that is traded by the\nmarket maker. The market maker is not a hedging player and does not have\nsufficient information about the assets to be able to hedge it. We show that\nthe modeling of the market maker as a multi-asset portfolio is equivalent to\nthe modeling of the market maker as a hedging player. As a result, the\nportfolio theory can be applied to the puzzle model. Our approach involves\nsimulation, which allows us to test the robustness of the model against\nvarious market conditions. The approach is applied to the puzzle of the\nBritish pound against the euro.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21052631578947367,
          "p": 0.2222222222222222,
          "f": 0.21621621121986864
        },
        "rouge-2": {
          "r": 0.019417475728155338,
          "p": 0.017391304347826087,
          "f": 0.018348618868362598
        },
        "rouge-l": {
          "r": 0.19736842105263158,
          "p": 0.20833333333333334,
          "f": 0.20270269770635516
        }
      }
    },
    {
      "paper_id": "math.KT.math/KT/2503.04251v1",
      "true_abstract": "We prove separation and excision results in functor homology. These results\nexplain how the global Steinberg decomposition of functors proved by Djament,\nTouz{\\'e} and Vespa behaves in Ext and Tor computations.",
      "generated_abstract": "the existence and uniqueness of a solution to the system of\nparameters\n  \\begin{align}\n  \\begin{cases}\n    \\partial_t^2 y + \\Delta y = f, & \\text{in } (0, \\infty) \\times \\Omegaf, \\\\\n    y = 0, & \\text{in } \\Omegaf, \\\\\n    \\frac{\\partial y}{\\partial \\nu} = g, & \\text{on } \\Gammac,\n  \\end{cases}\n  \\end{align}\n  where $f, g \\in L^2(\\Omegaf)$, $f$ is smooth enough, while $g$ is\n  singular. The problem is connected to the regularity of boundary\n  curves for the system. The solution concepts are based on the\n  self-similar solutions and the structure of the boundary curves. The\n  main",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.04838709677419355,
          "f": 0.06741572611033986
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.04838709677419355,
          "f": 0.06741572611033986
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2502.15822v1",
      "true_abstract": "This paper proposes a financial fraud detection system based on improved\nRandom Forest (RF) and Gradient Boosting Machine (GBM). Specifically, the\nsystem introduces a novel model architecture called GBM-SSRF (Gradient Boosting\nMachine with Simplified and Strengthened Random Forest), which cleverly\ncombines the powerful optimization capabilities of the gradient boosting\nmachine (GBM) with improved randomization. The computational efficiency and\nfeature extraction capabilities of the Simplified and Strengthened Random\nForest (SSRF) forest significantly improve the performance of financial fraud\ndetection. Although the traditional random forest model has good classification\ncapabilities, it has high computational complexity when faced with large-scale\ndata and has certain limitations in feature selection. As a commonly used\nensemble learning method, the GBM model has significant advantages in\noptimizing performance and handling nonlinear problems. However, GBM takes a\nlong time to train and is prone to overfitting problems when data samples are\nunbalanced. In response to these limitations, this paper optimizes the random\nforest based on the structure, reducing the computational complexity and\nimproving the feature selection ability through the structural simplification\nand enhancement of the random forest. In addition, the optimized random forest\nis embedded into the GBM framework, and the model can maintain efficiency and\nstability with the help of GBM's gradient optimization capability. Experiments\nshow that the GBM-SSRF model not only has good performance, but also has good\nrobustness and generalization capabilities, providing an efficient and reliable\nsolution for financial fraud detection.",
      "generated_abstract": "This paper studies the stochastic volatility model with stochastic\nintensity and with a deterministic term structure of volatility. The term\nstructure of the volatility is modelled by a Gumbel distribution. We derive the\nstationary and deterministic solutions to the stochastic volatility model and\nstudy their properties. We also derive the stochastic volatility models with\nlogarithmic intensities and with a log-normal intensity. We show that the\nlog-normal intensities can be interpreted as a truncated normal distribution.\nFinally, we derive the stationary and deterministic solutions to the log-normal\nmodel and the log-normal model with log-intensity. We also study the\nstochastic volatility with a log-normal intensity and with a log-normal\nintensity and show that the log-normal intensities can be interpreted as a\ntruncated normal distribution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10948905109489052,
          "p": 0.3333333333333333,
          "f": 0.16483516111278837
        },
        "rouge-2": {
          "r": 0.023696682464454975,
          "p": 0.06666666666666667,
          "f": 0.03496503109565303
        },
        "rouge-l": {
          "r": 0.10948905109489052,
          "p": 0.3333333333333333,
          "f": 0.16483516111278837
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/MM/2503.10324v1",
      "true_abstract": "Multi-modal object Re-IDentification (ReID) aims to retrieve specific objects\nby utilizing complementary information from various modalities. However,\nexisting methods focus on fusing heterogeneous visual features, neglecting the\npotential benefits of text-based semantic information. To address this issue,\nwe first construct three text-enhanced multi-modal object ReID benchmarks. To\nbe specific, we propose a standardized multi-modal caption generation pipeline\nfor structured and concise text annotations with Multi-modal Large Language\nModels (MLLMs). Besides, current methods often directly aggregate multi-modal\ninformation without selecting representative local features, leading to\nredundancy and high complexity. To address the above issues, we introduce IDEA,\na novel feature learning framework comprising the Inverted Multi-modal Feature\nExtractor (IMFE) and Cooperative Deformable Aggregation (CDA). The IMFE\nutilizes Modal Prefixes and an InverseNet to integrate multi-modal information\nwith semantic guidance from inverted text. The CDA adaptively generates\nsampling positions, enabling the model to focus on the interplay between global\nfeatures and discriminative local features. With the constructed benchmarks and\nthe proposed modules, our framework can generate more robust multi-modal\nfeatures under complex scenarios. Extensive experiments on three multi-modal\nobject ReID benchmarks demonstrate the effectiveness of our proposed method.",
      "generated_abstract": "opment of large language models (LLMs) has transformed how we\ncommunicate with machines. However, the growing sophistication of LLMs and the\ngrowing complexity of real-world tasks have raised concerns about their ability\nto effectively model human-like language and comprehend their context.\nFurthermore, the rapid advancement of AI technologies, such as generative\nmodels, has led to a surge in the generation of fake content, raising questions\nabout the safety and integrity of AI-generated content. This paper examines the\nimpact of AI on human language and argues that the development of large\nlanguage models poses a significant threat to the safety and integrity of\nAI-generated content. We propose a set of principles for responsible AI\ndevelopment that address the challenges posed by AI-generated content,\nincluding the need for ethical considerations, data quality, and AI",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12121212121212122,
          "p": 0.19047619047619047,
          "f": 0.14814814339506188
        },
        "rouge-2": {
          "r": 0.01675977653631285,
          "p": 0.025210084033613446,
          "f": 0.020134223390614176
        },
        "rouge-l": {
          "r": 0.11363636363636363,
          "p": 0.17857142857142858,
          "f": 0.13888888413580264
        }
      }
    },
    {
      "paper_id": "q-fin.GN.econ/GN/2412.19817v1",
      "true_abstract": "Digital transformation significantly impacts firm investment, financing, and\nvalue enhancement. A systematic investigation from the corporate finance\nperspective has not yet been formed. This paper combines bibliometric and\ncontent analysis methods to systematically review the evolutionary trend,\nstatus quo, hotspots and overall structure of research in digital\ntransformation from 2011 to 2024. The study reveals an emerging and rapidly\ngrowing focus on digital transformation research, particularly in developed\ncountries. We categorize the literature into three areas according to\nbibliometric clustering: the measurements (qualitative and quantitative),\nimpact factors (internal and external), and the economic consequences\n(investment, financing, and firm value). These areas are divided into ten\nsub-branches, with a detailed literature review. We also review the existing\ntheories related to digital transformation, identify the current gaps in these\npapers, and provide directions for future research on each sub-branches.",
      "generated_abstract": "r introduces a novel framework for analyzing the effect of market\nfacilitation on firm performance. By employing a novel methodology, we integrate\nthe mechanisms of market facilitation with the firm-specific factors that\ninfluence the firm's performance. The novelty of our approach lies in the fact\nthat we integrate market facilitation with the firm-specific factors that\ninfluence the firm's performance. The empirical results reveal that market\nfacilitation is positively related to firm performance. Market facilitation\nsignificantly enhances firm performance when firms are in a position to\nfacilitate market transactions. Market facilitation enhances firm performance\nwhen firms are in a position to facilitate market transactions. Market\nfacilitation enhances firm performance when firms are in a position to\nfacilitate market transactions. Market facilitation enhances firm performance\nwhen firms are in a position to facilitate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13725490196078433,
          "p": 0.28,
          "f": 0.18421052190096965
        },
        "rouge-2": {
          "r": 0.007518796992481203,
          "p": 0.014084507042253521,
          "f": 0.009803917030471149
        },
        "rouge-l": {
          "r": 0.13725490196078433,
          "p": 0.28,
          "f": 0.18421052190096965
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.07332v1",
      "true_abstract": "Change-plane analysis is a pivotal tool for identifying subgroups within a\nheterogeneous population, yet it presents challenges when applied to functional\ndata. In this paper, we consider a change-plane model within the framework of\nfunctional response quantile regression, capable of identifying and testing\nsubgroups in non-Gaussian functional responses with scalar predictors. The\nproposed model naturally extends the change-plane method to account for the\nheterogeneity in functional data. To detect the existence of subgroups, we\ndevelop a weighted average of the squared score test statistic, which has a\nclosed form and thereby reduces the computational stress. An alternating\ndirection method of multipliers algorithm is formulated to estimate the\nfunctional coefficients and the grouping parameters. We establish the\nasymptotic theory for the estimates based on the reproducing kernel Hilbert\nspace and derive the asymptotic distributions of the proposed test statistic\nunder both null and alternative hypotheses. Simulation studies are conducted to\nevaluate the performance of the proposed approach in subgroup identification\nand hypothesis test. The proposed methods are also applied to two datasets, one\nfrom a study on China stocks and another from the COVID-19 pandemic.",
      "generated_abstract": "opment of large-scale high-dimensional datasets, including the\nlarge-scale genome-wide association studies (GWAS), is a major challenge in\nstatistical genetics. In this paper, we introduce a novel statistical\nframework for detecting and characterizing rare genetic variants in\nhigh-dimensional settings. The novel approach leverages the principle of\nmultiple testing. We first introduce a novel statistical model for detecting\nand characterizing rare genetic variants. In this model, we introduce a\ngeneralized F-test to characterize the detection performance. We further\nintroduce a novel test statistic, the generalized GWAS statistic, to quantify\nthe false discovery rate (FDR) of rare variants. To further improve the\nperformance of our model, we propose a method for selecting the number of\ntests. We demonstrate the effectiveness of our framework by applying it to a\ngenome-wide association study dataset",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.225,
          "p": 0.3698630136986301,
          "f": 0.2797927414105077
        },
        "rouge-2": {
          "r": 0.04,
          "p": 0.06306306306306306,
          "f": 0.0489510442014284
        },
        "rouge-l": {
          "r": 0.20833333333333334,
          "p": 0.3424657534246575,
          "f": 0.25906735280947146
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.cond-mat/dis-nn/2503.09684v1",
      "true_abstract": "When traffic is routed through a network that is susceptible to congestion,\nthe self-interested decisions made by individual users do not, in general,\nproduce the optimal flow. This discrepancy is quantified by the so-called\n\"price of anarchy.\" Here we consider whether the traffic produced by\nself-interested users is made better or worse when users have uncertain\nknowledge about the cost functions of the links in the network, and we define a\nparallel concept that we call the \"price of ignorance.\" We introduce a simple\nmodel in which fast, congestible links and slow, incongestible links are mixed\nrandomly in a large network and users plan their routes with finite uncertainty\nabout which of the two cost functions describes each link. One of our key\nfindings is that a small level of user ignorance universally improves traffic,\nregardless of the network composition. Further, there is an optimal level of\nignorance which, in our model, causes the self-interested user behavior to\ncoincide with the optimum. Many features of our model can be understood\nanalytically, including the optimal level of user ignorance and the existence\nof critical scaling near the percolation threshold for fast links, where the\npotential benefit of user ignorance is greatest.",
      "generated_abstract": "t a quantum Monte Carlo study of the dynamics of the spin-1/2\nchiral chains in the presence of the antiferromagnetic Heisenberg interaction.\nWe focus on the two-dimensional Hubbard model with a square lattice. Our\nresults indicate that the dynamics of the spin-1/2 chains is dominated by\nspin-liquid-like behavior, characterized by a universal spin correlation\nlength $l_s$ and the onset of long-range spin correlations. Furthermore, we\ndemonstrate that the dynamics of the spin-1/2 chains is driven by a\nnon-monotonic decay of the spin-1/2 chains' correlations in the presence of\nthe Heisenberg interaction. We also show that the emergence of long-range\nspin-liquid-like correlations in the dynamics of the spin-1/2 chains is\nconnected to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10833333333333334,
          "p": 0.22807017543859648,
          "f": 0.14689265100067045
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.038461538461538464,
          "f": 0.022727268564050346
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.21052631578947367,
          "f": 0.13559321597242188
        }
      }
    },
    {
      "paper_id": "cond-mat.quant-gas.physics/space-ph/2503.09553v1",
      "true_abstract": "Cosmic rays are deemed to be generated by a process known as ``Fermi\nacceleration\", in which charged particles scatter against magnetic fluctuations\nin astrophysical plasmas. The process itself is however universal, has both\nclassical and quantum formulations, and is at the basis of dynamical systems\nwith interesting mathematical properties, such as the celebrated Fermi-Ulam\nmodel. Despite its effectiveness in accelerating particles, Fermi acceleration\nhas so far eluded unambiguous verifications in laboratory settings. Here, we\nrealize the first fully controllable Fermi accelerator by colliding ultracold\natoms against engineered movable potential barriers. We demonstrate that our\nFermi accelerator, which is only 100 um in size, can produce ultracold atomic\njets with velocities above half a meter per second. Adding dissipation, we also\nexperimentally test Bell's general argument for the ensuing energy spectra,\nwhich is at the basis of any model of cosmic ray acceleration. On the one hand,\nour work effectively opens the window to the study of high energy astrophysics\nwith cold atoms, offering new capabilities for the understanding of phenomena\nsuch as diffusive acceleration at collisionless shocks. On the other, the\nperformance of our Fermi accelerator is competitive with those of best-in-class\naccelerating methods used in quantum technology and quantum colliders, but with\nsubstantially simpler implementation and virtually no upper limit.",
      "generated_abstract": "t a novel approach for studying the quantum properties of\ndynamical quantum gases in a magnetic trap by employing the dynamical\ninterferometer method. The method relies on a simple quantum interferometric\ndevice that measures the phase differences between two counter-propagating\nlaser beams, which are subsequently used to determine the phase difference\nbetween the quantum gases. We demonstrate the application of this technique to\nstudy the dynamical properties of ultracold Fermi gases in a magnetic trap,\nincluding the ground state energy and the two-body interaction energy of the\ngases. Our findings reveal that the ground state energy of the Fermi gas\nremains constant with respect to the trap frequency, while the two-body\ninteraction energy exhibits a non-trivial dependence on the trap frequency. In\nparticular, we find that the two-body interaction energy exhibits a minimum at\nthe trap",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16891891891891891,
          "p": 0.3246753246753247,
          "f": 0.22222221772009884
        },
        "rouge-2": {
          "r": 0.020100502512562814,
          "p": 0.036036036036036036,
          "f": 0.025806447015817677
        },
        "rouge-l": {
          "r": 0.16216216216216217,
          "p": 0.3116883116883117,
          "f": 0.21333332883120998
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.09849v1",
      "true_abstract": "After-action reviews (AARs) are professional discussions that help operators\nand teams enhance their task performance by analyzing completed missions with\npeers and professionals. Previous studies that compared different formats of\nAARs have mainly focused on human teams. However, the inclusion of robotic\nteammates brings along new challenges in understanding teammate intent and\ncommunication. Traditional AAR between human teammates may not be satisfactory\nfor human-robot teams. To address this limitation, we propose a new training\nreview (TR) tool, called the Virtual Spectator Interface (VSI), to enhance\nhuman-robot team performance and situational awareness (SA) in a simulated\nsearch mission. The proposed VSI primarily utilizes visual feedback to review\nsubjects' behavior. To examine the effectiveness of VSI, we took elements from\nAAR to conduct our own TR, designed a 1 x 3 between-subjects experiment with\nexperimental conditions: TR with (1) VSI, (2) screen recording, and (3)\nnon-technology (only verbal descriptions). The results of our experiments\ndemonstrated that the VSI did not result in significantly better team\nperformance than other conditions. However, the TR with VSI led to more\nimprovement in the subjects SA over the other conditions.",
      "generated_abstract": "We study the task of retrieving scientific documents from a large document\ncollection, which includes scientific papers and technical reports, but excludes\njournals and conference proceedings. We introduce a novel framework for\nretrieval that employs a retrieval-augmented ranking method to guide the\nselection of documents that meet a given query. This method utilizes the\nexistence of a ranking function for each document in the collection, which is\ndefined by the collection's ranking-aware retrieval model. We show that this\nretrieval-augmented ranking method can improve retrieval performance over\nstandard retrieval models in several scenarios. Our evaluation shows that our\nretrieval-augmented ranking method significantly outperforms state-of-the-art\nmodels on various retrieval tasks, such as retrieving scientific documents from\na large document collection.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13432835820895522,
          "p": 0.24,
          "f": 0.17224879922620834
        },
        "rouge-2": {
          "r": 0.00558659217877095,
          "p": 0.009433962264150943,
          "f": 0.00701753918769163
        },
        "rouge-l": {
          "r": 0.13432835820895522,
          "p": 0.24,
          "f": 0.17224879922620834
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.06599v1",
      "true_abstract": "The study examines the return connectedness between climate policy\nuncertainty (CPU), clean energy, fossil energy, and food markets. Using the\ntime-domain method of Diebold and Yilmaz (2012) and frequency-domain methods of\nBarun{\\'{i}}k and K{\\v{r}}hl{\\'{i}}k (2018), we find substantial spillover\neffects between these markets. Furthermore, high frequency domain is the\nprimary driver of overall connectedness. In addition, CPU is a net contributor\nof return shocks in the short term, whereas it turns to be a net recipient in\nthe medium and long terms. Across all frequencies, clean energy and oils are\nconsistent net recipients, while meat is a dominant net contributor.",
      "generated_abstract": "the role of incentives in the evolution of group behavior in\nevolutionary games. We provide a general framework to analyze the emergence of\ncoalitional group selection, where individuals with different fitness levels\ncompete over the same resources but are subject to different incentives. We\ndemonstrate that group selection can occur even when individuals' fitness\nfunctions are linear. Moreover, we show that group selection can be\nirreversibly suppressed by strong incentives, which can be achieved by\nenforcing strict group-level competition. Our analysis reveals that\ngroup-level competition can play a crucial role in shaping group behavior,\nimpacting the evolution of group selection and its subsequent evolutionary\nstability. Finally, we provide a theoretical framework to study the effects of\ngroup-level competition on the evolution of group selection and show that it can\nsignificantly alter the outcome of the evolutionary",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1643835616438356,
          "p": 0.1518987341772152,
          "f": 0.15789473184989627
        },
        "rouge-2": {
          "r": 0.010526315789473684,
          "p": 0.008547008547008548,
          "f": 0.00943395731799832
        },
        "rouge-l": {
          "r": 0.1506849315068493,
          "p": 0.13924050632911392,
          "f": 0.14473683711305416
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/PR/2412.13172v1",
      "true_abstract": "This paper derives the expressions of correlations between prices of two\nassets, returns of two assets, and price-return correlations of two assets that\ndepend on statistical moments and correlations of the current values, past\nvalues, and volumes of their market trades. The usual frequency-based\nexpressions of correlations of time series of prices and returns describe a\npartial case of our model when all trade volumes and past trade values are\nconstant. Such an assumptions are rather far from market reality, and its use\nresults in excess losses and wrong forecasts. Traders, banks, and funds that\nperform multi-million market transactions or manage billion-valued portfolios\nshould consider the impact of large trade volumes on market prices and returns.\nThe use of the market-based correlations of prices and returns of two assets is\nmandatory for them. The development of macroeconomic models and market\nforecasts like those being created by BlackRock's Aladdin, JP Morgan, and the\nU.S. Fed., is impossible without the use of market-based correlations of prices\nand returns of two assets.",
      "generated_abstract": "r investigates the impact of the COVID-19 pandemic on the\ndistribution of income, using a dataset on wealth distribution from 2010 to\n2021 in China. We find that the pandemic caused a significant decrease in the\nwealth distribution, with the wealthiest 10% of the population losing wealth\nwhile the bottom 10% gained wealth. We also find that the pandemic resulted in\na large increase in the gap between rich and poor, particularly among the\nbottom 10%. This finding is consistent with the findings of previous studies\nthat show that the pandemic disproportionately affected the poor. However, our\nanalysis also reveals that the pandemic had a mixed impact on income inequality\nin China. On the one hand, the pandemic led to a significant increase in the\nwealth of the richest 10%, while the poor experienced a decline in wealth. On\nthe",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14583333333333334,
          "p": 0.1917808219178082,
          "f": 0.16568046846539
        },
        "rouge-2": {
          "r": 0.02097902097902098,
          "p": 0.02564102564102564,
          "f": 0.023076918126924133
        },
        "rouge-l": {
          "r": 0.14583333333333334,
          "p": 0.1917808219178082,
          "f": 0.16568046846539
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/RM/2411.06080v1",
      "true_abstract": "Portfolio diversification, traditionally measured through asset correlations\nand volatilitybased metrics, is fundamental to managing financial risk.\nHowever, existing diversification metrics often overlook non-numerical\nrelationships between assets that can impact portfolio stability, particularly\nduring market stresses. This paper introduces the lexical ratio (LR), a novel\nmetric that leverages textual data to capture diversification dimensions absent\nin standard approaches. By treating each asset as a unique document composed of\nsectorspecific and financial keywords, the LR evaluates portfolio\ndiversification by distributing these terms across assets, incorporating\nentropy-based insights from information theory. We thoroughly analyze LR's\nproperties, including scale invariance, concavity, and maximality,\ndemonstrating its theoretical robustness and ability to enhance risk-adjusted\nportfolio returns. Using empirical tests on S&P 500 portfolios, we compare LR's\nperformance to established metrics such as Markowitz's volatility-based\nmeasures and diversification ratios. Our tests reveal LR's superiority in\noptimizing portfolio returns, especially under varied market conditions. Our\nfindings show that LR aligns with conventional metrics and captures unique\ndiversification aspects, suggesting it is a viable tool for portfolio managers.",
      "generated_abstract": "We study the problem of optimizing a stochastic investment process in a\nunderlying asset by trading with a hedging strategy, where the investor's\nexpectation of the future return of the underlying asset is known. The goal is\nto maximize the expected discounted sum of future realized profits, which is\nequivalent to maximizing the expected payoff of the investment process under\nthe hedging strategy. In the case of a constant discount factor, we establish\na new result by showing that the optimal hedging strategy is given by the\ndividend-yielding portfolio, which is known to be optimal for the constant\ndiscount factor. We also show that the optimal hedging strategy is given by\nthe inverse of the optimal investment strategy, which was previously unknown.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11940298507462686,
          "p": 0.25,
          "f": 0.16161615724109796
        },
        "rouge-2": {
          "r": 0.005952380952380952,
          "p": 0.00980392156862745,
          "f": 0.007407402706175822
        },
        "rouge-l": {
          "r": 0.11940298507462686,
          "p": 0.25,
          "f": 0.16161615724109796
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.07664v1",
      "true_abstract": "The Antibiotic Resistance Microbiology Dataset (ARMD) is a de-identified\nresource derived from electronic health records (EHR) that facilitates research\ninto antimicrobial resistance (AMR). ARMD encompasses data from adult patients,\nfocusing on microbiological cultures, antibiotic susceptibilities, and\nassociated clinical and demographic features. Key attributes include organism\nidentification, susceptibility patterns for 55 antibiotics, implied\nsusceptibility rules, and de-identified patient information. This dataset\nsupports studies on antimicrobial stewardship, causal inference, and clinical\ndecision-making. ARMD is designed to be reusable and interoperable, promoting\ncollaboration and innovation in combating AMR. This paper describes the\ndataset's acquisition, structure, and utility while detailing its\nde-identification process.",
      "generated_abstract": "llenge in the study of genetic diseases is to find genetic mutations\nthat cause the same phenotype in both the disease and the healthy state. This\ngoal is often referred to as a \"symbiotic mutation\". In this paper, we consider\nthe problem of finding such mutations in the context of a family of families\nwith mutations, where each family in the family has a unique phenotype. We\nintroduce a generalization of the graph isomorphism problem that is called\n\"mutation isomorphism\". We show that, in this setting, there exists a polynomial\ntime algorithm that finds a mutation that causes the same phenotype in both\nfamilies. We also show that this algorithm runs in polynomial time on graphs\nwith $n$ nodes and $m$ edges, where $n$ is the number of families and $m$ is\nthe number of mutations. Our results are based on an extension of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10714285714285714,
          "p": 0.11392405063291139,
          "f": 0.11042944285746567
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10714285714285714,
          "p": 0.11392405063291139,
          "f": 0.11042944285746567
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PR/2408.01642v2",
      "true_abstract": "The additive process generalizes the L\\'evy process by relaxing its\nassumption of time-homogeneous increments and hence covers a larger family of\nstochastic processes. Recent research in option pricing shows that modeling the\nunderlying log price with an additive process has advantages in easier\nconstruction of the risk-neural measure, an explicit option pricing formula and\ncharacteristic function, and more flexibility to fit the implied volatility\nsurface. Still, the challenge of calibrating an additive model arises from its\ntime-dependent parameterization, for which one has to prescribe parametric\nfunctions for the term structure. For this, we propose the neural term\nstructure model to utilize feedforward neural networks to represent the term\nstructure, which alleviates the difficulty of designing parametric functions\nand thus attenuates the misspecification risk. Numerical studies with S\\&P 500\noption data are conducted to evaluate the performance of the neural term\nstructure.",
      "generated_abstract": "We propose a novel framework for predicting the daily movement of the\nexchange rate between two currencies, with a particular focus on the USD/JPY\nexchange rate. This framework consists of three components: a model for the\nforecasting of the daily logarithm of the exchange rate, a model for the\nforecasting of the daily logarithm of the daily logarithm of the exchange rate,\nand a model for the forecasting of the daily logarithm of the daily logarithm\nof the exchange rate. The three components are trained using a combination of\nvarious data sources. Our results show that the three components of the\nframework perform well, with the best model achieving a root mean squared\nprediction error (RMSE) of 0.0053. We also discuss the implications of this\nframework for future research and practical applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13978494623655913,
          "p": 0.20634920634920634,
          "f": 0.16666666185157802
        },
        "rouge-2": {
          "r": 0.015384615384615385,
          "p": 0.021739130434782608,
          "f": 0.018018013164517174
        },
        "rouge-l": {
          "r": 0.10752688172043011,
          "p": 0.15873015873015872,
          "f": 0.12820512339003962
        }
      }
    },
    {
      "paper_id": "math.HO.math/HO/2502.17478v1",
      "true_abstract": "\"Math is not a spectator sport.\" \"Lecturing is educational malpractice.\"\nSlogans like these rally some mathematicians to teach classes that feature\n\"active learning\", where lecturing is eschewed for student participation. Yet\nas much as I believe that students must do math to learn math, I also find\nblanket statements to be more about bandwagons than considered reflection on\nteaching. In this column, published in the Fall 2021 AWM Newsletter, I urge us\nto think through the math we offer students and how we set up students to\nlearn. Although I draw primarily from my experiences teaching proofs in\nabstract algebra and real analysis, the scenarios extend to other topics in\nfirst year undergraduate education and beyond.",
      "generated_abstract": "In the study of the geometry of the projective plane $\\PP^2$, the\nalgebraic geometry of the projective plane $\\PP^2$ is a natural tool to study\nits geometric properties. In this paper, we study the geometry of $\\PP^2$\nthrough the geometric theory of symmetric spaces. In particular, we study the\nsymmetric space $G/H$ for a connected reductive group $G$ acting on a complex\nprojective space $\\PP^2$. We prove the existence of an irreducible symmetric\nspace of finite type $G/H$ when $G$ is a complex algebraic group of\nreductive type, such as $GL(n)$, $Sp(2n)$, $SU(n)$ or $SO(n)$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11827956989247312,
          "p": 0.21153846153846154,
          "f": 0.1517241333307968
        },
        "rouge-2": {
          "r": 0.017241379310344827,
          "p": 0.024390243902439025,
          "f": 0.020202015349455305
        },
        "rouge-l": {
          "r": 0.11827956989247312,
          "p": 0.21153846153846154,
          "f": 0.1517241333307968
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/AS/2502.03871v1",
      "true_abstract": "We consider a phase-shift mixing model for linear sensor arrays in the\ncontext of blind source extraction. We derive a blind Capon beamformer that\nseeks the direction where the output is independent of the other signals in the\nmixture. The algorithm is based on Independent Component Extraction and imposes\nan orthogonal constraint, thanks to which it optimizes only one real-valued\nparameter related to the angle of arrival. The Cram\\'er-Rao lower bound for the\nmean interference-to-signal ratio is derived. The algorithm and the bound are\ncompared with conventional blind and direction-of-arrival\nestimation+beamforming methods, showing improvements in terms of extraction\naccuracy. An application is demonstrated in frequency-domain speaker extraction\nin a low-reverberation room.",
      "generated_abstract": "This study proposes a deep learning-based multimodal fusion framework for\nmultimodal brain signal classification, specifically designed for the\nEpilepsy Brain Signal Classification (EBSC) task. The framework integrates\ntwo key components: (i) a two-stage multimodal fusion model, which combines\nfusion modules for each modality, and (ii) a classifier, which makes the\nfusion results more interpretable. Experimental results demonstrate that the\nproposed framework achieves the best performance on the EBSC dataset, with\nstate-of-the-art results on the test set. The framework can be easily applied\nto other brain signal classification tasks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13924050632911392,
          "p": 0.171875,
          "f": 0.15384614890116893
        },
        "rouge-2": {
          "r": 0.009259259259259259,
          "p": 0.012048192771084338,
          "f": 0.01047119927414501
        },
        "rouge-l": {
          "r": 0.13924050632911392,
          "p": 0.171875,
          "f": 0.15384614890116893
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.17830v1",
      "true_abstract": "Hypothesis tests and confidence intervals are ubiquitous in empirical\nresearch, yet their connection to subsequent decision-making is often unclear.\nWe develop a theory of certified decisions that pairs recommended decisions\nwith inferential guarantees. Specifically, we attach P-certificates -- upper\nbounds on loss that hold with probability at least $1-\\alpha$ -- to recommended\nactions. We show that such certificates allow \"safe,\" risk-controlling adoption\ndecisions for ambiguity-averse downstream decision-makers. We further prove\nthat it is without loss to limit attention to P-certificates arising as minimax\ndecisions over confidence sets, or what Manski (2021) terms \"as-if decisions\nwith a set estimate.\" A parallel argument applies to E-certified decisions\nobtained from e-values in settings with unbounded loss.",
      "generated_abstract": "We study the problem of estimating the mean of a random variable that has\na multivariate normal distribution. The mean is known but the unknown\ndistribution is unknown. We consider two scenarios: (i) the mean is known and\nthe distribution is known, and (ii) the mean is unknown and the distribution is\nunknown. In the first scenario, we assume that the distribution is known and\nconsider a simple estimator. In the second scenario, we assume that the mean is\nknown and the distribution is unknown and consider a Bayesian estimator. We\nstudy the computational complexity of the two estimators. We also compare the\ncomputational complexity of the two estimators with that of the\n\\emph{regression-then-normalization} estimator.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09090909090909091,
          "p": 0.18604651162790697,
          "f": 0.12213740017015341
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.18604651162790697,
          "f": 0.12213740017015341
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.astro-ph/EP/2503.10309v1",
      "true_abstract": "Observational data provided by JWST instruments continue to challenge\ntheories and models of cloud formation in sub-stellar atmospheres, requiring\nmore sophisticated approaches in an effort to understand their spatial\ncomplexity. However, to date, most cloud microphysical models using the moment\nmethod for sub-stellar atmospheres have assumed a monodisperse size\ndistribution, neglecting polydisperse properties. We aim to extend beyond the\ncommon assumption of a monodisperse size distribution and analyse cloud\nmicrophysical processes assuming an exponential distribution. We derive\nexpressions for the zeroth and first moments of condensation/evaporation and\ncollisional growth processes under the assumption of an exponential size\ndistribution. We then compare the differences between monodisperse and\nexponential distribution microphysics using a simple one-dimensional (1D)\ncolumn model applied to a Y-dwarf KCl cloud scenario. We find that adopting an\nexponential distribution modifies condensation/evaporation rates by a factor of\n$\\approx$0.9 and collisional growth rates by factors of $\\approx$1.1 (Kn $\\ll$\n1) and $\\approx$0.92 (Kn $\\gg$ 1) for Brownian coagulation and $\\approx$0.85\nfor gravitational coalescence, compared to the monodisperse case. In our\nspecific test cases, we find relative differences of a maximum 10-12% in total\nnumber density and 2-3% in mean radius of the cloud particles between the\nmonodisperse and exponential distributions. Our results offer a simple way to\ntake into account an assumed exponential size distribution for sub-stellar\natmospheric cloud microphysics using the two-moment method. In follow up\nstudies, we will examine more complex distributions, such as the log-normal and\ngamma distributions, that require more than two moments to characterise\nself-consistently.",
      "generated_abstract": "of the interactions between the interstellar medium (ISM) and\nthe Milky Way (MW) is crucial for understanding the galaxy's evolution and\nhaving a deeper understanding of the physical processes that shape the\ninterstellar medium. We present a new high-resolution, high-SNR, 100,000-pixel\nwide-field image of the MW obtained with the Subaru Prism Multi-Object\nspectrograph (SPMOS) and the Gemini Multi-Object Spectrograph (GMOS) on the\nGemini South telescope. The image covers the entire MW, with a field of view\n(FoV) of 6.8 deg$^2$. The observations were obtained during the first half of\n2024 and the second half of 2025. The data were reduced and analyzed using the\nHARPSon",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0821917808219178,
          "p": 0.17391304347826086,
          "f": 0.11162790261806399
        },
        "rouge-2": {
          "r": 0.013157894736842105,
          "p": 0.030927835051546393,
          "f": 0.01846153427389444
        },
        "rouge-l": {
          "r": 0.0821917808219178,
          "p": 0.17391304347826086,
          "f": 0.11162790261806399
        }
      }
    },
    {
      "paper_id": "cs.SD.cs/DL/2502.17726v1",
      "true_abstract": "The Musical Instrument Digital Interface (MIDI), introduced in 1983,\nrevolutionized music production by allowing computers and instruments to\ncommunicate efficiently. MIDI files encode musical instructions compactly,\nfacilitating convenient music sharing. They benefit Music Information Retrieval\n(MIR), aiding in research on music understanding, computational musicology, and\ngenerative music. The GigaMIDI dataset contains over 1.4 million unique MIDI\nfiles, encompassing 1.8 billion MIDI note events and over 5.3 million MIDI\ntracks. GigaMIDI is currently the largest collection of symbolic music in MIDI\nformat available for research purposes under fair dealing. Distinguishing\nbetween non-expressive and expressive MIDI tracks is challenging, as MIDI files\ndo not inherently make this distinction. To address this issue, we introduce a\nset of innovative heuristics for detecting expressive music performance. These\ninclude the Distinctive Note Velocity Ratio (DNVR) heuristic, which analyzes\nMIDI note velocity; the Distinctive Note Onset Deviation Ratio (DNODR)\nheuristic, which examines deviations in note onset times; and the Note Onset\nMedian Metric Level (NOMML) heuristic, which evaluates onset positions relative\nto metric levels. Our evaluation demonstrates these heuristics effectively\ndifferentiate between non-expressive and expressive MIDI tracks. Furthermore,\nafter evaluation, we create the most substantial expressive MIDI dataset,\nemploying our heuristic, NOMML. This curated iteration of GigaMIDI encompasses\nexpressively-performed instrument tracks detected by NOMML, containing all\nGeneral MIDI instruments, constituting 31% of the GigaMIDI dataset, totalling\n1,655,649 tracks.",
      "generated_abstract": "aper, we propose a novel architecture called Vision-Language\nbi-modal Transformer, which can simultaneously process visual and textual\ninformation. The architecture is designed for large-scale vision-language\nmodeling, enabling efficient learning and robust generalization. The Vision-Language\nbi-modal Transformer consists of two key components: a visual encoder and a\ntext encoder. The visual encoder captures visual features by using a\nmulti-layer perceptron (MLP) with a residual connection, and a vision-language\nencoder is designed to fuse the visual and textual information. The text\nencoder consists of two encoders: a pre-trained encoder and an encoder designed\nbased on the multi-head self-attention mechanism. The Vision-Language bi-modal\nTransformer is then fine-tuned with a downstream vision-language model.\nExperimental results",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07741935483870968,
          "p": 0.18461538461538463,
          "f": 0.10909090492768612
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07096774193548387,
          "p": 0.16923076923076924,
          "f": 0.09999999583677703
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/LG/2503.10632v1",
      "true_abstract": "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of\nlearnable activation functions with the potential to capture more complex\nrelationships from data. Although KANs are useful in finding symbolic\nrepresentations and continual learning of one-dimensional functions, their\neffectiveness in diverse machine learning (ML) tasks, such as vision, remains\nquestionable. Presently, KANs are deployed by replacing multilayer perceptrons\n(MLPs) in deep network architectures, including advanced architectures such as\nvision Transformers (ViTs). In this paper, we are the first to design a general\nlearnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate\non any choice of basis. However, the computing and memory costs of training\nthem motivated us to propose a more modular version, and we designed particular\nlearnable attention, called Fourier-KArAt. Fourier-KArAt and its variants\neither outperform their ViT counterparts or show comparable performance on\nCIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures'\nperformance and generalization capacity by analyzing their loss landscapes,\nweight distributions, optimizer path, attention visualization, and spectral\nbehavior, and contrast them with vanilla ViTs. The goal of this paper is not to\nproduce parameter- and compute-efficient attention, but to encourage the\ncommunity to explore KANs in conjunction with more advanced architectures that\nrequire a careful understanding of learnable activations. Our open-source code\nand implementation details are available on: https://subhajitmaity.me/KArAt",
      "generated_abstract": "uce a novel approach for evaluating the performance of a\ndifferential privacy (DP) mechanism by examining the evolution of a\ndifferentially private model. The model is evaluated using a set of pre-defined\nmetrics, including the mean squared error (MSE), mean absolute error (MAE), and\nthe average time to discovery (ATD) for each instance. The proposed method\nprovides a framework for evaluating the performance of differentially private\nmechanisms across a range of scenarios, including the application of differential\nprivacy to real-world problems and the comparison of different privacy\nsettings. Our analysis reveals that the performance of the mechanism can be\ninfluenced by the type of underlying data, the type of noise used, and the\nsensitivity of the model. Additionally, we show that the performance of the\nmechanism can be improved by combining differential privacy with other\nmechanisms, such",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11688311688311688,
          "p": 0.23376623376623376,
          "f": 0.15584415139971153
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11688311688311688,
          "p": 0.23376623376623376,
          "f": 0.15584415139971153
        }
      }
    },
    {
      "paper_id": "cs.LG.math/OC/2503.09737v1",
      "true_abstract": "Soccer analysis tools emphasize metrics such as expected goals, leading to an\noverrepresentation of attacking players' contributions and overlooking players\nwho facilitate ball control and link attacks. Examples include Rodri from\nManchester City and Palhinha who just transferred to Bayern Munich. To address\nthis bias, we aim to identify players with pivotal roles in a soccer team,\nincorporating both spatial and temporal features.\n  In this work, we introduce a GNN-based framework that assigns individual\ncredit for changes in expected threat (xT), thus capturing overlooked yet vital\ncontributions in soccer. Our pipeline encodes both spatial and temporal\nfeatures in event-centric graphs, enabling fair attribution of non-scoring\nactions such as defensive or transitional plays. We incorporate centrality\nmeasures into the learned player embeddings, ensuring that ball-retaining\ndefenders and defensive midfielders receive due recognition for their overall\nimpact. Furthermore, we explore diverse GNN variants-including Graph Attention\nNetworks and Transformer-based models-to handle long-range dependencies and\nevolving match contexts, discussing their relative performance and\ncomputational complexity. Experiments on real match data confirm the robustness\nof our approach in highlighting pivotal roles that traditional attacking\nmetrics typically miss, underscoring the model's utility for more comprehensive\nsoccer analytics.",
      "generated_abstract": "This paper addresses the problem of learning and classifying the continuous\ndistributions in the space of continuous functions. We introduce a\nparametrized continuous distribution class and propose a novel approach for\nlearning and classifying it by means of a neural network. The proposed\napproach is based on a generalized Bayesian approach and incorporates the\ninformation of the joint probability density function of the continuous\ndistributions as well as the joint probability mass functions. In addition, we\npropose a novel method for the initialization of the neural network. Our\nexperiments demonstrate the superior performance of the proposed approach in\nclassifying continuous distributions compared to a number of other established\ntechniques. Additionally, the proposed approach is shown to be robust to\noutliers in the dataset.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1095890410958904,
          "p": 0.23880597014925373,
          "f": 0.15023473747184213
        },
        "rouge-2": {
          "r": 0.010869565217391304,
          "p": 0.019230769230769232,
          "f": 0.013888884274692889
        },
        "rouge-l": {
          "r": 0.1095890410958904,
          "p": 0.23880597014925373,
          "f": 0.15023473747184213
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/RM/2501.11552v1",
      "true_abstract": "We explore the interplay between sovereign debt default/renegotiation and\nenvironmental factors (e.g., pollution from land use, natural resource\nexploitation). Pollution contributes to the likelihood of natural disasters and\ninfluences economic growth rates. The country can default on its debt at any\ntime while also deciding whether to invest in pollution abatement. The\nframework provides insights into the credit spreads of sovereign bonds and\nexplains the observed relationship between bond spread and a country's climate\nvulnerability. Through calibration for developing and low-income countries, we\ndemonstrate that there is limited incentive for these countries to address\nclimate risk, and the sensitivity of bond spreads to climate vulnerability\nremains modest. Climate risk does not play a relevant role on the decision to\ndefault on sovereign debt. Financial support for climate abatement expenditures\ncan effectively foster climate adaptation actions, instead renegotiation\nconditional upon pollution abatement does not produce any effect.",
      "generated_abstract": "aper, we develop a framework for predicting the price of a\nproduct using only the historical price data. We first present a novel\napproach for predicting the future price using only historical data. Our\napproach is based on a novel approach for predicting the future price that\ncaptures the interrelationship between past prices and future prices. In this\napproach, we first estimate the price process using the historical data, and\nthen predict the future price using the estimated price process. We show that\nthis approach outperforms the traditional approach that uses the price process\nfrom the first period as the initial condition for the price process in the\nfollowing period. We then apply this approach to price the shares of 10\nindustrial companies in the United States. We find that the average return of\nthe estimated price process is 1.87% per year, which is much higher than the\nreturn of the price process estimated from",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14423076923076922,
          "p": 0.2112676056338028,
          "f": 0.1714285666063675
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14423076923076922,
          "p": 0.2112676056338028,
          "f": 0.1714285666063675
        }
      }
    },
    {
      "paper_id": "math.CO.math/AC/2503.01647v1",
      "true_abstract": "Classical results of Cauchy and Dehn imply that the 1-skeleton of a convex\npolyhedron $P$ is rigid i.e. every continuous motion of the vertices of $P$ in\n$\\mathbb R^3$ which preserves its edge lengths results in a polyhedron which is\ncongruent to $P$. This result was extended to convex poytopes in $\\mathbb R^d$\nfor all $d\\geq 3$ by Whiteley, and to generic realisations of 1-skeletons of\nsimplicial $(d-1)$-manifolds in $\\mathbb R^{d}$ by Kalai for $d\\geq 4$ and\nFogelsanger for $d\\geq 3$. We will generalise Kalai's result by showing that,\nfor all $d\\geq 4$ and any fixed $1\\leq k\\leq d-3$, every generic realisation of\nthe $k$-skeleton of a simplicial $(d-1)$-manifold in $\\mathbb R^{d}$ is volume\nrigid, i.e. every continuous motion of its vertices in $\\mathbb R^d$ which\npreserves the volumes of its $k$-faces results in a congruent realisation. In\naddition, we conjecture that our result remains true for $k=d-2$ and verify\nthis conjecture when $d=4,5,6$.",
      "generated_abstract": "In 1949, Atiyah proved the following: if $C$ is acyclic and $\\mathcal{A}$\nis a subalgebra of a commutative ring $A$, then the homology functor\n$\\mathrm{H}_n(\\mathrm{Hom}_A(\\mathcal{A},C)) \\to \\mathrm{H}_n(C)$ is an\nisomorphism if and only if $\\mathrm{H}_n(\\mathrm{Hom}_A(\\mathcal{A},C))$ is an\nabelian group. In this paper, we generalize Atiyah's result to arbitrary\ncommutative rings. We also discuss the relationship between this generalization\nand the case of acyclic coalgebras.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13253012048192772,
          "p": 0.23404255319148937,
          "f": 0.16923076461420133
        },
        "rouge-2": {
          "r": 0.007575757575757576,
          "p": 0.016129032258064516,
          "f": 0.01030927400148977
        },
        "rouge-l": {
          "r": 0.13253012048192772,
          "p": 0.23404255319148937,
          "f": 0.16923076461420133
        }
      }
    },
    {
      "paper_id": "stat.AP.q-bio/OT/2503.08378v1",
      "true_abstract": "Objectives: To examine the distribution, temporal associations, and\nage/sex-specific patterns of multiple long-term conditions (MLTCs) in adults\nwith intellectual disability (ID).\n  Study Design: Observational study using longitudinal healthcare data.\nMethods: Analysis of 18144 adults with ID (10168 males and 7976 females)\nidentified in the Clinical Practice Research Datalink, linked to Hospital\nEpisode Statistics Admitted Patient Care and Outpatient data (2000-2021). We\nused temporal analysis to establish directional associations among 40 long-term\nconditions, stratified by sex and age groups (under 45, 45-64, 65 and over).\n  Results: The high prevalence of enduring mental illness across all age groups\nis an important finding unique to this population. In males, mental illness\noccurred along with upper gastrointestinal conditions (specifically reflux\ndisorders), while in females, mental illness presented alongside reflux\ndisorders, chronic pain, and endocrine conditions such as thyroid problems.\nAmong young males with intellectual disability, the combination of cerebral\npalsy with dysphagia, epilepsy, chronic constipation, and chronic pneumonia\nrepresents a distinctive pattern. In those aged 45-64, we observed early onset\nof lifestyle conditions like diabetes and hypertension, though notably these\nconditions co-occurred with mental illness and anaemia at rates exceeding those\nin the general population. The health conditions in those aged 65 and over\nlargely mirrored those seen in the general aging population.\n  Conclusions: Our findings highlight the complex patterns of MLTCs in this\npopulation, revealing sex-specific associations across age groups, and\nidentified temporal associations, thus providing insights into disease\nprogression, which can inform targeted prevention strategies and interventions\nto prevent premature mortality.",
      "generated_abstract": "tion of the genome has been a topic of intense debate, with\nresearchers frequently using different methods to analyze the genomic\nevolutionary history. Here, we propose a novel framework to analyze the\nevolutionary history of the genome using the genome sequence alone. We show\nthat a linear model of evolution can be used to infer the mutation rate and\nevolutionary rate from the observed sequence data. This framework is general and\ndoes not require prior knowledge of the evolutionary history. We apply our\nframework to the human genome and use it to analyze the sequence data of\nhuman-chimpanzee and human-gorilla genomes. Our analysis reveals that the\nsequence data of the human genome is consistent with a recent common ancestry\nwith chimpanzees and gorillas. We also find evidence of recent positive\nselection on the human genome, including a significant increase in the number of\nC",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10404624277456648,
          "p": 0.21951219512195122,
          "f": 0.14117646622499053
        },
        "rouge-2": {
          "r": 0.00425531914893617,
          "p": 0.008130081300813009,
          "f": 0.005586587668147582
        },
        "rouge-l": {
          "r": 0.09248554913294797,
          "p": 0.1951219512195122,
          "f": 0.12549019171518663
        }
      }
    },
    {
      "paper_id": "math.PR.stat/TH/2502.16916v1",
      "true_abstract": "This paper establishes sharp dimension-free concentration inequalities and\nexpectation bounds for the deviation of the sum of simple random tensors from\nits expectation. As part of our analysis, we use generic chaining techniques to\nobtain a sharp high-probability upper bound on the suprema of multi-product\nempirical processes. In so doing, we generalize classical results for quadratic\nand product empirical processes to higher-order settings.",
      "generated_abstract": "We establish a connection between the geometric $L^2$-norm of a random\nvector and its squared sum of squares. Our result generalizes a recent\nresult of Peng and Zhang [SIAM J. Math. Anal. 55 (2013) 3136--3157",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14,
          "p": 0.2413793103448276,
          "f": 0.1772151852267266
        },
        "rouge-2": {
          "r": 0.01639344262295082,
          "p": 0.029411764705882353,
          "f": 0.021052626982826488
        },
        "rouge-l": {
          "r": 0.12,
          "p": 0.20689655172413793,
          "f": 0.1518987295305241
        }
      }
    },
    {
      "paper_id": "math.LO.math/RA/2503.10528v1",
      "true_abstract": "The first-order model theory of modules has been studied for decades. More\nrecently, the model theoretic study of nonelementary classes of\nmodules--especially Abstract Elementary Classes of modules--has produced\ninteresting results. This survey aims to discuss these recent results and give\nan introduction to the framework of Abstract Elementary Classes for module\ntheorists.",
      "generated_abstract": "In this paper we present a new method to compute the Chern classes of\nAtiyah-Bott-McCammon (ABM) classes of smooth manifolds. We use the $K$-theoretic\nrepresentation of the Chern classes of smooth manifolds, known as the\n$K$-homology of Atiyah-Bott-McCammon (ABM) classes. Our method is based on the\n$K$-theory of a particular complex analytic space. We show that for each\nsmooth manifold $M$ with a basis of ABM classes $\\alpha_1,\\ldots,\\alpha_k$, the\n$K$-homology of the ABM classes $\\alpha_1,\\ldots,\\alpha_k$ is the Chern class\n$c_1(M)$ of $M$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.10416666666666667,
          "f": 0.11363635867768616
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.014705882352941176,
          "f": 0.017094012225876174
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.10416666666666667,
          "f": 0.11363635867768616
        }
      }
    },
    {
      "paper_id": "math.OC.econ/GN/2502.12035v1",
      "true_abstract": "The transition to a low-carbon economy necessitates effective carbon capture\nand storage (CCS) solutions, particularly for hard-to-abate sectors. Herein,\npipeline networks are indispensable for cost-efficient $CO_2$ transportation\nover long distances. However, there is deep uncertainty regarding which\nindustrial sectors will participate in such systems. This poses a significant\nchallenge due to substantial investments as well as the lengthy planning and\ndevelopment timelines required for $CO_2$ pipeline projects, which are further\nconstrained by limited upgrade options for already built infrastructure. The\neconomies of scale inherent in pipeline construction exacerbate these\nchallenges, leading to potential regret over earlier decisions. While numerous\nmodels were developed to optimize the initial layout of pipeline infrastructure\nbased on known demand, a gap exists in addressing the incremental development\nof infrastructure in conjunction with deep uncertainty. Hence, this paper\nintroduces a novel optimization model for $CO_2$ pipeline infrastructure\ndevelopment, minimizing regret as its objective function and incorporating\nvarious upgrade options, such as looping and pressure increases. The model's\neffectiveness is also demonstrated by presenting a comprehensive case study of\nGermany's cement and lime industries. The developed approach quantitatively\nillustrates the trade-off between different options, which can help in deriving\neffective strategies for $CO_2$ infrastructure development.",
      "generated_abstract": "the optimal transportation problem between two convex bodies in\n$\\mathbb{R}^n$ where one body is contained in the other. We introduce a\nnon-convex potential $\\Phi$ and show that the optimal transport problem is\nequivalent to finding the minimizer of $\\Phi$ between the two bodies. We then\nprove the existence of a minimizer for $\\Phi$ in terms of the minimizer of a\nconvex function. We then show that the minimizer of $\\Phi$ is the minimizer of\nthe convex function $\\Phi$ and this minimizer is unique. We prove that the\nconvex function $\\Phi$ is strictly convex. We show that the minimizer of\n$\\Phi$ is the unique minimizer of the convex function $\\Phi$ in the case where\nthe two bodies are convex bodies and $\\Phi$ is a convex function. Finally, we\nstudy the case where the two bodies are non-convex bodies",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11594202898550725,
          "p": 0.35555555555555557,
          "f": 0.17486338426946169
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10144927536231885,
          "p": 0.3111111111111111,
          "f": 0.15300546077219393
        }
      }
    },
    {
      "paper_id": "nucl-ex.nucl-ex/2503.07841v1",
      "true_abstract": "We present the first measurements with a new collinear laser spectroscopy\nsetup at the Argonne Tandem Linac Accelerator System utilizing its unique\ncapability to deliver neutron-rich refractory metal isotopes produced by the\nspontaneous fission of 252Cf. We measured isotope shifts from optical spectra\nfor nine radioactive ruthenium isotopes 106-114Ru, reaching deep into the\nmid-shell region. The extracted charge radii are in excellent agreement with\npredictions from the Brussels-Skyrme-on-a-Grid models that account for the\ntriaxial deformation of nuclear ground states in this region. We show that\ntriaxial deformation impacts charge radii in models that feature shell effects,\nin contrast to what could be concluded from a liquid drop analysis. This\nindicates that this exotic type of deformation should not be neglected in\nregions where it is known to occur, even if its presence cannot be\nunambiguously inferred through laser spectroscopy.",
      "generated_abstract": "We present an updated analysis of the inclusive charged-current (CC)\n$p$-$p$ cross section in the energy range 50-160 GeV/$c^2$. This analysis\nincludes the data collected at the CERN SPS from 2017 to 2021. The updated\ncross section is in good agreement with the previous analysis. The\nmeasurements are in agreement with the Standard Model (SM) prediction. The\nuncertainty of the previous analysis was mainly due to the lack of data at\nhigher energies. We have improved the description of the data by including\nthe $pp$ data at $\\sqrt{s}=19.6$ GeV/$c$. The measured cross section at\n$\\sqrt{s}=19.6$ GeV/$c$ is in good agreement with the previous analysis. The\nnew cross section is in good agreement with the SM prediction.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18627450980392157,
          "p": 0.31666666666666665,
          "f": 0.23456789657064478
        },
        "rouge-2": {
          "r": 0.029850746268656716,
          "p": 0.0449438202247191,
          "f": 0.03587443466548758
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.3,
          "f": 0.22222221755829916
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.02609v1",
      "true_abstract": "People are influenced by their peers when making decisions. In this paper, we\nstudy the linear-in-means model which is the standard empirical model of peer\neffects. As data on the underlying social network is often difficult to come\nby, we focus on data that only captures an agent's choices. Under exogenous\nagent participation variation, we study two questions. We first develop a\nrevealed preference style test for the linear-in-means model. We then study the\nidentification properties of the linear-in-means model. With sufficient\nparticipation variation, we show how an analyst is able to recover the\nunderlying network structure and social influence parameters from choice data.\nOur identification result holds when we allow the social network to vary across\ncontexts. To recover predictive power, we consider a refinement which allows us\nto extrapolate the underlying network structure across groups and provide a\ntest of this version of the model.",
      "generated_abstract": "We study a game between two agents who jointly choose between two distinct\ngame-theoretic strategies. We show that this game admits a Nash equilibrium\nwhich is a mixed Nash equilibrium, and that this equilibrium is unique. We\nfurthermore provide a characterization of this mixed Nash equilibrium as a\nminimal strategy of the game in which the agents' payoffs are maximized. This\nminimal strategy is a Nash equilibrium in the original game as well.\n  We apply the results to the problem of minimizing a budget-constrained\nutility function, and provide a characterization of the minimal strategy as\nthe strategy that minimizes the budget-constrained utility function subject to\na budget constraint.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16304347826086957,
          "p": 0.2777777777777778,
          "f": 0.20547944739350732
        },
        "rouge-2": {
          "r": 0.03787878787878788,
          "p": 0.053763440860215055,
          "f": 0.0444444395946672
        },
        "rouge-l": {
          "r": 0.16304347826086957,
          "p": 0.2777777777777778,
          "f": 0.20547944739350732
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/RM/2412.04263v1",
      "true_abstract": "A simple model-free and distribution-free statistic, the functional\nrelationship between the number of \"effective\" degrees of freedom and portfolio\nsize, or N*(N), is used to discriminate between two alternative models for the\ncorrelation of daily cryptocurrency returns within a retail universe of defined\nby the list of tradable assets available to account holders at the Robinhood\nbrokerage. The average pairwise correlation between daily cryptocurrency\nreturns is found to be high (of order 60%) and the data collected supports\ndescription of the cross-section of returns by a simple isotropic correlation\nmodel distinct from a decomposition into a linear factor model with additive\nnoise with high confidence. This description appears to be relatively stable\nthrough time.",
      "generated_abstract": "We investigate the application of the stochastic volatility framework to\nthe pricing of American options, focusing on the Black-Scholes model with\nstochastic volatility. We employ a Monte Carlo simulation method to obtain\nstochastic volatility parameters, and evaluate their effect on option prices.\nWe also consider the effect of the volatility surface function, and show how\nit can be used to control the volatility. Our results highlight the\nsignificance of stochastic volatility in pricing American options, and\ndemonstrate the usefulness of using Monte Carlo simulations for this task.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1282051282051282,
          "p": 0.18518518518518517,
          "f": 0.15151514668044092
        },
        "rouge-2": {
          "r": 0.02727272727272727,
          "p": 0.0379746835443038,
          "f": 0.0317460268805472
        },
        "rouge-l": {
          "r": 0.11538461538461539,
          "p": 0.16666666666666666,
          "f": 0.1363636315289258
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.quant-ph/2503.10462v1",
      "true_abstract": "Deep neural quantum states have recently achieved remarkable performance in\nsolving challenging quantum many-body problems. While transformer networks\nappear particularly promising due to their success in computer science, we show\nthat previously reported transformer wave functions haven't so far been capable\nto utilize their full power. Here, we introduce the convolutional transformer\nwave function (CTWF). We show that our CTWFs exhibit superior performance in\nground-state search and non-equilibrium dynamics compared to previous results,\ndemonstrating promising capacity in complex quantum problems.",
      "generated_abstract": "t a novel approach to the study of quantum systems by employing\nthe formalism of the quantum-field theory of gauge fields. We demonstrate\nthat the system of interacting bosons, fermions, and gauge fields is\nnon-linearly realized in the form of a closed string network. The resulting\nquantum field theory is a generalized version of the Standard Model. We\nexplain how the network can be constructed by coupling the gauge fields to\nthe localized bosonic modes. We show that the resulting system can be\ndescribed by a master field theory. This framework allows us to study the\neffects of interactions on the system by integrating out the gauge fields and\nanalyzing the resulting effective field theory. We demonstrate that the\nsystem exhibits a rich spectrum of collective excitations, including\npolarons, phonons, and glueballs. Furthermore, we discuss the interplay of\nthe gauge fields with the local",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.11538461538461539,
          "f": 0.127659569524672
        },
        "rouge-2": {
          "r": 0.02631578947368421,
          "p": 0.016129032258064516,
          "f": 0.01999999528800111
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.11538461538461539,
          "f": 0.127659569524672
        }
      }
    },
    {
      "paper_id": "math.NA.math/AP/2503.09580v1",
      "true_abstract": "We introduce a fast Fourier spectral method to compute linearized collision\noperators of the Boltzmann equation for variable hard-sphere gases. While the\nstate-of-the-art method provides a computational cost O(MN^4 log N), with N\nbeing the number of modes in each direction and M being the number of\nquadrature points on a hemisphere, our method reduces the cost to O(N^4 log N),\nremoving the factor M, which could be large in our numerical tests. The method\nis applied in a numerical solver for the steady-state Boltzmann equation with\nquadratic collision operators. Numerical experiments for both spatially\nhomogeneous and inhomogeneous Boltzmann equations have been carried out to test\nthe accuracy and efficiency of our method.",
      "generated_abstract": "In this work, we propose an efficient method for the solution of a class of\nintroduction problems, including the Dirichlet and Neumann problems for the\nconvection-diffusion equation. The proposed method is based on the use of the\nspectral decomposition of the matrix of the linearization of the problem,\nassuming that the matrix is not necessarily invertible. We prove that, under\nsuitable assumptions, the spectral decomposition can be performed in a\ncomputationally efficient way.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1891891891891892,
          "p": 0.27450980392156865,
          "f": 0.22399999516928012
        },
        "rouge-2": {
          "r": 0.0380952380952381,
          "p": 0.06153846153846154,
          "f": 0.04705881880622886
        },
        "rouge-l": {
          "r": 0.14864864864864866,
          "p": 0.21568627450980393,
          "f": 0.17599999516928014
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.10510v1",
      "true_abstract": "Whole-slide image classification represents a key challenge in computational\npathology and medicine. Attention-based multiple instance learning (MIL) has\nemerged as an effective approach for this problem. However, the effect of\nattention mechanism architecture on model performance is not well-documented\nfor biomedical imagery. In this work, we compare different methods and\nimplementations of MIL, including deep learning variants. We introduce a new\nmethod using higher-dimensional feature spaces for deep MIL. We also develop a\nnovel algorithm for whole-slide image classification where extreme machine\nlearning is combined with attention-based MIL to improve sensitivity and reduce\ntraining complexity. We apply our algorithms to the problem of detecting\ncirculating rare cells (CRCs), such as erythroblasts, in peripheral blood. Our\nresults indicate that nonlinearities play a key role in the classification, as\nremoving them leads to a sharp decrease in stability in addition to a decrease\nin average area under the curve (AUC) of over 4%. We also demonstrate a\nconsiderable increase in robustness of the model with improvements of over 10%\nin average AUC when higher-dimensional feature spaces are leveraged. In\naddition, we show that extreme learning machines can offer clear improvements\nin terms of training efficiency by reducing the number of trained parameters by\na factor of 5 whilst still maintaining the average AUC to within 1.5% of the\ndeep MIL model. Finally, we discuss options of enriching the classical\ncomputing framework with quantum algorithms in the future. This work can thus\nhelp pave the way towards more accurate and efficient single-cell diagnostics,\none of the building blocks of precision medicine.",
      "generated_abstract": "In this paper, we present a novel computational framework to perform\nthe multi-species metabolic network dynamics simulation for multi-species\nnetworks with different species. The framework is based on the concept of\n``reaction order'' as a key parameter to control the metabolic dynamics in the\nmulti-species metabolic network. We use this framework to simulate the\ndynamics of a multi-species metabolic network and compare the results with the\nresults obtained using the conventional method. We find that the framework\nprovides a more accurate and efficient method for simulating the metabolic\ndynamics of multi-species networks compared to the conventional method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.5283018867924528,
          "f": 0.25339366151225406
        },
        "rouge-2": {
          "r": 0.03278688524590164,
          "p": 0.0963855421686747,
          "f": 0.04892965982062893
        },
        "rouge-l": {
          "r": 0.1488095238095238,
          "p": 0.4716981132075472,
          "f": 0.22624434024528575
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.01687v2",
      "true_abstract": "We establish Rezk completion functors for $\\Theta_n$ spaces with respect to\neach and all of the completeness conditions. As a consequence, we obtain a\ncharacterization of Dwyer-Kan equivalences between Segal $\\Theta_n$ spaces.",
      "generated_abstract": "We introduce a new method to compute the canonical isomorphism classes of\ngeneralised affine schemes, and study the homotopy type of their affine\ndeformation spaces. Our approach is based on the theory of simplicial\ncohomology, and the homotopy type of a generalised affine scheme is recovered\nfrom the homotopy type of its canonical basis of simplices. This approach\nprovides a new perspective on the construction of the K\\\"unneth isomorphism\nbetween the homotopy groups of the canonical basis of simplices of a generalised\naffine scheme.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2857142857142857,
          "p": 0.18604651162790697,
          "f": 0.22535210789922644
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.015625,
          "f": 0.021276591398823885
        },
        "rouge-l": {
          "r": 0.2857142857142857,
          "p": 0.18604651162790697,
          "f": 0.22535210789922644
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.03916v1",
      "true_abstract": "We study stability properties of fully faithful functors, and compute mapping\nanima in pushouts of $\\infty$-categories along fully faithful functors. We\nprovide applications of these calculations to pushouts along Dwyer functors and\nReedy categories.",
      "generated_abstract": "We present a novel approach to the solution of the $n$-pointed Euler system,\nestablished by Nishimura. We derive the explicit formula for the $n$-pointed\nEuler system by combining the characteristic equation for the $n$-pointed\nLaplace operator with the fundamental theorem of calculus. We also provide a\nsimple proof of the result that the $n$-pointed Euler system is linearly\nrecurrent. We also establish the existence of the characteristic equation for\nthe $n$-pointed Euler system and derive its explicit form. In addition, we\nderive the explicit formula for the $n$-pointed Euler system when $n=2$. We\nalso present a new proof of the existence of the characteristic equation for the\n$2$-pointed Euler system. Finally, we show that the $2$-pointed Euler system\nis linearly recurrent.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.09615384615384616,
          "f": 0.12987012548490487
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.09615384615384616,
          "f": 0.12987012548490487
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2411.18830v1",
      "true_abstract": "We study the relationship between model complexity and out-of-sample\nperformance in the context of mean-variance portfolio optimization.\nRepresenting model complexity by the number of assets, we find that the\nperformance of low-dimensional models initially improves with complexity but\nthen declines due to overfitting. As model complexity becomes sufficiently\nhigh, the performance improves with complexity again, resulting in a double\nascent Sharpe ratio curve similar to the double descent phenomenon observed in\nartificial intelligence. The underlying mechanisms involve an intricate\ninteraction between the theoretical Sharpe ratio and estimation accuracy. In\nhigh-dimensional models, the theoretical Sharpe ratio approaches its upper\nlimit, and the overfitting problem is reduced because there are more parameters\nthan data restrictions, which allows us to choose well-behaved parameters based\non inductive bias.",
      "generated_abstract": "We develop a model of investment in the presence of an uncertainty about the\ndividend policy. We show that in this case the investor is able to gain a\npositive expected return by investing in the stocks that are more likely to\nreceive a higher dividend. This result depends on the fact that the risk of\nmissing the dividend is lower in the stocks that are less likely to receive a\nhigher dividend. This lower risk is due to the fact that the probability of\nreceiving a higher dividend is higher in the stocks that are more likely to\nreceive a higher dividend.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16853932584269662,
          "p": 0.32608695652173914,
          "f": 0.22222221772949255
        },
        "rouge-2": {
          "r": 0.043859649122807015,
          "p": 0.0684931506849315,
          "f": 0.05347593106923317
        },
        "rouge-l": {
          "r": 0.1348314606741573,
          "p": 0.2608695652173913,
          "f": 0.17777777328504812
        }
      }
    },
    {
      "paper_id": "math-ph.math/MP/2503.09421v1",
      "true_abstract": "We study coined Random Quantum Walks on the hexagonal lattice, where the\nstrength of disorder is monitored by the coin matrix. Each lattice site is\nequipped with an i.i.d. random variable that is uniformly distributed on the\ntorus and acts as a random phase in every step of the QW. We show dynamical\nlocalization in the regime of strong disorder, that is whenever the coin matrix\nis sufficiently close to the fully localized case, using a fractional moment\ncriterion and a finite volume method. Moreover, we adapt a topological index to\nour model and thereby obtain transport for some coin matrices.",
      "generated_abstract": "In this work, we investigate the topological properties of a new family of\nthe minimal surfaces of the class of the $C^2$ surfaces. We call them\n$C^2$-minimal surfaces. They are constructed by an appropriate scaling of the\nminimal surface of the class of the $C^2$ surfaces. In particular, we obtain\n$C^2$-minimal surfaces with boundary. The new surface is non-smooth and has\narbitrary genus. We show that the surface has a minimal surface of the class of\nthe $C^2$ surfaces. Moreover, we prove that the minimal surface is a\n$C^2$-minimal surface.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.3488372093023256,
          "f": 0.2542372835033037
        },
        "rouge-2": {
          "r": 0.030612244897959183,
          "p": 0.045454545454545456,
          "f": 0.036585361044022054
        },
        "rouge-l": {
          "r": 0.18666666666666668,
          "p": 0.32558139534883723,
          "f": 0.23728813096093085
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.07897v1",
      "true_abstract": "This work introduces a new markovian stochastic model that can be described\nas a non-homogeneous Pure Birth process. We propose a functional form of birth\nrate that depends on the number of individuals in the population and on the\nelapsed time, allowing us to model a contagion effect. Thus, we model the early\nstages of an epidemic. The number of individuals then becomes the infectious\ncases and the birth rate becomes the incidence rate. We obtain this way a\nprocess that depends on two competitive phenomena, infection and immunization.\nVariations in those rates allow us to monitor how effective the actions taken\nby government and health organizations are. From our model, three useful\nindicators for the epidemic evolution over time are obtained: the immunization\nrate, the infection/immunization ratio and the mean time between infections\n(MTBI). The proposed model allows either positive or negative concavities for\nthe mean value curve, provided the infection/immunization ratio is either\ngreater or less than one. We apply this model to the present SARS-CoV-2\npandemic still in its early growth stage in Latin American countries. As it is\nshown, the model accomplishes a good fit for the real number of both positive\ncases and deaths. We analyze the evolution of the three indicators for several\ncountries and perform a comparative study between them. Important conclusions\nare obtained from this analysis.",
      "generated_abstract": "tive of this study is to explore the feasibility of using\nexisting data from the National Survey of Children's Health (NSCH) to develop\na novel approach for predicting school-aged children's asthma exacerbations\nthrough the use of artificial intelligence (AI). The NSCH is a nationally\nrepresentative survey of children aged 2-17 years that collects detailed\ninformation on asthma symptoms, medication use, and environmental factors. By\nusing existing data from the NSCH to train an artificial intelligence model,\nwe aim to develop a novel approach for predicting school-aged children's\nasthma exacerbations. We first analyzed the existing data and determined that\nthere are significant gaps in the data related to asthma symptoms, medication\nuse, and environmental factors. We then developed a new methodology for\npredicting asthma exacerbations",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15328467153284672,
          "p": 0.2876712328767123,
          "f": 0.1999999954643992
        },
        "rouge-2": {
          "r": 0.009708737864077669,
          "p": 0.020202020202020204,
          "f": 0.013114749713734332
        },
        "rouge-l": {
          "r": 0.1386861313868613,
          "p": 0.2602739726027397,
          "f": 0.18095237641678016
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/PR/2405.12479v2",
      "true_abstract": "We present a unified, market-complete model that integrates both the\nBachelier and Black-Scholes-Merton frameworks for asset pricing. The model\nallows for the study, within a unified framework, of asset pricing in a natural\nworld that experiences the possibility of negative security prices or riskless\nrates. In contrast to classical Black-Scholes-Merton, we show that option\npricing in the unified model displays a difference depending on whether the\nreplicating, self-financing portfolio uses riskless bonds or a single riskless\nbank account. We derive option price formulas and extend our analysis to the\nterm structure of interest rates by deriving the pricing of zero-coupon bonds,\nforward contracts, and futures contracts. We identify a necessary condition for\nthe unified model to support a perpetual derivative. Discrete binomial pricing\nunder the unified model is also developed. In every scenario analyzed, we show\nthat the unified model simplifies to the standard Black-Scholes-Merton pricing\nunder specific limits and provides pricing in the Bachelier model limit. We\nnote that the Bachelier limit within the unified model allows for positive\nriskless rates. The unified model prompts us to speculate on the possibility of\na mixed multiplicative and additive deflator model for risk-neutral option\npricing.",
      "generated_abstract": "aper, we study the effect of liquidity on the long-term performance\nof the S\\&P 500 index. We show that liquidity is a key factor in driving\nperformance. Specifically, we find that liquidity drives an index's long-term\nperformance. In addition, liquidity is a key factor in determining the\noptimal asset allocation strategy. We show that the optimal asset allocation\nstrategy is to hold a large portion of the S\\&P 500 index and a small portion of\nthe NASDAQ-100 index, as the former has a higher expected return and risk. We\nalso find that liquidity is a key factor in determining the optimal\nperformance-risk ratio. Finally, we provide insights into the optimal liquidity\nlevel of the index, which depends on the desired level of risk. Our findings\nprovide valuable insights into the management",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14423076923076922,
          "p": 0.234375,
          "f": 0.1785714238548754
        },
        "rouge-2": {
          "r": 0.018072289156626505,
          "p": 0.031578947368421054,
          "f": 0.022988501117130744
        },
        "rouge-l": {
          "r": 0.14423076923076922,
          "p": 0.234375,
          "f": 0.1785714238548754
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/SC/2408.05119v1",
      "true_abstract": "Stress generation by the actin cytoskeleton shapes cells and tissues. Despite\nimpressive progress in live imaging and quantitative physical descriptions of\ncytoskeletal network dynamics, the connection between processes at molecular\nscales and cell-scale spatio-temporal patterns is still unclear. Here we review\nstudies reporting acto-myosin clusters of micrometer size and with lifetimes of\nseveral minutes in a large number of organisms ranging from fission yeast to\nhumans. Such structures have also been found in reconstituted systems in vitro\nand in theoretical analysis of cytoskeletal dynamics. We propose that tracking\nthese clusters can serve as a simple readout for characterising living matter.\nSpatio-temporal patterns of clusters could serve as determinants of\nmorphogenetic processes that play similar roles in diverse organisms.",
      "generated_abstract": "tudy, we developed a novel deep learning approach to identify\ncoding genes in complex systems using the long read sequencing data. In our\napproach, we trained a Convolutional Neural Network (CNN) model on a\ncombination of gene expression and DNA sequence data to identify the genomic\nregions that code for proteins. The model was trained using both unbiased and\nbiased data. We validated the model using biased and unbiased data from\nhuman-derived cells. We also used the model to identify coding genes in\nsingle-cell RNA sequencing data from human embryonic stem cells. The results\nshow that the model can accurately identify coding genes in complex systems\nusing only unbiased data. Moreover, the model is able to detect coding genes in\nsingle-cell RNA sequencing data using biased data. We also validated the\npredictions using",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.22535211267605634,
          "f": 0.19631901348789954
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.17391304347826086,
          "p": 0.22535211267605634,
          "f": 0.19631901348789954
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.03503v1",
      "true_abstract": "Molecular optimization is a crucial yet complex and time-intensive process\nthat often acts as a bottleneck for drug development. Traditional methods rely\nheavily on trial and error, making multi-objective optimization both\ntime-consuming and resource-intensive. Current AI-based methods have shown\nlimited success in handling multi-objective optimization tasks, hampering their\npractical utilization. To address this challenge, we present MultiMol, a\ncollaborative large language model (LLM) system designed to guide\nmulti-objective molecular optimization. MultiMol comprises two agents,\nincluding a data-driven worker agent and a literature-guided research agent.\nThe data-driven worker agent is a large language model being fine-tuned to\nlearn how to generate optimized molecules considering multiple objectives,\nwhile the literature-guided research agent is responsible for searching\ntask-related literature to find useful prior knowledge that facilitates\nidentifying the most promising optimized candidates. In evaluations across six\nmulti-objective optimization tasks, MultiMol significantly outperforms existing\nmethods, achieving a 82.30% success rate, in sharp contrast to the 27.50%\nsuccess rate of current strongest methods. To further validate its practical\nimpact, we tested MultiMol on two real-world challenges. First, we enhanced the\nselectivity of Xanthine Amine Congener (XAC), a promiscuous ligand that binds\nboth A1R and A2AR, successfully biasing it towards A1R. Second, we improved the\nbioavailability of Saquinavir, an HIV-1 protease inhibitor with known\nbioavailability limitations. Overall, these results indicate that MultiMol\nrepresents a highly promising approach for multi-objective molecular\noptimization, holding great potential to accelerate the drug development\nprocess and contribute to the advancement of pharmaceutical research.",
      "generated_abstract": "rial DNA (mtDNA) is the major genetic material in most cells,\ncontaining approximately 20,000 genes. The genetic code for mtDNA is different\nfrom the genetic code for the rest of the cell, and a number of genetic\ndisorders are caused by mutations in mtDNA. However, the mtDNA genome is\nprotected from mutations by the mtDNA replication checkpoint, which is\nactivated in the presence of damaged or unrepaired mutations. This checkpoint\nrelies on a number of checkpoint proteins, including the checkpoint kinases\n(CKs), which are involved in the activation of the checkpoint. The mtDNA\nreplication checkpoint is activated in the absence of damaged or unrepaired\nmtDNA, but it is not activated in the presence of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07272727272727272,
          "p": 0.20689655172413793,
          "f": 0.10762331453678953
        },
        "rouge-2": {
          "r": 0.004366812227074236,
          "p": 0.010638297872340425,
          "f": 0.006191946337837867
        },
        "rouge-l": {
          "r": 0.06666666666666667,
          "p": 0.1896551724137931,
          "f": 0.0986547046713187
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.astro-ph/IM/2503.08854v1",
      "true_abstract": "Modern astronomical surveys detect asteroids by linking together their\nappearances across multiple images taken over time. This approach faces\nlimitations in detecting faint asteroids and handling the computational\ncomplexity of trajectory linking. We present a novel method that adapts\n``digital tracking\" - traditionally used for short-term linear asteroid motion\nacross images - to work with large-scale synoptic surveys such as the Vera\nRubin Observatory Legacy Survey of Space and Time (Rubin/LSST). Our approach\ncombines hundreds of sparse observations of individual asteroids across their\nnon-linear orbital paths to enhance detection sensitivity by several\nmagnitudes. To address the computational challenges of processing massive data\nsets and dense orbital phase spaces, we developed a specialized\nhigh-performance computing architecture. We demonstrate the effectiveness of\nour method through experiments that take advantage of the extensive\ncomputational resources at Lawrence Livermore National Laboratory. This work\nenables the detection of significantly fainter asteroids in existing and future\nsurvey data, potentially increasing the observable asteroid population by\norders of magnitude across different orbital families, from near-Earth objects\n(NEOs) to Kuiper belt objects (KBOs).",
      "generated_abstract": "t of next-generation telescopes such as the James Webb Space\nTelescope (JWST) promises to revolutionize our understanding of the cosmos.\nHowever, to fully exploit the capabilities of JWST, it is crucial to understand\nits impact on the detectability of exoplanets. This work focuses on the\nimpact of JWST's coronagraphic capabilities on the sensitivity of the\ncoronagraph-based adaptive optics (CAO) system. We use the DAODEL framework to\ncalculate the impact of JWST's coronagraphic capabilities on the sensitivity of\nthe CAO system. We consider a 100-m JWST coronagraph with four imaging\napertures and two imaging modes. Our results show that JWST's coronagraphic\ncapabilities will significantly impact the sensitivity of the CAO system",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1328125,
          "p": 0.2537313432835821,
          "f": 0.17435896984825786
        },
        "rouge-2": {
          "r": 0.023121387283236993,
          "p": 0.045454545454545456,
          "f": 0.0306513365264756
        },
        "rouge-l": {
          "r": 0.1328125,
          "p": 0.2537313432835821,
          "f": 0.17435896984825786
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2407.00332v1",
      "true_abstract": "With emerging prevalence beyond traditionally endemic regions, the global\nburden of dengue disease is forecasted to be one of the fastest growing. With\nlimited direct treatment or vaccination currently available, prevention through\nvector control is widely believed to be the most effective form of managing\noutbreaks. This study examines traditional state space models (moving average,\nautoregressive, ARIMA, SARIMA), supervised learning techniques (XGBoost, SVM,\nKNN) and deep networks (LSTM, CNN, ConvLSTM) for forecasting weekly dengue\ncases in Singapore. Meteorological data and search engine trends were included\nas features for ML techniques. Forecasts using CNNs yielded lowest RMSE in\nweekly cases in 2019.",
      "generated_abstract": "f computational approaches to study gene expression in complex\ngrowth environments has resulted in a vast amount of data, allowing\ninterpretation of the complex dynamics of gene expression. This paper\nexplores how a single-cell RNA sequencing (scRNA-seq) data set can be used to\nperform an analysis of gene expression in a bacterial growth environment. By\ncomparing the expression of 168 genes in two different environments (culture\nmedium and static culture) we are able to identify a subset of genes that\nsignificantly contribute to the growth of the bacteria in each environment.\nThese genes are then used to create a gene expression model that predicts the\ngrowth of the bacteria in each environment based on the gene expression\npatterns. This work highlights the power of scRNA-seq data to provide insights\ninto the complex dynamics of gene expression in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10588235294117647,
          "p": 0.11842105263157894,
          "f": 0.11180123725164946
        },
        "rouge-2": {
          "r": 0.01020408163265306,
          "p": 0.008771929824561403,
          "f": 0.009433957292633448
        },
        "rouge-l": {
          "r": 0.10588235294117647,
          "p": 0.11842105263157894,
          "f": 0.11180123725164946
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.03217v1",
      "true_abstract": "Social media usage is often cited as a potential driver behind the rising\nsuicide rates. However, distinguishing the causal effect - whether social media\nincreases the risk of suicide - from reverse causality, where individuals\nalready at higher risk of suicide are more likely to use social media, remains\na significant challenge. In this paper, we use an instrumental variable\napproach to study the quasi-exogenous geographical adoption of Twitter and its\ncausal relationship with suicide rates. Our analysis first demonstrates that\nTwitter's geographical adoption was driven by the presence of certain users at\nthe 2007 SXSW festival, which led to long-term disparities in adoption rates\nacross counties in the United States. Then, using a two-stage least squares\n(2SLS) regression and controlling for a wide range of geographic, socioeconomic\nand demographic factors, we find no significant relationship between Twitter\nadoption and suicide rates.",
      "generated_abstract": "r examines how the U.S. tax code, the tax system in place in 2017,\nand the economic environment in 2017-2021 shape the incentives for U.S.\nbusinesses to engage in tax avoidance. We use data from the U.S. tax code and\nthe Taxpayer Bill of Rights (Taxpayer Bill of Rights) to construct a tax\navoidance index that measures the incentives for U.S. businesses to engage in\ntax avoidance. Using this index, we analyze how the tax code and the\nTaxpayer Bill of Rights shape the incentives for U.S. businesses to engage in\ntax avoidance. We find that the incentives for U.S. businesses to engage in\ntax avoidance are stronger in the tax code and the Taxpayer Bill of Rights",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13861386138613863,
          "p": 0.2978723404255319,
          "f": 0.18918918485482114
        },
        "rouge-2": {
          "r": 0.007352941176470588,
          "p": 0.014925373134328358,
          "f": 0.00985221232643553
        },
        "rouge-l": {
          "r": 0.1188118811881188,
          "p": 0.2553191489361702,
          "f": 0.1621621578277941
        }
      }
    },
    {
      "paper_id": "cs.NE.cs/NE/2503.09340v1",
      "true_abstract": "The nature inspired algorithms are becoming popular due to their simplicity\nand wider applicability. In the recent past several such algorithms have been\ndeveloped. They are mainly bio-inspired, swarm based, physics based and\nsocio-inspired; however, the domain based on symbiotic relation between\ncreatures is still to be explored. A novel metaheuristic optimization algorithm\nreferred to as Fig Tree-Wasp Symbiotic Coevolutionary (FWSC) algorithm is\nproposed. It models the symbiotic coevolutionary relationship between fig trees\nand wasps. More specifically, the mating of wasps, pollinating the figs,\nsearching for new trees for pollination and wind effect drifting of wasps are\nmodeled in the algorithm. These phenomena help in balancing the two important\naspects of exploring the search space efficiently as well as exploit the\npromising regions. The algorithm is successfully tested on a variety of test\nproblems. The results are compared with existing methods and algorithms. The\nWilcoxon Signed Rank Test and Friedman Test are applied for the statistical\nvalidation of the algorithm performance. The algorithm is also further applied\nto solve the real-world engineering problems. The performance of the FWSC\nunderscored that the algorithm can be applied to wider variety of real-world\nproblems.",
      "generated_abstract": "aper, we propose a novel generative architecture for audio-text\nmixed-domain modeling. We first design a pre-trained transformer-based model\nwith a large-vocabulary and large-head design to effectively capture the\nintrinsic speech-text interplay. Then, we integrate a novel text-guided\naudio-to-text (A2T) module into the transformer-based model to further improve\nthe model's ability to capture the text-specific information. Finally, we\nintegrate the text-guided audio-to-text (A2T) module into the transformer-based\nmodel to further improve the model's ability to capture the text-specific\ninformation. The proposed model has achieved significant improvements in\nspeech-text mixing, text-to-speech generation, and text-to-speech synthesis\nacross various domains. Moreover, it has",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09016393442622951,
          "p": 0.1896551724137931,
          "f": 0.12222221785432115
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09016393442622951,
          "p": 0.1896551724137931,
          "f": 0.12222221785432115
        }
      }
    },
    {
      "paper_id": "physics.app-ph.physics/atm-clus/2502.20913v1",
      "true_abstract": "Recent research on silver nanowires prepared on DNA templates has focused on\ntwo fundamental applications: nano-scale circuits and sensors. Despite its\nbroad potential, the formation kinetics of DNA-templated silver nanowires\nremains unclear. Here, we present an experimental demonstration of the\nformation of silver nanowires with a diameter of 2.2+0.4 nm at the\nsingle-molecule level through chemical reduction. We conducted equilibrium and\nperturbation kinetic experiments to measure force spectroscopy during the\nformation of Ag+ -DNA complexes and Ag-DNA complexes, using optical tweezers\ncombined with microfluidics. The addition of AgNO3 resulted in an increase in\nforce of 5.5-7.5 pN within 2 minutes, indicating that Ag+ compacts the DNA\nstructure. In contrast, the addition of hydroquinone caused the force to\ndecrease by 4-5 pN. Morphological characterization confirmed the presence of a\ndense structure formed by silver atoms bridging the DNA strands, and revealed\nconformational differences before and after metallization. We compare our\nexperimental data with Brownian dynamics simulations using a coarse-grained\ndouble-stranded DNA (dsDNA) model that provides insights on the dependency of\nthe force on the persistence length.",
      "generated_abstract": "The present work aims to investigate the role of the nonlinear interaction\nbetween the electric field and the magnetic field in the ion-acoustic\noscillations. To this end, we employ the Lorentz-Faraday equation and adopt a\ndifferent approach compared to previous studies, where the interaction is\nassumed to be linear. The obtained results are in good agreement with the\nexperimental observations and reveal that the interaction between the electric\nand magnetic fields plays a crucial role in the ion-acoustic oscillations. We\nalso discuss the impact of the nonlinear interaction on the ion-acoustic\noscillations and show that the nonlinearity significantly affects the\nion-acoustic oscillations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11666666666666667,
          "p": 0.23333333333333334,
          "f": 0.15555555111111127
        },
        "rouge-2": {
          "r": 0.011904761904761904,
          "p": 0.023255813953488372,
          "f": 0.015748027017175308
        },
        "rouge-l": {
          "r": 0.11666666666666667,
          "p": 0.23333333333333334,
          "f": 0.15555555111111127
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.physics/hist-ph/2502.17438v1",
      "true_abstract": "Henrietta Swan Leavitt's discovery of the relationship between the period and\nluminosity (hereafter the Leavitt Law) of 25 variable stars in the Small\nMagellanic Cloud, published in 1912, revolutionized cosmology. These variables,\neventually identified as Cepheids, became the first known \"standard candles\"\nfor measuring extragalactic distances and remain the gold standard for this\ntask today. Leavitt measured light curves, periods, and minimum and maximum\nmagnitudes from painstaking visual inspection of photographic plates. Her work\npaved the way for the first precise series of distance measurements that helped\nset the scale of the Universe, and later the discovery of its expansion by\nEdwin Hubble in 1929. Here, we re-analyze Leavitt's first Period-Luminosity\nrelation using observations of the same set of stars but with modern data and\nmethods of Cepheid analysis. Using only data from Leavitt's notebooks, we\nassess the quality of her light curves, measured periods, and the slope and\nscatter of her Period-Luminosity relations. We show that modern data and\nmethods, for the same objects, reduce the scatter of the Period-Luminosity\nrelation by a factor of two. We also find a bias brightward at the short period\nend, due to the non-linearity of the plates and environmental crowding.\nOverall, Leavitt's results are in excellent agreement with contemporary\nmeasurements, reinforcing the value of Cepheids in cosmology today, a testament\nto the enduring quality of her work.",
      "generated_abstract": "y explores the impact of the 2022 Lunar Eclipse on the 1962\nconspiracy theory that the moon's gravitational pull on the Earth would cause\na \"Black Hole\" effect. This event was a key moment in the Cold War and has\nsince been a source of fascination, as the event itself is notoriously\nuncertain. We examine the historical record of the eclipse itself and the\ndebate surrounding it. We then compare the eclipse with a number of other\ncosmic events that have been a source of controversy, such as the Lunar\nOrbital Mechanics (LOM) debate, the Lunar Reconnaissance Orbiter (LRO)\nimagery, and the 2019 Lunar Eclipse. We find that the conspiracy theory has\nhistorically received less attention than it deserves, and that the eclipse",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07352941176470588,
          "p": 0.13157894736842105,
          "f": 0.09433961804200806
        },
        "rouge-2": {
          "r": 0.014563106796116505,
          "p": 0.027777777777777776,
          "f": 0.019108275741816148
        },
        "rouge-l": {
          "r": 0.07352941176470588,
          "p": 0.13157894736842105,
          "f": 0.09433961804200806
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.09096v1",
      "true_abstract": "Consider a simple algebraic valued field extension $(L/K,v)$ and denote by\n$\\mathcal O_L$ and $\\mathcal O_K$ the corresponding valuation rings. The main\ngoal of this paper is to present, under certain assumptions, a description of\n$\\mathcal O_L$ in terms of generators and relations over $\\mathcal O_K$. The\nmain tool used here are complete sequences of key polynomials. It is known that\nif the ramification index of $(L/K,v)$ is one, then every complete set gives\nrise to a set of generators of $\\mathcal O_L$ over $\\mathcal O_K$. We show that\nwe can find a sequence of key polynomials for $(L/K,v)$ which satisfies good\nproperties (called neat). Then we present explicit ``neat\" relations that\ngenerate all the relations between the corresponding generators of $\\mathcal\nO_L$ over $\\mathcal O_K$.",
      "generated_abstract": "We consider the problem of finding a $k$-regular graph that is not a\ncluster graph. The problem is known to be NP-hard for $k\\geq 3$ and to be\nNP-hard for $k=2$ if we allow $k$-regular graphs to be isolated. We show that\nthe problem is NP-hard even for $k=1$ if we allow $k$-regular graphs to be\ncoplanar. In addition, we prove that the problem is in P if we allow\n$k$-regular graphs to be non-planar.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19736842105263158,
          "p": 0.39473684210526316,
          "f": 0.2631578902923978
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.05660377358490566,
          "f": 0.03726707632884586
        },
        "rouge-l": {
          "r": 0.19736842105263158,
          "p": 0.39473684210526316,
          "f": 0.2631578902923978
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.05979v1",
      "true_abstract": "Autoregressive models (ARMs) have become the workhorse for sequence\ngeneration tasks, since many problems can be modeled as next-token prediction.\nWhile there appears to be a natural ordering for text (i.e., left-to-right),\nfor many data types, such as graphs, the canonical ordering is less obvious. To\naddress this problem, we introduce a variant of ARM that generates\nhigh-dimensional data using a probabilistic ordering that is sequentially\ninferred from data. This model incorporates a trainable probability\ndistribution, referred to as an \\emph{order-policy}, that dynamically decides\nthe autoregressive order in a state-dependent manner. To train the model, we\nintroduce a variational lower bound on the exact log-likelihood, which we\noptimize with stochastic gradient estimation. We demonstrate experimentally\nthat our method can learn meaningful autoregressive orderings in image and\ngraph generation. On the challenging domain of molecular graph generation, we\nachieve state-of-the-art results on the QM9 and ZINC250k benchmarks, evaluated\nusing the Fr\\'{e}chet ChemNet Distance (FCD).",
      "generated_abstract": "aper, we investigate the problem of detecting an adversary's\nattack by observing a single sample from a given distribution. We propose a\nvariational Bayesian method for this task. In particular, we develop a\nvariational distribution for the adversary's attack that is conditionally\nindependent of the sample and the model used to generate it. We then apply our\nmethod to the problem of detecting the presence of a poisoned sample in a\nclassification task. This is a challenging problem, as it requires distinguishing\nthe presence of a poisoned sample from the presence of a genuine sample. We\nestablish a connection between the problem of detecting the presence of a\npoisoned sample and the problem of detecting an adversary's attack, and we\npropose a variational distribution for the adversary's attack that is conditionally\nindependent of the sample and the model used to generate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19298245614035087,
          "p": 0.38596491228070173,
          "f": 0.2573099370760234
        },
        "rouge-2": {
          "r": 0.026490066225165563,
          "p": 0.044444444444444446,
          "f": 0.033195016067217135
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.3333333333333333,
          "f": 0.22222221777777784
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2410.19741v1",
      "true_abstract": "Identifying client needs to provide optimal services is crucial in tourist\ndestination management. The events held in tourist destinations may help to\nmeet those needs and thus contribute to tourist satisfaction. As with product\nmanagement, the creation of hierarchical catalogs to classify those events can\naid event management. The events that can be found on the internet are listed\nin dispersed, heterogeneous sources, which makes direct classification a\ndifficult, time-consuming task. The main aim of this work is to create a novel\nprocess for automatically classifying an eclectic variety of tourist events\nusing a hierarchical taxonomy, which can be applied to support tourist\ndestination management. Leveraging data science methods such as CRISP-DM,\nsupervised machine learning, and natural language processing techniques, the\nautomatic classification process proposed here allows the creation of a\nnormalized catalog across very different geographical regions. Therefore, we\ncan build catalogs with consistent filters, allowing users to find events\nregardless of the event categories assigned at source, if any. This is very\nvaluable for companies that offer this kind of information across multiple\nregions, such as airlines, travel agencies or hotel chains. Ultimately, this\ntool has the potential to revolutionize the way companies and end users\ninteract with tourist events information.",
      "generated_abstract": "the problem of optimal investment in a single stock market under\nstochastic price volatility and uncertainty. The uncertainty is modelled by\nfinitely many jump-diffusions, which are characterised by their mean-reversion\nand the diffusion coefficient. We formulate the problem as a linear\nreinforcement learning problem. We develop a reinforcement learning algorithm\nand show that it converges to the optimal solution. We also prove that the\nresulting reinforcement learning algorithm is asymptotically optimal, in the\nsense that the regret of the reinforcement learning algorithm is asymptotically\nzero. We also propose a Monte Carlo simulation method to compute the regret.\nFinally, we study the regret in a setting where the uncertainty is modelled by\na deterministic jumps-diffusions. We show that the regret of the reinforcement\nlearning algorithm is asymptotically zero when the number of jumps is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1037037037037037,
          "p": 0.20588235294117646,
          "f": 0.13793103002742135
        },
        "rouge-2": {
          "r": 0.005208333333333333,
          "p": 0.009523809523809525,
          "f": 0.00673400216304769
        },
        "rouge-l": {
          "r": 0.0962962962962963,
          "p": 0.19117647058823528,
          "f": 0.12807881327865286
        }
      }
    },
    {
      "paper_id": "math.CA.math/CA/2503.10190v1",
      "true_abstract": "In a famous paper published in 1904, Helge von Koch introduced the curve that\nstill serves nowadays as an iconic representation of fractal shapes. In fact,\nvon Koch's main goal was the construction of a continuous but nowhere\ndifferentiable function, very similar to the snowflake, using elementary\ngeometric procedures, and not analytical formulae. We prove that a parametrized\nfamily of functions (including and) generalizing von Koch's example enjoys a\nrich multifractal behavior, thus enriching the class of historical mathematical\nobjects having surprising regularity properties. The analysis relies on the\nstudy of the orbits of an underlying dynamical system and on the introduction\nof self-similar measures and non-trivial iterated functions systems adapted to\nthe problem.",
      "generated_abstract": "Let $G$ be a finite group acting on a set $X$ via right-invariant actions. In\nthis paper, we study the behavior of the spectral radius of the action\ncommutator $[\\cdot,\\cdot",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06896551724137931,
          "p": 0.23076923076923078,
          "f": 0.10619468672253125
        },
        "rouge-2": {
          "r": 0.00909090909090909,
          "p": 0.03571428571428571,
          "f": 0.014492750388574548
        },
        "rouge-l": {
          "r": 0.06896551724137931,
          "p": 0.23076923076923078,
          "f": 0.10619468672253125
        }
      }
    },
    {
      "paper_id": "cs.DS.econ/TH/2501.13346v1",
      "true_abstract": "We study a general class of sequential search problems for selecting multiple\ncandidates from different societal groups under \"ex-ante constraints\" aimed at\nproducing socially desirable outcomes, such as demographic parity, diversity\nquotas, or subsidies for disadvantaged groups. Starting with the canonical\nPandora's box model [Weitzman, 1978] under a single affine constraint on\nselection and inspection probabilities, we show that the optimal constrained\npolicy retains an index-based structure similar to the unconstrained case, but\nmay randomize between two dual-based adjustments that are both easy to compute\nand economically interpretable. We then extend our results to handle multiple\naffine constraints by reducing the problem to a variant of the exact\nCarath\\'eodory problem and providing a novel polynomial-time algorithm to\ngenerate an optimal randomized dual-adjusted index-based policy that satisfies\nall constraints simultaneously. Building on these insights, we consider richer\nsearch processes (e.g., search with rejection and multistage search) modeled by\njoint Markov scheduling (JMS) [Dumitriu et al., 2003; Gittins, 1979]. By\nimposing general affine and convex ex-ante constraints, we develop a\nprimal-dual algorithm that randomizes over a polynomial number of dual-based\nadjustments to the unconstrained JMS Gittins indices, yielding a near-feasible,\nnear-optimal policy. Our approach relies on the key observation that a suitable\nrelaxation of the Lagrange dual function for these constrained problems admits\nindex-based policies akin to those in the unconstrained setting. Using a\nnumerical study, we investigate the implications of imposing various\nconstraints, in particular the utilitarian loss (price of fairness), and\nwhether these constraints induce their intended societally desirable outcomes.",
      "generated_abstract": "st few years, there has been significant progress in applying\nthe theory of online learning to various problems in resource allocation. In\nthis paper, we extend the results of Chakrabarti and Chatterjee (2023) to the\nsetting of multi-agent resource allocation. We show that if the agents have\nexact valuations and are fully rational, then the multi-agent version of the\nCoverage-Consistency problem admits a polynomial-time algorithm that runs in\n$O(n\\log n + m\\log m + m\\sqrt{n})$ time. In particular, this result improves\nthe prior work of Chakrabarti and Chatterjee (2023), which required that the\nagents have approximate valuations and are rational. This result also\ngeneralizes the previous results by Chakrabarti and Chatterjee (2023) and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13218390804597702,
          "p": 0.3108108108108108,
          "f": 0.1854838667806973
        },
        "rouge-2": {
          "r": 0.024291497975708502,
          "p": 0.0594059405940594,
          "f": 0.03448275450076017
        },
        "rouge-l": {
          "r": 0.1206896551724138,
          "p": 0.28378378378378377,
          "f": 0.16935483452263278
        }
      }
    },
    {
      "paper_id": "math.RT.math/RT/2503.10461v1",
      "true_abstract": "This article studies the compatibility of Koenig's notion of an exact Borel\nsubalgebra of a quasi-hereditary or, more generally, standardly stratified\nalgebra with taking idempotent subalgebras or quotients. As an application, we\nprovide bounds for the multiplicities of indecomposable projectives in the\nprincipal blocks of BGG category $\\mathcal{O}$ having basic regular exact Borel\nsubalgebras.",
      "generated_abstract": "We consider the problem of computing the $\\ell_1$ norm of a polynomial\ndifference operator of degree $d\\ge 3$ on $\\mathbb{R}^d$ with respect to a\nsuitable norm. We establish a connection between the $\\ell_1$ norm of the\npolynomial difference operator and the norm of the associated Fourier\ntransform. In particular, we obtain an upper bound on the $\\ell_1$ norm of the\npolynomial difference operator for which the associated Fourier transform is\nnon-negative. This result extends a result of Zhang and Wang from the\nmathematics of probability, which has been previously used in the literature on\ncomputing the $\\ell_1$ norm of difference operators on $\\mathbb{R}^d$ with\nrespect to a suitable norm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20454545454545456,
          "p": 0.16666666666666666,
          "f": 0.18367346443981686
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.025,
          "f": 0.030303025528008103
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.14814814814814814,
          "f": 0.16326530117451077
        }
      }
    },
    {
      "paper_id": "math.OC.eess/SY/2503.03357v1",
      "true_abstract": "Given a max-plus linear system and a semimodule, the problem of computing the\nmaximal controlled invariant subsemimodule is still open to this day. In this\npaper, we consider this problem for the specific class of fully actuated\nsystems and constraints in the form of precedence semimodules. The assumption\nof full actuation corresponds to the existence of an input for each component\nof the system state. A precedence semimodule is the set of solutions of\ninequalities typically used to represent time-window constraints. We prove\nthat, in this setting, it is possible to (i) compute the maximal controlled\ninvariant subsemimodule and (ii) decide the convergence of a fixed-point\nalgorithm introduced by R.D. Katz in strongly polynomial time.",
      "generated_abstract": "ence of decentralized energy systems (DES) has revolutionized\nthe electricity market, but their robustness and resilience are still\nunder-studied. This paper proposes a resilient decentralized energy system\n(RDES) model and analyzes its robustness, resilience, and stability under\nvarious disturbances. First, we formulate the RDES as a mixed-integer\nprogramming (MIP) model, which is then transformed into an optimization\nproblem, and further approximated into a quadratic program (QP) model, which\nis solved by using the QP-QP method. Then, we introduce a resilient RDES\nstrategy, which involves two key steps: (1) a resilient control strategy for\nthe central controller and (2) an adaptive decentralized controller (ADC) for\neach decentralized unit. Next, we",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16883116883116883,
          "p": 0.16049382716049382,
          "f": 0.16455695702852122
        },
        "rouge-2": {
          "r": 0.018018018018018018,
          "p": 0.019417475728155338,
          "f": 0.018691583792035567
        },
        "rouge-l": {
          "r": 0.16883116883116883,
          "p": 0.16049382716049382,
          "f": 0.16455695702852122
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/PL/2503.05924v1",
      "true_abstract": "Techniques that rigorously bound the overall rounding error exhibited by a\nnumerical program are of significant interest for communities developing\nnumerical software. However, there are few available tools today that can be\nused to rigorously bound errors in programs that employ conditional statements\n(a basic need) as well as mixed-precision arithmetic (a direction of\nsignificant future interest) employing global optimization in error analysis.\nIn this paper, we present a new tool that fills this void while also employing\nan abstraction-guided optimization approach to allow designers to trade\nerror-bound tightness for gains in analysis time -- useful when searching for\ndesign alternatives. We first present the basic rigorous analysis framework of\nSatire and then show how to extend it to incorporate abstractions,\nconditionals, and mixed-precision arithmetic. We begin by describing Satire's\ndesign and its performance on a collection of benchmark examples. We then\ndescribe these aspects of Satire: (1) how the error-bound and tool execution\ntime vary with the abstraction level; (2) the additional machinery to handle\nconditional expression branches, including defining the concepts of instability\njumps and instability window widths and measuring these quantities; and (3) how\nthe error changes when a mix of precision values are used. To showcase how\n\\satire can add value during design, we start with a Conjugate Gradient solver\nand demonstrate how its step size and search direction are affected by\ndifferent precision settings. Satire is freely available for evaluation, and\ncan be used during the design of numerical routines to effect design tradeoffs\nguided by rigorous empirical error guarantees.",
      "generated_abstract": "t a novel framework for the verification of concurrent\nprograms with time-dependent state transitions. The framework is based on\npreviously introduced event-based models, such as the Event-B model, and it\nenables the verification of concurrent programs with time-dependent\nstate-transitions. Our framework is based on the Event-B model, which is a\nmodel-theoretic approach to verifying concurrent programs with time-dependent\nstate transitions. The Event-B model provides a formal framework for verifying\nconcurrent programs with time-dependent state transitions. This framework is\nbased on the specification of concurrent programs with a set of event types,\nsuch as \"write\", \"read\", \"modify\", \"exit\", \"error\", and \"wait\". These event types\nare defined with respect to a program point. A program point is a point in\ntime when the program is executed. The Event-B model is based on",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12025316455696203,
          "p": 0.3064516129032258,
          "f": 0.17272726867933894
        },
        "rouge-2": {
          "r": 0.008064516129032258,
          "p": 0.02197802197802198,
          "f": 0.011799406101932197
        },
        "rouge-l": {
          "r": 0.12025316455696203,
          "p": 0.3064516129032258,
          "f": 0.17272726867933894
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2412.09631v1",
      "true_abstract": "Limit order book (LOB) is a dynamic, event-driven system that records\nreal-time market demand and supply for a financial asset in a stream flow.\nEvent stream prediction in LOB refers to forecasting both the timing and the\ntype of events. The challenge lies in modeling the time-event distribution to\ncapture the interdependence between time and event type, which has\ntraditionally relied on stochastic point processes. However, modeling complex\nmarket dynamics using stochastic processes, e.g., Hawke stochastic process, can\nbe simplistic and struggle to capture the evolution of market dynamics. In this\nstudy, we present LOBDIF (LOB event stream prediction with diffusion model),\nwhich offers a new paradigm for event stream prediction within the LOB system.\nLOBDIF learns the complex time-event distribution by leveraging a diffusion\nmodel, which decomposes the time-event distribution into sequential steps, with\neach step represented by a Gaussian distribution. Additionally, we propose a\ndenoising network and a skip-step sampling strategy. The former facilitates\neffective learning of time-event interdependence, while the latter accelerates\nthe sampling process during inference. By introducing a diffusion model, our\napproach breaks away from traditional modeling paradigms, offering novel\ninsights and providing an effective and efficient solution for learning the\ntime-event distribution in order streams within the LOB system. Extensive\nexperiments using real-world data from the limit order books of three widely\ntraded assets confirm that LOBDIF significantly outperforms current\nstate-of-the-art methods.",
      "generated_abstract": "igate the practical implications of using the LSTM-based deep\nlearning model for the financial time series forecasting task, which is\nsignificantly more challenging than the traditional time series forecasting\nproblems. We propose a novel method for forecasting the daily return series of\nthe Dow Jones Industrial Average (DJIA), using the LSTM-based model. The\nproposed method is based on the idea of using the LSTM-based model to simulate\nthe next time series of the target time series. The simulation method is\ndeveloped to ensure that the model predictions are close to the actual time\nseries data. The proposed method is compared to the traditional time series\nforecasting methods, and the results show that the proposed method has a\nsignificantly higher forecasting accuracy. The results also show that the\nproposed method has a lower variance, which means that it is more robust to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1506849315068493,
          "p": 0.3235294117647059,
          "f": 0.20560747229976425
        },
        "rouge-2": {
          "r": 0.009389671361502348,
          "p": 0.01818181818181818,
          "f": 0.012383896437233845
        },
        "rouge-l": {
          "r": 0.136986301369863,
          "p": 0.29411764705882354,
          "f": 0.18691588351471755
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.17503v1",
      "true_abstract": "When does a Sender, in a Sender-Receiver game, strictly value commitment? In\na setting with finite actions and finite states, we establish that,\ngenerically, Sender values commitment if and only if he values randomization.\nIn other words, commitment has no value if and only if a partitional experiment\nis optimal under commitment. Moreover, if Sender's preferred cheap-talk\nequilibrium necessarily involves randomization, then Sender values commitment.\nWe also ask: how often (i.e., for what share of preference profiles) does\ncommitment have no value? For any prior, any independent, atomless distribution\nof preferences, and any state space: if there are $\\left|A\\right|$ actions, the\nlikelihood that commitment has no value is at least\n$\\frac{1}{\\left|A\\right|^{\\left|A\\right|}}$. As the number of states grows\nlarge, this likelihood converges precisely to\n$\\frac{1}{\\left|A\\right|^{\\left|A\\right|}}$.",
      "generated_abstract": "e the conditions for the existence of a Nash equilibrium in a\ngeneral class of non-cooperative games. We show that the existence of a Nash\nequilibrium in such a game is equivalent to the existence of a solution to the\nfollowing linear optimization problem:\n  \\begin{equation*}\n  \\begin{aligned}\n  \\min_{x\\in \\mathbb{R}^n} \\quad & f(x) \\\\\n  \\text{s.t.}\\quad & g(x) \\leq 0\n  \\end{aligned}\n  \\end{equation*}\n  where $f$ and $g$ are two convex functions. We characterize the\n  conditions under which the linear program above has a solution. We then\n  propose an algorithm to solve the linear program and prove that the\n  solution is unique under mild conditions. We further apply our method to\n  study the existence of a Nash",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17777777777777778,
          "p": 0.22857142857142856,
          "f": 0.19999999507812513
        },
        "rouge-2": {
          "r": 0.008547008547008548,
          "p": 0.010638297872340425,
          "f": 0.009478668045194732
        },
        "rouge-l": {
          "r": 0.17777777777777778,
          "p": 0.22857142857142856,
          "f": 0.19999999507812513
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/LG/2503.10633v1",
      "true_abstract": "As there are now millions of publicly available neural networks, searching\nand analyzing large model repositories becomes increasingly important.\nNavigating so many models requires an atlas, but as most models are poorly\ndocumented charting such an atlas is challenging. To explore the hidden\npotential of model repositories, we chart a preliminary atlas representing the\ndocumented fraction of Hugging Face. It provides stunning visualizations of the\nmodel landscape and evolution. We demonstrate several applications of this\natlas including predicting model attributes (e.g., accuracy), and analyzing\ntrends in computer vision models. However, as the current atlas remains\nincomplete, we propose a method for charting undocumented regions.\nSpecifically, we identify high-confidence structural priors based on dominant\nreal-world model training practices. Leveraging these priors, our approach\nenables accurate mapping of previously undocumented areas of the atlas. We\npublicly release our datasets, code, and interactive atlas.",
      "generated_abstract": "The design of optimal quantum-in-silicon architectures is a key challenge in\nthe field of quantum computing. In this work, we propose a novel approach for\nthe design of optimal quantum-in-silicon architectures, based on the\ninterplay between the quantum and classical phases of the system. The proposed\nframework includes a quantum phase space analysis of the system, where the\nquantum and classical phases are identified and mapped to the Hilbert space.\nThe classical phases of the system are then mapped to a suitable Hilbert space,\nand the quantum phases are mapped to a suitable Hilbert space through a\ntransformation. The resulting Hilbert spaces are then used to compute the\nquantum-in-silicon architectures. We validate the proposed framework through a\nseries of simulations and comparisons with the results obtained using the\ntraditional quantum-in-silicon approach.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14150943396226415,
          "p": 0.234375,
          "f": 0.1764705835404846
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.03773584905660377,
          "f": 0.0325203202987647
        },
        "rouge-l": {
          "r": 0.14150943396226415,
          "p": 0.234375,
          "f": 0.1764705835404846
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.00772v1",
      "true_abstract": "With the rapid advancement of information technology and data collection\nsystems, large-scale spatial panel data presents new methodological and\ncomputational challenges. This paper introduces a dynamic spatial panel\nquantile model that incorporates unobserved heterogeneity. The proposed model\ncaptures the dynamic structure of panel data, high-dimensional cross-sectional\ndependence, and allows for heterogeneous regression coefficients. To estimate\nthe model, we propose a novel Bayesian Markov Chain Monte Carlo (MCMC)\nalgorithm. Contributions to Bayesian computation include the development of\nquantile randomization, a new Gibbs sampler for structural parameters, and\nstabilization of the tail behavior of the inverse Gaussian random generator. We\nestablish Bayesian consistency for the proposed estimation method as both the\ntime and cross-sectional dimensions of the panel approach infinity. Monte Carlo\nsimulations demonstrate the effectiveness of the method. Finally, we illustrate\nthe applicability of the approach through a case study on the quantile\nco-movement structure of the gasoline market.",
      "generated_abstract": "This paper introduces a novel method for constructing the conditional\nheteroskedasticity (CH) parameter of the variance of the maximum likelihood\nestimator for a semiparametric dynamic model. The new method is based on the\ngeneralized Kullback-Leibler (GKL) divergence and is illustrated through\nsimulations. The proposed approach is applied to semiparametric dynamic models\nwith state-dependent fixed effects and heteroskedasticity. We show that the\nproposed estimator is consistent and asymptotically normal when the conditional\nheteroskedasticity is constant.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2268041237113402,
          "p": 0.43137254901960786,
          "f": 0.29729729278031414
        },
        "rouge-2": {
          "r": 0.05755395683453238,
          "p": 0.11594202898550725,
          "f": 0.07692307248936787
        },
        "rouge-l": {
          "r": 0.20618556701030927,
          "p": 0.39215686274509803,
          "f": 0.2702702657532871
        }
      }
    },
    {
      "paper_id": "cs.DC.cs/DC/2503.09917v1",
      "true_abstract": "MareNostrum5 is a pre-exascale supercomputer at the Barcelona Supercomputing\nCenter (BSC), part of the EuroHPC Joint Undertaking. With a peak performance of\n314 petaflops, MareNostrum5 features a hybrid architecture comprising Intel\nSapphire Rapids CPUs, NVIDIA Hopper GPUs, and DDR5 and high-bandwidth memory\n(HBM), organized into four partitions optimized for diverse workloads. This\ndocument evaluates MareNostrum5 through micro-benchmarks (floating-point\nperformance, memory bandwidth, interconnect throughput), HPC benchmarks (HPL\nand HPCG), and application studies using Alya, OpenFOAM, and IFS. It highlights\nMareNostrum5's scalability, efficiency, and energy performance, utilizing the\nEAR (Energy Aware Runtime) framework to assess power consumption and the\neffects of direct liquid cooling. Additionally, HBM and DDR5 configurations are\ncompared to examine memory performance trade-offs. Designed to complement\nstandard technical documentation, this study provides insights to guide both\nnew and experienced users in optimizing their workloads and maximizing\nMareNostrum5's computational capabilities.",
      "generated_abstract": "As the number of IoT devices increases, the cost of data storage\nfor cloud-based edge computing services becomes a significant barrier to\ndeployment. This study investigates the storage efficiency of cloud-based\nedge computing services using the SAP HANA Edge database. The study explores\nthe impact of hardware configuration, storage capacity, and data size on\nservice performance. In addition, the study examines the impact of storage\nefficiency metrics on service performance, such as latency, throughput, and\nthroughput rate. The results show that storage capacity is the most significant\nfactor in determining service performance, followed by data size and storage\nefficiency metrics. Additionally, the study identifies the optimal hardware\nconfiguration for SAP HANA Edge based on service performance. These findings\nprovide valuable insights into the storage requirements of edge computing\nservices, helping organizations choose the appropriate hardware configuration\nand storage capacity to optimize performance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.19753086419753085,
          "f": 0.16580310393728706
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.1728395061728395,
          "f": 0.1450777153362508
        }
      }
    },
    {
      "paper_id": "nlin.PS.q-bio/CB/2409.00623v1",
      "true_abstract": "For a cell-bulk ODE-PDE model in $\\mathbb{R}^2$, a hybrid\nasymptotic-numerical theory is developed to provide a new theoretical and\ncomputationally efficient approach for studying how oscillatory dynamics\nassociated with spatially segregated dynamically active ``units\" or ``cells\"\nare regulated by a PDE bulk diffusion field that is both produced and absorbed\nby the entire cell population. The study of oscillator synchronization in a PDE\ndiffusion field was one of the initial aims of Yoshiki Kuramoto's foundational\nwork. For this cell-bulk model, strong localized perturbation theory, as\nextended to a time-dependent setting, is used to derive a new\nintegro-differential ODE system that characterizes intracellular dynamics in a\nmemory-dependent bulk-diffusion field. For this nonlocal reduced system, a\nnovel fast time-marching scheme, relying in part on the\n\\emph{sum-of-exponentials method} to numerically treat convolution integrals,\nis developed to rapidly and accurately compute numerical solutions to the\nintegro-differential system over long time intervals. For the special case of\nSel'kov reaction kinetics, a wide variety of large-scale oscillatory dynamical\nbehavior including phase synchronization, mixed-mode oscillations, and\nquorum-sensing are illustrated for various ranges of the influx and efflux\npermeability parameters, the bulk degradation rate and bulk diffusivity, and\nthe specific spatial configuration of cells. Results from our fast algorithm,\nobtained in under one minute of CPU time on a laptop, are benchmarked against\nPDE simulations of the cell-bulk model, which are performed with a commercial\nPDE solver, that have run-times that are orders of magnitude larger.",
      "generated_abstract": "e a theoretical framework to understand the dynamics of the\nbiological community in the presence of spatial heterogeneity. We introduce a\nspatially extended version of the Lotka-Volterra system, which includes an\narbitrary number of species, which can exhibit non-monotonic growth rates. The\nmodel is characterized by a single parameter that characterizes the level of\ndiversity, or species richness. We show that the population dynamics in this\nmodel can be described by a generalized Lotka-Volterra system with a single\nparameter. We then use the theory of generalized Lotka-Volterra systems to\nconstruct exact solutions that capture the most important features of the\ndynamics. These solutions show that, in general, the system undergoes two\ndistinct regimes: a regime of stable equilibrium, and a regime of unstable\nequilibrium. We also derive a mathematical expression for the size of the\npopulation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17834394904458598,
          "p": 0.32941176470588235,
          "f": 0.2314049541202787
        },
        "rouge-2": {
          "r": 0.02643171806167401,
          "p": 0.04838709677419355,
          "f": 0.034188029618591374
        },
        "rouge-l": {
          "r": 0.15286624203821655,
          "p": 0.2823529411764706,
          "f": 0.19834710288060936
        }
      }
    },
    {
      "paper_id": "math-ph.math-ph/2503.09827v1",
      "true_abstract": "This paper defines coherent manifolds and discusses their properties and\ntheir application in quantum mechanics. Every coherent manifold with a large\ngroup of symmetries gives rise to a Hilbert space, the completed quantum space\nof $Z$, which contains a distinguished family of coherent states labeled by the\npoints of the manifold.\n  The second quantization map in quantum field theory is generalized to\nquantization operators on arbitrary coherent manifolds. It is shown how the\nSchr\\\"odinger equation on any such completed quantum space can be solved in\nterms of computations only involving the coherent product. In particular, this\napplies to a description of Bosonic Fock spaces as completed quantum spaces of\na class of coherent manifolds called Klauder spaces.",
      "generated_abstract": "We provide an explicit construction of a Hilbert space of mixed states for\nthe general quantum Many-Body System. Our construction uses a direct\ncalculation of the entanglement entropy of a mixed state, which is a general\nquantization of the entanglement entropy of pure states. This construction\nallows us to define a Hilbert space of mixed states for general quantum\nmany-body systems, and to show that the Hilbert space of mixed states is\nisomorphic to the subspace of mixed states of the Hilbert space of pure\nstates. Furthermore, we derive a general formula for the entanglement entropy\nof mixed states of the general quantum Many-Body System.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17105263157894737,
          "p": 0.29545454545454547,
          "f": 0.21666666202222234
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.05970149253731343,
          "f": 0.04571428098873518
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.2727272727272727,
          "f": 0.19999999535555565
        }
      }
    },
    {
      "paper_id": "math.CO.cs/DM/2503.09525v1",
      "true_abstract": "The complexity of continuous piecewise affine (CPA) functions can be measured\nby the number of pieces $p$ or the number of distinct affine functions $n$. For\nCPA functions on $\\mathbb{R}^d$, this paper shows an upper bound of\n$p=O(n^{d+1})$ and constructs a family of functions achieving a lower bound of\n$p=\\Omega(n^{d+1-\\frac{c}{\\sqrt{\\log_2(n)}}})$.",
      "generated_abstract": "We present an alternative proof of the classical Fubini-Study metric on\nthe sphere, focusing on the role of the Riemannian metric on the tangent\nspace. Our proof is based on the idea of using a metric on the tangent space\nthat is naturally induced by the natural metric on the sphere. We also\nintroduce a natural notion of \"conformal\" metrics on the sphere, which are\ninterpreted as metrics on the tangent space induced by a conformal\ntransformation. We show that such conformal metrics are related to the\nFubini-Study metric via the conformal transformation, and we explain how to\nderive these relations from the conformal transformation. We also discuss\ngeometric properties of conformal metrics on the sphere, such as their\nsymmetries, and we provide an explicit formula for the conformal factor in\nterms of the Fubini-Study metric.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1891891891891892,
          "p": 0.10606060606060606,
          "f": 0.13592232549344913
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.009708737864077669,
          "f": 0.013422814523671462
        },
        "rouge-l": {
          "r": 0.1891891891891892,
          "p": 0.10606060606060606,
          "f": 0.13592232549344913
        }
      }
    },
    {
      "paper_id": "nlin.SI.nlin/SI/2503.06013v1",
      "true_abstract": "Hirota's discrete KdV (dKdV) equation is an integrable autonomous partial\ndifference equation on $\\mathbb{Z}^2$ that reduces to the Korteweg-de Vries\n(KdV) equation in a continuum limit. In this paper, we introduce a new\nnon-autonomous version of the dKdV equation. Furthermore, we show that the new\nequation is integrable and admits discrete Painlev\\'e transcendent solutions\ndescribed by $q$-Painlev\\'e equations of $A_J^{(1)}$-surface types\n($J=3,4,5,6$).",
      "generated_abstract": "nt work deals with the evolution of the density of a turbulent\nfocusing nonlinear Schr\\\"odinger (NLS) system. The system is described by the\ndifference equation\n\\begin{equation}\n\\label{eq:eqdif}\n\\frac{\\partial \\psi}{\\partial t} = \\frac{1}{2} \\nabla^2 \\psi - \\frac{\\alpha}{2}\n\\psi^3 + \\beta \\psi^2,\n\\end{equation}\nwhere $0 \\leqslant \\alpha \\leqslant 1$ and $\\beta \\geqslant 0$ are\nparameter constants. The system is nonlinear and non-integrable, and its\nevolution is governed by the non-autonomous difference equation\n\\eqref{eq:eqdif}. The system is governed by the autonomous difference\nequation\n\\begin{equation}\n\\label{eq",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22916666666666666,
          "p": 0.19642857142857142,
          "f": 0.21153845656804746
        },
        "rouge-2": {
          "r": 0.05,
          "p": 0.04285714285714286,
          "f": 0.04615384118343249
        },
        "rouge-l": {
          "r": 0.20833333333333334,
          "p": 0.17857142857142858,
          "f": 0.19230768733727824
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.08231v1",
      "true_abstract": "We discuss necessary conditions for a PAC-Bayes bound to provide a meaningful\ngeneralisation guarantee. Our analysis reveals that the optimal generalisation\nguarantee depends solely on the distribution of the risk induced by the prior\ndistribution. In particular, achieving a target generalisation level is only\nachievable if the prior places sufficient mass on high-performing predictors.\nWe relate these requirements to the prevalent practice of using data-dependent\npriors in deep learning PAC-Bayes applications, and discuss the implications\nfor the claim that PAC-Bayes ``explains'' generalisation.",
      "generated_abstract": "In this paper, we introduce the concept of \\emph{semi-Markovianity}, a\ngeneralization of Markovianity, and derive a new class of regularity\nconditions for a class of semi-Markovian processes. We then derive a\ncorresponding characterization of Markovian processes. As an application, we\nprove the local Markov property for the Markovian process introduced in\n\\cite{BKM2025}.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15517241379310345,
          "p": 0.24324324324324326,
          "f": 0.18947367945484778
        },
        "rouge-2": {
          "r": 0.0379746835443038,
          "p": 0.061224489795918366,
          "f": 0.04687499527465868
        },
        "rouge-l": {
          "r": 0.13793103448275862,
          "p": 0.21621621621621623,
          "f": 0.16842104787590043
        }
      }
    },
    {
      "paper_id": "stat.ML.q-bio/PE/2502.04730v1",
      "true_abstract": "Learning informative representations of phylogenetic tree structures is\nessential for analyzing evolutionary relationships. Classical distance-based\nmethods have been widely used to project phylogenetic trees into Euclidean\nspace, but they are often sensitive to the choice of distance metric and may\nlack sufficient resolution. In this paper, we introduce phylogenetic\nvariational autoencoders (PhyloVAEs), an unsupervised learning framework\ndesigned for representation learning and generative modeling of tree\ntopologies. Leveraging an efficient encoding mechanism inspired by\nautoregressive tree topology generation, we develop a deep latent-variable\ngenerative model that facilitates fast, parallelized topology generation.\nPhyloVAE combines this generative model with a collaborative inference model\nbased on learnable topological features, allowing for high-resolution\nrepresentations of phylogenetic tree samples. Extensive experiments demonstrate\nPhyloVAE's robust representation learning capabilities and fast generation of\nphylogenetic tree topologies.",
      "generated_abstract": "al mechanics provides a theoretical framework for understanding\nphysical systems. The concept of an average is a key component of statistical\nmechanics, and the notion of an average has a wide range of applications in\nvarious fields, including physics, chemistry, and biology. In this paper, we\nintroduce a new concept of average, the average of averages, which provides a\nunified framework for understanding the average of averages and its various\napplications. This new concept provides a unified approach to understanding the\naverage of averages and its various applications. This approach is particularly\napplicable to biological systems, where it provides a unified approach to\nunderstanding the average of averages and its various applications. This\napproach is particularly applicable to biological systems, where it provides a\nunified approach to understanding the average of averages and its various\napplications. This approach is particularly applicable to biological systems",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14583333333333334,
          "p": 0.2545454545454545,
          "f": 0.18543045894478322
        },
        "rouge-2": {
          "r": 0.03361344537815126,
          "p": 0.05063291139240506,
          "f": 0.04040403560810178
        },
        "rouge-l": {
          "r": 0.14583333333333334,
          "p": 0.2545454545454545,
          "f": 0.18543045894478322
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.06821v1",
      "true_abstract": "The exploration of Bird's-Eye View (BEV) mapping technology has driven\nsignificant innovation in visual perception technology for autonomous driving.\nBEV mapping models need to be applied to the unlabeled real world, making the\nstudy of unsupervised domain adaptation models an essential path. However,\nresearch on unsupervised domain adaptation for BEV mapping remains limited and\ncannot perfectly accommodate all BEV mapping tasks. To address this gap, this\npaper proposes HierDAMap, a universal and holistic BEV domain adaptation\nframework with hierarchical perspective priors. Unlike existing research that\nsolely focuses on image-level learning using prior knowledge, this paper\nexplores the guiding role of perspective prior knowledge across three distinct\nlevels: global, sparse, and instance levels. With these priors, HierDA consists\nof three essential components, including Semantic-Guided Pseudo Supervision\n(SGPS), Dynamic-Aware Coherence Learning (DACL), and Cross-Domain Frustum\nMixing (CDFM). SGPS constrains the cross-domain consistency of perspective\nfeature distribution through pseudo labels generated by vision foundation\nmodels in 2D space. To mitigate feature distribution discrepancies caused by\nspatial variations, DACL employs uncertainty-aware predicted depth as an\nintermediary to derive dynamic BEV labels from perspective pseudo-labels,\nthereby constraining the coarse BEV features derived from corresponding\nperspective features. CDFM, on the other hand, leverages perspective masks of\nview frustum to mix multi-view perspective images from both domains, which\nguides cross-domain view transformation and encoding learning through mixed BEV\nlabels. The proposed method is verified on multiple BEV mapping tasks, such as\nBEV semantic segmentation, high-definition semantic, and vectorized mapping.\nThe source code will be made publicly available at\nhttps://github.com/lynn-yu/HierDAMap.",
      "generated_abstract": "vances in generative models have shown promise in generating\nhigh-quality images from few-shot training data, but the inherent low\nrepresentational capacity of these models limits their ability to capture\ncomplex visual information. In this paper, we propose a novel multi-task\ngenerative model for image generation, which simultaneously leverages the\nsemantic information from a multi-task learning task (MTask) and the spatial\ninformation from a spatially-aware task (SATask). This multi-task approach\nenables the model to simultaneously learn discriminative visual features from\nboth the semantic and spatial dimensions. Additionally, we propose a\ncross-modal fusion method to fuse the semantic and spatial information from the\nMTask and SATask, thereby enhancing the model's ability to capture both\nsemantic and spatial information. We evaluate our model on the Visual Genome\nbenchmark and the Visual Genome-Enhanced (",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12921348314606743,
          "p": 0.2875,
          "f": 0.17829456936482194
        },
        "rouge-2": {
          "r": 0.012396694214876033,
          "p": 0.026785714285714284,
          "f": 0.016949148216669367
        },
        "rouge-l": {
          "r": 0.10674157303370786,
          "p": 0.2375,
          "f": 0.14728681742683747
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.01287v1",
      "true_abstract": "Simulation-based inference (SBI) methods typically require fully observed\ndata to infer parameters of models with intractable likelihood functions.\nHowever, datasets often contain missing values due to incomplete observations,\ndata corruptions (common in astrophysics), or instrument limitations (e.g., in\nhigh-energy physics applications). In such scenarios, missing data must be\nimputed before applying any SBI method. We formalize the problem of missing\ndata in SBI and demonstrate that naive imputation methods can introduce bias in\nthe estimation of SBI posterior. We also introduce a novel amortized method\nthat addresses this issue by jointly learning the imputation model and the\ninference network within a neural posterior estimation (NPE) framework.\nExtensive empirical results on SBI benchmarks show that our approach provides\nrobust inference outcomes compared to standard baselines for varying levels of\nmissing data. Moreover, we demonstrate the merits of our imputation model on\ntwo real-world bioactivity datasets (Adrenergic and Kinase assays). Code is\navailable at https://github.com/Aalto-QuML/RISE.",
      "generated_abstract": "st decade, deep learning has emerged as a powerful tool for\nmicrobial community analysis. While many studies have examined the\ncomposition of single-cell communities, the analysis of community structure\nacross multiple samples remains limited. In this work, we present a novel\nmethod for detecting communities in multiple samples that exploits the\ninterplay between community structure and the underlying population structure.\nWe introduce the concept of \\emph{intersection communities}, which are communities\nthat share a common community structure but are not necessarily associated\nwith a common underlying population. We demonstrate that the intersection\ncommunity structure can be used to disentangle the underlying population\nstructure from the community structure. We propose a novel clustering\nalgorithm, the \\emph{intersection community algorithm}, which uses the\nintersection community structure to guide the clustering process. We validate\nthe efficiency of the algorithm on synthetic and real datasets. Finally, we\napply the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19469026548672566,
          "p": 0.25882352941176473,
          "f": 0.22222221732221215
        },
        "rouge-2": {
          "r": 0.019867549668874173,
          "p": 0.023622047244094488,
          "f": 0.02158272885021593
        },
        "rouge-l": {
          "r": 0.18584070796460178,
          "p": 0.24705882352941178,
          "f": 0.21212120722120206
        }
      }
    },
    {
      "paper_id": "cs.CG.cs/CG/2503.09115v1",
      "true_abstract": "We prove a quasi-linear upper bound on the size of $K_{t,t}$-free polygon\nvisibility graphs. For visibility graphs of star-shaped and monotone polygons\nwe show a linear bound. In the more general setting of $n$ points on a simple\nclosed curve and visibility pseudo-segments, we provide an $O(n \\log n)$ upper\nbound and an $\\Omega(n\\alpha(n))$ lower bound.",
      "generated_abstract": "t a novel approach for the problem of minimizing the sum of the\nminimum of two convex functions. We show that this problem is equivalent to\nfinding the best linear unbiased estimator of the convex function. We provide a\nsimple and efficient algorithm for computing the best linear unbiased\nestimator. We then show that the resulting estimator is asymptotically\noptimal. We provide a simple characterization of the best linear unbiased\nestimator and derive its asymptotic optimality. We also provide a simple\ncharacterization of the best linear unbiased estimator and derive its\nasymptotic optimality. We provide an efficient algorithm for computing the\nbest linear unbiased estimator. We provide an efficient algorithm for computing\nthe best linear unbiased estimator. We show that our algorithm is optimal for\nthe minimax rate of the problem. We also show that our algorithm is optimal for\nthe minim",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2564102564102564,
          "p": 0.20833333333333334,
          "f": 0.22988505252477218
        },
        "rouge-2": {
          "r": 0.03773584905660377,
          "p": 0.025974025974025976,
          "f": 0.03076922593964573
        },
        "rouge-l": {
          "r": 0.2564102564102564,
          "p": 0.20833333333333334,
          "f": 0.22988505252477218
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2502.17461v1",
      "true_abstract": "We prove that if a given reaction network $\\mathcal{N}$ has a weakly\nreversible deficiency zero realization for all choice of rate constants, then\nthere exists a $\\textit{unique}$ weakly reversible deficiency zero network\n$\\mathcal{N}'$ such that $\\mathcal{N}$ is realizable by $\\mathcal{N}'$.\nAdditionally, we propose an algorithm to find this weakly reversible deficiency\nzero network $\\mathcal{N}'$ when it exists.",
      "generated_abstract": "f artificial intelligence (AI) to advance drug discovery is\nsignificant, but many of these advancements are still in the early stages.\nThese efforts can be improved by using large language models (LLMs) to aid in\nthe discovery process. LLMs have the potential to provide more efficient and\neffective methods for drug discovery, but their use requires careful\nconsideration of potential risks. This study explores the potential risks and\nchallenges associated with using LLMs in drug discovery. By conducting a\nliterature review, this paper examines the benefits and drawbacks of using LLMs\nin drug discovery, as well as the potential risks and challenges that may arise\nfrom their use. The findings of this study highlight the need for careful\nconsideration of the potential risks and challenges associated with using LLMs\nin drug discovery. This paper provides a comprehensive",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.10126582278481013,
          "f": 0.13445377704964354
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.10126582278481013,
          "f": 0.13445377704964354
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.14708v1",
      "true_abstract": "We conduct an incentivized laboratory experiment to study people's perception\nof generative artificial intelligence (GenAI) alignment in the context of\neconomic decision-making. Using a panel of economic problems spanning the\ndomains of risk, time preference, social preference, and strategic\ninteractions, we ask human subjects to make choices for themselves and to\npredict the choices made by GenAI on behalf of a human user. We find that\npeople overestimate the degree of alignment between GenAI's choices and human\nchoices. In every problem, human subjects' average prediction about GenAI's\nchoice is substantially closer to the average human-subject choice than it is\nto the GenAI choice. At the individual level, different subjects' predictions\nabout GenAI's choice in a given problem are highly correlated with their own\nchoices in the same problem. We explore the implications of people\noverestimating GenAI alignment in a simple theoretical model.",
      "generated_abstract": "We study the optimal allocation of resources under uncertainty, where the\nresource provider is only able to observe the distribution of the outcomes\nof the allocation. We focus on the case where the distribution of the\noutcomes is a discrete random variable, and the outcomes are non-negative and\nbounded. We first characterize the optimal allocation in terms of a\nnon-negative, convex function. This function is maximized by the uniform\nallocation. We then characterize the optimal allocation in terms of a\ncontinuous, convex function. This function is maximized by the discrete\nbinning of the outcomes, where the bins are uniformly spaced. We provide a\nconcluding comment on the difference between these two optimal allocations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14444444444444443,
          "p": 0.23214285714285715,
          "f": 0.17808218705197987
        },
        "rouge-2": {
          "r": 0.007462686567164179,
          "p": 0.011764705882352941,
          "f": 0.00913241534163421
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.21428571428571427,
          "f": 0.16438355691499357
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/SC/2402.14887v4",
      "true_abstract": "Metabolic pathways are fundamental maps in biochemistry that detail how\nmolecules are transformed through various reactions. The complexity of\nmetabolic network, where a single compound can play a part in multiple\npathways, poses a challenge in inferring metabolic balance changes over time or\nafter different treatments. Isotopic labeling experiment is the standard method\nto infer metabolic flux, which is currently defined as the flow of a single\nmetabolite through a given pathway over time. However, there is still no way to\naccurately infer the metabolic balance changes after different treatments in an\nexperiment. This study introduces a different concept: molecular weight\ndistribution, which is the empirical distribution of the molecular weights of\nall metabolites of interest. By estimating the differences of the location and\nscale estimates of these distributions, it becomes possible to quantitatively\ninfer the metabolic balance changes even without requiring knowledge of the\nexact chemical structures of these compounds and their related pathways. This\nresearch article provides a mathematical framing for a classic biological\nconcept.",
      "generated_abstract": "nt study, we have proposed a novel method to estimate the\neffective number of binding sites on the surface of a protein based on\nstochastic conformational variability. This method uses the information on the\nconformational variability of the protein in the presence of a ligand to\nestimate the effective number of binding sites on the protein surface. This\nmethod is based on the assumption that the ligand-protein interaction is\ndominated by conformational variability rather than by site-specific\ninteractions. In this paper, we have revisited our previous study and extended\nthe method to incorporate the information on the binding affinity of the\nligand to the protein, thereby allowing us to estimate the effective number of\nbinding sites on the protein surface. We have also investigated the influence of\nthe ligand-protein interaction strength on the effective number of binding\nsites on the protein surface. Furthermore, we have investigated the influence",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1,
          "p": 0.18032786885245902,
          "f": 0.12865496617078775
        },
        "rouge-2": {
          "r": 0.019867549668874173,
          "p": 0.03125,
          "f": 0.024291493223623672
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.16393442622950818,
          "f": 0.11695905973803924
        }
      }
    },
    {
      "paper_id": "astro-ph.GA.astro-ph/GA/2503.09753v1",
      "true_abstract": "This thesis investigates the evolution of galaxies in diverse environments,\nutilizing Sloan Digital Sky Survey (SDSS) data to explore the impact of\nenvironmental richness on central and satellite galaxies across stellar mass\nranges, compared to isolated systems. The sample is limited to 0.03 < z < 0.1\nand apparent magnitudes brighter than 17.78, ensuring spectroscopic\ncompleteness and reliable stellar population estimates. Galaxies are\ncategorized by environment as field or cluster/group systems, with further\nseparation into satellites and centrals. By analyzing the star formation rate\n(SFR)-stellar mass plane, this work identifies systematic differences in the\nblue cloud (BC), green valley (GV), and red sequence (RS) across environments.\nMorphological and stellar population analyses reveal that T-type, metallicity,\nand stellar age transitions highlight the role of environmental quenching. A\nnewly introduced T-Type \\emph{vs.} specific SFR diagram provides evidence that\nmorphological transformation precedes full quenching. Correlating galaxy\nproperties with time since infall through projected phase space confirms the\ndelayed-then-rapid quenching model for low- and intermediate-mass galaxies,\nextending it to morphology. Time-scales for quenching and morphological\ntransitions are also derived as a function of stellar mass.",
      "generated_abstract": "aper we present a detailed study of the stellar population of\nthe globular cluster NGC 6826 using a sample of HST images taken over 10 years\n(2004-2013) and archival Spitzer data (2007-2013). NGC 6826 is one of the\nmost massive and rich globular clusters in the Local Group. It has a stellar\nmass of $1.5\\times10^8 M_\\odot$, the largest among the known globular clusters\nin the Local Group. The age of the cluster is estimated to be $\\sim 12.7$\nGyr, which is the youngest among the globular clusters in the Local Group. The\nmain cluster stellar population is dominated by O-type stars, with a small\ncontribution from B-type stars. The star-formation history of the cluster is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11188811188811189,
          "p": 0.22857142857142856,
          "f": 0.15023473737133297
        },
        "rouge-2": {
          "r": 0.016666666666666666,
          "p": 0.031578947368421054,
          "f": 0.021818177295868706
        },
        "rouge-l": {
          "r": 0.1048951048951049,
          "p": 0.21428571428571427,
          "f": 0.14084506600983063
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.00578v1",
      "true_abstract": "I propose a model of aggregation of intervals relevant to the study of legal\nstandards of tolerance. Seven axioms: responsiveness, anonymity, continuity,\nstrategyproofness, and three variants of neutrality are then used to prove\nseveral important results about a new class of aggregation methods called\nendpoint rules. The class of endpoint rules includes extreme tolerance\n(allowing anything permitted by anyone) and a form of majoritarianism (the\nmedian rule).",
      "generated_abstract": "We explore the problem of maximizing an expected utility function subject to\nconstraints on the number of agents and the time horizon. The problem is\nsolved using the notion of a threshold allocation. The threshold allocation\nprovides an efficient allocation of agents such that the expected utility of\neach agent is maximized. The threshold allocation can be implemented by\nallocating agents according to a threshold rule. We show that the threshold\nallocation is unique and that it has a closed form expression. Furthermore, we\nderive the optimality conditions and show that the threshold allocation is\noptimal if and only if the expected utility is concave in the number of agents\nand convex in the time horizon.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1568627450980392,
          "p": 0.13559322033898305,
          "f": 0.1454545404809919
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1568627450980392,
          "p": 0.13559322033898305,
          "f": 0.1454545404809919
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.09174v1",
      "true_abstract": "This paper examines the number of communication modes, that is, the degrees\nof freedom (DoF), in a wireless setup comprising a small continuous linear\nintelligent antenna array in the near field of a large one. The framework\nallows for any orientations between the arrays and any positions in a\ntwo-dimensional space assuming that the transmitting array is placed at the\norigin. Therefore, apart from the length of the two continuous arrays, four key\nparameters determine the DoF and are hence considered in the analysis: the\nCartesian coordinates of the center of the receiving array and two angles that\nmodel the rotation of each array around its center. The paper starts with the\ncalculation of the deterministic DoF for a generic geometric setting, which\nextends beyond the widely studied paraxial case. Subsequently, a stochastic\ngeometry framework is proposed to study the statistical DoF, as a first step\ntowards the investigation of the system-level performance in near field\nnetworks. Numerical results applied to millimeter wave networks reveal the\nlarge number of DoF provided by near-field communications and unveiled key\nsystem-level insights.",
      "generated_abstract": "net of Things (IoT) has brought significant improvements in\ncommunication efficiency, but it also introduces new challenges, including\nsecurity threats, latency, and energy efficiency. To address these challenges,\nwe propose a novel energy-efficient wireless IoT (EEWiT) architecture that\ncombines the advantages of time-division multiplexing (TDM) and orthogonal\nfrequency-division multiplexing (OFDM) in a two-layer structure. The first\nlayer is dedicated to energy-efficient communication while the second layer\nenables low-latency communication. The EEWiT architecture combines the\nadvantages of TDM and OFDM, and its architecture is flexible enough to\nadapt to different network scenarios. The EEWiT architecture is designed to\nachieve low energy consumption and low latency, which can improve the\nefficiency of the IoT network.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12389380530973451,
          "p": 0.18666666666666668,
          "f": 0.14893616541704408
        },
        "rouge-2": {
          "r": 0.011764705882352941,
          "p": 0.018867924528301886,
          "f": 0.014492748892041031
        },
        "rouge-l": {
          "r": 0.09734513274336283,
          "p": 0.14666666666666667,
          "f": 0.11702127180002284
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.03151v1",
      "true_abstract": "Subset selection is central to many wireless communication problems,\nincluding link scheduling, power allocation, and spectrum management. However,\nthese problems are often NP-complete, because of which heuristic algorithms\napplied to solve these problems struggle with scalability in large-scale\nsettings. To address this, we propose a determinantal point process-based\nlearning (DPPL) framework for efficiently solving general subset selection\nproblems in massive networks. The key idea is to model the optimal subset as a\nrealization of a determinantal point process (DPP), which balances the\ntrade-off between quality (signal strength) and similarity (mutual\ninterference) by enforcing negative correlation in the selection of {\\em\nsimilar} links (those that create significant mutual interference). However,\nconventional methods for constructing similarity matrices in DPP impose\ndecomposability and symmetry constraints that often do not hold in practice. To\novercome this, we introduce a new method based on the Gershgorin Circle Theorem\nfor constructing valid similarity matrices. The effectiveness of the proposed\napproach is demonstrated by applying it to two canonical wireless network\nsettings: an ad hoc network in 2D and a cellular network serving drones in 3D.\nSimulation results show that DPPL selects near-optimal subsets that maximize\nnetwork sum-rate while significantly reducing computational complexity compared\nto traditional optimization methods, demonstrating its scalability for\nlarge-scale networks.",
      "generated_abstract": "r investigates the design of a low-complexity wireless communications\nsystem with a single-antenna transmitter and multiple-input multiple-output\n(MIMO) receivers, in which the transmitter has limited power and the receivers\nare equipped with low-power sensors. The system is assumed to operate in the\nsidelobe-limited (SSL) regime and the transmitter's power is limited to a\ngiven fraction of the maximum transmit power. The receiver's sensing range is\nlimited, and the receiver's sensing resolution is also limited. The problem of\nmaximizing the system's throughput with such constraints is formulated as a\nconstrained optimization problem. A polynomial-time algorithm is designed to\nfind the optimal solution to the problem, and the obtained solution is\nverified to be optimal. Moreover, the problem is solved under the assumption\nthat the transmitter's power is not known",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1292517006802721,
          "p": 0.25333333333333335,
          "f": 0.17117116669710267
        },
        "rouge-2": {
          "r": 0.025,
          "p": 0.041666666666666664,
          "f": 0.031249995312500705
        },
        "rouge-l": {
          "r": 0.12244897959183673,
          "p": 0.24,
          "f": 0.1621621576880936
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.09560v1",
      "true_abstract": "Solving medical imaging data scarcity through semantic image generation has\nattracted significant attention in recent years. However, existing methods\nprimarily focus on generating whole-organ or large-tissue structures, showing\nlimited effectiveness for organs with fine-grained structure. Due to stringent\ntopological consistency, fragile coronary features, and complex 3D\nmorphological heterogeneity in cardiac imaging, accurately reconstructing\nfine-grained anatomical details of the heart remains a great challenge. To\naddress this problem, in this paper, we propose the Fine-grained Cardiac image\nSynthesis(FCaS) framework, established on 3D template conditional diffusion\nmodel. FCaS achieves precise cardiac structure generation using Template-guided\nConditional Diffusion Model (TCDM) through bidirectional mechanisms, which\nprovides the fine-grained topological structure information of target image\nthrough the guidance of template. Meanwhile, we design a deformable Mask\nGeneration Module (MGM) to mitigate the scarcity of high-quality and diverse\nreference mask in the generation process. Furthermore, to alleviate the\nconfusion caused by imprecise synthetic images, we propose a Confidence-aware\nAdaptive Learning (CAL) strategy to facilitate the pre-training of downstream\nsegmentation tasks. Specifically, we introduce the Skip-Sampling Variance (SSV)\nestimation to obtain confidence maps, which are subsequently employed to\nrectify the pre-training on downstream tasks. Experimental results demonstrate\nthat images generated from FCaS achieves state-of-the-art performance in\ntopological consistency and visual quality, which significantly facilitates the\ndownstream tasks as well. Code will be released in the future.",
      "generated_abstract": "This paper presents a method to estimate the 3D volume of a 3D shape,\nwithout the need for ground truth. The method is based on a modified\nthree-view-based approach, which relies on the assumption that the 3D shape is\nrepresented by a point cloud. The method consists of three key components: a\nnovel volumetric descriptor that is used to select the most representative\npoints for each 3D shape, a multi-view image selection method, and an\niterative optimization strategy. The proposed method is evaluated on both\nsynthetic and real datasets. The results demonstrate that the proposed method\ncan effectively estimate the 3D volume of the shapes. The synthetic datasets\ndemonstrate that the proposed method can achieve high-quality results without\nground truth data, while the real datasets demonstrate that the proposed method\nis able to handle complex 3D shapes with multiple views.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12025316455696203,
          "p": 0.25,
          "f": 0.16239315800715917
        },
        "rouge-2": {
          "r": 0.018779342723004695,
          "p": 0.034482758620689655,
          "f": 0.024316104857125237
        },
        "rouge-l": {
          "r": 0.12025316455696203,
          "p": 0.25,
          "f": 0.16239315800715917
        }
      }
    },
    {
      "paper_id": "math.GN.math/GN/2503.01241v1",
      "true_abstract": "This paper will discuss the problem of defining the new topological\ntransitivity. To do this several equivalent topological transitive and\nnon-wandering point has been discussed through this paper. This paper also\nconsider the ideal version of transitivity with the help of the amendment of\nthe result Remark $6.9(2)$ of \\cite{LL2013}. Corrected version of the Remark:\n``If $\\mathcal{\\bf I}$ is codense, then $\\mathcal{\\bf I}$-denseness,\n$*$-denseness and denseness are equivalent\" will be ``If $\\mathcal{\\bf I}$ is\ncompletely codense, then $\\mathcal{\\bf I}$-denseness, $*$-denseness and\ndenseness are equivalent\".",
      "generated_abstract": "We consider the problem of constructing a certain class of unital $C^*$-algebras\n$\\mathfrak{A}$ such that, for any $A,B\\in\\mathfrak{A}$, there exists an element\n$C\\in\\mathfrak{A}$ such that $A=BC$. We show that this class of algebras\nincludes the class of unital $C^*$-algebras $\\mathfrak{A}$ such that for any\n$A,B\\in\\mathfrak{A}$ the operator $BA^*$ is in $\\mathfrak{A}$. Our construction\nof such an algebra $\\mathfrak{A}$ is based on a simple $C^*$-algebraic\nconstruction. We also present a criterion for a unital $C^*$-algebra $\\mathfrak{A}$\nsuch that for any $A,B\\in\\mathfrak{A}$ the operator $BA^*$ is in $\\mathfrak{A}$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13725490196078433,
          "p": 0.1590909090909091,
          "f": 0.14736841607977857
        },
        "rouge-2": {
          "r": 0.043478260869565216,
          "p": 0.045454545454545456,
          "f": 0.04444443944691414
        },
        "rouge-l": {
          "r": 0.13725490196078433,
          "p": 0.1590909090909091,
          "f": 0.14736841607977857
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.02763v1",
      "true_abstract": "The Index of Dissimilarity (ID), widely utilized in economic literature as a\nmeasure of segregation, is inadequate for cross-country or time series studies\ndue to its failure to account for structural variations across countries' labor\nmarkets or changes over time within a single country's labor market. Building\non the works of Karmel and MacLachlan (1988) and Blackburn et al. (1993), we\npropose a new measure - the standardized ID - that isolates structural\ndifferences from true differences in segregation across space or time. A key\nadvantage of our proposed measure lies in its ease of implementation and\ninterpretation, even when working with datasets encompassing a large number of\ncountries or time periods. Moreover, our measure can be consistently applied in\nthe case of lumpy sectors or occupations that account for a large fraction of\nthe workforce. We illustrate the new measure in an analysis of the\ncross-country relationship between economic development (as measured by GDP per\ncapita) and occupational and sectoral gender segregation. Comparing the crude\nID with the standardized ID, we show that the crude ID overestimates the\npositive correlation between income and segregation, especially between low-\nand middle-income countries. This suggests that analyses relying on the crude\nID risk overestimating the importance of income differentials in explaining\ncross-country variation in gender segregation.",
      "generated_abstract": "The study presents the results of an empirical analysis of the impact of\nEgypt's economic reforms on the employment situation of the youth. The study\nemployed a panel data analysis using a sample of 1,630 individuals (18 to 30\nyears old) from the Egyptian provinces of Aswan, Qena, and Asyut. The findings\nshow that the economic reforms implemented by Egypt's government have had a\npositive impact on the employment situation of the youth, particularly those\nwith a university degree. Furthermore, the study found that the reforms have\nsignificantly improved the employment prospects of the youth in the labor\nmarket, thereby contributing to the development of the Egyptian economy and\nenhancing social cohesion and stability.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1450381679389313,
          "p": 0.27941176470588236,
          "f": 0.1909547693704705
        },
        "rouge-2": {
          "r": 0.03,
          "p": 0.06060606060606061,
          "f": 0.04013377483473388
        },
        "rouge-l": {
          "r": 0.1450381679389313,
          "p": 0.27941176470588236,
          "f": 0.1909547693704705
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.08646v1",
      "true_abstract": "We consider isotropic and Lagrangian embeddings of coadjoint orbits of\ncompact Lie groups into products of coadjoint orbits. After reviewing the known\nfacts in the case of $\\mathrm{SU}(n)$ we initiate a similar study for\n$\\mathrm{SO}$ and $\\mathrm{Sp}$ cases. In the second part we apply this to the\nstudy of dynamical systems with $\\mathrm{SU}(n)$ symmetry, proving equivalence\nbetween systems of two types: those describing magnetic geodesic flow on flag\nmanifolds and classical `spin chains' of a special type.",
      "generated_abstract": "In this article we study the topological properties of the real line with\ntopology defined by the action of a free group. We prove that the real line\nwithout the diagonal is an acyclic topological space. In particular, we obtain\nthat the real line with the diagonal is a topological space of infinite type.\nWe also study the topological properties of the real line with the diagonal. We\nshow that the real line with the diagonal has a non-zero fundamental group. We\nprove that the real line with the diagonal is not a topological space of\nfinite type.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1694915254237288,
          "p": 0.2564102564102564,
          "f": 0.20408162786130787
        },
        "rouge-2": {
          "r": 0.013513513513513514,
          "p": 0.01694915254237288,
          "f": 0.015037589048562871
        },
        "rouge-l": {
          "r": 0.1694915254237288,
          "p": 0.2564102564102564,
          "f": 0.20408162786130787
        }
      }
    },
    {
      "paper_id": "cs.ET.cs/ET/2503.09237v1",
      "true_abstract": "We attempt to take a comprehensive look at the challenges of representing the\nspatio-temporal structures and dynamic processes defining a city's overall\ncharacteristics. For the task of urban planning and urban operation, we take\nthe stance that even if the necessary representations of these structures and\nprocesses can be achieved, the most important representation of the relevant\nmindsets of the citizens are, unfortunately, mostly neglected.\n  After a review of major \"traditional\" urban models of structures behind urban\nscale, form, and dynamics, we turn to major recent modeling approaches\ntriggered by recent advances in AI that enable multi-modal generative models.\nSome of these models can create representations of geometries, networks and\nimages, and reason flexibly at a human-compatible semantic level. They provide\nhuge amounts of knowledge extracted from Terabytes of text and image documents\nand cover the required rich representation spectrum including geographic\nknowledge by different knowledge sources, degrees of granularity and scales.\n  We then discuss what these new opportunities mean for the modeling challenges\nposed by cities, in particular with regard to the role and impact of citizens\nand their interactions within the city infrastructure. We propose to integrate\nthese possibilities with existing approaches, such as agent-based models, which\nopens up new modeling spaces including rich citizen models which are able to\nalso represent social interactions.\n  Finally, we put forward some thoughts about a vision of a \"social AI in a\ncity ecosystem\" that adds relevant citizen models to state-of-the-art\nstructural and process models. This extended city representation will enable\nurban planners to establish citizen-oriented planning of city infrastructures\nfor human culture, city resilience and sustainability.",
      "generated_abstract": "guage Models (LLMs) have shown promise in many domains, including\nresearch and education. However, they have also been used for malicious\npurposes, including data harvesting and malware distribution. This paper\nexplores the ethical implications of LLM-based AI systems and offers recommendations\nfor how researchers, educators, and policy makers can better integrate AI\nsystems into their work. The paper begins with an overview of the ethical\nconsiderations that researchers and educators should consider when incorporating\nLLMs into their work, focusing on the ethical implications of LLM-based\nresearch and education initiatives. The paper then addresses the challenges\nfaced by researchers when integrating LLMs into their work, including\nethical concerns about data collection and bias in model training. The paper\nconcludes with recommendations for researchers, educ",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.225,
          "f": 0.1487603261525853
        },
        "rouge-2": {
          "r": 0.007692307692307693,
          "p": 0.018691588785046728,
          "f": 0.010899178430311391
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.225,
          "f": 0.1487603261525853
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.00994v2",
      "true_abstract": "This study evaluates the performance of Vehicle-to-Vehicle Visible Light\nCommunication in dynamic environments, focusing on the effects of speed,\nhorizontal offset, and other factors on communication reliability. Using On-Off\nKeying modulation, we analyze the BER, optimal communication distance,\ncorrelation time and the maximum amount of data per communication. Our results\ndemonstrate that maintaining an optimal vehicle distance is critical for stable\ncommunication, with speed and horizontal offset significantly influencing\ncommunication. This work extends the analysis of V-VLC to real-world dynamic\nscenarios, providing insights for future research.",
      "generated_abstract": "r presents a new hybrid framework for the simultaneous localization\nand mapping (SLAM) of an underwater robot. This framework integrates the\nGaussian process Kalman filter (GP-KF) with a particle filter (PF) to\nautomatically estimate the robot's position and state. The PF employs a\nconvolutional neural network (CNN) to extract features from the depth images\ncaptured by a multi-sensor fusion system. The GP-KF then uses these features to\nestimate the robot's state. The GP-KF is used to estimate the robot's state\nduring the sensor fusion process, while the PF is used to estimate the robot's\nposition. The effectiveness of the proposed framework is demonstrated through\nsimulation and real-world experiments. The results show that the proposed\nframework significantly outperforms existing SLAM methods in terms of\naccur",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20588235294117646,
          "p": 0.1794871794871795,
          "f": 0.19178081694126492
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.19117647058823528,
          "p": 0.16666666666666666,
          "f": 0.17808218680427862
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.econ/GN/2412.14996v1",
      "true_abstract": "In socioeconomic systems, nonequilibrium dynamics naturally stem from the\ngenerically non-reciprocal interactions between self-interested agents, whereas\nequilibrium descriptions often only apply to scenarios where individuals act\nwith the common good in mind. We bridge these two contrasting paradigms by\nstudying a Sakoda-Schelling occupation model with both individualistic and\naltruistic agents, who, in isolation, follow nonequilibrium and equilibrium\ndynamics respectively. We investigate how the relative fraction of these two\npopulations impacts the behavior of the system. In particular, we find that\nwhen fluctuations in the agents' decision-making process are small (high\nrationality), a very moderate amount of altruistic agents mitigates the\nsub-optimal concentration of individualists in dense clusters. In the regime\nwhere fluctuations carry more weight (low rationality), on the other hand,\naltruism progressively allows the agents to coordinate in a way that is\nsignificantly more robust, which we understand by reducing the model to a\nsingle effective population studied through the lens of active matter physics.\nWe highlight that localizing the altruistic intervention at the right point in\nspace may be paramount for its effectiveness.",
      "generated_abstract": "The study of random walks in random environments is a central subject in\nthe theory of statistical mechanics. In the present paper, we consider the\nconvergence of random walks in random environments and focus on the behavior of\ntheir first and second moments. We derive the convergence rates of the first and\nsecond moments of random walks in random environments and derive their limit\ntheorems. Furthermore, we obtain the limit theorems for the first and second\nmoments of the number of jumps of random walks in random environments.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0967741935483871,
          "p": 0.3076923076923077,
          "f": 0.14723926016334835
        },
        "rouge-2": {
          "r": 0.03468208092485549,
          "p": 0.1,
          "f": 0.05150214209876799
        },
        "rouge-l": {
          "r": 0.08064516129032258,
          "p": 0.2564102564102564,
          "f": 0.12269938286273488
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.04826v1",
      "true_abstract": "The reliance on large labeled datasets presents a significant challenge in\nmedical image segmentation. Few-shot learning offers a potential solution, but\nexisting methods often still require substantial training data. This paper\nproposes a novel approach that leverages the Segment Anything Model 2 (SAM2), a\nvision foundation model with strong video segmentation capabilities. We\nconceptualize 3D medical image volumes as video sequences, departing from the\ntraditional slice-by-slice paradigm. Our core innovation is a support-query\nmatching strategy: we perform extensive data augmentation on a single labeled\nsupport image and, for each frame in the query volume, algorithmically select\nthe most analogous augmented support image. This selected image, along with its\ncorresponding mask, is used as a mask prompt, driving SAM2's video\nsegmentation. This approach entirely avoids model retraining or parameter\nupdates. We demonstrate state-of-the-art performance on benchmark few-shot\nmedical image segmentation datasets, achieving significant improvements in\naccuracy and annotation efficiency. This plug-and-play method offers a powerful\nand generalizable solution for 3D medical image segmentation.",
      "generated_abstract": ": To explore the effects of the 3D-LSTM network on the\nconsolidation of pre-trained segmentation models and its application to\nmicroscopy images.\n  Methods: The 3D-LSTM network was designed to consolidate the pre-trained\nsegmentation model with the 3D-ConvNet framework. The performance of the\nconsolidated segmentation model was evaluated using the Dice similarity\ncoefficient, and the consolidated model was tested on the microscopy image\ndataset.\n  Results: The results of the evaluation showed that the 3D-LSTM network\nachieved a Dice similarity coefficient of 0.874, which was significantly\nhigher than that of the 3D-ConvNet network (p<0.05), and the consolidated model\nwas able to detect tumor structures and accur",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10256410256410256,
          "p": 0.2033898305084746,
          "f": 0.13636363190663756
        },
        "rouge-2": {
          "r": 0.006578947368421052,
          "p": 0.011363636363636364,
          "f": 0.008333328688891476
        },
        "rouge-l": {
          "r": 0.09401709401709402,
          "p": 0.1864406779661017,
          "f": 0.12499999554300119
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/CO/2501.15194v3",
      "true_abstract": "Short text clustering has gained significant attention in the data mining\ncommunity. However, the limited valuable information contained in short texts\noften leads to low-discriminative representations, increasing the difficulty of\nclustering. This paper proposes a novel short text clustering framework, called\nReliable \\textbf{P}seudo-labeling via \\textbf{O}ptimal \\textbf{T}ransport with\n\\textbf{A}ttention for Short Text Clustering (\\textbf{POTA}), that generate\nreliable pseudo-labels to aid discriminative representation learning for\nclustering. Specially, \\textbf{POTA} first implements an instance-level\nattention mechanism to capture the semantic relationships among samples, which\nare then incorporated as a semantic consistency regularization term into an\noptimal transport problem. By solving this OT problem, we can yield reliable\npseudo-labels that simultaneously account for sample-to-sample semantic\nconsistency and sample-to-cluster global structure information. Additionally,\nthe proposed OT can adaptively estimate cluster distributions, making\n\\textbf{POTA} well-suited for varying degrees of imbalanced datasets. Then, we\nutilize the pseudo-labels to guide contrastive learning to generate\ndiscriminative representations and achieve efficient clustering. Extensive\nexperiments demonstrate \\textbf{POTA} outperforms state-of-the-art methods. The\ncode is available at:\n\\href{https://github.com/YZH0905/POTA-STC/tree/main}{https://github.com/YZH0905/POTA-STC/tree/main}.",
      "generated_abstract": "This paper introduces a novel method for efficient sampling from the\ndistributions of the Fourier coefficients of a Gaussian random vector. Our\nmethod uses the Cholesky factorization of the covariance matrix of the\nGaussian random vector, and allows for arbitrary dimensions. The method is\nefficient, requiring only the storage of a small number of matrices and\noperations on a single matrix. We also demonstrate that the method can be\nextended to the case where the random vector is not Gaussian.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12,
          "p": 0.28846153846153844,
          "f": 0.16949152127421888
        },
        "rouge-2": {
          "r": 0.012422360248447204,
          "p": 0.02702702702702703,
          "f": 0.017021272281033235
        },
        "rouge-l": {
          "r": 0.12,
          "p": 0.28846153846153844,
          "f": 0.16949152127421888
        }
      }
    },
    {
      "paper_id": "math.NA.cs/NA/2503.10199v1",
      "true_abstract": "The Bayesian inversion method demonstrates significant potential for solving\ninverse problems, enabling both point estimation and uncertainty\nquantification. However, Bayesian maximum a posteriori (MAP) estimation may\nbecome unstable when handling data from diverse distributions (e.g., solutions\nof stochastic partial differential equations (SPDEs)). Additionally, Monte\nCarlo sampling methods are computationally expensive. To address these\nchallenges, we propose a novel two-stage optimization method based on optimal\ncontrol theory and variational Bayesian methods. This method not only achieves\nstable solutions for stochastic inverse problems but also efficiently\nquantifies the uncertainty of the solutions. In the first stage, we introduce a\nnew weighting formulation to ensure the stability of the Bayesian MAP\nestimation. In the second stage, we derive the necessary condition to\nefficiently quantify the uncertainty of the solutions, by combining the new\nweighting formula with variational inference. Furthermore, we establish an\nerror estimation theorem that relates the exact solution to the optimally\nestimated solution under different amounts of observed data. Finally, the\nefficiency of the proposed method is demonstrated through numerical examples.",
      "generated_abstract": "In this paper, we propose a novel deep learning approach to image segmentation\ninvolving a two-stage training process. In the first stage, we design a\nproposed convolutional neural network architecture that incorporates a\ncomplementary classifier to improve the robustness of the model in the presence\nof image noise. In the second stage, we further enhance the model's performance\nby incorporating a loss function that explicitly encourages the network to\nlearn to segment the target region. The proposed approach is evaluated in\nnumerical simulations, where it demonstrates superior performance compared to\nstate-of-the-art methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1652892561983471,
          "p": 0.3125,
          "f": 0.21621621169086935
        },
        "rouge-2": {
          "r": 0.06134969325153374,
          "p": 0.11235955056179775,
          "f": 0.07936507479623357
        },
        "rouge-l": {
          "r": 0.1652892561983471,
          "p": 0.3125,
          "f": 0.21621621169086935
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06318v1",
      "true_abstract": "This article discusses the key principles of radio spectrum management with a\nfocus on spectrum allocation and access. We show the current regime's inherent\nrigidity and constrained possibilities for introducing new radiocommunication\nservices and applications. The article proposes how governments and spectrum\nusers could cooperate in taking spectrum management to a qualitatively new\nlevel, characterized by light touch regulation and flexible use. This could be\nachieved through the broader introduction of emerging practices such as\nSpectrum Usage Rights, liberalized spectrum trading, and full shared spectrum\naccess. We conclude by presenting a vision for a 'perfect' spectrum management\narrangement and future research directions.",
      "generated_abstract": "r proposes a novel hybrid system that combines an electromechanical\nengine with a digital controller for the control of the gear train of a\nhigh-speed, high-power electric vehicle (EV). The proposed hybrid system\nintegrates a permanent magnet synchronous motor (PMSM) with a DC electric motor\n(DEM) and a digital controller. The PMSM is used as a power source to provide\nthe initial torque, while the DEM provides a constant power output. The PMSM\nand DEM operate in a closed-loop feedback system with the digital controller.\nThe controller is designed to operate in a hybrid mode, which integrates the\nPMSM and DEM to provide a variable torque. The proposed system is evaluated\nusing a simulation environment and a real EV. The results demonstrate that the\nproposed hybrid system can enhance the performance of the EV by providing\nen",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16216216216216217,
          "p": 0.16216216216216217,
          "f": 0.16216215716216234
        },
        "rouge-2": {
          "r": 0.01020408163265306,
          "p": 0.008403361344537815,
          "f": 0.009216584908580036
        },
        "rouge-l": {
          "r": 0.16216216216216217,
          "p": 0.16216216216216217,
          "f": 0.16216215716216234
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2412.16850v1",
      "true_abstract": "We introduce a model for limit order book of a certain security with two main\nfeatures: First, both the limit orders and market orders for the given asset\nare allowed to appear and interact with each other. Second, the high frequency\ntrading activities are allowed and described by the scaling limit of\nnearly-unstable multi-dimensional Hawkes processes with power law decay. The\nmodel has been derived as a stochastic partial differential equation (SPDE, for\nshort), under certain intuitive identifications. Its diffusion coefficient is\ndetermined by a Volterra integral equation driven by a Hawkes process, whose\nHurst exponent is less than 1/2 (so that the relevant process is negatively\ncorrelated). As a result, the volatility path of the SPDE is rougher than that\ndriven by a (standard) Brownian motion. The well-posedness follows from a\nresult in literature. Hence, a foundation is laid down for further studies in\nthis direction.",
      "generated_abstract": "igate the effect of market liquidity on the efficiency of the\nmarket maker in a market with multiple buyers and sellers. We focus on the\ncase of a single-asset market with a single liquidation price and a single\nliquidation strategy. We first show that the market maker's optimal strategy\ninvolves an equal weighted market order with an equal weighted limit order. We\nthen use the Shapley value to determine the optimal strategy, which depends on\nthe liquidity level and market liquidity. Our results show that the market\nmakers' optimal strategy in this model is a market order with an equal weighted\nlimit order. However, if the liquidity is very low, the market maker can\nchoose an equal weighted limit order with a higher liquidity, or a market order\nwith a higher liquidity, as long as the market liquidity is not very low. If\nthe liquidity is very",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1553398058252427,
          "p": 0.24615384615384617,
          "f": 0.19047618573200123
        },
        "rouge-2": {
          "r": 0.04225352112676056,
          "p": 0.057692307692307696,
          "f": 0.048780482924185825
        },
        "rouge-l": {
          "r": 0.14563106796116504,
          "p": 0.23076923076923078,
          "f": 0.17857142382723934
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/OT/2503.02645v1",
      "true_abstract": "Mixup is a widely adopted data augmentation technique known for enhancing the\ngeneralization of machine learning models by interpolating between data points.\nDespite its success and popularity, limited attention has been given to\nunderstanding the statistical properties of the synthetic data it generates. In\nthis paper, we delve into the theoretical underpinnings of mixup, specifically\nits effects on the statistical structure of synthesized data. We demonstrate\nthat while mixup improves model performance, it can distort key statistical\nproperties such as variance, potentially leading to unintended consequences in\ndata synthesis. To address this, we propose a novel mixup method that\nincorporates a generalized and flexible weighting scheme, better preserving the\noriginal data's structure. Through theoretical developments, we provide\nconditions under which our proposed method maintains the (co)variance and\ndistributional properties of the original dataset. Numerical experiments\nconfirm that the new approach not only preserves the statistical\ncharacteristics of the original data but also sustains model performance across\nrepeated synthesis, alleviating concerns of model collapse identified in\nprevious research.",
      "generated_abstract": "We present a novel method to construct a sparse and low-rank approximation of\nthe joint distribution of two high-dimensional variables. This method is\nderived by combining a two-step iterative process that we call the\n\\textit{two-step joint sparse embedding} (2SE) algorithm. The first step of the\nalgorithm constructs a sparse representation of the joint distribution, while\nthe second step performs a rank-constrained sparse reconstruction. We show that\nthe two-step joint sparse embedding algorithm achieves excellent numerical\nperformance and provides a practical way to construct low-rank approximations\nof high-dimensional distributions. In particular, the algorithm can be used to\nconstruct a low-rank approximation of the joint distribution of the\nGaussian-free, multivariate Gaussian random variables. We illustrate the\nnumerical performance of the algorithm through a variety of experiments.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14049586776859505,
          "p": 0.25,
          "f": 0.17989417528736612
        },
        "rouge-2": {
          "r": 0.0189873417721519,
          "p": 0.028846153846153848,
          "f": 0.022900758571179836
        },
        "rouge-l": {
          "r": 0.12396694214876033,
          "p": 0.22058823529411764,
          "f": 0.15873015412334493
        }
      }
    },
    {
      "paper_id": "physics.comp-ph.hep-ex/2503.09213v1",
      "true_abstract": "The scientific communities of nuclear, particle, and astroparticle physics\nare continuing to advance and are facing unprecedented software challenges due\nto growing data volumes, complex computing needs, and environmental\nconsiderations. As new experiments emerge, software and computing needs must be\nrecognised and integrated early in design phases. This document synthesises\ninsights from ECFA, NuPECC and APPEC, representing particle physics, nuclear\nphysics, and astroparticle physics, and presents collaborative strategies for\nimproving software, computing frameworks, infrastructure, and career\ndevelopment within these fields.",
      "generated_abstract": "We present the first study of the 1S-3P phase space for the decay of the\nCharmonium system to $J/\\psi\\to\\phi\\eta$ and $\\phi\\to K\\bar{K}\\eta$. The\nexperimental data was collected by the BaBar Collaboration in 2012. The new\ndecay mode is a new experimental signature for the decays of the\nCharmonium-Hadrons systems. The decay width was obtained by a Monte Carlo\nsimulation and the phase space was evaluated in the framework of the\ncollinear-soft-collinear-expert approach. The results are presented in the\nform of a phase space histogram and a phase space distribution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.140625,
          "p": 0.17647058823529413,
          "f": 0.15652173419432908
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.140625,
          "p": 0.17647058823529413,
          "f": 0.15652173419432908
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.17005v1",
      "true_abstract": "This study documents the relationship between computer skills/digital\nliteracy and influenza vaccination take-up among older adults in Europe during\nand after the COVID-19 pandemic. Using data from the Survey of Health, Aging\nand Retirement in Europe, we find a positive partial association between\ninfluenza vaccination take-up and two indicators of computer skills/digital\nliteracy, self-assessed pre-pandemic computer skills and having used a computer\nat work in any pre-pandemic job. We do not estimate significant behavioural\nchanges for individuals with better computer skills that may have been driven\nby spillover effects from the pandemic experience.",
      "generated_abstract": "r introduces a new approach to analyzing the social and economic\nimpacts of climate change mitigation policies by combining a novel\ndynamic-differential-difference (DDD) model with a multi-sectoral optimization\nframework. Our model incorporates both short-term and long-term climate\npolicies, including carbon pricing, carbon taxes, and green banking, along with\nthe land-use change and forestry (LUC) sector, which is commonly considered a\nhigh-carbon component. We develop a multi-sectoral DDD model that incorporates\nthe LUC sector, along with the main climate mitigation policies, to assess\ntheir impact on greenhouse gas (GHG) emissions and economic growth. We find\nthat climate mitigation policies have a positive impact on GHG emissions and\neconomic growth, while the LUC sector has a negative",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15942028985507245,
          "p": 0.14864864864864866,
          "f": 0.15384614885226677
        },
        "rouge-2": {
          "r": 0.011494252873563218,
          "p": 0.009615384615384616,
          "f": 0.010471199228093685
        },
        "rouge-l": {
          "r": 0.14492753623188406,
          "p": 0.13513513513513514,
          "f": 0.13986013486625282
        }
      }
    },
    {
      "paper_id": "physics.gen-ph.physics/gen-ph/2502.15841v1",
      "true_abstract": "The present paper reanalyzes the problem of the refractive properties of the\nphysical vacuum and their modification under the action of the gravitational\nfield and the electromagnetic field. This problem was studied in our previous\nworks and in the subsequent works of the researchers: Leuchs, Urban, Mainland\nand their collaborators. By modeling the physical vacuum as a\nparticle-antiparticle system, we can deduce with a certain approximation, in a\nsemiclassical theory, the properties of the free vacuum and the vacuum modified\nby the interaction with a gravitational field and an electromagnetic field.\nMore precise calculation of permittivities of free vacuum and near a particle\ncan lead to a non-point model of the particle. This modeling can follow both\nthe quantum and the general relativistic path as well as the phenomenological\npath, the results complementing each other.",
      "generated_abstract": "t a novel approach for the generation of highly localized\nmanifolds and their applications to data compression and high-fidelity\nreconstruction of 3D objects. By exploiting the symmetries of 3D manifolds,\nwe demonstrate that a manifold can be compressed into a significantly smaller\nnumber of data points than is required to represent the manifold's global\ntopology. This compression is achieved through the generation of localized\ndeformations that are invariant under the group of diffeomorphisms that\npreserves the manifold's topology. The compressed representation is then\nreconstructed using a data-driven approach. We demonstrate that this method\nachieves compression ratios in excess of 1000, enabling the compression of\nmanifolds with a diameter of 300 units, which are typically inaccessible to\ntraditional compression techniques. The method is also able to recover the\nman",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1891891891891892,
          "p": 0.17073170731707318,
          "f": 0.1794871745003289
        },
        "rouge-2": {
          "r": 0.02586206896551724,
          "p": 0.02459016393442623,
          "f": 0.025210079036792177
        },
        "rouge-l": {
          "r": 0.1891891891891892,
          "p": 0.17073170731707318,
          "f": 0.1794871745003289
        }
      }
    },
    {
      "paper_id": "math.FA.math/OA/2503.01331v1",
      "true_abstract": "We introduce a new family of non-negative real-valued functions on a\n$C^*$-algebra $\\mathcal{A}$, i.e., for $0\\leq \\mu \\leq 1,$\n$$\\|a\\|_{\\sigma_{\\mu}}= \\text{sup}\\left\\lbrace \\sqrt{|f(a)|^2 \\sigma_{\\mu}\nf(a^*a)}: f\\in \\mathcal{A}', \\, f(1)=\\|f\\|=1 \\right\\rbrace, \\quad $$ where\n$a\\in \\mathcal{A}$ and $\\sigma_{\\mu}$ is an interpolation path of the symmetric\nmean $\\sigma$. These functions are semi-norms as they satisfy the norm axioms,\nexcept for the triangle inequality. Special cases satisfying triangle\ninequality, and a complete equality characterization is also discussed. Various\nbounds and relationships will be established for this new family, with a\nconnection to the existing literature in the algebra of all bounded linear\noperators on a Hilbert space.",
      "generated_abstract": "We prove that a non-degenerate quadratic form $Q$ on a finite-dimensional\nalgebra $A$ is non-degenerate if and only if the following conditions hold:\n\\begin{enumerate}\n\\item[(i)",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08139534883720931,
          "p": 0.3181818181818182,
          "f": 0.12962962638545963
        },
        "rouge-2": {
          "r": 0.009708737864077669,
          "p": 0.041666666666666664,
          "f": 0.015748028430777456
        },
        "rouge-l": {
          "r": 0.05813953488372093,
          "p": 0.22727272727272727,
          "f": 0.0925925893484226
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.09917v1",
      "true_abstract": "Economists disagree about the factors driving the substantial increase in\nresidual wage inequality in the US over the past few decades. To identify\nchanges in the returns to unobserved skills, we make a novel assumption about\nthe dynamics of skills rather than about the stability of skill distributions\nacross cohorts, as is standard. We show that our assumption is supported by\ndata on test score dynamics for older workers in the HRS. Using survey data\nfrom the PSID and administrative data from the IRS and SSA, we estimate that\nthe returns to unobserved skills $declined$ substantially in the late-1980s and\n1990s despite an increase in residual inequality. Accounting for firm-specific\npay differences yields similar results. Extending our framework to consider\noccupational differences in returns to skill and multiple unobserved skills, we\nfurther show that skill returns display similar patterns for workers employed\nin each of cognitive, routine, and social occupations. Finally, our results\nsuggest that increasing skill dispersion, driven by rising skill volatility,\nexplains most of the growth in residual wage inequality since the 1980s.",
      "generated_abstract": "This paper examines the relationship between the U.S. economy and the\nworld economy. In particular, we examine the effects of the U.S. economy on\nthe world economy, focusing on the impact of U.S. monetary policy on world\neconomic activity. We use the Growth and Shadow Economy Model to analyze\nthe relationship between the U.S. economy and the world economy. Our analysis\nfinds that the U.S. economy directly influences the world economy through its\ninfluence on global monetary policy. Furthermore, we show that changes in the\nU.S. economy have a significant impact on global monetary policy, with\nsignificant implications for global economic stability and prosperity.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1346153846153846,
          "p": 0.25,
          "f": 0.17499999545000014
        },
        "rouge-2": {
          "r": 0.03225806451612903,
          "p": 0.06097560975609756,
          "f": 0.04219408830137669
        },
        "rouge-l": {
          "r": 0.1346153846153846,
          "p": 0.25,
          "f": 0.17499999545000014
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.00348v1",
      "true_abstract": "The increasing frequency of environmental hazards due to climate change\nunderscores the urgent need for effective monitoring systems. Current\napproaches either rely on expensive labelled datasets, struggle with seasonal\nvariations, or require multiple observations for confirmation (which delays\ndetection). To address these challenges, this work presents SHAZAM -\nSelf-Supervised Change Monitoring for Hazard Detection and Mapping. SHAZAM uses\na lightweight conditional UNet to generate expected images of a region of\ninterest (ROI) for any day of the year, allowing for the direct modelling of\nnormal seasonal changes and the ability to distinguish potential hazards. A\nmodified structural similarity measure compares the generated images with\nactual satellite observations to compute region-level anomaly scores and\npixel-level hazard maps. Additionally, a theoretically grounded seasonal\nthreshold eliminates the need for dataset-specific optimisation. Evaluated on\nfour diverse datasets that contain bushfires (wildfires), burned regions,\nextreme and out-of-season snowfall, floods, droughts, algal blooms, and\ndeforestation, SHAZAM achieved F1 score improvements of between 0.066 and 0.234\nover existing methods. This was achieved primarily through more effective\nhazard detection (higher recall) while using only 473K parameters. SHAZAM\ndemonstrated superior mapping capabilities through higher spatial resolution\nand improved ability to suppress background features while accentuating both\nimmediate and gradual hazards. SHAZAM has been established as an effective and\ngeneralisable solution for hazard detection and mapping across different\ngeographical regions and a diverse range of hazards. The Python code is\navailable at: https://github.com/WiseGamgee/SHAZAM",
      "generated_abstract": "ntext of human-computer interaction, computer vision (CV) has\nbeen a crucial technology for enhancing human capabilities. However, the\ncomplexities of large-scale CV tasks and the lack of human-like perceptual\nabilities pose significant challenges to CV-based interfaces. In this paper, we\npropose a novel method to enhance the perceptual ability of a CV-based interface\nby leveraging the human perception capabilities. Specifically, we introduce\na new framework named Sense-to-Perception, which integrates two key\ncomponents, namely Sense and Perception. Sense, which captures the\ninteraction-related information from the user's sensory system, serves as the\nbasis for Perception, which guides the interface to adapt to the user's\nsensory signals. By integrating these two key components, Sense-to-Perception\ncan enhance the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07428571428571429,
          "p": 0.1625,
          "f": 0.10196078000768953
        },
        "rouge-2": {
          "r": 0.01293103448275862,
          "p": 0.027777777777777776,
          "f": 0.01764705448858238
        },
        "rouge-l": {
          "r": 0.06857142857142857,
          "p": 0.15,
          "f": 0.09411764275278758
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.06331v1",
      "true_abstract": "Models with unnormalized probability density functions are ubiquitous in\nstatistics, artificial intelligence and many other fields. However, they face\nsignificant challenges in model selection if the normalizing constants are\nintractable. Existing methods to address this issue often incur high\ncomputational costs, either due to numerical approximations of normalizing\nconstants or evaluation of bias corrections in information criteria. In this\npaper, we propose a novel and fast selection criterion, T-GIC, for nested\nmodels, allowing direct data sampling from a possibly unnormalized probability\ndensity function. T-GIC gives a consistent selection under mild regularity\nconditions and is computationally efficient, benefiting from a multiplying\nfactor that depends only on the sample size and the model complexity. Extensive\nsimulation studies and real-data applications demonstrate the efficacy of T-GIC\nin the selection of nested models with unnormalized probability densities.",
      "generated_abstract": "r introduces a novel approach to identify the optimal number of\nmodels for a multivariate longitudinal dataset, using a novel method for\nidentifying the optimal number of time series models, which we call\n\"optimal-time-series model identification\". The proposed method consists of\nseveral steps, each of which is a variant of the traditional\noptimal-time-series model identification method. The first step is to\nidentify the optimal number of time series models. The second step is to\nidentify the optimal number of time series models that are compatible with the\ndataset. The third step is to identify the optimal number of models for each\ntime series model. The fourth step is to identify the optimal number of\nmodels for each time series model. The fifth step is to identify the optimal\nnumber of models for each time series model. The sixth step is to identify the\noptimal number of models for each time series model. The seventh",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.2692307692307692,
          "f": 0.18666666213688898
        },
        "rouge-2": {
          "r": 0.007936507936507936,
          "p": 0.0125,
          "f": 0.009708733113396612
        },
        "rouge-l": {
          "r": 0.11224489795918367,
          "p": 0.21153846153846154,
          "f": 0.14666666213688903
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2503.01148v1",
      "true_abstract": "This paper investigates the risk spillovers among AI ETFs, AI tokens, and\ngreen markets using the R2 decomposition method. We reveal several key\ninsights. First, the overall transmission connectedness index (TCI) closely\naligns with the contemporaneous TCI, while the lagged TCI is significantly\nlower. Second, AI ETFs and clean energy act as risk transmitters, whereas AI\ntokens and green bond function as risk receivers. Third, AI tokens are\ndifficult to hedge and provide limited hedging ability compared to AI ETFs and\ngreen assets. However, multivariate portfolios effectively reduce AI tokens\ninvestment risk. Among them, the minimum correlation portfolio outperforms the\nminimum variance and minimum connectedness portfolios.",
      "generated_abstract": "We propose a new method to estimate the impact of interest rate changes on\nthe volatility of stock returns. Our approach uses the concept of conditional\nrisk aversion to estimate the effect of interest rate changes on stock returns\nwithout the need to assume that the market is risk neutral. We derive\nasymptotic expressions for the estimated conditional risk aversion coefficient\nand the mean squared error of the estimated coefficient. Additionally, we show\nthat the estimated coefficient can be used to construct a risk-neutral model\nwith a more tractable price process. Finally, we show that the estimated\nconditional risk aversion coefficient can be used to construct a new model for\nvolatility forecasting.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10666666666666667,
          "p": 0.13793103448275862,
          "f": 0.12030074696138864
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09333333333333334,
          "p": 0.1206896551724138,
          "f": 0.10526315297642627
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2501.04718v1",
      "true_abstract": "Gene panel selection aims to identify the most informative genomic biomarkers\nin label-free genomic datasets. Traditional approaches, which rely on domain\nexpertise, embedded machine learning models, or heuristic-based iterative\noptimization, often introduce biases and inefficiencies, potentially obscuring\ncritical biological signals. To address these challenges, we present an\niterative gene panel selection strategy that harnesses ensemble knowledge from\nexisting gene selection algorithms to establish preliminary boundaries or prior\nknowledge, which guide the initial search space. Subsequently, we incorporate\nreinforcement learning through a reward function shaped by expert behavior,\nenabling dynamic refinement and targeted selection of gene panels. This\nintegration mitigates biases stemming from initial boundaries while\ncapitalizing on RL's stochastic adaptability. Comprehensive comparative\nexperiments, case studies, and downstream analyses demonstrate the\neffectiveness of our method, highlighting its improved precision and efficiency\nfor label-free biomarker discovery. Our results underscore the potential of\nthis approach to advance single-cell genomics data analysis.",
      "generated_abstract": "tion of life on Earth is a complex and multifaceted process, with\nmany factors influencing the emergence and diversification of life forms.\nFossils are among the most reliable sources of information about the evolution\nof life, but their interpretation requires understanding of the geological\nhistory of a region, including the stratigraphy and geochemistry of the\nsediments. The most common method to date fossils is based on radiometric\ndating, which is affected by the uncertainty of the age of the sediments and\nthe inherent uncertainties of the radioactive decay rates. To address these\nissues, we developed a new approach that combines the use of the 10Be-26Al\ndating method with the stratigraphic and geochemical information available in\nthe sedimentary record. Our approach is based on the idea that the 10Be-26Al\nd",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14166666666666666,
          "p": 0.2073170731707317,
          "f": 0.16831682686011187
        },
        "rouge-2": {
          "r": 0.02054794520547945,
          "p": 0.025423728813559324,
          "f": 0.022727267783518065
        },
        "rouge-l": {
          "r": 0.14166666666666666,
          "p": 0.2073170731707317,
          "f": 0.16831682686011187
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.10260v1",
      "true_abstract": "Our study focuses on isolating swallowing dynamics from interfering patient\nmotion in videofluoroscopy, an X-ray technique that records patients swallowing\na radiopaque bolus. These recordings capture multiple motion sources, including\nhead movement, anatomical displacements, and bolus transit. To enable precise\nanalysis of swallowing physiology, we aim to eliminate distracting motion,\nparticularly head movement, while preserving essential swallowing-related\ndynamics. Optical flow methods fail due to artifacts like flickering and\ninstability, making them unreliable for distinguishing different motion groups.\nWe evaluated markerless tracking approaches (CoTracker, PIPs++, TAP-Net) and\nquantified tracking accuracy in key medical regions of interest. Our findings\nshow that even sparse tracking points generate morphing displacement fields\nthat outperform leading registration methods such as ANTs, LDDMM, and\nVoxelMorph. To compare all approaches, we assessed performance using MSE and\nSSIM metrics post-registration. We introduce a novel motion correction pipeline\nthat effectively removes disruptive motion while preserving swallowing dynamics\nand surpassing competitive registration techniques. Code will be available\nafter review.",
      "generated_abstract": "thways that connect the visual cortex with other parts of the brain\nare essential for many cognitive functions, including language processing.\nResearch on cortical pathways has focused on the role of the primary visual\ncortex (V1), which is known to have a large-scale representation of the visual\nworld. However, the role of other cortical regions, including the superior\nvisual cortex (SVC), is less understood. We studied the role of SVC in\nlanguage-related tasks using functional magnetic resonance imaging (fMRI) and\ncomputational modeling. We found that SVC was involved in language processing\nin the context of visual words, but was not involved in the processing of\nnon-visual words. This suggests that SVC is involved in language processing\nin a different way from V1. Additionally, we found that SVC was not involved\nin the processing of non-visual words in a word-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12096774193548387,
          "p": 0.19480519480519481,
          "f": 0.14925372661666805
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11290322580645161,
          "p": 0.18181818181818182,
          "f": 0.13930347786044914
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.10635v1",
      "true_abstract": "Large language models (LLMs) offer the potential to automate a large number\nof tasks that previously have not been possible to automate, including some in\nscience. There is considerable interest in whether LLMs can automate the\nprocess of causal inference by providing the information about causal links\nnecessary to build a structural model. We use the case of confounding in the\nCoronary Drug Project (CDP), for which there are several studies listing\nexpert-selected confounders that can serve as a ground truth. LLMs exhibit\nmediocre performance in identifying confounders in this setting, even though\ntext about the ground truth is in their training data. Variables that experts\nidentify as confounders are only slightly more likely to be labeled as\nconfounders by LLMs compared to variables that experts consider\nnon-confounders. Further, LLM judgment on confounder status is highly\ninconsistent across models, prompts, and irrelevant concerns like\nmultiple-choice option ordering. LLMs do not yet have the ability to automate\nthe reporting of causal links.",
      "generated_abstract": "We study the identification of a linearly augmented vector autoregressive\nmodel with a vector of lagged dependent variables and a vector of\ncorresponding exogenous variables. We propose a novel methodology for\nidentifying the exogenous variables and a simple and efficient method for\nidentifying the dependent variables. We show that the exogenous variables are\nidentified under the assumption of linear dynamics of the dependent variables.\nWe then provide conditions under which the exogenous variables are\nidentified under the assumption of nonlinear dynamics of the dependent\nvariables.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10714285714285714,
          "p": 0.3076923076923077,
          "f": 0.15894039351958256
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09821428571428571,
          "p": 0.28205128205128205,
          "f": 0.1456953604069998
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.02146v1",
      "true_abstract": "Reliance on stereotypes is a persistent feature of human decision-making and\nhas been extensively documented in educational settings, where it can shape\nstudents' confidence, performance, and long-term human capital accumulation.\nWhile effective techniques exist to mitigate these negative effects, a crucial\nfirst step is to establish whether teachers can recognize stereotypes in their\nprofessional environment. We introduce the Stereotype Identification Test\n(SIT), a novel survey tool that asks teachers to evaluate and comment on the\npresence of stereotypes in images randomly drawn from school textbooks. Their\nresponses are systematically linked to established measures of implicit bias\n(Implicit Association Test, IAT) and explicit bias (survey scales on teaching\nstereotypes and social values). Our findings demonstrate that the SIT is a\nvalid and reliable measure of stereotype recognition. Teachers' ability to\nrecognize stereotypes is linked to trainable traits such as implicit bias\nawareness and inclusive teaching practices. Moreover, providing personalized\nfeedback on implicit bias improves SIT scores by 0.25 standard deviations,\nreinforcing the idea that stereotype recognition is malleable and can be\nenhanced through targeted interventions.",
      "generated_abstract": "This paper examines how the financial sector shapes the development of\nmarket-based environmental regulations in China. We find that the financial\nsector has a significant impact on regulatory compliance by shaping the\nperceptions of the public, investors, and regulators. This perception-driven\neffect is particularly pronounced in the electricity sector, where the financial\nsector has a disproportionate influence on compliance. This effect is further\nenhanced by the presence of shadow banks, which are often connected to the\nfinancial sector, facilitating the exchange of information and coordinating\nactions. Our findings suggest that effective regulation in the electricity\nsector requires a nuanced and systematic approach that considers the complex\ninterplay between financial and non-financial actors.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1532258064516129,
          "p": 0.2638888888888889,
          "f": 0.19387754637234497
        },
        "rouge-2": {
          "r": 0.024096385542168676,
          "p": 0.039603960396039604,
          "f": 0.02996254211280917
        },
        "rouge-l": {
          "r": 0.13709677419354838,
          "p": 0.2361111111111111,
          "f": 0.17346938310703885
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.05946v1",
      "true_abstract": "Despite growing evidence that neighborhoods play a critical role in shaping\neconomic mobility and well-being, effective policies to address neighborhood\ndisadvantage remain elusive. This study evaluates the impact of the Promise\nZone program, which aims to revitalize disadvantaged neighborhoods through\nstreamlined federal support and grant incentives. I use an event study\nframework with newly obtained data on the location of failed finalist\napplications as a comparison group to estimate the effects of the program. The\nresults reveal significant improvements in poverty, household incomes, and\nemployment in Promise Zone neighborhoods, particularly in later-designated\nzones and initially low-status neighborhoods. I also find that effects are\ndriven partly by changes in residential composition, and that Promise Zones\nappear to induce positive spillovers in adjacent areas.",
      "generated_abstract": "r explores the impact of the 2008 financial crisis on the\nperformance of the European stock market. In this study, we employ an\nalternative event study framework and the Randomised Event Study (RES)\nmethodology to examine the impact of the financial crisis on the stock market.\nWe use a novel event-driven approach that incorporates both the 2008 financial\ncrisis and its aftershocks. Our findings suggest that the financial crisis\nincreased volatility, decreased the predictability of returns, and reduced the\nlevel of risk aversion in the market. These results are consistent with\ntraditional financial theory and the contagion effects of the crisis.\nAdditionally, our findings reveal that the financial crisis had a negative\neffect on the stock market's profitability. Our analysis provides insights\ninto the impact of the financial crisis on the stock market and offers",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20430107526881722,
          "p": 0.24675324675324675,
          "f": 0.22352940680899666
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.06666666666666667,
          "f": 0.06249999501953165
        },
        "rouge-l": {
          "r": 0.1935483870967742,
          "p": 0.23376623376623376,
          "f": 0.21176470092664368
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.01084v1",
      "true_abstract": "The United States leads the world in the number of violent mass shootings\nthat occur each year, and policy making on firearms remains polarized along\nparty lines. Are legislators responsive to mass shootings? We estimate the\nlatent positions of nearly 2,000 state legislators on gun policy from their\nroll-call voting records on firearm-related bills from 2011 to 2022. Employing\na staggered difference-in-differences design, we find that mass shootings\nwithin or near a state legislator's district do not alter their voting behavior\non firearm policy, on average, for members of both parties. Our estimated\neffects of mass shootings on treated legislators' support for restrictive gun\npolicies (on a -1 to 1 scale) range from a 4.8% reduction among California\nDemocrats and a 0.9% increase among California Republicans to, across six total\nstates, a 5% (among Democrats) and 7.1% (among Republicans) increase, with 95%\nconfidence intervals spanning opposite directions. We conclude that, on\naverage, mass shootings fail to produce changes in a legislator's support\n(opposition) for restrictive (permissive) firearms bills. Our findings suggest\nthat even the most heinous acts of mass violence -- that are squarely in the\ndomain of events that state legislators might respond to -- fail to produce any\nmeasurable effects on legislators' positions on firearm-related policy.",
      "generated_abstract": "This paper examines the role of government spending in the economic\ndevelopment of countries. We conduct a systematic analysis of the effect of\ngovernment spending on economic growth and employment in different countries\nand across different time periods. Our findings indicate that government\nspending can be a significant contributor to economic growth, employment, and\nreducing poverty. However, the effect of government spending varies across\ncountries and time periods. Our analysis highlights the importance of\nunderstanding the specific context of a country and the role of government\nspending in economic development. This study provides a nuanced and comprehensive\nperspective on the role of government spending in economic growth and\ndevelopment.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08823529411764706,
          "p": 0.23076923076923078,
          "f": 0.12765957046627446
        },
        "rouge-2": {
          "r": 0.010101010101010102,
          "p": 0.023809523809523808,
          "f": 0.014184392980233615
        },
        "rouge-l": {
          "r": 0.08823529411764706,
          "p": 0.23076923076923078,
          "f": 0.12765957046627446
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.cond-mat/mes-hall/2503.10359v1",
      "true_abstract": "In this study, we systematically explore the non-Hermitian skin effect (NHSE)\nand its associated complex-frequency detection in the context of a\nfrequency-dependent non-Hermitian Hamiltonian. This Hamiltonian arises from the\nself-energy correction of the subsystem and can be calculated exactly within\nour theoretical model, without the need for non-Hermitian approximations.\nAdditionally, complex frequency detection, which encompasses complex frequency\nexcitation, synthesis, and fingerprint, enables us to detect the physical\nresponses driven by complex frequency excitations. Our calculations reveal that\nboth complex frequency excitation and synthesis are sensitive to the\nnon-Hermitian approximation and are unable to characterize the presence or\nabscence of the NHSE. In contrast, the complex-frequency fingerprint\nsuccessfully detects the novel responses induced by the NHSE through the\nintroduction of a double-frequency Green's function. Our work paves the way for\na rigorous understanding of non-Hermitian physics in quantum systems and their\nexperimental verification through complex frequency-domain techniques.",
      "generated_abstract": "t a novel method for measuring the temperature dependence of\nthe Hall angle in magnets based on the anomalous Hall effect. This technique\nis based on the fact that the magnetoresistance of ferromagnetic materials\ndecreases with increasing temperature, which is a direct consequence of the\ngap opening of the magnetoresistance tunneling current due to the\nmagnetization-dependent tunneling rates. The method is based on a\nhysteresis loop measurement of the temperature dependence of the Hall angle,\nwhich can be used to determine the temperature-dependent magnetization of the\nmagnet. We show how the temperature dependence of the Hall angle can be\nextracted directly from hysteresis loops, without the need for temperature\ncalibration. We demonstrate this method on a 12-layer Pt/Co/Fe/Co/Pt\n(001)/Pt single-crystal magnet, where we show",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1836734693877551,
          "p": 0.25,
          "f": 0.21176470099930808
        },
        "rouge-2": {
          "r": 0.043478260869565216,
          "p": 0.0594059405940594,
          "f": 0.050209200140754306
        },
        "rouge-l": {
          "r": 0.15306122448979592,
          "p": 0.20833333333333334,
          "f": 0.17647058335224927
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.05512v1",
      "true_abstract": "Recently, large language model (LLM) based text-to-speech (TTS) systems have\ngradually become the mainstream in the industry due to their high naturalness\nand powerful zero-shot voice cloning capabilities.Here, we introduce the\nIndexTTS system, which is mainly based on the XTTS and Tortoise model. We add\nsome novel improvements. Specifically, in Chinese scenarios, we adopt a hybrid\nmodeling method that combines characters and pinyin, making the pronunciations\nof polyphonic characters and long-tail characters controllable. We also\nperformed a comparative analysis of the Vector Quantization (VQ) with\nFinite-Scalar Quantization (FSQ) for codebook utilization of acoustic speech\ntokens. To further enhance the effect and stability of voice cloning, we\nintroduce a conformer-based speech conditional encoder and replace the\nspeechcode decoder with BigVGAN2. Compared with XTTS, it has achieved\nsignificant improvements in naturalness, content consistency, and zero-shot\nvoice cloning. As for the popular TTS systems in the open-source, such as\nFish-Speech, CosyVoice2, FireRedTTS and F5-TTS, IndexTTS has a relatively\nsimple training process, more controllable usage, and faster inference speed.\nMoreover, its performance surpasses that of these systems. Our demos are\navailable at https://index-tts.github.io.",
      "generated_abstract": "r presents a novel approach for the generation of speech-driven\nmusic, which employs a mixture of experts (MoE) model that generates speech\nsamples with varying speaker characteristics. MoE is a neural network that\ncombines multiple speech models, each specialized in different speaker\ncharacteristics. The generated samples are further processed by a\nmulti-stage neural network to obtain music, which is then combined with the\ngenerated speech samples. The proposed approach is evaluated in terms of\nacoustic quality, which is evaluated using the ANSI-S2316 standard, and\nspeech-to-music (S2M) quality, which is evaluated using the S2M-1.0 standard.\nThe results show that the proposed approach is able to generate music with a\nhigh level of quality, even for music with low S2M quality, and that it\ncompares favorably with the state-of-the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15267175572519084,
          "p": 0.26666666666666666,
          "f": 0.19417475265105114
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.03571428571428571,
          "f": 0.02777777302469217
        },
        "rouge-l": {
          "r": 0.1450381679389313,
          "p": 0.25333333333333335,
          "f": 0.18446601478697347
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2410.07566v1",
      "true_abstract": "Transaction Fee Mechanism Design studies auctions run by untrusted miners for\ntransaction inclusion in a blockchain. Under previously-considered desiderata,\nan auction is considered `good' if, informally-speaking, each party (i.e., the\nminer, the users, and coalitions of both miners and users) has no incentive to\ndeviate from the fixed and pre-determined protocol.\n  In this paper, we propose a novel desideratum for transaction fee mechanisms.\nWe say that a TFM is off-chain influence proof when the miner cannot achieve\nadditional revenue by running a separate auction off-chain. While the\npreviously-highlighted EIP-1559 is the gold-standard according to prior\ndesiderata, we show that it does not satisfy off-chain influence proofness.\nIntuitively, this holds because a Bayesian revenue-maximizing miner can\nstrictly increase profits by persuasively threatening to censor any bids that\ndo not transfer a tip directly to the miner off-chain.\n  On the other hand, we reconsider the Cryptographic (multi-party computation\nassisted) Second Price Auction mechanism, which is technically not `simple for\nminers' according to previous desiderata (since miners may wish to set a\nreserve by fabricating bids). We show that, in a slightly different model where\nthe miner is allowed to set the reserve directly, this auction satisfies\nsimplicity for users and miners, and off-chain influence proofness.\n  Finally, we prove a strong impossibility result: no mechanism satisfies all\npreviously-considered properties along with off-chain influence proofness, even\nwith unlimited supply, and even after soliciting input from the miner.",
      "generated_abstract": "em of maximizing the expected payoff from a finite number of\ndifferent actions is a fundamental yet challenging problem in game theory.\nTypically, it is addressed by defining a payoff function that is expected to be\nthe optimal solution to a special class of games. However, this approach is\ninherently non-deterministic, and its computational cost can be prohibitively\nhigh in large-scale problems. In this work, we present a novel approach that\nuses a randomized polynomial-time algorithm to solve the problem of\nmaximizing the expected payoff from a finite number of actions. The algorithm\nuses a randomized polynomial-time deterministic polynomial-time algorithm, and\nis provably guaranteed to return an optimal solution. Our approach is\ncomputationally efficient, and can be applied to problems of varying sizes,\nincluding those with millions of actions. We also discuss the theoretical\nproperties of our algorithm, and provide a polynomial",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13548387096774195,
          "p": 0.25301204819277107,
          "f": 0.17647058369288904
        },
        "rouge-2": {
          "r": 0.009009009009009009,
          "p": 0.016129032258064516,
          "f": 0.011560689042736301
        },
        "rouge-l": {
          "r": 0.10967741935483871,
          "p": 0.20481927710843373,
          "f": 0.14285713831473779
        }
      }
    },
    {
      "paper_id": "gr-qc.quant-ph/2503.10348v1",
      "true_abstract": "The fact that quantum theory is non-differentiable, while general relativity\nis built on the assumption of differentiability sources an incompatibility\nbetween quantum theory and gravity. Higher order geometry addresses this issue\ndirectly by extending differential geometry, such that it can be applied to\ntheories that are non-differentiable, but have a certain degree of H\\\"older\nregularity. As this includes the path integral formulation of quantum theory,\nit provides a natural mathematical framework for describing the interplay\nbetween gravity and quantum theory. In this article, we review the motivation\nfor and the basic features of this framework and point towards future\ndevelopments.",
      "generated_abstract": "uct a model of quantum gravity that is consistent with the\nproperties of the Einstein-Hilbert action and the equivalence of general\nrelativity and quantum theory. We then show that this model can be constructed\nin a way that is consistent with the fact that gravity is invariant under\ntime-reparametrization. The model is based on a non-perturbative construction\nof a conformal field theory that captures the behavior of the quantum\neffects of gravity. We demonstrate that the model, when coupled to a matter\ntheory, can reproduce the effective gravitational action, the equations of\nstate, and the observed matter content of the Universe. Our model is based on\nthe fact that gravity is a low-energy effective theory of quantum fields. In\nthis framework, we can study the properties of gravity by studying the properties\nof quantum fields, which are in turn determined by the properties of spacetime.\nIn this sense, gravity",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3194444444444444,
          "p": 0.3108108108108108,
          "f": 0.31506848815162325
        },
        "rouge-2": {
          "r": 0.09278350515463918,
          "p": 0.07258064516129033,
          "f": 0.08144795887553521
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.24324324324324326,
          "f": 0.2465753374666918
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.10552v1",
      "true_abstract": "In this paper, we propose a new workflow to analyze macrophage motion during\nwound healing. These immune cells are attracted to the wound after an injury\nand they move showing both directional and random motion. Thus, first, we\nsmooth the trajectories and we separate the random from the directional parts\nof the motion. The smoothing model is based on curve evolution where the curve\nmotion is influenced by the smoothing term and the attracting term. Once we\nobtain the random sub-trajectories, we analyze them using the mean squared\ndisplacement to characterize the type of diffusion. Finally, we compute the\nvelocities on the smoothed trajectories and use them as sparse samples to\nreconstruct the wound attractant field. To do that, we consider a minimization\nproblem for the vector components and lengths, which leads to solving the\nLaplace equation with Dirichlet conditions for the sparse samples and zero\nNeumann boundary conditions on the domain boundary.",
      "generated_abstract": "The problem of finding a solution to a linear system of differential\nequations is a fundamental problem in mathematical physics. In this paper, we\npropose a new numerical method to solve the linear system of equations in the\npresence of noise. Our method is based on the discretization of the\nFloquet-Bloch equations. This discretization is obtained through the\ndiscretization of the WKB method. We show that the linear system of equations\ncan be rewritten as a linear system of ODEs. Then we propose a method to\nsolve the linear system of ODEs, which is based on the Newton-Raphson method.\nThis method is applied to the linear system of equations to obtain the\nsolution to the linear system of equations. We test the proposed method on the\nsolution of the linear system of equations. We find that the proposed method\ncan obtain a good solution to the linear system of equations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1875,
          "p": 0.32727272727272727,
          "f": 0.2384105913951143
        },
        "rouge-2": {
          "r": 0.0821917808219178,
          "p": 0.12244897959183673,
          "f": 0.09836065093120151
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.2909090909090909,
          "f": 0.21192052516994878
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2501.11983v2",
      "true_abstract": "We provide closed-form market equilibrium formula consolidating informational\nimperfections and investors beliefs. Based on Merton's model, we characterize\nthe equilibrium expected excess returns vector with incomplete information. We\nthen derive the corresponding market portfolio as the solution to a non-linear\nsystem of equations and analyze the sensitivities of extra excess returns to\nshadow-costs and market weights. We derive the market reference model for\nexcess returns under random shadow-costs. The conditional posterior\ndistribution of excess returns integrates the pick-matrix and pick-vector of\nviews and the vector of shadow-costs into a multivariate distribution with mean\nand covariance dependent on the reference model.",
      "generated_abstract": "This paper examines the role of the information asymmetry in the pricing\nof the European options on the underlying European index. It is shown that the\ninformation asymmetry is a driving factor in the pricing of the European options\non the underlying European index. The paper presents a model of the\ninformation asymmetry based on the concept of the information gap. It is\ndemonstrated that the information gap is a key parameter in the pricing of the\nEuropean options on the underlying European index. A numerical example is\npresented to demonstrate the effect of the information gap on the pricing of\nthe European options on the underlying European index. The results show that the\ninformation gap has a significant impact on the pricing of the European options\non the underlying European index.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13114754098360656,
          "p": 0.18604651162790697,
          "f": 0.1538461489959321
        },
        "rouge-2": {
          "r": 0.010638297872340425,
          "p": 0.015151515151515152,
          "f": 0.012499995153126877
        },
        "rouge-l": {
          "r": 0.13114754098360656,
          "p": 0.18604651162790697,
          "f": 0.1538461489959321
        }
      }
    },
    {
      "paper_id": "cs.CR.econ/GN/2501.09025v2",
      "true_abstract": "The digital age, driven by the AI revolution, brings significant\nopportunities but also conceals security threats, which we refer to as cyber\nshadows. These threats pose risks at individual, organizational, and societal\nlevels. This paper examines the systemic impact of these cyber threats and\nproposes a comprehensive cybersecurity strategy that integrates AI-driven\nsolutions, such as Intrusion Detection Systems (IDS), with targeted policy\ninterventions. By combining technological and regulatory measures, we create a\nmultilevel defense capable of addressing both direct threats and indirect\nnegative externalities. We emphasize that the synergy between AI-driven\nsolutions and policy interventions is essential for neutralizing cyber threats\nand mitigating their negative impact on the digital economy. Finally, we\nunderscore the need for continuous adaptation of these strategies, especially\nin response to the rapid advancement of autonomous AI-driven attacks, to ensure\nthe creation of secure and resilient digital ecosystems.",
      "generated_abstract": "t of the COVID-19 pandemic on the education sector has been\nstark. However, the impact of remote learning on students' academic performance\nhas not been systematically analyzed. This study examines the impact of\nremote learning on student performance using data from 133,305 students in\nhigh-poverty schools in the United States. The results show that students who\nreceived remote learning in 2020 had lower average test scores, higher test\nscore variability, and lower average test score growth than students who\nreceived in-person instruction. The findings suggest that remote learning may\nnot be as beneficial for students as was previously believed and that schools\nshould consider providing students with more in-person instruction in the\nfuture. These findings have important implications for policy makers and\neducation leaders, who should consider providing students with more in-person\ninstruction during remote learning",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14423076923076922,
          "p": 0.189873417721519,
          "f": 0.16393442132282257
        },
        "rouge-2": {
          "r": 0.022058823529411766,
          "p": 0.02654867256637168,
          "f": 0.024096380584830292
        },
        "rouge-l": {
          "r": 0.1346153846153846,
          "p": 0.17721518987341772,
          "f": 0.15300545957418868
        }
      }
    },
    {
      "paper_id": "cs.LG.q-fin/PR/2407.14573v6",
      "true_abstract": "Since the advent of generative artificial intelligence, every company and\nresearcher has been rushing to develop their own generative models, whether\ncommercial or not. Given the large number of users of these powerful new tools,\nthere is currently no intrinsically verifiable way to explain from the ground\nup what happens when LLMs (large language models) learn. For example, those\nbased on automatic speech recognition systems, which have to rely on huge and\nastronomical amounts of data collected from all over the web to produce fast\nand efficient results, In this article, we develop a backdoor attack called\nMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is\nmainly based on modern stock market models. In order to show the possible\nvulnerabilities of speech-based transformers that may rely on LLMs.",
      "generated_abstract": "We present a novel framework for analyzing the interplay between algorithmic\ndynamics and financial market structure. Our approach, based on a\nself-consistent equilibrium model for algorithmic trading, identifies\ninterest-rate-based \"wild-cards\" that may significantly influence the\nfinancial system's response to algorithmic shocks. Our analysis shows that\nincreased algorithmic activity can exacerbate liquidity problems, thereby\npotentially triggering financial crises. In particular, we find that\nincreased algorithmic activity can cause systemic imbalances in interest rate\nmarkets, amplifying the impact of algorithmic shocks on financial stability.\nOur results emphasize the need for continued research on the potential\nimpact of algorithmic trading on financial stability, particularly in\nhigh-risk markets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1188118811881188,
          "p": 0.1643835616438356,
          "f": 0.13793102961223427
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.021052631578947368,
          "f": 0.018099542609694003
        },
        "rouge-l": {
          "r": 0.10891089108910891,
          "p": 0.1506849315068493,
          "f": 0.12643677673867107
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.07834v1",
      "true_abstract": "The Uniswap is a Decentralized Exchange (DEX) protocol that facilitates\nautomatic token exchange without the need for traditional order books. Every\npair of tokens forms a liquidity pool on Uniswap, and each token can be paired\nwith any other token to create liquidity pools. This characteristic motivates\nus to employ a complex network approach to analyze the features of the Uniswap\nmarket. This research presents a comprehensive analysis of the Uniswap network\nusing complex network methods. The network on October 31, 2023, is built to\nobserve its recent features, showcasing both scale-free and core-periphery\nproperties. By employing node and edge-betweenness metrics, we detect the most\nimportant tokens and liquidity pools. Additionally, we construct daily networks\nspanning from the beginning of Uniswap V2 on May 5, 2020, until October 31,\n2023, and our findings demonstrate that the network becomes increasingly\nfragile over time. Furthermore, we conduct a robustness analysis by simulating\nthe deletion of nodes to estimate the impact of some extreme events such as the\nTerra collapse. The results indicate that the Uniswap network exhibits\nrobustness, yet it is notably fragile when deleting tokens with high\nbetweenness centrality. This finding highlights that, despite being a\ndecentralized exchange, Uniswap exhibits significant centralization tendencies\nin terms of token network connectivity and the distribution of TVL across nodes\n(tokens) and edges (liquidity pools).",
      "generated_abstract": "r presents a novel approach to support the development of a\nsustainable energy infrastructure by enhancing the integration of distributed\nenergy resources (DERs) in the power system. The proposed framework is based\non the integration of energy system modelling with multi-agent reinforcement\nlearning, aiming to develop an efficient energy system with higher resilience\nand reliability. To support the development of a sustainable energy infrastructure,\nthe framework leverages the energy system modelling and energy system\noperations and control (ESOC) to estimate the demand, supply, and generation\nof renewable energy sources. The energy system modelling is performed by\nintegrating the IEEE 39-bus system and the BATTERY-39-bus system, while ESOC\nis employed to control the operation of the power system. The proposed framework\nenables the integration of DERs with a more holistic view",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09523809523809523,
          "p": 0.20588235294117646,
          "f": 0.1302325538146026
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.05555555555555555,
          "f": 0.03773584457102225
        },
        "rouge-l": {
          "r": 0.08843537414965986,
          "p": 0.19117647058823528,
          "f": 0.12093022823320732
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.q-bio/SC/2406.16569v1",
      "true_abstract": "In this article we present a comprehensive study of the totally asymmetric\nsimple exclusion process with pausing particles (pTASEP), a model initially\nintroduced to describe RNAP dynamics during transcription. We extend previous\nmean-field approaches and demonstrate that the pTASEP is equivalent to the\nexclusion process with dynamical defects (ddTASEP), thus broadening the scope\nof our investigation to a larger class of problems related to transcription and\ntranslation. We extend the mean-field theory to the open boundary case,\nrevealing the system's phase diagram and critical values of entry and exit\nrates. However, we identify a significant discrepancy between theory and\nsimulations in a region of the parameter space, indicating severe finite-size\neffects. To address this, we develop a single-cluster approximation that\ncaptures the relationship between current and lattice size, providing a more\naccurate representation of the system's dynamics. Finally, we extend our\napproach to open boundary conditions, demonstrating its applicability in\ndifferent scenarios. Our findings underscore the importance of considering\nfinite-size effects, often overlooked in the literature, when modelling\nbiological processes such as transcription and translation.",
      "generated_abstract": "e a non-equilibrium mean-field theory (MEFT) to describe the\nnon-equilibrium dynamics of a system of Brownian particles interacting via\nbond-dependent hopping and random pair-interaction strengths. The theory is\nincorporated via a mean-field approximation to the Langevin equation for the\ndistribution of particle positions, and is used to analyze the influence of\nrandom hopping strengths on the emergence of collective motion. We find that\nin the absence of random hopping strengths, collective motion emerges as a\nresponse to an external noise, while for a given noise amplitude, the emergence\nof collective motion depends on the strength of random hopping strengths. For\nsmall noise amplitudes, random hopping strengths are only weakly coupled to the\nsystem, and collective motion is not observed. However, for larger noise\namplitudes, the system",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.2608695652173913,
          "f": 0.19354838242976077
        },
        "rouge-2": {
          "r": 0.024390243902439025,
          "p": 0.03669724770642202,
          "f": 0.029304024506971227
        },
        "rouge-l": {
          "r": 0.1452991452991453,
          "p": 0.2463768115942029,
          "f": 0.18279569425771777
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.19754v3",
      "true_abstract": "This paper examines whether artificial intelligence (AI) acts as a substitute\nor complement to human labour, drawing on 12 million online job vacancies from\nthe United States spanning 2018-2023. We adopt a two-pronged approach: first,\nanalysing \"internal effects\" within roles explicitly requiring AI, and second,\ninvestigating \"external effects\" that arise when industries, occupations, and\nregions experience increases in AI demand. Our focus centres on whether\ncomplementary skills-such as digital literacy, teamwork, resilience, agility,\nor analytical thinking-become more prevalent and valuable as AI adoption grows.\nResults show that AI-focused roles are nearly twice as likely to require skills\nlike resilience, agility, or analytical thinking compared to non-AI roles.\nFurthermore, these skills command a significant wage premium; data scientists,\nfor instance, are offered 5-10% higher salaries if they also possess resilience\nor ethics capabilities. We observe positive spillover effects: a doubling of\nAI-specific demand across industries correlates with a 5% increase in demand\nfor complementary skills, even outside AI-related roles. Conversely, tasks\nvulnerable to AI substitution, such as basic data skills or translation,\nexhibit modest declines in demand. However, the external effect is clearly net\npositive: Complementary effects are up to 1.7x larger than substitution\neffects. These results are consistent across economies, including the United\nKingdom and Australia. Our findings highlight the necessity of reskilling\nworkers in areas where human expertise remains increasingly valuable and\nensuring workers can effectively complement and leverage emerging AI\ntechnologies.",
      "generated_abstract": "e the impact of the COVID-19 pandemic on the production,\ndistribution, and consumption of goods. Using an exogenous shock to the\ndemand for goods, we show that the pandemic leads to a decline in the\ndistribution of goods across countries, with the most severe effects in\nemerging markets. A decrease in the demand for goods increases the production\nof raw materials, particularly coal and oil, and reduces the demand for\nconsumer goods, especially luxury goods. The impact of the pandemic is\ndisproportionately felt in developing countries, where the demand for goods\ndrops significantly, particularly in emerging markets. This results in a\ndiversification of production toward raw materials and a decrease in the\nconsumption of consumer goods, especially luxury goods, in these countries.\nThese findings highlight the impact of the pandemic on the production,\ndistribution, and consumption of goods, with significant",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14450867052023122,
          "p": 0.36764705882352944,
          "f": 0.20746887561715543
        },
        "rouge-2": {
          "r": 0.017543859649122806,
          "p": 0.037037037037037035,
          "f": 0.02380951944727971
        },
        "rouge-l": {
          "r": 0.1329479768786127,
          "p": 0.3382352941176471,
          "f": 0.19087136524371145
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.08013v1",
      "true_abstract": "Most of the existing research on pursuit-evasion game (PEG) is conducted in a\ntwo-dimensional (2D) environment. In this paper, we investigate the PEG in a 3D\nspace. We extend the Apollonius circle (AC) to the 3D space and introduce its\ndetailed analytical form. To enhance the capture efficiency, we derive the\noptimal motion space for both the pursuer and the evader. To address the issue\narising from a discrete state space, we design a fuzzy actor-critic learning\n(FACL) algorithm to obtain the agents' strategies. To improve learning\nperformance, we devise a reward function for the agents, which enables obstacle\navoidance functionality. The effectiveness of the proposed algorithm is\nvalidated through simulation experiments.",
      "generated_abstract": "time control of aerial robots in dynamic environments requires\nthe simultaneous computation of the desired motion and the optimal control\nlaw. A key challenge lies in the lack of a closed-loop controller that\nefficiently minimizes the residual distance to a desired trajectory while\nsatisfying the desired constraints. This paper addresses this challenge by\nproposing a novel architecture that combines a low-level controller with a\nproposed optimal control law, which is based on the design of a multi-layer\nfeedback controller that effectively minimizes the residual distance to the\ndesired trajectory. This design is a combination of the optimal control law\nproposed by Chen et al. (2016) and the feedback controller designed by\nDasgupta et al. (2022) to address the limitations of the former. Simulation\nresults show that the proposed approach significantly outperforms the\noriginal optimal control law proposed",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18518518518518517,
          "p": 0.20270270270270271,
          "f": 0.19354838210697206
        },
        "rouge-2": {
          "r": 0.05555555555555555,
          "p": 0.05217391304347826,
          "f": 0.05381165419775227
        },
        "rouge-l": {
          "r": 0.1728395061728395,
          "p": 0.1891891891891892,
          "f": 0.18064515630052042
        }
      }
    },
    {
      "paper_id": "cs.LG.math/OC/2503.10352v1",
      "true_abstract": "Popular safe Bayesian optimization (BO) algorithms learn control policies for\nsafety-critical systems in unknown environments. However, most algorithms make\na smoothness assumption, which is encoded by a known bounded norm in a\nreproducing kernel Hilbert space (RKHS). The RKHS is a potentially\ninfinite-dimensional space, and it remains unclear how to reliably obtain the\nRKHS norm of an unknown function. In this work, we propose a safe BO algorithm\ncapable of estimating the RKHS norm from data. We provide statistical\nguarantees on the RKHS norm estimation, integrate the estimated RKHS norm into\nexisting confidence intervals and show that we retain theoretical guarantees,\nand prove safety of the resulting safe BO algorithm. We apply our algorithm to\nsafely optimize reinforcement learning policies on physics simulators and on a\nreal inverted pendulum, demonstrating improved performance, safety, and\nscalability compared to the state-of-the-art.",
      "generated_abstract": "We study the following problem: given a set of $n$ points in $\\mathbb{R}^d$,\nfind an $m \\ll n$ set of points such that the set of distances between any two\npoints in this set is as small as possible. We prove that this problem can be\nsolved in polynomial time if the set of points is $O(n^d)$ in size, and\nsublinear time if the set is $O(n^{1-1/d})$ in size. Furthermore, we show that\nthe problem can be solved in polynomial time if the set of points is\n$O(\\log n)$ in size, and sublinear time if the set is $O(\\log^2 n)$ in size.\nThis extends the sublinear time result for $d=2$ to all dimensions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15151515151515152,
          "p": 0.2631578947368421,
          "f": 0.19230768767011847
        },
        "rouge-2": {
          "r": 0.007633587786259542,
          "p": 0.01282051282051282,
          "f": 0.009569373311968587
        },
        "rouge-l": {
          "r": 0.1414141414141414,
          "p": 0.24561403508771928,
          "f": 0.1794871748496056
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2412.02435v1",
      "true_abstract": "In approval-based budget division, a budget needs to be distributed to some\ncandidates based on the voters' approval ballots over these candidates. In the\npursuit of simple, well-behaved, and approximately fair rules for this setting,\nwe introduce the class of sequential payment rules, where each voter controls a\npart of the budget and repeatedly spends his share on his approved candidates\nto determine the final distribution. We show that all sequential payment rules\nsatisfy a demanding population consistency notion and we identify two\nparticularly appealing rules within this class called the maximum payment rule\n(MP) and the $\\frac{1}{3}$-multiplicative sequential payment rule\n($\\frac{1}{3}$-MP). More specifically, we prove that (i) MP is, apart from one\nother rule, the only monotonic sequential payment rule and gives a\n$2$-approximation to a fairness notion called average fair share, and (ii)\n$\\frac{1}{3}$-MP gives a $\\frac{3}{2}$-approximation to average fair share,\nwhich is optimal among sequential payment rules.",
      "generated_abstract": "uce the concept of a \"future-proofed\" algorithm for the\nproblem of allocating goods to agents in a mechanism design problem. A\n\"future-proofed\" algorithm is one that, in the worst-case, does not change its\noutput in response to the feedback received from any agent. We show that a\nfuture-proofed mechanism must be an exact mechanism, and that it must be a\nfeedback-free mechanism. We also show that future-proofed mechanisms are\nsuper-exact, and super-feedback-free, and that their output is a unique\noutput. We establish a connection between future-proofed mechanisms and\noptimal-output algorithms. Finally, we show that future-proofed mechanisms are\nmore efficient than optimal-output mechanisms. We provide an efficient\nimplementation of a future-proofed mechanism for the problem of\nallocating goods to agents in the class of linear",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1595744680851064,
          "p": 0.23076923076923078,
          "f": 0.18867924044934947
        },
        "rouge-2": {
          "r": 0.02877697841726619,
          "p": 0.0392156862745098,
          "f": 0.03319501586474134
        },
        "rouge-l": {
          "r": 0.13829787234042554,
          "p": 0.2,
          "f": 0.16352200774494696
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.15849v1",
      "true_abstract": "Western music is an innately hierarchical system of interacting levels of\nstructure, from fine-grained melody to high-level form. In order to analyze\nmusic compositions holistically and at multiple granularities, we propose a\nunified, hierarchical meta-representation of musical structure called the\nstructural temporal graph (STG). For a single piece, the STG is a data\nstructure that defines a hierarchy of progressively finer structural musical\nfeatures and the temporal relationships between them. We use the STG to enable\na novel approach for deriving a representative structural summary of a music\ncorpus, which we formalize as a dually NP-hard combinatorial optimization\nproblem extending the Generalized Median Graph problem. Our approach first\napplies simulated annealing to develop a measure of structural distance between\ntwo music pieces rooted in graph isomorphism. Our approach then combines the\nformal guarantees of SMT solvers with nested simulated annealing over\nstructural distances to produce a structurally sound, representative centroid\nSTG for an entire corpus of STGs from individual pieces. To evaluate our\napproach, we conduct experiments verifying that structural distance accurately\ndifferentiates between music pieces, and that derived centroids accurately\nstructurally characterize their corpora.",
      "generated_abstract": "ual synchronization (AVS) is a critical component in enabling\naudio-visual applications in virtual and mixed reality (VR/AR). However,\nexisting AVS methods are often limited by their reliance on manual control\nparameters, which hinders their scalability and generalization to diverse\nenvironments. In this paper, we propose the Adaptive Synchronization\nFramework (ASF) for Audio-Visual Synchronization (AVSS), which addresses this\nchallenge by introducing a novel adaptive strategy that adapts to the\nenvironmental changes through a multi-stage optimization process. First, we\nintroduce an audio-visual synchronization controller to synchronize audio and\nvisual signals. Then, to capture the environmental changes, we propose a\nmulti-stage adaptive optimization strategy to enhance the robustness of the\ncontroller. Finally, we integrate the controller with a generative adversarial\nnetwork (GAN",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1487603305785124,
          "p": 0.21176470588235294,
          "f": 0.17475727670609875
        },
        "rouge-2": {
          "r": 0.022222222222222223,
          "p": 0.03508771929824561,
          "f": 0.027210879605720665
        },
        "rouge-l": {
          "r": 0.14049586776859505,
          "p": 0.2,
          "f": 0.16504853884202106
        }
      }
    },
    {
      "paper_id": "cs.GR.cs/GR/2503.09864v1",
      "true_abstract": "Recent advances in text-to-image (T2I) diffusion models have enabled\nremarkable control over various attributes, yet precise color specification\nremains a fundamental challenge. Existing approaches, such as ColorPeel, rely\non model personalization, requiring additional optimization and limiting\nflexibility in specifying arbitrary colors. In this work, we introduce\nColorWave, a novel training-free approach that achieves exact RGB-level color\ncontrol in diffusion models without fine-tuning. By systematically analyzing\nthe cross-attention mechanisms within IP-Adapter, we uncover an implicit\nbinding between textual color descriptors and reference image features.\nLeveraging this insight, our method rewires these bindings to enforce precise\ncolor attribution while preserving the generative capabilities of pretrained\nmodels. Our approach maintains generation quality and diversity, outperforming\nprior methods in accuracy and applicability across diverse object categories.\nThrough extensive evaluations, we demonstrate that ColorWave establishes a new\nparadigm for structured, color-consistent diffusion-based image synthesis.",
      "generated_abstract": "We introduce the first model that learns an accurate 3D reconstruction of\ninteractive virtual objects by integrating both visual and semantic cues. Our\nmodel, named S-V-O-I-C-Net, uses a self-supervised learning (SSL) method to\nlearn the 3D visual content of virtual objects, and a cross-attention mechanism\nto capture semantic information from the interactive world. We demonstrate that\nS-V-O-I-C-Net outperforms existing methods in 3D reconstruction accuracy and\nperformance on various datasets. Our code is available at https://github.com/yunzhu2025/S-V-O-I-C-Net.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15517241379310345,
          "p": 0.3050847457627119,
          "f": 0.2057142812447348
        },
        "rouge-2": {
          "r": 0.022058823529411766,
          "p": 0.04054054054054054,
          "f": 0.028571424007256967
        },
        "rouge-l": {
          "r": 0.13793103448275862,
          "p": 0.2711864406779661,
          "f": 0.18285713838759193
        }
      }
    },
    {
      "paper_id": "cond-mat.other.cond-mat/other/2503.04533v1",
      "true_abstract": "In this article it is argued that synthetic axions, emergent collective\nexcitations in topological insulators or Weyl semimetals hybridize with the\ncosmological axion, a compelling dark matter candidate via a common two photon\ndecay channel since they both couple to electromagnetic fields via a\nChern-Simons term. We point out an analogy to a V-type three level system with\nthe two upper levels identified with the synthetic and cosmological axions\ndecaying into a two-photon state. The Weisskopf-Wigner theory of spontaneous\ndecay in multi level atoms is complemented and extended to describe the\ndynamics of hybridization. The final two-photon state features both kinematic\nand polarization entanglement and displays quantum beats as a consequence of\nthe interference between the decay paths. An initial population of either axion\ninduces a population of the other via hybridization. Consequently, a dark\nmatter axion condensate induces a condensate of the synthetic axion, albeit\nwith small amplitude. We obtain a momentum and polarization resolved Hanbury-\nBrown Twiss (HBT) second order coherence describing coincident correlated\ntwo-photon detection. It exhibits quantum beats with a frequency given by the\ndifference between the energies of the synthetic and cosmological axion and\n\\emph{perhaps may be harnessed} to detect either type of axion excitations. The\ncase of synthetic axions individually is obtained in the limit of vanishing\ncoupling of the cosmological axion and features similar two-photon\ncorrelations. Hence second order (HBT) two-photon coherence \\emph{may} provide\nan alternative detection mechanism for emergent condensed matter axionic\ncollective excitations. Similarities and differences with parametrically down\nconverted photons are discussed.",
      "generated_abstract": "igate the effect of anisotropy on the spin transport properties of\nstrontium titanate (SrTiO$_3$) films grown on silicon substrates. We use\nsingle-crystal neutron diffraction to investigate the crystal structure and\nmagnetic ordering of the films. We show that the spin textures are anisotropic\nand the anisotropy is controlled by the crystallographic orientation of the\nsubstrate. We perform magnetoresistance measurements and find that the\nmagnetoresistance depends on the crystallographic orientation of the\nsubstrate. We interpret this dependence on the crystallographic orientation of\nthe substrate as evidence for the presence of magnetic impurities. Our results\nshow that anisotropy in the spin textures and the crystallographic orientation\nof the substrate affect the spin transport properties of SrTiO$_3$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.087248322147651,
          "p": 0.22807017543859648,
          "f": 0.1262135882302763
        },
        "rouge-2": {
          "r": 0.013157894736842105,
          "p": 0.03571428571428571,
          "f": 0.019230765295858793
        },
        "rouge-l": {
          "r": 0.0738255033557047,
          "p": 0.19298245614035087,
          "f": 0.106796112502121
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2409.06877v1",
      "true_abstract": "We present some results helpful for parameterising positive equilibria, and\nbounding the number of positive nondegenerate equilibria, in mass action\nnetworks. Any mass action network naturally gives rise to a set of polynomial\nequations whose positive solutions are precisely the positive equilibria of the\nnetwork. Here we derive alternative systems of equations, often also\npolynomial, whose solutions are in smooth, one-to-one correspondence with\npositive equilibria of the network. Often these alternative systems are simpler\nthan the original mass action equations, and allow us to infer useful bounds on\nthe number of positive equilibria. The alternative equation systems can also be\nhelpful for parameterising the equilibrium set explicitly, for deriving\ndescriptions of the parameter regions for multistationarity, and for studying\nbifurcations. We present the main construction, some bounds which follow for\nparticular classes of networks, numerous examples, and some open questions and\nconjectures.",
      "generated_abstract": "uce a new approach to the study of multiscale gene regulatory networks\n(GRNs) by using the notion of a \"crude\" regulatory module. Crude regulatory\nmodules are subsets of genes in a GRN that are connected in a way that is\nunlikely to be a consequence of a particular regulatory mechanism. We show\nthat crude regulatory modules can be detected and analyzed in a\nhigh-dimensional, noisy, and incomplete manner. Crude regulatory modules are\ndetermined using a combination of \"inference\" and \"verification\" techniques.\nInference methods rely on a set of mathematical models that provide a\ncomprehensive characterization of the underlying GRN. Verification methods\ninvolve a set of heuristic criteria that can be used to assess the\nincompleteness and the noise in the information contained in the inference\nresults. We provide theoretical guarantees for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1927710843373494,
          "p": 0.21052631578947367,
          "f": 0.20125785664491133
        },
        "rouge-2": {
          "r": 0.024,
          "p": 0.025210084033613446,
          "f": 0.02459015893745063
        },
        "rouge-l": {
          "r": 0.1686746987951807,
          "p": 0.18421052631578946,
          "f": 0.1761006239405088
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/SR/2503.09699v1",
      "true_abstract": "We introduce a novel approach to detecting microlensing events and other\ntransients in light curves, utilising the isolation forest (iForest) algorithm\nfor anomaly detection. Focusing on the Legacy Survey of Space and Time by the\nVera C. Rubin Observatory, we show that an iForest trained on signal-less light\ncurves can efficiently identify microlensing events by different types of dark\nobjects and binaries, as well as variable stars. We further show that the\niForest has real-time applicability through a drip-feed analysis, demonstrating\nits potential as a valuable tool for LSST alert brokers to efficiently\nprioritise and classify transient candidates for follow-up observations.",
      "generated_abstract": "t an improved algorithm for the identification of faint sources in\nradial-velocity surveys. Our algorithm, named RVSift, is based on the\nK-means++ clustering algorithm, which has been shown to be effective in\nidentifying stars and galaxies from the CANDELS-G13 data release, with the\naddition of a new robustness measure that penalises outliers. We developed RVSift\nusing the same data as the original K-means++ algorithm, which was then\nevaluated on the CANDELS-G13 data release. RVSift outperforms K-means++ in\nidentifying faint sources, with an average of 10.8 stars per pixel for the\nCANDELS-G13 data release compared to 8.9 stars per pixel for the original K-means\n++ algorithm. We also present a new version of the algorithm,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19480519480519481,
          "p": 0.2112676056338028,
          "f": 0.2027026977109205
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.02,
          "f": 0.02020201520253159
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.19718309859154928,
          "f": 0.189189184197407
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.cond-mat/supr-con/2503.09709v1",
      "true_abstract": "We study the interplay between quantum geometry, interactions, and external\nfields in complex band systems. When Wannier obstructions preclude a\ndescription based solely on atomic-like orbitals,this complicates the\nprediction of electromagnetic responses particularly in the presence of\ndisorder and interactions. In this work, we introduce a generalized Peierls\nsubstitution framework based on Lagrange multipliers to enforce the constraints\nof the Wannier obstruction in the band of interest. Thus we obtain effective\ndescriptions of interactions and disorder in the presence of non-trivial\nquantum geometry of that band. We apply our approach to examples including the\ndiamagnetic response in flat-band superconductors and delocalization effects in\nflat-band metals caused by interactions and disorder.",
      "generated_abstract": "the first observation of a phase transition in the non-equilibrium\nstress-induced magnetization of a single layer of magnetic tunnel junctions\n(STJ2s). In the regime of weak magnetic fields, the magnetization of the\nnon-magnetic STJ2s is driven by the applied electric field, which results in a\nsignificant in-plane magnetization. This behavior is accompanied by an\nenhancement of the in-plane magnetization, which is not observed in the\nmagnetization of the out-of-plane magnetization. The onset of the\nmagnetization-induced in-plane magnetization transition is accompanied by a\nsignificant change in the magnetic susceptibility. We argue that the\nenhancement of the in-plane magnetization is a signature of a phase transition\nfrom the magnetization of the out-of-plane magnetization to the magnetization\nof",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.16981132075471697,
          "f": 0.14399999511552017
        },
        "rouge-2": {
          "r": 0.0196078431372549,
          "p": 0.022988505747126436,
          "f": 0.02116401619551641
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.1509433962264151,
          "f": 0.12799999511552015
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/AP/2503.08902v1",
      "true_abstract": "Mutual Information (MI) is a crucial measure for capturing dependencies\nbetween variables, but exact computation is challenging in high dimensions with\nintractable likelihoods, impacting accuracy and robustness. One idea is to use\nan auxiliary neural network to train an MI estimator; however, methods based on\nthe empirical distribution function (EDF) can introduce sharp fluctuations in\nthe MI loss due to poor out-of-sample performance, destabilizing convergence.\nWe present a Bayesian nonparametric (BNP) solution for training an MI estimator\nby constructing the MI loss with a finite representation of the Dirichlet\nprocess posterior to incorporate regularization in the training process. With\nthis regularization, the MI loss integrates both prior knowledge and empirical\ndata to reduce the loss sensitivity to fluctuations and outliers in the sample\ndata, especially in small sample settings like mini-batches. This approach\naddresses the challenge of balancing accuracy and low variance by effectively\nreducing variance, leading to stabilized and robust MI loss gradients during\ntraining and enhancing the convergence of the MI approximation while offering\nstronger theoretical guarantees for convergence. We explore the application of\nour estimator in maximizing MI between the data space and the latent space of a\nvariational autoencoder. Experimental results demonstrate significant\nimprovements in convergence over EDF-based methods, with applications across\nsynthetic and real datasets, notably in 3D CT image generation, yielding\nenhanced structure discovery and reduced overfitting in data synthesis. While\nthis paper focuses on generative models in application, the proposed estimator\nis not restricted to this setting and can be applied more broadly in various\nBNP learning procedures.",
      "generated_abstract": "We propose a novel method for generating synthetic data that captures\nthe complex interplay between the distribution of observed data and the\ndynamics of the underlying process. The method is based on the idea of\n\"dynamic synthesis\", which combines data augmentation and a diffusion model\nthat captures the evolution of the data. We demonstrate the effectiveness of\nour approach through various synthetic and real-world applications. Our\napproach can be seen as a more flexible and powerful alternative to traditional\napproaches that rely on a single model, such as density estimation,\nconditional generation, and generative adversarial networks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1566265060240964,
          "p": 0.3939393939393939,
          "f": 0.22413792696343643
        },
        "rouge-2": {
          "r": 0.0371900826446281,
          "p": 0.09782608695652174,
          "f": 0.0538922115773247
        },
        "rouge-l": {
          "r": 0.13855421686746988,
          "p": 0.3484848484848485,
          "f": 0.19827585799791922
        }
      }
    },
    {
      "paper_id": "math.DG.math/AT/2503.08457v1",
      "true_abstract": "This paper explores foliated differential graded algebras (dga) and their\nrole in extending fundamental theorems of differential geometry to foliations.\nWe establish an $A_{\\infty}$ de Rham theorem for foliations, demonstrating that\nthe classical quasi-isomorphism between singular cochains and de Rham forms\nlifts to an $A_{\\infty}$ quasi-isomorphism in the foliated setting.\nFurthermore, we investigate the Riemann-Hilbert correspondence for foliations,\nbuilding upon the established higher Riemann-Hilbert correspondence for\nmanifolds. By constructing an integration functor, we prove a higher\nRiemann-Hilbert correspondence for foliations, revealing an equivalence between\n$\\infty$-representations of $L_{\\infty}$-algebroids and\n$\\infty$-representations of Lie $\\infty$-groupoids within the context of\nfoliations. This work generalizes the classical Riemann-Hilbert correspondence\nto foliations, providing a deeper understanding of the relationship between\nrepresentations of Lie algebroids and Lie groupoids in this framework.",
      "generated_abstract": "aper, we prove that the generalized $L^p$-norm on the space of\ngeneralized functions\n$\\mathcal{F}_p(C_0^{\\infty}(\\mathbb{R}^d))$ is a seminorm. We also show that\nthe generalized $L^p$-norm on the space of generalized functions\n$\\mathcal{F}_p(C_0^{\\infty}(\\mathbb{R}^d))$ is coercive, which implies that the\nspace of generalized functions $\\mathcal{F}_p(C_0^{\\infty}(\\mathbb{R}^d))$ is\na Banach space. As a consequence, we obtain the following result.\n\\begin{Theorem}\n  The space of generalized functions $\\mathcal{F}_p(C_0^{\\infty}(\\mathbb{R}^d))$\n  is a Banach space.\n\\end{Theorem}\n  \\section{Preliminaries}\\label{sec:pre",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0945945945945946,
          "p": 0.22580645161290322,
          "f": 0.13333332917188223
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.05128205128205128,
          "f": 0.026845633719202396
        },
        "rouge-l": {
          "r": 0.0945945945945946,
          "p": 0.22580645161290322,
          "f": 0.13333332917188223
        }
      }
    },
    {
      "paper_id": "math.NT.cs/DM/2503.10158v1",
      "true_abstract": "Integral linear systems $Ax=b$ with matrices $A$, $b$ and solutions $x$ are\nalso required to be in integers, can be solved using invariant factors of $A$\n(by computing the Smith Canonical Form of $A$). This paper explores a new\nproblem which arises in applications, that of obtaining conditions for solving\nthe Modular Linear System $Ax=b\\rem n$ given $A,b$ in $\\zz_n$ for $x$ in\n$\\zz_n$ along with the constraint that the value of the linear function\n$\\phi(x)=<w,x>$ is coprime to $n$ for some solution $x$. In this paper we\ndevelop decomposition of the system to coprime moduli $p^{r(p)}$ which are\ndivisors of $n$ and show how such a decomposition simplifies the computation of\nSmith form. This extends the well known index calculus method of computing the\ndiscrete logarithm where the moduli over which the linear system is reduced\nwere assumed to be prime (to solve the reduced systems over prime fields) to\nthe case when the factors of the modulus are prime powers $p^{r(p)}$. It is\nshown how this problem can be addressed effciently using the invariant factors\nand Smith form of the augmented matrix $[A,-p^{r(p)}I]$ and conditions modulo\n$p$ satisfied by $w$, where $p^{r(p)}$ vary over all divisors of $n$ with $p$\nprime.",
      "generated_abstract": "In this paper, we introduce a new method to compute the Hilbert-Schmidt\nIndependence Coefficient (HSC) between two random variables. Our approach\nencompasses both the classical and the recent generalized HSC estimators, and\nemploys a novel technique to reduce the computational burden in the estimation\nof the covariance matrix of the random variables. We derive new results for the\nHSC between two random vectors, and we establish a connection between the\nHSC between two random variables and the so-called \"Cauchy-Born-Weizs\\\"acker\n(CBW) inequality, which is a key ingredient in the proof of the classical\nHSC estimator. Our approach provides a robust and efficient tool for the\nestimation of the HSC between random variables, which has applications in\nvarious fields, including statistics, machine learning, and finance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13157894736842105,
          "p": 0.2112676056338028,
          "f": 0.16216215743228646
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.029411764705882353,
          "f": 0.02061855214794446
        },
        "rouge-l": {
          "r": 0.13157894736842105,
          "p": 0.2112676056338028,
          "f": 0.16216215743228646
        }
      }
    },
    {
      "paper_id": "q-bio.SC.q-bio/SC/2407.15697v1",
      "true_abstract": "Voltage distribution in sub-cellular micro-domains such as neuronal synapses,\nsmall protrusions or dendritic spines regulates the opening and closing of\nionic channels, energy production and thus cellular homeostasis and\nexcitability. Yet how voltage changes at such a small scale in vivo remains\nchallenging due to the experimental diffraction limit, large signal\nfluctuations and the still limited resolution of fast voltage indicators. Here,\nwe study the voltage distribution in nano-compartments using a computational\napproach based on the Poisson-Nernst-Planck equations for the electro-diffusion\nmotion of ions, where inward and outward fluxes are generated between channels.\nWe report a current-voltage (I-V) logarithmic relationship generalizing Nernst\nlaw that reveals how the local membrane curvature modulates the voltage. We\nfurther find that an influx current penetrating a cellular electrolyte can lead\nto perturbations from tens to hundreds of nanometers deep depending on the\nlocal channels organization. Finally, we show that the neck resistance of\ndendritic spines can be completely shunted by the transporters located on the\nhead boundary, facilitating ionic flow. To conclude, we propose that voltage is\nregulated at a subcellular level by channels organization, membrane curvature\nand narrow passages.",
      "generated_abstract": "tion of cellular and organismal metabolism is driven by the\ncontinuous generation of energy through metabolism. This energy is released\ninto the extracellular medium through cellular respiration, a process that\ntransforms carbon dioxide (CO$_2$) and glucose into carbon monoxide (CO),\nfatty acids (FA), and pyruvate. The rate of cellular respiration depends on the\ncellular metabolic state (i.e., ATP production and cellular respiration rate),\nand thus on the cellular state itself. As a result, the extracellular medium is\nnot a steady state, but a dynamic medium, driven by cellular activity and\nmetabolic state. Here, we introduce the concept of a cellular state space,\nwhich provides a framework to understand the relationship between cellular\nstate and extracellular state. This concept is illustrated",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13076923076923078,
          "p": 0.23943661971830985,
          "f": 0.16915422428652768
        },
        "rouge-2": {
          "r": 0.033707865168539325,
          "p": 0.05405405405405406,
          "f": 0.04152248661821632
        },
        "rouge-l": {
          "r": 0.11538461538461539,
          "p": 0.2112676056338028,
          "f": 0.1492537267740899
        }
      }
    },
    {
      "paper_id": "hep-ex.physics/ins-det/2503.09392v1",
      "true_abstract": "The muon anomalous magnetic moment, $a_\\mu=\\frac{g-2}{2}$, is a low-energy\nobservable which can be both measured and computed to high precision, making it\na sensitive test of the Standard Model and a probe for new physics. This\nanomaly was measured with a precision of $0.20$~parts per million (ppm) by the\nFermilab's Muon g-2 (E989) experiment. The final goal of the E989 experiment is\nto reach a precision of $0.14$~ppm. The experiment is based on the measurement\nof the muon spin anomalous precession frequency, $\\omega_a$, based on the\narrival time distribution of high-energy decay positrons observed by 24\nelectromagnetic calorimeters, placed around the inner circumference of a $14$~m\ndiameter storage ring, and on the precise knowledge of the storage ring\nmagnetic field and of the beam time and space distribution. Achieving this\nlevel of precision requires strict control over systematics, which is ensured\nthrough several diagnostic devices. At the accelerator level, these devices\nmonitor the quality of the injected beam (e.g., verifying that it has the\ncorrect momentum), while at the detector level, they track both the magnetic\nfield and the gain of the calorimeters. In this work the devices and techniques\nused by the E989 experiment will be presented.",
      "generated_abstract": "h for the WIMP-nucleus scattering cross section, the WIMP-nucleon\nscattering cross section, or both is a central component of dark matter\nexperiments, with the goal of measuring the dark matter cross section in\nunits of $10^{-26} \\, \\mathrm{cm}^2$ with a precision of $10^{-40} \\, \\mathrm{cm}^2$\n(e.g., CRESST-II) or $10^{-45} \\, \\mathrm{cm}^2$ (e.g., XENON1T). For the\nWIMP-nucleus scattering, the goal is to measure the scattering angle distribution\nwithin 2-3\\%, and for the WIMP-nucleon scattering, the goal is to measure the\nreaction rate with at least 20-3",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.128,
          "p": 0.32653061224489793,
          "f": 0.18390804193090246
        },
        "rouge-2": {
          "r": 0.04371584699453552,
          "p": 0.11428571428571428,
          "f": 0.0632411027168057
        },
        "rouge-l": {
          "r": 0.12,
          "p": 0.30612244897959184,
          "f": 0.17241378905733923
        }
      }
    },
    {
      "paper_id": "cs.CG.math/MG/2503.01988v1",
      "true_abstract": "Metric spaces defined within convex polygons, such as the Thompson, Funk,\nreverse Funk, and Hilbert metrics, are subjects of recent exploration and study\nin computational geometry. This paper contributes an educational piece of\nsoftware for understanding these unique geometries while also providing a tool\nto support their research. We provide dynamic software for manipulating the\nFunk, reverse Funk, and Thompson balls in convex polygonal domains.\nAdditionally, we provide a visualization program for traversing the Hilbert\npolygonal geometry.",
      "generated_abstract": "We show that the set of all paths in a 3-connected finite graph with\nedge weights is the same as the set of all minimal paths in the graph. We also\nshow that if a graph has a path of length 3, then it is either a path or a\ncycle. We also show that if a graph has no path of length 3, then it is\ncontractible.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1206896551724138,
          "p": 0.21212121212121213,
          "f": 0.15384614922352388
        },
        "rouge-2": {
          "r": 0.013888888888888888,
          "p": 0.020833333333333332,
          "f": 0.016666661866668046
        },
        "rouge-l": {
          "r": 0.1206896551724138,
          "p": 0.21212121212121213,
          "f": 0.15384614922352388
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2411.12375v3",
      "true_abstract": "In this paper, we introduce a novel pricing model for Uniswap V3, built upon\nstochastic processes and the Martingale Stopping Theorem. This model\ninnovatively frames the valuation of positions within Uniswap V3. We further\nconduct a numerical analysis and examine the sensitivities through Greek risk\nmeasures to elucidate the model's implications. The results underscore the\nmodel's significant academic contribution and its practical applicability for\nUniswap liquidity providers, particularly in assessing risk exposure and\nguiding hedging strategies.",
      "generated_abstract": "r introduces a novel approach to the pricing of credit default\nopportunities (CDOs) and other financial instruments. The methodology is based\non the use of a generalised diffusion model to describe the dynamics of the\nunderlying security. A key feature of the proposed framework is the\nelimination of the need for the existence of a certain correlation structure\nbetween the underlying security and the credit default risk. This approach is\nbased on the assumption that the correlation structure of the underlying\nsecurity is uncorrelated with the correlation structure of the credit default\nrisk. In addition, the model is characterised by a set of non-linear equations\ndescribing the dynamics of the diffusion coefficient. These equations can be\nsolved numerically to obtain the probability density functions of the\nunderlying security and the credit default risk. The model is applied to the\nCDO pricing problem, where the underlying security is the credit default swap",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20967741935483872,
          "p": 0.18055555555555555,
          "f": 0.19402984577411464
        },
        "rouge-2": {
          "r": 0.0273972602739726,
          "p": 0.018018018018018018,
          "f": 0.021739125648039807
        },
        "rouge-l": {
          "r": 0.20967741935483872,
          "p": 0.18055555555555555,
          "f": 0.19402984577411464
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/QM/2502.16660v3",
      "true_abstract": "The applications of large language models (LLMs) in various biological\ndomains have been explored recently, but their reasoning ability in complex\nbiological systems, such as pathways, remains underexplored, which is crucial\nfor predicting biological phenomena, formulating hypotheses, and designing\nexperiments. This work explores the potential of LLMs in pathway reasoning. We\nintroduce BioMaze, a dataset with 5.1K complex pathway problems derived from\nreal research, covering various biological contexts including natural dynamic\nchanges, disturbances, additional intervention conditions, and multi-scale\nresearch targets. Our evaluation of methods such as CoT and graph-augmented\nreasoning, shows that LLMs struggle with pathway reasoning, especially in\nperturbed systems. To address this, we propose PathSeeker, an LLM agent that\nenhances reasoning through interactive subgraph-based navigation, enabling a\nmore effective approach to handling the complexities of biological systems in a\nscientifically aligned manner. The dataset and code are available at\nhttps://github.com/zhao-ht/BioMaze.",
      "generated_abstract": "ning has been revolutionizing the field of protein structure\nrepresentation, with the most prominent architectures being transformer-based\nmodels. While these models have demonstrated strong performance in protein\nstructure prediction, their ability to represent protein complexes and\ninteractions remains limited. In this work, we investigate the application of\ntransformer-based models to protein complex prediction and interaction\nrepresentation. By integrating the structural information of the protein\ncomplex with the structural information of the interacting proteins, we find\nthat the transformer-based models outperform the traditional GPT-2 models in\nboth task performance and interpretability. Additionally, we demonstrate that\nthe transformer-based models can also effectively represent protein complexes\nand interactions in protein sequence representations. This work highlights the\npotential of transformer-based models in protein complex prediction and\ninteraction representation, providing a new framework for understanding protein\nfunction and designing new computational models for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1875,
          "p": 0.2692307692307692,
          "f": 0.2210526267390583
        },
        "rouge-2": {
          "r": 0.028368794326241134,
          "p": 0.03571428571428571,
          "f": 0.03162054842537845
        },
        "rouge-l": {
          "r": 0.16071428571428573,
          "p": 0.23076923076923078,
          "f": 0.18947367937063725
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.06538v1",
      "true_abstract": "In two-way contingency tables under an asymmetric situation, where the row\nand column variables are defined as explanatory and response variables\nrespectively, quantifying the extent to which the explanatory variable\ncontributes to predicting the response variable is important. One\nquantification method is the association measure, which indicates the degree of\nassociation in a range from $0$ to $1$. Among various measures, those based on\nproportional reduction in error (PRE) are particularly notable for their\nsimplicity and intuitive interpretation. These measures, including\nGoodman-Kruskal's lambda proposed in 1954, are widely implemented in\nstatistical software such as R and SAS and remain extensively used. However, a\nknown limitation of PRE measures is their potential to return a value of $0$\ndespite no independence. This issue arises because the measures are constructed\nbased solely on the maximum joint and marginal probabilities, failing to\nutilize the information available in the contingency table fully. To address\nthis problem, we propose new association measures designed for the proportional\nreduction in error with multiple categories. The properties of the proposed\nmeasure are examined and their utility is demonstrated through simulations and\nreal data analyses. The results suggest their potential as practical tools in\napplied statistics.",
      "generated_abstract": "nt paper develops an iterative approach for estimating the\nrandom effects in the generalized linear mixed model (GLMM) that is\nflexible, efficient, and interpretable. The proposed approach is based on the\nproposed iterative algorithm (PIA) and is applied to the generalized linear\nadditive model (GAM) for longitudinal data. PIA is a novel iterative\nprocedure for estimating the random effects in the GAM. PIA is a\nnon-conjugate estimator for the random effects. It is based on a\ncombination of the proposed algorithm (PIA) and the estimator proposed by\nWang (2016). We show that PIA is an efficient estimator, and we prove that the\nestimator is consistent and asymptotically normal. We also show that the\nestimator is equivalent to the GAM for the fixed effect, which is a\nwell-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1297709923664122,
          "p": 0.2698412698412698,
          "f": 0.17525772757306846
        },
        "rouge-2": {
          "r": 0.03125,
          "p": 0.06060606060606061,
          "f": 0.04123710891274361
        },
        "rouge-l": {
          "r": 0.12213740458015267,
          "p": 0.25396825396825395,
          "f": 0.164948449222553
        }
      }
    },
    {
      "paper_id": "hep-ph.nucl-th/2503.10157v1",
      "true_abstract": "In this work, we perform a systematic study on the multiplicity dependence of\nhadron productions at mid-rapidity ($|y|<0.5$), ranging from the light to the\ncharm sector in pp collisions at $\\sqrt{s}=13$ TeV. This study utilizes a\nmulti-phase transport model (AMPT) coupled with PYTHIA8 initial conditions. We\nhave investigated the baryon to meson ratios as well as the strange to\nnon-strange meson ratios varying with the charged particle density. By tuning\nthe coalescence parameters, the AMPT model provides a reasonable description to\nthe experimental data for inclusive productions of both light and charm\nhadrons, comparable to the string fragmentation model calculations with color\nreconnection effects. Additionally, we have analyzed the relative production of\nhadrons by examining self-normalized particle ratios as a function of the\ncharged hadron density. Our findings suggest that parton evolution effects and\nthe coalescence hadronization process in AMPT model lead to a strong flavor\nhierarchy in the multiplicity dependence of the baryon to meson ratio.\nFurthermore, our investigation on the $p_T$ differential double ratio of baryon\nto meson fraction between high and low multiplicity events indicates distinct\nmodifications to the flavor associated baryon to meson ratio $p_T$ shape in\nhigh multiplicity events when comparing the coalescence hadronization model to\nthe color reconnection model. These observations highlight the importance of\nunderstanding the hadronization process in high-energy proton-proton collisions\nthrough a comprehensive multiplicity dependent multi-flavor analysis.",
      "generated_abstract": "e the possibility of using a pion-nucleus scattering experiment to\ndetermine the nucleon-nucleon scattering length $a_N$ by measuring the ratio\nof the pion-nucleon scattering cross section to the nucleon-nucleon scattering\ncross section. In this approach, the ratio is determined from the ratio of the\ntotal cross section to the pion-nucleon cross section, which is a measure of\nthe total nuclear interaction strength. We present a detailed analysis of the\nnucleon-nucleon scattering cross section, in particular focusing on the\nsystematic uncertainties that arise from the pion-nucleon scattering cross\nsection and the nuclear response function. We conclude that the present\nuncertainties in the pion-nucleon scattering cross section are such that\ndetermination",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13178294573643412,
          "p": 0.3269230769230769,
          "f": 0.1878452997722903
        },
        "rouge-2": {
          "r": 0.04040404040404041,
          "p": 0.1,
          "f": 0.05755395273536596
        },
        "rouge-l": {
          "r": 0.10077519379844961,
          "p": 0.25,
          "f": 0.14364640474466603
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.00800v1",
      "true_abstract": "The article examines the impact of 16 key parameters of the Georgian economy\non economic inequality, using the Perelman model and Ricci flow mathematical\nmethods. The study aims to conduct a deep analysis of the impact of\nsocio-economic challenges and technological progress on the dynamics of the\nGini coefficient. The article examines the following parameters: income\ndistribution, productivity (GDP per hour), unemployment rate, investment rate,\ninflation rate, migration (net negative), education level, social mobility,\ntrade infrastructure, capital flows, innovative activities, access to\nhealthcare, fiscal policy (budget deficit), international trade (turnover\nrelative to GDP), social protection programs, and technological access. The\nresults of the study confirm that technological innovations and social\nprotection programs have a positive impact on reducing inequality. Productivity\ngrowth, improving the quality of education, and strengthening R&D investments\nincrease the possibility of inclusive development. Sensitivity analysis shows\nthat social mobility and infrastructure are important factors that affect\neconomic stability. The accuracy of the model is confirmed by high R^2 values\n(80-90%) and the statistical reliability of the Z-statistic (<0.05). The study\nuses Ricci flow methods, which allow for a geometric analysis of the\ntransformation of economic parameters in time and space. Recommendations\ninclude the strategic introduction of technological progress, the expansion of\nsocial protection programs, improving the quality of education, and encouraging\ninternational trade, which will contribute to economic sustainability and\nreduce inequality. The article highlights multifaceted approaches that combine\ntechnological innovation and responses to socio-economic challenges to ensure\nsustainable and inclusive economic development.",
      "generated_abstract": "This paper provides a framework for deriving efficient estimators of\nparameter functions in a regression framework that is based on a\nnon-parametric representation. The estimator is based on a\nsemiparametric representation, and is obtained via a two-stage\nsemiparametric estimator. The first stage estimates the parameter function by\napproximating it with a linear combination of a linear function and a\nsemiparametric component, and the second stage estimates the linear component\nby maximizing a novel semiparametric likelihood. The resulting estimator is\nefficient in the sense that it achieves a rate of convergence that is\nsuperior to the rate of convergence of a non-parametric estimator in the same\nsetting.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0821917808219178,
          "p": 0.2222222222222222,
          "f": 0.11999999605800013
        },
        "rouge-2": {
          "r": 0.004524886877828055,
          "p": 0.01098901098901099,
          "f": 0.00641025227831463
        },
        "rouge-l": {
          "r": 0.07534246575342465,
          "p": 0.2037037037037037,
          "f": 0.10999999605800015
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2410.18062v1",
      "true_abstract": "Undergraduate graders are frequently important contributors to the teaching\nteam in post-secondary education settings. This study set out to investigate\nagreement for a team of undergraduate graders as they acquired training and\nexperience for scoring responses to open-ended tasks. Results demonstrate\ncompelling evidence that undergraduate students can develop the ability to\nestablish and sustain substantial agreement with an instructor, especially when\nequipped with proper training and a high-quality scoring rubric.",
      "generated_abstract": "We propose a novel method to identify the optimal number of clusters in a\nclustered data set. Our method is based on the principle that the distribution\nof clustered data is more similar to a multivariate Gaussian than to a\nmultivariate normal distribution. We derive the asymptotic distribution of the\noptimal number of clusters and provide a computationally efficient method to\nestimate this number. In simulation studies, we compare the performance of our\nproposed method against existing approaches in terms of their ability to\ndetect outliers and to cluster data in a way that is consistent with the\ndistribution of the data. In a real data example, we apply the proposed method\nto cluster 34,223 genes in a large cancer dataset.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.15384615384615385,
          "f": 0.16666666170138905
        },
        "rouge-2": {
          "r": 0.014705882352941176,
          "p": 0.009708737864077669,
          "f": 0.011695901642216658
        },
        "rouge-l": {
          "r": 0.14545454545454545,
          "p": 0.12307692307692308,
          "f": 0.13333332836805575
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.09349v1",
      "true_abstract": "Correlation-based auditory attention decoding (AAD) algorithms exploit neural\ntracking mechanisms to determine listener attention among competing speech\nsources via, e.g., electroencephalography signals. The correlation coefficients\nbetween the decoded neural responses and encoded speech stimuli of the\ndifferent speakers then serve as AAD decision variables. A critical trade-off\nexists between the temporal resolution (the decision window length used to\ncompute these correlations) and the AAD accuracy. This trade-off is typically\ncharacterized by evaluating AAD accuracy across multiple window lengths,\nleading to the performance curve. We propose a novel method to model this\ntrade-off curve using labeled correlations from only a single decision window\nlength. Our approach models the (un)attended correlations with a normal\ndistribution after applying the Fisher transformation, enabling accurate AAD\naccuracy prediction across different window lengths. We validate the method on\ntwo distinct AAD implementations: a linear decoder and the non-linear VLAAI\ndeep neural network, evaluated on separate datasets. Results show consistently\nlow modeling errors of approximately 2 percent points, with 94% of true\naccuracies falling within estimated 95%-confidence intervals. The proposed\nmethod enables efficient performance curve modeling without extensive\nmulti-window length evaluation, facilitating practical applications in, e.g.,\nperformance tracking in neuro-steered hearing devices to continuously adapt the\nsystem parameters over time.",
      "generated_abstract": "communication systems are becoming increasingly critical in\ncommunication-intensive applications such as smart cities, vehicular networks,\nand cognitive radio networks. The emergence of large-scale intelligent\ntransportation systems (LISTS) with massive sensing and computing (S&C)\ncapabilities and massive machine-type communications (mMTC) has introduced\nsignificant challenges in wireless networks. This paper presents an iterative\nmethod for optimizing the system design and resource allocation in multi-S&C\nand multi-mMTC LISTS, which is based on a combination of the\nIterative-Quadratic-Optimization (IQO) algorithm and the\nBayesian-Adaptive-Mixed-Integer-Optimization (BA-MIO) method. The IQO\nalgorithm is used to solve the sub-problems that are constrained by the\nsystem",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12080536912751678,
          "p": 0.24,
          "f": 0.16071428125996506
        },
        "rouge-2": {
          "r": 0.02030456852791878,
          "p": 0.0425531914893617,
          "f": 0.0274914045611183
        },
        "rouge-l": {
          "r": 0.11409395973154363,
          "p": 0.22666666666666666,
          "f": 0.15178570983139364
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.15505v1",
      "true_abstract": "We develop a dynamic model of the Bitcoin market where users set fees\nthemselves and miners decide whether to operate and whom to validate based on\nthose fees. Our analysis reveals how, in equilibrium, users adjust their bids\nin response to short-term congestion (i.e., the amount of pending\ntransactions), how miners decide when to start operating based on the level of\ncongestion, and how the interplay between these two factors shapes the overall\nmarket dynamics. The miners hold off operating when the congestion is mild,\nwhich harms social welfare. However, we show that a block reward (a fixed\nreward paid to miners upon a block production) can mitigate these\ninefficiencies. We characterize the socially optimal block reward and\ndemonstrate that it is always positive, suggesting that Bitcoin's halving\nschedule may be suboptimal.",
      "generated_abstract": "the economic consequences of the use of a \"crude oil tax\" on\nEU-based energy carriers. Crude oil is the primary input for the production of\ndifferent types of fossil-fuel-based products, including chemicals, metals,\nand plastics. The tax is estimated to raise about EUR 150 billion annually\n(USD 174 billion) for the EU, which would be reinvested in the economy and\nreduce the net external debt. We find that this tax would reduce the\nintermediate-term price of crude oil by about 10%, leading to a 0.1% decrease\nin the EU's GDP in 2030. The tax is estimated to raise about 0.1% of EU GDP in\n10 years, with 7% of this reinvested in the economy. The tax would reduce",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13829787234042554,
          "p": 0.1780821917808219,
          "f": 0.15568861783355462
        },
        "rouge-2": {
          "r": 0.007751937984496124,
          "p": 0.00980392156862745,
          "f": 0.008658003726319781
        },
        "rouge-l": {
          "r": 0.13829787234042554,
          "p": 0.1780821917808219,
          "f": 0.15568861783355462
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/MF/2411.13792v1",
      "true_abstract": "Traditional Markowitz portfolio optimization constrains daily portfolio\nvariance to a target value, optimising returns, Sharpe or variance within this\nconstraint. However, this approach overlooks the relationship between variance\nat different time scales, typically described by $\\sigma(\\Delta t) \\propto\n(\\Delta t)^{H}$ where $H$ is the Hurst exponent, most of the time assumed to be\n\\(\\frac{1}{2}\\). This paper introduces a multifrequency optimization framework\nthat allows investors to specify target portfolio variance across a range of\nfrequencies, characterized by a target Hurst exponent $H_{target}$, or optimize\nthe portfolio at multiple time scales. By incorporating this scaling behavior,\nwe enable a more nuanced and comprehensive risk management strategy that aligns\nwith investor preferences at various time scales. This approach effectively\nmanages portfolio risk across multiple frequencies and adapts to different\nmarket conditions, providing a robust tool for dynamic asset allocation. This\novercomes some of the traditional limitations of Markowitz, when it comes to\ndealing with crashes, regime changes, volatility clustering or multifractality\nin markets. We illustrate this concept with a toy example and discuss the\npractical implementation for assets with varying scaling behaviors.",
      "generated_abstract": "This paper introduces a novel model for optimal allocation of cash\nand derivatives portfolios in a continuous-time financial market. Our\napproach combines a portfolio theory-based dynamic portfolio allocation\nframework with a stochastic volatility model that captures price dynamics.\nTheoretical results demonstrate that our model yields optimal portfolio\nallocation in a wide class of stochastic volatility models and under mild\nassumptions. We also show that the model can be used to analyze the optimal\nallocation of cash and derivatives in a continuous-time financial market with\ninfinite time horizon, which was not previously addressed in the literature.\nFinally, we discuss how our framework can be extended to incorporate other\nasset classes, such as equities and bonds, as well as to address the case of\nmultiple assets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21951219512195122,
          "p": 0.34177215189873417,
          "f": 0.267326727910499
        },
        "rouge-2": {
          "r": 0.022988505747126436,
          "p": 0.03636363636363636,
          "f": 0.028169009338425713
        },
        "rouge-l": {
          "r": 0.2032520325203252,
          "p": 0.31645569620253167,
          "f": 0.24752474771247926
        }
      }
    },
    {
      "paper_id": "math.ST.stat/ME/2502.20123v1",
      "true_abstract": "We study two G-modeling strategies for estimating the signal distribution\n(the empirical Bayesian's prior) from observations corrupted with normal noise.\nFirst, we choose the signal distribution by minimizing Stein's unbiased risk\nestimate (SURE) of the implied Eddington/Tweedie Bayes denoiser, an approach\nmotivated by optimal empirical Bayesian shrinkage estimation of the signals.\nSecond, we select the signal distribution by minimizing Hyv\\\"arinen's score\nmatching objective for the implied score (derivative of log-marginal density),\ntargeting minimal Fisher divergence between estimated and true marginal\ndensities. While these strategies appear distinct, they are known to be\nmathematically equivalent. We provide a unified analysis of SURE and score\nmatching under both well-specified signal distribution classes and\nmisspecification. In the classical well-specified setting with homoscedastic\nnoise and compactly supported signal distribution, we establish nearly\nparametric rates of convergence of the empirical Bayes regret and the Fisher\ndivergence. In a commonly studied misspecified model, we establish fast rates\nof convergence to the oracle denoiser and corresponding oracle inequalities.\nOur empirical results demonstrate competitiveness with nonparametric maximum\nlikelihood in well-specified settings, while showing superior performance under\nmisspecification, particularly in settings involving heteroscedasticity and\nside information.",
      "generated_abstract": "We present a novel method for estimating a multivariate time series\napproach that uses the cross-correlation function to model the joint\nspatiotemporal evolution of a set of spatially correlated variables. We\ninvestigate the robustness of the proposed method to outliers, as well as its\ncapability to capture the inter- and intra-variate relationships in a\nspatiotemporal time series. The method is applied to a real-world dataset\nfrom a major Spanish airport and we demonstrate its robustness to\noutliers, the ability to capture the inter- and intra-variate relationships\nand the ability to detect temporal trends. Furthermore, we analyze the\nefficiency and the computational complexity of the proposed method. The\nresults show that the proposed method outperforms the traditional\ntime-series-based methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.112,
          "p": 0.21212121212121213,
          "f": 0.14659685411584125
        },
        "rouge-2": {
          "r": 0.01764705882352941,
          "p": 0.0297029702970297,
          "f": 0.022140216726352753
        },
        "rouge-l": {
          "r": 0.096,
          "p": 0.18181818181818182,
          "f": 0.12565444573887793
        }
      }
    },
    {
      "paper_id": "physics.ao-ph.stat/AP/2502.19234v1",
      "true_abstract": "Arctic sea ice is in reduction and has been a key significant indicator of\nclimate change. In this paper, we explore Arctic Sea ice extent data to\nidentify teleconnection with weather change in the polar and sub-tropical jet\nstream intersection in eastern United States (US) and hence the potential\ninfluence in ground level ozone pollution. Several statistical methods\nincluding Bayesian techniques such as: spatio-temporal modelling and Bayesian\nnetwork are implemented to identify the teleconnection and also validated based\non theories in atmospheric science. We observe that the teleconnection is\nrelatively strong in autumn, winter and spring seasons compared to the summer.\nFurthermore, the sudden decremental effect of Arctic sea-ice extent in\nmid-2000s has a shifting influence in ozone pollutions compared to the previous\nyears. A similar downward shift in the Arctic sea-ice extent has been projected\nin 2030. These findings indicate to initiate further strategic policies for the\nArctic influence, ozone concentrations together the seasonal and global\nchanging patterns of climate.",
      "generated_abstract": "We study the role of the initial condition in the dynamics of the\ntemperature field in a convective box. We consider the two-dimensional\nthree-level system where the initial condition is an inhomogeneous random\nfield with mean zero. We show that the mean-field dynamics of this system is\ndetermined by the initial condition. We also show that the system is\nnon-ergodic, and the mean-field dynamics is given by the mean-field limit of\nthe original system. Finally, we study the role of the initial condition in the\ndynamics of the temperature field in a three-dimensional convective box. We\nshow that the mean-field dynamics of this system is also determined by the\ninitial condition.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.2926829268292683,
          "f": 0.1610738215143463
        },
        "rouge-2": {
          "r": 0.013422818791946308,
          "p": 0.030303030303030304,
          "f": 0.018604646907951217
        },
        "rouge-l": {
          "r": 0.08333333333333333,
          "p": 0.21951219512195122,
          "f": 0.1208053651385074
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/GN/2503.09934v1",
      "true_abstract": "It is time to move on from attempts to make the pharmacy benefit manager\n(PBM) reseller business model more transparent. Time and time again the Big 3\nPBMs have developed opaque alternatives to piece-meal 100% pass-through\nmandates. Time and time again PBMs have demonstrated expertise in finding\nloopholes in state government disclosure laws. The purpose of this paper is to\nprovide quantitative estimates of two transparent insurance business models as\na solution to the PBM agency issue. The key parameter used is an 8% gross\nprofit margin figure disclosed by the Big 3 PBMs themselves. Based on reported\ndrug trend delivered to plans, we use a $1,200 to $1,500 per member per year\n(PMPY) as the range for this key performance indicator (KPI). We propose that\ndiscussions of PBM insurance business models start with the following figures:\n(1) a fixed premium model with medical loss ratio ranging from 92% to 85%; (2)\na fee-for-service model ranging from $96 to $180 PMPY with risk sharing of\ndeviations from a contracted PMPY delivered drug spend.",
      "generated_abstract": "tudy, we investigate the impact of COVID-19 on the financial\nenvironment of the United States (US) and the impact of the COVID-19 on the\nfinancial environment of the European Union (EU). To achieve this, we employ\nthe financial environment and the financial environment index (FEI), which are\nboth constructed using the same data, to analyze the impact of the COVID-19 on\nthe financial environment. The empirical results show that the financial\nenvironment of the United States and the financial environment of the EU are\nnegatively affected by the COVID-19. However, the financial environment of the\nUnited States and the financial environment of the EU are positively affected\nby the COVID-19, with the FEI of the United States and the FEI of the EU\nexhibiting a positive correlation. The results suggest that the financial\nenvironment of the United States and the financial environment of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09401709401709402,
          "p": 0.21568627450980393,
          "f": 0.13095237672406476
        },
        "rouge-2": {
          "r": 0.012345679012345678,
          "p": 0.028169014084507043,
          "f": 0.017167377736927492
        },
        "rouge-l": {
          "r": 0.09401709401709402,
          "p": 0.21568627450980393,
          "f": 0.13095237672406476
        }
      }
    },
    {
      "paper_id": "math.OC.stat/OT/2403.10987v3",
      "true_abstract": "The Fundamental Risk Quadrangle (FRQ) is a unified framework linking risk\nmanagement, statistical estimation, and optimization. Distributionally robust\noptimization (DRO) based on $\\varphi$-divergence minimizes the maximal expected\nloss, where the maximum is over a $\\varphi$-divergence ambiguity set. This\npaper introduces the \\emph{extended} $\\varphi$-divergence and the extended\n$\\varphi$-divergence quadrangle, which integrates DRO into the FRQ framework.\nWe derive the primal and dual representations of the quadrangle elements (risk,\ndeviation, regret, error, and statistic). The dual representation provides an\ninterpretation of classification, portfolio optimization, and regression as\nrobust optimization based on the extended $\\varphi$-divergence. The primal\nrepresentation offers tractable formulations of these robust optimizations as\nconvex optimization. We provide illustrative examples showing that many common\nproblems, such as least-squares regression, quantile regression, support vector\nmachines, and CVaR optimization, fall within this framework. Additionally, we\nconduct a case study to visualize the optimal solution of the inner\nmaximization in robust optimization.",
      "generated_abstract": "We consider the problem of estimating a function of two variables using a\ndata set with two variables, where the unknown function is expressed as a\ncombination of two functions, and the data set contains two variables. The\nestimator is obtained by solving a mixed-integer linear programming problem\nwith quadratic constraints. The problem is non-convex and has a large number\nof variables. A greedy algorithm is proposed to solve the problem. The\nestimator is shown to be a convex function of the parameters. Numerical\nexperiments are carried out to illustrate the effectiveness of the proposed\nmethod.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11764705882352941,
          "p": 0.21428571428571427,
          "f": 0.1518987296010256
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.034482758620689655,
          "f": 0.026315784754155965
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.21428571428571427,
          "f": 0.1518987296010256
        }
      }
    },
    {
      "paper_id": "physics.class-ph.physics/class-ph/2503.06910v1",
      "true_abstract": "Hidden momentum is a puzzling phenomenon associated with magnetic dipoles and\nother extended relativistic systems. We point out that the origin of hidden\nmomentum lies in the effective change of individual particle masses of a\ncomposite body, during which the total momentum of the system is not equal to\nthe momentum of the center of mass. Defining the hidden momentum as the\ndifference between the total momentum and the center-of-mass momentum, we\nexplain in detail how hidden momentum arises in certain simple non-relativistic\nsystems, in typical relativistic systems due to velocity-dependent\n``relativistic mass'', and in magnetic dipoles as a special case of\nrelativistic systems.",
      "generated_abstract": "The development of quantum technologies, such as quantum computers and\nquantum sensors, has led to significant advances in quantum information\nprocessing. However, the interplay between quantum information processing and\nclassical sensing remains a challenging issue. In this review, we explore\ndifferent quantum sensing techniques, such as quantum sensors based on\nquantum-enhanced sensors, quantum metrology, quantum sensors based on quantum\ncomputing, and quantum sensors based on quantum computers, and discuss the\nimplications of quantum sensing on quantum information processing. We also\naddress the challenges and potential future directions in quantum sensing.\nFurthermore, we discuss the relationship between quantum sensing and\nquantum computing, and explore the prospects of quantum sensing in quantum\ncomputing.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15873015873015872,
          "p": 0.18181818181818182,
          "f": 0.16949152044671087
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.16363636363636364,
          "f": 0.15254236790433798
        }
      }
    },
    {
      "paper_id": "math.OC.q-bio/PE/2502.00509v1",
      "true_abstract": "The main aim of this study is to analyze a fractional parabolic SIR epidemic\nmodel of a reaction-diffusion, by using the nonlocal Caputo fractional\ntime-fractional derivative and employing the $p$-Laplacian operator. The\nimmunity is imposed through the vaccination program, which is regarded as a\ncontrol variable. Finding the optimal control pair that reduces the number of\nsick people, the associated vaccination, and treatment expenses across a\nconstrained time and space is our main study. The existence and uniqueness of\nthe nonnegative solution for the spatiotemporal SIR model are established. It\nis also demonstrated that an optimal control exists. In addition, we obtain a\ndescription of the optimal control in terms of state and adjoint functions.\nThen, the optimality system is resolved by a discrete iterative scheme that\nconverges after an appropriate test, similar to the forward-backward sweep\nmethod. Finally, numerical approximations are given to show the effectiveness\nof the proposed control program, which provides meaningful results using\ndifferent values of the fractional order and $p$, respectively the order of the\nCaputo derivative and the $p$-Laplacian operators.",
      "generated_abstract": "ration of biochemical and structural models in computational\nschemes is a promising approach to simulate complex biological systems.\nHowever, current approaches often fail to fully capture the complex interactions\nbetween molecular and structural components. In this study, we develop a\ntwo-step approach for integrating structural and molecular models in the\nMonte Carlo simulation framework. The first step leverages the structural model\nto determine the location and orientation of atoms in the molecular model,\nallowing for a more accurate representation of the system. The second step\nintegrates the structural model to capture the dynamics of atoms in the\nmolecular model. The approach is demonstrated through simulations of the\nmembrane-spanning protein, TOMM40, using the TOMM40-Molecular model and the\nCrystallographic-Molecular Model (Cryo-EM) structure. By integrating",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1574074074074074,
          "p": 0.2328767123287671,
          "f": 0.18784529905436356
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.027522935779816515,
          "f": 0.021897805427834174
        },
        "rouge-l": {
          "r": 0.14814814814814814,
          "p": 0.2191780821917808,
          "f": 0.17679557529745749
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.20788v1",
      "true_abstract": "The stock assessment model SAM contains a large number of age-dependent\nparameters that must be manually grouped together to obtain robust inference.\nThis can make the model selection process slow, non-extensive and highly\nsubjective, while producing unrealistic looking parameter estimates with\ndiscrete jumps. We propose to model age-dependent SAM parameters using\nsmoothing spline functions. This can lead to more smooth parameter estimates,\nwhile speeding up and making the model selection process more automatic and\nless subjective. We develop different spline models and compare them with\nalready existing SAM models for a selection of 17 different fish stocks, using\ncross- and forward-validation methods. The results show that our automated\nspline models overall outcompete the officially developed SAM models. We also\ndemonstrate how the developed spline models can be employed as a diagnostics\ntool for improving and better understanding properties of the officially\ndeveloped SAM models.",
      "generated_abstract": "st decade, there has been a significant increase in the use of\nstatistical methods to analyze the results of experimental research.\nStatistical methods, such as descriptive and inferential statistics, are\nimportant for understanding the data obtained in experimental research. In\nthis article, we will discuss different statistical methods and their\napplications in experimental research. We will also discuss the advantages and\ndisadvantages of each method. By using these methods, researchers can\ninferential statistics to analyze the results of experimental research. This\narticle is intended for students and researchers in the field of experimental\nresearch. The article is divided into three parts. In the first part, we will\ndiscuss different statistical methods and their applications in experimental\nresearch. We will also discuss the advantages and disadvantages of each\nmethod. In the second part, we will discuss the advantages and disadvantages of\ninferential statistics. In the third",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18681318681318682,
          "p": 0.2463768115942029,
          "f": 0.21249999509453138
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.17582417582417584,
          "p": 0.2318840579710145,
          "f": 0.19999999509453137
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ME/2503.06690v1",
      "true_abstract": "Dynamic Treatment Regimes (DTRs) provide a systematic approach for making\nsequential treatment decisions that adapt to individual patient\ncharacteristics, particularly in clinical contexts where survival outcomes are\nof interest. Censoring-Aware Tree-Based Reinforcement Learning (CA-TRL) is a\nnovel framework to address the complexities associated with censored data when\nestimating optimal DTRs. We explore ways to learn effective DTRs, from\nobservational data. By enhancing traditional tree-based reinforcement learning\nmethods with augmented inverse probability weighting (AIPW) and censoring-aware\nmodifications, CA-TRL delivers robust and interpretable treatment strategies.\nWe demonstrate its effectiveness through extensive simulations and real-world\napplications using the SANAD epilepsy dataset, where it outperformed the\nrecently proposed ASCL method in key metrics such as restricted mean survival\ntime (RMST) and decision-making accuracy. This work represents a step forward\nin advancing personalized and data-driven treatment strategies across diverse\nhealthcare settings.",
      "generated_abstract": "ian nonparametrics (BNP) framework is an alternative to the\ntypical parametric methods in the statistics. It has been widely applied to\naddress the heteroskedasticity and nonlinearity problems in many fields. The\nmain challenge in BNP is to find the appropriate prior distribution for the\nunknown parameters. To address this issue, the efficient Bayesian neural\nnetwork (EBNN) is introduced, which is a Bayesian neural network that can\nlearn the prior distribution of the unknown parameters. In this paper, we\npresent a Bayesian BNP framework for the estimation of the mean and covariance\nmatrices of a Gaussian random field, which can be applied to any Gaussian\nrandom field. We first construct the EBNN model for the mean and covariance\nmatrices and derive the posterior distribution of the unknown parameters. Then\nwe propose a two-step algorithm for the estimation of the mean and covariance\nmat",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12931034482758622,
          "p": 0.19230769230769232,
          "f": 0.1546391704495697
        },
        "rouge-2": {
          "r": 0.022388059701492536,
          "p": 0.02608695652173913,
          "f": 0.024096380571282133
        },
        "rouge-l": {
          "r": 0.12931034482758622,
          "p": 0.19230769230769232,
          "f": 0.1546391704495697
        }
      }
    },
    {
      "paper_id": "math-ph.math/CV/2503.05690v1",
      "true_abstract": "Inspired by the duality between Jackiw-Teitelboim gravity and Schwarzian\nfield theory, we show identities between the Schwarzian action of a circle\ndiffeomorphism with (1) the hyperbolic area enclosed by the Epstein curve in\nthe hyperbolic disk $\\mathbb{D}$, (2) the asymptotic excess in the\nisoperimetric inequality for the equidistant Epstein foliation, (3) the\nvariation of the Loewner energy along its equipotential foliation, and (4) the\nasymptotic change in hyperbolic area under a conformal distortion near the\ncircle. From these geometric interpretations, we obtain two new proofs of the\nnon-negativity of the Schwarzian action for circle diffeomorphisms, one from\nthe isoperimetric inequality and the other from the monotonicity of the Loewner\nenergy. Moreover, we show that the horocycle truncation used in Epstein's\nconstruction of the Epstein curve also defines a renormalized length of\nhyperbolic geodesics in $\\mathbb{D}$, which coincides with the log of the\nbi-local observable. From this, we show that the bi-local observables on the\nedges of any ideal triangulation of $\\mathbb{D}$ determine the circle\ndiffeomorphism.",
      "generated_abstract": "igate the dynamics of a class of interacting $p$-particle systems\nin the framework of the time-dependent Gross-Pitaevskii equation. We find that\nthe phase space of the system exhibits a rich structure and reveals interesting\nconnections with the one of the $p$-particle Schr\\\"odinger equation. We\nidentify the dynamical attractors of the system and investigate their\ndynamical properties. In particular, we establish the existence of a\n$2$-dimensional attractor, which is a unique attractor of the dynamics, for\nall values of the interaction strength. Furthermore, we establish the\nexistence of a $3$-dimensional attractor for all values of the interaction\nstrength and for all values of the number of particles $N$. We find that the\nexistence of the attractor can be understood from the existence of a\n$1$-dimensional attractor. Moreover, we examine the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14444444444444443,
          "p": 0.2,
          "f": 0.16774193061394393
        },
        "rouge-2": {
          "r": 0.050724637681159424,
          "p": 0.07291666666666667,
          "f": 0.059829054990138464
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.18461538461538463,
          "f": 0.15483870480749237
        }
      }
    },
    {
      "paper_id": "cs.SI.q-fin/EC/2502.21037v2",
      "true_abstract": "Recent advances in artificial intelligence have led to the proliferation of\nartificial agents in social contexts, ranging from education to online social\nmedia and financial markets, among many others. The increasing rate at which\nartificial and human agents interact makes it urgent to understand the\nconsequences of human-machine interactions for the propagation of new ideas,\nproducts, and behaviors in society. Across two distinct empirical contexts, we\nfind here that artificial agents lead to significantly faster and wider social\ncontagion. To this end, we replicate a choice experiment previously conducted\nwith human subjects by using artificial agents powered by large language models\n(LLMs). We use the experiment's results to measure the adoption thresholds of\nartificial agents and their impact on the spread of social contagion. We find\nthat artificial agents tend to exhibit lower adoption thresholds than humans,\nwhich leads to wider network-based social contagions. Our findings suggest that\nthe increased presence of artificial agents in real-world networks may\naccelerate behavioral shifts, potentially in unforeseen ways.",
      "generated_abstract": "In this paper, we present a novel approach for estimating the intrinsic\ndynamics of a system by leveraging the idea of an optimal control problem.\nInspired by the optimal control theory, we propose to find an optimal control\nfor a given system. This control is then used to compute the trajectories of\nthe system. This approach is validated through several numerical examples,\nincluding the dynamics of a two-dimensional system of coupled particles.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08181818181818182,
          "p": 0.20930232558139536,
          "f": 0.11764705478234881
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07272727272727272,
          "p": 0.18604651162790697,
          "f": 0.10457515935751222
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/CB/2411.12123v7",
      "true_abstract": "Colorectal cancer (CRC) poses a major public health challenge due to its\nincreasing prevalence, particularly among younger populations. Microsatellite\ninstability-high (MSI-H) CRC and deficient mismatch repair (dMMR) CRC\nconstitute 15% of all CRC and exhibit remarkable responsiveness to\nimmunotherapy, especially with PD-1 inhibitors. Despite this, there is a\nsignificant need to optimise immunotherapeutic regimens to maximise clinical\nefficacy and patient quality of life whilst minimising monetary costs. To\naddress this, we employ a novel framework driven by delay integro-differential\nequations to model the interactions among cancer cells, immune cells, and\nimmune checkpoints. Several of these components are being modelled\ndeterministically for the first time in cancer, paving the way for a deeper\nunderstanding of the complex underlying immune dynamics. We consider two\ncompartments: the tumour site and the tumour-draining lymph node, incorporating\nphenomena such as dendritic cell (DC) migration, T cell proliferation, and CD8+\nT cell exhaustion and reinvigoration. Parameter values and initial conditions\nare derived from experimental data, integrating various pharmacokinetic,\nbioanalytical, and radiographic studies, along with deconvolution of bulk\nRNA-sequencing data from the TCGA COADREAD and GSE26571 datasets. We finally\noptimise neoadjuvant treatment with pembrolizumab, a widely used PD-1\ninhibitor, to balance efficacy, efficiency, and toxicity in locally advanced\nMSI-H/dMMR CRC patients. We improve upon currently FDA-approved therapeutic\nregimens for metastatic MSI-H/dMMR CRC, demonstrating that a single\nmedium-to-high dose of pembrolizumab is sufficient for effective tumour\neradication whilst being efficient, safe and practical.",
      "generated_abstract": "The role of non-coding DNA in disease and development is becoming more\nand more apparent. In the past decades, the field of DNA methylation has\nemerged as a powerful tool for studying these processes. Here, we present a\nreview of the current state of the art in epigenomics and the application of\nepigenomic methods in human and mouse disease. We highlight how these techniques\nhave revolutionized our understanding of DNA methylation, focusing on its role\nin development, aging, and disease. In doing so, we provide a comprehensive\noverview of the current state of the art in epigenomics, including the\ntechnological advancements, epigenomics applications, and the ethical and\nsocietal challenges in using these techniques. We also highlight the critical\nrole of ethics in advancing epigenomic research and discuss potential\nsolutions to these challenges.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08092485549132948,
          "p": 0.17721518987341772,
          "f": 0.11111110680681549
        },
        "rouge-2": {
          "r": 0.01293103448275862,
          "p": 0.025210084033613446,
          "f": 0.017094012612236465
        },
        "rouge-l": {
          "r": 0.08092485549132948,
          "p": 0.17721518987341772,
          "f": 0.11111110680681549
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.18261v2",
      "true_abstract": "Traditionally, the impact of minimum wages on employment has been studied,\nand it is generally believed to have a negative effect. Yet, some recent\nstudies have shown that the impact of minimum wages on employment can sometimes\nbe positive. In addition, certain recent proposals set a higher minimum wage\nthan the wage earned by some high-productivity workers. However, the impact of\nminimum wages on employment has been primarily studied on low-skilled workers,\nwhereas there is limited research on high-skilled workers. To address this gap\nand examine the effects of minimum wages on high-productivity workers'\nemployment, I construct a macroeconomic model incorporating productivity\nfluctuations, incomplete markets, directed search, and on-the-job search and\ncompare the steady-state distributions between the baseline model and the model\nwith a minimum wage. As a result, binding minimum wages increase the\nunemployment rate of both low and high-productivity workers.",
      "generated_abstract": "the effect of state-dependent taxes on the production function.\nState-dependent taxes are commonly implemented to address uneven distribution\nof tax burdens among firms, and they are also used to target high-income\nindividuals who may be under-taxed. However, state-dependent taxes can lead to\nfirm-specific distortions in the production function, potentially leading to\ninstability. To address this, we propose a state-dependent tax policy that\nmeasures the firm's productivity and taxes it accordingly. We show that this\npolicy can improve stability and reduce distortions. We also show that the\ntax policy can be implemented by either an incumbent firm or a new entrant. We\nalso show that the policy can be implemented by a firm that is not subject to\nthe tax. Finally, we empirically demonstrate that the policy improves firm\nstability and reduces distortions",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20454545454545456,
          "p": 0.24,
          "f": 0.22085889073732556
        },
        "rouge-2": {
          "r": 0.01652892561983471,
          "p": 0.017543859649122806,
          "f": 0.017021271600182544
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.21333333333333335,
          "f": 0.19631901343671213
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.01495v1",
      "true_abstract": "Vovk (2015) introduced cross-conformal prediction, a modification of split\nconformal designed to improve the width of prediction sets. The method, when\ntrained with a miscoverage rate equal to $\\alpha$ and $n \\gg K$, ensures a\nmarginal coverage of at least $1 - 2\\alpha - 2(1-\\alpha)(K-1)/(n+K)$, where $n$\nis the number of observations and $K$ denotes the number of folds. A simple\nmodification of the method achieves coverage of at least $1-2\\alpha$. In this\nwork, we propose new variants of both methods that yield smaller prediction\nsets without compromising the latter theoretical guarantee. The proposed\nmethods are based on recent results deriving more statistically efficient\ncombination of p-values that leverage exchangeability and randomization.\nSimulations confirm the theoretical findings and bring out some important\ntradeoffs.",
      "generated_abstract": "We consider the problem of constructing a kernel-based classifier for\nprediction in the space of probability distributions. We propose a kernel\napproach to this problem based on the Radial Basis Function (RBF) kernel,\nwhich has been shown to be effective in many applications. We derive\nclosed-form expressions for the kernel and its gradient. Moreover, we derive\nthe closed-form expressions for the Hessian of the kernel. To perform this\nanalysis, we provide a new representation of the RBF kernel and a result that\nenables us to perform a reduction of the problem to an equivalent problem with\na constant kernel. We also derive the gradient and Hessian of the reduced\nproblem and present an implementation in the R package kernels2.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15217391304347827,
          "p": 0.208955223880597,
          "f": 0.1761006240544284
        },
        "rouge-2": {
          "r": 0.02608695652173913,
          "p": 0.02830188679245283,
          "f": 0.027149316275261456
        },
        "rouge-l": {
          "r": 0.14130434782608695,
          "p": 0.19402985074626866,
          "f": 0.1635220077022271
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.09812v1",
      "true_abstract": "Selective inference aims at providing valid inference after a data-driven\nselection of models or hypotheses. It is essential to avoid overconfident\nresults and replicability issues. While significant advances have been made in\nthis area for standard regression models, relatively little attention has been\ngiven to linear mixed models (LMMs), which are widely used for analyzing\nclustered or longitudinal data. This paper reviews the existing selective\ninference approaches developed for LMMs, focusing on selection of fixed\neffects, where the random effects structure is given. We present these methods\nin detail and, through comparative simulations, assess their practical\nperformance and computational feasibility under varying data structures. In\naddition, we apply them to a real-world biological dataset to examine how\nmethod choice can impact inference in practice. Our findings highlight an\nexisting trade-off between computational complexity and statistical power and\nemphasize the scarcity of methods that perform well as the number of variables\nincreases. In such scenarios, basic sample splitting emerges as the most\nreliable approach.",
      "generated_abstract": "This paper presents a new method for testing the hypothesis of a\nregression coefficient being constant over time. We derive the test statistic\nand derive asymptotic confidence intervals for the test statistic, and discuss\nthe interpretation of the test result. We also derive a two-sided test for the\nhypothesis that a regression coefficient is constant over time. We illustrate\nthe method using simulated data, and also apply it to data from a study on\npost-traumatic stress disorder (PTSD).",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12307692307692308,
          "p": 0.3333333333333333,
          "f": 0.17977527695997988
        },
        "rouge-2": {
          "r": 0.00625,
          "p": 0.014705882352941176,
          "f": 0.008771925638660045
        },
        "rouge-l": {
          "r": 0.12307692307692308,
          "p": 0.3333333333333333,
          "f": 0.17977527695997988
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.18541v2",
      "true_abstract": "In the dynamic landscape of contemporary society, the popularity of ideas,\nopinions, and interests fluctuates rapidly. Traditional dynamical models in\nsocial sciences often fail to capture this inherent volatility, attributing\nchanges to exogenous shocks rather than intrinsic features of the system. This\npaper introduces a novel, tractable model that simulates the natural rise and\nfall of ideas' popularity, offering a more accurate representation of\nreal-world dynamics. Building upon the SIRS (Susceptible, Infectious,\nRecovered, Susceptible) epidemiological model, we incorporate a feedback\nmechanism that allows the recovery rate to vary dynamically based on the\ncurrent state of the system. This modification reflects the cyclical nature of\nidea adoption and abandonment, driven by social saturation and renewed\ninterest. Our model successfully captures the rapid and recurrent shifts in\npopularity, providing valuable insights into the mechanisms behind these\nfluctuations. This approach offers a robust framework for studying the\ndiffusion dynamics of popular ideas, with potential applications across various\nfields such as marketing, technology adoption, and political movements.",
      "generated_abstract": "This paper studies the welfare effects of intergenerational transfers in\nnon-cooperative and cooperative social choice models. We consider a class of\nnon-cooperative social choice models in which agents can transfer resources to\ntheir children, and we characterize the welfare effects of these transfers\nusing a class of static models. We also propose a novel extension of the\nHorn-Lindblad model that captures the intergenerational transfers, and we\ncharacterize the welfare effects of these transfers using a class of dynamic\nmodels. Our results provide insights into the welfare implications of\nnon-cooperative social choice models with intergenerational transfers, and they\nmay be relevant for policy makers who are interested in designing policies to\nincentivize or promote intergenerational transfers.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.168,
          "p": 0.3230769230769231,
          "f": 0.22105262707756246
        },
        "rouge-2": {
          "r": 0.0440251572327044,
          "p": 0.08139534883720931,
          "f": 0.05714285258675589
        },
        "rouge-l": {
          "r": 0.144,
          "p": 0.27692307692307694,
          "f": 0.18947367970914136
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/FL/2503.02719v1",
      "true_abstract": "Motion planning with simple objectives, such as collision-avoidance and\ngoal-reaching, can be solved efficiently using modern planners. However, the\ncomplexity of the allowed tasks for these planners is limited. On the other\nhand, signal temporal logic (STL) can specify complex requirements, but\nSTL-based motion planning and control algorithms often face scalability issues,\nespecially in large multi-robot systems with complex dynamics. In this paper,\nwe propose an algorithm that leverages the best of the two worlds. We first use\na single-robot motion planner to efficiently generate a set of alternative\nreference paths for each robot. Then coordination requirements are specified\nusing STL, which is defined over the assignment of paths and robots' progress\nalong those paths. We use a Mixed Integer Linear Program (MILP) to compute task\nassignments and robot progress targets over time such that the STL\nspecification is satisfied. Finally, a local controller is used to track the\ntarget progress. Simulations demonstrate that our method can handle tasks with\ncomplex constraints and scales to large multi-robot teams and intricate task\nallocation scenarios.",
      "generated_abstract": "This paper introduces a novel approach to robot navigation in complex\nenvironments. We propose a framework that leverages the power of geometric\nconstraints to address the challenges of navigation in dynamic environments.\nOur approach combines geometric constraints with a novel hierarchical approach\nto achieve efficient navigation and robust planning. We demonstrate that this\nframework can achieve high-level navigation in challenging scenarios,\ndemonstrating the effectiveness of the proposed approach in both simulation\nand real-world robot navigation. The source code is available at:\nhttps://github.com/miketan/Geo-Constrained-Hierarchical-Planning-for-Robot-\nNavigation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1557377049180328,
          "p": 0.34545454545454546,
          "f": 0.21468926125315207
        },
        "rouge-2": {
          "r": 0.023809523809523808,
          "p": 0.05194805194805195,
          "f": 0.032653056914286285
        },
        "rouge-l": {
          "r": 0.13934426229508196,
          "p": 0.3090909090909091,
          "f": 0.19209039119665494
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.04225v1",
      "true_abstract": "This study presents the development and validation of a digital twin for a\nsemi-autogenous grinding (SAG) mill controlled by an expert system. The digital\ntwin integrates three key components of the closed-loop operation: (1) fuzzy\nlogic for expert control, (2) a state-space model for regulatory control, and\n(3) a recurrent neural network to simulate the SAG mill process. The digital\ntwin is combined with a statistical framework for automatically detecting\nprocess disturbances (or critical operations), which triggers model retraining\nonly when deviations from expected behaviour are identified, ensuring\ncontinuous updates with new data to enhance the SAG supervision. The model was\ntrained with 68 hours of operational industrial data and validated with an\nadditional 8 hours, allowing it to predict mill behaviour within a 2.5-minute\nhorizon at 30-second intervals with errors smaller than 5%.",
      "generated_abstract": "opment of high-performance digital signal processors (DSPs) has\nimproved the performance of several control systems, including those in\nsmartphones, wearable devices, and medical devices. However, the high\ncomplexity of the control system and the limited processing power of\nDSPs limit the performance of these control systems. This paper proposes a\nnew control system that improves the performance of the control system. The\nproposed control system is based on the neural network control (NNC) and the\nneural network architecture. This control system uses the NNC architecture,\nwhich has a low computational complexity. The NNC architecture can be\noptimized by reducing the number of parameters in the architecture. This\noptimization is performed by training a neural network to identify the\nparameters of the NNC architecture. The proposed control system is evaluated\nusing the fuzzy control theory. The results show that the proposed control\nsystem",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16494845360824742,
          "p": 0.21621621621621623,
          "f": 0.18713449801443194
        },
        "rouge-2": {
          "r": 0.03076923076923077,
          "p": 0.03418803418803419,
          "f": 0.03238865898146252
        },
        "rouge-l": {
          "r": 0.14432989690721648,
          "p": 0.1891891891891892,
          "f": 0.16374268514893486
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.20939v1",
      "true_abstract": "Neuroblastoma is a paediatric extracranial solid cancer that arises from the\ndeveloping sympathetic nervous system and is characterised by an abnormal\ndistribution of cell types in tumours compared to healthy infant tissues. In\nthis paper, we propose a new mathematical model of cell differentiation during\nsympathoadrenal development. By performing Bayesian inference of the model\nparameters using clinical data from patient samples, we show that the model\nsuccessfully accounts for the observed differences in cell type heterogeneity\namong healthy adrenal tissues and four common types of neuroblastomas. Using a\nphenotypically structured model, we show that alterations in healthy\ndifferentiation dynamics are related to cell malignancy, and tumour volume\ngrowth. We use this model to analyse the evolution of malignant traits in a\ntumour. Our findings suggest that normal development dynamics make the\nembryonic sympathetic nervous system more robust to perturbations and\naccumulation of malignancies, and that the diversity of differentiation\ndynamics found in the neuroblastoma subtypes lead to unique risk profiles for\nneuroblastoma relapse after treatment.",
      "generated_abstract": "opment of accurate and reliable methods for protein-ligand docking\ninto protein binding sites remains a significant challenge in the field of\nprotein-ligand binding. In this study, we propose a novel approach for\nautomated docking, which leverages the capabilities of Convolutional Neural\nNetworks (CNNs) to model protein-ligand interactions, enabling robust and\nefficient protein-ligand docking. The proposed method, named Docking with\nCNNs (DockCNN), is based on a convolutional neural network architecture that\nfeatures a biologically inspired network topology and a novel multi-task\ntraining mechanism that simultaneously optimizes the docking performance and\nthe protein-ligand binding affinity. The proposed methodology was validated\nusing the UM16 model of the human protein-ligand docking problem and compared\nto state-of-the-art methods, including DeepWalk,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16037735849056603,
          "p": 0.20987654320987653,
          "f": 0.18181817690754679
        },
        "rouge-2": {
          "r": 0.04487179487179487,
          "p": 0.06422018348623854,
          "f": 0.05283018383652589
        },
        "rouge-l": {
          "r": 0.1509433962264151,
          "p": 0.19753086419753085,
          "f": 0.17112298974177143
        }
      }
    },
    {
      "paper_id": "math-ph.nlin/SI/2503.09502v1",
      "true_abstract": "An infinite 3-parametric family of superintegrable and exactly-solvable\nquantum models on a plane, admitting separation of variables in polar\ncoordinates, marked by integer index $k$ was introduced in Journ Phys A 42\n(2009) 242001 and was called in literature the TTW system. In this paper it is\nconjectured that the Hamiltonian and both integrals of TTW system have hidden\nalgebra $g^{(k)}$ - it was checked for $k=1,2,3,4$ - having its\nfinite-dimensional representation spaces as the invariant subspaces. It is\nchecked that for $k=1,2,3,4$ that the Hamiltonian $H$, two integrals ${\\cal\nI}_{1,2}$ and their commutator ${\\cal I}_{12} = [{\\cal I}_1,{\\cal I}_2]$ are\nfour generating elements of the polynomial algebra of integrals of the order\n$(k+1)$: $[{\\cal I}_1,{\\cal I}_{12}] = P_{k+1}(H, {\\cal I}_{1,2},{\\cal\nI}_{12})$, $[{\\cal I}_2,{\\cal I}_{12}] = Q_{k+1}(H, {\\cal I}_{1,2},{\\cal\nI}_{12})$, where $P_{k+1},Q_{k+1}$ are polynomials of degree $(k+1)$ written in\nterms of ordered monomials of $H, {\\cal I}_{1,2},{\\cal I}_{12}$. This implies\nthat polynomial algebra of integrals is subalgebra of $g^{(k)}$. It is\nconjectured that all is true for any integer $k$.",
      "generated_abstract": "We introduce a new class of quantum many-body models that are in the spirit\nof the Bethe ansatz, but are not based on the Bethe ansatz equations. We show\nthat they can be obtained as a variational principle. We use the model for\nnumerical studies and demonstrate that it can be used to find the exact\nsolutions of the Bethe ansatz equations. We also discuss the effect of the\nnon-standard coupling of the two fermionic species on the Bethe ansatz\nequations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1308411214953271,
          "p": 0.27450980392156865,
          "f": 0.1772151855015223
        },
        "rouge-2": {
          "r": 0.006535947712418301,
          "p": 0.014925373134328358,
          "f": 0.009090904854960651
        },
        "rouge-l": {
          "r": 0.11214953271028037,
          "p": 0.23529411764705882,
          "f": 0.15189872980531977
        }
      }
    },
    {
      "paper_id": "math.OC.eess/SY/2503.07324v1",
      "true_abstract": "Distribution shifts have long been regarded as troublesome external forces\nthat a decision-maker should either counteract or conform to. An intriguing\nfeedback phenomenon termed decision dependence arises when the deployed\ndecision affects the environment and alters the data-generating distribution.\nIn the realm of performative prediction, this is encoded by distribution maps\nparameterized by decisions due to strategic behaviors. In contrast, we\nformalize an endogenous distribution shift as a feedback process featuring\nnonlinear dynamics that couple the evolving distribution with the decision.\nStochastic optimization in this dynamic regime provides a fertile ground to\nexamine the various roles played by dynamics in the composite problem\nstructure. To this end, we develop an online algorithm that achieves optimal\ndecision-making by both adapting to and shaping the dynamic distribution.\nThroughout the paper, we adopt a distributional perspective and demonstrate how\nthis view facilitates characterizations of distribution dynamics and the\noptimality and generalization performance of the proposed algorithm. We\nshowcase the theoretical results in an opinion dynamics context, where an\nopportunistic party maximizes the affinity of a dynamic polarized population,\nand in a recommender system scenario, featuring performance optimization with\ndiscrete distributions in the probability simplex.",
      "generated_abstract": "aper, we consider the problem of designing distributed controllers for\nnonlinear systems with uncertain parameters and input-to-state stability (ISS)\nrequirements. First, we propose a robust controller design methodology that\nenables the design of controllers that guarantee ISS, while also maintaining\nsufficient performance guarantees. Next, we present a distributed controller\ndesign methodology that leverages the robust controller design methodology and\nassumes the availability of the system's state estimate. This approach\nenables the design of controllers that guarantee ISS, while also ensuring\nsufficient performance guarantees. Finally, we present a distributed controller\ndesign methodology that combines the robust controller design methodology and\nthe distributed controller design methodology. This approach enables the design\nof controllers that guarantee ISS, while also ensuring sufficient performance\nguarantees. The proposed methods are applied to the control of a nonlinear\nquadruped robot",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0967741935483871,
          "p": 0.2033898305084746,
          "f": 0.13114753661441084
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.03529411764705882,
          "f": 0.021897805939315663
        },
        "rouge-l": {
          "r": 0.08870967741935484,
          "p": 0.1864406779661017,
          "f": 0.120218574865777
        }
      }
    },
    {
      "paper_id": "quant-ph.math-ph/2503.10400v1",
      "true_abstract": "We elucidate the requirements for quantum operations that achieve\nenvironment-assisted invariance (envariance), a symmetry of entanglement. While\nenvariance has traditionally been studied within the framework of local unitary\noperations, we extend the analysis to consider non-unitary local operations.\nFirst, we investigate the conditions imposed on operators acting on pure\nbipartite entanglement to attain envariance. We show that the local operations\nmust take a direct-sum form in their Kraus operator representations,\nestablishing decoherence-free subspaces. Furthermore, we prove that the unitary\noperation on the system's subspace uniquely determines the corresponding\nunitary operator on the environment's subspace. As an immediate consequence, we\ndemonstrate that environment-assisted shortcuts to adiabaticity cannot be\nachieved through non-unitary operations. In addition, we identify the\nrequirements that local operations must satisfy to ensure that the eternal\nblack hole states remain static in AdS/CFT.",
      "generated_abstract": "igate the quantum dynamics of the coupled bosonic and fermionic\ncoherent modes of an interacting quantum gas. By coupling the bosonic and\nfermionic modes to a common oscillator, we introduce a non-adiabatic coupling\nmechanism. We analyze the effect of the non-adiabatic coupling on the\nquantum-to-classical transitions of the modes, and find that this non-adiabatic\ncoupling can lead to a phase transition between quantum coherence and\nquantum decoherence. We further investigate the effects of non-adiabatic\ncoupling on the dynamics of the bosonic and fermionic coherent modes, and show\nthat the non-adiabatic coupling can lead to a qualitative change in the\ndynamics of the two modes. These findings provide insights into the effects of\nnon-adiabatic interactions on quantum coherence and deco",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14772727272727273,
          "p": 0.23214285714285715,
          "f": 0.18055555080246927
        },
        "rouge-2": {
          "r": 0.031746031746031744,
          "p": 0.04597701149425287,
          "f": 0.03755868061363549
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.21428571428571427,
          "f": 0.16666666191358037
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.04358v1",
      "true_abstract": "We propose a novel approach for learning causal response representations. Our\nmethod aims to extract directions in which a multidimensional outcome is most\ndirectly caused by a treatment variable. By bridging conditional independence\ntesting with causal representation learning, we formulate an optimisation\nproblem that maximises the evidence against conditional independence between\nthe treatment and outcome, given a conditioning set. This formulation employs\nflexible regression models tailored to specific applications, creating a\nversatile framework. The problem is addressed through a generalised eigenvalue\ndecomposition. We show that, under mild assumptions, the distribution of the\nlargest eigenvalue can be bounded by a known $F$-distribution, enabling\ntestable conditional independence. We also provide theoretical guarantees for\nthe optimality of the learned representation in terms of signal-to-noise ratio\nand Fisher information maximisation. Finally, we demonstrate the empirical\neffectiveness of our approach in simulation and real-world experiments. Our\nresults underscore the utility of this framework in uncovering direct causal\neffects within complex, multivariate settings.",
      "generated_abstract": "aper, we propose a novel framework for the non-parametric estimation\nof the conditional mean of a random variable given a set of conditional\nprobabilities. This estimation problem has a well-known closed-form solution in\nthe case where the random variable and the conditional probabilities are\nindependent, and the conditional mean can be computed exactly. However, this\ncase is rarely encountered in real-world applications. We generalize this\nclosed-form solution to the case where the conditional mean depends on the\nconditional probabilities through a neural network. We derive the\nhigh-dimensional asymptotic distribution of the neural network estimator and\nshow that the resulting estimator follows a non-parametric distribution with\nhigh probability. To address the curse of dimensionality, we propose a\nnon-parametric Bayesian approach to estimate the conditional mean using\nmaximum-likelihood estimation. We derive the asymptotic distribution of the\nestim",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25217391304347825,
          "p": 0.4084507042253521,
          "f": 0.31182795226904847
        },
        "rouge-2": {
          "r": 0.05263157894736842,
          "p": 0.07407407407407407,
          "f": 0.06153845668165719
        },
        "rouge-l": {
          "r": 0.22608695652173913,
          "p": 0.36619718309859156,
          "f": 0.2795698877529195
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.15980v1",
      "true_abstract": "This paper studies inter-firm heterogeneity in production. Unlike much of the\nexisting research, which primarily addresses heterogeneous production through\nunobserved fixed effects, our approach also focuses on differences in factors'\noutput elasticities. Using manufacturing data from Chile, Colombia, and Japan,\nwe apply an innovative Empirical Bayes methodology to estimate heterogeneous\nCobb-Douglas production functions. We uncover substantial heterogeneity in both\nfactor neutral productivity and factor elasticities, with a strong negative\ncorrelation between them. These findings are consistently observed across\ndatasets and remain robust when using CES and intensive Cobb-Douglas\nspecifications. We show that accounting for these features has significant\nimplications for issues such as markup estimation, firms' technology adoption,\nand productivity measurement.",
      "generated_abstract": "We analyze the impact of climate policies on the labor market, focusing on\ntheir effects on the labor supply and wage distributions. We develop a\ndynamic-game framework that incorporates uncertainties and adaptability in the\nconsumption and production decisions. We examine the impact of climate\npolicies on the labor market through the following two dimensions: (i)\nincreased or decreased labor supply, and (ii) shifts in wage distributions.\nWe find that policies that increase labor supply tend to decrease wage\ndispersion, while policies that decrease labor supply tend to increase wage\ndispersion. However, these effects are highly uncertain, as the future path of\nclimate policies is highly uncertain and can be affected by a number of\npotential shocks. Our analysis highlights the importance of considering\nuncertainty in climate policies and their effects on the labor market.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14583333333333334,
          "p": 0.1917808219178082,
          "f": 0.16568046846539
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14583333333333334,
          "p": 0.1917808219178082,
          "f": 0.16568046846539
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.00642v1",
      "true_abstract": "Real-world low-light images captured by imaging devices suffer from poor\nvisibility and require a domain-specific enhancement to produce artifact-free\noutputs that reveal details. In this paper, we propose an unpaired low-light\nimage enhancement network leveraging novel controlled transformation-based\nself-supervision and unpaired self-conditioning strategies. The model\ndetermines the required degrees of enhancement at the input image pixels, which\nare learned from the unpaired low-lit and well-lit images without any direct\nsupervision. The self-supervision is based on a controlled transformation of\nthe input image and subsequent maintenance of its enhancement in spite of the\ntransformation. The self-conditioning performs training of the model on\nunpaired images such that it does not enhance an already-enhanced image or a\nwell-lit input image. The inherent noise in the input low-light images is\nhandled by employing low gradient magnitude suppression in a detail-preserving\nmanner. In addition, our noise handling is self-conditioned by preventing the\ndenoising of noise-free well-lit images. The training based on low-light image\nenhancement-specific attributes allows our model to avoid paired supervision\nwithout compromising significantly in performance. While our proposed\nself-supervision aids consistent enhancement, our novel self-conditioning\nfacilitates adequate enhancement. Extensive experiments on multiple standard\ndatasets demonstrate that our model, in general, outperforms the\nstate-of-the-art both quantitatively and subjectively. Ablation studies show\nthe effectiveness of our self-supervision and self-conditioning strategies, and\nthe related loss functions.",
      "generated_abstract": "vancements in image generation have inspired researchers to explore\nuse of generative models for visual object tracking. Existing approaches,\nlargely inspired by image-to-image translation, rely on an end-to-end\napproach, which requires extensive training data. Moreover, existing approaches\nfocus on single objects, limiting the generalizability to more complex tasks.\nIn this paper, we propose a novel method for visual object tracking, which\nintegrates a pre-trained image-to-image translation model with a\nlightweight object-centric image representation, enabling effective tracking\nof complex objects. We demonstrate the effectiveness of our approach in\ntracking a variety of objects, including a human, a moving object, and a\ndynamic object. In addition, we introduce a novel evaluation protocol for\nobject-centric tracking, which captures both visual similarity and\nobject-specific features to enable a more objective evaluation of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18604651162790697,
          "p": 0.2727272727272727,
          "f": 0.22119815186051955
        },
        "rouge-2": {
          "r": 0.04326923076923077,
          "p": 0.07258064516129033,
          "f": 0.05421686278995541
        },
        "rouge-l": {
          "r": 0.17829457364341086,
          "p": 0.26136363636363635,
          "f": 0.21198156199876841
        }
      }
    },
    {
      "paper_id": "math.SG.math/SG/2503.00123v1",
      "true_abstract": "This survey explores the geometry of three-dimensional Anosov flows from the\nperspective of contact and symplectic geometry, following the work of\nMitsumatsu, Eliashberg-Thurston, Hozoori, and the author. We also present a few\noriginal results and discuss various open questions and conjectures.",
      "generated_abstract": "aper, we consider the problem of classifying simple Lie algebras up to\nrealization by parabolic subalgebras. We present a new algorithm for the\ncomputation of the realization table for any given simple Lie algebra, which\nconsists of a matrix-vector product algorithm. We also introduce a new method\nfor computing the realization table of any given parabolic subalgebra. We show\nthat the realization table of a parabolic subalgebra is always obtained from\nthat of the Lie algebra that it is realized by. Finally, we present a new\nalgorithm for computing the realization table of any given parabolic subalgebra\nin terms of a matrix-vector product algorithm. We apply our methods to compute\nthe realization table of a number of simple Lie algebras. Our results yield\nnew answers to several open questions posed by Zimmer and Zuckerman in their\npioneering paper on realization tables of simple",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3333333333333333,
          "p": 0.15942028985507245,
          "f": 0.21568627013264136
        },
        "rouge-2": {
          "r": 0.075,
          "p": 0.02830188679245283,
          "f": 0.04109588643272698
        },
        "rouge-l": {
          "r": 0.3333333333333333,
          "p": 0.15942028985507245,
          "f": 0.21568627013264136
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2502.02371v1",
      "true_abstract": "Accurate identification of druggable pockets is essential for structure-based\ndrug design. However, most pocket-identification algorithms prioritize their\ngeometric properties over downstream docking performance. To address this\nlimitation, we developed RAPID-Net, a pocket-finding algorithm for seamless\nintegration with docking workflows. When guiding AutoDock Vina, RAPID-Net\noutperforms DiffBindFR on the PoseBusters benchmark and enables blind docking\non large proteins that AlphaFold 3 cannot process as a whole. Furthermore,\nRAPID-Net surpasses PUResNet and Kalasanty in docking accuracy and\npocket-ligand intersection rates across diverse datasets, including\nPoseBusters, Astex Diverse Set, BU48, and Coach420. When accuracy is evaluated\nas ``at least one correct pose in the ensemble'', RAPID-Net outperforms\nAlphaFold 3 on the PoseBusters benchmark, suggesting that our approach can be\nfurther improved with a suitable pose reweighting tool offering a\ncost-effective and competitive alternative to AlphaFold 3 for docking. Finally,\nusing several therapeutically relevant examples, we demonstrate the ability of\nRAPID-Net to identify remote functional sites, highlighting its potential to\nfacilitate the development of innovative therapeutics.",
      "generated_abstract": "very of protein-protein interaction (PPI) networks has revolutionized\nthe field of biology by providing a detailed picture of protein interactions.\nThese networks are typically represented as sparse adjacency matrices, which\nlimit their computational efficiency. In this work, we propose a novel\napproach to represent PPI networks as dense matrices, achieving higher\nefficiency. To this end, we propose a new method to construct a dense PPI\nnetwork, and we present an algorithm to efficiently compute the adjacency matrix\nof this network. We validate the proposed approach on simulated and real\ndatasets, and demonstrate its superiority compared to the sparse PPI networks.\nOur method is generic and can be applied to any type of PPI network, including\ncomplex networks and networks with large-scale interactions. The efficiency of\nour method can be further enhanced by introducing other methods, such as\ndecomposition methods and network embedding",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19008264462809918,
          "p": 0.25,
          "f": 0.21596243640723853
        },
        "rouge-2": {
          "r": 0.01282051282051282,
          "p": 0.014705882352941176,
          "f": 0.013698625160444665
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.2391304347826087,
          "f": 0.2065727650457362
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.03557v1",
      "true_abstract": "Clinical practice guidelines are designed to guide clinical practice and\ninvolve causal language. Sometimes guidelines make or require stronger causal\nclaims than those in the references they rely on, a phenomenon we refer to as\n'causal language jump'. We evaluated the strength of expressed causation in\ndiabetes guidelines and the evidence they reference to assess the pattern of\njumps. We randomly sampled 300 guideline statements from four diabetes\nguidelines. We rated the causation strength in the statements and the\ndependence on causation in recommendations supported by these statements using\nexisting scales. Among the causal statements, the cited original studies were\nsimilarly assessed. We also assessed how well they report target trial\nemulation (TTE) components as a proxy for reliability. Of the sampled\nstatements, 114 (38.0%) were causal, and 76 (66.7%) expressed strong causation.\n27.2% (31/114) of causal guideline statements demonstrated a \"causal language\njump\", and 34.9% (29/83) of guideline recommendations cannot be effectively\nsupported. Of the 53 eligible studies for TTE rating, most did not report\ntreatment assignment and causal contrast in detail. Our findings suggest causal\nlanguage jumps were common among diabetes guidelines. While these jumps are\nsometimes inevitable, they should always be supported by good causal inference\npractices.",
      "generated_abstract": "This paper proposes a novel methodology for the analysis of data generated by\nthe Gaussian random field (GRF) model, which is widely used in the field of\ngeostatistics. The proposed methodology provides a theoretical framework for\nthe analysis of GRF data and offers a number of advantages over traditional\napproaches. The proposed methodology is based on the use of the discrete\nmultivariate Gaussian distribution, which enables the analysis of data with\nmultiple dimensions. The theoretical framework of the proposed methodology is\nillustrated through a series of examples. These examples demonstrate the\neffectiveness of the proposed methodology in analyzing data generated by the\nGRF model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06299212598425197,
          "p": 0.14285714285714285,
          "f": 0.08743168974170644
        },
        "rouge-2": {
          "r": 0.005154639175257732,
          "p": 0.011764705882352941,
          "f": 0.007168454544522743
        },
        "rouge-l": {
          "r": 0.05511811023622047,
          "p": 0.125,
          "f": 0.07650272799307259
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.08455v1",
      "true_abstract": "Instead of performing text-conditioned denoising in the image domain, latent\ndiffusion models (LDMs) operate in latent space of a variational autoencoder\n(VAE), enabling more efficient processing at reduced computational costs.\nHowever, while the diffusion process has moved to the latent space, the\ncontrastive language-image pre-training (CLIP) models, as used in many image\nprocessing tasks, still operate in pixel space. Doing so requires costly\nVAE-decoding of latent images before they can be processed. In this paper, we\nintroduce Latent-CLIP, a CLIP model that operates directly in the latent space.\nWe train Latent-CLIP on 2.7B pairs of latent images and descriptive texts, and\nshow that it matches zero-shot classification performance of similarly sized\nCLIP models on both the ImageNet benchmark and a LDM-generated version of it,\ndemonstrating its effectiveness in assessing both real and generated content.\nFurthermore, we construct Latent-CLIP rewards for reward-based noise\noptimization (ReNO) and show that they match the performance of their CLIP\ncounterparts on GenEval and T2I-CompBench while cutting the cost of the total\npipeline by 21%. Finally, we use Latent-CLIP to guide generation away from\nharmful content, achieving strong performance on the inappropriate image\nprompts (I2P) benchmark and a custom evaluation, without ever requiring the\ncostly step of decoding intermediate images.",
      "generated_abstract": "aper, we introduce a novel framework for audio-based image\nrepresentation learning by leveraging the spatial and spectral features of audio\nsignals. Specifically, we propose a novel architecture, called ASPIRE, which\ncombines audio-to-image (A2I) and image-to-audio (I2A) transformers to\ncapture the audio-visual information in a unified framework. The proposed ASPIRE\narchitecture consists of a pre-training module and an end-to-end training\nmodule, which are integrated through a self-attention mechanism. The pre-training\nmodule leverages the audio-visual dataset to learn the basic features of\naudios, while the end-to-end training module further refines the features of\naudios with the proposed ASPIRE transformer. Experimental results demonstrate\nthat ASPIRE outperforms existing state-of-the-art methods, achieving significant\nim",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10869565217391304,
          "p": 0.19480519480519481,
          "f": 0.13953487912341822
        },
        "rouge-2": {
          "r": 0.015544041450777202,
          "p": 0.0297029702970297,
          "f": 0.02040815875491793
        },
        "rouge-l": {
          "r": 0.10144927536231885,
          "p": 0.18181818181818182,
          "f": 0.1302325535420229
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2409.11569v1",
      "true_abstract": "We consider the Merton problem of optimizing expected power utility of\nterminal wealth in the case of an unobservable Markov-modulated drift. What\nmakes the model special is that the agent is allowed to purchase costly expert\nopinions of varying quality on the current state of the drift, leading to a\nmixed stochastic control problem with regular and impulse controls involving\nrandom consequences. Using ideas from filtering theory, we first embed the\noriginal problem with unobservable drift into a full information problem on a\nlarger state space. The value function of the full information problem is\ncharacterized as the unique viscosity solution of the dynamic programming PDE.\nThis characterization is achieved by a new variant of the stochastic Perron's\nmethod, which additionally allows us to show that, in between purchases of\nexpert opinions, the problem reduces to an exit time control problem which is\nknown to admit an optimal feedback control. Under the assumption of sufficient\nregularity of this feedback map, we are able to construct optimal trading and\nexpert opinion strategies.",
      "generated_abstract": "r examines the effectiveness of the mean-variance (MV) portfolio\nin a continuous-time environment with stochastic returns. The MV portfolio is\ndefined as the mean of the expected return over a given time horizon. We show\nthat if the mean is the optimal investment strategy, the MV portfolio is\noptimal under the same conditions. We further show that the MV portfolio is\noptimal if and only if the mean is the optimal strategy under the condition that\nthe risk-neutral probability measure is a probability measure of the return\nprocess. The mean is optimal if and only if the return process is a\n$\\beta$-mixing process, where $\\beta$ is a constant. We show that the mean is\noptimal if and only if the mean is the optimal strategy under the condition that\nthe mean is the optimal strategy. This condition is equivalent to the\nconditional variance of the mean being zero",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16216216216216217,
          "p": 0.3103448275862069,
          "f": 0.21301774697104453
        },
        "rouge-2": {
          "r": 0.018404907975460124,
          "p": 0.03333333333333333,
          "f": 0.02371541043603332
        },
        "rouge-l": {
          "r": 0.13513513513513514,
          "p": 0.25862068965517243,
          "f": 0.17751478839116291
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.17597v1",
      "true_abstract": "This paper presents a model of costly information acquisition where\ndecision-makers can choose whether to elaborate information superficially or\nprecisely. The former action is costless, while the latter entails a processing\ncost. Within this framework, decision-makers' beliefs may polarize even after\nthey have access to the same evidence. From the perspective of a Bayesian\nobserver who neglects information processing constraints, the decision-makers'\noptimal behavior and belief updating may appear consistent with biases such as\ndisconfirmation, underreaction to information, and confirmation bias. However,\nthese phenomena emerge naturally within the model and are fully compatible with\nstandard Bayesian inference and rational decision-making when accounting for\nthe costs of information acquisition.",
      "generated_abstract": "This paper examines the role of the informal sector in Bangladesh's economic\ndevelopment. We examine the informal sector's role in the economy through\nfour categories: (1) labor force participation, (2) employment, (3) income,\nand (4) productivity. We find that the informal sector accounts for about 40\npercent of the labor force, and its contribution to employment and income is\nhigher than that of the formal sector. Moreover, we find that the informal\nsector's contribution to productivity is significant. This study provides\ninsights into the importance of the informal sector in Bangladesh's economy,\nhighlighting the need for further research to understand its role.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09523809523809523,
          "p": 0.13114754098360656,
          "f": 0.11034482271200972
        },
        "rouge-2": {
          "r": 0.009433962264150943,
          "p": 0.011627906976744186,
          "f": 0.010416661720922486
        },
        "rouge-l": {
          "r": 0.09523809523809523,
          "p": 0.13114754098360656,
          "f": 0.11034482271200972
        }
      }
    },
    {
      "paper_id": "cs.CL.econ/GN/2503.01870v1",
      "true_abstract": "Identifying customer needs (CNs) is important for product management, product\ndevelopment, and marketing. Applications rely on professional analysts\ninterpreting textual data (e.g., interview transcripts, online reviews) to\nunderstand the nuances of customer experience and concisely formulate \"jobs to\nbe done.\" The task is cognitively complex and time-consuming. Current practice\nfacilitates the process with keyword search and machine learning but relies on\nhuman judgment to formulate CNs. We examine whether Large Language Models\n(LLMs) can automatically extract CNs. Because evaluating CNs requires\nprofessional judgment, we partnered with a marketing consulting firm to conduct\na blind study of CNs extracted by: (1) a foundational LLM with prompt\nengineering only (Base LLM), (2) an LLM fine-tuned with professionally\nidentified CNs (SFT LLM), and (3) professional analysts. The SFT LLM performs\nas well as or better than professional analysts when extracting CNs. The\nextracted CNs are well-formulated, sufficiently specific to identify\nopportunities, and justified by source content (no hallucinations). The SFT LLM\nis efficient and provides more complete coverage of CNs. The Base LLM was not\nsufficiently accurate or specific. Organizations can rely on SFT LLMs to reduce\nmanual effort, enhance the precision of CN articulation, and provide improved\ninsight for innovation and marketing strategy.",
      "generated_abstract": "vancements in natural language processing (NLP) have led to the\nexploration of large language models (LLMs) for economic analysis, with\npromising results. However, existing approaches often focus on a single LLM,\nsuggesting that additional models could offer additional benefits. This paper\ncompares the performance of several LLMs across multiple economic analysis\ntasks, finding that while each model performs well in its own right, their\ncombined performance is significantly superior to that of a single model. By\nutilizing a machine learning framework, this study also explores the potential\nof combining LLMs, identifying key factors that influence the performance of\ncombinations of LLMs. The results demonstrate that combining two or more LLMs\ncan enhance performance across various economic analysis tasks, with some LLMs\noutperforming single models. The study also highlights the potential for\ncombined models to enh",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14184397163120568,
          "p": 0.2247191011235955,
          "f": 0.17391303873383757
        },
        "rouge-2": {
          "r": 0.005128205128205128,
          "p": 0.007874015748031496,
          "f": 0.006211175347212502
        },
        "rouge-l": {
          "r": 0.14184397163120568,
          "p": 0.2247191011235955,
          "f": 0.17391303873383757
        }
      }
    },
    {
      "paper_id": "cs.PF.cs/PF/2503.02982v1",
      "true_abstract": "We consider a discrete-time parallel service system consisting of $n$\nheterogeneous single server queues with infinite capacity. Jobs arrive to the\nsystem as an i.i.d. process with rate proportional to $n$, and must be\nimmediately dispatched in the time slot that they arrive. The dispatcher is\nassumed to be able to exchange messages with the servers to obtain their queue\nlengths and make dispatching decisions, introducing an undesirable\ncommunication overhead.\n  In this setting, we propose a ultra-low communication overhead load balancing\npolicy dubbed $k$-Skip-the-$d$-Longest-Queues ($k$-SLQ-$d$), where queue\nlengths are only observed every $k(n-d)$ time slots and, between observations,\nincoming jobs are sent to a queue that is not one of the $d$ longest ones at\nthe time that the queues were last observed. For this policy, we establish\nconditions on $d$ for it to be throughput optimal and we show that, under that\ncondition, it is asymptotically delay-optimal in heavy-traffic for arbitrarily\nlow communication overheads (i.e., for arbitrarily large $k$).",
      "generated_abstract": "r studies the problem of online linear prediction for the sum of\ngaussian noise. We consider a scenario where the online learner is given a\nsingle observation at each round and needs to predict the mean and variance of\nthe next observation. The learner receives only a single sample from the\ngaussian distribution at each round. This paper is motivated by the fact that\nin many practical applications, online linear prediction is of fundamental\ninterest. Examples include online regression, online clustering, and online\nsparse regression. In these applications, the learner observes a single\nobservation at each round. Our work is an extension of the recent work of\n\\cite{zhang2023online}, which considers a single observation at each round and\na single sample from the gaussian distribution. We study the case of a\nmulti-round setting with multiple samples. Our results characterize the\nexpected accuracy of online linear prediction in this setting",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.25316455696202533,
          "f": 0.20618556218248496
        },
        "rouge-2": {
          "r": 0.0189873417721519,
          "p": 0.024793388429752067,
          "f": 0.021505371432022805
        },
        "rouge-l": {
          "r": 0.16521739130434782,
          "p": 0.24050632911392406,
          "f": 0.19587628383196953
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/GN/2405.10917v2",
      "true_abstract": "It is a challenge to estimate fund performance by compounded returns.\nArguably, it is incorrect to use yearly returns directly for compounding, with\nreported annualized return of above 60% for Medallion for the 31 years up to\n2018. We propose an estimation based on fund sizes and trading profits and\nobtain a compounded return of 31.8% before fees. Alternatively, we suggest\nusing the manager's wealth as a proxy and arriving at a compounded growth rate\nof 25.6% for Simons for the 33 years up to 2020. We conclude that the\nannualized compounded return of Medallion before fees is probably under 35%.\nOur findings have implications for correctly estimating fund performance.",
      "generated_abstract": "introduces a novel approach to the analysis of risk premiums in\nthe context of the credit default swap (CDS) market. We propose a novel\nregression-based methodology for the estimation of credit risk premiums,\nwhich provides an efficient framework for evaluating the volatility of CDS\nprices, both on a global and on a regional level. The proposed approach is\nbased on a regression model that incorporates both macroeconomic and\ninterbank-related variables, as well as the cross-sectional correlation\nrelationships between CDS prices and macroeconomic variables. The analysis\nshows that the volatility of CDS prices is significantly affected by macroeconomic\nvariables, while the influence of interbank variables is weaker. This\ndifference in the effectiveness of macroeconomic and interbank variables in\ninfluencing CDS prices can be attributed to their different characteristics.\nF",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21621621621621623,
          "p": 0.2077922077922078,
          "f": 0.21192052480329823
        },
        "rouge-2": {
          "r": 0.0392156862745098,
          "p": 0.034482758620689655,
          "f": 0.036697242727043854
        },
        "rouge-l": {
          "r": 0.1891891891891892,
          "p": 0.18181818181818182,
          "f": 0.18543045857813267
        }
      }
    },
    {
      "paper_id": "cs.MA.q-fin/EC/2502.13267v1",
      "true_abstract": "BeforeIT is an open-source software for building and simulating\nstate-of-the-art macroeconomic agent-based models (macro ABMs) based on the\nrecently introduced macro ABM developed in [1] and here referred to as the base\nmodel. Written in Julia, it combines extraordinary computational efficiency\nwith user-friendliness and extensibility. We present the main structure of the\nsoftware, demonstrate its ease of use with illustrative examples, and benchmark\nits performance. Our benchmarks show that the base model built with BeforeIT is\norders of magnitude faster than a Matlab version, and significantly faster than\nMatlab-generated C code. BeforeIT is designed to facilitate reproducibility,\nextensibility, and experimentation. As the first open-source, industry-grade\nsoftware to build macro ABMs of the type of the base model, BeforeIT can\nsignificantly foster collaboration and innovation in the field of agent-based\nmacroeconomic modelling. The package, along with its documentation, is freely\navailable at https://github.com/bancaditalia/BeforeIT.jl under the AGPL-3.0.",
      "generated_abstract": "This paper examines the use of machine learning (ML) models in equity\nresearch, focusing on the challenges of evaluating complex, uncertain data.\nWhile ML models have advanced financial analysis, their reliance on\nhuman-in-the-loop processes, such as feature engineering and domain expertise,\ncan lead to biases and errors. This paper examines these challenges in the\ncontext of stock price prediction, highlighting the need for robust data\npreprocessing, robust feature selection, and the need for effective\ndata-driven training strategies. Additionally, we discuss the importance of\nmodel interpretability, ensuring transparency and fairness in ML-driven\ninvestment decisions. By addressing these challenges, we aim to improve the\nreliability and accuracy of equity research, enhancing investors' ability to\nmake informed decisions and drive long-term returns.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11650485436893204,
          "p": 0.14285714285714285,
          "f": 0.12834224104092215
        },
        "rouge-2": {
          "r": 0.014388489208633094,
          "p": 0.017699115044247787,
          "f": 0.015873010926242406
        },
        "rouge-l": {
          "r": 0.08737864077669903,
          "p": 0.10714285714285714,
          "f": 0.09625667954359601
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.07208v1",
      "true_abstract": "In this work, various analysis methods are conducted on frequency-dependent\nmethods on SED to further delve into their detailed characteristics and\nbehaviors on SED. While SED has been rapidly advancing through the adoption of\nvarious deep learning techniques from other pattern recognition fields, these\ntechniques are often not suitable for SED. To address this issue, two\nfrequency-dependent SED methods were previously proposed: FilterAugment, a data\naugmentation randomly weighting frequency bands, and frequency dynamic\nconvolution (FDY Conv), an architecture applying frequency adaptive convolution\nkernels. These methods have demonstrated superior performance in SED, and we\naim to further analyze their detailed effectiveness and characteristics in SED.\nWe compare class-wise performance to find out specific pros and cons of\nFilterAugment and FDY Conv. We apply Gradient-weighted Class Activation Mapping\n(Grad-CAM), which highlights time-frequency region that is more inferred by the\nmodel, on SED models with and without frequency masking and two types of\nFilterAugment to observe their detailed characteristics. We propose simpler\nfrequency dependent convolution methods and compare them with FDY Conv to\nfurther understand which components of FDY Conv affects SED performance.\nLastly, we apply PCA to show how FDY Conv adapts dynamic kernel across\nfrequency dimensions on different sound event classes. The results and\ndiscussions demonstrate that frequency dependency plays a significant role in\nsound event detection and further confirms the effectiveness of frequency\ndependent methods on SED.",
      "generated_abstract": "In this paper, we propose a novel approach for the multi-channel speech\nremoval task. Our approach employs a multi-channel speech enhancement\nsub-network to extract the shared representation of the target and the\nnon-target channels. Then, the enhanced features of the non-target channels are\ncombined with the features of the target channels to form the final speech\nremoval feature. In addition, to mitigate the problem of feature over-fitting,\nwe propose to optimize the network parameters by leveraging the\nreconstruction error. Additionally, we propose a novel loss function to\nenhance the alignment between the speech features of the target and non-target\nchannels. The performance of the proposed method is validated through\nexperiments on public speech removal benchmarks. The results demonstrate that\nthe proposed method achieves state-of-the-art performance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1510791366906475,
          "p": 0.30434782608695654,
          "f": 0.20192307248936772
        },
        "rouge-2": {
          "r": 0.014218009478672985,
          "p": 0.02912621359223301,
          "f": 0.019108275846282812
        },
        "rouge-l": {
          "r": 0.12949640287769784,
          "p": 0.2608695652173913,
          "f": 0.17307691864321387
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.07461v1",
      "true_abstract": "We study the optimal management of a photovoltaic system's battery owned by a\nself-consumption group that aims to minimize energy consumption costs. We\nassume that the photovoltaic system is composed of a photovoltaic panel and a\nbattery, where the photovoltaic panel produces energy according to a certain\nstochastic process. The management of the battery is the responsibility of a\ngroup administrator, who makes the joint decision to either store part of the\nphotovoltaic energy production and sell the remaining energy at the electricity\nspot price, or discharge part of the energy stored in the battery and sell it\nin the electricity market. Inspired by European Union and Italian legislation,\nwhich promote incentives for energy transition and renewable energy production,\nwe assume that the group receives a monetary incentive for the virtual\nself-consumed energy, defined as the minimum between the power bought from the\ngrid to satisfy the group's power demand and the energy sold to the market. In\nthis case, the energy sold by the group is a mix of part of the photovoltaic\nproduction that is not stored and part of the energy discharged from the\nbattery. We model the problem as a stochastic optimal control problem, where\nthe optimal strategy is the joint charge-discharge decision that minimizes the\ngroup's energy consumption costs. We find the solution numerically by applying\na finite difference scheme to solve the Hamilton-Jacobi-Bellman equation\nassociated with the value function of the optimal control problem.",
      "generated_abstract": "the interplay between government and business in the context of\nco-production of infrastructure. We focus on the case of the Hexagonal Model\n(HM) where government and private sector cooperate in the design and\nconstruction of a large infrastructure project. The model is studied from the\nperspective of a government procurement decision maker and a private\ninvestor. We provide a complete model specification and analyze the\ninteractions between government and private sector. The main results of the\npaper are: (1) the government procurement decision maker must allocate the\nproject to the private investor based on its own evaluation of the project\ninvestment potential; (2) the private investor's decision to participate in the\nproject depends on the government's decision to allocate the project to it.\n(3) The government's decision to allocate the project to the private investor\nis determined",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.234375,
          "f": 0.1675977607690148
        },
        "rouge-2": {
          "r": 0.035,
          "p": 0.06666666666666667,
          "f": 0.0459016348293474
        },
        "rouge-l": {
          "r": 0.13043478260869565,
          "p": 0.234375,
          "f": 0.1675977607690148
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.05403v2",
      "true_abstract": "We propose a decentralized framework for guaranteeing the small-signal\nstability of future power systems with grid-forming converters. Our approach\nleverages dynamic loop-shifting techniques to compensate for the lack of\npassivity in the network dynamics and establishes decentralized parametric\nstability certificates, depending on the local device-level controls and\nincorporating the effects of the network dynamics. By following practical\ntuning rules, we are able to ensure plug-and-play operation without centralized\ncoordination. Unlike prior works, our approach accommodates coupled frequency\nand voltage dynamics, incorporates network dynamics, and does not rely on\nspecific network configurations or operating points, offering a general and\nscalable solution for the integration of power-electronics-based devices into\nfuture power systems. We validate our theoretical stability results through\nnumerical case studies in a high-fidelity simulation model.",
      "generated_abstract": "opment of autonomous driving technology is accelerating, and\nautomated driving systems (ADS) are gradually replacing human drivers. In\nthis paper, a novel framework for autonomous driving control is proposed,\nincluding vehicle control and perception. The vehicle control framework\nconsists of two main modules: the trajectory planning module and the control\nmodule. The trajectory planning module is responsible for optimizing the\nvehicle's trajectory, which is based on the vehicle's current location,\nobserved road conditions, and driving policies. The control module is\nresponsible for driving the vehicle, and it consists of two main modules:\nlane keeping module and path following module. The lane keeping module\ndetermines the vehicle's lane by using lane change strategies, and the path\nfollowing module calculates the optimal path for the vehicle to follow. The\nmain advantage of the proposed framework is that it can effectively integrate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12087912087912088,
          "p": 0.15492957746478872,
          "f": 0.13580246421201053
        },
        "rouge-2": {
          "r": 0.03361344537815126,
          "p": 0.03418803418803419,
          "f": 0.03389830008510559
        },
        "rouge-l": {
          "r": 0.12087912087912088,
          "p": 0.15492957746478872,
          "f": 0.13580246421201053
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.20857v1",
      "true_abstract": "Sound event detection (SED) has significantly benefited from self-supervised\nlearning (SSL) approaches, particularly masked audio transformer for SED\n(MAT-SED), which leverages masked block prediction to reconstruct missing audio\nsegments. However, while effective in capturing global dependencies, masked\nblock prediction disrupts transient sound events and lacks explicit enforcement\nof temporal order, making it less suitable for fine-grained event boundary\ndetection. To address these limitations, we propose JiTTER (Jigsaw Temporal\nTransformer for Event Reconstruction), an SSL framework designed to enhance\ntemporal modeling in transformer-based SED. JiTTER introduces a hierarchical\ntemporal shuffle reconstruction strategy, where audio sequences are randomly\nshuffled at both the block-level and frame-level, forcing the model to\nreconstruct the correct temporal order. This pretraining objective encourages\nthe model to learn both global event structures and fine-grained transient\ndetails, improving its ability to detect events with sharp onset-offset\ncharacteristics. Additionally, we incorporate noise injection during block\nshuffle, providing a subtle perturbation mechanism that further regularizes\nfeature learning and enhances model robustness. Experimental results on the\nDESED dataset demonstrate that JiTTER outperforms MAT-SED, achieving a 5.89%\nimprovement in PSDS, highlighting the effectiveness of explicit temporal\nreasoning in SSL-based SED. Our findings suggest that structured temporal\nreconstruction tasks, rather than simple masked prediction, offer a more\neffective pretraining paradigm for sound event representation learning.",
      "generated_abstract": "r proposes a multi-agent architecture for robot navigation in\nmulti-agent, multi-objective environments. It aims to solve the task of\nguiding a fleet of robots to collect a target object in an unknown environment\nby optimizing their trajectories under the constraints of object detection,\ncollision avoidance, and localization. To address the challenges of\nmulti-agent and multi-objective, we introduce a dynamic multi-agent\nrepresentation and a multi-agent constraint learning framework, which\nrepresent the agents as multi-agent systems and learn multi-agent constraints\nthrough a multi-agent constraint learning framework. To ensure the\nconsistency between the multi-agent representations and the multi-agent\nconstraints, we propose a multi-agent constraint learning framework that\noptimizes the multi-agent constraints. Experimental results show that the\nproposed multi-agent architecture can successfully optimize the trajectories of\nfleets",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13245033112582782,
          "p": 0.2702702702702703,
          "f": 0.17777777336335815
        },
        "rouge-2": {
          "r": 0.014563106796116505,
          "p": 0.02727272727272727,
          "f": 0.01898733723361749
        },
        "rouge-l": {
          "r": 0.11920529801324503,
          "p": 0.24324324324324326,
          "f": 0.15999999558558037
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2409.11451v1",
      "true_abstract": "Experiment 1. Rooting of quince hardwood cuttings: Rooting success was\ninfluenced by both the concentrations of IBA and the selection of rooting\nmedia. However, the control group (without IBA) notably enhanced rooting when\ncompared to the various IBA concentrations. Cuttings in the control group\n(without IBA) and those planted in river sand exhibited notably high\npercentages of successful rooting, underscoring the importance of the selected\nplanting medium. Experiment 2. Bench grafting of loquat: The success of\ngrafting loquat cutting stocks varied based on grafting dates, types of\ncuttings, and concentrations of IBA. However, IBA at different concentrations\ndid not have a significant impact. Notably, certain interactions such as\ngrafting on February 20 with loquat stock cuttings, yielded higher percentages\nof successful graft bud sprouting. Experiment 3. Performance of grafting\nloquats onto different rootstocks: Grafting success was notably influenced by\nthe selection of rootstock, with loquat rootstock demonstrating superior\nperformance compared to quince. The highest significant levels of successful\ngrafting were attained on February 20, underscoring the crucial role of\ngrafting dates. Experiment 4. Impact of tree stock types on grafting success:\nGrafting success percentage was higher in loquat tree stock when compared to\nquince. The consistency of grafting success percentages across three dates\nunderscores the significant influence of rootstock type. Experiment 5. Bench\ngrafting of loquat cutting stocks: Graft bud sprout percentages exhibited\nvariations, with loquat stock cuttings surpassing quince. Grafting success\ndemonstrated a consistent increase from February 20 to March 30, underscoring\nthe importance of selecting appropriate grafting dates.",
      "generated_abstract": "of the effects of foods on the human body is known as Nutrition\noften referred to as the study of the effects of foods on the human body is known\nas Nutrition. The foods we eat are the main source of calories for our body.\nCalories are the energy used by the body for the maintenance and growth of the\nbody. Calories are the energy that is used by the body for the maintenance and\ngrowth of the body. The calories in the foods we eat are the main source of\nenergy for the body. The foods we eat are the main source of energy for the body\nand are the main source of calories. The foods we eat are the main source of\nenergy for the body and are the main source of calories. The foods we eat are\nthe main source of energy for the body and are the main source of calories. The\nfo",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07086614173228346,
          "p": 0.2647058823529412,
          "f": 0.11180123890436336
        },
        "rouge-2": {
          "r": 0.014218009478672985,
          "p": 0.05084745762711865,
          "f": 0.02222221880685924
        },
        "rouge-l": {
          "r": 0.06299212598425197,
          "p": 0.23529411764705882,
          "f": 0.09937887865591617
        }
      }
    },
    {
      "paper_id": "cs.NE.q-fin/PM/2501.14736v1",
      "true_abstract": "In this study, we applied the NEAT (NeuroEvolution of Augmenting Topologies)\nalgorithm to stock trading using multiple technical indicators. Our approach\nfocused on maximizing earning, avoiding risk, and outperforming the Buy & Hold\nstrategy. We used progressive training data and a multi-objective fitness\nfunction to guide the evolution of the population towards these objectives. The\nresults of our study showed that the NEAT model achieved similar returns to the\nBuy & Hold strategy, but with lower risk exposure and greater stability. We\nalso identified some challenges in the training process, including the presence\nof a large number of unused nodes and connections in the model architecture. In\nfuture work, it may be worthwhile to explore ways to improve the NEAT algorithm\nand apply it to shorter interval data in order to assess the potential impact\non performance.",
      "generated_abstract": "opment of large language models (LLMs) has inspired a surge in\nthe use of generative AI (GAI) for financial applications. These applications\nhave the potential to transform financial services, but they face significant\nchallenges. One of the key challenges is that existing GAI models often\nrely on large pre-trained language models (PLMs) to generate synthetic data\n(e.g., stocks, bonds, futures). However, these PLMs are trained on large\ndatasets that are unsuitable for GAI applications. Another challenge is that\nfinancial GAI models typically rely on large datasets to train, which can\nbecome expensive and time-consuming. Additionally, existing financial GAI\nmodels often focus on a limited set of financial instruments, limiting their\napplicability in other financial domains. To address these challenges, we\npropose a novel financial G",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15463917525773196,
          "p": 0.17857142857142858,
          "f": 0.16574585137938416
        },
        "rouge-2": {
          "r": 0.023076923076923078,
          "p": 0.02586206896551724,
          "f": 0.02439023891863411
        },
        "rouge-l": {
          "r": 0.14432989690721648,
          "p": 0.16666666666666666,
          "f": 0.15469612762247809
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2503.01053v1",
      "true_abstract": "Each period, two players bargain over a unit of surplus. Each player chooses\nbetween remaining flexible and committing to a take-it-or-leave-it offer at a\ncost. If players' committed demands are incompatible, then the current-period\nsurplus is destroyed in the conflict. When both players are flexible, the\nsurplus is split according to the status quo, which is the division in the last\nperiod where there was no conflict. We show that when players are patient and\nthe cost of commitment is small, there exist a class of symmetric Markov\nPerfect equilibria that are asymptotically efficient and renegotiation proof,\nin which players commit to fair demands in almost all periods.",
      "generated_abstract": "the problem of optimizing a set of discrete decisions, each with a\nnon-negative utility function. A key challenge is the combinatorial nature of\nthe decision set. We present a novel algorithm for this problem that can be\nimplemented in polynomial time. Our approach is based on a novel concept of\ndecision sets, which are sets of decisions that are both tractable and\nfeasible. We show that this concept is equivalent to the classical notion of\ndecision sets and provide a characterization of those decision sets that are\nfeasible. We also introduce a variant of our algorithm, which is based on\ndecision sets of finite length. This variant provides a polynomial-time\nimplementable algorithm for the general case. We apply our algorithm to\noptimizing a set of discrete decisions that is a subset of a continuous\nmulti-dimensional space. We show that this problem is NP-hard and present a\npol",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18055555555555555,
          "p": 0.17333333333333334,
          "f": 0.17687074330140232
        },
        "rouge-2": {
          "r": 0.057692307692307696,
          "p": 0.047619047619047616,
          "f": 0.05217390808922543
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.16,
          "f": 0.16326530112453158
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2501.12841v1",
      "true_abstract": "This study systematically examines how several alternative approaches\nconsidered affect three aspects that determine portfolio performance (the gross\nreturn, the transaction costs and the portfolio risk). We find that it is\ndifficult to exploit the possible predictability of asset returns. However, the\npredictability of asset return volatility produces obvious economic value,\nalthough in a highly correlated cryptocurrencies market.",
      "generated_abstract": "r addresses the problem of designing optimal trading strategies in\nmarkets with an incomplete information. The incomplete information is caused by\nthe existence of market makers, which act as information brokers. Market\nmakers may also be strategic and may act as agents in the market. We consider\nthe case of a single market maker. The market maker is an informed trader who\ntrades based on his/her own information and tries to maximize his/her profits.\nThe trading strategies proposed by the market maker can be either buy or sell\nstrategies. We focus on a two-asset model with a single market maker. We study\nthe case where the market maker has no information and we also consider the\ncase where the market maker has information but may be strategic. We present\nseveral optimal trading strategies for the case of a single market maker.\nFinally",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22,
          "p": 0.15492957746478872,
          "f": 0.1818181769687864
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.22,
          "p": 0.15492957746478872,
          "f": 0.1818181769687864
        }
      }
    },
    {
      "paper_id": "math.OC.econ/TH/2502.11780v2",
      "true_abstract": "This paper studies distributionally robust optimization for a large class of\nrisk measures with ambiguity sets defined by $\\phi$-divergences. The risk\nmeasures are allowed to be non-linear in probabilities, are represented by a\nChoquet integral possibly induced by a probability weighting function, and\ninclude many well-known examples (for example, CVaR, Mean-Median Deviation,\nGini-type). Optimization for this class of robust risk measures is challenging\ndue to their rank-dependent nature. We show that for many types of probability\nweighting functions including concave, convex and inverse $S$-shaped, the\nrobust optimization problem can be reformulated into a rank-independent\nproblem. In the case of a concave probability weighting function, the problem\ncan be further reformulated into a convex optimization problem with finitely\nmany constraints that admits explicit conic representability for a collection\nof canonical examples. While the number of constraints in general scales\nexponentially with the dimension of the state space, we circumvent this\ndimensionality curse and provide two types of upper and lower bounds\nalgorithms. They yield tight upper and lower bounds on the exact optimal value\nand are formally shown to converge asymptotically. This is illustrated\nnumerically in two examples given by a robust newsvendor problem and a robust\nportfolio choice problem.",
      "generated_abstract": "This paper studies a cooperative game between two agents in which one agent\nis the leader and the other is the follower. We consider a situation in which\nthe leader can choose to either inform the follower of the decision or not. We\nshow that the follower can distinguish whether the leader informs the follower\nof the decision based on the number of times the follower sends a request to\nthe leader. Moreover, we prove that the follower will eventually find out the\ndecision of the leader if the leader is not cooperative. This implies that the\nfollower can design a protocol to distinguish whether the leader is\ncooperative or not. We also prove that the follower will eventually find out\nthe decision of the leader if the leader is cooperative.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.3333333333333333,
          "f": 0.21052631146814413
        },
        "rouge-2": {
          "r": 0.0449438202247191,
          "p": 0.09302325581395349,
          "f": 0.06060605621326937
        },
        "rouge-l": {
          "r": 0.1282051282051282,
          "p": 0.2777777777777778,
          "f": 0.17543859216989854
        }
      }
    },
    {
      "paper_id": "stat.AP.physics/ao-ph/2503.09065v1",
      "true_abstract": "Contributions from photosynthesis and other natural components of the carbon\ncycle present the largest uncertainties in our understanding of carbon dioxide\n(CO$_2$) sources and sinks. While the global spatiotemporal distribution of the\nnet flux (the sum of all contributions) can be inferred from atmospheric CO$_2$\nconcentrations through flux inversion, attributing the net flux to its\nindividual components remains challenging. The advent of solar-induced\nfluorescence (SIF) satellite observations provides an opportunity to isolate\nnatural components by anchoring gross primary productivity (GPP), the\nphotosynthetic component of the net flux. Here, we introduce a novel\nstatistical flux-inversion framework that simultaneously assimilates\nobservations of SIF and CO$_2$ concentration, extending WOMBAT v2.0 (WOllongong\nMethodology for Bayesian Assimilation of Trace-gases, version 2.0) with a\nhierarchical model of spatiotemporal dependence between GPP and SIF processes.\nWe call the new framework WOMBAT v2.S, and we apply it to SIF and CO$_2$ data\nfrom NASA's Orbiting Carbon Observatory-2 (OCO-2) satellite and other\ninstruments to estimate natural fluxes over the globe during a recent six-year\nperiod. In a simulation experiment that matches OCO-2's retrieval\ncharacteristics, the inclusion of SIF improves accuracy and uncertainty\nquantification of component flux estimates. Comparing estimates from WOMBAT\nv2.S, v2.0, and the independent FLUXCOM initiative, we observe that linking GPP\nto SIF has little effect on net flux, as expected, but leads to spatial\nredistribution and more realistic seasonal structure in natural flux\ncomponents.",
      "generated_abstract": "udy of the dynamical behavior of fluid flows, the concept of\nscaling is central. While the classical scaling relations in the literature are\nsufficient to describe the velocity field in a variety of situations, the\ninvestigation of the full Navier-Stokes equations (NSEs) in the context of\nscaling remains a challenging task. In this study, we propose a novel scaling\nrelation that provides a unified description of the velocity field in the\nframework of the NSEs. Our scaling relation is derived by combining the\nHydrodynamic-Elastic-Tension (HET) theory with the notion of the dynamic\nenergy dissipation rate. We show that our scaling relation can be expressed as\na linear combination of the HET theory and the energy dissipation rate. The\nmain advantage of our scaling relation is that it is applicable to a wide range\nof NSEs, including the incompress",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1568627450980392,
          "p": 0.2962962962962963,
          "f": 0.20512820060157802
        },
        "rouge-2": {
          "r": 0.02304147465437788,
          "p": 0.04201680672268908,
          "f": 0.029761900187252693
        },
        "rouge-l": {
          "r": 0.1568627450980392,
          "p": 0.2962962962962963,
          "f": 0.20512820060157802
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2503.10132v1",
      "true_abstract": "This paper analyzes Shinohara Rock-Paper-Scissors (RPS), a variant of the\nclassic RPS game introduced by board game designer Yoshiteru Shinohara. In this\ngame, players compete against a host who always plays rock, so players choose\neither rock or paper. The catch is that if two or more players choose paper,\nthey are eliminated, creating strategic tension among the players. The last\nremaining player wins. We derive subgame perfect equilibria (SPE) of Shinohara\nRPS. A unique symmetric SPE exists, in which the probability of choosing paper\nsatisfies the equation $(1-p)^{n-1} + p^{n-1}/n = 1/n$. The game also admits a\ncontinuum of asymmetric SPE, making it unlikely that any specific SPE will be\nobserved in actual play.",
      "generated_abstract": "In this paper, we study a class of continuous-time, discrete-time, and\ncontinuous-time, continuous-time Markov decision processes (MDPs). We show\nthat for any MDP with finite state space, the optimal policy is\nindependent. Moreover, we show that the optimal policy is unique in the sense\nthat the optimal value function is also unique. Finally, we show that the\noptimality of the optimal policy is preserved even when the policy is not\ndeterministic. Our results generalize those in [Ho, 2004",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14444444444444443,
          "p": 0.2549019607843137,
          "f": 0.18439715850309352
        },
        "rouge-2": {
          "r": 0.017699115044247787,
          "p": 0.03125,
          "f": 0.02259886543968941
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.23529411764705882,
          "f": 0.17021276133997296
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.16574v1",
      "true_abstract": "Algorithmic agents are used in a variety of competitive decision settings,\nnotably in making pricing decisions in contexts that range from online retail\nto residential home rentals. Business managers, algorithm designers, legal\nscholars, and regulators alike are all starting to consider the ramifications\nof \"algorithmic collusion.\" We study the emergent behavior of multi-armed\nbandit machine learning algorithms used in situations where agents are\ncompeting, but they have no information about the strategic interaction they\nare engaged in. Using a general-form repeated Prisoner's Dilemma game, agents\nengage in online learning with no prior model of game structure and no\nknowledge of competitors' states or actions (e.g., no observation of competing\nprices). We show that these context-free bandits, with no knowledge of\nopponents' choices or outcomes, still will consistently learn collusive\nbehavior - what we call \"naive collusion.\" We primarily study this system\nthrough an analytical model and examine perturbations to the model through\nsimulations.\n  Our findings have several notable implications for regulators. First, calls\nto limit algorithms from conditioning on competitors' prices are insufficient\nto prevent algorithmic collusion. This is a direct result of collusion arising\neven in the naive setting. Second, symmetry in algorithms can increase\ncollusion potential. This highlights a new, simple mechanism for\n\"hub-and-spoke\" algorithmic collusion. A central distributor need not imbue its\nalgorithm with supra-competitive tendencies for apparent collusion to arise; it\ncan simply arise by using certain (common) machine learning algorithms.\nFinally, we highlight that collusive outcomes depend starkly on the specific\nalgorithm being used, and we highlight market and algorithmic conditions under\nwhich it will be unknown a priori whether collusion occurs.",
      "generated_abstract": "-19 pandemic has had a profound impact on the global economy,\nresulting in a significant decline in economic activity. This study examines\nthe effects of the pandemic on the labor market using data from the United\nStates and several European countries. The analysis reveals that the pandemic\nhad a significant impact on labor market participation and employment, with\nparticularly pronounced effects in the euro area and the United States. The\nstudy identifies the main drivers of the economic downturn, including the\neffects of the pandemic on business activity, the response of governments to\nthe crisis, and the impact of the crisis on labor markets. It concludes that\nthe effects of the pandemic on the labor market will be long-lasting and\nsignificant, with significant implications for economic growth and job\ncreation. The study provides insights into the impact of the pandemic on the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10555555555555556,
          "p": 0.2676056338028169,
          "f": 0.1513944182536786
        },
        "rouge-2": {
          "r": 0.023166023166023165,
          "p": 0.05555555555555555,
          "f": 0.032697543530355644
        },
        "rouge-l": {
          "r": 0.10555555555555556,
          "p": 0.2676056338028169,
          "f": 0.1513944182536786
        }
      }
    },
    {
      "paper_id": "cs.SD.cs/SD/2503.06984v1",
      "true_abstract": "Video-to-audio generation is essential for synthesizing realistic audio\ntracks that synchronize effectively with silent videos. Following the\nperspective of extracting essential signals from videos that can precisely\ncontrol the mature text-to-audio generative diffusion models, this paper\npresents how to balance the representation of mel-spectrograms in terms of\ncompleteness and complexity through a new approach called Mel\nQuantization-Continuum Decomposition (Mel-QCD). We decompose the\nmel-spectrogram into three distinct types of signals, employing quantization or\ncontinuity to them, we can effectively predict them from video by a devised\nvideo-to-all (V2X) predictor. Then, the predicted signals are recomposed and\nfed into a ControlNet, along with a textual inversion design, to control the\naudio generation process. Our proposed Mel-QCD method demonstrates\nstate-of-the-art performance across eight metrics, evaluating dimensions such\nas quality, synchronization, and semantic consistency. Our codes and demos will\nbe released at \\href{Website}{https://wjc2830.github.io/MelQCD/}.",
      "generated_abstract": "asing adoption of AI-driven technologies in medical applications has\nprompted the need for more robust and secure medical data storage and\nmanagement. While existing data storage solutions are tailored for specific\ndata types, they struggle to accommodate various medical data formats,\nincluding images, audio, and video. This paper presents a novel data storage\narchitecture that is designed to address the limitations of existing data\nstorage solutions by integrating a blockchain-based data management system\nwith secure data storage. The proposed architecture leverages the blockchain\nnetwork to ensure data integrity and confidentiality, while also providing\nefficient storage for medical data. The proposed architecture is validated\nthrough extensive experiments, demonstrating its ability to store, access, and\nsecurely manage medical data in a secure and efficient manner. The results\ndemonstrate that the proposed architecture outperforms existing data storage\nsolutions in terms of storage capacity, security, and accessibility",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16071428571428573,
          "p": 0.21428571428571427,
          "f": 0.18367346448979607
        },
        "rouge-2": {
          "r": 0.02158273381294964,
          "p": 0.023622047244094488,
          "f": 0.02255638598762053
        },
        "rouge-l": {
          "r": 0.15178571428571427,
          "p": 0.20238095238095238,
          "f": 0.173469382857143
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.09762v1",
      "true_abstract": "We study a centralized discrete-time dynamic two-way matching model with\nfinitely many agent types. Agents arrive stochastically over time and join\ntheir type-dedicated queues waiting to be matched. We focus on\nstate-independent greedy policies that achieve constant regret at all times by\nmaking matching decisions based solely on agent availability across types,\nrather than requiring complete queue-length information. Such policies are\nparticularly appealing for life-saving applications such as kidney exchange, as\nthey require less information and provide more transparency compared to\nstate-dependent policies.\n  First, for acyclic matching networks, we analyze a deterministic priority\npolicy proposed by Kerimov et al. [2023] that follows a static priority order\nover matches. We derive the first explicit regret bound in terms of the general\nposition gap (GPG) parameter $\\epsilon$, which measures the distance of the\nfluid relaxation from degeneracy. Second, for general two-way matching\nnetworks, we design a randomized state-independent greedy policy that achieves\nconstant regret with optimal scaling $O(\\epsilon^{-1})$, matching the existing\nlower bound established by Kerimov et al. [2024].",
      "generated_abstract": "We prove the first known upper bound on the space complexity of computing\nprimes of small size. We show that the space complexity of computing primes in\n$\\mathbb{N}$ is bounded above by $2^{\\Omega(n)}$ and that the space complexity\nof computing primes in $\\mathbb{Z}_n$ is bounded above by $2^{\\Omega(n^2)}$.\n  We also prove that the space complexity of computing primes in $\\mathbb{Z}_n$\nis $2^{\\Omega(n)}$ if $n$ is a power of $2$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09016393442622951,
          "p": 0.3333333333333333,
          "f": 0.14193548051945898
        },
        "rouge-2": {
          "r": 0.006369426751592357,
          "p": 0.022727272727272728,
          "f": 0.009950245336502747
        },
        "rouge-l": {
          "r": 0.08196721311475409,
          "p": 0.30303030303030304,
          "f": 0.12903225471300736
        }
      }
    },
    {
      "paper_id": "math.AT.math/AT/2503.01614v1",
      "true_abstract": "Recently, bipath persistent homology has been proposed as an extension of\nstandard persistent homology, along with its visualization (bipath persistence\ndiagram) and computational methods. In the setting of standard persistent\nhomology, the stability theorem with respect to real-valued functions on a\ntopological space is one of the fundamental results, which gives a mathematical\njustification for using persistent homology to noisy data. In proving the\nstability theorem, the algebraic stability theorem/the isometry theorem for\npersistence modules plays a central role. In this point of view, the stability\nproperty for bipath persistent homology is desired for analyzing data. In this\npaper, we prove the stability theorem of bipath persistent homology with\nrespect to bipath functions on a topological space. This theorem suggests a\nstability of bipath persistence diagrams: small changes in a bipath function\n(except at their ends) result in only small changes in the bipath persistence\ndiagram. Similar to the stability theorem of standard persistent homology, we\ndeduce the stability theorem of bipath persistent homology by using the\nalgebraic stability theorem/the isometry theorem of bipath persistence modules.",
      "generated_abstract": "We present a generalization of the classical Kneser conjecture, which\ninvestigates the topological structure of the union of $n$ parallel lines in\nthe plane. We prove that for any $n \\geq 4$, the union of $n$ parallel lines\nin the plane has either $n$ or $n-1$ connected components. Furthermore, we\nprovide a counterexample for $n=5$ showing that this generalization of the\nKneser conjecture does not hold in the case $n=5$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13253012048192772,
          "p": 0.25,
          "f": 0.173228341928204
        },
        "rouge-2": {
          "r": 0.015384615384615385,
          "p": 0.03508771929824561,
          "f": 0.021390370093512235
        },
        "rouge-l": {
          "r": 0.0963855421686747,
          "p": 0.18181818181818182,
          "f": 0.12598424744001505
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/TR/2412.19245v1",
      "true_abstract": "We investigate the efficacy of large language models (LLMs) in sentiment\nanalysis of U.S. financial news and their potential in predicting stock market\nreturns. We analyze a dataset comprising 965,375 news articles that span from\nJanuary 1, 2010, to June 30, 2023; we focus on the performance of various LLMs,\nincluding BERT, OPT, FINBERT, and the traditional Loughran-McDonald dictionary\nmodel, which has been a dominant methodology in the finance literature. The\nstudy documents a significant association between LLM scores and subsequent\ndaily stock returns. Specifically, OPT, which is a GPT-3 based LLM, shows the\nhighest accuracy in sentiment prediction with an accuracy of 74.4%, slightly\nahead of BERT (72.5%) and FINBERT (72.2%). In contrast, the Loughran-McDonald\ndictionary model demonstrates considerably lower effectiveness with only 50.1%\naccuracy. Regression analyses highlight a robust positive impact of OPT model\nscores on next-day stock returns, with coefficients of 0.274 and 0.254 in\ndifferent model specifications. BERT and FINBERT also exhibit predictive\nrelevance, though to a lesser extent. Notably, we do not observe a significant\nrelationship between the Loughran-McDonald dictionary model scores and stock\nreturns, challenging the efficacy of this traditional method in the current\nfinancial context. In portfolio performance, the long-short OPT strategy excels\nwith a Sharpe ratio of 3.05, compared to 2.11 for BERT and 2.07 for FINBERT\nlong-short strategies. Strategies based on the Loughran-McDonald dictionary\nyield the lowest Sharpe ratio of 1.23. Our findings emphasize the superior\nperformance of advanced LLMs, especially OPT, in financial market prediction\nand portfolio management, marking a significant shift in the landscape of\nfinancial analysis tools with implications to financial regulation and policy\nanalysis.",
      "generated_abstract": "aper, we propose a deep learning-based model that forecasts\nequity prices by combining historical data, a forecasting model, and\nfeature-enhanced stock market data. The proposed model, called the\nMulti-Layer Perceptron (MLP), is an extension of the multi-layer perceptron\n(MLP) neural network and incorporates features that were previously ignored in\ntraditional MLP models. In addition, we explore the use of historical data to\nimprove forecasting accuracy. The results show that the MLP model's forecast\naccuracy increases significantly when historical data is used. Furthermore, we\npresent an empirical study of the effectiveness of the MLP model in predicting\nstock market prices, demonstrating that it can successfully forecast future\nprices. Our findings underscore the importance of incorporating historical data\ninto the MLP model's training process, as well as",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14457831325301204,
          "p": 0.2857142857142857,
          "f": 0.19199999553792008
        },
        "rouge-2": {
          "r": 0.01568627450980392,
          "p": 0.034482758620689655,
          "f": 0.021563338019922246
        },
        "rouge-l": {
          "r": 0.12048192771084337,
          "p": 0.23809523809523808,
          "f": 0.15999999553792013
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.12116v1",
      "true_abstract": "Do home prices incorporate flood risk in the immediate aftermath of specific\nflood events, or is it the repeated exposure over the years that plays a more\nsignificant role? We address this question through the first systematic study\nof the Italian housing market, which is an ideal case study because it is\nhighly exposed to floods, though unevenly distributed across the national\nterritory. Using a novel dataset containing about 550,000 mortgage-financed\ntransactions between 2016 and 2024, as well as hedonic regressions and a\ndifference-in-difference design, we find that: (i) specific floods do not\ndecrease home prices in areas at risk; (ii) the repeated exposure to floods in\nflood-prone areas leads to a price decline, up to 4\\% in the most frequently\nflooded regions; (iii) responses are heterogeneous by buyers' income and age.\nYoung buyers (with limited exposure to prior floods) do not obtain any price\nreduction for settling in risky areas, while experienced buyers do. At the same\ntime, buyers who settle in risky areas have lower incomes than buyers in safe\nareas in the most affected regions. Our results emphasize the importance of\ncultural and institutional factors in understanding how flood risk affects the\nhousing market and socioeconomic outcomes.",
      "generated_abstract": "y examines the effects of COVID-19 on the U.S. housing market,\nusing data from the National Association of Realtors (NAR) Multiple Listing\nServices (MLS) for the period 2013-2024. The research employs a\ndynamic-discrete choice model with endogenous treatment variables,\nheterogeneity, and random effects, and examines the impact of the pandemic on\nthe real estate market using a series of linear and nonlinear regressions. The\nfindings indicate that the pandemic had a significant negative impact on\nhousehold housing decisions. Specifically, the study shows that homeownership\nand housing equity declined by 2.4 percent and 4.4 percent, respectively,\nfollowed by an increase in the number of renters by 1.4 percent. In addition,\nthe study finds that the pandemic significantly increased",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10714285714285714,
          "p": 0.18518518518518517,
          "f": 0.13574660169120223
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.02654867256637168,
          "f": 0.019867544985528052
        },
        "rouge-l": {
          "r": 0.09285714285714286,
          "p": 0.16049382716049382,
          "f": 0.11764705417989003
        }
      }
    },
    {
      "paper_id": "math.HO.math/HO/2502.17533v1",
      "true_abstract": "The constant $\\pi$ has fascinated scholars for centuries, inspiring the\nderivation of countless formulas rooted in profound mathematical insight. This\nabundance of formulas raises a question: Are they interconnected, and can a\nunifying structure explain their relationships?\n  We propose a systematic methodology for discovering and proving formula\nequivalences, leveraging modern large language models, large-scale data\nprocessing, and novel mathematical algorithms. Analyzing 457,145 arXiv papers,\nover a third of the validated formulas for $\\pi$ were proven to be derivable\nfrom a single mathematical object - including formulas by Euler, Gauss, Lord\nBrouncker, and newer ones from algorithmic discoveries by the Ramanujan\nMachine.\n  Our approach extends to other constants, such as $e$, $\\zeta(3)$, and\nCatalan's constant, proving its broad applicability. This work represents a\nstep toward the automatic unification of mathematical knowledge, laying a\nfoundation for AI-driven discoveries of connections across scientific domains.",
      "generated_abstract": "aper, we prove that if $\\mathscr{H}$ is a class of unital, separable\nalgebraic Hilbert space that contains all finite-dimensional subspaces of a\nHilbert space, then there is a positive, bounded, and trace-class operator\n$T$ on $\\mathscr{H}$ such that $T\\mathscr{H}$ is the class of Hilbert space\nsubspaces of $\\mathscr{H}$ that are finite-dimensional and $T$ is a\n$\\mathscr{H}$-operator. In other words, for every Hilbert space $\\mathscr{H}$\nwith a positive, bounded, and trace-class operator $T$, there is a Hilbert\nspace $\\mathscr{H}'$ and a finite-dimensional Hilbert space $F$ with a\npositive, bounded, and trace-class operator $T'$ such that $T\\mathscr{H} =\n\\mathscr{H}'$ and $T",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06481481481481481,
          "p": 0.14583333333333334,
          "f": 0.08974358548323493
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.06481481481481481,
          "p": 0.14583333333333334,
          "f": 0.08974358548323493
        }
      }
    }
  ]
}