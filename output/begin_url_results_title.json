{
  "model_path": "models/begin_url/final_model",
  "evaluation_params": {
    "max_new_tokens": 200,
    "temperature": 0.7,
    "top_p": 0.9,
    "batch_size": 32,
    "num_samples": 1604
  },
  "average_scores": {
    "rouge-1": 0.2459315228732402,
    "rouge-2": 0.05312287586704228,
    "rouge-l": 0.22452057476225445
  },
  "evaluation_time": 275.1315002441406,
  "samples_per_second": 5.8299394964832265,
  "individual_results": [
    {
      "paper_id": "econ.EM.econ/EM/2411.16978v2",
      "true_abstract": "We establish normal approximation in the Wasserstein metric and central limit\ntheorems for both non-degenerate and degenerate U-statistics with\ncross-sectionally dependent samples using Stein's method. For the\nnon-degenerate case, our results extend recent studies on the asymptotic\nproperties of sums of cross-sectionally dependent random variables. The\ndegenerate case is more challenging due to the additional dependence induced by\nthe nonlinearity of the U-statistic kernel. Through a specific implementation\nof Stein's method, we derive convergence rates under conditions on the mixing\nrate, the sparsity of the cross-sectional dependence structure, and the moments\nof the U-statistic kernel. Finally, we demonstrate the application of our\ntheoretical results with a nonparametric specification test for data with\ncross-sectional dependence.",
      "generated_abstract": "We consider the problem of estimating a function $f$ of a random vector\n$X$ that has a U-statistic with a cross-sectional dependence structure. We\npropose a method to estimate $f$ using a simple and efficient estimator\nbased on the Central Limit Theorem and a normal approximation for the U-statistic\ndistribution. We show that the estimator is consistent and asymptotically\nnormal in the class of U-statistics with a constant cross-sectional dependence\nstructure. We also establish the consistency of the estimator under a more\ngeneral class of U-statistics with cross-sectional dependence structures.\nNumerical studies and a simulation study are presented to illustrate the\neffectiveness of our proposed method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3246753246753247,
          "p": 0.4032258064516129,
          "f": 0.35971222527405416
        },
        "rouge-2": {
          "r": 0.09433962264150944,
          "p": 0.10526315789473684,
          "f": 0.09950248257716418
        },
        "rouge-l": {
          "r": 0.2597402597402597,
          "p": 0.3225806451612903,
          "f": 0.2877697792308887
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.17215v1",
      "true_abstract": "This paper presents an integrated modelling assessment that estimated the\nsensitivities of five endogenous factors in commercial rangelands, i.e. number\nof active farmers, profits, stocking rate, standing herbage biomass, and soil\nerosion, to the same percentage variation in 70 factors, including economic and\nclimate drivers. The assessment utilised a system dynamics model (107\nequations) which represents an area of extensive private farms, its farmers,\nthe main local markets on which they trade, and key ecosystem services\ninvolved. The assessment procedure consisted in analysing the behaviours of\n288,000 variants of this system during 300 years, each under a different\neconomic and climate scenario. Our key findings were as follows: 1) It is\nlikely that at least annual grasslands will suffer environmental degradation in\nthe future, and that such degradation will be primarily caused by climate\nchange, not by the increasing demand for livestock products; 2) Private farming\nsystems provide social and economic security to farmers against the effects of\nclimate change, especially in a scenario of rising prices of animal products.\nHowever, this research will remain incomplete until its methods and results can\nbe contrasted with other similar assessments.",
      "generated_abstract": "f natural grasslands (NGs) for agricultural purposes has increased\nin many regions, but the economic, social, and environmental impacts of NGs\nare not well understood. We developed a multidisciplinary integrated model to\nexplore the economic, social, and environmental impacts of NGs in the\nHuancavelica region of Peru. Our model simulated the impacts of NG expansion on\nthe local economy, agricultural production, and ecosystem services. We used\nsensor data to assess soil and water quality and land cover changes, and\nemployed satellite data to assess NG expansion. We conducted a sensitivity\nanalysis to assess the impacts of changing economic and social conditions and\nclimatic variability. Our results showed that NG expansion could enhance\neconomic development in the region, especially in areas where the economic\ndevelopment potential was the highest. However, increased NG",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17266187050359713,
          "p": 0.2962962962962963,
          "f": 0.21818181352933894
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.035398230088495575,
          "f": 0.026936022221769575
        },
        "rouge-l": {
          "r": 0.12949640287769784,
          "p": 0.2222222222222222,
          "f": 0.16363635898388443
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.15264v1",
      "true_abstract": "Speech recognition systems often face challenges due to domain mismatch,\nparticularly in real-world applications where domain-specific data is\nunavailable because of data accessibility and confidentiality constraints.\nInspired by Retrieval-Augmented Generation (RAG) techniques for large language\nmodels (LLMs), this paper introduces a LLM-based retrieval-augmented speech\nrecognition method that incorporates domain-specific textual data at the\ninference stage to enhance recognition performance. Rather than relying on\ndomain-specific textual data during the training phase, our model is trained to\nlearn how to utilize textual information provided in prompts for LLM decoder to\nimprove speech recognition performance. Benefiting from the advantages of the\nRAG retrieval mechanism, our approach efficiently accesses locally available\ndomain-specific documents, ensuring a convenient and effective process for\nsolving domain mismatch problems. Experiments conducted on the CSJ database\ndemonstrate that the proposed method significantly improves speech recognition\naccuracy and achieves state-of-the-art results on the CSJ dataset, even without\nrelying on the full training data.",
      "generated_abstract": "ecific speech recognition (SSR) models are essential for\nincreasing the effectiveness of automatic speech recognition (ASR) systems\nacross diverse domains. However, the challenge of addressing domain-specific\nspeech recognition remains a significant limitation. In this paper, we propose\na novel retrieval-augmented speech recognition (RASR) approach for SSR. The\nprimary objective of the RASR approach is to enhance the generalization of\nretrieval-based models, which is critical for addressing the challenges of\ndomain-specific speech recognition. The RASR approach leverages a retrieval\nmodel to provide contextual information for speech recognition. The model\nconsiders retrieval-based features as input to the ASR model, enhancing the\ngeneralization of the ASR model. The RASR approach is evaluated using the\nDataset of Speech Recognition and Speech Recognition",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19626168224299065,
          "p": 0.3088235294117647,
          "f": 0.2399999952483266
        },
        "rouge-2": {
          "r": 0.028169014084507043,
          "p": 0.039603960396039604,
          "f": 0.032921805841928616
        },
        "rouge-l": {
          "r": 0.14953271028037382,
          "p": 0.23529411764705882,
          "f": 0.1828571381054695
        }
      }
    },
    {
      "paper_id": "math.GM.math/GM/2502.17466v1",
      "true_abstract": "For a hypergroup $(H,\\circ)$ we consider $\\gamma^{\\ast}$, as the smallest\nequivalence relation on $H$ such that the quotion\n$(H/\\gamma^{\\ast},\\tiny{\\otimes})$ is an abelian group. We study some more\nproperties of $\\gamma^{\\ast}$. Initially, it is investigated which\nsubhypergroup the congruence relation modulo is strongly regular on, and its\nquotient results in an abelian group? This is directly related to the\nfundamental relation $\\gamma^{\\ast}$, since such subhypergroups must contain\n$S_{\\gamma}$. Then, we examine the functor $\\gamma^{\\ast}$ from a categorical\nperspective and investigate properties such as continuity and cocontinuity\nconcerning it using the decomposition $\\gamma=\\delta\\tiny{\\ast}\\beta$. For this\npurpose, we define the reduced words on strongly regular hypergroups. This has\na direct application in studying how the functor $\\gamma^{\\ast}$ affects on the\nstalks of the sheaves of hypergroups.",
      "generated_abstract": "The fundamental functor is a central object in the theory of hypergroups. It\nis a morphism of categories, sending a hypergroup to its category of\nsubquotients. We give a combinatorial description of the fundamental functor\nfor a class of hypergroups introduced by Matsushita. This class includes all\nhypergroups of characteristic 2 and all Cohen-Macaulay hypergroups of degree\n$d$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17073170731707318,
          "p": 0.358974358974359,
          "f": 0.23140495430913197
        },
        "rouge-2": {
          "r": 0.03389830508474576,
          "p": 0.07547169811320754,
          "f": 0.04678362145343905
        },
        "rouge-l": {
          "r": 0.15853658536585366,
          "p": 0.3333333333333333,
          "f": 0.21487602868929728
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2411.05951v1",
      "true_abstract": "Multifractality is a concept that helps compactly grasping the most essential\nfeatures of the financial dynamics. In its fully developed form, this concept\napplies to essentially all mature financial markets and even to more liquid\ncryptocurrencies traded on the centralized exchanges. A new element that adds\ncomplexity to cryptocurrency markets is the possibility of decentralized\ntrading. Based on the extracted tick-by-tick transaction data from the\nUniversal Router contract of the Uniswap decentralized exchange, from June 6,\n2023, to June 30, 2024, the present study using Multifractal Detrended\nFluctuation Analysis (MFDFA) shows that even though liquidity on these new\nexchanges is still much lower compared to centralized exchanges convincing\ntraces of multifractality are already emerging on this new trading as well. The\nresulting multifractal spectra are however strongly left-side asymmetric which\nindicates that this multifractality comes primarily from large fluctuations and\nsmall ones are more of the uncorrelated noise type. What is particularly\ninteresting here is the fact that multifractality is more developed for time\nseries representing transaction volumes than rates of return. On the level of\nthese larger events a trace of multifractal cross-correlations between the two\ncharacteristics is also observed.",
      "generated_abstract": "This study aims to develop a multifractal complexity approach for analyzing\ncryptocurrency trading patterns using Bitcoin and Ethereum, using a sample of\n60,000 transactions over 12 years. Multifractal complexity is a powerful tool\nfor analyzing complex data. This study explores the potential of multifractal\ncomplexity in the context of cryptocurrency trading, using Bitcoin and Ethereum\nas case studies. The findings indicate that multifractal complexity can be\nuseful for analyzing complex data, and it may provide valuable insights for\napplications in cryptocurrency trading and other fields.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.140625,
          "p": 0.3157894736842105,
          "f": 0.19459459033104465
        },
        "rouge-2": {
          "r": 0.010810810810810811,
          "p": 0.02666666666666667,
          "f": 0.015384611279586891
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.2807017543859649,
          "f": 0.17297296870942305
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2502.08613v4",
      "true_abstract": "As operators acting on the undetermined final settlement of a derivative\nsecurity, expectation is linear but price is non-linear. When the market of\nunderlying securities is incomplete, non-linearity emerges from the bid-offer\naround the mid price that accounts for the residual risks of the optimal\nfunding and hedging strategy. At the extremes, non-linearity also arises from\nthe embedded options on capital that are exercised upon default. In this essay,\nthese convexities are quantified in an entropic risk metric that evaluates the\nstrategic risks, which is realised as a cost with the introduction of bilateral\nmargin. Price is then adjusted for market incompleteness and the risk of\ndefault caused by the exhaustion of capital.\n  In the complete market theory, price is derived from a martingale condition.\nIn the incomplete market theory presented here, price is instead derived from a\nlog-martingale condition: \\begin{equation}\np=-\\frac{1}{\\alpha}\\log\\mathbb{E}\\exp[-\\alpha P] \\notag \\end{equation} for the\nprice $p$ and payoff $P$ of a funded and hedged derivative security, where the\nprice measure $\\mathbb{E}$ has minimum entropy relative to economic\nexpectations, and the parameter $\\alpha$ matches the risk aversion of the\ninvestor. This price principle is easily applied to standard models for market\nevolution, with applications considered here including model risk analysis,\ndeep hedging and decentralised finance.",
      "generated_abstract": "aper, we consider a stochastic price process driven by a linear\ndifferential equation (LDE) with a continuous-time stochastic generator. We\nintroduce the relative entropy of expectation (RE) and the relative entropy of\nprice (REP), which are defined as the negative logarithmic of the relative\nentropy of expectation and price, respectively. Then, we prove that the\nREP-price process is a solution to a LDE with a continuous-time stochastic\ngenerator. Furthermore, we show that the RE-price process is a solution to a\nLDE with a discrete-time stochastic generator. Finally, we show that the RE-price\nprocess is a solution to a LDE with a discrete-time stochastic generator when\nthe generator is given by a stochastic integral. The results are applied to the\nprice of an asset in a foreign exchange market, where the price process is\ngenerated by the L",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1590909090909091,
          "p": 0.3387096774193548,
          "f": 0.21649484101179728
        },
        "rouge-2": {
          "r": 0.03125,
          "p": 0.06382978723404255,
          "f": 0.041958037545112695
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.2903225806451613,
          "f": 0.1855670059602509
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2411.02531v3",
      "true_abstract": "The techniques suggested in Fr\\\"uhwirth-Schnatter et al. (2024) concern\nsparsity and factor selection and have enormous potential beyond standard\nfactor analysis applications. We show how these techniques can be applied to\nLatent Space (LS) models for network data. These models suffer from well-known\nidentification issues of the latent factors due to likelihood invariance to\nfactor translation, reflection, and rotation (see Hoff et al., 2002). A set of\nobservables can be instrumental in identifying the latent factors via auxiliary\nequations (see Liu et al., 2021). These, in turn, share many analogies with the\nequations used in factor modeling, and we argue that the factor loading\nrestrictions may be beneficial for achieving identification.",
      "generated_abstract": "We respond to the authors' comments on our paper. We argue that their\ncomment on our paper is not entirely fair. In particular, we point out that\nour paper has been written as a response to a very different paper than the one\nthe authors cite.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07407407407407407,
          "p": 0.17647058823529413,
          "f": 0.10434782192211736
        },
        "rouge-2": {
          "r": 0.009433962264150943,
          "p": 0.023809523809523808,
          "f": 0.01351350944850378
        },
        "rouge-l": {
          "r": 0.07407407407407407,
          "p": 0.17647058823529413,
          "f": 0.10434782192211736
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.13228v1",
      "true_abstract": "We investigate whether and why people might reduce compensation for workers\nwho use AI tools. Across 10 studies (N = 3,346), participants consistently\nlowered compensation for workers who used AI tools. This \"AI Penalization\"\neffect was robust across (1) different types of work and worker statuses and\nworker statuses (e.g., full-time, part-time, or freelance), (2) different forms\nof compensation (e.g., required payments or optional bonuses) and their timing,\n(3) various methods of eliciting compensation (e.g., slider scale, multiple\nchoice, and numeric entry), and (4) conditions where workers' output quality\nwas held constant, subject to varying inferences, or controlled for. Moreover,\nthe effect emerged not only in hypothetical compensation scenarios (Studies\n1-5) but also with real gig workers and real monetary compensation (Study 6).\nPeople reduced compensation for workers using AI tools because they believed\nthese workers deserved less credit than those who did not use AI (Studies 3 and\n4). This effect weakened when it is less permissible to reduce worker\ncompensation, such as when employment contracts provide stricter constraints\n(Study 4). Our findings suggest that adoption of AI tools in the workplace may\nexacerbate inequality among workers, as those protected by structured contracts\nface less vulnerability to compensation reductions, while those without such\nprotections risk greater financial penalties for using AI.",
      "generated_abstract": "the moral hazard and moral hazard premium due to the use of\nartificial intelligence (AI) in the workplace. We model a firm as a principal\nwho hires workers, and the firm's workers as AI users. We show that workers\nunderestimate the AI's skill at predicting and identifying their\nself-employment risks. As a result, workers underestimate the AI's\npredictive power, which leads to a moral hazard in workers' compensation\npolicies. We show that when workers overestimate the AI's skill, they are\nmore likely to use the AI. In this case, workers' compensation policies\nreduce workers' compensation. We also show that workers overestimate the AI's\nskill, which leads to a moral hazard in workers' compensation policies. In\nthis case,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.3559322033898305,
          "f": 0.20388349105806397
        },
        "rouge-2": {
          "r": 0.01,
          "p": 0.022727272727272728,
          "f": 0.013888884645063023
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.3559322033898305,
          "f": 0.20388349105806397
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2405.10453v1",
      "true_abstract": "Team and player evaluation in professional sport is extremely important given\nthe financial implications of success/failure. It is especially critical to\nidentify and retain elite shooters in the National Basketball Association\n(NBA), one of the premier basketball leagues worldwide because the ultimate\ngoal of the game is to score more points than one's opponent. To this end we\npropose two novel basketball metrics: \"expected points\" for team-based\ncomparisons and \"expected points above average (EPAA)\" as a player-evaluation\ntool. Both metrics leverage posterior samples from Bayesian hierarchical\nmodeling framework to cluster teams and players based on their shooting\npropensities and abilities. We illustrate the concepts for the top 100 shot\ntakers over the last decade and offer our metric as an additional metric for\nevaluating players.",
      "generated_abstract": "This paper proposes a novel approach for analyzing the performance of NBA\nplayer using Bayesian hierarchical modeling. We consider the average points\nscored per game as a baseline, and construct a Bayesian hierarchical model that\nestimates the player's actual performance based on the observed data. We then\nempirically demonstrate the effectiveness of our proposed model in predicting\nthe next game's points. We demonstrate that our model not only accurately\npredicts the next game's points, but also accurately estimates the average\npoints scored per game, suggesting that our proposed model is a powerful tool\nfor analyzing NBA player performance. Additionally, we propose an\ninterpretation of our model, highlighting the potential of our proposed model\nfor interpreting player performance. Finally, we conclude by discussing future\ndirections for this work.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25510204081632654,
          "p": 0.32894736842105265,
          "f": 0.28735631691901187
        },
        "rouge-2": {
          "r": 0.04065040650406504,
          "p": 0.046296296296296294,
          "f": 0.043290038311126676
        },
        "rouge-l": {
          "r": 0.19387755102040816,
          "p": 0.25,
          "f": 0.21839079967763256
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.13850v1",
      "true_abstract": "We provide a formal framework accounting for a widespread idea in the theory\nof economic design: analytically established incompatibilities between given\naxioms should be qualified by the likelihood of their violation. We define the\ndegree to which rules satisfy an axiom, as well as several axioms, on the basis\nof a probability measure over the inputs of the rules. Armed with this notion\nof degree, we propose and characterize i) a criterion to evaluate and compare\nrules given a set of axioms, allowing the importance of each combination of\naxioms to differ, and ii) a criterion to measure the compatibility between\ngiven axioms, building on a analogy with cooperative game theory.",
      "generated_abstract": "r investigates how the probability of satisfying a set of axioms\ndepends on the structure of the axiomatisation. We use the Bayesian approach to\nthis problem, focusing on the non-binary case. We show that, for any axiomatisation\n$\\mathcal{A}$, the probability of satisfying the axioms of $\\mathcal{A}$ is a\nnon-negative linear function of the posterior probability of the axioms. This\nimplies that the probability of satisfying a set of axioms is a convex\nfunction of the posterior probability of the axioms. We also show that, for\nfixed probability of satisfaction, the posterior probability of the axioms can\nbe either strictly positive or strictly negative, and in this case, the\nprobability of satisfying the axioms is a convex function of the posterior\nprobability of the axioms. This implies that the probability of satisfying the\naxioms is either a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19718309859154928,
          "p": 0.2692307692307692,
          "f": 0.2276422715420716
        },
        "rouge-2": {
          "r": 0.04672897196261682,
          "p": 0.0641025641025641,
          "f": 0.0540540491769179
        },
        "rouge-l": {
          "r": 0.16901408450704225,
          "p": 0.23076923076923078,
          "f": 0.1951219463388196
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2409.18816v1",
      "true_abstract": "This paper presents a novel approach to evaluating blue-chip art as a viable\nasset class for portfolio diversification. We present the Arte-Blue Chip Index,\nan index that tracks 100 top-performing artists based on 81,891 public\ntransactions from 157 artists across 584 auction houses over the period 1990 to\n2024. By comparing blue-chip art price trends with stock market fluctuations,\nour index provides insights into the risk and return profile of blue-chip art\ninvestments. Our analysis demonstrates that a 20% allocation of blue-chip art\nin a diversified portfolio enhances risk-adjusted returns by around 20%, while\nmaintaining volatility levels similar to the S&P 500.",
      "generated_abstract": "This paper develops a modern portfolio diversification method using the\nArte-Blue Chip Index (ABCI), a widely used index tracking the Blue Chip\ncompanies in Spain. The ABCI is a broad index that has a low correlation with\nthe S&P 500 and other more traditional indexes, making it a valuable tool for\ndiversifying investment portfolios. By using the ABCI as a benchmark, this\nstudy aims to find the optimal weighting of the traditional indices and the\nmodern asset classes. This study includes a theoretical framework, a\ncomputational method, and a case study that uses a portfolio optimization\nframework to analyze the performance of the proposed method. The results show\nthat the proposed method achieves superior results in terms of diversification\nand returns, providing a better alternative to traditional diversification\nmethods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25301204819277107,
          "p": 0.25609756097560976,
          "f": 0.25454544954563824
        },
        "rouge-2": {
          "r": 0.08247422680412371,
          "p": 0.06451612903225806,
          "f": 0.07239818511987914
        },
        "rouge-l": {
          "r": 0.24096385542168675,
          "p": 0.24390243902439024,
          "f": 0.2424242374244262
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.02011v1",
      "true_abstract": "Regression models are essential for a wide range of real-world applications.\nHowever, in practice, target values are not always precisely known; instead,\nthey may be represented as intervals of acceptable values. This challenge has\nled to the development of Interval Regression models. In this study, we provide\na comprehensive review of existing Interval Regression models and introduce\nalternative models for comparative analysis. Experiments are conducted on both\nreal-world and synthetic datasets to offer a broad perspective on model\nperformance. The results demonstrate that no single model is universally\noptimal, highlighting the importance of selecting the most suitable model for\neach specific scenario.",
      "generated_abstract": "The concept of interval regression is a new approach for time series\nintervention in machine learning. It is a way of using the interval of the\ntime series to model the trend of the time series. In this paper, we\nintroduce a new interval regression model called the Gaussian interval model\n(GIM). The GIM is based on the Gaussian mixture model and the\ninterval-of-interest model. We then compare the GIM with the interval-of-\ninterest model. We provide a comparative study of the GIM with the interval-\nof-interest model on three real datasets: the M30, the M31, and the M32\ndatasets. The results of the comparison show that the GIM performs\nsignificantly better than the interval-of-interest model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2597402597402597,
          "p": 0.31746031746031744,
          "f": 0.2857142807642858
        },
        "rouge-2": {
          "r": 0.030612244897959183,
          "p": 0.030927835051546393,
          "f": 0.030769225769363074
        },
        "rouge-l": {
          "r": 0.23376623376623376,
          "p": 0.2857142857142857,
          "f": 0.2571428521928572
        }
      }
    },
    {
      "paper_id": "physics.acc-ph.physics/acc-ph/2503.09665v1",
      "true_abstract": "Optimizing accelerator control is a critical challenge in experimental\nparticle physics, requiring significant manual effort and resource expenditure.\nTraditional tuning methods are often time-consuming and reliant on expert\ninput, highlighting the need for more efficient approaches. This study aims to\ncreate a simulation-based framework integrated with Reinforcement Learning (RL)\nto address these challenges. Using \\texttt{Elegant} as the simulation backend,\nwe developed a Python wrapper that simplifies the interaction between RL\nalgorithms and accelerator simulations, enabling seamless input management,\nsimulation execution, and output analysis.\n  The proposed RL framework acts as a co-pilot for physicists, offering\nintelligent suggestions to enhance beamline performance, reduce tuning time,\nand improve operational efficiency. As a proof of concept, we demonstrate the\napplication of our RL approach to an accelerator control problem and highlight\nthe improvements in efficiency and performance achieved through our\nmethodology. We discuss how the integration of simulation tools with a\nPython-based RL framework provides a powerful resource for the accelerator\nphysics community, showcasing the potential of machine learning in optimizing\ncomplex physical systems.",
      "generated_abstract": "In the framework of accelerator control, the objective is to minimise the\nenergy of the beam. In this work, the problem is reduced to a reinforcement\nlearning (RL) task, where the optimal beam energy is the value that maximises\nthe expected return. The RL algorithm is implemented in an industrial\nsimulator and trained using gradient-based methods. The performance of the\nRL algorithm is validated using the Monte Carlo simulation of a beamline. The\nresults show that the RL algorithm is able to achieve a return of 0.811,\nsignificantly higher than the 0.735 achieved by the baseline algorithm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17073170731707318,
          "p": 0.35,
          "f": 0.22950819231389416
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13821138211382114,
          "p": 0.2833333333333333,
          "f": 0.1857923453193587
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2408.00942v2",
      "true_abstract": "Composition is a powerful principle for systems biology, focused on the\ninterfaces, interconnections, and orchestration of distributed processes to\nenable integrative multiscale simulations. Whereas traditional models focus on\nthe structure or dynamics of specific subsystems in controlled conditions,\ncompositional systems biology aims to connect these models, asking critical\nquestions about the space between models: What variables should a submodel\nexpose through its interface? How do coupled models connect and translate\nacross scales? How do domain-specific models connect across biological and\nphysical disciplines to drive the synthesis of new knowledge? This approach\nrequires robust software to integrate diverse datasets and submodels, providing\nresearchers with tools to flexibly recombine, iteratively refine, and\ncollaboratively expand their models. This article offers a comprehensive\nframework to support this vision, including: a conceptual and graphical\nframework to define interfaces and composition patterns; standardized schemas\nthat facilitate modular data and model assembly; biological templates that\nintegrate detailed submodels that connect molecular processes to the emergence\nof the cellular interface; and user-friendly software interfaces that empower\nresearch communities to construct and improve multiscale models of cellular\nsystems. By addressing these needs, compositional systems biology will foster a\nunified and scalable approach to understanding complex cellular systems.",
      "generated_abstract": "e a novel framework for understanding the emergence of complex\nsystems from their constituent parts. This framework draws on the concept of\ncompositional structures, which are the building blocks of complex systems. We\ndemonstrate how the emergence of these structures is driven by evolutionary\npressures, and we show how these pressures interact with the emergence of\ncompositional structures to generate the diversity of complex systems observed\nin nature. The framework offers a unified view of how evolutionary pressures\nshape the emergence of compositional structures, and how these structures can be\nused to understand the emergence of complex systems. We apply our framework to\nthe study of the evolution of the human immune system, showing how the emergence\nof immune system compositional structures is driven by the selective pressure\nto prevent infection, and we demonstrate how this selective pressure interacts\nwith the emergence of compositional structures to generate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16417910447761194,
          "p": 0.30985915492957744,
          "f": 0.21463414181368243
        },
        "rouge-2": {
          "r": 0.042328042328042326,
          "p": 0.07407407407407407,
          "f": 0.0538720492439551
        },
        "rouge-l": {
          "r": 0.1417910447761194,
          "p": 0.2676056338028169,
          "f": 0.18536584913075563
        }
      }
    },
    {
      "paper_id": "math.NA.cs/NA/2503.10196v1",
      "true_abstract": "In this paper, we present an error estimate for the filtered Lie splitting\nscheme applied to the Zakharov system, characterized by solutions exhibiting\nvery low regularity across all dimensions. Our findings are derived from the\napplication of multilinear estimates established within the framework of\ndiscrete Bourgain spaces. Specifically, we demonstrate that when the solution\n$(E,z,z_t) \\in H^{s+r+1/2}\\times H^{s+r}\\times H^{s+r-1}$, the error in\n$H^{r+1/2}\\times H^{r}\\times H^{r-1}$ is $\\mathcal{O}(\\tau^{s/2})$ for\n$s\\in(0,2]$, where $r=\\max(0,\\frac d2-1)$. To the best of our knowledge, this\nrepresents the first explicit error estimate for the splitting method based on\nthe original Zakharov system, as well as the first instance where low\nregularity error estimates for coupled equations have been considered within\nthe Bourgain framework. Furthermore, numerical experiments confirm the validity\nof our theoretical results.",
      "generated_abstract": "We introduce a novel method for the solution of the Zakharov system with a\nlow regularity initial condition. The method relies on a finite element\ndiscretization of the system, and is based on a filtering approach, which\neliminates the need to solve the original problem. The method is based on a\nsplitting of the system into two parts: the first part contains the original\nproblem, while the second part contains a low regularity part. The low\nregularity part is discretized using the finite element method, and the\noriginal problem is solved using a filtering approach. The method is shown to\nbe effective and robust under various conditions, including the absence of\nregularity in the solution. The method is illustrated on both simple and\ncomplicated examples, and is applied to the Zakharov system. The results\nsuggest that the method can be used to solve complex problems in physics,\nchemistry and engineering.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2247191011235955,
          "p": 0.2631578947368421,
          "f": 0.2424242374552802
        },
        "rouge-2": {
          "r": 0.07692307692307693,
          "p": 0.07317073170731707,
          "f": 0.07499999500312533
        },
        "rouge-l": {
          "r": 0.21348314606741572,
          "p": 0.25,
          "f": 0.23030302533406807
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.09435v1",
      "true_abstract": "In this paper, we consider the dynamic oscillation in the Cournot oligopoly\nmodel, which involves multiple firms producing homogeneous products. To explore\nthe oscillation under the updates of best response strategies, we focus on the\nlinear price functions. In this setting, we establish the existence of\noscillations. In particular, we show that for the scenario of different costs\namong firms, the best response converges to either a unique equilibrium or a\ntwo-period oscillation. We further characterize the oscillations and propose\nlinear-time algorithms for finding all types of two-period oscillations. To the\nbest of our knowledge, our work is the first step toward fully analyzing the\nperiodic oscillation in the Cournot oligopoly model.",
      "generated_abstract": "We study the oscillations of the equilibrium price in a Cournot game with\nbest response strategies. We show that the oscillations of the equilibrium\nprice are determined by the existence of a critical point of the profit function\nand the existence of a critical point of the potential function. We characterize\nthe equilibrium price for the case when the potential function has a unique\ncritical point. We further characterize the equilibria of the game when the\npotential function is a concave function. Finally, we study the case when the\npotential function is a convex function.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.28378378378378377,
          "p": 0.525,
          "f": 0.3684210480763312
        },
        "rouge-2": {
          "r": 0.09615384615384616,
          "p": 0.15873015873015872,
          "f": 0.11976047434328967
        },
        "rouge-l": {
          "r": 0.24324324324324326,
          "p": 0.45,
          "f": 0.31578946912896283
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.08732v1",
      "true_abstract": "Background: Circadian desynchrony characterized by the misalignment between\nan individual's internal biological rhythms and external environmental cues,\nsignificantly affects various physiological processes and health outcomes.\nQuantifying circadian desynchrony often requires prolonged and frequent\nmonitoring, and currently, an easy tool for this purpose is missing.\nAdditionally, its association with the incidence of delirium has not been\nclearly explored. Methods: A prospective observational study was carried out in\nintensive care units (ICU) of a tertiary hospital. Circadian transcriptomics of\nblood monocytes from 86 individuals were collected on two consecutive days,\nalthough a second sample could not be obtained from all participants. Using two\npublic datasets comprised of healthy volunteers, we replicated a model for\ndetermining internal circadian time. We developed an approach to quantify\ncircadian desynchrony by comparing internal circadian time and external blood\ncollection time. We applied the model and quantified circadian desynchrony\nindex among ICU patients, and investigated its association with the incidence\nof delirium. Results: The replicated model for determining internal circadian\ntime achieved comparable high accuracy. The quantified circadian desynchrony\nindex was significantly higher among critically ill ICU patients compared to\nhealthy subjects, with values of 10.03 hours vs 2.50-2.95 hours (p < 0.001).\nMost ICU patients had a circadian desynchrony index greater than 9 hours.\nAdditionally, the index was lower in patients whose blood samples were drawn\nafter 3pm, with values of 5.00 hours compared to 10.01-10.90 hours in other\ngroups (p < 0.001)...",
      "generated_abstract": "dian system is a critical physiological mechanism for maintaining\nreliable and efficient functioning, yet its impact on patients in intensive\ncare units (ICUs) remains poorly understood. This study aimed to investigate\nthe circadian dysregulation in patients with delirium, a clinical syndrome\nassociated with increased risk of mortality. We used a cohort of ICU patients\nwith delirium and a control group without delirium. We evaluated circadian\nrhythm dysregulation using a novel method, the circadian phase deviation\nscoring system (CPDSS). We also performed a multivariate analysis to investigate\nthe relationship between circadian dysregulation and delirium. Our results\nshowed that delirium patients had a significant delay in their circadian\nrhythm, with a median of 65 minutes of dysregulation. This",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1503267973856209,
          "p": 0.2948717948717949,
          "f": 0.19913419466126958
        },
        "rouge-2": {
          "r": 0.03271028037383177,
          "p": 0.06363636363636363,
          "f": 0.04320987205837571
        },
        "rouge-l": {
          "r": 0.1437908496732026,
          "p": 0.28205128205128205,
          "f": 0.1904761860032609
        }
      }
    },
    {
      "paper_id": "hep-ex.hep-ex/2503.09392v1",
      "true_abstract": "The muon anomalous magnetic moment, $a_\\mu=\\frac{g-2}{2}$, is a low-energy\nobservable which can be both measured and computed to high precision, making it\na sensitive test of the Standard Model and a probe for new physics. This\nanomaly was measured with a precision of $0.20$~parts per million (ppm) by the\nFermilab's Muon g-2 (E989) experiment. The final goal of the E989 experiment is\nto reach a precision of $0.14$~ppm. The experiment is based on the measurement\nof the muon spin anomalous precession frequency, $\\omega_a$, based on the\narrival time distribution of high-energy decay positrons observed by 24\nelectromagnetic calorimeters, placed around the inner circumference of a $14$~m\ndiameter storage ring, and on the precise knowledge of the storage ring\nmagnetic field and of the beam time and space distribution. Achieving this\nlevel of precision requires strict control over systematics, which is ensured\nthrough several diagnostic devices. At the accelerator level, these devices\nmonitor the quality of the injected beam (e.g., verifying that it has the\ncorrect momentum), while at the detector level, they track both the magnetic\nfield and the gain of the calorimeters. In this work the devices and techniques\nused by the E989 experiment will be presented.",
      "generated_abstract": "g-2 experiment at Fermilab is designed to measure the muon's\ng-2 in the $g-2$ channel to an accuracy of a few parts in $10^{18}$. This\npaper describes the diagnostic system for this experiment. The system is\ndesigned to measure the muon's position and velocity in the laboratory frame,\nusing a combination of particle detectors and an ionization chamber.\n  The diagnostic system consists of a 10 m$^3$ cryostat that contains the\ndetectors, a 6 m$^3$ cryostat that contains the ionization chamber, and a\n4 m$^3$ cryostat that contains the particle detectors. The cryostats are\nmounted on a moving base, which allows them to be moved to different locations\nin the experiment. The base is equipped with a high-precision control",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16,
          "p": 0.29850746268656714,
          "f": 0.20833332878960512
        },
        "rouge-2": {
          "r": 0.02185792349726776,
          "p": 0.04081632653061224,
          "f": 0.028469746347184778
        },
        "rouge-l": {
          "r": 0.144,
          "p": 0.26865671641791045,
          "f": 0.18749999545627177
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SP/2503.08726v1",
      "true_abstract": "Traditional single-modality sensing faces limitations in accuracy and\ncapability, and its decoupled implementation with communication systems\nincreases latency in bandwidth-constrained environments. Additionally,\nsingle-task-oriented sensing systems fail to address users' diverse demands. To\novercome these challenges, we propose a semantic-driven integrated multimodal\nsensing and communication (SIMAC) framework. This framework leverages a joint\nsource-channel coding architecture to achieve simultaneous sensing decoding and\ntransmission of sensing results. Specifically, SIMAC first introduces a\nmultimodal semantic fusion (MSF) network, which employs two extractors to\nextract semantic information from radar signals and images, respectively. MSF\nthen applies cross-attention mechanisms to fuse these unimodal features and\ngenerate multimodal semantic representations. Secondly, we present a large\nlanguage model (LLM)-based semantic encoder (LSE), where relevant communication\nparameters and multimodal semantics are mapped into a unified latent space and\ninput to the LLM, enabling channel-adaptive semantic encoding. Thirdly, a\ntask-oriented sensing semantic decoder (SSD) is proposed, in which different\ndecoded heads are designed according to the specific needs of tasks.\nSimultaneously, a multi-task learning strategy is introduced to train the SIMAC\nframework, achieving diverse sensing services. Finally, experimental\nsimulations demonstrate that the proposed framework achieves diverse sensing\nservices and higher accuracy.",
      "generated_abstract": "aper, we propose a Semantic-Driven Integrated Multimodal Sensing\nand Communication (SIMAC) framework to address the challenges of sensing and\ncommunication in heterogeneous multi-agent environments. The framework integrates\nthe sensing and communication capabilities of each agent within a shared\ncommunication domain. We first introduce a semantic-driven sensing framework\nbased on the semantic embedding of semantic labels. This framework integrates\nthe sensing and communication capabilities of each agent within a shared\ncommunication domain. We then propose a novel resource allocation strategy for\nthe sensing and communication capabilities of each agent. Finally, we design a\ncooperative communication strategy to maximize the mutual information\ntransmitted between the sensing and communication capabilities of each agent.\nThe framework is evaluated through simulation and real-world experiments. The\nexperiments demonstrate that our framework can achieve high performance in\nterms",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1925925925925926,
          "p": 0.37681159420289856,
          "f": 0.2549019563076702
        },
        "rouge-2": {
          "r": 0.053763440860215055,
          "p": 0.10309278350515463,
          "f": 0.07067137358638544
        },
        "rouge-l": {
          "r": 0.1925925925925926,
          "p": 0.37681159420289856,
          "f": 0.2549019563076702
        }
      }
    },
    {
      "paper_id": "cs.GR.cs/GR/2503.08929v1",
      "true_abstract": "Accurate and efficient 3D mapping of large-scale outdoor environments from\nLiDAR measurements is a fundamental challenge in robotics, particularly towards\nensuring smooth and artifact-free surface reconstructions. Although the\nstate-of-the-art methods focus on memory-efficient neural representations for\nhigh-fidelity surface generation, they often fail to produce artifact-free\nmanifolds, with artifacts arising due to noisy and sparse inputs. To address\nthis issue, we frame surface mapping as a physics-informed energy optimization\nproblem, enforcing surface smoothness by optimizing an energy functional that\npenalizes sharp surface ridges. Specifically, we propose a deep learning based\napproach that learns the signed distance field (SDF) of the surface manifold\nfrom raw LiDAR point clouds using a physics-informed loss function that\noptimizes the $L_2$-Hessian energy of the surface. Our learning framework\nincludes a hierarchical octree based input feature encoding and a multi-scale\nneural network to iteratively refine the signed distance field at different\nscales of resolution. Lastly, we introduce a test-time refinement strategy to\ncorrect topological inconsistencies and edge distortions that can arise in the\ngenerated mesh. We propose a \\texttt{CUDA}-accelerated least-squares\noptimization that locally adjusts vertex positions to enforce\nfeature-preserving smoothing. We evaluate our approach on large-scale outdoor\ndatasets and demonstrate that our approach outperforms current state-of-the-art\nmethods in terms of improved accuracy and smoothness. Our code is available at\n\\href{https://github.com/HrishikeshVish/HessianForge/}{https://github.com/HrishikeshVish/HessianForge/}",
      "generated_abstract": "uce HessianForge, a novel method for LiDAR reconstruction that\noptimizes a physics-informed neural representation (PINR) with smoothness\nconstraints. Unlike existing methods that rely on optimization-based methods,\nour approach directly optimizes the PINR without additional optimization steps,\nenabling scalable reconstruction with a single forward pass. We demonstrate the\neffectiveness of our approach by reconstructing a 3D urban dataset from\nsingle-view LiDAR, achieving a reconstruction error of 0.065m, with an\niteration count of 100. We further show that our approach generalizes to\nmulti-view LiDAR, enabling the reconstruction of 3D buildings, roads, and\nsidewalks. Additionally, we introduce an end-to-end training method that\noptimizes the PINR without additional optimization steps, enabling efficient",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18620689655172415,
          "p": 0.3698630136986301,
          "f": 0.24770641756375733
        },
        "rouge-2": {
          "r": 0.029411764705882353,
          "p": 0.061224489795918366,
          "f": 0.039735094953730586
        },
        "rouge-l": {
          "r": 0.1724137931034483,
          "p": 0.3424657534246575,
          "f": 0.22935779371054635
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.08062v1",
      "true_abstract": "Orthogonal frequency division multiplexing (OFDM), which has been the\ndominating waveform for contemporary wireless communications, is also regarded\nas a competitive candidate for future integrated sensing and communication\n(ISAC) systems. Existing works on OFDM-ISAC usually assume that the maximum\nsensing range should be limited by the cyclic prefix (CP) length since\ninter-symbol interference (ISI) and inter-carrier interference (ICI) should be\navoided. However, in this paper, we provide rigorous analysis to reveal that\nthe random data embedded in OFDM-ISAC signal can actually act as a free ``mask\"\nfor ISI, which makes ISI/ICI random and hence greatly attenuated after radar\nsignal processing. The derived signal-to-interference-plus-noise ratio (SINR)\nin the range profile demonstrates that the maximum sensing range of OFDM-ISAC\ncan greatly exceed the ISI-free distance that is limited by the CP length,\nwhich is validated by simulation results. To further mitigate power degradation\nfor long-range targets, a novel sliding window sensing method is proposed,\nwhich iteratively detects and cancels short-range targets before shifting the\ndetection window. The shifted detection window can effectively compensate the\npower degradation due to insufficient CP length for long-range targets. Such\nresults provide valuable guidance for the CP length design in OFDM-ISAC\nsystems.",
      "generated_abstract": "r investigates the impact of channel length on the sensing range for\nOFDM-ISAC systems. In this scenario, a large number of OFDM symbols is transmitted\nover a narrowband carrier frequency band, while the transmitter is equipped with\na passive beamforming antenna. The channel is assumed to be a scalar discrete\ntime-frequency channel with a constant bandwidth and a discrete time-frequency\nsampling rate. The sensing range is defined as the distance from the transmitter\nto the first non-zero OFDM symbol, where a non-zero symbol is defined as one\nthat is not zero in the time-frequency domain. The sensing range is measured in\nthe time-frequency domain, and the channel length is measured in the\nfrequency-time domain. The sensing range of OFDM-ISAC systems with various\nchannel lengths is analyzed, and the impact of the channel",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.3088235294117647,
          "f": 0.21649484080773737
        },
        "rouge-2": {
          "r": 0.0335195530726257,
          "p": 0.05555555555555555,
          "f": 0.04181184199589704
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.3088235294117647,
          "f": 0.21649484080773737
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2501.13135v1",
      "true_abstract": "The complexity of human biology and its intricate systems holds immense\npotential for advancing human health, disease treatment, and scientific\ndiscovery. However, traditional manual methods for studying biological\ninteractions are often constrained by the sheer volume and complexity of\nbiological data. Artificial Intelligence (AI), with its proven ability to\nanalyze vast datasets, offers a transformative approach to addressing these\nchallenges. This paper explores the intersection of AI and microscopy in life\nsciences, emphasizing their potential applications and associated challenges.\nWe provide a detailed review of how various biological systems can benefit from\nAI, highlighting the types of data and labeling requirements unique to this\ndomain. Particular attention is given to microscopy data, exploring the\nspecific AI techniques required to process and interpret this information. By\naddressing challenges such as data heterogeneity and annotation scarcity, we\noutline potential solutions and emerging trends in the field. Written primarily\nfrom an AI perspective, this paper aims to serve as a valuable resource for\nresearchers working at the intersection of AI, microscopy, and biology. It\nsummarizes current advancements, key insights, and open problems, fostering an\nunderstanding that encourages interdisciplinary collaborations. By offering a\ncomprehensive yet concise synthesis of the field, this paper aspires to\ncatalyze innovation, promote cross-disciplinary engagement, and accelerate the\nadoption of AI in life science research.",
      "generated_abstract": "The integration of AI with microscopy technologies has the potential to\nchange the way we view and study cells. The applications of AI in life\nscience research span the full range of cellular processes, from\nprotein-protein interactions to gene regulation, disease diagnosis, and drug\ndevelopment. This review examines the current state of AI in microscopy and\nhighlights its potential to advance research in areas such as cellular\ndynamics, cell biology, and disease diagnosis. We discuss the challenges\nencountered in applying AI to microscopy-based research and provide insight into\npotential solutions. This review serves as a guide for researchers interested in\nexploring the potential of AI in microscopy-based research and highlights the\nneed for more research on the ethical and legal implications of AI applications\nin this domain.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23129251700680273,
          "p": 0.44155844155844154,
          "f": 0.3035714240597099
        },
        "rouge-2": {
          "r": 0.052884615384615384,
          "p": 0.09649122807017543,
          "f": 0.06832297679256231
        },
        "rouge-l": {
          "r": 0.19047619047619047,
          "p": 0.36363636363636365,
          "f": 0.2499999954882813
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.18182v1",
      "true_abstract": "Blind source separation (BSS) refers to the process of recovering multiple\nsource signals from observations recorded by an array of sensors. Common\napproaches to BSS, including independent vector analysis (IVA), and independent\nlow-rank matrix analysis (ILRMA), typically rely on second-order models to\ncapture the statistical independence of source signals for separation. However,\nthese methods generally do not account for the implicit structural information\nacross frequency bands, which may lead to model mismatches between the assumed\nsource distributions and the distributions of the separated source signals\nestimated from the observed mixtures. To tackle these limitations, this paper\nshows that conventional approaches such as IVA and ILRMA can easily be\nleveraged by the Sinkhorn divergence, incorporating an optimal transport (OT)\nframework to adaptively correct source variance estimates. This allows for the\nrecovery of the source distribution while modeling the inter-band signal\ndependence and reallocating source power across bands. As a result, enhanced\nversions of these algorithms are developed, integrating a Sinkhorn iterative\nscheme into their standard implementations. Extensive simulations demonstrate\nthat the proposed methods consistently enhance BSS performance.",
      "generated_abstract": "We address the blind source separation (BSS) problem with a sinkhorn divergence-based\noptimization algorithm. The sinkhorn divergence-based objective is designed to\nprovide the optimal allocation of the source power among the separated sources.\nThe optimization problem is formulated as a constrained nonlinear programming\nproblem, and a novel nonlinear dual program is derived to provide the optimal\nsolution. The dual program is then solved using the interior-point method, and\nthe solution is used to obtain the optimal allocation of the source power.\nExperimental results on synthetic data demonstrate the effectiveness of the\nalgorithm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11023622047244094,
          "p": 0.2641509433962264,
          "f": 0.1555555514006174
        },
        "rouge-2": {
          "r": 0.04093567251461988,
          "p": 0.08974358974358974,
          "f": 0.05622489529588265
        },
        "rouge-l": {
          "r": 0.10236220472440945,
          "p": 0.24528301886792453,
          "f": 0.1444444402895063
        }
      }
    },
    {
      "paper_id": "physics.ins-det.physics/ins-det/2503.10098v1",
      "true_abstract": "The linear response of CsI(Tl) crystals to $\\gamma$-rays plays a crucial role\nin their calibration, as any deviation from linearity can introduce systematic\nerrors not negligible in the measurement of $\\gamma$ energy spectra,\nparticularly at high energies. In this study, the responses of CsI(Tl) crystals\nto high-energy photons up to 20 MeV are investigated using quasi monochromatic\n$\\gamma$ beam provided by the Shanghai Laser Electron Gamma Source. The spectra\nare folded using a detector filter implemented by Geant4. Both quadratic and\nlinear fits to six energy points are used to assess the linearity of the\nCsI(Tl) detector. The results demonstrate that the difference between the\nlinear and non-linear fits is at the level of 4\\%. Applying these findings to\nthe $\\gamma$ hodoscope of the Compact Spectrometer for Heavy Ion Experiment\n(CSHINE), the potential systematic uncertainties caused by CsI(Tl)\nnon-linearity are evaluated. This work provides a comprehensive calibration\nmethodology for employing CsI(Tl) crystal to detect high energy $\\gamma$-rays.",
      "generated_abstract": "r response of CsI(Tl) crystal to photons at energies below 20 MeV\nis studied by means of the photoelectron spectroscopy. The photon energy\ndistributions are measured at a low laser power of 1.5 mW, where the\nphotoelectron yield is dominated by low energy photons. The photoelectron\nspectra are fitted with the formulae proposed by D. R. Hutchinson and S. E.\nFirsov to obtain the photoelectron yield and the photoelectron energy\ndistributions. The photoelectron yields are in agreement with the theoretical\ncalculations. The photoelectron energy distributions are shown to be\nindependent of the incident laser energy. The energy dependence of the photoelectron\nyield is measured as a function of the laser energy and is in good agreement\nwith the theory of D. R. Hutchinson",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.3387096774193548,
          "f": 0.2514970013195167
        },
        "rouge-2": {
          "r": 0.046052631578947366,
          "p": 0.06930693069306931,
          "f": 0.055335963582621606
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.3387096774193548,
          "f": 0.2514970013195167
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2411.10726v2",
      "true_abstract": "We study an optimal execution problem in the infinite horizon setup. Our\nfinancial market is given by the Black-Scholes model with a linear price\nimpact. The main novelty of the current note is that we study the constrained\ncase where the number of shares and the selling rate are non-negative\nprocesses. For this case we give a complete characterization of the value and\nthe optimal control via a solution of a non-linear ordinary differential\nequation (ODE). Furthermore, we provide an example where the non-linear ODE can\nbe solved explicitly. Our approach is purely probabilistic.",
      "generated_abstract": "We investigate the optimal execution problem with monotone strategies. We\ndevelop a general framework for the study of optimal execution problems in\nfinite-state markets and introduce the notion of optimal execution for\nmonotone strategies. We characterize the optimal execution for monotone\nstrategies using a bilevel optimization approach. We also develop a\nrepresentation of the optimal execution for monotone strategies in terms of\na weighted sum of the optimal solution to a series of sequential execution\nproblems. We provide an explicit solution to the optimal execution problem in\nthe case where the market is in a continuous-time setting. We also provide a\nclosed-form solution to the optimal execution problem for a special class of\nmonotone strategies with bounded deviation. We demonstrate the effectiveness of\nour approach through numerical examples.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27941176470588236,
          "p": 0.3333333333333333,
          "f": 0.30399999503872005
        },
        "rouge-2": {
          "r": 0.12222222222222222,
          "p": 0.1134020618556701,
          "f": 0.11764705383053582
        },
        "rouge-l": {
          "r": 0.2647058823529412,
          "p": 0.3157894736842105,
          "f": 0.28799999503872004
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.16298v1",
      "true_abstract": "Speech foundation models have demonstrated exceptional capabilities in\nspeech-related tasks. Nevertheless, these models often struggle with non-verbal\naudio data, such as vocalizations, baby crying, etc., which are critical for\nvarious real-world applications. Audio foundation models well handle non-speech\ndata but also fail to capture the nuanced features of non-verbal human sounds.\nIn this work, we aim to overcome the above shortcoming and propose a novel\nfoundation model, termed voc2vec, specifically designed for non-verbal human\ndata leveraging exclusively open-source non-verbal audio datasets. We employ a\ncollection of 10 datasets covering around 125 hours of non-verbal audio.\nExperimental results prove that voc2vec is effective in non-verbal vocalization\nclassification, and it outperforms conventional speech and audio foundation\nmodels. Moreover, voc2vec consistently outperforms strong baselines, namely\nOpenSmile and emotion2vec, on six different benchmark datasets. To the best of\nthe authors' knowledge, voc2vec is the first universal representation model for\nvocalization tasks.",
      "generated_abstract": "ork, we introduce voc2vec, a foundation model that leverages\nvocalization data to enhance language understanding. Vocalization data is\ncharacterized by its rich expressive capabilities and its ability to encode\nnon-verbal information. We propose a novel voc2vec model that learns from\nvocalization data to enhance text understanding. This model is built on the\npre-trained vocoder voc2vec. It learns to perform a vocoder-like transformation\nfrom a source text to a target text, while preserving the non-verbal aspects of\nthe vocalization. We evaluate our voc2vec model on a variety of vocalization\ndatasets, including the Vocoder dataset, the MIT-Vocoder dataset, the\nFairseq-voc2vec dataset, and the Diva dataset. Our results show that voc2vec\nachieves state-of-the-art performance on these",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2018348623853211,
          "p": 0.3142857142857143,
          "f": 0.24581005110327397
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.03773584905660377,
          "f": 0.0325203202987647
        },
        "rouge-l": {
          "r": 0.1926605504587156,
          "p": 0.3,
          "f": 0.23463686674573214
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.10116v1",
      "true_abstract": "The knowledge transfer from 3D printing technology paved the way for\nunlocking the innovative potential of 3D Food Printing (3DFP) technology.\nHowever, this technology-oriented approach neglects userderived issues that\ncould be addressed with advancements in 3DFP technology. To explore potential\nnew features and application areas for 3DFP technology, we created the Mobile\nFood Printer (MFP) prototype. We collected insights from novice chefs for MFP\nin the restaurant context through four online focus group sessions (N=12). Our\nresults revealed how MFP can be applied in the current kitchen routines\n(preparation, serving, and eating) and introduce novel dining experiences. We\ndiscuss our learnings under two themes: 1) dealing with the kitchen rush and 2)\nstreamlining workflows in the kitchen. The opportunities we present in this\nstudy act as a starting point for HCI and HFI researchers and encourage them to\nimplement mobility in 3DFP with a useroriented lens. We further provide a\nground for future research to uncover potentials for advancing 3DFP technology.",
      "generated_abstract": "This study explores the potential for food printing to enhance the\nconsumer experience in professional kitchens. By integrating food printing\ntechnologies with existing cooking techniques, we aim to enhance the efficiency\nand flexibility of the kitchen environment. We conducted a mixed-methods\nstudy with 20 novice chefs to examine the perceived benefits, potential use\ncases, and challenges of food printing in professional kitchens. Our findings\nshow that food printing offers potential benefits in terms of time and\ninventory management, flexibility, and safety. However, there are also challenges\nrelated to cost, printer reliability, and food quality. This study provides\ninsights into the potential of food printing in professional kitchens and\nhighlights the importance of understanding the needs and preferences of novice\nchefs to enhance the adoption of this technology.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1896551724137931,
          "p": 0.29333333333333333,
          "f": 0.23036648737699086
        },
        "rouge-2": {
          "r": 0.01935483870967742,
          "p": 0.027777777777777776,
          "f": 0.022813683372610142
        },
        "rouge-l": {
          "r": 0.1810344827586207,
          "p": 0.28,
          "f": 0.2198952831885092
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/TH/2503.04876v1",
      "true_abstract": "Given two populations from which independent binary observations are taken\nwith parameters $p_1$ and $p_2$ respectively, estimators are proposed for the\nrelative risk $p_1/p_2$, the odds ratio $p_1(1-p_2)/(p_2(1-p_1))$ and their\nlogarithms. The estimators guarantee that the relative mean-square error, or\nthe mean-square error for the logarithmic versions, is less than a target value\nfor any $p_1, p_2 \\in (0,1)$, and the ratio of average sample sizes from the\ntwo populations is close to a prescribed value. The estimators can also be used\nwith group sampling, whereby samples are taken in batches of fixed size from\nthe two populations. The efficiency of the estimators with respect to the\nCram\\'er-Rao bound is good, and in particular it is close to $1$ for small\nvalues of the target error.",
      "generated_abstract": "We provide a simple and elegant way to estimate the relative risk (RR),\nopportunity ratio (OR), and logarithm of the relative risk (LRR) using a single\nestimator. This is achieved by leveraging the fact that the RR, OR, and LRR are\nconsecutive combinations of the logarithm of the probability of an event and\nthe probability of the event, and we show that the estimator is consistent and\nhas finite sample size ratio. Theoretical results guarantee that the ratio of\nthe RR, OR, and LRR is asymptotically normally distributed. The estimator is\nalso highly efficient, with an asymptotic efficiency of 1 and a finite sample\nsize ratio of 1. The estimator is simple and can be computed in closed form,\nmaking it suitable for applications where a large number of samples is not\navailable.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3116883116883117,
          "p": 0.3116883116883117,
          "f": 0.31168830668831177
        },
        "rouge-2": {
          "r": 0.07894736842105263,
          "p": 0.08333333333333333,
          "f": 0.08108107608473368
        },
        "rouge-l": {
          "r": 0.2857142857142857,
          "p": 0.2857142857142857,
          "f": 0.2857142807142858
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.15376v1",
      "true_abstract": "Land use is a critical factor in the siting of renewable energy facilities\nand is often scrutinized due to perceived conflicts with other land demands.\nMeanwhile, substantial areas are devoted to activities such as golf, which are\naccessible to only a select few and have a significant land and environmental\nfootprint. Our study shows that in countries such as the United States and the\nUnited Kingdom, far more land is allocated to golf courses than to renewable\nenergy facilities. Areas equivalent to those currently used for golf could\nsupport the installation of up to 842 GW of solar and 659 GW of wind capacity\nin the top ten countries with the most golf courses. In many of these\ncountries, this potential exceeds both current installed capacity and\nmedium-term projections. These findings underscore the untapped potential of\nrethinking land use priorities to accelerate the transition to renewable\nenergy.",
      "generated_abstract": "scarce resource and is often scarce in rural areas, where the\ngrowth of the tourism industry is often linked to the availability of land.\nTourism-related activities such as golf courses and sports fields require a\nsignificant amount of land, but the use of land for golf courses is not\nalways linked to the growth of tourism. This paper explores how the presence of\ngolf courses varies across countries and how this varies over time. To do this,\nI apply an unbalanced panel data regression to explore the relationship between\nthe presence of golf courses and the growth of tourism in a sample of 114\ncountries. I find that countries that have more golf courses have more\ntourism, but that this relationship is not always linear. In addition, the\ngrowth of golf courses is negatively correlated with the growth of the wind\nindustry and the solar",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2736842105263158,
          "p": 0.3466666666666667,
          "f": 0.3058823480103807
        },
        "rouge-2": {
          "r": 0.06569343065693431,
          "p": 0.0782608695652174,
          "f": 0.07142856646667961
        },
        "rouge-l": {
          "r": 0.23157894736842105,
          "p": 0.29333333333333333,
          "f": 0.25882352448096896
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08361v1",
      "true_abstract": "This technical report analyzes non-contrast CT image segmentation in computer\nvision. It revisits a proposed method, examines the background of non-contrast\nCT imaging, and highlights the significance of segmentation. The study reviews\nrepresentative methods, including convolutional-based and CNN-Transformer\nhybrid approaches, discussing their contributions, advantages, and limitations.\nThe nnUNet stands out as the state-of-the-art method across various\nsegmentation tasks. The report explores the relationship between the proposed\nmethod and existing approaches, emphasizing the role of global context modeling\nin semantic labeling and mask generation. Future directions include addressing\nthe long-tail problem, utilizing pre-trained models for medical imaging, and\nexploring self-supervised or contrastive pre-training techniques. This report\noffers insights into non-contrast CT image segmentation and potential\nadvancements in the field.",
      "generated_abstract": "l image segmentation is a crucial task in medical imaging, playing a\nimportant role in disease diagnosis, prognosis, and treatment planning. However,\ntraditional 3D segmentation methods, such as pixel-based and voxel-based\nmodels, are limited by their reliance on contrast, which is often unavailable\nin clinical scenarios. In this paper, we propose a novel non-contrast CT\nsegmentation model, CT-3D Seg, that utilizes CT scans to segment 3D structures\nin real-world medical images. CT-3D Seg is based on the Transformer architecture\nand employs a novel non-contrast CT module that extracts structural features\nfrom CT scans without the need for contrast. Our approach outperforms state-of-the-art\nmethods in both synthetic and real-world datasets, demonstrating its ability to\naccurately",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19047619047619047,
          "p": 0.18604651162790697,
          "f": 0.18823528911833923
        },
        "rouge-2": {
          "r": 0.026785714285714284,
          "p": 0.027522935779816515,
          "f": 0.0271493162678906
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.16279069767441862,
          "f": 0.16470587735363335
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.07692v1",
      "true_abstract": "This paper narrowly replicates Chen and Kung's 2019 paper ($The$ $Quarterly$\n$Journal$ $of$ $Economics$ 134(1): 185-226). Inspecting the data reveals that\nnearly one-third of the transactions (388,903 out of 1,208,621) are perfect\nduplicates of other rows, excluding the transaction number. Replicating the\nanalysis on the data sans-duplicates yields a slightly smaller but still\nstatistically significant princeling effect, robust across the regression\nresults. Further analysis also reveals that coefficients interpreted as the\neffect of logarithm of area actually reflect the effect of scaled values of\narea; this paper also reinterprets and contextualizes these results in light of\nthe true scaled values.",
      "generated_abstract": "r investigates the validity of the \"princeling\" phenomenon, which\naccuses Chinese property developers of artificially inflating prices through\nmassive land acquisitions. To empirically assess the extent of the phenomenon,\nwe use a unique dataset on the acquisition of land parcels in Guangdong Province\nfrom 2011 to 2016. We find that the \"princeling\" phenomenon is not as\npervasive as commonly believed. There is no significant correlation between\nthe number of acquisitions and transaction discounts. Furthermore, we find\nthat the transaction discounts are significantly affected by transaction\ncosts and the volume of land acquisitions. We also find that the transaction\ndiscounts decrease significantly after 2016. Our findings suggest that the\nprinceling phenomenon may not be as widespread as previously believed, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1917808219178082,
          "p": 0.18421052631578946,
          "f": 0.18791945808927538
        },
        "rouge-2": {
          "r": 0.03225806451612903,
          "p": 0.02830188679245283,
          "f": 0.03015074879018289
        },
        "rouge-l": {
          "r": 0.1506849315068493,
          "p": 0.14473684210526316,
          "f": 0.1476510017134365
        }
      }
    },
    {
      "paper_id": "gr-qc.physics/ins-det/2503.10332v1",
      "true_abstract": "The double-pass interferometer scheme was proposed in Ref.\\,[Light Sci. Appl.\n{\\bf 7}, 11 (2018)] as the method of implementation of the quantum speed meter\nconcept in future laser gravitational-wave (GW) detectors. Later it was shown\nin Ref.\\,[Phys. Rev. D {\\bf 110}, 062006 (2024)] that it allows to implement\nthe new type of the optical spring that does not require detuning of the\ninterferometer. Here we show that both these regimes can coexist, combining the\nspeed meter type broadband sensitivity gain with the additional lows-frequency\nminimum in the quantum noise originated from the optical spring. We show that\nthe location of this minimum can be varied without affecting the core optics of\nthe interferometer, allowing to tune the quantum noise shape in real time to\nfollow the ``chirp'' GW signals.",
      "generated_abstract": "Quantum speed meter (QSM) experiments measure the speed of light by\nquantum interference. The sensitivity of the QSM is limited by the uncertainty\nin the position of the optical spring. In this work, we demonstrate that\nincreasing the uncertainty in the position of the optical spring by using an\noptical spring with higher spring constant results in an increased sensitivity\nof the QSM. Using the optical spring, we increase the uncertainty in the\nposition of the optical spring by a factor of 100. We use this optical spring\nto increase the sensitivity of the QSM by a factor of 1000.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.34782608695652173,
          "f": 0.23880596564045448
        },
        "rouge-2": {
          "r": 0.05042016806722689,
          "p": 0.08823529411764706,
          "f": 0.06417111836655356
        },
        "rouge-l": {
          "r": 0.17045454545454544,
          "p": 0.32608695652173914,
          "f": 0.22388059250612619
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.06611v1",
      "true_abstract": "Performance and reliability analyses of autonomous vehicles (AVs) can benefit\nfrom tools that ``amplify'' small datasets to synthesize larger volumes of\nplausible samples of the AV's behavior. We consider a specific instance of this\ndata synthesis problem that addresses minimizing the AV's exposure to adverse\nenvironmental conditions during travel to a fixed goal location. The\nenvironment is characterized by a threat field, which is a strictly positive\nscalar field with higher intensities corresponding to hazardous and unfavorable\nconditions for the AV. We address the problem of synthesizing datasets of\nminimum exposure paths that resemble a training dataset of such paths. The main\ncontribution of this paper is an inverse reinforcement learning (IRL) model to\nsolve this problem. We consider time-invariant (static) as well as time-varying\n(dynamic) threat fields. We find that the proposed IRL model provides excellent\nperformance in synthesizing paths from initial conditions not seen in the\ntraining dataset, when the threat field is the same as that used for training.\nFurthermore, we evaluate model performance on unseen threat fields and find low\nerror in that case as well. Finally, we demonstrate the model's ability to\nsynthesize distinct datasets when trained on different datasets with distinct\ncharacteristics.",
      "generated_abstract": "aper, we propose an inverse reinforcement learning (IRL) method\nfor minimum-exposure paths (MEPs) in scalar fields, which can be considered as\nnon-stationary stochastic dynamical systems. MEPs are the minimum-time paths\nthrough a set of control points in a stochastic scalar field. The IRL method\nensures that the minimum-exposure paths are achieved with minimal exposure\nbetween control points, and therefore, the method is suitable for\nhigh-dimensional scalar fields. In this paper, we propose a novel IRL method\nbased on a modified policy gradient method (MPG-PG) that incorporates the\nuncertainty of the scalar field. The MPG-PG method is also designed to\noptimize the MEP through the stochastic control problem, and the uncertainty\nof the scalar field is reflected through the policy gradient. The proposed\nIRL method is evaluated by numerical",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.39473684210526316,
          "f": 0.30612244423157026
        },
        "rouge-2": {
          "r": 0.04712041884816754,
          "p": 0.08035714285714286,
          "f": 0.059405935933950195
        },
        "rouge-l": {
          "r": 0.24166666666666667,
          "p": 0.3815789473684211,
          "f": 0.2959183625989172
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/SY/2503.09892v1",
      "true_abstract": "As inverter-based resources (IBRs) penetrate power systems, the dynamics\nbecome more complex, exhibiting multiple timescales, including electromagnetic\ntransient (EMT) dynamics of power electronic controllers and electromechanical\ndynamics of synchronous generators. Consequently, the power system model\nbecomes highly stiff, posing a challenge for efficient simulation using\nexisting methods that focus on dynamics within a single timescale. This paper\nproposes a Heterogeneous Multiscale Method for highly efficient multi-timescale\nsimulation of a power system represented by its EMT model. The new method\nalternates between the microscopic EMT model of the system and an automatically\nreduced macroscopic model, varying the step size accordingly to achieve\nsignificant acceleration while maintaining accuracy in both fast and slow\ndynamics of interests. It also incorporates a semi-analytical solution method\nto enable a more adaptive variable-step mechanism. The new simulation method is\nillustrated using a two-area system and is then tested on a detailed EMT model\nof the IEEE 39-bus system.",
      "generated_abstract": "The increasing use of inverter-based resources (IBRs) has necessitated the\ndevelopment of efficient simulation tools for power systems with IBRs. This\npaper proposes a multiscale approach that combines a local-in-time (LIT)\ndiscretization of the power system with a nonlocal-in-time (NLT) discretization\nof the IBRs. The LIT discretization is used to solve the power system\nequations and the NLT discretization is used to solve the IBR equations\nefficiently. To ensure the convergence of the solution, a novel error estimator\nis proposed based on the spectral norm of the gradient of the power system\nequations. Numerical experiments show that the proposed multiscale method\nperforms well for both small and large power systems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21568627450980393,
          "p": 0.3384615384615385,
          "f": 0.2634730491376529
        },
        "rouge-2": {
          "r": 0.06338028169014084,
          "p": 0.0967741935483871,
          "f": 0.07659573989823479
        },
        "rouge-l": {
          "r": 0.20588235294117646,
          "p": 0.3230769230769231,
          "f": 0.2514970012334613
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.20467v1",
      "true_abstract": "A one-shot device is a unit that operates only once, after which it is either\ndestroyed or needs to be rebuilt. For this type of device, the operational\nstatus can only be assessed at a specific inspection time, determining whether\nfailure occurred before or after it. Consequently, lifetimes are subject to\nleft- or right-censoring. One-shot devices are usually highly reliables. To\nanalyze the reliability of such products, an accelerated life test (ALT) plan\nis typically employed by subjecting the devices to increased levels of stress\nfactors, thus allowing life characteristics observed under high-stress\nconditions to be extrapolated to normal operating conditions. By accelerating\nthe degradation process, ALT significantly reduces both the time required for\ntesting and the associated experimental costs.\n  Recently, robust inferential methods have gained considerable interest in\nstatistical analysis. Among them, weighted minimum density power divergence\nestimators (WMDPDEs) are widely recognized for their robust statistical\nproperties with small loss of efficiency. In this work, robust WMDPDE and\nassociated statistical tests are developed under a log-logistic lifetime\ndistribution with multiple stresses. Explicit expressions for the estimating\nequations and asymptotic distribution of the estimators are obtained. Further,\na Monte Carlo simulation study is presented to evaluate the performance of the\nWMDPDE in practical applications.",
      "generated_abstract": "er the estimation of parameters in a statistical model for a\nnon-inferior product in which a device, initially placed at a fixed location,\nis used to obtain an accelerated life-test result. The device's performance is\ndescribed by a log-logistic distribution and is assumed to be independent of\nthe test and of the random location of the device. The location of the device\nis assumed to be unknown. We provide a robust estimator of the model parameters\nunder the log-logistic distribution and derive the asymptotic distribution of\nthe estimator. We also derive the asymptotic normality of the estimator and\nshow that the asymptotic distribution is a consistent estimator of the\nclassical exponential distribution, which is the asymptotic distribution for\nthe classical device. We further consider the case when the location of the\ndevice is unknown and provide a robust estimator of the model parameters under\nthe log-logistic distribution. We",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16778523489932887,
          "p": 0.4032258064516129,
          "f": 0.23696682049459813
        },
        "rouge-2": {
          "r": 0.04975124378109453,
          "p": 0.09174311926605505,
          "f": 0.06451612447263301
        },
        "rouge-l": {
          "r": 0.1342281879194631,
          "p": 0.3225806451612903,
          "f": 0.1895734555656882
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.gr-qc/2503.10346v1",
      "true_abstract": "The Hubble tension has emerged as a critical crisis in cosmology, with the\ncause remaining unclear. Determining the Hubble constant ($H_0$) independently\nof cosmological models and distance ladders will help resolve this crisis. In\nthis letter, we for the first time use 47 gravitational-wave (GW) standard\nsirens from the third Gravitational-Wave Transient Catalog to calibrate\ndistances in the strong lensing system, RXJ1131-1231, and constrain $H_0$\nthrough the distance-sum rule, with minimal cosmological assumptions. We assume\nthat light propagation over long distances is described by the\nFriedmann-Lemaitre-Robertson-Walker metric and that geometrical optics holds,\nbut we do not need to assume the universe's contents or the theory of gravity\non cosmological scales. Fixing $\\Omega_K=0$, we obtain\n$H_0=73.22^{+5.95}_{-5.43}$ ${\\rm km}~{\\rm s}^{-1}~{\\rm Mpc}^{-1}$ and\n$H_0=70.40^{+8.03}_{-5.60}$ ${\\rm km}~{\\rm s}^{-1}~{\\rm Mpc}^{-1}$ by using the\ndeflector galaxy's mass model and kinematic measurements to break mass-sheet\ntransform, respectively. When $\\Omega_K$ is not fixed, the central value of\n$H_0$ increases further. We find that our results are still dominated by\nstatistical errors, and at the same time, we notice the great potential of\nusing GW dark sirens to provide calibration, owing to their higher redshifts.\nWhen using 42 binary black holes and RXJ1131-1231, we obtain a $8.46 \\%$ $H_0$\nconstraint precision, which is better than that from the bright siren GW170817\nusing the Hubble law by about $40\\%$. In the future, as the redshift range of\nGW dark sirens increases, more and more SGLTDs can be included, and we can\nachieve high-precision, model-independent measurements of $H_0$ without the\nneed for GW bright sirens.",
      "generated_abstract": "t a model-independent analysis of the Hubble constant ($H_0$) from\nGWTC-3, a set of 14 gravitational-wave (GW) standard sirens and\nstrong-lensing time delays. Our analysis employs a simple model for $H_0$\nwithin the framework of the flat-space Friedmann-Lemaitre-Robertson-Walker\n(FLRW) metric, where $H_0$ is a constant. We use the results of\nGWTC-3 to set a constraint on the tensor-to-scalar ratio $r$ of the\ninflationary scalar-sector of the early universe, and to estimate the value\nof the dark energy equation of state ($w_{\\rm DE}$) to within $\\sim 10\\%$.\nWe also use the GW standard siren constraints to estimate $H_0$",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1676300578034682,
          "p": 0.4603174603174603,
          "f": 0.24576270795066074
        },
        "rouge-2": {
          "r": 0.028,
          "p": 0.07777777777777778,
          "f": 0.04117646669550211
        },
        "rouge-l": {
          "r": 0.1329479768786127,
          "p": 0.36507936507936506,
          "f": 0.19491525032354215
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/TR/2411.05951v1",
      "true_abstract": "Multifractality is a concept that helps compactly grasping the most essential\nfeatures of the financial dynamics. In its fully developed form, this concept\napplies to essentially all mature financial markets and even to more liquid\ncryptocurrencies traded on the centralized exchanges. A new element that adds\ncomplexity to cryptocurrency markets is the possibility of decentralized\ntrading. Based on the extracted tick-by-tick transaction data from the\nUniversal Router contract of the Uniswap decentralized exchange, from June 6,\n2023, to June 30, 2024, the present study using Multifractal Detrended\nFluctuation Analysis (MFDFA) shows that even though liquidity on these new\nexchanges is still much lower compared to centralized exchanges convincing\ntraces of multifractality are already emerging on this new trading as well. The\nresulting multifractal spectra are however strongly left-side asymmetric which\nindicates that this multifractality comes primarily from large fluctuations and\nsmall ones are more of the uncorrelated noise type. What is particularly\ninteresting here is the fact that multifractality is more developed for time\nseries representing transaction volumes than rates of return. On the level of\nthese larger events a trace of multifractal cross-correlations between the two\ncharacteristics is also observed.",
      "generated_abstract": "r introduces a novel approach to quantifying trading complexity in\ndecentralized cryptocurrency trading. Our approach combines multifractal\ndimension with multifractal entropy to provide a more robust framework for\nquantifying trading complexity. By extending the multifractal theory to the\nrealm of cryptocurrency trading, we provide a unified framework for\ncharacterizing the complexity of trading activities across a range of\ntransaction types. This approach offers a more comprehensive approach to\nquantifying trading complexity, enabling more accurate assessments of risk\nexposure and risk management strategies. Additionally, our approach provides\nvaluable insights into the complexities associated with cryptocurrency\ntrading, offering a more comprehensive understanding of the underlying\nbehavioral dynamics. This research provides a novel approach to quantifying\ntrading complexity in cryptocurrency trading, offering a more comprehensive\nunderstanding of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.109375,
          "p": 0.21875,
          "f": 0.14583332888888903
        },
        "rouge-2": {
          "r": 0.005405405405405406,
          "p": 0.01020408163265306,
          "f": 0.007067133281726983
        },
        "rouge-l": {
          "r": 0.0859375,
          "p": 0.171875,
          "f": 0.11458332888888906
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2412.11113v1",
      "true_abstract": "We consider an economic environment with one buyer and one seller. For a\nbundle $(t,q)\\in [0,\\infty[\\times [0,1]=\\mathbb{Z}$, $q$ refers to the winning\nprobability of an object, and $t$ denotes the payment that the buyer makes. We\nconsider continuous and monotone preferences on $\\mathbb{Z}$ as the primitives\nof the buyer. These preferences can incorporate both quasilinear and\nnon-quasilinear preferences, and multidimensional pay-off relevant parameters.\nWe define rich single-crossing subsets of this class and characterize\nstrategy-proof mechanisms by using monotonicity of the mechanisms and\ncontinuity of the indirect preference correspondences. We also provide a\ncomputationally tractable optimization program to compute the optimal mechanism\nfor mechanisms with finite range. We do not use revenue equivalence and virtual\nvaluations as tools in our proofs. Our proof techniques bring out the geometric\ninteraction between the single-crossing property and the positions of bundles\n$(t,q)$s in the space $\\mathbb{Z}$. We also provide an extension of our\nanalysis to an $n-$buyer environment, and to the situation where $q$ is a\nqualitative variable.",
      "generated_abstract": "aper, we consider the problem of designing an optimal mechanism for a\nsingle-crossing domain. We show that, under certain structural conditions, the\nproblem is polynomial-time solvable and characterize the optimal mechanism in\nterms of a randomized algorithm. We also provide a polynomial-time approximation\nscheme for the problem, which generalizes known results for the same\nproblem. The main contribution of the paper is the characterization of the\noptimal strategy-proof mechanism in terms of a randomized algorithm. The\nalgorithm is based on a simple notion of \"recentness\" of an edge, which is\ndefined based on the recentness of the two vertices on the edge, and a notion of\n\"recentness\" of an edge as a whole. The algorithm works for any single-crossing\ndomain and does not require the existence of a proper, closed, connected\nsubgraph of the graph. In addition, we",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18518518518518517,
          "p": 0.2777777777777778,
          "f": 0.22222221742222234
        },
        "rouge-2": {
          "r": 0.057692307692307696,
          "p": 0.0782608695652174,
          "f": 0.06642065932108804
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.25,
          "f": 0.1999999952000001
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2502.07868v1",
      "true_abstract": "This paper studies the ubiquitous problem of liquidating large quantities of\nhighly correlated stocks, a task frequently encountered by institutional\ninvestors and proprietary trading firms. Traditional methods in this setting\nsuffer from the curse of dimensionality, making them impractical for\nhigh-dimensional problems. In this work, we propose a novel method based on\nstochastic optimal control to optimally tackle this complex multidimensional\nproblem. The proposed method minimizes the overall execution shortfall of\nhighly correlated stocks using a reinforcement learning approach. We rigorously\nestablish the convergence of our optimal trading strategy and present an\nimplementation of our algorithm using intra-day market data.",
      "generated_abstract": "r proposes a novel approach for liquidating a basket of stocks\nwith a given time horizon by using a multi-agent reinforcement learning (MLR)\nalgorithm. The MLR agent learns the optimal decision-making process by\nexperimenting with a set of market conditions and making a decision based on\nthe best available information. The agent's policy is evaluated by measuring\nits success rate. The performance of the MLR agent is compared with that of a\nsingle-agent reinforcement learning (SARL) agent and a multi-agent reinforcement\nlearning (MARL) agent. The MLR agent's performance is evaluated using\nhistorical stock price data from 1990 to 2024, while the SARL agent's and MARL\nagent's performance is evaluated using historical stock price data from\n2010 to 2024. The results show that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2911392405063291,
          "p": 0.32857142857142857,
          "f": 0.3087248272330076
        },
        "rouge-2": {
          "r": 0.041666666666666664,
          "p": 0.038461538461538464,
          "f": 0.039999995008000624
        },
        "rouge-l": {
          "r": 0.26582278481012656,
          "p": 0.3,
          "f": 0.281879189649115
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.09908v1",
      "true_abstract": "We present a work optimal algorithm for parallel fully batch-dynamic maximal\nmatching against an oblivious adversary. In particular it processes batches of\nupdates (either insertions or deletions of edges) in constant expected\namortized work per edge update, and in $O(\\log^3 m)$ depth per batch whp, where\n$m$ is the maximum number of edges in the graph over time. This greatly\nimproves on the recent result by Ghaffari and Trygub (2024) that requires\n$O(\\log^9 m)$ amortized work per update and $O(\\log^4 m )$ depth per batch,\nboth whp. The algorithm can also be used for hyperedge maximal matching. For\nhypergraphs with rank $r$ (maximum cardinality of any edge) the algorithm\nsupports batches of insertions and deletions with $O(r^3)$ expected amortized\nwork per edge update, and $O(\\log^3 m)$ depth per batch whp. This is a factor\nof $O(r)$ work off of the best sequential algorithm, Assadi and Solomon (2021),\nwhich uses $O(r^2)$ work per update. Ghaffari and Trygub's parallel\nbatch-dynamic algorithm on hypergraphs requires $O(r^8 \\log^9 m)$ amortized\nwork per edge update whp. We leverage ideas from the prior algorithms but\nintroduce substantial new ideas. Furthermore, our algorithm is relatively\nsimple, perhaps even simpler than the sequential hyperedge algorithm. We also\npresent the first work-efficient algorithm for maximal matching on hypergraphs.\nFor a hypergraph with total cardinality $m'$ (i.e., sum over the cardinality of\neach edge), the algorithm runs in $O(m')$ work in expectation and $O(\\log^2 m)$\ndepth whp. The algorithm also has some properties that allow us to use it as a\nsubroutine in the dynamic algorithm to select random edges in the graph to add\nto the matching.",
      "generated_abstract": "aper, we propose a parallel batch-dynamic maximal matching\nunder a dynamic constraint for the work of each worker. We first show that the\nmaximal matching with a fixed cost per edge can be computed in polynomial time\nwhen the work of each worker is bounded by $O(\\log n)$ and the cost of an edge\nis bounded by $O(1)$. Then, we propose a parallel batch-dynamic maximal matching\nwith constant work per update. We prove that the work of each worker is\nbounded by $O(1)$ and the cost of an edge is bounded by $O(1)$. The\nparallel batch-dynamic maximal matching with constant work per update can be\ncomputed in $O(n\\log^2 n)$ work and $O(n)$ cost in polynomial time. We also\nprovide a $O(n)$ time and $O(n\\log^2 n)$ work upper bound for the parallel\nbatch-dynamic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19594594594594594,
          "p": 0.5576923076923077,
          "f": 0.289999996152
        },
        "rouge-2": {
          "r": 0.03404255319148936,
          "p": 0.0975609756097561,
          "f": 0.050473182284628465
        },
        "rouge-l": {
          "r": 0.17567567567567569,
          "p": 0.5,
          "f": 0.25999999615200003
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/class-ph/2503.03342v1",
      "true_abstract": "For linear electromagnetic systems possessing time-reversal symmetry, we\npresent an approach to bound ratios of internal fields excited from different\nports, using only the scattering matrix (S matrix), improving upon previous\nrelated bounds by Sounas and Al\\`u (2017). By reciprocity, emitted-wave\namplitudes from internal dipole sources are bounded in a similar way. When\napplied to coupled-resonant systems, our method constrains ratios of resonant\ncoupling/decay coefficients. We also obtain a relation for the relative phase\nof fields excited from the two ports and the ratio of field intensities in a\ntwo-port system. In addition, although lossy systems do not have time-reversal\nsymmetry, we can still approximately bound loss-induced non-unitarity of the S\nmatrix using only the lossless S matrix. We show numerical validations of the\nnear-tightness of our bounds in various scattering systems.",
      "generated_abstract": "We present an experimental test of the time-reversal symmetry (TRS)\nin the electromagnetic field. We measure the electric field strength in the\nfield of a laser pulse and compare it to the field of a free-space pulse of\nsimilar energy. The experiment demonstrates that the two fields are indistinguishable\nto the degree of accuracy required by the TRS. We present an alternative test\nof the TRS by comparing the time-reversed version of the field with a\ncounter-propagating field.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1827956989247312,
          "p": 0.37777777777777777,
          "f": 0.24637680719911786
        },
        "rouge-2": {
          "r": 0.024793388429752067,
          "p": 0.046153846153846156,
          "f": 0.032258059969361264
        },
        "rouge-l": {
          "r": 0.16129032258064516,
          "p": 0.3333333333333333,
          "f": 0.2173912999527411
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/CO/2501.18901v1",
      "true_abstract": "We introduce sliced optimal transport dataset distance (s-OTDD), a\nmodel-agnostic, embedding-agnostic approach for dataset comparison that\nrequires no training, is robust to variations in the number of classes, and can\nhandle disjoint label sets. The core innovation is Moment Transform Projection\n(MTP), which maps a label, represented as a distribution over features, to a\nreal number. Using MTP, we derive a data point projection that transforms\ndatasets into one-dimensional distributions. The s-OTDD is defined as the\nexpected Wasserstein distance between the projected distributions, with respect\nto random projection parameters. Leveraging the closed form solution of\none-dimensional optimal transport, s-OTDD achieves (near-)linear computational\ncomplexity in the number of data points and feature dimensions and is\nindependent of the number of classes. With its geometrically meaningful\nprojection, s-OTDD strongly correlates with the optimal transport dataset\ndistance while being more efficient than existing dataset discrepancy measures.\nMoreover, it correlates well with the performance gap in transfer learning and\nclassification accuracy in data augmentation.",
      "generated_abstract": "uce a novel geometric dataset, Lightspeed, designed to measure the\ndistance between images. Lightspeed consists of pairs of images from the\ndataset, where the distance between these images is measured using the Sliced\nOptimal Transport (SOT) distance. In this paper, we propose a novel method to\nconstruct Lightspeed, which we call Lightspeed-GD. We demonstrate that Lightspeed\ncan be used to measure the distance between images, and that the SOT distance\ncan be used to measure this distance. We compare the performance of Lightspeed\nwith existing datasets, and demonstrate that Lightspeed is a stronger\ncomparator than existing datasets. We also demonstrate that Lightspeed-GD\noutperforms the SOT distance in terms of the distance between the images. We\nalso propose the Lightspeed-GD algorithm, which we use to construct Lightspe",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17117117117117117,
          "p": 0.3064516129032258,
          "f": 0.21965317459186753
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.04807692307692308,
          "f": 0.03937007390414841
        },
        "rouge-l": {
          "r": 0.17117117117117117,
          "p": 0.3064516129032258,
          "f": 0.21965317459186753
        }
      }
    },
    {
      "paper_id": "cond-mat.supr-con.cond-mat/str-el/2503.10085v1",
      "true_abstract": "We consider multiband BCS superconductors that exhibit time-reversal symmetry\nand uniform pairing, and analyze their dynamic density and spin structure\nfactors using linear-response theory within the mean-field BCS-BEC crossover\nframework at zero temperature. Our results for the multi-orbital Hubbard model\nsatisfy the associated f-sum rules in several limits. In particular, in the\nstrong-coupling limit, they coincide with those of a weakly-interacting Bose\ngas of Cooper pairs, where the low-energy collective Goldstone modes serve as\nBogoliubov phonons. We further reveal that the quantum-geometric origin of the\nlow-energy structure factors, along with related observables such as the\nsuperfluid-weight tensor and the effective-mass tensor of Cooper pairs, can be\ntraced all the way back to the effective-mass theorem for Bloch bands in this\nlimit. As an illustration, we investigate the pyrochlore-Hubbard model\nnumerically and demonstrate that the Goldstone modes are the only relevant\ncollective degrees of freedom in the flat-band regime.",
      "generated_abstract": "imental observation of the quantum geometry of superconducting\nquantum wires (SCWs) is the key to understanding the nature of superconductivity\nin these systems. This geometry is characterized by the quantum Hall effect,\nwhich is a consequence of the non-trivial topological phase of the superconducting\norder parameter. In multiband systems, the quantum geometry of the SCWs may\ndepend on the band structure of the superconducting order parameter. In this\npaper, we study the quantum geometry of the SCWs in multiband BCS superconductors\nusing the structure factor approach. We find that the quantum geometry of the\nSCWs in multiband BCS superconductors can be understood in terms of the band\nstructure of the superconducting order parameter. The quantum geometry of the\nSCWs is also related to the gap closure of the superconducting order",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16822429906542055,
          "p": 0.29508196721311475,
          "f": 0.21428570966057264
        },
        "rouge-2": {
          "r": 0.04285714285714286,
          "p": 0.06818181818181818,
          "f": 0.05263157420744887
        },
        "rouge-l": {
          "r": 0.1588785046728972,
          "p": 0.2786885245901639,
          "f": 0.20238094775581073
        }
      }
    },
    {
      "paper_id": "eess.SP.cs/IT/2503.10472v1",
      "true_abstract": "In this letter, we propose to deploy rotatable antennas (RAs) at the base\nstation (BS) to enhance both communication and sensing (C&S) performances, by\nexploiting a new spatial degree-of-freedom (DoF) offered by array rotation.\nSpecifically, we formulate a multi-objective optimization problem to\nsimultaneously maximize the sum-rate of multiple communication users and\nminimize the Cram\\'er-Rao bound (CRB) for target angle estimation, by jointly\noptimizing the transmit beamforming vectors and the array rotation angle at the\nBS. To solve this problem, we first equivalently decompose it into two\nsubproblems, corresponding to an inner problem for beamforming optimization and\nan outer problem for array rotation optimization. Although these two\nsubproblems are non-convex, we obtain their high-quality solutions by applying\nthe block coordinate descent (BCD) technique and one-dimensional exhaustive\nsearch, respectively. Moreover, we show that for the communication-only case,\nRAs provide an additional rotation gain to improve communication performance;\nwhile for the sensing-only case, the equivalent spatial aperture can be\nenlarged by RAs for achieving higher sensing accuracy. Finally, numerical\nresults are presented to showcase the performance gains of RAs over\nfixed-rotation antennas in integrated sensing and communications (ISAC).",
      "generated_abstract": "r presents a novel antenna design for the integrated sensing and\ncommunication (ISAC) system, which can support both the sensing and\ncommunication tasks. The proposed ISAC antenna is a multi-beam antenna, which\nis designed to be rotatable around the central axis to support both sensing and\ncommunication. It is designed with a large feed area, and a single-port\npolarization with a large beam width, which enables the ISAC system to perform\nboth sensing and communication tasks. The proposed antenna design can support\nboth the frequency-hopping (FH) and time-division multiple access (TDMA)\nschemes, which can be easily implemented in a commercial multi-antenna system.\nThe performance of the proposed antenna design is evaluated by comparing it\nwith the conventional design, which is an orthogonally polarized antenna. The\nresults show that the proposed anten",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18548387096774194,
          "p": 0.323943661971831,
          "f": 0.2358974312667982
        },
        "rouge-2": {
          "r": 0.027932960893854747,
          "p": 0.04672897196261682,
          "f": 0.0349650302819215
        },
        "rouge-l": {
          "r": 0.1693548387096774,
          "p": 0.29577464788732394,
          "f": 0.21538461075397775
        }
      }
    },
    {
      "paper_id": "math.ST.stat/CO/2502.17738v1",
      "true_abstract": "Motivated by learning dynamical structures from static snapshot data, this\npaper presents a distribution-on-scalar regression approach for estimating the\ndensity evolution of a stochastic process from its noisy temporal point clouds.\nWe propose an entropy-regularized nonparametric maximum likelihood estimator\n(E-NPMLE), which leverages the entropic optimal transport as a smoothing\nregularizer for the density flow. We show that the E-NPMLE has almost\ndimension-free statistical rates of convergence to the ground truth\ndistributions, which exhibit a striking phase transition phenomenon in terms of\nthe number of snapshots and per-snapshot sample size. To efficiently compute\nthe E-NPMLE, we design a novel particle-based and grid-free coordinate KL\ndivergence gradient descent (CKLGD) algorithm and prove its polynomial\niteration complexity. Moreover, we provide numerical evidence on synthetic data\nto support our theoretical findings. This work contributes to the theoretical\nunderstanding and practical computation of estimating density evolution from\nnoisy observations in arbitrary dimensions.",
      "generated_abstract": "er the problem of learning the distribution of a random variable\nfrom a single snapshot, which is a single snapshot of its trajectory. This\nproblem arises in a variety of domains, including machine learning, computer\nscience, and statistics. We develop a general framework for learning\ndistributions from a single snapshot data, in which the distribution of the\nsnapshot is given by a random density function. Our approach is based on the\nwell-known law of total probability, which is used to show that the distribution\nof the snapshot can be learned from the data using a single, non-informative\nsample from the density. This enables us to learn the density by training a\nsingle neural network with the data. We apply our method to the problem of\nlearning the density of a Gaussian random variable from a single snapshot. We\nalso present a method for learning the density of a Gaussian random variable\nfrom a set of snapshots. Our approach is computation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23008849557522124,
          "p": 0.34210526315789475,
          "f": 0.27513227032389914
        },
        "rouge-2": {
          "r": 0.055944055944055944,
          "p": 0.06666666666666667,
          "f": 0.060836496939380764
        },
        "rouge-l": {
          "r": 0.19469026548672566,
          "p": 0.2894736842105263,
          "f": 0.2328042279958569
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/CO/2503.10496v1",
      "true_abstract": "Modeling natural phenomena with artificial neural networks (ANNs) often\nprovides highly accurate predictions. However, ANNs often suffer from\nover-parameterization, complicating interpretation and raising uncertainty\nissues. Bayesian neural networks (BNNs) address the latter by representing\nweights as probability distributions, allowing for predictive uncertainty\nevaluation. Latent binary Bayesian neural networks (LBBNNs) further handle\nstructural uncertainty and sparsify models by removing redundant weights. This\narticle advances LBBNNs by enabling covariates to skip to any succeeding layer\nor be excluded, simplifying networks and clarifying input impacts on\npredictions. Ultimately, a linear model or even a constant can be found to be\noptimal for a specific problem at hand. Furthermore, the input-skip LBBNN\napproach reduces network density significantly compared to standard LBBNNs,\nachieving over 99% reduction for small networks and over 99.9% for larger ones,\nwhile still maintaining high predictive accuracy and uncertainty measurement.\nFor example, on MNIST, we reached 97% accuracy and great calibration with just\n935 weights, reaching state-of-the-art for compression of neural networks.\nFurthermore, the proposed method accurately identifies the true covariates and\nadjusts for system non-linearity. The main contribution is the introduction of\nactive paths, enhancing directly designed global and local explanations within\nthe LBBNN framework, that have theoretical guarantees and do not require post\nhoc external tools for explanations.",
      "generated_abstract": "r proposes a novel method for interpreting and evaluating deep\nlearning models, namely Input-Skip Latent Binary Bayesian Neural Networks (ISLBNNs).\nISLBNNs are a Bayesian deep learning model, which utilizes the generative\napproach to produce a prior distribution over the latent variables. The model\nis trained with the loss function that maximizes the likelihood of the\ntraining data. ISLBNNs are designed to incorporate a prior distribution over\nthe latent variables, which can be interpreted as a prior distribution over\nthe input variables. The model produces an output distribution over the\ninterpretable variables, which can be interpreted as a posterior distribution.\nThe output distribution and the posterior distribution are used to compute the\ninterpretable variables. The model can be used to generate explanations for\ncertain outputs, such as the outputs of the model trained on the training data\nand the outputs",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14743589743589744,
          "p": 0.32857142857142857,
          "f": 0.20353981873286878
        },
        "rouge-2": {
          "r": 0.0049504950495049506,
          "p": 0.009523809523809525,
          "f": 0.006514653479615625
        },
        "rouge-l": {
          "r": 0.1346153846153846,
          "p": 0.3,
          "f": 0.18584070368862096
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.19906v1",
      "true_abstract": "Single-channel speech enhancement is a challenging ill-posed problem focused\non estimating clean speech from degraded signals. Existing studies have\ndemonstrated the competitive performance of combining convolutional neural\nnetworks (CNNs) with Transformers in speech enhancement tasks. However,\nexisting frameworks have not sufficiently addressed computational efficiency\nand have overlooked the natural multi-scale distribution of the spectrum.\nAdditionally, the potential of CNNs in speech enhancement has yet to be fully\nrealized. To address these issues, this study proposes a Deep Separable Dilated\nDense Block (DSDDB) and a Group Prime Kernel Feedforward Channel Attention\n(GPFCA) module. Specifically, the DSDDB introduces higher parameter and\ncomputational efficiency to the Encoder/Decoder of existing frameworks. The\nGPFCA module replaces the position of the Conformer, extracting deep temporal\nand frequency features of the spectrum with linear complexity. The GPFCA\nleverages the proposed Group Prime Kernel Feedforward Network (GPFN) to\nintegrate multi-granularity long-range, medium-range, and short-range receptive\nfields, while utilizing the properties of prime numbers to avoid periodic\noverlap effects. Experimental results demonstrate that PrimeK-Net, proposed in\nthis study, achieves state-of-the-art (SOTA) performance on the\nVoiceBank+Demand dataset, reaching a PESQ score of 3.61 with only 1.41M\nparameters.",
      "generated_abstract": "This paper presents a novel group-prime kernel convolutional neural network\n(PrimeK-Net) for single channel speech enhancement. The proposed framework\ncombines a multi-scale spectral learning module and a group-prime kernel\nconvolutional neural network to learn and extract the multi-scale features of\nthe speech signal. The multi-scale features are then employed for the subsequent\nenhancement tasks, including speech enhancement, channel separation, and\ntime-frequency compression. The performance of the proposed framework is\nevaluated by means of both simulation and real-world speech data. The results\ndemonstrate that the proposed framework outperforms the state-of-the-art\napproaches in terms of both speech enhancement performance and computational\nefficiency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17164179104477612,
          "p": 0.3770491803278688,
          "f": 0.23589743159815918
        },
        "rouge-2": {
          "r": 0.0625,
          "p": 0.125,
          "f": 0.08333332888888913
        },
        "rouge-l": {
          "r": 0.16417910447761194,
          "p": 0.36065573770491804,
          "f": 0.22564102134174893
        }
      }
    },
    {
      "paper_id": "quant-ph.math-ph/2503.10123v1",
      "true_abstract": "Based on the generalized Bloch representation, we study the separability and\ngenuine multipartite entanglement of arbitrary dimensional multipartite quantum\nstates. Some sufficient and some necessary criteria are presented. For certain\nstates, these criteria together are both sufficient and necessary. Detailed\nexamples show that our criteria are better than some existing ones in\nidentifying entanglement. Based on these criteria, the largest separable ball\naround the maximally mixed state for arbitrary multi-qubit systems is found,\nand it is proved that its radius is the constant 1. Furthermore, the criteria\nin this paper can be implemented experimentally.",
      "generated_abstract": "In this paper we prove that entanglement monotones such as the concurrence,\nthe partial transpose, and the Schmidt rank are sufficient and necessary\ncriteria for multipartite entanglement. We give an explicit characterization of\nthe necessary criteria and show that all other entanglement monotones are\nsufficient only for local quantum operations and classical communication. We\nalso show that the partial transpose and the Schmidt rank are sufficient and\nnecessary criteria for multipartite pure entanglement. We show that the\npartial transpose is not a sufficient criterion for multipartite pure\nentanglement, and give an explicit construction of a pair of entanglement\nmonotones for pure entanglement that are not sufficient. We give a simple\ncharacterization of the entanglement monotones that are sufficient and\nnecessary for pure entanglement.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.3469387755102041,
          "f": 0.2905982857301484
        },
        "rouge-2": {
          "r": 0.06666666666666667,
          "p": 0.06976744186046512,
          "f": 0.0681818131844012
        },
        "rouge-l": {
          "r": 0.23529411764705882,
          "p": 0.32653061224489793,
          "f": 0.27350426863613125
        }
      }
    },
    {
      "paper_id": "physics.space-ph.physics/space-ph/2503.08878v1",
      "true_abstract": "Ion measurements made with the Hot Plasma Composition Analyzers of the\nMagnetospheric Multiscale Mission (MMS-HPCAs) during the Mother's Day Storm\n(Gannon Storm) of 10-13 May 2024 yield the first observations of atomic and\nmolecular nitrogen ions in the Earth's dayside outer magnetosphere. A\npopulation of ions identified as doubly charged nitrogen and oxygen was also\nmeasured. These observations were made within a highly compressed magnetosphere\nat a geocentric distance of ~6 Earth Radii during the early recovery phase of\nthe storm. From the ion composition measurements and accompanying magnetic\nfield data, we determine the reconnection rate at the magnetopause; we compare\nthis result to a model reconnection rate that assumes the presence of only\natomic oxygen and hydrogen. The heavy ion-laden-mass density in the\nmagnetosphere was greater than the shocked solar wind mass density in the\nmagnetosheath. Despite these conditions, magnetic reconnection still occurred\nat the magnetopause.",
      "generated_abstract": "'s magnetopause is the boundary between the inner and outer\n(magnetosheath) plasmas, where the electric field is perpendicular to the\nmagnetic field. It is a dynamic and unstable region, subject to strong\nmagnetic forces. The presence of atomic and molecular nitrogen ions, which\naccount for a large fraction of the plasma, has been observed at the\nmagnetopause during the 2024 Mother's Day storm. The nitrogen ions were\nfound to form a plasma sheet at the magnetopause, and to exhibit a complex\nstructure, which is different from the plasma sheet observed at other\npositions on the Earth's magnetosphere. This study presents the first\nexperimental observations of nitrogen ions at the dayside magnetopause.\nNitrogen ions were observed to be confined to a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26732673267326734,
          "p": 0.36486486486486486,
          "f": 0.30857142369044904
        },
        "rouge-2": {
          "r": 0.11510791366906475,
          "p": 0.1415929203539823,
          "f": 0.12698412203735215
        },
        "rouge-l": {
          "r": 0.2079207920792079,
          "p": 0.28378378378378377,
          "f": 0.2399999951190205
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.02942v1",
      "true_abstract": "Semantic information refers to the meaning conveyed through words, phrases,\nand contextual relationships within a given linguistic structure. Humans can\nleverage semantic information, such as familiar linguistic patterns and\ncontextual cues, to reconstruct incomplete or masked speech signals in noisy\nenvironments. However, existing speech enhancement (SE) approaches often\noverlook the rich semantic information embedded in speech, which is crucial for\nimproving intelligibility, speaker consistency, and overall quality of enhanced\nspeech signals. To enrich the SE model with semantic information, we employ\nlanguage models as an efficient semantic learner and propose a comprehensive\nframework tailored for language model-based speech enhancement, called\n\\textit{GenSE}. Specifically, we approach SE as a conditional language modeling\ntask rather than a continuous signal regression problem defined in existing\nworks. This is achieved by tokenizing speech signals into semantic tokens using\na pre-trained self-supervised model and into acoustic tokens using a\ncustom-designed single-quantizer neural codec model. To improve the stability\nof language model predictions, we propose a hierarchical modeling method that\ndecouples the generation of clean semantic tokens and clean acoustic tokens\ninto two distinct stages. Moreover, we introduce a token chain prompting\nmechanism during the acoustic token generation stage to ensure timbre\nconsistency throughout the speech enhancement process. Experimental results on\nbenchmark datasets demonstrate that our proposed approach outperforms\nstate-of-the-art SE systems in terms of speech quality and generalization\ncapability.",
      "generated_abstract": "hancement has emerged as a key task in speech processing. While\ngenerative models have proven effective for speech enhancement, the\ngenerative nature of the models often limits the ability to model complex\nacoustic conditions. To address this, we propose GenSE, a generative speech\nenhancement model that uses a hierarchical architecture to model complex\nacoustic conditions and generate speech with enhanced quality. We introduce\ntwo key components that improve modeling ability: (1) an autoregressive\ngenerative language model (GenLM) that generates a multi-modal representation\nof the acoustic conditions, and (2) a multi-stage speech enhancement model\nthat generates high-quality speech by sequentially optimizing the condition\nmodeling. Our experiments demonstrate that GenSE achieves state-of-the-art\nperformance on the speech enhancement benchmarks (SEBench) while maintaining\nhigh quality",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23448275862068965,
          "p": 0.41975308641975306,
          "f": 0.30088495115318353
        },
        "rouge-2": {
          "r": 0.04245283018867924,
          "p": 0.08181818181818182,
          "f": 0.05590061661972954
        },
        "rouge-l": {
          "r": 0.18620689655172415,
          "p": 0.3333333333333333,
          "f": 0.23893804849831632
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.08826v1",
      "true_abstract": "Integrating BD-RIS into wireless communications systems has attracted\nsignificant interest due to its transformative potential in enhancing system\nperformance. This survey provides a comprehensive analysis of BD-RIS\ntechnology, examining its modeling, structural characteristics, and network\nintegration while highlighting its advantages over traditional diagonal RIS.\nSpecifically, we review various BD-RIS modeling approaches, including multiport\nnetwork theory, graph theory, and matrix theory, and emphasize their\napplication in diverse wireless scenarios. The survey also covers BD-RIS's\nstructural diversity, including different scattering matrix types, transmission\nmodes, intercell architectures, and circuit topologies, showing their\nflexibility in improving network performance. We delve into the potential\napplications of BD-RIS, such as enhancing wireless coverage, improving PLS,\nenabling multi-cell interference cancellation, improving precise sensing and\nlocalization, and optimizing channel manipulation. Further, we explore BD-RIS\narchitectural development, providing insights into new configurations focusing\non channel estimation, optimization, performance analysis, and circuit\ncomplexity perspectives. Additionally, we investigate the integration of BD-RIS\nwith emerging wireless technologies, such as millimeter-wave and terahertz\ncommunications, integrated sensing and communications, mobile edge computing,\nand other cutting-edge technologies. These integrations are pivotal in\nadvancing the capabilities and efficiency of future wireless networks. Finally,\nthe survey identifies key challenges, including channel state information\nestimation, interference modeling, and phase-shift designs, and outlines future\nresearch directions. The survey aims to provide valuable insights into BD-RIS's\npotential in shaping the future of wireless communications systems.",
      "generated_abstract": "r provides a comprehensive review of research progress in the area of\ndiagonal RIS-aided wireless communications systems, covering both theory and\npractical applications. The theoretical study includes the analysis of the\nrelationship between the phase shift matrix and the RIS parameters, the\noptimization of the phase shift matrix, and the impact of the RIS parameters on\nthe transmission rate and the channel estimation error probability. The\npractical application covers the scenario where the RIS is installed at the base\nstation (BS), which can achieve both spatial multiplexing and beamforming\ncapabilities. The paper examines the performance of the system in different\nscenarios, including a one-user system with a single BS, multiple-user systems\nwith multiple BSs, and a massive MIMO system with multiple BSs. The paper\nexplains the differences between diagonal RISs and non-diag",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19727891156462585,
          "p": 0.34523809523809523,
          "f": 0.25108224645415195
        },
        "rouge-2": {
          "r": 0.018691588785046728,
          "p": 0.03361344537815126,
          "f": 0.02402401943096337
        },
        "rouge-l": {
          "r": 0.17687074829931973,
          "p": 0.30952380952380953,
          "f": 0.22510822048012605
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/TH/2502.10161v1",
      "true_abstract": "Reasoning about fairness through correlation-based notions is rife with\npitfalls. The 1973 University of California, Berkeley graduate school\nadmissions case from Bickel et. al. (1975) is a classic example of one such\npitfall, namely Simpson's paradox. The discrepancy in admission rates among\nmales and female applicants, in the aggregate data over all departments,\nvanishes when admission rates per department are examined. We reason about the\nBerkeley graduate school admissions case through a causal lens. In the process,\nwe introduce a statistical test for causal hypothesis testing based on Pearl's\ninstrumental-variable inequalities (Pearl 1995). We compare different causal\nnotions of fairness that are based on graphical, counterfactual and\ninterventional queries on the causal model, and develop statistical tests for\nthese notions that use only observational data. We study the logical relations\nbetween notions, and show that while notions may not be equivalent, their\ncorresponding statistical tests coincide for the case at hand. We believe that\na thorough case-based causal analysis helps develop a more principled\nunderstanding of both causal hypothesis testing and fairness.",
      "generated_abstract": "t the Berkeley Admissions data, a dataset containing a wealth of\ndata about students who applied to Berkeley College, the University of\nCalifornia, Berkeley. In our original paper, we used data from 2000 to 2003 to\nestimate a model for students' chances of acceptance, and used it to test\ncausal hypotheses. Our results suggested that students' performance in\nmathematics, and especially their performance in calculus, was a significant\npredictor of their chances of acceptance. In the current paper, we revisit our\nresults and the original data, and also analyze data from 2004 to 2014. We\nfind that the model is no longer supported by the data. Instead, we find that\nthe data supports the model of student performance as a significant predictor\nof chances of acceptance, with the performance in math being",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18421052631578946,
          "p": 0.2876712328767123,
          "f": 0.22459892572163928
        },
        "rouge-2": {
          "r": 0.030864197530864196,
          "p": 0.042735042735042736,
          "f": 0.03584228903688354
        },
        "rouge-l": {
          "r": 0.17543859649122806,
          "p": 0.273972602739726,
          "f": 0.21390373855586386
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.17726v1",
      "true_abstract": "The Musical Instrument Digital Interface (MIDI), introduced in 1983,\nrevolutionized music production by allowing computers and instruments to\ncommunicate efficiently. MIDI files encode musical instructions compactly,\nfacilitating convenient music sharing. They benefit Music Information Retrieval\n(MIR), aiding in research on music understanding, computational musicology, and\ngenerative music. The GigaMIDI dataset contains over 1.4 million unique MIDI\nfiles, encompassing 1.8 billion MIDI note events and over 5.3 million MIDI\ntracks. GigaMIDI is currently the largest collection of symbolic music in MIDI\nformat available for research purposes under fair dealing. Distinguishing\nbetween non-expressive and expressive MIDI tracks is challenging, as MIDI files\ndo not inherently make this distinction. To address this issue, we introduce a\nset of innovative heuristics for detecting expressive music performance. These\ninclude the Distinctive Note Velocity Ratio (DNVR) heuristic, which analyzes\nMIDI note velocity; the Distinctive Note Onset Deviation Ratio (DNODR)\nheuristic, which examines deviations in note onset times; and the Note Onset\nMedian Metric Level (NOMML) heuristic, which evaluates onset positions relative\nto metric levels. Our evaluation demonstrates these heuristics effectively\ndifferentiate between non-expressive and expressive MIDI tracks. Furthermore,\nafter evaluation, we create the most substantial expressive MIDI dataset,\nemploying our heuristic, NOMML. This curated iteration of GigaMIDI encompasses\nexpressively-performed instrument tracks detected by NOMML, containing all\nGeneral MIDI instruments, constituting 31% of the GigaMIDI dataset, totalling\n1,655,649 tracks.",
      "generated_abstract": "r introduces the GigaMIDI Dataset with Features (GigaMIDI-F),\na music performance detection dataset that leverages large-scale MIDI\ndatasets and features to enhance the robustness of performance detection\nmodels. This dataset features 144 hours of MIDI data, recorded over 1000\nmusicians, and includes features that capture a wide range of musical\ncharacteristics, such as rhythm, pitch, and timbre. We evaluate the\nperformance of state-of-the-art performance detection models on GigaMIDI-F,\nincluding a newly designed baseline model that utilizes the MIDI-F features\nand a recently proposed model that incorporates features from the MIDI-F and\nMIDI-S datasets. Our results demonstrate that the MIDI-F features are a\nsignificant enhancement to traditional MIDI",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1032258064516129,
          "p": 0.23529411764705882,
          "f": 0.14349775360855851
        },
        "rouge-2": {
          "r": 0.009523809523809525,
          "p": 0.020202020202020204,
          "f": 0.012944979463978592
        },
        "rouge-l": {
          "r": 0.1032258064516129,
          "p": 0.23529411764705882,
          "f": 0.14349775360855851
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.00850v1",
      "true_abstract": "In this paper we develop the theory of the depth of a simple algebraic\nextension of valued fields $(L/K,v)$. This is defined as the minimal number of\naugmentations appearing in some Mac Lane-Vaqui\\'e chain for the valuation on\n$K[x]$ determined by the choice of some generator of the extension. In the\ndefectless and unibranched case, this concept leads to a generalization of a\nclassical result of Ore about the existence of $p$-regular generators for\nnumber fields. Also, we find what valuation-theoretic conditions characterize\nthe extensions having depth one.",
      "generated_abstract": "Let $V$ be a valuation ring, $O$ a maximal ideal, $S$ a non-zero $V$-algebra\nand $\\varphi : V\\to O$ a $S$-morphism. We show that there exists an integer\n$n\\geq 0$ such that $\\varphi^{n+1} = \\varphi^{n} \\circ \\varphi$ if and only if\n$\\varphi$ is a finite composition of extensions of $V$ by $O$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11475409836065574,
          "p": 0.16666666666666666,
          "f": 0.13592232526722614
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09836065573770492,
          "p": 0.14285714285714285,
          "f": 0.1165048495390708
        }
      }
    },
    {
      "paper_id": "quant-ph.quant-ph/2503.10400v1",
      "true_abstract": "We elucidate the requirements for quantum operations that achieve\nenvironment-assisted invariance (envariance), a symmetry of entanglement. While\nenvariance has traditionally been studied within the framework of local unitary\noperations, we extend the analysis to consider non-unitary local operations.\nFirst, we investigate the conditions imposed on operators acting on pure\nbipartite entanglement to attain envariance. We show that the local operations\nmust take a direct-sum form in their Kraus operator representations,\nestablishing decoherence-free subspaces. Furthermore, we prove that the unitary\noperation on the system's subspace uniquely determines the corresponding\nunitary operator on the environment's subspace. As an immediate consequence, we\ndemonstrate that environment-assisted shortcuts to adiabaticity cannot be\nachieved through non-unitary operations. In addition, we identify the\nrequirements that local operations must satisfy to ensure that the eternal\nblack hole states remain static in AdS/CFT.",
      "generated_abstract": "We present a novel no-go theorem for the existence of environment-assisted\ninvariance (EAI) in non-unitary dynamical systems. In particular, we show that\nin order for a quantum system to be EAI, it must either be an irreversible\nsystem, or possess a quantum-classical parameter that is not accessible to the\nenvironment. Furthermore, we provide a new proof of the no-go theorem for\nclassical systems in the context of EAI. Finally, we discuss the implications\nof our results for the existence of quantum control in non-unitary dynamics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2159090909090909,
          "p": 0.3392857142857143,
          "f": 0.2638888841358025
        },
        "rouge-2": {
          "r": 0.023809523809523808,
          "p": 0.038461538461538464,
          "f": 0.02941175998269972
        },
        "rouge-l": {
          "r": 0.20454545454545456,
          "p": 0.32142857142857145,
          "f": 0.2499999952469137
        }
      }
    },
    {
      "paper_id": "physics.hist-ph.q-bio/OT/2412.18030v1",
      "true_abstract": "The 2024 Nobel Prize in Physics was awarded to John Hopfield and Geoffrey\nHinton, \"for foundational discoveries and inventions that enable machine\nlearning with artificial neural networks.\" As noted by the Nobel committee,\ntheir work moved the boundaries of physics. This is a brief reflection on\nHopfield's work, its implications for the emergence of biological physics as a\npart of physics, the path from his early papers to the modern revolution in\nartificial intelligence, and prospects for the future.",
      "generated_abstract": "John Hopfield is known for his 1925 paper on the 'neuron' model of\nthe brain, and his 1974 paper on the 'neuronal network' model. But his\ninfluence stretches far beyond the study of neurons and networks. Hopfield\ncontinues to be a prominent figure in artificial intelligence, where his\nmodel of the 'neural net' is still used for learning, and in quantum\ninformation, where he was one of the founders of the field. This article\nrecalls Hopfield's early career and his development of the 'neuron' and 'neural\nnet' models, while discussing his later contributions to AI and quantum\ninformation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.296875,
          "p": 0.3064516129032258,
          "f": 0.3015872965885614
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.034482758620689655,
          "f": 0.03636363137851309
        },
        "rouge-l": {
          "r": 0.234375,
          "p": 0.24193548387096775,
          "f": 0.23809523309649797
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.19898v1",
      "true_abstract": "This study utilizes a simulated dataset to establish Python code for Double\nMachine Learning (DML) using Anaconda's Jupyter Notebook and the DML software\npackage from GitHub. The research focuses on causal inference experiments for\nboth binary and continuous treatment variables. The findings reveal that the\nDML model demonstrates relatively stable performance in calculating the Average\nTreatment Effect (ATE) and its robustness metrics. However, the study also\nhighlights that the computation of Conditional Average Treatment Effect (CATE)\nremains a significant challenge for future DML modeling, particularly in the\ncontext of continuous treatment variables. This underscores the need for\nfurther research and development in this area to enhance the model's\napplicability and accuracy.",
      "generated_abstract": "r introduces an implementation of a Bayesian Differential\nEvolution (DML) algorithm for Bayesian causal inference. The DML algorithm\nallows for a fast and scalable Bayesian model estimation process, making it a\npopular tool in the machine learning community. We have implemented this algorithm\nin Python, enabling its easy usage and adaptation by researchers and practitioners\nin econometrics and other fields. The DML algorithm is implemented in the\npackage econDML, which is available at https://github.com/C-H-J-J-J-J-J-J/econDML.\nThis implementation provides an efficient and scalable Bayesian model\nestimation process for the DML algorithm, making it a useful tool for\nBayesian causal inference. The package is open-source and available at\nhttps://github.com/C-H-J-J-J-J",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20253164556962025,
          "p": 0.25,
          "f": 0.22377621883123883
        },
        "rouge-2": {
          "r": 0.028846153846153848,
          "p": 0.03260869565217391,
          "f": 0.030612239916702186
        },
        "rouge-l": {
          "r": 0.189873417721519,
          "p": 0.234375,
          "f": 0.20979020484522481
        }
      }
    },
    {
      "paper_id": "cs.CL.econ/GN/2412.15239v1",
      "true_abstract": "Understanding when and why consumers engage with stories is crucial for\ncontent creators and platforms. While existing theories suggest that audience\nbeliefs of what is going to happen should play an important role in engagement\ndecisions, empirical work has mostly focused on developing techniques to\ndirectly extract features from actual content, rather than capturing\nforward-looking beliefs, due to the lack of a principled way to model such\nbeliefs in unstructured narrative data. To complement existing feature\nextraction techniques, this paper introduces a novel framework that leverages\nlarge language models to model audience forward-looking beliefs about how\nstories might unfold. Our method generates multiple potential continuations for\neach story and extracts features related to expectations, uncertainty, and\nsurprise using established content analysis techniques. Applying our method to\nover 30,000 book chapters from Wattpad, we demonstrate that our framework\ncomplements existing feature engineering techniques by amplifying their\nmarginal explanatory power on average by 31%. The results reveal that different\ntypes of engagement-continuing to read, commenting, and voting-are driven by\ndistinct combinations of current and anticipated content features. Our\nframework provides a novel way to study and explore how audience\nforward-looking beliefs shape their engagement with narrative media, with\nimplications for marketing strategy in content-focused industries.",
      "generated_abstract": "ding how people use and react to stories is a key challenge in\ncommunication and narrative analysis. The recent surge of large language models\n(LLMs) has provided new opportunities for research in this area, but the\npractical applications of LLM-based models remain limited. In this paper, we\nintroduce a novel approach to understanding story expectations by leveraging\nLLMs to generate engagement-based story summaries. Our approach consists of a\ndata generation module that generates story summaries based on the\ncharacter-level summarization model and a data augmentation module that\ngenerates additional data using a text-to-text generative model. We evaluate\nthis model on a dataset of 150 million stories collected from social media\nplatforms, including Facebook, Instagram, and Twitter. We show that our model\ngenerates stories that match the user expectations of the stories they post on\nsocial media",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2198581560283688,
          "p": 0.34065934065934067,
          "f": 0.26724137454258334
        },
        "rouge-2": {
          "r": 0.030612244897959183,
          "p": 0.046153846153846156,
          "f": 0.036809811155858956
        },
        "rouge-l": {
          "r": 0.2127659574468085,
          "p": 0.32967032967032966,
          "f": 0.25862068488741086
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.06619v1",
      "true_abstract": "We study the problem of synthetic generation of samples of environmental\nfeatures for autonomous vehicle navigation. These features are described by a\nspatiotemporally varying scalar field that we refer to as a threat field. The\nthreat field is known to have some underlying dynamics subject to process\nnoise. Some \"real-world\" data of observations of various threat fields are also\navailable. The assumption is that the volume of ``real-world'' data is\nrelatively small. The objective is to synthesize samples that are statistically\nsimilar to the data. The proposed solution is a generative artificial\nintelligence model that we refer to as a split variational recurrent neural\nnetwork (S-VRNN). The S-VRNN merges the capabilities of a variational\nautoencoder, which is a widely used generative model, and a recurrent neural\nnetwork, which is used to learn temporal dependencies in data. The main\ninnovation in this work is that we split the latent space of the S-VRNN into\ntwo subspaces. The latent variables in one subspace are learned using the\n``real-world'' data, whereas those in the other subspace are learned using the\ndata as well as the known underlying system dynamics. Through numerical\nexperiments we demonstrate that the proposed S-VRNN can synthesize data that\nare statistically similar to the training data even in the case of very small\nvolume of ``real-world'' training data.",
      "generated_abstract": "mous navigation, the need for real-time decision-making is crucial to\nadhere to the given time constraints and to keep the human operator safe.\nHowever, in dynamic environments, the real-time processing of the navigation\ncriteria can be challenging due to the high variability of the environment.\nThis paper presents a novel approach to synthesize real-time navigation\ncriteria, leveraging Generative Adversarial Networks (GANs). The proposed\nmethodology is based on a time-varying environment, in which the navigation\ncriteria are updated according to the driver's driving habits and driving\nconditions. A GAN-based methodology is developed to generate real-time\ncriteria, which are then used to optimize the time-varying navigation\nsolution. The proposed methodology uses the Lidar dataset and an existing\ngenerative model, to generate synthetic data for the navigation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1891891891891892,
          "p": 0.26582278481012656,
          "f": 0.22105262672077572
        },
        "rouge-2": {
          "r": 0.031578947368421054,
          "p": 0.05309734513274336,
          "f": 0.03960395571893879
        },
        "rouge-l": {
          "r": 0.18018018018018017,
          "p": 0.25316455696202533,
          "f": 0.21052631093130206
        }
      }
    },
    {
      "paper_id": "math.RA.math/RA/2503.08615v1",
      "true_abstract": "Let $H$ be a multiplicative monoid and $\\mathcal{P}_{{\\rm fin},1}(H)$ be the\nmonoid obtained by endowing the family of all non-empty finite subsets of $H$\ncontaining the identity $1_H$ with the operation of setwise multiplication\ninduced by $H$. We study fundamental aspects of the arithmetic of these\nmonoids, taking into account the possible presence of nontrivial idempotents.\nWe consider for this purpose minimal factorizations into irreducibles, a\nconcept recently introduced in the abstract context of a new general theory of\nfactorization. Among other results, we provide necessary and sufficient\nconditions on $H$ for $\\mathcal{P}_{{\\rm fin},1}(H)$ to admit unique minimal\nfactorizations. Our results generalize and shed new light on recent\ndevelopments on the topic.",
      "generated_abstract": "aper, we study the factorization property in power monoids, which is\nthe property that every non-zero element can be written as a product of powers\nof the identity element. This property is weaker than the well-known factorization\nproperty in groups, which states that every non-zero element can be written as a\nproduct of powers of the identity element in a group. We prove the following\nresults:\n  (i) The factorization property is weaker than the factorization property in\npower monoids.\n  (ii) If a power monoid satisfies the factorization property, then it is a\ngroup, but not a group.\n  (iii) If a power monoid satisfies the factorization property, then it is\namenable.\n  (iv) If a power monoid satisfies the factorization property, then it is\namenable and satisfies the factorization property in all finite factors.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19230769230769232,
          "p": 0.2727272727272727,
          "f": 0.22556390492396416
        },
        "rouge-2": {
          "r": 0.01834862385321101,
          "p": 0.02564102564102564,
          "f": 0.021390369468959326
        },
        "rouge-l": {
          "r": 0.19230769230769232,
          "p": 0.2727272727272727,
          "f": 0.22556390492396416
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2501.07828v1",
      "true_abstract": "To trade tokens in cryptoeconomic systems, automated market makers (AMMs)\ntypically rely on liquidity providers (LPs) that deposit tokens in exchange for\nrewards. To profit from such rewards, LPs must use effective liquidity\nprovisioning strategies. However, LPs lack guidance for developing such\nstrategies, which often leads them to financial losses. We developed a\nmeasurement model based on impermanent loss to analyze the influences of key\nparameters (i.e., liquidity pool type, position duration, position range size,\nand position size) of liquidity provisioning strategies on LPs' returns. To\nreveal the influences of those key parameters on LPs' profits, we used the\nmeasurement model to analyze 700 days of historical liquidity provision data of\nUniswap v3. By uncovering the influences of key parameters of liquidity\nprovisioning strategies on profitability, this work supports LPs in developing\nmore profitable strategies.",
      "generated_abstract": "Market Makers (AMMs) have been widely adopted by exchanges for\nprovidin\n   $ \\textstyle \\text{liquidity provisioning} $ and $ \\textstyle \\text{liquidity\n   management} $. However, many existing AMMs lack the ability to provide more\nprofitable liquidity provisioning strategies. In this paper, we propose a\nframework that enables AMMs to provide more profitable liquidity provisioning\nstrategies. The proposed framework is based on the notion of\n$ \\textstyle \\text{liquidity} $ \\textsubscript{min} \\textsubscript{max}\n\\textsubscript{profit}, which quantifies the minimum, maximum, and profitable\nprofit of different liquidity providers. The proposed framework employs a\nmarket-based approach to provide more profitable liquidity provisioning\nstrategies. The framework consists",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24444444444444444,
          "p": 0.3548387096774194,
          "f": 0.28947367938019397
        },
        "rouge-2": {
          "r": 0.04201680672268908,
          "p": 0.06329113924050633,
          "f": 0.05050504570911178
        },
        "rouge-l": {
          "r": 0.24444444444444444,
          "p": 0.3548387096774194,
          "f": 0.28947367938019397
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2410.18024v1",
      "true_abstract": "Systems Biology Graphical Notation (SBGN) is a standardised notational system\nthat visualises biochemical processes as networks. These visualizations lack a\nformal framework, so that the analysis of such networks through modelling and\nsimulation is an entirely separate task, determined by a chosen modelling\nframework (e.g. differential equations, Petri nets, stochastic processes,\ngraphs). A second research gap is the lack of a mathematical framework to\ncompose network representations. The complexity of molecular and cellular\nprocesses forces experimental studies to focus on subsystems. To study the\nfunctioning of biological systems across levels of structural and functional\norganisation, we require tools to compose and organise networks with different\nlevels of detail and abstraction.\n  We address these challenges by introducing a category-theoretic formalism for\nbiochemical processes visualised using SBGN Process Description (SBGN-PD)\nlanguage. Using the theory of structured cospans, we construct a symmetric\nmonoidal double category and demonstrate its horizontal 1-morphisms as SBGN\nProcess Descriptions. We obtain organisational principles such as\n'compositionality' (building a large SBGN-PD from smaller ones) and\n'zooming-out' (abstracting away details in biochemical processes) defined in\ncategory-theoretic terms. We also formally investigate how a particular portion\nof a biochemical network influences the remaining portion of the network and\nvice versa. Throughout the paper, we illustrate our findings using standard\nSBGN-PD examples.",
      "generated_abstract": "aper, we introduce a new mathematical framework to study graphical\nrepresentations of biochemical processes. We define the graphical representation\nof a biochemical process as a graph whose vertices represent reaction\ninstructions, and whose edges represent biochemical interactions. We show how\nthe graphical representation of a biochemical process is equivalent to the\ngraphical representation of a system, which is the graph obtained by\naggregating the graphical representations of all the processes in the system.\nUsing this equivalence, we show that the graphical representation of a\nbiochemical process can be understood as a global view of the system, and that\nthe graphical representation of a system can be understood as a local view of\nthe system. This allows us to study the mathematical properties of the graphical\nrepresentation of a biochemical process, such as its connectivity, its\nequivalence, and its homology. We also show how",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16216216216216217,
          "p": 0.39344262295081966,
          "f": 0.22966506763672997
        },
        "rouge-2": {
          "r": 0.05392156862745098,
          "p": 0.10784313725490197,
          "f": 0.07189542039215714
        },
        "rouge-l": {
          "r": 0.1554054054054054,
          "p": 0.3770491803278688,
          "f": 0.22009568964629936
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/SC/2501.13639v2",
      "true_abstract": "Droplet formation has emerged as an essential concept for the spatiotemporal\norganisation of biomolecules in cells. However, classical descriptions of\ndroplet dynamics based on passive liquid-liquid phase separation cannot capture\nthe complex situations inside cells. This review discusses three general\naspects that are crucial in cells: (i) biomolecules are diverse and\nindividually complex, implying that cellular droplets posses complex internal\nbehaviour, e.g., in terms of their material properties; (ii) the cellular\nenvironment contains many solid-like structures that droplets can wet; (iii)\ncells are alive and use fuel to drive processes out of equilibrium. We\nillustrate how these principles control droplet nucleation, growth, position,\nand count to unveil possible regulatory mechanisms in biological cells and\nother applications of phase separation.",
      "generated_abstract": "the smallest units of biological matter, play a critical role in\ncell physiology. They regulate cell behavior through interactions with their\nenvironment, such as cell-cell contact and mechanical cues. Droplet\nregulation is a fundamental yet poorly understood process in biological cells.\nTo address this challenge, we developed a microfluidic system that mimics\ndroplet-mediated cell-cell interactions in cell cultures. We investigated the\nbehavior of droplets within this system by performing dynamic imaging\nexperiments. We observed that the droplet boundary layer is anisotropic and\nweakly compressible. We also found that the droplet shape and volume change\nslowly during its motion in the cell culture, suggesting that droplets\ninteract with their environment via adhesion and diffusion. Our work reveals\nthat droplets exhibit key features of active",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14893617021276595,
          "p": 0.1590909090909091,
          "f": 0.15384614885158815
        },
        "rouge-2": {
          "r": 0.025210084033613446,
          "p": 0.02564102564102564,
          "f": 0.025423723813919396
        },
        "rouge-l": {
          "r": 0.13829787234042554,
          "p": 0.14772727272727273,
          "f": 0.14285713786257717
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.06873v1",
      "true_abstract": "We analyze over 44,000 NBER and CEPR working papers from 1980 to 2023 using a\ncustom language model to construct knowledge graphs that map economic concepts\nand their relationships. We distinguish between general claims and those\ndocumented via causal inference methods (e.g., DiD, IV, RDD, RCTs). We document\na substantial rise in the share of causal claims-from roughly 4% in 1990 to\nnearly 28% in 2020-reflecting the growing influence of the \"credibility\nrevolution.\" We find that causal narrative complexity (e.g., the depth of\ncausal chains) strongly predicts both publication in top-5 journals and higher\ncitation counts, whereas non-causal complexity tends to be uncorrelated or\nnegatively associated with these outcomes. Novelty is also pivotal for top-5\npublication, but only when grounded in credible causal methods: introducing\ngenuinely new causal edges or paths markedly increases both the likelihood of\nacceptance at leading outlets and long-run citations, while non-causal novelty\nexhibits weak or even negative effects. Papers engaging with central, widely\nrecognized concepts tend to attract more citations, highlighting a divergence\nbetween factors driving publication success and long-term academic impact.\nFinally, bridging underexplored concept pairs is rewarded primarily when\ngrounded in causal methods, yet such gap filling exhibits no consistent link\nwith future citations. Overall, our findings suggest that methodological rigor\nand causal innovation are key drivers of academic recognition, but sustained\nimpact may require balancing novel contributions with conceptual integration\ninto established economic discourse.",
      "generated_abstract": "This paper explores the use of causal claims in economics. We argue that the\nstylized facts of economic life are not sufficiently well understood to permit\nthe development of an epistemically valid causal framework. We show that a\ncausal framework is needed in order to develop a coherent theoretical framework\nfor the study of the determinants of economic outcomes. We identify a set of\ncausal claims that are necessary for a causal framework. Our paper provides\ninsights into the epistemic structure of the social sciences and the need for\ncausal claims in economic research.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09090909090909091,
          "p": 0.2909090909090909,
          "f": 0.1385281349000207
        },
        "rouge-2": {
          "r": 0.008658008658008658,
          "p": 0.023809523809523808,
          "f": 0.012698408787302792
        },
        "rouge-l": {
          "r": 0.07954545454545454,
          "p": 0.2545454545454545,
          "f": 0.12121211758400342
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10126v1",
      "true_abstract": "For a regularized least squares estimation of discrete-valued signals, we\npropose an LiGME regularizer, as a nonconvex regularizer, of designated\nisolated minimizers. The proposed regularizer is designed as a Generalized\nMoreau Enhancement (GME) of the so-called SOAV convex regularizer. Every\ncandidate vector in the discrete-valued set is aimed to be assigned to an\nisolated local minimizer of the proposed regularizer while the overall\nconvexity of the regularized least squares model is maintained. Moreover, a\nglobal minimizer of the proposed model can be approximated iteratively by using\na variant of cLiGME algorithm. To enhance the accuracy of the proposed\nestimation, we also propose a pair of simple modifications, called respectively\nan iterative reweighting and a generalized superiorization. Numerical\nexperiments demonstrate the effectiveness of the proposed model and algorithms\nin a scenario of MIMO signal detection.",
      "generated_abstract": "This paper introduces a novel LiGME regularizer that can be applied to\ndiscrete-valued signal estimation problems, specifically designed to minimize\nthe sum of the squared errors of the isolated minimizers. The LiGME regularizer\nis designed to optimize the local signal models, which are specific to each\ndiscrete-valued signal. Theoretically, this regularizer is designed to\nminimize the sum of the squared errors of the isolated minimizers for each\ndiscrete-valued signal. Experimental results show that this regularizer\nsignificantly outperforms existing regularizers, such as the $\\ell_1$ and\n$\\ell_2$ regularizers.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23170731707317074,
          "p": 0.37254901960784315,
          "f": 0.28571428098592355
        },
        "rouge-2": {
          "r": 0.05042016806722689,
          "p": 0.09375,
          "f": 0.06557376594344445
        },
        "rouge-l": {
          "r": 0.2073170731707317,
          "p": 0.3333333333333333,
          "f": 0.2556390930159987
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.09828v1",
      "true_abstract": "Deep learning has significantly advanced medical imaging analysis, yet\nvariations in image resolution remain an overlooked challenge. Most methods\naddress this by resampling images, leading to either information loss or\ncomputational inefficiencies. While solutions exist for specific tasks, no\nunified approach has been proposed. We introduce a resolution-invariant\nautoencoder that adapts spatial resizing at each layer in the network via a\nlearned variable resizing process, replacing fixed spatial down/upsampling at\nthe traditional factor of 2. This ensures a consistent latent space resolution,\nregardless of input or output resolution. Our model enables various downstream\ntasks to be performed on an image latent whilst maintaining performance across\ndifferent resolutions, overcoming the shortfalls of traditional methods. We\ndemonstrate its effectiveness in uncertainty-aware super-resolution,\nclassification, and generative modelling tasks and show how our method\noutperforms conventional baselines with minimal performance loss across\nresolutions.",
      "generated_abstract": "In this work, we propose a resolution invariant autoencoder (RIAE) for\nencapsulating resolution-invariant features from natural images. The proposed\nmethod is inspired by the recent success of resolution-invariant autoencoders\n(RIAEs) in image processing, particularly in the field of image denoising and\nrestoration. The proposed RIAE is a deep generative model that captures\nresolution-invariant features by transforming the input image into a latent\nspace. The latent space is then mapped back to the input image by an encoder\nnetwork, which preserves the resolution-invariant features. We conduct\nexperiments on both synthetic and real-world datasets to validate the effect of\nthe proposed RIAE on image restoration. The results demonstrate that the RIAE\nachieves superior image restoration performance, especially in terms of\nresolution-invariant features preservation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22123893805309736,
          "p": 0.3333333333333333,
          "f": 0.26595744201278865
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.027522935779816515,
          "f": 0.024291493044633756
        },
        "rouge-l": {
          "r": 0.21238938053097345,
          "p": 0.32,
          "f": 0.25531914414044826
        }
      }
    },
    {
      "paper_id": "math.FA.math/OA/2502.19028v2",
      "true_abstract": "We give a new proof of the Weyl-von Neumann-Berg theorem. Our proof improves\nHalmos' proof in 1972 by observing the fact that every compact set in the\ncomplex plane is the continuous image of a compact set in the real line.",
      "generated_abstract": "The von Neumann-Berg theorem states that any non-trivial, finite-dimensional\nalgebraic $C^*$-algebra is unitarily equivalent to a direct sum of finite-dimensional\n$C^*$-algebras. This theorem was first proved by Weyl in 1937. Later, in 1949,\nvon Neumann and Berg independently proved the same result. The original proof by\nWeyl was by hand computation. The proof by von Neumann and Berg was by\ncomputational methods. This paper provides a new proof of the von Neumann-Berg\ntheorem, which is an easy to understand and elementary proof. In the course of\nthis proof, we also prove the Weyl-von Neumann-Berg theorem, which is a corollary\nof this new proof.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.41379310344827586,
          "p": 0.19672131147540983,
          "f": 0.2666666622987655
        },
        "rouge-2": {
          "r": 0.1891891891891892,
          "p": 0.07777777777777778,
          "f": 0.11023621634323286
        },
        "rouge-l": {
          "r": 0.41379310344827586,
          "p": 0.19672131147540983,
          "f": 0.2666666622987655
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2502.01461v1",
      "true_abstract": "Computational prediction of enzymatic reactions represents a crucial\nchallenge in sustainable chemical synthesis across various scientific domains,\nranging from drug discovery to materials science and green chemistry. These\nsyntheses rely on proteins that selectively catalyze complex molecular\ntransformations. These protein catalysts exhibit remarkable substrate\nadaptability, with the same protein often catalyzing different chemical\ntransformations depending on its molecular partners. Current approaches to\nprotein representation in reaction prediction either ignore protein structure\nentirely or rely on static embeddings, failing to capture how proteins\ndynamically adapt their behavior to different substrates. We present\nDocking-Aware Attention (DAA), a novel architecture that generates dynamic,\ncontext-dependent protein representations by incorporating molecular docking\ninformation into the attention mechanism. DAA combines physical interaction\nscores from docking predictions with learned attention patterns to focus on\nprotein regions most relevant to specific molecular interactions. We evaluate\nour method on enzymatic reaction prediction, where it outperforms previous\nstate-of-the-art methods, achieving 62.2\\% accuracy versus 56.79\\% on complex\nmolecules and 55.54\\% versus 49.45\\% on innovative reactions. Through detailed\nablation studies and visualizations, we demonstrate how DAA generates\ninterpretable attention patterns that adapt to different molecular contexts.\nOur approach represents a general framework for context-aware protein\nrepresentation in biocatalysis prediction, with potential applications across\nenzymatic synthesis planning. We open-source our implementation and pre-trained\nmodels to facilitate further research.",
      "generated_abstract": "ration of molecular context with sequence-based representations\nis essential for improving protein-ligand docking. However, existing\nmethods often fail to address this challenge. To address this, we propose\nDocking-Aware Attention, a novel attention mechanism that dynamically integrates\nmolecular context with sequence-based representations during protein\ndocking. Our approach incorporates a sequence-to-attention module to encode\nmolecular context into the attention network, and a sequence-to-context\nmodule to integrate molecular context with sequence-based representations.\nExperiments on the Protein-Ligand Docking Benchmark (PLDB) demonstrate that our\nattention mechanism improves the performance of docking models. Our method\nreduces the error rate by 33.64% and 29.35% on the protein-ligand docking and\nprotein-protein docking benchmark",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1895424836601307,
          "p": 0.4084507042253521,
          "f": 0.25892856709861295
        },
        "rouge-2": {
          "r": 0.02358490566037736,
          "p": 0.053763440860215055,
          "f": 0.03278688100704167
        },
        "rouge-l": {
          "r": 0.16339869281045752,
          "p": 0.352112676056338,
          "f": 0.22321428138432722
        }
      }
    },
    {
      "paper_id": "eess.SP.math/IT/2503.10472v1",
      "true_abstract": "In this letter, we propose to deploy rotatable antennas (RAs) at the base\nstation (BS) to enhance both communication and sensing (C&S) performances, by\nexploiting a new spatial degree-of-freedom (DoF) offered by array rotation.\nSpecifically, we formulate a multi-objective optimization problem to\nsimultaneously maximize the sum-rate of multiple communication users and\nminimize the Cram\\'er-Rao bound (CRB) for target angle estimation, by jointly\noptimizing the transmit beamforming vectors and the array rotation angle at the\nBS. To solve this problem, we first equivalently decompose it into two\nsubproblems, corresponding to an inner problem for beamforming optimization and\nan outer problem for array rotation optimization. Although these two\nsubproblems are non-convex, we obtain their high-quality solutions by applying\nthe block coordinate descent (BCD) technique and one-dimensional exhaustive\nsearch, respectively. Moreover, we show that for the communication-only case,\nRAs provide an additional rotation gain to improve communication performance;\nwhile for the sensing-only case, the equivalent spatial aperture can be\nenlarged by RAs for achieving higher sensing accuracy. Finally, numerical\nresults are presented to showcase the performance gains of RAs over\nfixed-rotation antennas in integrated sensing and communications (ISAC).",
      "generated_abstract": "ng sensing and communication in wireless networks is of increasing\n importance due to the growing importance of sensing in cognitive radio networks\n and the growing need for efficient communication in the presence of\n multipath. However, most existing methods for sensing and communicating are\n designed for the case of static environments, where the antenna configuration\n remains fixed. In this paper, we propose a novel rotatable antenna for\n integrated sensing and communication, which can adaptively rotate the\n antenna configuration to optimize the sensing performance in the presence of\n multipath. This is achieved by designing the antenna configuration in such a\n way that it maximizes the sensing gain at the receiver, while ensuring that\n the transmit signal is sufficiently strong to transmit over the multipath.\n  We derive the closed-form expressions for the sensing gain and transmit\n signal power, and prove that the proposed antenna configuration always achieves\n the optimal sensing gain and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20967741935483872,
          "p": 0.3170731707317073,
          "f": 0.2524271796738619
        },
        "rouge-2": {
          "r": 0.0446927374301676,
          "p": 0.06299212598425197,
          "f": 0.052287576843735764
        },
        "rouge-l": {
          "r": 0.18548387096774194,
          "p": 0.2804878048780488,
          "f": 0.22330096608162892
        }
      }
    },
    {
      "paper_id": "physics.optics.q-bio/CB/2412.16427v1",
      "true_abstract": "Compressed streak imaging (CSI), introduced in 2014, has proven to be a\npowerful imaging technology for recording ultrafast phenomena such as light\npropagation and fluorescence lifetimes at over 150 trillion frames per second.\nDespite these achievements, CSI has faced challenges in detecting subtle\nintensity fluctuations in slow-moving, continuously illuminated objects. This\nlimitation, largely attributable to high streak compression and motion blur,\nhas curtailed broader adoption of CSI in applications such as cellular\nfluorescence microscopy. To address these issues and expand the utility of CSI,\nwe present a novel encoding strategy, termed two-axis compressed streak imaging\n(TACSI) that results in significant improvements to the reconstructed image\nfidelity. TACSI introduces a second scanning axis which shuttles a conjugate\nimage of the object with respect to the coded aperture. The moving image\ndecreases the streak compression ratio and produces a flash and shutter\nphenomenon that reduces coded aperture motion blur, overcoming the limitations\nof current CSI technologies. We support this approach with an analytical model\ndescribing the two-axis streak compression ratio, along with both simulated and\nempirical measurements. As proof of concept, we demonstrate the ability of\nTACSI to measure rapid variations in cell membrane potentials using\nvoltage-sensitive dye, which were previously unattainable with conventional\nCSI. This method has broad implications for high-speed photography, including\nthe visualization of action potentials, muscle contractions, and enzymatic\nreactions that occur on microsecond and faster timescales using fluorescence\nmicroscopy.",
      "generated_abstract": "he cellular dynamics of living cells in their native microscopic\nenvironment is crucial for understanding the underlying mechanisms of cell\nbehavior. However, standard microscopy techniques often require prolonged\nexposure times to achieve sufficient signal-to-noise ratio (SNR) for accurate\nmeasurements. To address this challenge, we present a novel microsecond-scale\nimaging technique that employs two-axis compressed streak imaging (CS-CS)\nfluorescence microscopy. By utilizing the fast temporal and spatial resolution\nof CS-CS, we are able to acquire microsecond-scale images of living cells\nwithin the native confines of the confocal microscope. This technique is\ndemonstrated for cellular dynamics in yeast and a human cell line, and is\ncomparable in SNR to existing techniques. Furthermore, the imaging speed of\nour method",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16352201257861634,
          "p": 0.3132530120481928,
          "f": 0.21487602855098706
        },
        "rouge-2": {
          "r": 0.04035874439461883,
          "p": 0.08035714285714286,
          "f": 0.05373133883252432
        },
        "rouge-l": {
          "r": 0.1509433962264151,
          "p": 0.2891566265060241,
          "f": 0.19834710293115232
        }
      }
    },
    {
      "paper_id": "cs.IT.stat/OT/2402.08135v1",
      "true_abstract": "Since its introduction in 2011, the partial information decomposition (PID)\nhas triggered an explosion of interest in the field of multivariate information\ntheory and the study of emergent, higher-order (\"synergistic\") interactions in\ncomplex systems. Despite its power, however, the PID has a number of\nlimitations that restrict its general applicability: it scales poorly with\nsystem size and the standard approach to decomposition hinges on a definition\nof \"redundancy\", leaving synergy only vaguely defined as \"that information not\nredundant.\" Other heuristic measures, such as the O-information, have been\nintroduced, although these measures typically only provided a summary statistic\nof redundancy/synergy dominance, rather than direct insight into the synergy\nitself. To address this issue, we present an alternative decomposition that is\nsynergy-first, scales much more gracefully than the PID, and has a\nstraightforward interpretation. Our approach defines synergy as that\ninformation in a set that would be lost following the minimally invasive\nperturbation on any single element. By generalizing this idea to sets of\nelements, we construct a totally ordered \"backbone\" of partial synergy atoms\nthat sweeps systems scales. Our approach starts with entropy, but can be\ngeneralized to the Kullback-Leibler divergence, and by extension, to the total\ncorrelation and the single-target mutual information. Finally, we show that\nthis approach can be used to decompose higher-order interactions beyond just\ninformation theory: we demonstrate this by showing how synergistic combinations\nof pairwise edges in a complex network supports signal communicability and\nglobal integration. We conclude by discussing how this perspective on\nsynergistic structure (information-based or otherwise) can deepen our\nunderstanding of part-whole relationships in complex systems.",
      "generated_abstract": "Higher-order structures (HOS) have been widely studied as a powerful\nmodeling framework for complex systems, yet their decomposition remains\nchallenging due to the inherent nonlinearity and the inherent complexity of\nHOS. This paper introduces a novel backbone decomposition framework that\nintegrates structural and temporal information to capture the synergy between\nHOS and their underlying network structures. Our framework first integrates\nthe temporal and structural information to capture the synergy between the\nHOS and their underlying network structures. Then, we propose a\ndecomposition-based backbone decomposition method to automatically and\nefficiently extract the backbone and the HOS from complex networks. Experimental\nresults show that our proposed framework outperforms existing methods,\nespecially in handling high-order structures and complex networks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11242603550295859,
          "p": 0.27941176470588236,
          "f": 0.1603375486506793
        },
        "rouge-2": {
          "r": 0.01968503937007874,
          "p": 0.05,
          "f": 0.028248583516869933
        },
        "rouge-l": {
          "r": 0.10650887573964497,
          "p": 0.2647058823529412,
          "f": 0.1518987300852785
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2411.01319v1",
      "true_abstract": "This paper addresses the estimation of the systemic risk measure known as\nCoVaR, which quantifies the risk of a financial portfolio conditional on\nanother portfolio being at risk. We identify two principal challenges:\nconditioning on a zero-probability event and the repricing of portfolios. To\ntackle these issues, we propose a decoupled approach utilizing smoothing\ntechniques and develop a model-independent theoretical framework grounded in a\nfunctional perspective. We demonstrate that the rate of convergence of the\ndecoupled estimator can achieve approximately $O_{\\rm P}(\\Gamma^{-1/2})$, where\n$\\Gamma$ represents the computational budget. Additionally, we establish the\nsmoothness of the portfolio loss functions, highlighting its crucial role in\nenhancing sample efficiency. Our numerical results confirm the effectiveness of\nthe decoupled estimators and provide practical insights for the selection of\nappropriate smoothing techniques.",
      "generated_abstract": "r proposes a novel nested estimator for the CoVaR of a random\nvector. The estimator combines a coarse-grained estimator for the mean with\na refined estimator for the tail, which are both based on the recursive\nexpectation. Our estimator is based on a decoupling approach, where a\ncoarse-grained estimator is used for the mean and a refined estimator for the\ntail of the random vector, both based on a recursive expectation. The\nrecursive expectation is used to define the coarse-grained estimator for the\nmean, which is then used to define a refined estimator for the tail of the\nrandom vector. The decoupling approach allows us to use the recursive\nexpectation to define both coarse-grained and refined estimators, which\nenables us to combine them in a nested manner. This approach allows us to\ncombine the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.26,
          "f": 0.18439715854333294
        },
        "rouge-2": {
          "r": 0.04132231404958678,
          "p": 0.05555555555555555,
          "f": 0.04739336003683706
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.26,
          "f": 0.18439715854333294
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.19370v1",
      "true_abstract": "In this project, we propose a Variational Inference algorithm to approximate\nposterior distributions. Building on prior methods, we develop the\nGradient-Steered Stein Variational Gradient Descent (G-SVGD) approach. This\nmethod introduces a novel loss function that combines a weighted gradient and\nthe Evidence Lower Bound (ELBO) to enhance convergence speed and accuracy. The\nlearning rate is determined through a suboptimal minimization of this loss\nfunction within a gradient descent framework.\n  The G-SVGD method is compared against the standard Stein Variational Gradient\nDescent (SVGD) approach, employing the ADAM optimizer for learning rate\nadaptation, as well as the Markov Chain Monte Carlo (MCMC) method. We assess\nperformance in two wave prospection models representing low-contrast and\nhigh-contrast subsurface scenarios. To achieve robust numerical approximations\nin the forward model solver, a five-point operator is employed, while the\nadjoint method improves accuracy in computing gradients of the log posterior.\n  Our findings demonstrate that G-SVGD accelerates convergence and offers\nimproved performance in scenarios where gradient evaluation is computationally\nexpensive. The abstract highlights the algorithm's applicability to wave\nprospection models and its potential for broader applications in Bayesian\ninference. Finally, we discuss the benefits and limitations of G-SVGD,\nemphasizing its contribution to advancing computational efficiency in\nuncertainty quantification.",
      "generated_abstract": "aper, we propose a novel variational gradient descent (VGD)\nmethods for the solution of the wave propagation problems. The VGD algorithm\nis based on the Stein's method. The Stein's method is a method for the\nestimation of the gradient of a non-convex loss function. The main advantage\nof the Stein's method is that it can be used for the estimation of the\ngradient of a non-convex function in the minimax sense. Therefore, the Stein's\nmethod provides a way for estimating the gradient of a loss function in the\noptimization sense. The Stein's method is also applied for the solution of\nwave propagation problems. We propose the Greedy Stein Variational Gradient\nDescent (GS-VGD) method for the wave propagation problems. The GS-VGD method\nis an iterative algorithm for the wave propagation problems. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17647058823529413,
          "p": 0.42857142857142855,
          "f": 0.2499999958680556
        },
        "rouge-2": {
          "r": 0.05759162303664921,
          "p": 0.12643678160919541,
          "f": 0.07913668634723899
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.42857142857142855,
          "f": 0.2499999958680556
        }
      }
    },
    {
      "paper_id": "physics.med-ph.physics/med-ph/2503.09505v1",
      "true_abstract": "Photoacoustic (PA) imaging of deep tissue tends to employ Q-switched lasers\nwith high pulse energy to generate high optical fluence and therefore high PA\nsignal. Compared to Q-switched lasers, pulsed laser diodes (PLDs) typically\ngenerate low pulse energy. In PA imaging applications with strong acoustic\nattenuation, such as through human skull bone, the broadband PA waves generated\nby nanoseconds laser pulses are significantly reduced in bandwidth during their\npropagation to a detector. As high-frequency PA signal components are not\ntransmitted through skull, we propose to not generate them by increasing\nexcitation pulse duration. Because PLDs are mainly limited in their peak power\noutput, an increase in pulse duration linearly increases pulse energy and\ntherefore PA signal amplitude. Here we show that the optimal pulse duration for\ndeep PA sensing through thick skull bone is far higher than in typical PA\napplications. Counterintuitively, this makes PLD excitation well-suited for\ntranscranial photoacoustics. We show this in PA sensing experiments on ex vivo\nhuman skull bone.",
      "generated_abstract": "ial photoacoustic imaging (PACI) is a non-invasive imaging modality\nthat is widely used for diagnostics and therapeutic interventions. While\ntranscranial laser ablation is widely used for treatment of brain tumours, it\nis not always feasible due to the limited access and the high cost of the\nlaser. This study explores the use of a pulsed laser diode as a light source\nfor transcranial PACI. The pulsed laser diode was designed and fabricated using\nlaser-machining technology. A pulsed laser diode was chosen over a conventional\nlaser due to its lower power consumption, which facilitates the use in\nresource-limited settings. The pulsed laser diode was used for transcranial\nPACI and the diode was found to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16822429906542055,
          "p": 0.2727272727272727,
          "f": 0.20809248082996434
        },
        "rouge-2": {
          "r": 0.013157894736842105,
          "p": 0.021505376344086023,
          "f": 0.01632652590220877
        },
        "rouge-l": {
          "r": 0.1588785046728972,
          "p": 0.25757575757575757,
          "f": 0.19653178718834585
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.16794v1",
      "true_abstract": "Auditory foundation models, including auditory large language models (LLMs),\nprocess all sound inputs equally, independent of listener perception. However,\nhuman auditory perception is inherently selective: listeners focus on specific\nspeakers while ignoring others in complex auditory scenes. Existing models do\nnot incorporate this selectivity, limiting their ability to generate\nperception-aligned responses. To address this, we introduce Intention-Informed\nAuditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM\n(AAD-LLM), a prototype system that integrates brain signals to infer listener\nattention. AAD-LLM extends an auditory LLM by incorporating intracranial\nelectroencephalography (iEEG) recordings to decode which speaker a listener is\nattending to and refine responses accordingly. The model first predicts the\nattended speaker from neural activity, then conditions response generation on\nthis inferred attentional state. We evaluate AAD-LLM on speaker description,\nspeech transcription and extraction, and question answering in multitalker\nscenarios, with both objective and subjective ratings showing improved\nalignment with listener intention. By taking a first step toward\nintention-aware auditory AI, this work explores a new paradigm where listener\nperception informs machine listening, paving the way for future\nlistener-centered auditory systems. Demo and code available:\nhttps://aad-llm.github.io.",
      "generated_abstract": "ual understanding (AVU) has emerged as a critical challenge in\ntechnology, transforming natural language processing (NLP) from a passive\nretrieval task to a more active exploration of audio-visual data. Existing\nmethods either focus on a single task or focus on a single modality, limiting\ntheir generalization and effectiveness. In this paper, we propose AAD-LLM, a\nmulti-modal neural attention-driven audio-visual understanding model, which\nintegrates audio-visual representations to achieve multi-modal AVU. We introduce\na novel Audio-Visual Attention Network (AVANet) that generates multi-modal\nrepresentations through an attention mechanism, enabling multi-modal\nunderstanding. Additionally, we design a multi-modal audio-visual alignment\nmodule to align the audio-visual representations across different modalities.\nTo effectively capture the temporal information of audio and visual,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17123287671232876,
          "p": 0.29411764705882354,
          "f": 0.21645021179887944
        },
        "rouge-2": {
          "r": 0.01092896174863388,
          "p": 0.018518518518518517,
          "f": 0.013745699799484349
        },
        "rouge-l": {
          "r": 0.1506849315068493,
          "p": 0.25882352941176473,
          "f": 0.19047618582485348
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.physics/pop-ph/2501.10701v1",
      "true_abstract": "It is found from textbooks that there are the different versions of the\nschematic diagram related to the Nernst equation, and consequently, it leads to\nsome discussion related to the Nernst equation and the discovery of other\nmeaningful schematic diagrams never appearing in literature. It is also found\nthat through the introduction of a new function, the schematic diagram of the\nNernst equation in the isothermal process of any thermodynamic system can be\ngenerated in a unified way and that the Nernst equation can be re-obtained from\nthe experimental data of low-temperature chemical reactions without any\nartificial additional assumptions. The results obtained here show clearly that\nthe centenary progress from the Nernst theorem to the Nernst statement is\ncompleted.",
      "generated_abstract": "t equation has been at the core of thermodynamic theory for a\ncrowning 150 years. In this review, we highlight some of the milestones in its\ndevelopment, including the discovery of the Nernst effect in 1841 and the\nformulation of the Nernst theorem in 1842 by J. J. Fermat, R. Fizeau, and M.\nLaplace. We then describe the Nernst statement, a key thermodynamic result that\nstates that the Nernst effect, or thermopower, is a thermodynamic quantity,\nproviding a fundamental link between the thermodynamics of heat, electricity,\nand magnetism. We also discuss how the Nernst equation, and the Nernst\nstatement, have been applied to study a wide range of phenomena, from heat and\nmagnetic transport to electrical",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24285714285714285,
          "p": 0.21794871794871795,
          "f": 0.22972972474433903
        },
        "rouge-2": {
          "r": 0.0891089108910891,
          "p": 0.08490566037735849,
          "f": 0.08695651674204793
        },
        "rouge-l": {
          "r": 0.17142857142857143,
          "p": 0.15384615384615385,
          "f": 0.16216215717677152
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.08252v1",
      "true_abstract": "Environmental and mental conditions are known risk factors for dermatitis and\nsymptoms of skin inflammation, but their interplay is difficult to quantify;\nepidemiological studies rarely include both, along with possible confounding\nfactors. Infodemiology leverages large online data sets to address this issue,\nbut fusing them produces strong patterns of spatial and temporal correlation,\nmissingness, and heterogeneity.\n  In this paper, we design a causal network that correctly models these complex\nstructures in large-scale infodemiological data from Google, EPA, NOAA and US\nCensus (434 US counties, 134 weeks). Our model successfully captures known\ncausal relationships between weather patterns, pollutants, mental conditions,\nand dermatitis. Key findings reveal that anxiety accounts for 57.4% of\nexplained variance in dermatitis, followed by NO2 (33.9%), while environmental\nfactors show significant mediation effects through mental conditions. The model\npredicts that reducing PM2.5 emissions by 25% could decrease dermatitis\nprevalence by 18%. Through statistical validation and causal inference, we\nprovide unprecedented insights into the complex interplay between environmental\nand mental health factors affecting dermatitis, offering valuable guidance for\npublic health policies and environmental regulations.",
      "generated_abstract": "miological study of infodemiology aims to identify the sources of\ninformation that are associated with the occurrence of diseases. This study\nconsiders dermatitis as a model disease for which the sources of information\nthat are associated with the occurrence of the disease are the skin's\nenvironment. This study aims to model the causal network of information sources\nthat influence the occurrence of dermatitis. In this study, the network of\ninformation sources is represented as a causal graph. The network is represented\nas a directed graph. The network of information sources is represented as a\nnetwork of causal graphs. We consider the information sources that are\nindependent and identically distributed (i.i.d.) as the source of information.\nThe data that are used in this study are the data that are collected from the\nInternet. The network of information sources is represented as a causal graph.\nThe network of information sources is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.35714285714285715,
          "f": 0.2094240796250103
        },
        "rouge-2": {
          "r": 0.017142857142857144,
          "p": 0.031578947368421054,
          "f": 0.022222217661180634
        },
        "rouge-l": {
          "r": 0.14814814814814814,
          "p": 0.35714285714285715,
          "f": 0.2094240796250103
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2502.12940v1",
      "true_abstract": "Global discrete optimization is notoriously difficult due to the lack of\ngradient information and the curse of dimensionality, making exhaustive search\ninfeasible. Tensor cross approximation is an efficient technique to approximate\nmultivariate tensors (and discretized functions) by tensor product\ndecompositions based on a small number of tensor elements, evaluated on\nadaptively selected fibers of the tensor, that intersect on submatrices of\n(nearly) maximum volume. The submatrices of maximum volume are empirically\nknown to contain large elements, hence the entries selected for cross\ninterpolation can also be good candidates for the globally maximal element\nwithin the tensor. In this paper we consider evolution of epidemics on\nnetworks, and infer the contact network from observations of network nodal\nstates over time. By numerical experiments we demonstrate that the contact\nnetwork can be inferred accurately by finding the global maximum of the\nlikelihood using tensor cross interpolation. The proposed tensor product\napproach is flexible and can be applied to global discrete optimization for\nother problems, e.g. discrete hyperparameter tuning.",
      "generated_abstract": "network inference is a fundamental task in Bayesian network\napplications. This work introduces a novel approach for global discrete optimization\nthat employs tensor cross interpolation (TCI). This method is applied to the\nproblem of Bayesian network inference. Specifically, we propose a new\noptimization algorithm that combines TCI with a greedy search. Our proposed\nmethod is designed to solve the optimization problem by iteratively constructing\na set of TCI-based models that are then used to refine the initial model. We\napply our approach to a number of real-world datasets to demonstrate its\neffectiveness in solving the inference problem. The proposed method is compared\nto other existing methods for Bayesian network inference. Our results show that\nthe proposed method achieves significantly better performance in terms of\nobjective value and number of iterations. This work provides a novel\noptimization framework for Bayesian network inference, offering a solution to\ncomplex problems in this",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24770642201834864,
          "p": 0.3,
          "f": 0.27135677896517774
        },
        "rouge-2": {
          "r": 0.05732484076433121,
          "p": 0.06666666666666667,
          "f": 0.06164383064482119
        },
        "rouge-l": {
          "r": 0.22935779816513763,
          "p": 0.2777777777777778,
          "f": 0.2512562764526149
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.18488v1",
      "true_abstract": "Aquaculture has been the fastest growing food production sector globally due\nto its potential to improve food security, stimulate economic growth, and\nreduce poverty. Its rapid development has been linked to sustainability\nchallenges, many of which are still unresolved and poorly understood.\nSmall-scale producers account for an increasing fraction of aquacultural\noutput. At the same time, many of these producers experience poverty, food\ninsecurity, and rely on unimproved production practices. We develop a stylized\nmathematical model to explore the effects of ecological, social, and economic\nfactors on the dynamics of a small-scale pond aquaculture system. Using\nanalytical and numerical methods, we explore the stability, asymptotic\ndynamics, and bifurcations of the model. Depending on the characteristics of\nthe system, the model exhibits one of three distinct configurations:\nmonostability with a global poverty trap in a nutrient-dominated or\nfish-dominated system; bistability with poverty trap and well-being attractors;\nmultistability with poverty trap and two well-being attractors with different\ncharacteristics. The model results show that intensification can be sustainable\nonly if it takes into account the local social-ecological context. In addition,\nthe heterogeneity of small-scale aquaculture producers matters, as the effects\nof intensification can be unevenly distributed among them. Finally, more is not\nalways better because too high nutrient input or productivity can lead to a\nsuboptimal attractor or system collapse.",
      "generated_abstract": "inable intensification of aquaculture systems is a complex\nprocess that involves multiple factors, such as the local context and\ncharacteristics of the producer. This study explores the relationship between\nthe sustainable intensification of small-scale aquaculture systems and the\ncharacteristics of the producer. It uses a multi-criteria decision-making\nframework and a mixed-method approach that includes interviews with fish\nproducers and focus groups, as well as a literature review. The results show\nthat the characteristics of the producer have a significant impact on the\nsustainable intensification of small-scale aquaculture systems. These\ncharacteristics include the type of fish farming, the degree of specialization\nof the producer, the number of employees, the type of production, and the\ncapacity of the producer to adapt to environmental changes. The study also\nreveals that some characteristics, such as the type of fish farming",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14383561643835616,
          "p": 0.2916666666666667,
          "f": 0.19266054603484567
        },
        "rouge-2": {
          "r": 0.04950495049504951,
          "p": 0.09174311926605505,
          "f": 0.06430867711913682
        },
        "rouge-l": {
          "r": 0.14383561643835616,
          "p": 0.2916666666666667,
          "f": 0.19266054603484567
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2407.00420v1",
      "true_abstract": "The potential health risks of cement dust exposure are increasingly raising\nconcern worldwide as the cement industry expands in response to rising cement\ndemand. This necessitates the need to determine the nature of the risks in\norder to develop appropriate measures. This study determined the effects of\ncement dust exposure on the weight and blood glucose levels of people residing\nor working around a cement company in Sokoto, Nigeria. Demographic information\nwas obtained using questionnaires from 72 participants, which included age,\ngender, educational level, exposure hours, occupation, and lifestyle. The blood\nglucose levels and body mass index (BMI) were measured using a Fine Test\nglucometer and a mechanical scale, respectively. The results showed that most\nof the people living or working around the cement company were middle-aged men\n(31-40; 42.06%) with a primary (33.33%) or secondary (45.83%) school education.\nIt showed that 30 (41.69%) of the participants were overweight while 5 (6.94%)\nwere obese. Additionally, 52.78% of the participants were diabetic while 31.94%\nwere prediabetic. Participants that were exposed for long hours (> 15 hours per\nday) were the most diabetic (20% of the participants), followed by smokers\n(15%), and artisans (7%). It can be concluded that exposure to cement dust from\nthe company increased the risk of overweight, obesity, and hyperglycemia among\nthe participants. These health risks were worsened by daily long hours of\nexposure, smoking, and artisanal pollutant exposure. Human settlements and\nartisans should not be located near the cement company, and the company should\nminimize pollutant emissions.",
      "generated_abstract": "d: Cement dust is a major environmental hazard. It is linked to\nhyperglycemia and overweight. Objectives: To examine the association of cement\ndust exposure with these outcomes among artisans and residents in Sokoto, Nigeria.\nDesign, Setting, and Participants: This cross-sectional study was conducted on\nartisans and residents of Sokoto. Data were collected using a self-administered\nquestionnaire. The exposure was cement dust. Data were analyzed using SPSS\nv27.0. Results: A total of 181 respondents participated in the study. The\nexposure of artisans was 0.285 mg/m3, while that of residents was 0.085 mg/m3.\nThe odds ratio (OR) of cement dust exposure and hyperglycemia",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16455696202531644,
          "p": 0.3880597014925373,
          "f": 0.2311111069289877
        },
        "rouge-2": {
          "r": 0.030042918454935622,
          "p": 0.07446808510638298,
          "f": 0.0428134515609427
        },
        "rouge-l": {
          "r": 0.16455696202531644,
          "p": 0.3880597014925373,
          "f": 0.2311111069289877
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.21129v1",
      "true_abstract": "We propose Microscopic Propagator Imaging (MPI) as a novel method to retrieve\nthe indices of the microscopic propagator which is the probability density\nfunction of water displacements due to diffusion within the nervous tissue\nmicrostructures. Unlike the Ensemble Average Propagator indices or the\nDiffusion Tensor Imaging metrics, MPI indices are independent from the\nmesoscopic organization of the tissue such as the presence of multiple axonal\nbundle directions and orientation dispersion. As a consequence, MPI indices are\nmore specific to the volumes, sizes, and types of microstructures, like axons\nand cells, that are present in the tissue. Thus, changes in MPI indices can be\nmore directly linked to alterations in the presence and integrity of\nmicrostructures themselves. The methodology behind MPI is rooted on zonal\nmodeling of spherical harmonics, signal simulation, and machine learning\nregression, and is demonstrated on both synthetic and Human Diffusion MRI data.",
      "generated_abstract": "ic Propagator Imaging (MPI) is a novel technique that allows for\n(1) the reconstruction of a diffusion MRI image directly from the motion\ncorrelation function of the probe magnetic resonance imaging (MRI) data, and\n(2) the reconstruction of a 3D diffusion MRI image directly from the motion\ncorrelation function of the probe diffusion MRI data. MPI is a promising\nmethodology for both in vivo and in vitro studies, as it enables the reconstruction\nof 3D diffusion MRI images from the motion correlation function of a 2D\nprobe-MRI sequence. MPI has also been demonstrated to be effective for\nunderstanding the effects of anisotropic diffusion on 3D diffusion MRI data,\nand for improving the contrast of 3D diffusion MRI images. In this paper, we\nintroduce a novel M",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2631578947368421,
          "p": 0.373134328358209,
          "f": 0.3086419704580095
        },
        "rouge-2": {
          "r": 0.058394160583941604,
          "p": 0.08791208791208792,
          "f": 0.07017543380001572
        },
        "rouge-l": {
          "r": 0.22105263157894736,
          "p": 0.31343283582089554,
          "f": 0.2592592544086268
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2502.02674v1",
      "true_abstract": "We address functional uncertainty quantification for ill-posed inverse\nproblems where it is possible to evaluate a possibly rank-deficient forward\nmodel, the observation noise distribution is known, and there are known\nparameter constraints. We present four constraint-aware confidence intervals\nextending the work of Batlle et al. (2023) by making the intervals both\ncomputationally feasible and less conservative. Our approach first shrinks the\npotentially unbounded constraint set compact in a data-adaptive way, obtains\nsamples of the relevant test statistic inside this set to estimate a quantile\nfunction, and then uses these computed quantities to produce the intervals. Our\ndata-adaptive bounding approach is based on the approach by Berger and Boos\n(1994), and involves defining a subset of the constraint set where the true\nparameter exists with high probability. This probabilistic guarantee is then\nincorporated into the final coverage guarantee in the form of an uncertainty\nbudget. We then propose custom sampling algorithms to efficiently sample from\nthis subset, even when the parameter space is high-dimensional.\nOptimization-based interval methods formulate confidence interval computation\nas two endpoint optimizations, where the optimization constraints can be set to\nachieve different types of interval calibration while seamlessly incorporating\nparameter constraints. However, choosing valid optimization constraints has\nbeen elusive. We show that all four proposed intervals achieve nominal coverage\nfor a particular functional both theoretically and in practice, with numerical\nexamples demonstrating superior performance of our intervals over the OSB\ninterval in terms of both coverage and expected length. In particular, we show\nthe superior performance in a realistic unfolding simulation from high-energy\nphysics that is severely ill-posed and involves a rank-deficient forward model.",
      "generated_abstract": "e problems, it is often desired to obtain a confidence interval\nfor the unknown functional value $f$ of interest. In this paper, we introduce\na novel methodology for constructing confidence intervals for functionals\n$f$ defined on a domain $\\Omega \\subset \\mathbb{R}^d$, where $f$ is unknown.\nThe methodology is data-driven and employs a data-adaptive sampling-based\ncalibration procedure to obtain confidence intervals for $f$. We develop a\ncomputational scheme for constructing confidence intervals for $f$ using\n$\\hat{f}_{\\hat{\\theta}}$, where $\\hat{\\theta}$ is the optimal parameter for a\ngiven functional estimator $\\hat{f}$. We derive a closed-form expression for\nthe confidence interval for $f$ using $\\hat{f}_{\\hat{\\theta}}$. Furthermore,\nwe establish the equivalence between the confidence interval for $f$ and the\ninterval on",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.328125,
          "f": 0.18103447876337703
        },
        "rouge-2": {
          "r": 0.01968503937007874,
          "p": 0.05154639175257732,
          "f": 0.028490024490386193
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.328125,
          "f": 0.18103447876337703
        }
      }
    },
    {
      "paper_id": "math.NT.math/NT/2503.09207v1",
      "true_abstract": "In this paper, we study the $p$-Selmer groups in the family of $p$-twists of\nan elliptic curve $E$ over a number field $K$. We prove that if $E/K$ is an\nelliptic curve over a number field $K$, and if $d$ is congruent to the\ndimension of the Selmer group of $E/K$ modulo $2$ and is greater than that\ndimension, then there exist infinitely many characters $\\chi \\in\n\\text{Hom}(G_K, \\mu_p)$ such that $\\text{dim}_{\\mathbb{F}_p}(\\text{Sel}_p(E/K,\n\\chi)) = d$ under certain conditions.",
      "generated_abstract": "In this paper, we study the twisting of the p-Selmer groups of modular forms\nby a finite group. In particular, we prove that the p-Selmer rank of a\nmodular form is at most the rank of its twist by a finite subgroup. This result\ncan be used to obtain new bounds for the rank of p-Selmer groups. We also\nprove that the rank of the p-Selmer group of a modular form is at most the rank\nof the twist by a subgroup of the p-adic unit group.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25862068965517243,
          "p": 0.36585365853658536,
          "f": 0.30303029817773697
        },
        "rouge-2": {
          "r": 0.1111111111111111,
          "p": 0.13333333333333333,
          "f": 0.12121211625344373
        },
        "rouge-l": {
          "r": 0.2413793103448276,
          "p": 0.34146341463414637,
          "f": 0.28282827797571686
        }
      }
    },
    {
      "paper_id": "math.OC.q-fin/PM/2411.07949v2",
      "true_abstract": "We consider a simplified model for optimizing a single-asset portfolio in the\npresence of transaction costs given a signal with a certain autocorrelation and\ncross-correlation structure. In our setup, the portfolio manager is given two\none-parameter controls to influence the construction of the portfolio. The\nfirst is a linear filtering parameter that may increase or decrease the level\nof autocorrelation in the signal. The second is a numerical threshold that\ndetermines a symmetric \"no-trade\" zone. Portfolio positions are constrained to\na single unit long or a single unit short. These constraints allow us to focus\non the interplay between the signal filtering mechanism and the hysteresis\nintroduced by the \"no-trade\" zone. We then formulate an optimization problem\nwhere we aim to minimize the frequency of trades subject to a fixed return\nlevel of the portfolio. We show that maintaining a no-trade zone while removing\nautocorrelation entirely from the signal yields a locally optimal solution. For\nany given \"no-trade\" zone threshold, this locally optimal solution also\nachieves the maximum attainable return level, and we derive a quantitative\nlower bound for the amount of improvement in terms of the given threshold and\nthe amount of autocorrelation removed.",
      "generated_abstract": "er the problem of constructing an optimal two-parameter portfolio\nmanagement strategy for a firm with transaction costs. We develop an\nalgorithmic framework to solve this problem, where the strategy is the\nlinear-quadratic utility function with transaction costs. We prove that the\nalgorithmic solution is a Nash equilibrium of the problem. We also present a\nsimplified algorithmic solution that approximates the Nash equilibrium\nprobability measure and is valid for any parameter set. This simple algorithm\nallows us to study the long-run performance of the portfolio management\nstrategy under transaction costs. We show that the optimal portfolio strategy\nalways outperforms the benchmark portfolio strategy, regardless of the\ntransaction costs. In addition, we analyze the effect of transaction costs on\nthe performance of the strategy. We find that the optimal portfolio strategy\ndoes not outperform the benchmark strategy when transaction costs are low. On\nthe other hand, when transaction costs are",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2543859649122807,
          "p": 0.37662337662337664,
          "f": 0.30366491665360057
        },
        "rouge-2": {
          "r": 0.05747126436781609,
          "p": 0.08333333333333333,
          "f": 0.0680272060530338
        },
        "rouge-l": {
          "r": 0.20175438596491227,
          "p": 0.2987012987012987,
          "f": 0.2408376915227105
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.06093v1",
      "true_abstract": "Bayesian Optimization (BO) is a well-established method for addressing\nblack-box optimization problems. In many real-world scenarios, optimization\noften involves multiple functions, emphasizing the importance of leveraging\ndata and learned functions from prior tasks to enhance efficiency in the\ncurrent task. To expedite convergence to the global optimum, recent studies\nhave introduced meta-learning strategies, collectively referred to as meta-BO,\nto incorporate knowledge from historical tasks. However, in practical settings,\nthe underlying functions are often heterogeneous, which can adversely affect\noptimization performance for the current task. Additionally, when the number of\nhistorical tasks is large, meta-BO methods face significant scalability\nchallenges. In this work, we propose a scalable and robust meta-BO method\ndesigned to address key challenges in heterogeneous and large-scale meta-tasks.\nOur approach (1) effectively partitions transferred meta-functions into highly\nhomogeneous clusters, (2) learns the geometry-based surrogate prototype that\ncapture the structural patterns within each cluster, and (3) adaptively\nsynthesizes meta-priors during the online phase using statistical\ndistance-based weighting policies. Experimental results on real-world\nhyperparameter optimization (HPO) tasks, combined with theoretical guarantees,\ndemonstrate the robustness and effectiveness of our method in overcoming these\nchallenges.",
      "generated_abstract": "r introduces a novel framework for meta-learning by integrating\nClustering-based Meta-Bayesian Optimization (C-MBO) into the meta-learning\nframework. C-MBO has been widely used in Bayesian optimization to reduce the\nnumber of model evaluations. However, it has not been explored in meta-learning\napplications, especially for the case where a large number of models are\navailable. This paper introduces a novel meta-learning framework that integrates\nC-MBO with the Bayesian optimization framework. Specifically, C-MBO is applied\nto a meta-optimizer to generate a set of candidate models, and the Bayesian\noptimization framework is used to search for the best model within this set.\nThe theoretical analysis of this framework shows that the performance of the\nmeta-optimizer is closely related to the performance of the Bayesian\noptimization framework. This paper provides a theoretical",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15714285714285714,
          "p": 0.3283582089552239,
          "f": 0.21256038209526487
        },
        "rouge-2": {
          "r": 0.0223463687150838,
          "p": 0.03636363636363636,
          "f": 0.02768165618467292
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.29850746268656714,
          "f": 0.1932367105976803
        }
      }
    },
    {
      "paper_id": "math.KT.math/KT/2503.06267v1",
      "true_abstract": "We present the fundamental properties of the K-theory groups of complex\nvector bundles endowed with actions of magnetic groups. In this work we show\nthat the magnetic equivariant K-theory groups define an equivariant cohomology\ntheory, we determine its coefficients, we show Bott's, Thom's and the degree\nshift isomorphism, we present the Atiyah-Hirzeburh spectral sequence, and we\nexplicitly calculate two magnetic equivariant K-theory groups in order to\nshowcase its applicability. These magnetic equivariant K-theory groups are\nrelevant in condensed matter physics since they provide topological invariants\nof gapped Hamiltonians in magnetic crystals.",
      "generated_abstract": "Magnetic K-theory is a recently proposed generalization of K-theory that\ncan be defined on any compact connected Lie group. In this paper, we define\nmagnetic K-theory for a compact connected Lie group $G$ and prove that it\ncoincides with magnetic K-theory of the group algebra of $G$. Furthermore, we\nshow that the equivariant K-theory of $G$ can be obtained from the magnetic\nK-theory by applying the equivariant K-theory functor. The results in this\npaper generalize those in the literature on magnetic K-theory.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23333333333333334,
          "p": 0.2916666666666667,
          "f": 0.25925925432098773
        },
        "rouge-2": {
          "r": 0.08641975308641975,
          "p": 0.1,
          "f": 0.09271522681461365
        },
        "rouge-l": {
          "r": 0.21666666666666667,
          "p": 0.2708333333333333,
          "f": 0.24074073580246924
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.09934v1",
      "true_abstract": "It is time to move on from attempts to make the pharmacy benefit manager\n(PBM) reseller business model more transparent. Time and time again the Big 3\nPBMs have developed opaque alternatives to piece-meal 100% pass-through\nmandates. Time and time again PBMs have demonstrated expertise in finding\nloopholes in state government disclosure laws. The purpose of this paper is to\nprovide quantitative estimates of two transparent insurance business models as\na solution to the PBM agency issue. The key parameter used is an 8% gross\nprofit margin figure disclosed by the Big 3 PBMs themselves. Based on reported\ndrug trend delivered to plans, we use a $1,200 to $1,500 per member per year\n(PMPY) as the range for this key performance indicator (KPI). We propose that\ndiscussions of PBM insurance business models start with the following figures:\n(1) a fixed premium model with medical loss ratio ranging from 92% to 85%; (2)\na fee-for-service model ranging from $96 to $180 PMPY with risk sharing of\ndeviations from a contracted PMPY delivered drug spend.",
      "generated_abstract": "r proposes a pharmacy benefit manager (PBM) insurance business\nmodel, which is a novel approach to insurance management by pharmacy\nbenefit managers (PBMs). The proposed model is designed to leverage the\nexpertise and resources of PBMs to provide comprehensive pharmacy benefits,\nensuring affordable and accessible pharmacy services to customers. The model\nuses the PBM's existing infrastructure and relationships with pharmacies to\nprovide comprehensive pharmacy benefits, including medication management,\ndrug rebates, and cost-sharing reductions. The model also incorporates\npredictive analytics to optimize healthcare spending and customer experience,\nenhancing customer satisfaction and reducing costs. This paper provides a\nfoundation for further research into PBM insurance models, with the goal of\nimproving pharmacy benefits and customer",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19658119658119658,
          "p": 0.2948717948717949,
          "f": 0.23589743109743597
        },
        "rouge-2": {
          "r": 0.043209876543209874,
          "p": 0.06666666666666667,
          "f": 0.052434452156714655
        },
        "rouge-l": {
          "r": 0.18803418803418803,
          "p": 0.28205128205128205,
          "f": 0.22564102084102575
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.07900v1",
      "true_abstract": "Cost-effective localization methods for Autonomous Underwater Vehicle (AUV)\nnavigation are key for ocean monitoring and data collection at high resolution\nin time and space. Algorithmic solutions suitable for real-time processing that\nhandle nonlinear measurement models and different forms of measurement\nuncertainty will accelerate the development of field-ready technology. This\npaper details a Bayesian estimation method for landmark-aided navigation using\na Side-scan Sonar (SSS) sensor. The method bounds navigation filter error in\nthe GPS-denied undersea environment and captures the highly nonlinear nature of\nslant range measurements while remaining computationally tractable. Combining a\nnovel measurement model with the chosen statistical framework facilitates the\nefficient use of SSS data and, in the future, could be used in real time. The\nproposed filter has two primary steps: a prediction step using an unscented\ntransform and an update step utilizing particles. The update step performs\nprobabilistic association of sonar detections with known landmarks. We evaluate\nalgorithm performance and tractability using synthetic data and real data\ncollected field experiments. Field experiments were performed using two\ndifferent marine robotic platforms with two different SSS and at two different\nsites. Finally, we discuss the computational requirements of the proposed\nmethod and how it extends to real-time applications.",
      "generated_abstract": "r navigation using sonar has been a challenging task in the\nunderwater domain. This paper presents a novel approach for landmark-based\nnavigation in an underwater environment using side-scan sonar data. The\nproposed method utilizes landmarks and side-scan sonar data to generate a\nlandmark-based map, which is used to determine the trajectory of the\nunderwater vehicle. The approach uses a deep learning algorithm to train the\nunderwater vehicle on the landmark-based map generated by the side-scan sonar.\nThe landmarks are extracted from the side-scan sonar data and used to train the\nunderwater vehicle. The trained underwater vehicle is used to navigate the\nunderwater vehicle using the landmarks in the landmark-based map. The results\nshow that the proposed approach outperforms existing approaches in terms of\ntrajectory accuracy, navigation stability, and energy efficiency. The proposed\napproach",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18518518518518517,
          "p": 0.38461538461538464,
          "f": 0.24999999561250005
        },
        "rouge-2": {
          "r": 0.04639175257731959,
          "p": 0.08411214953271028,
          "f": 0.05980065986953821
        },
        "rouge-l": {
          "r": 0.18518518518518517,
          "p": 0.38461538461538464,
          "f": 0.24999999561250005
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.16998v1",
      "true_abstract": "We provide a counterexample to the conduct parameter identification result\nestablished in the foundational work of Lau (1982), which generalizes the\nidentification theorem of Bresnahan (1982) by relaxing the linearity\nassumptions. We identify a separable demand function that still permits\nidentification and validate this case both theoretically and through numerical\nsimulations.",
      "generated_abstract": "In this study, we analyze the demand model of Lau (1982) to understand the\ncritical role of conduct parameters, which have been widely used in econometric\nanalysis. We find that the two-parameter model cannot explain the data, and we\npropose a counterexample to Lau's conclusions. Moreover, we propose a new\ndemand model that better fits the data. The model has one parameter less than\nthe two-parameter model and is easily estimated, and we examine the\ncharacteristics of this model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.38095238095238093,
          "p": 0.2962962962962963,
          "f": 0.3333333284114584
        },
        "rouge-2": {
          "r": 0.06,
          "p": 0.041666666666666664,
          "f": 0.04918032303144364
        },
        "rouge-l": {
          "r": 0.30952380952380953,
          "p": 0.24074074074074073,
          "f": 0.2708333284114584
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.07374v1",
      "true_abstract": "Recent statistical postprocessing methods for wind speed forecasts have\nincorporated linear models and neural networks to produce more skillful\nprobabilistic forecasts in the low-to-medium wind speed range. At the same\ntime, these methods struggle in the high-to-extreme wind speed range. In this\nwork, we aim to increase the performance in this range by training using a\nweighted version of the continuous ranked probability score (wCRPS). We develop\nan approach using shifted Gaussian cdf weight functions, whose parameters are\ntuned using a multi-objective hyperparameter tuning algorithm that balances\nperformance on low and high wind speed ranges. We explore this approach for\nboth linear models and convolutional neural network models combined with\nvarious parametric distributions, namely the truncated normal, log-normal, and\ngeneralized extreme value distributions, as well as adaptive mixtures. We apply\nthese methods to forecasts from KNMI's deterministic Harmonie-Arome numerical\nweather prediction model to obtain probabilistic wind speed forecasts in the\nNetherlands for 48 hours ahead. For linear models we observe that even with a\ntuned weight function, training using the wCRPS produces a strong body-tail\ntrade-off, where increased performance on extremes comes at the price of lower\nperformance for the bulk of the distribution. For the best models using\nconvolutional neural networks, we find that using a tuned weight function the\nperformance on extremes can be increased without a significant deterioration in\nperformance on the bulk. The best-performing weight function is shown to be\nmodel-specific. Finally, the choice of distribution has no significant impact\non the performance of our models.",
      "generated_abstract": "d is a critical factor in extreme wind events, affecting both\nthe physical and human dimensions of life. Wind speed is measured using\nsophisticated sensors that record high-precision readings. However, these\nmeasurements are often subject to errors, making it difficult to accurately\npredict extreme wind speeds. This paper proposes a statistical method for\nimproving wind speed prediction accuracy by incorporating tuned weighted\nscoring rules. The method combines multiple statistical models, such as\nextreme value models, to capture the complex dynamics of extreme wind speeds.\nThe proposed method utilizes the weighted sum of prediction scores to\nquantify the importance of each model. This method improves the accuracy of\nwind speed prediction by tailoring the weighting factor to each model,\naccounting for the unique characteristics of each dataset. The proposed\nmethod demonstrates the effectiveness of this approach in simulated and real\ndata,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1875,
          "p": 0.3,
          "f": 0.23076922603550304
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.030534351145038167,
          "f": 0.022792018113490294
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.26666666666666666,
          "f": 0.2051282003944774
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2411.13813v3",
      "true_abstract": "I examine the value of information from sell-side analysts by analyzing a\nlarge corpus of their written reports. Using embeddings from state-of-the-art\nlarge language models, I show that textual information in analyst reports\nexplains 10.19% of contemporaneous stock returns out-of-sample, a value that is\neconomically more significant than quantitative forecasts. I then perform a\nShapley value decomposition to assess how much each topic within the reports\ncontributes to explaining stock returns. The results show that analysts' income\nstatement analyses account for more than half of the reports' explanatory\npower. Expressing these findings in economic terms, I estimate that early\nacquisition of analysts' reports can yield significant profits. Analysts'\ninformation value peeks in the first week following earnings announcements,\nhighlighting their vital role in interpreting new financial data.",
      "generated_abstract": "of information from sell-side analysts is examined using the\ninformation content of the estimates of earnings and cash flows. A\ncorrelation between the information content of the analyst estimates of earnings\nand cash flows and the information content of the analyst estimates of\ndividend yield is found. The analyst information content estimates are\nderived using the value of information from the analyst estimates of earnings\nand cash flows. The analyst information content estimates are used to construct\nan index of the value of information from the analyst estimates of earnings\nand cash flows. This index is used to construct a measure of the value of\ninformation from the analyst estimates of earnings and cash flows. The\ninformation content of the analyst estimates of earnings and cash flows is\nused to construct a measure of the value of information from the analyst\nestimates of earnings and cash flows",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.4117647058823529,
          "f": 0.22399999603968004
        },
        "rouge-2": {
          "r": 0.056,
          "p": 0.125,
          "f": 0.07734806202496895
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.4117647058823529,
          "f": 0.22399999603968004
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.06413v1",
      "true_abstract": "Despite a plethora of anomaly detection models developed over the years,\ntheir ability to generalize to unseen anomalies remains an issue, particularly\nin critical systems. This paper aims to address this challenge by introducing\nSwift Hydra, a new framework for training an anomaly detection method based on\ngenerative AI and reinforcement learning (RL). Through featuring an RL policy\nthat operates on the latent variables of a generative model, the framework\nsynthesizes novel and diverse anomaly samples that are capable of bypassing a\ndetection model. These generated synthetic samples are, in turn, used to\naugment the detection model, further improving its ability to handle\nchallenging anomalies. Swift Hydra also incorporates Mamba models structured as\na Mixture of Experts (MoE) to enable scalable adaptation of the number of Mamba\nexperts based on data complexity, effectively capturing diverse feature\ndistributions without increasing the model's inference time. Empirical\nevaluations on ADBench benchmark demonstrate that Swift Hydra outperforms other\nstate-of-the-art anomaly detection models while maintaining a relatively short\ninference time. From these results, our research highlights a new and\nauspicious paradigm of integrating RL and generative AI for advancing anomaly\ndetection.",
      "generated_abstract": "anomalies in large datasets is crucial for enhancing data\nrelevance and accuracy. In this paper, we propose Swift Hydra, a novel\nframework for generating multiple Mamba models and utilizing them for\nanomaly detection. Mamba is a powerful method that generates anomalous\nsamples by using random perturbations to model the distribution of data.\nHowever, Mamba models are limited to only one or two dimensions. To address\nthis limitation, we propose a new framework that enables the generation of\nmultiple Mamba models and their simultaneous usage for anomaly detection.\nSwift Hydra provides the ability to generate multiple Mamba models in a\nself-reinforcing manner, enabling the model to learn and refine its\npredictions. Additionally, we propose a new method for evaluating the\nperformance of multiple Mamba models, which is based on the Shapley value.\nWe validate the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2601626016260163,
          "p": 0.38095238095238093,
          "f": 0.3091787391388364
        },
        "rouge-2": {
          "r": 0.06857142857142857,
          "p": 0.10084033613445378,
          "f": 0.08163264824263067
        },
        "rouge-l": {
          "r": 0.24390243902439024,
          "p": 0.35714285714285715,
          "f": 0.2898550676412519
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.10718v1",
      "true_abstract": "The escalating challenges of managing vast sensor-generated data,\nparticularly in audio applications, necessitate innovative solutions. Current\nsystems face significant computational and storage demands, especially in\nreal-time applications like gunshot detection systems (GSDS), and the\nproliferation of edge sensors exacerbates these issues. This paper proposes a\ngroundbreaking approach with a near-sensor model tailored for intelligent\naudio-sensing frameworks. Utilizing a Fast Fourier Transform (FFT) module,\nconvolutional neural network (CNN) layers, and HyperDimensional Computing\n(HDC), our model excels in low-energy, rapid inference, and online learning. It\nis highly adaptable for efficient ASIC design implementation, offering superior\nenergy efficiency compared to conventional embedded CPUs or GPUs, and is\ncompatible with the trend of shrinking microphone sensor sizes. Comprehensive\nevaluations at both software and hardware levels underscore the model's\nefficacy. Software assessments through detailed ROC curve analysis revealed a\ndelicate balance between energy conservation and quality loss, achieving up to\n82.1% energy savings with only 1.39% quality loss. Hardware evaluations\nhighlight the model's commendable energy efficiency when implemented via ASIC\ndesign, especially with the Google Edge TPU, showcasing its superiority over\nprevalent embedded CPUs and GPUs.",
      "generated_abstract": "cessing has emerged as a critical technology for enhancing user\n experiences in multimedia applications, but scalability challenges still exist\n due to the large size of audio data and the limited computational capacity of\n edge devices. In this paper, we propose an innovative architecture for audio\n processing on extreme edge devices, which is based on a hyperdimensional\n sensing model. Specifically, we leverage the hyperdimensional sensing model\n to design an efficient and scalable algorithm for audio processing. By\n decomposing audio into a set of low-dimensional representations, our\n framework reduces the computational burden of audio processing on edge\n devices. Additionally, the hyperdimensional sensing model enables the\n effective and efficient aggregation of information across different levels of\n dimensionality. We conduct extensive experiments on both simulated and\n real-world audio datasets to evaluate the performance of our framework. The\n results demonstrate that our framework outperforms the baseline method in\n terms of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14383561643835616,
          "p": 0.22340425531914893,
          "f": 0.17499999523472237
        },
        "rouge-2": {
          "r": 0.011235955056179775,
          "p": 0.015151515151515152,
          "f": 0.012903220916547116
        },
        "rouge-l": {
          "r": 0.11643835616438356,
          "p": 0.18085106382978725,
          "f": 0.14166666190138905
        }
      }
    },
    {
      "paper_id": "cs.CE.q-fin/CP/2412.18174v1",
      "true_abstract": "Recent advancements have underscored the potential of large language model\n(LLM)-based agents in financial decision-making. Despite this progress, the\nfield currently encounters two main challenges: (1) the lack of a comprehensive\nLLM agent framework adaptable to a variety of financial tasks, and (2) the\nabsence of standardized benchmarks and consistent datasets for assessing agent\nperformance. To tackle these issues, we introduce \\textsc{InvestorBench}, the\nfirst benchmark specifically designed for evaluating LLM-based agents in\ndiverse financial decision-making contexts. InvestorBench enhances the\nversatility of LLM-enabled agents by providing a comprehensive suite of tasks\napplicable to different financial products, including single equities like\nstocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we\nassess the reasoning and decision-making capabilities of our agent framework\nusing thirteen different LLMs as backbone models, across various market\nenvironments and tasks. Furthermore, we have curated a diverse collection of\nopen-source, multi-modal datasets and developed a comprehensive suite of\nenvironments for financial decision-making. This establishes a highly\naccessible platform for evaluating financial agents' performance across various\nscenarios.",
      "generated_abstract": "t advancements of large language models (LLMs) in financial\ntask-oriented domains have significantly improved their performance in\nfinancial decision-making tasks. However, existing benchmarks for these\ntasks are limited in scope and are primarily focused on the fintech domain.\nTo address this gap, we introduce INVESTORBENCH, the first benchmark for\nfinancial decision-making tasks with LLM-based agents. The benchmark comprises\ntwo datasets: INVESTORBENCH-2.0, a multidimensional dataset with\n10,000-dimensional agent attributes, and INVESTORBENCH-3.0, a\nmulti-agent-type-based dataset with 5,000-dimensional agent attributes. We\nprovide a comprehensive evaluation framework for LLM-based agents in financial\ndecision-making tasks, which includes 11 different evaluation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2777777777777778,
          "p": 0.4225352112676056,
          "f": 0.33519552593988955
        },
        "rouge-2": {
          "r": 0.07142857142857142,
          "p": 0.125,
          "f": 0.09090908628099197
        },
        "rouge-l": {
          "r": 0.25925925925925924,
          "p": 0.39436619718309857,
          "f": 0.3128491572248057
        }
      }
    },
    {
      "paper_id": "econ.TH.cs/GT/2503.06582v1",
      "true_abstract": "The steady rise of e-commerce marketplaces underscores the need to study a\nmarket structure that captures the key features of this setting. To this end,\nwe consider a price-quantity Stackelberg duopoly in which the leader is the\nmarketplace operator and the follower is an independent seller. The objective\nof the marketplace operator is to maximize a weighted sum of profit and a term\ncapturing positive customer experience, whereas the independent seller solely\nseeks to maximize their own profit. Furthermore, the independent seller is\nrequired to share a fraction of their revenue with the marketplace operator for\nthe privilege of selling on the platform. We derive the subgame-perfect Nash\nequilibrium of this game and find that the equilibrium strategies depend on the\nassumed rationing rule. We then consider practical implications for marketplace\noperators. Finally, we show that, under intensity rationing, consumer surplus\nand total welfare in the duopoly marketplace is always at least as high as\nunder an independent seller monopoly, demonstrating that it is socially\nbeneficial for the operator to join the market as a seller.",
      "generated_abstract": "e the impact of a marketplace operator on the price of a\nproduct. In the absence of a marketplace operator, the price is a monotonic\nfunction of the number of sellers. When the marketplace operator can set the\nprice, the price is a convex function of the number of sellers. Our analysis\nshows that the marketplace operator can influence the price either by reducing\nthe number of sellers or by increasing the price. We show that a marketplace\noperator can induce a decrease in the number of sellers by setting the price\nhigher than the true price and a price increase by setting the price lower than\nthe true price. We further analyze the effect of the marketplace operator on\nthe market. We show that, in the presence of a marketplace operator, a\nprice-increasing marketplace operator will induce a price decrease in the\nmarket, while a price-decreasing marketplace operator will indu",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1485148514851485,
          "p": 0.2631578947368421,
          "f": 0.18987341310927747
        },
        "rouge-2": {
          "r": 0.05555555555555555,
          "p": 0.09183673469387756,
          "f": 0.06923076453372812
        },
        "rouge-l": {
          "r": 0.1485148514851485,
          "p": 0.2631578947368421,
          "f": 0.18987341310927747
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2410.20915v1",
      "true_abstract": "In the literature on stochastic frontier models until the early 2000s, the\njoint consideration of spatial and temporal dimensions was often inadequately\naddressed, if not completely neglected. However, from an evolutionary economics\nperspective, the production process of the decision-making units constantly\nchanges over both dimensions: it is not stable over time due to managerial\nenhancements and/or internal or external shocks, and is influenced by the\nnearest territorial neighbours. This paper proposes an extension of the Fusco\nand Vidoli [2013] SEM-like approach, which globally accounts for spatial and\ntemporal effects in the term of inefficiency. In particular, coherently with\nthe stochastic panel frontier literature, two different versions of the model\nare proposed: the time-invariant and the time-varying spatial stochastic\nfrontier models. In order to evaluate the inferential properties of the\nproposed estimators, we first run Monte Carlo experiments and we then present\nthe results of an application to a set of commonly referenced data,\ndemonstrating robustness and stability of estimates across all scenarios.",
      "generated_abstract": "r develops a spatio-temporal stochastic frontier (STSFR) model for\nexamining the performance of manufacturing firms in terms of profitability and\nproductivity. The STSFR model incorporates the spatio-temporal dynamics of\nplant-level production activities, sales, and productivity, with the\nproduction activities modeled as a spatio-temporal stochastic frontier\nprocess. The production activities are driven by plant-level inputs, with the\ninputs being influenced by plant-level production activities, sales, and\nproductivity. The model is estimated by maximizing the log likelihood over\nplant-level production activities and the plant-level inputs. A two-step\nstochastic approximation method is used to estimate the model. The two-step\nmethod ensures that the estimation of the model is performed in a sequential\nmanner. The estimation procedure is iterative and includes an",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16379310344827586,
          "p": 0.296875,
          "f": 0.21111110652839515
        },
        "rouge-2": {
          "r": 0.045454545454545456,
          "p": 0.06930693069306931,
          "f": 0.054901956000308
        },
        "rouge-l": {
          "r": 0.14655172413793102,
          "p": 0.265625,
          "f": 0.18888888430617293
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.10169v1",
      "true_abstract": "As part of work to connect phylogenetics with machine learning, there has\nbeen considerable recent interest in vector encodings of phylogenetic trees. We\npresent a simple new ``ordered leaf attachment'' (OLA) method for uniquely\nencoding a binary, rooted phylogenetic tree topology as an integer vector. OLA\nencoding and decoding take linear time in the number of leaf nodes, and the set\nof vectors corresponding to trees is a simply-described subset of integer\nsequences. The OLA encoding is unique compared to other existing encodings in\nhaving these properties. The integer vector encoding induces a distance on the\nset of trees, and we investigate this distance in relation to the NNI and SPR\ndistances.",
      "generated_abstract": "Phylogenetic trees are useful representations for studying evolutionary\nhistory. The structure of these trees can be represented using a vector of\nnumbers, where each entry is the relative frequency of the corresponding node.\nHowever, this representation is not unique and there is no standard way to\nchoose which entries to use. We describe a new vector encoding method for\nphylogenetic trees that uses ordered attachment of nodes to a vector. This\nmethod allows the same tree to be represented using different vector\nencodings, which can lead to different inferences about the tree. We show\nthat this encoding method is effective for trees with high levels of\ndivergence. We also show that the encoding method can be used to construct a\ndetailed tree from a phylogenetic tree, which is useful for investigating the\nbranching patterns of evolutionary trees.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2894736842105263,
          "p": 0.27848101265822783,
          "f": 0.28387096274380863
        },
        "rouge-2": {
          "r": 0.028037383177570093,
          "p": 0.023255813953488372,
          "f": 0.02542372385701044
        },
        "rouge-l": {
          "r": 0.27631578947368424,
          "p": 0.26582278481012656,
          "f": 0.270967736937357
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2412.20550v1",
      "true_abstract": "Achaete-scute complex homolog 2 (ASCL2) codes a part of the basic\nhelix-loop-helix (BHLH) transcription factor family. WNTs have been found to\ndirectly affect the stemness of the tumor cells via regulation of ASCL2.\nSwitching off the ASCL2 literally blocks the stemness process of the tumor\ncells and vice versa. In colorectal cancer (CRC) cells treated with\nETC-1922159, ASCL2 was found to be down regulated along with other genes. A\nrecently developed search engine ranked combinations of ASCL2-X (X, a\nparticular gene/protein) at 2nd order level after drug administration. Some\nrankings confirm the already tested combinations, while others point to those\nthat are untested/unexplored. These rankings reveal which ASCL2-X combinations\nmight be working synergistically in CRC. In this research work, I cover\ncombinations of ASCL2 with WNT, transforming growth factor beta (TGFB),\ninterleukin (IL), leucine rich repeat containing G protein-coupled receptor\n(LGR), NOTCH, solute carrier family (SLC), SRY-box transcription factor (SOX),\nsmall nucleolar RNA host gene (SNHG), KIAA, F-box protein (FBXO), family with\nsequence similarity (FAM), B cell CLL/lymphoma (BCL), autophagy related (ATG)\nand Rho GTPase activating protein (ARHGAP) family.",
      "generated_abstract": "chine learning to identify the ASCL2-X protein interaction network\nthat best predicts the outcome of the colorectal cancer cell line ETC-1922159\ntreated with ASCL2-X inhibitors, which are currently being tested in clinical\ntrials. The results indicate that the best model predicts the cell line\nresponse to ASCL2-X inhibitors with an accuracy of 70%. We further investigate\nthe ASCL2-X protein interaction network, identifying 1609 protein-protein\ninteractions and 211 protein-DNA interactions. We show that ASCL2-X inhibitors\ninduce a significant increase in DNA methylation. We also find that ASCL2-X\ninhibitors increase the expression of several oncogenic genes and decrease the\nexpression of tumor suppressor genes. These findings",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.29411764705882354,
          "f": 0.19230768790680483
        },
        "rouge-2": {
          "r": 0.01775147928994083,
          "p": 0.03225806451612903,
          "f": 0.022900758779500947
        },
        "rouge-l": {
          "r": 0.12857142857142856,
          "p": 0.2647058823529412,
          "f": 0.1730769186760356
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.stat/ML/2503.04454v1",
      "true_abstract": "By leveraging tools from the statistical mechanics of complex systems, in\nthese short notes we extend the architecture of a neural network for\nhetero-associative memory (called three-directional associative memories, TAM)\nto explore supervised and unsupervised learning protocols. In particular, by\nproviding entropic-heterogeneous datasets to its various layers, we predict and\nquantify a new emergent phenomenon -- that we term {\\em layer's\ncooperativeness} -- where the interplay of dataset entropies across network's\nlayers enhances their retrieval capabilities Beyond those they would have\nwithout reciprocal influence. Naively we would expect layers trained with less\ninformative datasets to develop smaller retrieval regions compared to those\npertaining to layers that experienced more information: this does not happen\nand all the retrieval regions settle to the same amplitude, allowing for\noptimal retrieval performance globally. This cooperative dynamics marks a\nsignificant advancement in understanding emergent computational capabilities\nwithin disordered systems.",
      "generated_abstract": "pt of associative memory is a fundamental memory model, enabling\nmemory retrieval by associating the location of a key element with other\nelements. In this work, we introduce a novel associative memory model that\nenables memory retrieval by associating the position of a key element with a\nsuitable number of key elements. In this model, we propose to assign a\ndistinctive number to each key element, and to store each key element in a\nspecific position in the memory. In this way, we can associate the key element\nwith any number of other elements. We show that this model extends the\nassociative memory concept, and introduces new types of associative memories,\nsuch as associative memory with cooperative elements and associative memory\nwith cooperative positions. We validate the model through numerical simulations\nand analysis of experimental data. Our results show that the model can\neffectively represent a variety of associative memories",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16216216216216217,
          "p": 0.2571428571428571,
          "f": 0.19889502288086458
        },
        "rouge-2": {
          "r": 0.014184397163120567,
          "p": 0.01639344262295082,
          "f": 0.015209120501382303
        },
        "rouge-l": {
          "r": 0.13513513513513514,
          "p": 0.21428571428571427,
          "f": 0.16574585161014638
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.09806v1",
      "true_abstract": "Interdependencies between units in online two-sided marketplaces complicate\nestimating causal effects in experimental settings. We propose a novel\nexperimental design to mitigate the interference bias in estimating the total\naverage treatment effect (TATE) of item-side interventions in online two-sided\nmarketplaces. Our Two-Sided Prioritized Ranking (TSPR) design uses the\nrecommender system as an instrument for experimentation. TSPR strategically\nprioritizes items based on their treatment status in the listings displayed to\nusers. We designed TSPR to provide users with a coherent platform experience by\nensuring access to all items and a consistent realization of their treatment by\nall users. We evaluate our experimental design through simulations using a\nsearch impression dataset from an online travel agency. Our methodology closely\nestimates the true simulated TATE, while a baseline item-side estimator\nsignificantly overestimates TATE.",
      "generated_abstract": "r examines the potential of recommender systems (RS) for designing\nexperimental design (ED) strategies in two-sided platforms (2P) with non-ideal\nrecommender systems (NIRS) to increase experimental coverage and reduce\nnon-replicability. We develop a two-stage optimization framework to identify\nthe optimal set of experiments to maximize the coverage of the ED. The first\nstage determines the set of NIRS, while the second stage prioritizes experiments\nbased on the user's personalized NIRS. We evaluate the performance of our\nframework using two-sided 2P with NIRS, and demonstrate that our framework\noutperforms conventional ED in terms of coverage and generalizability. Our\nfindings highlight the potential of RS in 2P to enhance experimental design\nstrategies for NIRS, contributing to the advancement of experimental design",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23076923076923078,
          "p": 0.3,
          "f": 0.26086956030245756
        },
        "rouge-2": {
          "r": 0.024390243902439025,
          "p": 0.02727272727272727,
          "f": 0.0257510679769392
        },
        "rouge-l": {
          "r": 0.23076923076923078,
          "p": 0.3,
          "f": 0.26086956030245756
        }
      }
    },
    {
      "paper_id": "math.ST.econ/EM/2412.18080v1",
      "true_abstract": "There are many nonparametric objects of interest that are a function of a\nconditional distribution. One important example is an average treatment effect\nconditional on a subset of covariates. Many of these objects have a conditional\ninfluence function that generalizes the classical influence function of a\nfunctional of a (unconditional) distribution. Conditional influence functions\nhave important uses analogous to those of the classical influence function.\nThey can be used to construct Neyman orthogonal estimating equations for\nconditional objects of interest that depend on high dimensional regressions.\nThey can be used to formulate local policy effects and describe the effect of\nlocal misspecification on conditional objects of interest. We derive\nconditional influence functions for functionals of conditional means and other\nfeatures of the conditional distribution of an outcome variable. We show how\nthese can be used for locally linear estimation of conditional objects of\ninterest. We give rate conditions for first step machine learners to have no\neffect on asymptotic distributions of locally linear estimators. We also give a\ngeneral construction of Neyman orthogonal estimating equations for conditional\nobjects of interest.",
      "generated_abstract": "We propose a new method to identify conditional influence functions. The\nmethod uses a data-driven approach to identify conditional influence functions,\nwhich are functions that define a link between two random variables given\nanother random variable. These functions can be used to estimate the\nconditional distribution of the second random variable given the first, and\nthese conditional distribution estimates can be used to estimate the conditional\ndistribution of the first random variable given the second. We show that the\nconditional distribution estimates can be obtained through a maximum likelihood\nestimator. We also provide a computationally efficient approach to identify\nconditional influence functions using the maximum likelihood estimator. We\ndemonstrate the method's effectiveness through simulation studies and an\napplication to the study of student motivation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2558139534883721,
          "p": 0.3548387096774194,
          "f": 0.29729729242878017
        },
        "rouge-2": {
          "r": 0.07801418439716312,
          "p": 0.11956521739130435,
          "f": 0.0944205960795007
        },
        "rouge-l": {
          "r": 0.2441860465116279,
          "p": 0.3387096774193548,
          "f": 0.2837837789152667
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.01910v1",
      "true_abstract": "The development of therapeutic antibodies heavily relies on accurate\npredictions of how antigens will interact with antibodies. Existing\ncomputational methods in antibody design often overlook crucial conformational\nchanges that antigens undergo during the binding process, significantly\nimpacting the reliability of the resulting antibodies. To bridge this gap, we\nintroduce dyAb, a flexible framework that incorporates AlphaFold2-driven\npredictions to model pre-binding antigen structures and specifically addresses\nthe dynamic nature of antigen conformation changes. Our dyAb model leverages a\nunique combination of coarse-grained interface alignment and fine-grained flow\nmatching techniques to simulate the interaction dynamics and structural\nevolution of the antigen-antibody complex, providing a realistic representation\nof the binding process. Extensive experiments show that dyAb significantly\noutperforms existing models in antibody design involving changing antigen\nconformations. These results highlight dyAb's potential to streamline the\ndesign process for therapeutic antibodies, promising more efficient development\ncycles and improved outcomes in clinical applications.",
      "generated_abstract": "engineering has emerged as a powerful tool for drug discovery,\nrevolutionizing the treatment of a range of diseases. However, the\nconstrained nature of antibody design restricts the ability to adapt\nresponse to the evolving nature of disease. We propose a novel approach to\naddress this limitation by designing antibodies with the ability to flexibly\nmatch their binding sites to the evolving target antigen. To this end, we\nintroduce dyAb, a method that uses the AlphaFold-driven pre-binding affinity\npredictions to dynamically match the antibody's binding sites to evolving\ntargets. Our approach leverages the ability of AlphaFold to predict binding\nsites across a wide range of protein-protein interactions, enabling the\nflexibility to dynamically adapt to changing targets. We demonstrate the\neffectiveness of dyAb in designing antibodies that adapt",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2336448598130841,
          "p": 0.32894736842105265,
          "f": 0.273224038859327
        },
        "rouge-2": {
          "r": 0.04225352112676056,
          "p": 0.05309734513274336,
          "f": 0.04705881859407972
        },
        "rouge-l": {
          "r": 0.22429906542056074,
          "p": 0.3157894736842105,
          "f": 0.2622950771106931
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2410.00158v1",
      "true_abstract": "Systemic risk is receiving increasing attention in the insurance industry, as\nthese risks can have severe impacts on the entire financial system. In this\npaper, we propose a multi-dimensional L/'{e}vy process-based renewal risk model\nwith heterogeneous insurance claims, where every dimension indicates a business\nline of an insurer. We use the systemic expected shortfall (SES) and marginal\nexpected shortfall (MES) defined with a Value-at-Risk (VaR) target level as the\nmeasurement of systemic risks. Assuming that all the claim sizes are pairwise\nasymptotically independent (PAI), we derive asymptotic formulas for the tail\nprobabilities of discounted aggregate claims and total loss, which holds\nuniformly for all time horizons. We further obtain the asymptotics of the above\nsystemic risk measures. The main technical issues involve the treatment of\nuniform convergence in the dynamic time setting. Finally, we conduct a Monte\nCarlo numerical study and verify that our asymptotics are accurate and\nconvenient in computation.",
      "generated_abstract": "r develops asymptotic expansions for systemic risk in a renewal model\nwith multiple business lines and heterogeneous claims. The model incorporates\na renewal component for the number of business lines, and a renewal component\nfor the number of claims per business line. The renewal components are modeled\nas renewal-type exponential random variables. A renewal component for the number\nof business lines and a renewal component for the number of claims per business\nline are subject to a common deterministic drift. The model is constructed\nthrough a generalization of the renewal process with an arbitrary\nnon-deterministic drift. The asymptotic systemic risk expansions are obtained\nfor a class of renewal processes with a non-deterministic drift. The results\nprovide new insights into the impact of the non-deterministic drift on the\nsystemic risk of the model. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1981981981981982,
          "p": 0.3728813559322034,
          "f": 0.2588235248795848
        },
        "rouge-2": {
          "r": 0.05405405405405406,
          "p": 0.08247422680412371,
          "f": 0.06530611766563968
        },
        "rouge-l": {
          "r": 0.18018018018018017,
          "p": 0.3389830508474576,
          "f": 0.23529411311487897
        }
      }
    },
    {
      "paper_id": "cs.AI.q-bio/NC/2502.18725v1",
      "true_abstract": "Traditional psychological experiments utilizing naturalistic stimuli face\nchallenges in manual annotation and ecological validity. To address this, we\nintroduce a novel paradigm leveraging multimodal large language models (LLMs)\nas proxies to extract rich semantic information from naturalistic images\nthrough a Visual Question Answering (VQA) strategy for analyzing human visual\nsemantic representation. LLM-derived representations successfully predict\nestablished neural activity patterns measured by fMRI (e.g., faces, buildings),\nvalidating its feasibility and revealing hierarchical semantic organization\nacross cortical regions. A brain semantic network constructed from LLM-derived\nrepresentations identifies meaningful clusters reflecting functional and\ncontextual associations. This innovative methodology offers a powerful solution\nfor investigating brain semantic organization with naturalistic stimuli,\novercoming limitations of traditional annotation methods and paving the way for\nmore ecologically valid explorations of human cognition.",
      "generated_abstract": "y investigates the use of large language models (LLMs) as proxies to\nmodel brain semantic representation. We compare the performance of two\ndifferent LLMs, namely the OpenAI GPT-3.5 model and the SIREN model, in\nidentifying semantic concepts of the brain. We focus on the semantic concepts of\nthe human brain, specifically, the visual cortex, auditory cortex, and motor\ncortex. The results show that the SIREN model outperforms the GPT-3.5 model in\nidentifying the semantic concepts of the brain. We also explore the\ninterpretability of the semantic concepts of the brain, and we find that the\nGPT-3.5 model outperforms the SIREN model in terms of semantic concepts\ninterpretability. These results suggest that LLMs can be used as proxies to\nmodel brain semantic representation, which could be useful",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16037735849056603,
          "p": 0.2698412698412698,
          "f": 0.20118342727635596
        },
        "rouge-2": {
          "r": 0.06504065040650407,
          "p": 0.08421052631578947,
          "f": 0.073394490495329
        },
        "rouge-l": {
          "r": 0.1509433962264151,
          "p": 0.25396825396825395,
          "f": 0.18934910774972877
        }
      }
    },
    {
      "paper_id": "cs.NI.eess/SY/2503.04637v1",
      "true_abstract": "Sensing is emerging as a vital future service in next-generation wireless\nnetworks, enabling applications such as object localization and activity\nrecognition. The IEEE 802.11bf standard extends Wi-Fi capabilities to\nincorporate these sensing functionalities. However, coexistence with legacy\nWi-Fi in densely populated networks poses challenges, as contention for\nchannels can impair both sensing and communication quality. This paper develops\nan analytical framework and a system-level simulation in ns-3 to evaluate the\ncoexistence of IEEE 802.11bf and legacy 802.11ax in terms of sensing delay and\ncommunication throughput. Forthis purpose, we have developed a dedicated ns-3\nmodule forIEEE 802.11bf, which is made publicly available as open-source. We\nprovide the first coexistence analysis between IEEE 802.11bfand IEEE 802.11ax,\nsupported by link-level simulation in ns-3to assess the impact on sensing delay\nand network performance. Key parameters, including sensing intervals, access\ncategories, network densities, and antenna configurations, are systematically\nanalyzed to understand their influence on the sensing delay and aggregated\nnetwork throughput. The evaluation is further extended to a realistic indoor\noffice environment modeled after the 3GPP TR 38.901 standard. Our findings\nreveal key trade-offs between sensing intervals and throughput and the need for\nbalanced sensing parameters to ensure effective coexistence in Wi-Fi networks.",
      "generated_abstract": "r presents the results of a systematic coexistence analysis of\nthe IEEE 802.11bf and IEEE 802.11ax standards. The analysis focuses on\nmeasurements from a 60-unit testbed at the University of Washington, where\nmultiple IEEE 802.11ax devices are deployed. The devices are equipped with\nmulti-antenna configurations that include both 802.11ax and 802.11ac devices.\nThe coexistence scenario is designed to mimic the scenario of multiple\nIEEE 802.11ac devices operating in the same frequency band, which is expected\nto become more common in future networks. The analysis focuses on both\nsingle-user and multi-user scenarios, considering the impact of\ninter-frequency coexistence. The results demonstrate that the coexistence\ncapability",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16176470588235295,
          "p": 0.3492063492063492,
          "f": 0.2211055233110276
        },
        "rouge-2": {
          "r": 0.041237113402061855,
          "p": 0.08247422680412371,
          "f": 0.054982813424971716
        },
        "rouge-l": {
          "r": 0.14705882352941177,
          "p": 0.31746031746031744,
          "f": 0.20100502079846477
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2502.12219v1",
      "true_abstract": "Optimizing chemical properties is a challenging task due to the vastness and\ncomplexity of chemical space. Here, we present a generative energy-based\narchitecture for implicit chemical property optimization, designed to\nefficiently generate molecules that satisfy target properties without explicit\nconditional generation. We use Graph Energy Based Models and a training\napproach that does not require property labels. We validated our approach on\nwell-established chemical benchmarks, showing superior results to\nstate-of-the-art methods and demonstrating robustness and efficiency towards de\nnovo drug design.",
      "generated_abstract": "ed models have gained popularity for their ability to capture\ncomplex biological relationships and their ability to generate molecular\nproperties in a scalable manner. These models, however, often face challenges\nsuch as high computational costs, memory constraints, and complex model\nstructures. To address these limitations, we propose a novel approach that\ncombines a Graph Convolutional Network (GCN) with an energy-based model\n(Energy-GCN) to address these challenges. We evaluate the proposed model on\ndifferent datasets, and find that the proposed model is more efficient and\neffective compared to state-of-the-art GCN-based models. Furthermore, we show\nthat the proposed model can be trained and optimized using a few hundred\nmilliseconds of CPU time, whereas the GCN-based models require tens of\nhours. This approach not only enhances the efficiency of molecular property\noptimization but also provides",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3181818181818182,
          "p": 0.21875,
          "f": 0.2592592544307271
        },
        "rouge-2": {
          "r": 0.025,
          "p": 0.01639344262295082,
          "f": 0.019801975414176238
        },
        "rouge-l": {
          "r": 0.2727272727272727,
          "p": 0.1875,
          "f": 0.22222221739369008
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.06092v1",
      "true_abstract": "The popular choice of using a $direct$ forecasting scheme implies that the\nindividual predictions do not contain information on cross-horizon dependence.\nHowever, this dependence is needed if the forecaster has to construct, based on\n$direct$ density forecasts, predictive objects that are functions of several\nhorizons ($e.g.$ when constructing annual-average growth rates from\nquarter-on-quarter growth rates). To address this issue we propose to use\ncopulas to combine the individual $h$-step-ahead predictive distributions into\na joint predictive distribution. Our method is particularly appealing to\npractitioners for whom changing the $direct$ forecasting specification is too\ncostly. In a Monte Carlo study, we demonstrate that our approach leads to a\nbetter approximation of the true density than an approach that ignores the\npotential dependence. We show the superior performance of our method in several\nempirical examples, where we construct (i) quarterly forecasts using\nmonth-on-month $direct$ forecasts, (ii) annual-average forecasts using monthly\nyear-on-year $direct$ forecasts, and (iii) annual-average forecasts using\nquarter-on-quarter $direct$ forecasts.",
      "generated_abstract": "r introduces two novel density forecast transformations for\nestimating a density function using a finite set of forecast samples. The\ntransformations are based on a density-fitting approach and are designed to\nmaximize the likelihood of the density function being the true underlying\nfunction. These transformations are based on the idea that a density function\nis a probability density function, and the probability density function is\nobtained by fitting the density function to a finite set of samples. We show\nthat these density-fitting transformations are equivalent to the\nhypothesis-testing based density-fitting transformations proposed by\nHsu and Zhang (2014) and Hsu and Zhang (2015). We also provide an analysis of\nthe asymptotic properties of these density-fitting transformations. These\ntransformations are motivated by the idea that the density function is\napproximated by a finite set of samples. Our",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18691588785046728,
          "p": 0.3389830508474576,
          "f": 0.24096385083974456
        },
        "rouge-2": {
          "r": 0.0457516339869281,
          "p": 0.06862745098039216,
          "f": 0.054901955984314146
        },
        "rouge-l": {
          "r": 0.17757009345794392,
          "p": 0.3220338983050847,
          "f": 0.22891565806866027
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2411.13965v1",
      "true_abstract": "Universal power laws have been scrutinised in physics and beyond, and a\nlong-standing debate exists in econophysics regarding the strict universality\nof the nonlinear price impact, commonly referred to as the square-root law\n(SRL). The SRL posits that the average price impact $I$ follows a power law\nwith respect to transaction volume $Q$, such that $I(Q) \\propto Q^{\\delta}$\nwith $\\delta \\approx 1/2$. Some researchers argue that the exponent $\\delta$\nshould be system-specific, without universality. Conversely, others contend\nthat $\\delta$ should be exactly $1/2$ for all stocks across all countries,\nimplying universality. However, resolving this debate requires high-precision\nmeasurements of $\\delta$ with errors of around $0.1$ across hundreds of stocks,\nwhich has been extremely challenging due to the scarcity of large microscopic\ndatasets -- those that enable tracking the trading behaviour of all individual\naccounts. Here we conclusively support the universality hypothesis of the SRL\nby a complete survey of all trading accounts for all liquid stocks on the Tokyo\nStock Exchange (TSE) over eight years. Using this comprehensive microscopic\ndataset, we show that the exponent $\\delta$ is equal to $1/2$ within\nstatistical errors at both the individual stock level and the individual trader\nlevel. Additionally, we rejected two prominent models supporting the\nnonuniversality hypothesis: the Gabaix-Gopikrishnan-Plerou-Stanley and the\nFarmer-Gerig-Lillo-Waelbroeck models. Our work provides exceptionally\nhigh-precision evidence for the universality hypothesis in social science and\ncould prove useful in evaluating the price impact by large investors -- an\nimportant topic even among practitioners.",
      "generated_abstract": "e-root price impact law (SPIL) is widely believed to have universal\nscalings, and its theoretical foundation is based on the law of large numbers.\nHowever, there are many cases in which SPIL fails to obey the law of large\nnumbers. In this paper, we systematically investigate the SPIL law by analyzing\nthe SPIL data from 2008 to 2023. The analysis reveals that SPIL violates the\nlaw of large numbers in the following three cases. First, the SPIL does not\nfollow the law of large numbers in the case of large market liquidity and\nsmall trading volume. Second, the SPIL does not follow the law of large numbers\nin the case of large trading volume and small market liquidity. Third, the SPIL\ndoes not follow the law of large numbers in the case of small trading volume\nand large market",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1337579617834395,
          "p": 0.3333333333333333,
          "f": 0.19090908682190091
        },
        "rouge-2": {
          "r": 0.013157894736842105,
          "p": 0.034482758620689655,
          "f": 0.019047615049433943
        },
        "rouge-l": {
          "r": 0.12738853503184713,
          "p": 0.31746031746031744,
          "f": 0.18181817773099185
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.07369v1",
      "true_abstract": "Skeletonization extracts thin representations from images that compactly\nencode their geometry and topology. These representations have become an\nimportant topological prior for preserving connectivity in curvilinear\nstructures, aiding medical tasks like vessel segmentation. Existing compatible\nskeletonization algorithms face significant trade-offs: morphology-based\napproaches are computationally efficient but prone to frequent breakages, while\ntopology-preserving methods require substantial computational resources.\n  We propose a novel framework for training iterative skeletonization\nalgorithms with a learnable component. The framework leverages synthetic data,\ntask-specific augmentation, and a model distillation strategy to learn compact\nneural networks that produce thin, connected skeletons with a fully\ndifferentiable iterative algorithm.\n  Our method demonstrates a 100 times speedup over topology-constrained\nalgorithms while maintaining high accuracy and generalizing effectively to new\ndomains without fine-tuning. Benchmarking and downstream validation in 2D and\n3D tasks demonstrate its computational efficiency and real-world applicability",
      "generated_abstract": "tonization problem is a fundamental challenge in computer vision,\nrequiring rapid, accurate, and efficient identification of key body parts.\nAlthough recent advances in deep learning have demonstrated state-of-the-art\nperformance in skeletonization, their high computational cost remains a\nsignificant challenge. In this paper, we introduce Skelite, a compact neural\nnetwork that achieves state-of-the-art performance on the DIV2K dataset while\nachieving faster inference times than the best previous model. Our method\nemploys a multi-stage architecture to reduce the number of parameters while\nstill achieving comparable performance. Specifically, our approach consists of\nthree stages: an input pre-processing stage, a feature extractor stage, and an\noutput refinement stage. The input pre-processing stage employs a simple\ntwo-channel extension of the U-Net architecture, which is then followed by a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17699115044247787,
          "p": 0.21052631578947367,
          "f": 0.19230768734513695
        },
        "rouge-2": {
          "r": 0.014925373134328358,
          "p": 0.01680672268907563,
          "f": 0.01581027169741912
        },
        "rouge-l": {
          "r": 0.1504424778761062,
          "p": 0.17894736842105263,
          "f": 0.16346153349898315
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/MS/2503.06010v1",
      "true_abstract": "In this paper, we propose the InfoFusion Controller, an advanced path\nplanning algorithm that integrates both global and local planning strategies to\nenhance autonomous driving in complex urban environments. The global planner\nutilizes the informed Theta-Rapidly-exploring Random Tree Star (Informed-TRRT*)\nalgorithm to generate an optimal reference path, while the local planner\ncombines Model Predictive Control (MPC) and Pure Pursuit algorithms. Mutual\nInformation (MI) is employed to fuse the outputs of the MPC and Pure Pursuit\ncontrollers, effectively balancing their strengths and compensating for their\nweaknesses. The proposed method addresses the challenges of navigating in\ndynamic environments with unpredictable obstacles by reducing uncertainty in\nlocal path planning and improving dynamic obstacle avoidance capabilities.\nExperimental results demonstrate that the InfoFusion Controller outperforms\ntraditional methods in terms of safety, stability, and efficiency across\nvarious scenarios, including complex maps generated using SLAM techniques.\n  The code for the InfoFusion Controller is available at https:\n//github.com/DrawingProcess/InfoFusionController.",
      "generated_abstract": "r presents a novel approach for enhancing path planning in\nRapid-Response-Trajectory-Refinement (TRRT) by incorporating a novel\ninformation-based controller that is informed by the mutual information\nbetween the pure pursuit (PP) trajectory and the reference motion. The proposed\ncontroller is designed to address the limitations of current TRRT methods,\nparticularly in terms of robustness and generalization, by leveraging the\ninformation from the PP trajectory and the reference motion to improve the\nplanning process. Specifically, we first design a PP-MPC hybrid controller to\ngenerate a reference motion based on the PP trajectory, which is then used to\ngenerate the PP trajectory. This hybrid controller is then incorporated into\nthe TRRT framework, enhancing the path planning process by leveraging the\ninformation from the PP trajectory and the reference motion. The proposed",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1651376146788991,
          "p": 0.2727272727272727,
          "f": 0.20571428101616338
        },
        "rouge-2": {
          "r": 0.034722222222222224,
          "p": 0.050505050505050504,
          "f": 0.041152258545953925
        },
        "rouge-l": {
          "r": 0.1651376146788991,
          "p": 0.2727272727272727,
          "f": 0.20571428101616338
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.08179v3",
      "true_abstract": "Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks.",
      "generated_abstract": "vances in language models have paved the way for large-scale\nprot(ect)ic understanding, with a focus on protein structure and function.\nHowever, the limited availability of large prot(ect)ic datasets limits their\napplicability in practical applications. To address this gap, we introduce\nProtTeX, a novel framework for protein structure-in-context reasoning and\nediting, leveraging Large Language Models (LLMs) to model the relationships\nbetween protein sequence and structure. We propose a novel method for\nmodeling protein-protein interactions (PPIs) using a pairwise LLM-based\nmodel. Our method allows for contextualized representations of PPIs,\nenabling effective reasoning about protein interactions. Additionally, we\nintroduce the ProtXML framework, which leverages the LLMs' ability to generate\nstructural templates, enabling the creation of customizable editable",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2119205298013245,
          "p": 0.38095238095238093,
          "f": 0.2723404209383432
        },
        "rouge-2": {
          "r": 0.03365384615384615,
          "p": 0.06306306306306306,
          "f": 0.04388714279773241
        },
        "rouge-l": {
          "r": 0.18543046357615894,
          "p": 0.3333333333333333,
          "f": 0.23829786774685383
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PR/2409.04903v2",
      "true_abstract": "In this paper, we propose a semi-analytical approach to pricing options on\nSOFR futures where the underlying SOFR follows a time-dependent CEV model. By\ndefinition, these options change their type at the beginning of the reference\nperiod: before this time, this is an American option written on a SOFR forward\nprice as an underlying, and after this point, this is an arithmetic Asian\noption with an American style exercise written on the daily SOFR rates. We\ndevelop a new version of the GIT method and solve both problems\nsemi-analytically, obtaining the option price, the exercise boundary, and the\noption Greeks. This work is intended to address the concern that the transfer\nfrom LIBOR to SOFR has resulted in a situation in which the options of the key\nmoney market (i.e., futures on the reference rate) are options without any\npricing model available. Therefore, the trading in options on 3M SOFR futures\ncurrently ends before their reference quarter starts, to eliminate the final\nmetamorphosis into exotic options.",
      "generated_abstract": "r proposes a semi-analytical method for pricing options written on\nS&P 500 (SP500) futures contracts. The method is based on the integration of a\nBayesian neural network (BNN) with a semiparametric model, with the latter\nallowing for the inclusion of a volatility smoothing term. In the present\nstudy, we focus on SP500 future options that are written on the SoftBank\nOptionable (SOFR) futures contract. We demonstrate the accuracy of the\nsemi-analytical pricing method by comparing its results with those obtained\nthrough the use of a full-analytical numerical scheme. We also present the\nresults of a sensitivity analysis, in which we vary the number of training\npoints and the number of predictors in the BNN, as well as the volatility\nsmoothing",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20192307692307693,
          "p": 0.28,
          "f": 0.2346368666396181
        },
        "rouge-2": {
          "r": 0.04487179487179487,
          "p": 0.06306306306306306,
          "f": 0.052434452070866916
        },
        "rouge-l": {
          "r": 0.17307692307692307,
          "p": 0.24,
          "f": 0.20111731356699242
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.07573v1",
      "true_abstract": "We prove an inversion formula for the exterior $k$-plane transform. As a\nconsequence, we show that if $m < k$ then an $m$-current in $\\mathbf R^n$ can\nbe reconstructed from its projections onto $\\mathbf R^k$, which proves a\nconjecture of Solomon.",
      "generated_abstract": "We prove a non-commutative version of the Gromov-Lax-Phillips inequality for\nreconstructing a current from its projection. We obtain a similar result for\nnon-commutative $L^2$ currents by adapting the arguments in [BG18",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23684210526315788,
          "p": 0.36,
          "f": 0.28571428092718576
        },
        "rouge-2": {
          "r": 0.05,
          "p": 0.06666666666666667,
          "f": 0.05714285224489839
        },
        "rouge-l": {
          "r": 0.21052631578947367,
          "p": 0.32,
          "f": 0.25396824918115396
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/LG/2503.10594v1",
      "true_abstract": "The structural analogies of ResNets and Multigrid (MG) methods such as common\nbuilding blocks like convolutions and poolings where already pointed out by He\net al.\\ in 2016. Multigrid methods are used in the context of scientific\ncomputing for solving large sparse linear systems arising from partial\ndifferential equations. MG methods particularly rely on two main concepts:\nsmoothing and residual restriction / coarsening. Exploiting these analogies, He\nand Xu developed the MgNet framework, which integrates MG schemes into the\ndesign of ResNets. In this work, we introduce a novel neural network building\nblock inspired by polynomial smoothers from MG theory. Our polynomial block\nfrom an MG perspective naturally extends the MgNet framework to Poly-Mgnet and\nat the same time reduces the number of weights in MgNet. We present a\ncomprehensive study of our polynomial block, analyzing the choice of initial\ncoefficients, the polynomial degree, the placement of activation functions, as\nwell as of batch normalizations. Our results demonstrate that constructing\n(quadratic) polynomial building blocks based on real and imaginary polynomial\nroots enhances Poly-MgNet's capacity in terms of accuracy. Furthermore, our\napproach achieves an improved trade-off of model accuracy and number of weights\ncompared to ResNet as well as compared to specific configurations of MgNet.",
      "generated_abstract": "-based neural networks have been shown to exhibit remarkable\npolynomial complexity properties, offering a promising framework for\nlarge-scale neural architecture search. However, existing approaches face\nsignificant challenges in scaling to large networks. In this work, we propose\na novel approach for Multigrid-Inspired ResNets, Poly-MgNet, which exploits\npolynomial building blocks to enhance both the expressiveness and scalability\nof neural architectures. Our approach introduces a new technique, Poly-Mg\nResNets, which is designed to optimize the structure of the Multigrid-Inspired\nResNet. We further introduce Poly-MgNet, a novel architecture that leverages\nPoly-Mg ResNets to enhance both expressiveness and scalability, and provide\ntheoretical analysis. Additionally, we propose a new training method for\nPoly-MgNet, which",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18840579710144928,
          "p": 0.35135135135135137,
          "f": 0.24528301432360278
        },
        "rouge-2": {
          "r": 0.030612244897959183,
          "p": 0.06,
          "f": 0.04054053606647237
        },
        "rouge-l": {
          "r": 0.18115942028985507,
          "p": 0.33783783783783783,
          "f": 0.2358490520594518
        }
      }
    },
    {
      "paper_id": "physics.ao-ph.physics/ao-ph/2503.07466v1",
      "true_abstract": "Supercell thunderstorms are the most hazardous thunderstorm category and\nparticularly impactful to society. Their monitoring is challenging and often\nconfined to the radar networks of single countries. By exploiting\nkilometer-scale climate simulations, a first-of-its-kind characterization of\nsupercell occurrence in Europe is derived for the current and a warmer climate.\nDespite previous notions of supercells being uncommon in Europe, the model\nshows ~700 supercells per convective season. Occurrence peaks are co-located\nwith complex topography e.g. the Alps. The absolute frequency maximum lies\nalong the southern Alps with minima over the oceans and flat areas. Contrasting\na current-climate simulation with a pseudo-global-warming +3$^\\circ$C global\nwarming scenario, the future climate simulation shows an average increase of\nsupercell occurrence by 11 %. However, there is a spatial dipole of change with\nstrong increases in supercell frequencies in central and eastern Europe and a\ndecrease in frequency over the Iberian Peninsula and southwestern France.",
      "generated_abstract": "We show that the current supercell thunderstorm hazard in Europe is\nunderestimated by 40% by conventional methods. By improving the modeling\nframework, we are able to increase the hazard by 110%. This work suggests\nthat supercell thunderstorms are an underestimated current hazard, and that\nimproved hazard estimation can reduce the threat to humans and property.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14423076923076922,
          "p": 0.375,
          "f": 0.20833332932098772
        },
        "rouge-2": {
          "r": 0.027586206896551724,
          "p": 0.07407407407407407,
          "f": 0.04020100107068042
        },
        "rouge-l": {
          "r": 0.1346153846153846,
          "p": 0.35,
          "f": 0.19444444043209885
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.08028v1",
      "true_abstract": "Denoising diffusions provide a general strategy to sample from a probability\ndistribution $\\mu$ in $\\mathbb{R}^d$ by constructing a stochastic process\n$(\\hat{\\boldsymbol x}_t:t\\ge 0)$ in ${\\mathbb R}^d$ such that the distribution\nof $\\hat{\\boldsymbol x}_T$ at large times $T$ approximates $\\mu$. The drift\n${\\boldsymbol m}:{\\mathbb R}^d\\times{\\mathbb R}\\to{\\mathbb R}^d$ of this\ndiffusion process is learned from data (samples from $\\mu$) by minimizing the\nso-called score-matching objective. In order for the generating process to be\nefficient, it must be possible to evaluate (an approximation of) ${\\boldsymbol\nm}({\\boldsymbol y},t)$ in polynomial time.\n  Is every probability distribution $\\mu$, for which sampling is tractable,\nalso amenable to sampling via diffusions? We provide evidence to the contrary\nby constructing a probability distribution $\\mu$ for which sampling is easy,\nbut the drift of the diffusion process is intractable -- under a popular\nconjecture on information-computation gaps in statistical estimation. We\nfurther show that any polynomial-time computable drift can be modified in a way\nthat changes minimally the score matching objective and yet results in\nincorrect sampling.",
      "generated_abstract": "Denoising diffusion models have emerged as a powerful tool for image\nreconstruction from noisy input. However, existing implementations often\nexhibit high computational costs, particularly for large-scale scenarios. In\nthis work, we propose a new algorithm that addresses these issues, specifically\nfor the case of high-resolution images. Our approach utilizes the\nspectral-norm regularization technique, which is computationally more efficient\nthan previous approaches. Furthermore, we show that our algorithm does not\nrequire the explicit calculation of the Jacobian matrix. This enables us to\nuse the fast Fourier transform (FFT) algorithm for matrix multiplication and\nthereby significantly reduces the computational costs. Our experiments on the\nMS-COCO dataset demonstrate that our approach outperforms existing methods in\nterms of reconstruction quality and computational efficiency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1559633027522936,
          "p": 0.1827956989247312,
          "f": 0.16831682671453796
        },
        "rouge-2": {
          "r": 0.01935483870967742,
          "p": 0.02564102564102564,
          "f": 0.02205881862700152
        },
        "rouge-l": {
          "r": 0.1559633027522936,
          "p": 0.1827956989247312,
          "f": 0.16831682671453796
        }
      }
    },
    {
      "paper_id": "cs.IR.cs/IR/2503.09223v1",
      "true_abstract": "Query and product relevance prediction is a critical component for ensuring a\nsmooth user experience in e-commerce search. Traditional studies mainly focus\non BERT-based models to assess the semantic relevance between queries and\nproducts. However, the discriminative paradigm and limited knowledge capacity\nof these approaches restrict their ability to comprehend the relevance between\nqueries and products fully. With the rapid advancement of Large Language Models\n(LLMs), recent research has begun to explore their application to industrial\nsearch systems, as LLMs provide extensive world knowledge and flexible\noptimization for reasoning processes. Nonetheless, directly leveraging LLMs for\nrelevance prediction tasks introduces new challenges, including a high demand\nfor data quality, the necessity for meticulous optimization of reasoning\nprocesses, and an optimistic bias that can result in over-recall. To overcome\nthe above problems, this paper proposes a novel framework called the LLM-based\nRElevance Framework (LREF) aimed at enhancing e-commerce search relevance. The\nframework comprises three main stages: supervised fine-tuning (SFT) with Data\nSelection, Multiple Chain of Thought (Multi-CoT) tuning, and Direct Preference\nOptimization (DPO) for de-biasing. We evaluate the performance of the framework\nthrough a series of offline experiments on large-scale real-world datasets, as\nwell as online A/B testing. The results indicate significant improvements in\nboth offline and online metrics. Ultimately, the model was deployed in a\nwell-known e-commerce application, yielding substantial commercial benefits.",
      "generated_abstract": "e platforms are increasingly challenged by the rise of\nincreasingly sophisticated, large language models (LLMs) that can process\nlarge volumes of textual data, effectively serving as the \"intelligent\nassistants\" for consumers. This paper explores the potential of leveraging\nLLMs to assist in the search process by proposing a novel framework for\nrelevance feedback in e-commerce search. The framework leverages LLMs to\ngenerate contextually relevant search results based on users' search queries.\nThe framework includes an architecture designed to capture the complexity of\nrelevance feedback, integrating two distinct components: an LLM-based relevance\ngeneration model and a user-based relevance feedback model. The proposed\nframework was tested on a real-world dataset from a leading e-commerce\nplatform and showed promising results in terms of both search accuracy and\nrelevance feedback",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18238993710691823,
          "p": 0.3493975903614458,
          "f": 0.23966941698073907
        },
        "rouge-2": {
          "r": 0.037383177570093455,
          "p": 0.06611570247933884,
          "f": 0.04776118941519314
        },
        "rouge-l": {
          "r": 0.18238993710691823,
          "p": 0.3493975903614458,
          "f": 0.23966941698073907
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2502.10512v1",
      "true_abstract": "Blockchain technology has revolutionized financial markets by enabling\ndecentralized exchanges (DEXs) that operate without intermediaries. Uniswap V2,\na leading DEX, facilitates the rapid creation and trading of new tokens,\noffering high return potential but exposing investors to significant risks. In\nthis work, we analyze the financial impact of newly created tokens, assessing\ntheir market dynamics, profitability and liquidity manipulations. Our findings\nreveal that a significant portion of market liquidity is trapped in honeypots,\nreducing market efficiency and misleading investors. Applying a simple\nbuy-and-hold strategy, we are able to uncover some major risks associated with\ninvesting in newly created tokens, including the widespread presence of rug\npulls and sandwich attacks. We extract the optimal sandwich amount, revealing\nthat their proliferation in new tokens stems from higher profitability in\nlow-liquidity pools. Furthermore, we analyze the fundamental differences\nbetween token price evolution in swap time and physical time. Using clustering\ntechniques, we highlight these differences and identify typical patterns of\nhoneypot and sellable tokens. Our study provides insights into the risks and\nfinancial dynamics of decentralized markets and their challenges for investors.",
      "generated_abstract": "UniswapV2 is the most popular decentralized exchange (DEX) that allows\ncoins to be swapped through smart contracts. The platform allows users to\nexchange tokens, and, as a result, a massive amount of coins has been\ncreated. The DeFi space is driven by this increase in the number of coins,\nleading to a proliferation of new tokens. In this paper, we will explore the\nevolution of the number of coins on UniswapV2, and we will discuss the\nimplications of this proliferation on the market. We will also discuss the\nimplications of this proliferation on the security of the platform.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18699186991869918,
          "p": 0.3898305084745763,
          "f": 0.25274724836553564
        },
        "rouge-2": {
          "r": 0.017241379310344827,
          "p": 0.03529411764705882,
          "f": 0.023166018756429634
        },
        "rouge-l": {
          "r": 0.17073170731707318,
          "p": 0.3559322033898305,
          "f": 0.23076922638751368
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.08653v1",
      "true_abstract": "National forest inventory (NFI) data are often costly to collect, which\ninhibits efforts to estimate parameters of interest for small spatial,\ntemporal, or biophysical domains. Traditionally, design-based estimators are\nused to estimate status of forest parameters of interest, but are unreliable\nfor small areas where data are sparse. Additionally, design-based estimates\nconstructed directly from the survey data are often unavailable when sample\nsizes are especially small. Traditional model-based small area estimation\napproaches, such as the Fay-Herriot (FH) model, rely on these direct estimates\nfor inference; hence, missing direct estimates preclude the use of such\napproaches. Here, we detail a Bayesian spatio-temporal small area estimation\nmodel that efficiently leverages sparse NFI data to estimate status and trends\nfor forest parameters. The proposed model bypasses the use of direct estimates\nand instead uses plot-level NFI measurements along with auxiliary data\nincluding remotely sensed tree canopy cover. We produce forest carbon estimates\nfrom the United States NFI over 14 years across the contiguous US (CONUS) and\nconduct a simulation study to assess our proposed model's accuracy, precision,\nand bias, compared to that of a design-based estimator. The proposed model\nprovides improved precision and accuracy over traditional estimation methods,\nand provides useful insights into county-level forest carbon dynamics across\nthe CONUS.",
      "generated_abstract": "e a novel method to estimate forest carbon density status and\ntrends for small areas. The proposed method is based on combining national\nforest inventory data with machine learning techniques. The proposed method\nuses the Forest Carbon Density Status (FCDS) indicator to measure the density\nof forest canopy cover, and the Forest Carbon Density Trend (FCDT) indicator to\nmeasure the trend of forest carbon density. We compare our method with\nestimates from national forest inventory data and the FCDS indicator alone. Our\nmethod uses 17 years of national forest inventory data to estimate FCDS and\nFCDT for small areas (500 ha or smaller) in the United States. We compare our\nmethod with national forest inventory data and the FCDS indicator alone for\nestimating FCDS and FCDT for small areas (500 ha or smaller) in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22137404580152673,
          "p": 0.46774193548387094,
          "f": 0.3005181303541035
        },
        "rouge-2": {
          "r": 0.07526881720430108,
          "p": 0.15730337078651685,
          "f": 0.10181817744026467
        },
        "rouge-l": {
          "r": 0.20610687022900764,
          "p": 0.43548387096774194,
          "f": 0.27979274175306723
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.08930v1",
      "true_abstract": "We consider the problem of optimizing neural implicit surfaces for 3D\nreconstruction using acoustic images collected with drifting sensor poses. The\naccuracy of current state-of-the-art 3D acoustic modeling algorithms is highly\ndependent on accurate pose estimation; small errors in sensor pose can lead to\nsevere reconstruction artifacts. In this paper, we propose an algorithm that\njointly optimizes the neural scene representation and sonar poses. Our\nalgorithm does so by parameterizing the 6DoF poses as learnable parameters and\nbackpropagating gradients through the neural renderer and implicit\nrepresentation. We validated our algorithm on both real and simulated datasets.\nIt produces high-fidelity 3D reconstructions even under significant pose drift.",
      "generated_abstract": "r presents a novel acoustic neural 3D reconstruction framework\nthat is robust to pose drift in a free-field environment. Our method\nincorporates a neural network with two branches, each trained to reconstruct\nthe acoustic field in a 3D space and 3D point cloud, respectively. In the first\nbranch, a convolutional neural network (CNN) is used to reconstruct the\nacoustic field from a single 2D acoustic measurement. A positional encoding is\napplied to the input signal, followed by a pointwise convolution and a linear\nregression layer to estimate the 3D sound field. In the second branch, a\nconvolutional neural network is used to reconstruct the 3D point cloud. A\nposition encoding is applied to the input signal, followed by a pointwise\nconvolution and a linear regression layer to estimate the 3D point cloud. We",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2073170731707317,
          "p": 0.265625,
          "f": 0.2328767074047665
        },
        "rouge-2": {
          "r": 0.019230769230769232,
          "p": 0.021505376344086023,
          "f": 0.020304563543509173
        },
        "rouge-l": {
          "r": 0.18292682926829268,
          "p": 0.234375,
          "f": 0.20547944713079389
        }
      }
    },
    {
      "paper_id": "cs.AI.cs/DS/2503.07545v1",
      "true_abstract": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems.",
      "generated_abstract": "guage Models (LLMs) excel at text generation, but have been\ndemonstrated to struggle with queueing, a fundamental problem in artificial\nintelligence, such as scheduling tasks within a queue. This paper investigates\nthe queueing behavior of LLMs and the underlying mathematical models, including\ntheir queue length distributions and waiting time distributions. We propose a\nframework to systematically analyze the queueing behavior of LLMs, which\nincorporates the predictive power of LLMs. By analyzing this framework, we\ndemonstrate that LLMs struggle with queueing when their predictions are\nunreliable or when their predictions are incorrect. Furthermore, we show that\nLLMs struggle with queueing in the presence of unobserved state and in\nnon-stationary environments. To address these challenges, we propose a\ndistributed LLM architecture, a queueing-aware L",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18253968253968253,
          "p": 0.27380952380952384,
          "f": 0.21904761424761912
        },
        "rouge-2": {
          "r": 0.011363636363636364,
          "p": 0.018518518518518517,
          "f": 0.014084502328904572
        },
        "rouge-l": {
          "r": 0.1746031746031746,
          "p": 0.2619047619047619,
          "f": 0.20952380472380963
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.06964v1",
      "true_abstract": "This paper seeks to clarify whether the processing of two types of messages\nthat promote responsible gambling, namely personalized messages and normative\nmessages, varies across gamblers severity habits and rational thinking",
      "generated_abstract": "This paper introduces a computational model of responsible gambling that\nintegrates the neurobiology of decision-making with a theory of normative\ngambling. The model reveals how the brain's reward circuitry and decision-making\nsystems integrate different types of information to make responsible gambling\ncrucial, even if gambling is harmful to the individual. These findings\nunderscore the importance of considering the biological underpinnings of\nresponsible gambling in designing effective interventions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.35714285714285715,
          "p": 0.20833333333333334,
          "f": 0.26315789008310253
        },
        "rouge-2": {
          "r": 0.06666666666666667,
          "p": 0.031746031746031744,
          "f": 0.043010748317725035
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.14583333333333334,
          "f": 0.18421052166205
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/MN/2501.03235v1",
      "true_abstract": "Neural networks based on soft and biological matter constitute an interesting\npotential alternative to traditional implementations based on electric\ncircuits. DNA is a particularly promising system in this context due its\nnatural ability to store information. In recent years, researchers have started\nto construct neural networks that are based on DNA. In this chapter, I provide\na very basic introduction to the concept of DNA neural networks, aiming at an\naudience that is not familiar with biochemistry.",
      "generated_abstract": "n is the fundamental unit of information processing in the brain.\nNeurotransmitters are messenger molecules that carry information between\nneurons. The molecular basis of neurotransmitter synthesis is still not fully\nunderstood. We show that a neuron's response to a neurotransmitter depends on\nits DNA sequence, which is encoded in the protein itself. We use a DNA-based\nneural network to predict a neuron's response to a neurotransmitter,\ncomputing the probability of a given neuron's response. We find that the\nprobability of a neuron's response is strongly dependent on its DNA sequence.\nThis means that the DNA sequence of a neuron is an important determinant of its\nresponse to a neurotransmitter. We also find that a neuron's response is\nsensitive to its environment, such as the concentration of the neuro",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.22058823529411764,
          "f": 0.23437499501953135
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.22058823529411764,
          "f": 0.23437499501953135
        }
      }
    },
    {
      "paper_id": "cs.MA.econ/TH/2502.16719v2",
      "true_abstract": "Recent research on instant runoff voting (IRV) shows that it exhibits a\nstriking combinatorial property in one-dimensional preference spaces: there is\nan \"exclusion zone\" around the median voter such that if a candidate from the\nexclusion zone is on the ballot, then the winner must come from the exclusion\nzone. Thus, in one dimension, IRV cannot elect an extreme candidate as long as\na sufficiently moderate candidate is running. In this work, we examine the\nmathematical structure of exclusion zones as a broad phenomenon in more general\npreference spaces. We prove that with voters uniformly distributed over any\n$d$-dimensional hyperrectangle (for $d > 1$), IRV has no nontrivial exclusion\nzone. However, we also show that IRV exclusion zones are not solely a\none-dimensional phenomenon. For irregular higher-dimensional preference spaces\nwith fewer symmetries than hyperrectangles, IRV can exhibit nontrivial\nexclusion zones. As a further exploration, we study IRV exclusion zones in\ngraph voting, where nodes represent voters who prefer candidates closer to them\nin the graph. Here, we show that IRV exclusion zones present a surprising\ncomputational challenge: even checking whether a given set of positions is an\nIRV exclusion zone is NP-hard. We develop an efficient randomized approximation\nalgorithm for checking and finding exclusion zones. We also report on\ncomputational experiments with exclusion zones in two directions: (i) applying\nour approximation algorithm to a collection of real-world school friendship\nnetworks, we find that about 60% of these networks have probable nontrivial IRV\nexclusion zones; and (ii) performing an exhaustive computer search of small\ngraphs and trees, we also find nontrivial IRV exclusion zones in most graphs.\nWhile our focus is on IRV, the properties of exclusion zones we establish\nprovide a novel method for analyzing voting systems in metric spaces more\ngenerally.",
      "generated_abstract": "r addresses the problem of exclusion zones in instant runoff\nvoting (IRV). We study the problem of designing exclusion zones in a\nsingle-winner, ranked voting system that is used to elect a single office.\nExclusion zones are used to prevent the election of an individual who would be\na disaster for the office, thereby preserving the integrity of the election.\nWe characterize the set of exclusion zones in a class of ranked voting systems,\ncalled exclusion zones classes, and show that they are closely related to the\nclasses of exclusion zones in proportional representation systems and\nsingle-winner systems. We then propose a new exclusion zone class that is\nclosely related to the exclusion zone classes in proportional representation\nsystems and single-winner systems. Our main result is that any exclusion zone\nclass that is closed under the rank-sum function, including the exclusion zone\nclasses in proportional representation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15606936416184972,
          "p": 0.38571428571428573,
          "f": 0.22222221812054402
        },
        "rouge-2": {
          "r": 0.04247104247104247,
          "p": 0.10091743119266056,
          "f": 0.0597826045263767
        },
        "rouge-l": {
          "r": 0.12716763005780346,
          "p": 0.3142857142857143,
          "f": 0.18106995474605844
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2502.05186v1",
      "true_abstract": "In an era where financial markets are heavily influenced by many static and\ndynamic factors, it has become increasingly critical to carefully integrate\ndiverse data sources with machine learning for accurate stock price prediction.\nThis paper explores a multimodal machine learning approach for stock price\nprediction by combining data from diverse sources, including traditional\nfinancial metrics, tweets, and news articles. We capture real-time market\ndynamics and investor mood through sentiment analysis on these textual data\nusing both ChatGPT-4o and FinBERT models. We look at how these integrated data\nstreams augment predictions made with a standard Long Short-Term Memory (LSTM\nmodel) to illustrate the extent of performance gains. Our study's results\nindicate that incorporating the mentioned data sources considerably increases\nthe forecast effectiveness of the reference model by up to 5%. We also provide\ninsights into the individual and combined predictive capacities of these\nmodalities, highlighting the substantial impact of incorporating sentiment\nanalysis from tweets and news articles. This research offers a systematic and\neffective framework for applying multimodal data analytics techniques in\nfinancial time series forecasting that provides a new view for investors to\nleverage data for decision-making.",
      "generated_abstract": "y addresses the problem of predicting the stock price of an\nstock by using the combination of different features. The study used a\nmultimodal approach, in which the features were categorized into two groups:\ntextual and numerical. The numerical features were used to predict the stock\nprice, and the categorical features were used to classify the stock into two\ncategories: buy and sell. The study applied a machine learning algorithm,\nRandom Forest, to the dataset. The study used the data for 12 months, from\nJanuary 2022 to December 2022, and the results showed that the Random Forest\nalgorithm was able to achieve an accuracy of 86.2%, a recall of 78.9%, and a\nprecision of 86.2% for the classification of the stock into two categories: buy\nand sell. The study concluded that the multimodal approach is a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16541353383458646,
          "p": 0.2972972972972973,
          "f": 0.2125603818796239
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.04310344827586207,
          "f": 0.03378377901753172
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.25675675675675674,
          "f": 0.1835748746332471
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/DB/2503.05530v1",
      "true_abstract": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems.",
      "generated_abstract": "-augmented generation (RAG) has shown remarkable performance in\ncreating natural language explanations for data artifacts. However, it is\noften time-consuming due to the necessity of querying and retrieving large\ndatasets, and the need for generating multiple outputs for each input. We\nintroduce RAG-Gen, a novel RAG framework that leverages approximate caching to\nenhance performance by reducing the number of retrievals, thereby increasing\nquery efficiency and accelerating the generation process. RAG-Gen incorporates\na cache-based approach that ensures efficient retrievals by maintaining a\ncached set of retrieved objects, which can be queried to improve the cache\neffectiveness. To this end, we develop a novel key-based similarity\nmechanism that leverages the similarity of retrieved objects to determine the\ncached set size. We evaluate RAG-Gen on the TREC",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2894736842105263,
          "p": 0.3626373626373626,
          "f": 0.32195121457513387
        },
        "rouge-2": {
          "r": 0.027586206896551724,
          "p": 0.03361344537815126,
          "f": 0.030303025351527436
        },
        "rouge-l": {
          "r": 0.2631578947368421,
          "p": 0.32967032967032966,
          "f": 0.29268292189220707
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.05974v1",
      "true_abstract": "Contrast enhancement, a key aspect of image-to-image translation (I2IT),\nimproves visual quality by adjusting intensity differences between pixels.\nHowever, many existing methods struggle to preserve fine-grained details, often\nleading to the loss of low-level features. This paper introduces LapLoss, a\nnovel approach designed for I2IT contrast enhancement, based on the Laplacian\npyramid-centric networks, forming the core of our proposed methodology. The\nproposed approach employs a multiple discriminator architecture, each operating\nat a different resolution to capture high-level features, in addition to\nmaintaining low-level details and textures under mixed lighting conditions. The\nproposed methodology computes the loss at multiple scales, balancing\nreconstruction accuracy and perceptual quality to enhance overall image\ngeneration. The distinct blend of the loss calculation at each level of the\npyramid, combined with the architecture of the Laplacian pyramid enables\nLapLoss to exceed contemporary contrast enhancement techniques. This framework\nachieves state-of-the-art results, consistently performing well across\ndifferent lighting conditions in the SICE dataset.",
      "generated_abstract": "nslation, i.e., translation between different spatial scales of the\nimage, is a challenging task due to the complex structure of natural images.\nExisting methods often resort to the simple translation between different\nspatial scales, ignoring the importance of the fine details of the image\nstructure. This paper proposes LapLoss, the first Laplacian pyramid-based\nmultiscale loss for image translation. We propose LapLoss as a multiscale\nloss, designed to capture the high-frequency information of fine details. The\nLaplacian pyramid is adopted to capture the high-frequency details of image\nstructure. Moreover, the Laplacian pyramid is constructed in a multi-scale\nmanner, which enhances the robustness to the image structure changes and\npreserves the local details. The proposed LapLoss achieves significant\nperformance improvements in translation tasks, especially for challenging\ntasks",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24324324324324326,
          "p": 0.36,
          "f": 0.29032257583246623
        },
        "rouge-2": {
          "r": 0.04794520547945205,
          "p": 0.06481481481481481,
          "f": 0.055118105348131136
        },
        "rouge-l": {
          "r": 0.23423423423423423,
          "p": 0.3466666666666667,
          "f": 0.2795698876604232
        }
      }
    },
    {
      "paper_id": "physics.med-ph.physics/med-ph/2503.06172v1",
      "true_abstract": "Noninvasive brain stimulation can activate neurons in the brain but requires\npower electronics with exceptionally high power in the mega-volt-ampere and\nhigh frequencies in the kilohertz range. Whereas oscillator circuits offered\nonly one or very few pulse shapes, modular power electronics solved a\nlong-standing problem for the first time and enabled arbitrary software-based\ndesign of the temporal shape of stimuli. However, synthesizing arbitrary\nstimuli with a high output quality requires a large number of modules. Systems\nwith few modules and pulse-width modulation may generate apparently smooth\ncurrent shapes in the highly inductive coil, but the stimulation effect of the\nneurons depends on the electric field and the electric field becomes a burst of\nultra-brief rectangular pulses. We propose an alternative solution that\nachieves high-resolution pulse shaping with fewer modules by implementing\nhigh-power wide-bandwidth voltage asymmetry. Rather than equal voltage steps,\nour system strategically assigns different voltages to each module to achieve a\nnear-exponential improvement in resolution. Compared to prior designs, our\nexperimental prototype achieved better output quality, although it uses only\nhalf the number of modules.",
      "generated_abstract": "cranial Magnetic Stimulation (TMS) technique has become a\nindispensable tool in the treatment of various neurological and psychiatric\ndisorders. Despite its widespread use, TMS is still limited by the\nsynchronization of the stimulation fields to the patient's brain. In this\nwork, we propose a modular pulse synthesis solution for TMS with high\ngranularity, based on a modular analogue circuit architecture, designed to\naccommodate arbitrary pulse shapes and frequencies. The modular pulse synthesis\ndesign is based on the use of a modular analogue circuit architecture,\nenabling the synthesis of arbitrary pulse shapes and frequencies. The modular\npulse synthesis circuit design is implemented in a low-cost digital platform,\nwhich is capable of synthesizing any pulse shape with a granularity of 15",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19008264462809918,
          "p": 0.3194444444444444,
          "f": 0.23834196423420767
        },
        "rouge-2": {
          "r": 0.030120481927710843,
          "p": 0.04950495049504951,
          "f": 0.037453178816928875
        },
        "rouge-l": {
          "r": 0.15702479338842976,
          "p": 0.2638888888888889,
          "f": 0.1968911870321352
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.08835v1",
      "true_abstract": "Roll-to-roll (R2R) printing technologies are promising for high-volume\ncontinuous production of substrate-based electronic products. One of the major\nchallenges in R2R flexible electronics printing is achieving tight alignment\ntolerances, as specified by the device resolution (usually at the micro-meter\nlevel), for multi-layer printed electronics. The alignment of the printed\npatterns in different layers is known as registration. Conventional\nregistration control methods rely on real-time feedback controllers, such as\nPID control, to regulate the web tension and the web speed. However, those\nmethods may lose effectiveness in compensating for recurring disturbances and\nsupporting effective mitigation of registration errors. In this paper, we\npropose a Spatial-Terminal Iterative Learning Control (STILC) method integrated\nwith PID control to iteratively learn and reduce registration error\ncycle-by-cycle, converging it to zero. This approach enables unprecedented\nprecision in the creation, integration, and manipulation of multi-layer\nmicrostructures in R2R processes. We theoretically prove the convergence of the\nproposed STILC-PID hybrid approach and validate its effectiveness through a\nsimulated registration error scenario caused by axis mismatch between roller\nand motor, a common issue in R2R systems. The results demonstrate that the\nSTILC-PID hybrid control method can fully eliminate the registration error\nafter a feasible number of iterations. Additionally, we analyze the impact of\ndifferent learning gains on the convergence performance of STILC.",
      "generated_abstract": "o-roll (R2R) manufacturing, a set of aligned microfabricated\nsub-assemblies are stacked together to form a functional device.\nHigh-precision overlay registration between these microfabricated assemblies\nis essential for accurate device assembly. Current approaches to\noverlay-registration typically require the manual assembly of a test sample,\nwhich is subject to errors. In this paper, we propose a novel\nspatial-terminal iterative learning (STIL) method for high-precision overlay\nregistration in R2R manufacturing. The STIL framework consists of a\nspatial-terminal-based initializer that provides initial registration\ncoefficients, followed by a spatial-terminal-based refiner that refines the\ninitialization. In the spatial-terminal-based initializer, we design an\nefficient spatial-terminal-based initializer to efficiently align the\ninitial",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16783216783216784,
          "p": 0.32,
          "f": 0.22018348172502322
        },
        "rouge-2": {
          "r": 0.034482758620689655,
          "p": 0.0707070707070707,
          "f": 0.0463576114869966
        },
        "rouge-l": {
          "r": 0.16083916083916083,
          "p": 0.30666666666666664,
          "f": 0.21100916979841775
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.cond-mat/mtrl-sci/2503.10012v1",
      "true_abstract": "Epitaxial thin-film growth is a versatile and powerful technique for\nachieving a precise control of composition, stabilizing non-equilibrium phases,\ntailoring growth orientation, as well as forming heterointerfaces of various\nquantum materials. For synthesis of highly crystalline thin films, in-depth\nunderstanding of epitaxial relationship between the desired thin film and the\nsingle-crystalline substrates is necessary. In this study, we investigate\nepitaxial relationship in thin-film growth of triangular-lattice\nantiferromagnet CrSe on the (001) plane of Al2O3 and the lattice-matched (111)\nplane of yttria-stabilized zirconia (YSZ) substrates. Structural\ncharacterization using out-of-plane and in-plane x-ray diffraction shows that\nthe presence of 19.1o-twisted domains of CrSe significantly dominates the\naligned domain on the Al2O3 substrate while it reveals a single-domain\nformation on the YSZ substrate. The stability of the 19.1o-twisted domain\nrather than the aligned domain can be explained by rotational commensurate\nepitaxy, which is well reproduced by density functional theory calculations.\nThe single-domain CrSe thin film on the YSZ substrate exhibits a superior\nmetallic conductivity compared to the twisted-domain thin film on the Al2O3\nsubstrate, implying contribution of the grain boundary scattering mechanism to\nelectrical transport.",
      "generated_abstract": "on the formation of single-domain (SD) CrSe thin films on\nlattice-matched YSZ(111) substrates, which is a rare and challenging\nphenomenon. By controlling the growth temperature, we were able to obtain\nsingle-domain films with a 1:1 stoichiometry (Cr/Se=1) and a thickness of\n100-120 nm, comparable to that of the bulk material. The formation of SD\nCrSe films is attributed to the formation of a stable phase-separated region\nwithin the film, which is formed by a sequence of chemical reactions in the\ngrowth environment. In this study, the formation of SD CrSe films on\nlattice-matched YSZ(111) substrates is reported for the first time,\ndemonstrating that the formation of single-domain CrSe films can",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21008403361344538,
          "p": 0.390625,
          "f": 0.2732240391674879
        },
        "rouge-2": {
          "r": 0.06666666666666667,
          "p": 0.11956521739130435,
          "f": 0.08560310824387979
        },
        "rouge-l": {
          "r": 0.18487394957983194,
          "p": 0.34375,
          "f": 0.24043715392158624
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2410.16101v1",
      "true_abstract": "Humans and other animals coactivate agonist and antagonist muscles in many\nmotor actions. Increases in muscle coactivation are thought to leverage\nviscoelastic properties of skeletal muscles to provide resistance against limb\nmotion. However, coactivation also emerges in scenarios where it seems\nparadoxical because the goal is not to resist limb motion but instead to\nrapidly mobilize the limb(s) or body to launch or correct movements. Here, we\npresent a new perspective on muscle coactivation: to prime the nervous system\nfor fast, task-dependent responses to sensory stimuli. We review distributed\nneural control mechanisms that may allow the healthy nervous system to leverage\nmuscle coactivation to produce fast and flexible responses to sensory feedback.",
      "generated_abstract": "The nervous system integrates muscle activity into a complex feedback\nprocess that enables the control of large muscle masses. We propose that\nmuscle coactivation is the mechanism that integrates muscle activity into this\nfeedback process. We provide a theoretical model that captures how muscle\ncoactivation induces a feedback bias in the spinal cord that enables fast and\ntask-dependent control. The model accounts for the fact that muscle activity\ninduces a feedback bias in the spinal cord that enables fast and task-dependent\ncontrol. We further demonstrate that this feedback bias is critical for\noptimizing control precision in fast and task-dependent control. Our model\nprovides a framework for understanding the mechanisms underlying muscle\ncoactivation and its impact on the nervous system.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24096385542168675,
          "p": 0.37037037037037035,
          "f": 0.2919707981437477
        },
        "rouge-2": {
          "r": 0.0380952380952381,
          "p": 0.044444444444444446,
          "f": 0.04102563605522743
        },
        "rouge-l": {
          "r": 0.1927710843373494,
          "p": 0.2962962962962963,
          "f": 0.23357663755980615
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/MM/2503.09938v1",
      "true_abstract": "Vision-and-language navigation (VLN) tasks require agents to navigate\nthree-dimensional environments guided by natural language instructions,\noffering substantial potential for diverse applications. However, the scarcity\nof training data impedes progress in this field. This paper introduces\nPanoGen++, a novel framework that addresses this limitation by generating\nvaried and pertinent panoramic environments for VLN tasks. PanoGen++\nincorporates pre-trained diffusion models with domain-specific fine-tuning,\nemploying parameter-efficient techniques such as low-rank adaptation to\nminimize computational costs. We investigate two settings for environment\ngeneration: masked image inpainting and recursive image outpainting. The former\nmaximizes novel environment creation by inpainting masked regions based on\ntextual descriptions, while the latter facilitates agents' learning of spatial\nrelationships within panoramas. Empirical evaluations on room-to-room (R2R),\nroom-for-room (R4R), and cooperative vision-and-dialog navigation (CVDN)\ndatasets reveal significant performance enhancements: a 2.44% increase in\nsuccess rate on the R2R test leaderboard, a 0.63% improvement on the R4R\nvalidation unseen set, and a 0.75-meter enhancement in goal progress on the\nCVDN validation unseen set. PanoGen++ augments the diversity and relevance of\ntraining environments, resulting in improved generalization and efficacy in VLN\ntasks.",
      "generated_abstract": "r presents PanoGen++, a novel approach for generating panoramic\nenvironment representations from text prompts. Traditional methods for\ngenerating panoramic representations rely on pre-trained visual models\nspecialized for panoramic representation learning. However, these models may\nlack the ability to adapt to new tasks and prompts. To address this, we\nintroduce PanoGen++, a domain-adapted method that leverages text prompts to\nadapt pre-trained visual models for panoramic representation learning. We\nexperiment with three text-guided methods: TF-PanoGen, TF-TextGen, and\nTGM-Gen. TF-PanoGen uses pre-trained text models to generate prompts for\nvisual models, TF-TextGen uses pre-trained text models to generate prompts for\nvisual models, and TGM-Gen uses a pre",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1386861313868613,
          "p": 0.3220338983050847,
          "f": 0.1938775468122658
        },
        "rouge-2": {
          "r": 0.011428571428571429,
          "p": 0.024390243902439025,
          "f": 0.01556419798937274
        },
        "rouge-l": {
          "r": 0.1386861313868613,
          "p": 0.3220338983050847,
          "f": 0.1938775468122658
        }
      }
    },
    {
      "paper_id": "math.MG.math/FA/2503.09542v2",
      "true_abstract": "In 1959, Marcus and Ree proved that any bistochastic matrix $A$ satisfies\n$\\Delta_n(A):= \\max_{\\sigma\\in S_n}\\sum_{i=1}^{n}A(i, \\sigma(i))-\\sum_{i,\nj=1}^n A(i, j)^2 \\geq 0$. Erd\\H{o}s asked to characterize the bistochastic\nmatrices satisfying $\\Delta_n(A)=0$. It was recently proved that there are only\nfinitely many such matrices for any $n$. However, a complete list of such\nmatrices was obtained in dimension $n=2, 3$ only recently, arXiv:2306.05518. In\nthis paper, we characterize all $4\\times 4$ bistochastic matrices satisfying\n$\\Delta_4(A)=0$. Furthermore, we show that for $n\\geq 3$, $\\Delta_n(A)=\\alpha$\nhas uncountably many solutions when $\\alpha\\notin \\{0, (n-1)/4\\}$. This answers\na question raised in arXiv:2410.06612. We extend the Marcus$\\unicode{x2013}$Ree\ninequality to infinite bistochastic arrays and bistochastic kernels. Our\ninvestigation into $4\\times 4$ Erd\\H{o}s matrices also raises several\nintriguing questions that are of independent interest. We propose several\nquestions and conjectures and present numerical evidence for them.",
      "generated_abstract": "We discuss the relation between the Erd\\H{o}s matrix and the\nMarcus matrix. The result is an extension of the Marcus inequality that\ngeneralizes to any $2\\times 2$ matrix $A$ the inequality\n$|A_{11}|+|A_{21}|\\leq |A_{12}|+|A_{22}|$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1188118811881188,
          "p": 0.48,
          "f": 0.1904761872952885
        },
        "rouge-2": {
          "r": 0.007518796992481203,
          "p": 0.03225806451612903,
          "f": 0.01219511888533688
        },
        "rouge-l": {
          "r": 0.1188118811881188,
          "p": 0.48,
          "f": 0.1904761872952885
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/optics/2503.10490v1",
      "true_abstract": "The effect of a constant electric field on two-photon absorption in a\nsemiconductor is calculated using an independent-particle theory. The\ntheoretical framework is an extension of a theory of the one-photon\nFranz-Keldysh effect [Wahlstrand and Sipe, Phys. Rev. B 82, 075206 (2010)]. The\ntheory includes the effect of the constant field, including field-induced\ncoupling between closely spaced bands, in the electronic wavefunctions and\ncalculates optical absorption perturbatively. Numerical calculations are\nperformed using a 14-band $\\mathbf{k} \\cdot\\mathbf{p}$ band structure model for\nGaAs. For all nonzero tensor elements, field-enabled two-photon absorption\n(TPA) below the band gap and Franz-Keldysh oscillations in the TPA spectrum are\npredicted, with a generally larger effect in tensor elements with more\ncomponents parallel to the constant electric field direction. Some tensor\nelements that are zero in the absence of a field become nonzero in the presence\nof the constant electric field and depend on its sign. Notably, these elements\nare linear in the electric field to lowest order and may be substantial away\nfrom band structure critical points at room temperature and/or with a\nnon-uniform field. Electric-field-induced changes in the carrier injection rate\ndue to interference between one- and two-photon absorption are also calculated.\nThe electric field enables this bichromatic coherent control process for\npolarization configurations where it is normally forbidden, and also modifies\nthe spectrum of the process for configurations where it is allowed by crystal\nsymmetry.",
      "generated_abstract": "-Keldysh effect, which enables the measurement of second-order\ncorrelation functions, is a powerful tool for studying nonlinear optics. We\npresent a theoretical framework for the measurement of second-order correlation\nfunctions in two-photon polarization space. We derive a general expression for\nthe Fourier transform of the second-order correlation function, which allows us\nto calculate the Fourier transform of the second-order correlation function\nunder the influence of an electric field. By integrating out the electric\nfield, we obtain a system of coupled differential equations for the second-order\ncorrelation function and the Fourier transform of the second-order correlation\nfunction. We use this theoretical framework to examine the effects of electric\nfield-induced bichromatic control on the two-photon Franz-Keldysh effect. We\nfind that the magnitude of the second-order correlation function decreases\nwhen the electric field is applied",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18796992481203006,
          "p": 0.35714285714285715,
          "f": 0.24630541420078145
        },
        "rouge-2": {
          "r": 0.024875621890547265,
          "p": 0.04950495049504951,
          "f": 0.03311257832967913
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.3,
          "f": 0.20689654720570758
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2412.09157v1",
      "true_abstract": "This paper studies the robust reinsurance and investment games for\ncompetitive insurers. Model uncertainty is characterized by a class of\nequivalent probability measures. Each insurer is concerned with relative\nperformance under the worst-case scenario. Insurers' surplus processes are\napproximated by drifted Brownian motion with common and idiosyncratic insurance\nrisks. The insurers can purchase proportional reinsurance to divide the\ninsurance risk with the reinsurance premium calculated by the variance\nprinciple. We consider an incomplete market driven by the 4/2 stochastic\nvolatility mode. This paper formulates the robust mean-field game for a\nnon-linear system originating from the variance principle and the 4/2 model.\nFor the case of an exponential utility function, we derive closed-form\nsolutions for the $n$-insurer game and the corresponding mean-field game. We\nshow that relative concerns lead to new hedging terms in the investment and\nreinsurance strategies. Model uncertainty can significantly change the\ninsurers' hedging demands. The hedging demands in the investment-reinsurance\nstrategies exhibit highly non-linear dependence with the insurers' competitive\ncoefficients, risk aversion and ambiguity aversion coefficients. Finally,\nnumerical results demonstrate the herd effect of competition.",
      "generated_abstract": "aper, we study the many-insurer robust game of reinsurance and\ninvestment under model uncertainty in incomplete markets. We first introduce a\nnovel stochastic game model for the many-insurer robust game of reinsurance\nand investment. This model is inspired by the many-insurer game of reinsurance\nand investment, which is a linear programming game that can be solved\nefficiently. However, the many-insurer game of reinsurance and investment is\noften not computationally tractable, even for relatively simple situations. To\naddress this issue, we introduce a two-insurer robust game model that\nfacilitates computation. We show that the many-insurer robust game of reinsurance\nand investment is equivalent to the two-insurer robust game of reinsurance and\ninvestment. Moreover, we show that the many-insurer robust game of reins",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22321428571428573,
          "p": 0.43103448275862066,
          "f": 0.29411764256332185
        },
        "rouge-2": {
          "r": 0.036585365853658534,
          "p": 0.075,
          "f": 0.0491803234614355
        },
        "rouge-l": {
          "r": 0.17857142857142858,
          "p": 0.3448275862068966,
          "f": 0.23529411315155715
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.14317v2",
      "true_abstract": "We develop a model that captures peer effect heterogeneity by modeling the\nendogenous spillover to be linear in ordered peer outcomes. Unlike the\ncanonical linear-in-means model, our approach accounts for the distribution of\npeer outcomes as well as the size of peer groups. Under a minimal condition,\nour model admits a unique equilibrium and is therefore tractable and\nidentified. Simulations show our estimator has good finite sample performance.\nFinally, we apply our model to educational data from Norway, finding that\nhigher-performing friends disproportionately drive GPA spillovers. Our\nframework provides new insights into the structure of peer effects beyond\naggregate measures.",
      "generated_abstract": "In this paper, we study a new rank-dependent peer effect model that is\nidentified through the lens of the conditional variance of the mean. The\nconditional variance of the mean is estimated by fitting a random coefficient\nmodel to the sample mean. We show that this model can be identified using\nestimators that are based on the conditional variance of the mean. We also show\nthat the rank-dependent peer effect model is equivalent to the rank-independent\npeer effect model, which is widely used in the peer effect literature. Our\nfindings show that the rank-dependent peer effect model can be identified\nthrough the lens of the conditional variance of the mean, even though the peer\neffect model is estimated using the sample mean.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2564102564102564,
          "p": 0.38461538461538464,
          "f": 0.30769230289230776
        },
        "rouge-2": {
          "r": 0.031578947368421054,
          "p": 0.038461538461538464,
          "f": 0.03468207597313713
        },
        "rouge-l": {
          "r": 0.23076923076923078,
          "p": 0.34615384615384615,
          "f": 0.27692307212307704
        }
      }
    },
    {
      "paper_id": "math.AT.math/AT/2503.02839v1",
      "true_abstract": "We show that the $\\infty$-category of normed algebras in genuine $G$-spectra,\nas introduced by Bachmann-Hoyois, is modelled by strictly commutative algebras\nin $G$-symmetric spectra for any finite group $G$. We moreover provide an\nanalogous description of Schwede's ultra-commutative global ring spectra in\nhigher categorical terms.\n  Using these new descriptions, we exhibit the $\\infty$-category of\nultra-commutative global ring spectra as a partially lax limit of the\n$\\infty$-categories of genuine $G$-spectra for varying $G$, in analogy with the\nnon-multiplicative comparison of Nardin, Pol, and the second author.\n  Along the way, we establish various new results in parametrized higher\nalgebra, which we hope to be of independent interest.",
      "generated_abstract": "uce a new notion of norm on the space of homotopically finite\nequivalence classes of representations of a group $G$ by a group homomorphism\n$\\phi:G\\to\\mathbb{Z}$, and show that it satisfies some natural properties. We\nshow that the set of norms on the space of homotopically finite representations\nof a group is a group under pointwise addition and multiplication. We show that\nthe norm on the space of homotopically finite representations of a group is\nweakly contracting and that the norm on the space of homotopically finite\nrepresentations of a group is a quasi-norm. We show that the norm on the space\nof homotopically finite representations of a group is a reflexive and\ntransitive norm, and that it is compatible with the group topology and\nhomotopy groups. We show that the norm on the space of homotopically finite",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1891891891891892,
          "p": 0.30434782608695654,
          "f": 0.23333332860555567
        },
        "rouge-2": {
          "r": 0.05102040816326531,
          "p": 0.07246376811594203,
          "f": 0.059880234671734776
        },
        "rouge-l": {
          "r": 0.16216216216216217,
          "p": 0.2608695652173913,
          "f": 0.19999999527222234
        }
      }
    },
    {
      "paper_id": "math.NA.stat/CO/2503.05533v1",
      "true_abstract": "Multilevel sampling methods, such as multilevel and multifidelity Monte\nCarlo, multilevel stochastic collocation, or delayed acceptance Markov chain\nMonte Carlo, have become standard uncertainty quantification tools for a wide\nclass of forward and inverse problems. The underlying idea is to achieve faster\nconvergence by leveraging a hierarchy of models, such as partial differential\nequation (PDE) or stochastic differential equation (SDE) discretisations with\nincreasing accuracy. By optimally redistributing work among the levels,\nmultilevel methods can achieve significant performance improvement compared to\nsingle level methods working with one high-fidelity model. Intuitively,\napproximate solutions on coarser levels can tolerate large computational error\nwithout affecting the overall accuracy. We show how this can be used in\nhigh-performance computing applications to obtain a significant performance\ngain.\n  As a use case, we analyse the computational error in the standard multilevel\nMonte Carlo method and formulate an adaptive algorithm which determines a\nminimum required computational accuracy on each level of discretisation. We\nshow two examples of how the inexactness can be converted into actual gains\nusing an elliptic PDE with lognormal random coefficients. Using a low precision\nsparse direct solver combined with iterative refinement results in a simulated\ngain in memory references of up to $3.5\\times$ compared to the reference double\nprecision solver; while using a MINRES iterative solver, a practical speedup of\nup to $1.5\\times$ in terms of FLOPs is achieved. These results provide a step\nin the direction of energy-aware scientific computing, with significant\npotential for energy savings.",
      "generated_abstract": "l Monte Carlo (MLMC) methods have been widely used for the\nexploitation of inexact computations in numerical integration. These methods\nimprove the accuracy of numerical integration, especially for high-frequency\nquantities. However, the use of inexact computations in MLMC methods is\nlimited to the first-order integrand, which is often the main source of inexact\ncomputations. In this paper, we introduce a novel technique that expands the\nMLMC method to the second-order integrand, which is not generally available.\nThe technique is based on the idea of splitting the multilevel Monte Carlo\nmethod into two parts. First, we develop an algorithm to compute the second-order\nintegrand. Second, we extend this algorithm to compute the second-order\nintegrand for the first-order integrand. We prove that the proposed\nsplitting-based algorithm is equivalent to the original ML",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17073170731707318,
          "p": 0.3684210526315789,
          "f": 0.23333332900555565
        },
        "rouge-2": {
          "r": 0.017167381974248927,
          "p": 0.036036036036036036,
          "f": 0.02325580958237643
        },
        "rouge-l": {
          "r": 0.1524390243902439,
          "p": 0.32894736842105265,
          "f": 0.20833332900555562
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.05682v1",
      "true_abstract": "Multi-contrast magnetic resonance imaging (MRI) plays a vital role in brain\ntumor segmentation and diagnosis by leveraging complementary information from\ndifferent contrasts. Each contrast highlights specific tumor characteristics,\nenabling a comprehensive understanding of tumor morphology, edema, and\npathological heterogeneity. However, existing methods still face the challenges\nof multi-level specificity perception across different contrasts, especially\nwith limited annotations. These challenges include data heterogeneity,\ngranularity differences, and interference from redundant information. To\naddress these limitations, we propose a Task-oriented Uncertainty Collaborative\nLearning (TUCL) framework for multi-contrast MRI segmentation. TUCL introduces\na task-oriented prompt attention (TPA) module with intra-prompt and\ncross-prompt attention mechanisms to dynamically model feature interactions\nacross contrasts and tasks. Additionally, a cyclic process is designed to map\nthe predictions back to the prompt to ensure that the prompts are effectively\nutilized. In the decoding stage, the TUCL framework proposes a dual-path\nuncertainty refinement (DUR) strategy which ensures robust segmentation by\nrefining predictions iteratively. Extensive experimental results on limited\nlabeled data demonstrate that TUCL significantly improves segmentation accuracy\n(88.2\\% in Dice and 10.853 mm in HD95). It shows that TUCL has the potential to\nextract multi-contrast information and reduce the reliance on extensive\nannotations. The code is available at:\nhttps://github.com/Zhenxuan-Zhang/TUCL_BrainSeg.",
      "generated_abstract": "or segmentation is a critical task in medical imaging. Despite\nlarge-scale efforts, the accuracy of segmentation remains low due to the\ncomplexity of the task. This paper proposes a novel method for segmenting\nbrain tumors based on a multimodal unified framework that integrates MRI,\nCT, and PET scans. Our approach incorporates uncertainty quantification to\naddress the lack of labels, which is a common problem in medical image\nsegmentation. To address this issue, we propose a task-oriented uncertainty\ncollaborative learning (TUCL) framework that dynamically adapts to the\nuncertainty of the data, enabling the model to adapt to unseen data. Additionally,\nwe introduce a multi-scale learning strategy to reduce the computational\ncomplexity of the model. Our method achieves state-of-the-art performance on\nthe PET-MRI-CT brain",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20945945945945946,
          "p": 0.3875,
          "f": 0.2719298200061558
        },
        "rouge-2": {
          "r": 0.04,
          "p": 0.07079646017699115,
          "f": 0.05111820624891589
        },
        "rouge-l": {
          "r": 0.19594594594594594,
          "p": 0.3625,
          "f": 0.25438596035703304
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2409.10805v1",
      "true_abstract": "Dengue virus (DENV) is a mosquito-borne virus with a significant human health\nconcern. With 390 million infections annually and 96 million showing clinical\nsymptoms, severe dengue can lead to life-threatening conditions like dengue\nhemorrhagic fever (DHF) and dengue shock syndrome (DSS). The only FDA-approved\nvaccine, Dengvaxia, has limitations due to antibody-dependent enhancement\n(ADE), necessitating careful administration. The recent pre-approval of TAK-003\nby WHO in 2024 highlights ongoing efforts to improve vaccine options. This\nreview explores recent advancements in dengue vaccine development, emphasizing\npotential utility of mRNA-based vaccines. By examining current clinical trial\ndata and innovations, we aim to identify promising strategies to address the\nlimitations of existing vaccines and enhance global dengue prevention efforts.",
      "generated_abstract": "rus (DENV) is a mosquito-borne flavivirus that causes encephalitis\nand meningoencephalitis. The current vaccine has limited efficacy due to its\nhigh risk of infection, and it lacks broad-spectrum protection. DENV vaccines\nhave significant challenges, including high production costs, limited\ndistribution, and limited safety and efficacy. To overcome these challenges,\nmRNA vaccines offer a promising alternative. This review discusses the\ndevelopment and application of mRNA vaccines for DENV. It analyzes their\nadvantages and disadvantages, including the ability to generate immune responses\nwithout the need for live virus, the potential to be produced at scale, and\ntheir potential to develop into a universal vaccine for other viral diseases.\nIt also explores the challenges that need to be addressed in order to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23333333333333334,
          "p": 0.25609756097560976,
          "f": 0.2441860415224447
        },
        "rouge-2": {
          "r": 0.04424778761061947,
          "p": 0.043478260869565216,
          "f": 0.04385964412319232
        },
        "rouge-l": {
          "r": 0.2222222222222222,
          "p": 0.24390243902439024,
          "f": 0.23255813454570048
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.01698v1",
      "true_abstract": "Fractional vegetation coverage (FVC) and its spatio-temporal variations are\ncritical indicators of regional ecological changes, which are of great\nsignificance to study the laws of surface variation and analyze regional\necosystem. Under the development of RS and GIS technology, this analysis\nemploys Landsat satellite images in 1994, 2008, 2013 and 2016 to estimate FVC\nin Yalong River Basin based on the Dimidiate Pixel Model. With consideration of\nthe vegetation coverage condition and land surface law in the study area, the\nresearch further analyzes the Spatio-temporal variations as well as the\ninfluencing factors of FVC in terms of topography and land use types\nrespectively. The results show that since 1994, FVC in Yalong River Basin has\nexperienced a downward trend yet displaying an uptick from 2013. Moreover,\ndifferent land use types indicate the versatility of land covers in Yalong\nRiver Basin, with grassland and forest performing probably the most important\nfactors that can induce changes to the stability of FVC in whole basin.\nOverall, the research reflects the impact of human activities on vegetation in\nYalong River Basin, and provides available data and theoretical basis for\necological assessment, ecological restoration and environmental protection.",
      "generated_abstract": "n coverage is one of the most important parameters of natural\nenvironment, which has a significant impact on the ecological balance and\nenvironmental protection. However, the study of vegetation coverage in\nrural areas is difficult due to the complex spatial structure of the rural\nenvironment, and the high cost of field measurement and large data volume, so\nthe application of remote sensing is a promising solution. The remote sensing\nis the most effective and efficient way to study the vegetation coverage. The\ndimidiate pixel model (DPM) is a powerful method for studying vegetation\ncoverage, which integrates the vegetation cover of 1996 and 2020, and the\nresults show that the average vegetation coverage in the Yalong River Basin\nin 2020 was 26.36% compared to 25.66% in 1996. The results",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.225,
          "p": 0.34615384615384615,
          "f": 0.27272726795224983
        },
        "rouge-2": {
          "r": 0.10344827586206896,
          "p": 0.15126050420168066,
          "f": 0.12286688937413384
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.3076923076923077,
          "f": 0.24242423764921955
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/AS/2503.06273v1",
      "true_abstract": "We explore a novel zero-shot Audio-Visual Speech Recognition (AVSR)\nframework, dubbed Zero-AVSR, which enables speech recognition in target\nlanguages without requiring any audio-visual speech data in those languages.\nSpecifically, we introduce the Audio-Visual Speech Romanizer (AV-Romanizer),\nwhich learns language-agnostic speech representations by predicting Roman text.\nThen, by leveraging the strong multilingual modeling capabilities of Large\nLanguage Models (LLMs), we propose converting the predicted Roman text into\nlanguage-specific graphemes, forming the proposed Cascaded Zero-AVSR. Taking it\na step further, we explore a unified Zero-AVSR approach by directly integrating\nthe audio-visual speech representations encoded by the AV-Romanizer into the\nLLM. This is achieved through finetuning the adapter and the LLM using our\nproposed multi-task learning scheme. To capture the wide spectrum of phonetic\nand linguistic diversity, we also introduce a Multilingual Audio-Visual\nRomanized Corpus (MARC) consisting of 2,916 hours of audio-visual speech data\nacross 82 languages, along with transcriptions in both language-specific\ngraphemes and Roman text. Extensive analysis and experiments confirm that the\nproposed Zero-AVSR framework has the potential to expand language support\nbeyond the languages seen during the training of the AV-Romanizer.",
      "generated_abstract": "ual speech recognition (AVSR) has become a major research topic in\nacoustics and speech processing, with significant progress in speech recognition\nusing natural language processing (NLP) models. However, it remains challenging\nto achieve zero-shot AVSR, where a language model (LM) is asked to recognize a\nsingle unknown speech signal without any training data. This is due to the\ncomplexities of AVSR, which includes multiple modalities such as audio, text,\nand visual, and the interference between modalities. This paper presents\nZero-AVSR, a novel zero-shot AVSR approach that addresses the aforementioned\nchallenges by leveraging zero-shot language-agnostic speech representations.\nThe proposed approach learns a universal speech representation from a\nlanguage-agnostic, zero-shot speech recognition dataset. The learned\nuniversal speech representation is then used to recognize unknown speech\nsignals without any language model training.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2644628099173554,
          "p": 0.37209302325581395,
          "f": 0.3091787391042965
        },
        "rouge-2": {
          "r": 0.047337278106508875,
          "p": 0.06666666666666667,
          "f": 0.05536331694304469
        },
        "rouge-l": {
          "r": 0.24793388429752067,
          "p": 0.3488372093023256,
          "f": 0.289855067606712
        }
      }
    },
    {
      "paper_id": "nlin.AO.nlin/AO/2503.01812v1",
      "true_abstract": "A novel model for dynamical traps in intermittent human control is proposed.\nIt describes probabilistic, step-wise transitions between two modes of a\nsubject's behavior - active and passive phases in controlling an object's\ndynamics - using an original stochastic differential equation. This equation\ngoverns time variations of a special variable, denoted as $\\zeta$, between two\nlimit values, $\\zeta=0$ and $\\zeta=1$. The introduced trap function,\n$\\Omega(\\Delta)$, quantifies the subject's perception of the object's deviation\nfrom a desired state, thereby determining the relative priority of the two\naction modes. Notably, these transitions - referred to as the subject's action\npoints - occur before the trap function reaches its limit values,\n$\\Omega(\\Delta)=0$ or $\\Omega(\\Delta)=1$. This characteristic enables the\napplication of the proposed model to describe intermittent human control over\nreal objects.",
      "generated_abstract": "In this work, we introduce a stochastic model that describes the dynamics of\nmechanical systems in a control loop. We use a multi-population model that\ndescribes the dynamics of a single mechanical system, combined with the\ndynamics of a control loop, and we call this model the dynamical trap model. We\nshow that the dynamical trap model can be used to analyze control and\nstabilization methods in dynamical systems. We show that a particular method\nfor stabilization of a dynamical trap is similar to the method for stabilization\nof a control loop, and we analyze this method in detail. We show that the\ndynamical trap model can be used to analyze control and stabilization methods\nin dynamical systems. We show that a particular method for stabilization of a\ndynamical trap is similar to the method for stabilization of a control loop,\nand we analyze this method in detail.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17045454545454544,
          "p": 0.3409090909090909,
          "f": 0.22727272282828287
        },
        "rouge-2": {
          "r": 0.008403361344537815,
          "p": 0.013333333333333334,
          "f": 0.010309273607718198
        },
        "rouge-l": {
          "r": 0.1590909090909091,
          "p": 0.3181818181818182,
          "f": 0.21212120767676776
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/SR/2503.10415v1",
      "true_abstract": "This article focuses on NGC7538 IRS1, one of the most luminous and studied HC\nHII regions in the northern hemisphere. Our aim is to identify the young\nstellar objects (YSOs) embedded within the ionized gas and study their\nkinematic structures. This work expands on a recent survey called \"Protostellar\nOutflows at the EarliesT Stages\" (POETS), which has been devoted to studying\nyoung outflow emission on scales of 10-100 au near luminous YSOs, before they\nstart photoionizing the surrounding medium. We carried out multi-epoch Very\nLong Baseline Array observations of the 22 GHz water masers toward NGC7538 IRS1\nto measure the maser 3D velocities, which, following POETS' findings, are\nreliable tracers of the protostellar winds. Recently, we reobserved the water\nmasers in NGC7538 IRS1 with sensitive global very long baseline interferometry\n(VLBI) observations to map weaker maser emission. Our study confirms the\npresence of two embedded YSOs, IRS1a and IRS1b, at the center of the two linear\ndistributions of 6.7 GHz methanol masers observed in the southern and northern\ncores of the HC HII region, which have been previously interpreted in terms of\nedge-on rotating disks. The water masers trace an extended (~200 au) stationary\nshock front adjacent to the inner portion of the disk around IRS1a. This shock\nfront corresponds to the edge of the southern tip of the ionized core and might\nbe produced by the interaction of the disk wind ejected from IRS1a with the\ninfalling envelope. The water masers closer to IRS1b follow the same LSR\nvelocity (Vlsr) pattern of the 6.7~GHz masers rotating in the disk, but the\ndirection and amplitude of the water maser proper motions are inconsistent with\nrotation. We propose that these water masers are tracing a photo-evaporated\ndisk wind, where the maser Vlsr traces mainly the disk rotation and the proper\nmotions the poloidal velocity of the wind.",
      "generated_abstract": "t the results of a study of the circumstellar material (CSM)\nin the H$_2$O maser-emitting region in NGC 7538 IRS1. This region was\nselected due to its close proximity to the H$_2$O maser-emitting region in\nNGC 7538 IRS2, and the H$_2$O maser emission was also detected there. The\nresults from the observations of the CSM in NGC 7538 IRS1 are consistent with\nthe models of protostellar outflows at the early stages. In the CSM, we found\nevidence for the presence of a disk-like structure, which is consistent with\nthe disk-like structure reported in previous studies. This disk-like structure\nis associated with the presence of the H$_2$O maser emission in NGC 7538 IRS1.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11891891891891893,
          "p": 0.36666666666666664,
          "f": 0.179591833036235
        },
        "rouge-2": {
          "r": 0.036101083032490974,
          "p": 0.11627906976744186,
          "f": 0.055096415117061155
        },
        "rouge-l": {
          "r": 0.10270270270270271,
          "p": 0.31666666666666665,
          "f": 0.15510203711786766
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2411.10139v1",
      "true_abstract": "The basic principle of any version of insurance is the paradigm that\nexchanging risk by sharing it in a pool is beneficial for the participants. In\ncase of independent risks with a finite mean this is the case for risk averse\ndecision makers. The situation may be very different in case of infinite mean\nmodels. In that case it is known that risk sharing may have a negative effect,\nwhich is sometimes called the nondiversification trap. This phenomenon is well\nknown for infinite mean stable distributions. In a series of recent papers\nsimilar results for infinite mean Pareto and Fr\\'echet distributions have been\nobtained. We further investigate this property by showing that many of these\nresults can be obtained as special cases of a simple result demonstrating that\nthis holds for any distribution that is more skewed than a Cauchy distribution.\nWe also relate this to the situation of deadly catastrophic risks, where we\nassume a positive probability for an infinite value. That case gives a very\nsimple intuition why this phenomenon can occur for such catastrophic risks. We\nalso mention several open problems and conjectures in this context.",
      "generated_abstract": "In this paper, we consider a general class of continuous time stochastic\ninterest rate models, including the generalised discounted cash-flow model and\nthe generalised discounted portfolio model. We propose an optimal control\nproblem for a finite portfolio model with the generalised discounted cash-flow\nmodel and the generalised discounted portfolio model. We establish the\nexistence and uniqueness of the optimal control and the strong solution of\nthe corresponding optimal control problem. We also study the effects of\nrisk sharing and diversification in the optimal portfolio problem. We prove the\nconvergence of the numerical solution to the optimal control problem and\nnumerically illustrate the effects of risk sharing and diversification on the\noptimal portfolio problem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1559633027522936,
          "p": 0.3269230769230769,
          "f": 0.2111801198503145
        },
        "rouge-2": {
          "r": 0.02197802197802198,
          "p": 0.05128205128205128,
          "f": 0.030769226569231346
        },
        "rouge-l": {
          "r": 0.14678899082568808,
          "p": 0.3076923076923077,
          "f": 0.1987577596018673
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.02763v1",
      "true_abstract": "The Index of Dissimilarity (ID), widely utilized in economic literature as a\nmeasure of segregation, is inadequate for cross-country or time series studies\ndue to its failure to account for structural variations across countries' labor\nmarkets or changes over time within a single country's labor market. Building\non the works of Karmel and MacLachlan (1988) and Blackburn et al. (1993), we\npropose a new measure - the standardized ID - that isolates structural\ndifferences from true differences in segregation across space or time. A key\nadvantage of our proposed measure lies in its ease of implementation and\ninterpretation, even when working with datasets encompassing a large number of\ncountries or time periods. Moreover, our measure can be consistently applied in\nthe case of lumpy sectors or occupations that account for a large fraction of\nthe workforce. We illustrate the new measure in an analysis of the\ncross-country relationship between economic development (as measured by GDP per\ncapita) and occupational and sectoral gender segregation. Comparing the crude\nID with the standardized ID, we show that the crude ID overestimates the\npositive correlation between income and segregation, especially between low-\nand middle-income countries. This suggests that analyses relying on the crude\nID risk overestimating the importance of income differentials in explaining\ncross-country variation in gender segregation.",
      "generated_abstract": "on is a common phenomenon in labor markets, yet the empirical\nanalysis of segregation metrics is limited. We develop and evaluate a\ncomprehensive set of segregation metrics, including the Gini coefficient, the\nIndicative Gini coefficient, the Tinbergen N-th percentile, the Bray-Curie\npercentile, and the Bays-Chen-Gao-Kunreuther-Wang-Xie-Xu-Zhu (BCGKWXZ)\npercentile, which are based on the empirical observations of the average\nwage gap and the mean wage gap. The empirical results show that the\nGini coefficient is a more appropriate metric than the wage gap for measuring\nthe segregation of the labor market. The Tinbergen N-th percentile is more\nsuitable for analyzing the segregation of the labor",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12213740458015267,
          "p": 0.2857142857142857,
          "f": 0.1711229904566903
        },
        "rouge-2": {
          "r": 0.03,
          "p": 0.06896551724137931,
          "f": 0.041811842465005467
        },
        "rouge-l": {
          "r": 0.12213740458015267,
          "p": 0.2857142857142857,
          "f": 0.1711229904566903
        }
      }
    },
    {
      "paper_id": "physics.med-ph.physics/med-ph/2503.06131v1",
      "true_abstract": "This study focuses on the rotation of the hips and shoulders during a\nbaseball bat swing, analyzing the time-series changes in rotational angles,\nrotational velocities, and axes using marker position data obtained from a\nmotion capture system with 12 infrared cameras. Previous studies have examined\nfactors such as ground reaction forces, muscle activation patterns, rotational\nenergy, angular velocity, and angles during a swing. However, to the best of\nour knowledge, the hip and shoulder rotational motions have not been adequately\nvisualized or compared. In particular, there is a lack of analysis regarding\nthe coordination and timing differences between hip and shoulder movements\nduring the swing. Therefore, this study aims to quantitatively compare the hip\nand shoulder rotational movements during the swing between skilled and\nunskilled players and visualizes the differences between them. Based on the\nobtained data, the study aims to improve the understanding of bat swing\nmechanics by visualizing the coordinated body movements during the swing.",
      "generated_abstract": "ional kinematics of the hip and shoulder during baseball bat swing\nremain relatively poorly understood. The aim of this study is to provide a\ncomprehensive assessment of hip and shoulder kinematics during baseball bat\nswinging. A total of 27 baseball bat swings (11 left-handed, 16 right-handed)\nwere recorded using a motion capture system. The swing kinematics were\nanalyzed using the 3D kinematic framework of the Fatigue Model. The results\nindicate that the hip has a predominantly horizontal swing axis, while the\nshoulder has a predominantly vertical swing axis. The shoulder plane is also\nlocated below the hip plane. The hip angle and shoulder angle are both\nslightly inclined with respect to the swing plane. The hip angle and shoulder\nangle are also highly correlated, suggesting that they may be influenced by the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20202020202020202,
          "p": 0.26666666666666666,
          "f": 0.2298850525663893
        },
        "rouge-2": {
          "r": 0.08633093525179857,
          "p": 0.10619469026548672,
          "f": 0.09523809029132048
        },
        "rouge-l": {
          "r": 0.20202020202020202,
          "p": 0.26666666666666666,
          "f": 0.2298850525663893
        }
      }
    },
    {
      "paper_id": "nlin.CG.nlin/CG/2411.03601v3",
      "true_abstract": "A one-dimensional cellular automaton $\\tau : A^\\mathbb{Z} \\to A^\\mathbb{Z}$\nis a transformation of the full shift defined via a finite neighborhood $S\n\\subset \\mathbb{Z}$ and a local function $\\mu : A^S \\to A$. We study the family\nof cellular automata whose finite neighborhood $S$ is an interval containing\n$0$, and there exists a pattern $p \\in A^S$ satisfying that $\\mu(z) = z(0)$ if\nand only if $z \\neq p$; this means that these cellular automata have a unique\n\\emph{active transition}. Despite its simplicity, this family presents\ninteresting and subtle problems, as the behavior of the cellular automaton\ncompletely depends on the structure of $p$. We show that every cellular\nautomaton $\\tau$ with a unique active transition $p \\in A^S$ is either\nidempotent or strictly almost equicontinuous, and we completely characterize\neach one of these situations in terms of $p$. In essence, the idempotence of\n$\\tau$ depends on the existence of a certain subpattern of $p$ with a\ntranslational symmetry.",
      "generated_abstract": "We study the properties of a one-dimensional cellular automaton (CA)\nwith a unique active transition. We show that the transition can occur in\n$\\Omega(1)$ steps or in $\\Omega(N)$ steps, depending on the parameter\n$\\gamma\\in(0,1)$. We also give the exact formula for the transition\nprobability and show that the transition probability vanishes exponentially in\nthe number of cells. We derive the critical parameter $\\gamma_*$ for the\ntransition and discuss its properties. We also show that the CA with a unique\nactive transition exhibits a phase transition in $\\Omega(1)$ steps.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18446601941747573,
          "p": 0.40425531914893614,
          "f": 0.2533333290302223
        },
        "rouge-2": {
          "r": 0.08275862068965517,
          "p": 0.16901408450704225,
          "f": 0.11111110669795972
        },
        "rouge-l": {
          "r": 0.17475728155339806,
          "p": 0.3829787234042553,
          "f": 0.23999999569688893
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2502.09962v1",
      "true_abstract": "We consider a two-sided matching problem in which the agents on one side have\ndichotomous preferences and the other side representing institutions has strict\npreferences (priorities). It captures several important applications in\nmatching market design in which the agents are only interested in getting\nmatched to an acceptable institution. These include centralized daycare\nassignment and healthcare rationing. We present a compelling new mechanism that\nsatisfies many prominent and desirable properties including individual\nrationality, maximum size, fairness, Pareto-efficiency on both sides,\nstrategyproofness on both sides, non-bossiness and having polynomial time\nrunning time. As a result, we answer an open problem whether there exists a\nmechanism that is agent-strategyproof, maximum, fair and non-bossy.",
      "generated_abstract": "We study the problem of designing a maximum matching in a graph under\ndichotomous agent preferences. We first characterize the problem as a\nclassical matching problem and then introduce a novel strategyproof matching\nalgorithm that runs in polynomial time and has the same number of edges as a\nstandard matching. Furthermore, we show that the problem is polynomially\nhard if the graph has maximum degree two and is NP-hard otherwise. The\napproach is based on a novel use of the graph Laplacian, which is a discrete\nversion of the graph Laplacian in continuous-time systems. We also show that\nthe problem is NP-hard if the graph has maximum degree three and polynomially\nhard otherwise. Finally, we discuss an application of the results to the design\nof a fair and efficient auction in a market with many sellers and buyers.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2857142857142857,
          "p": 0.32432432432432434,
          "f": 0.3037974633744593
        },
        "rouge-2": {
          "r": 0.028846153846153848,
          "p": 0.02564102564102564,
          "f": 0.027149316284270277
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.28378378378378377,
          "f": 0.2658227798301555
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/TR/2411.05013v1",
      "true_abstract": "This study utilizes machine learning algorithms to analyze and organize\nknowledge in the field of algorithmic trading. By filtering a dataset of 136\nmillion research papers, we identified 14,342 relevant articles published\nbetween 1956 and Q1 2020. We compare traditional practices-such as\nkeyword-based algorithms and embedding techniques-with state-of-the-art topic\nmodeling methods that employ dimensionality reduction and clustering. This\ncomparison allows us to assess the popularity and evolution of different\napproaches and themes within algorithmic trading. We demonstrate the usefulness\nof Natural Language Processing (NLP) in the automatic extraction of knowledge,\nhighlighting the new possibilities created by the latest iterations of Large\nLanguage Models (LLMs) like ChatGPT. The rationale for focusing on this topic\nstems from our analysis, which reveals that research articles on algorithmic\ntrading are increasing at a faster rate than the overall number of\npublications. While stocks and main indices comprise more than half of all\nassets considered, certain asset classes, such as cryptocurrencies, exhibit a\nmuch stronger growth trend. Machine learning models have become the most\npopular methods in recent years. The study demonstrates the efficacy of LLMs in\nrefining datasets and addressing intricate questions about the analyzed\narticles, such as comparing the efficiency of different models. Our research\nshows that by decomposing tasks into smaller components and incorporating\nreasoning steps, we can effectively tackle complex questions supported by case\nanalyses. This approach contributes to a deeper understanding of algorithmic\ntrading methodologies and underscores the potential of advanced NLP techniques\nin literature reviews.",
      "generated_abstract": "y introduces a novel literature review methodology that employs\nLLMs and Natural Language Processing (NLP) techniques to analyze the\nliterature in the area of algorithmic trading. The LLMs were trained on\nhistorical trade data to create a comprehensive understanding of the topic and\ndevelop a model that can predict the probability of a trade succeeding based\non the parameters of the trade. The NLP techniques were used to analyze the\nliterature to identify key terms and concepts that were central to the study.\nThese techniques were applied to identify and extract key concepts from the\nliterature, and then used to generate new concepts to better understand the\ntopic. The results of the study demonstrate the potential of LLMs and NLP\ntechniques in enhancing the understanding and analysis of literature in the\narea of algorithmic trading. The findings provide a framework for future\nresearch and highlight the potential of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1724137931034483,
          "p": 0.410958904109589,
          "f": 0.24291497559310926
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.1111111111111111,
          "f": 0.07692307239644997
        },
        "rouge-l": {
          "r": 0.1724137931034483,
          "p": 0.410958904109589,
          "f": 0.24291497559310926
        }
      }
    },
    {
      "paper_id": "math.CO.math/AC/2503.07299v1",
      "true_abstract": "Let $\\varphi:V\\times V\\to W$ be a bilinear map of finite vector spaces $V$\nand $W$ over a finite field $\\mathbb{F}_q$. We present asymptotic bounds on the\nnumber of isomorphism classes of bilinear maps under the natural action of\n$\\mathrm{GL}(V)$ and $\\mathrm{GL}(W)$, when $\\dim(V)$ and $\\dim(W)$ are\nlinearly related.\n  As motivations and applications of the results, we present almost tight upper\nbounds on the number of $p$-groups of Frattini class $2$ as first studied by\nHigman (Proc. Lond. Math. Soc., 1960). Such bounds lead to answers for some\nopen questions by Blackburn, Neumann, and Venkataraman (Cambridge Tracts in\nMathematics, 2007). Further applications include sampling matrix spaces with\nthe trivial automorphism group, and asymptotic bounds on the number of\nisomorphism classes of finite cube-zero commutative algebras.",
      "generated_abstract": "the average order of the automorphism group of bilinear maps\n$f : V \\times W \\rightarrow Z$, where $V, W$ are finite sets and $Z$ is a\nfinite field, using the results of Choi, Choi, Hennessy, Hsieh, and Lowe.\nMoreover, we characterize the automorphism groups of bilinear maps $f : V\n\\times W \\rightarrow Z$ with $V, W$ finite sets and $Z$ a finite field, and\nderive a necessary and sufficient condition for the order to be divisible by\n$p^n$, for $n \\geq 1$, where $p$ is a prime number. Finally, we derive an\nupper bound for the order of the automorphism group of bilinear maps.\n  In particular, we obtain the following result:\n  Let $V, W$ be finite sets and $",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21348314606741572,
          "p": 0.2923076923076923,
          "f": 0.2467532418746838
        },
        "rouge-2": {
          "r": 0.04504504504504504,
          "p": 0.05434782608695652,
          "f": 0.04926107878764397
        },
        "rouge-l": {
          "r": 0.1797752808988764,
          "p": 0.24615384615384617,
          "f": 0.2077922029136449
        }
      }
    },
    {
      "paper_id": "math.LO.math/LO/2503.08566v1",
      "true_abstract": "We provide a characterization of those relation algebras which are isomorphic\nto the algebras of compatible relations of some $\\Z_2$-set. We further prove\nthat this class is finitely axiomatizable in first-order logic in the language\nof relation algebras.",
      "generated_abstract": "We introduce a new notion of relation algebra that is compatible with\n$\\mathbb{Z}_2$-sets, and provide a characterization of these algebras. We\nalso give a partial answer to a question of J. D. Sparks. Finally, we\ncharacterize the relation algebra generated by a set of $n$ elements, $S$, in\nterms of the complement of $S$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.4482758620689655,
          "p": 0.3170731707317073,
          "f": 0.3714285665755102
        },
        "rouge-2": {
          "r": 0.1111111111111111,
          "p": 0.07692307692307693,
          "f": 0.09090908607438043
        },
        "rouge-l": {
          "r": 0.3793103448275862,
          "p": 0.2682926829268293,
          "f": 0.3142857094326531
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/PR/2405.02170v1",
      "true_abstract": "We consider the Fourier-Laplace transforms of a broad class of polynomial\nOrnstein-Uhlenbeck (OU) volatility models, including the well-known\nStein-Stein, Sch\\\"obel-Zhu, one-factor Bergomi, and the recently introduced\nQuintic OU models motivated by the SPX-VIX joint calibration problem. We show\nthe connection between the joint Fourier-Laplace functional of the log-price\nand the integrated variance, and the solution of an infinite dimensional\nRiccati equation. Next, under some non-vanishing conditions of the\nFourier-Laplace transforms, we establish an existence result for such Riccati\nequation and we provide a discretized approximation of the joint characteristic\nfunctional that is exponentially entire. On the practical side, we develop a\nnumerical scheme to solve the stiff infinite dimensional Riccati equations and\ndemonstrate the efficiency and accuracy of the scheme for pricing SPX options\nand volatility swaps using Fourier and Laplace inversions, with specific\nexamples of the Quintic OU and the one-factor Bergomi models and their\ncalibration to real market data.",
      "generated_abstract": "We study the Fourier-Laplace transform of the Ornstein-Uhlenbeck model\nwith polynomial noise, and show that its characteristic function can be\nrepresented as a function of the Fourier transform of the polynomial noise.\nThis characterization allows for the construction of a new class of\npolynomial-Ornstein-Uhlenbeck models. We provide numerical evidence of the\nvalidity of this characterization, and we discuss its potential applications to\nthe study of financial market models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22340425531914893,
          "p": 0.45652173913043476,
          "f": 0.29999999558775514
        },
        "rouge-2": {
          "r": 0.0364963503649635,
          "p": 0.07936507936507936,
          "f": 0.04999999568450037
        },
        "rouge-l": {
          "r": 0.14893617021276595,
          "p": 0.30434782608695654,
          "f": 0.1999999955877552
        }
      }
    },
    {
      "paper_id": "cs.CL.stat/AP/2502.16556v1",
      "true_abstract": "This study examines how Large Language Models (LLMs) perform when tackling\nquantitative management decision problems in a zero-shot setting. Drawing on\n900 responses generated by five leading models across 20 diverse managerial\nscenarios, our analysis explores whether these base models can deliver accurate\nnumerical decisions under varying presentation formats, scenario complexities,\nand repeated attempts. Contrary to prior findings, we observed no significant\neffects of text presentation format (direct, narrative, or tabular) or text\nlength on accuracy. However, scenario complexity -- particularly in terms of\nconstraints and irrelevant parameters -- strongly influenced performance, often\ndegrading accuracy. Surprisingly, the models handled tasks requiring multiple\nsolution steps more effectively than expected. Notably, only 28.8\\% of\nresponses were exactly correct, highlighting limitations in precision. We\nfurther found no significant ``learning effect'' across iterations: performance\nremained stable across repeated queries. Nonetheless, significant variations\nemerged among the five tested LLMs, with some showing superior binary accuracy.\nOverall, these findings underscore both the promise and the pitfalls of\nharnessing LLMs for complex quantitative decision-making, informing managers\nand researchers about optimal deployment strategies.",
      "generated_abstract": "vancements in large language models (LLMs) have demonstrated their\naccomplishments in various domains. While these advancements have been extensively\nstudied in text-based tasks, there is a lack of systematic research on LLMs'\nperformance in quantitative management problem-solving tasks. In this paper,\nwe conduct a systematic analysis of the performance of state-of-the-art LLMs in\nquantitative management problem-solving tasks, such as the 100-Item\nSAT-Simulated Answers Test (100-SAT), the 100-Item SAT-Simulated Answers\nTest (100-SAT), and the 100-Item SAT-Simulated Answers Test (100-SAT-2). We\nanalyze the performance of 10 LLMs across various quantitative",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12413793103448276,
          "p": 0.32142857142857145,
          "f": 0.17910447359223794
        },
        "rouge-2": {
          "r": 0.011494252873563218,
          "p": 0.0273972602739726,
          "f": 0.01619432781983093
        },
        "rouge-l": {
          "r": 0.12413793103448276,
          "p": 0.32142857142857145,
          "f": 0.17910447359223794
        }
      }
    },
    {
      "paper_id": "cs.GT.cs/GT/2503.04542v1",
      "true_abstract": "Professional networks are a key determinant of individuals' labor market\noutcomes. They may also play a role in either exacerbating or ameliorating\ninequality of opportunity across demographic groups. In a theoretical model of\nprofessional network formation, we show that inequality can increase even\nwithout exogenous in-group preferences, confirming and complementing existing\ntheoretical literature. Increased inequality emerges from the differential\nleverage privileged and unprivileged individuals have in forming connections\ndue to their asymmetric ex ante prospects. This is a formalization of a source\nof inequality in the labor market which has not been previously explored.\n  We next show how inequality-aware platforms may reduce inequality by\nsubsidizing connections, through link recommendations that reduce costs,\nbetween privileged and unprivileged individuals. Indeed, mixed-privilege\nconnections turn out to be welfare improving, over all possible equilibria,\ncompared to not recommending links or recommending some smaller fraction of\ncross-group links. Taken together, these two findings reveal a stark reality:\nprofessional networking platforms that fail to foster integration in the link\nformation process risk reducing the platform's utility to its users and\nexacerbating existing labor market inequality.",
      "generated_abstract": "play a fundamental role in the operation of modern organizations,\ninducing and sustaining trust, communication, and collaboration. However, the\ndevelopment of professional networks often faces challenges, such as over-connectivity,\ninefficiency, and inequity. This study aims to develop a novel approach to\naddress these issues through the incorporation of link recommendations. We\nintroduce a two-step framework that leverages a collaborative filtering approach\nto recommend professional links based on a user's profile, and a graph\nelimination method to ensure efficiency and equity in network formation. We\ndemonstrate the effectiveness of our approach through two case studies:\n(1) A multi-agent simulation that simulates the formation of a network of\nprofessionals, with the goal of minimizing over-connectivity; and (2) a\nreal-world network of professionals, which examines the formation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.171875,
          "p": 0.2619047619047619,
          "f": 0.2075471650267
        },
        "rouge-2": {
          "r": 0.03488372093023256,
          "p": 0.05042016806722689,
          "f": 0.04123710856792018
        },
        "rouge-l": {
          "r": 0.1484375,
          "p": 0.2261904761904762,
          "f": 0.17924527823424719
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PR/2412.08987v1",
      "true_abstract": "Computational efficiency is essential for enhancing the accuracy and\npracticality of pricing complex financial derivatives. In this paper, we\ndiscuss Isogeometric Analysis (IGA) for valuing financial derivatives, modeled\nby two nonlinear Black-Scholes PDEs: the Leland model for European call with\ntransaction costs and the AFV model for convertible bonds with default options.\nWe compare the solutions of IGA with finite difference methods (FDM) and finite\nelement methods (FEM). In particular, very accurate solutions can be\nnumerically calculated on far less mesh (knots) than FDM or FEM, by using\nnon-uniform knots and weighted cubic NURBS, which in turn reduces the\ncomputational time significantly.",
      "generated_abstract": "ng of financial derivatives is a critical task that involves\ncomplex nonlinear dynamics, and the use of Isogeometric Analysis (IGA) is a\npowerful method for this task. However, the use of IGA for pricing financial\nderivatives is not straightforward. Traditional IGA models are often not\nsuitable for this task due to their limitations, and it is often difficult to\ndetermine the optimal basis functions for use in the IGA model. In this work,\nwe develop a novel Isogeometric Analysis model for the pricing of financial\nderivatives. This model is based on a novel extension of the standard Finite\nElement Method (FEM), and is capable of accurately pricing derivatives with\nnonlinear models, such as the Black-Scholes model. In particular, we present\nresults for convertible bonds and options, demonstrating the accuracy of the\nmodel in pricing these financial derivatives. Additionally",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3170731707317073,
          "p": 0.34210526315789475,
          "f": 0.3291139190578434
        },
        "rouge-2": {
          "r": 0.11,
          "p": 0.08943089430894309,
          "f": 0.0986547035733679
        },
        "rouge-l": {
          "r": 0.25609756097560976,
          "p": 0.27631578947368424,
          "f": 0.265822779817337
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.17679v1",
      "true_abstract": "Adverse childhood experiences (ACEs) have been linked to a wide range of\nnegative health outcomes in adulthood. However, few studies have investigated\nwhat specific combinations of ACEs most substantially impact mental health. In\nthis article, we provide the protocol for our observational study of the\neffects of combinations of ACEs on adult depression. We use data from the 2023\nBehavioral Risk Factor Surveillance System (BRFSS) to assess these effects. We\nwill evaluate the replicability of our findings by splitting the sample into\ntwo discrete subpopulations of individuals. We employ data turnover for this\nanalysis, enabling a single team of statisticians and domain experts to\ncollaboratively evaluate the strength of evidence, and also integrating both\nqualitative and quantitative insights from exploratory data analysis. We\noutline our analysis plan using this method and conclude with a brief\ndiscussion of several specifics for our study.",
      "generated_abstract": ": Adverse Childhood Experiences (ACEs) have been associated with\ndepression. This study sought to identify factors associated with ACEs, and\ntheir impact on depression. Methods: A total of 364 adults were recruited\nthrough a mailing list of a local mental health center. The primary outcome was\nthe ACEs score, which was derived from the Childhood Trauma Questionnaire.\nSecondary outcomes included the Beck Depression Inventory II, and Hamilton\nDepression Scale. Results: The mean ACEs score was 5.27, with 11.6% of\nrespondents having a score of 7 or greater. ACEs scores were positively\nassociated with depression scores. Conclusion: This study suggests that ACEs\nhave an association with depression. The findings suggest that interventions\ntargeting ACEs may",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.24390243902439024,
          "f": 0.21978021482912705
        },
        "rouge-2": {
          "r": 0.029197080291970802,
          "p": 0.03636363636363636,
          "f": 0.032388659027357374
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.24390243902439024,
          "f": 0.21978021482912705
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.17431v1",
      "true_abstract": "In this paper, we show exponential dimensional dependence for the Hermite\nmethod of moments as a statistical test for Gaussianity in the case of i.i.d.\nGaussian variables, by constructing a lower bound for the the\nKolmogorov-Smirnov distance and an upper bound for the convex distance.",
      "generated_abstract": "The Hermite method of moments is a powerful tool for the estimation of\nhigh-dimensional functions, which is a key component of numerous practical\napplications. In this paper, we study the exponential dependence between\nconditional expectation and conditional variance of the Hermite method of\nmoments. We show that when the conditional expectation and conditional variance\nare both uniformly bounded, the Hermite method of moments is asymptotically\nexponentially efficient. This result extends the previously known result for\nlinear regression. We establish a new sufficient condition for the\nexponential dependence between the conditional expectation and conditional\nvariance. We then develop a novel method for constructing efficient estimators\nof the conditional expectation and conditional variance of the Hermite method of\nmoments. The proposed method is valid under a broad class of assumptions,\nincluding the usual assumptions on the conditional mean and covariance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.45714285714285713,
          "p": 0.22535211267605634,
          "f": 0.30188678802954794
        },
        "rouge-2": {
          "r": 0.18604651162790697,
          "p": 0.0784313725490196,
          "f": 0.11034482341403108
        },
        "rouge-l": {
          "r": 0.42857142857142855,
          "p": 0.2112676056338028,
          "f": 0.28301886350124605
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2503.00239v1",
      "true_abstract": "Bayesian posterior approximation has become more accessible to practitioners\nthan ever, thanks to modern black-box software. While these tools provide\nhighly accurate approximations with minimal user effort, certain posterior\ngeometries remain notoriously difficult for standard methods. As a result,\nresearch into alternative approximation techniques continues to flourish. In\nmany papers, authors validate their new approaches by testing them on posterior\nshapes deemed challenging or \"wild.\" However, these shapes are not always\ndirectly linked to real-world applications where they naturally occur. In this\nnote, we present examples of practical applications that give rise to some\ncommonly used benchmark posterior shapes.",
      "generated_abstract": "posteriors in the wild (WPw) are a class of wild posteriors that\nestimate the posterior distribution of a parameter by simulating a large\nnumber of parameter values from a specified posterior distribution. This\nsimulation is performed by using a large number of samples drawn from a\nspecified posterior distribution. The use of a large number of samples is\ncritical for the accuracy of the posterior distribution. However, simulating a\nlarge number of samples can be computationally expensive, especially when the\nnumber of parameters is large. In this paper, we propose a new method for\nestimating the posterior distribution of a parameter using a finite set of\nsamples. We first show that the set of samples is a subset of the wild posteriors\nin the wild and we use the subset of samples to estimate the posterior\ndistribution. The proposed method is more computationally efficient than the\nwild posteriors in the wild in terms of the number of samples used",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1839080459770115,
          "p": 0.25396825396825395,
          "f": 0.21333332846133343
        },
        "rouge-2": {
          "r": 0.01020408163265306,
          "p": 0.009433962264150943,
          "f": 0.009803916576319342
        },
        "rouge-l": {
          "r": 0.1839080459770115,
          "p": 0.25396825396825395,
          "f": 0.21333332846133343
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.10172v1",
      "true_abstract": "In this paper, for solving nonlinear systems we propose two\npseudoinverse-free greedy block methods with momentum by combining the\nresidual-based weighted nonlinear Kaczmarz and heavy ball methods. Without the\nfull column rank assumptions on Jacobi matrices of nonlinear systems, we\nprovide a thorough convergence analysis, and derive upper bounds for the\nconvergence rates of the new methods. Numerical experiments demonstrate that\nthe proposed methods with momentum are much more effective than the existing\nones.",
      "generated_abstract": "igate the convergence of the block nonlinear Kaczmarz (BNK) methods\nwith momentum in the finite-dimensional case. We show that the BNK method with\nmomentum converges to the unique solution of the original system of linear\nequations with a rate depending on the step size, and the convergence rate\ndepends on the direction of the momentum. We also derive a bound for the\nrelative error in the momentum direction. We show that the convergence rate of\nthe BNK methods with momentum depends on the smoothness of the system and the\ndirection of the momentum. We apply the new convergence result to analyze the\nconvergence of the BNK methods in the infinite-dimensional case. We also\nconsider the convergence of the BNK methods with momentum in the infinite-dimensional\ncase. We show that the convergence rate of the BNK methods with momentum depends\non the smoothness of the system and the direction",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2807017543859649,
          "p": 0.32653061224489793,
          "f": 0.30188678748131015
        },
        "rouge-2": {
          "r": 0.11267605633802817,
          "p": 0.09876543209876543,
          "f": 0.10526315291637835
        },
        "rouge-l": {
          "r": 0.22807017543859648,
          "p": 0.2653061224489796,
          "f": 0.2452830138964045
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.16800v1",
      "true_abstract": "This paper introduces a new solution concept for the Cooperative Game with\nPublic Externalities, called the w-value, which is characterized by three\nproperties (axioms), namely Pareto-optimality (PO), Market-equilbrium (ME) and\nFiscal-balance (FB). Additionally, the implementation mechanism for w-value is\nalso provided. The w-value exists and is unique. It belongs to the core. And,\nmore specifically, it belongs to the -core. Meanwhile, the computational cost\nof w-value is very low. Therefore, the w-value is a theoretically more\ncompelling solution concept than the existing cooperation game solutions when\nanalyzing cooperative games with public externalities. A numerical illustration\nshows the calculation steps of w-value. Meanwhile, the w-value well explains\nthe reason why the mandatory emission reduction mechanism must be transformed\ninto a \"nationally determined contribution\" mechanism in current international\nclimate negotiations.",
      "generated_abstract": "In this paper, we propose a new solution for a cooperative game with public\nexternalities. Our solution is based on axiomatic method. The axioms are\nderived by considering the relationship between the utility function of the\ngame player and the utility function of the other players. Based on the\naxioms, we define a new concept of the social welfare of the game. In addition,\nwe define a new concept of the social cost of the game. The axioms are\nsufficient for our solution. We analyze our solution by using axiomatic\nmethod.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18085106382978725,
          "p": 0.3469387755102041,
          "f": 0.23776223325737203
        },
        "rouge-2": {
          "r": 0.04201680672268908,
          "p": 0.07246376811594203,
          "f": 0.05319148471536936
        },
        "rouge-l": {
          "r": 0.18085106382978725,
          "p": 0.3469387755102041,
          "f": 0.23776223325737203
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2402.01820v1",
      "true_abstract": "We consider a stochastic volatility model where the dynamics of the\nvolatility are given by a possibly infinite linear combination of the elements\nof the time extended signature of a Brownian motion. First, we show that the\nmodel is remarkably universal, as it includes, but is not limited to, the\ncelebrated Stein-Stein, Bergomi, and Heston models, together with some\npath-dependent variants. Second, we derive the joint characteristic functional\nof the log-price and integrated variance provided that some infinite\ndimensional extended tensor algebra valued Riccati equation admits a solution.\nThis allows us to price and (quadratically) hedge certain European and\npath-dependent options using Fourier inversion techniques. We highlight the\nefficiency and accuracy of these Fourier techniques in a comprehensive\nnumerical study.",
      "generated_abstract": "This paper proposes a novel approach to signature volatility modelling,\nusing Fourier series as a basis for modelling volatility in a way that is\nflexible, interpretable, and simple to implement. The approach is based on\ntransforming the original volatility process into a Fourier series, which\nallows for a more efficient analysis of the volatility process. The paper\ndiscusses the use of Fourier series in modelling volatility, and provides\nexamples of applications to the pricing and hedging of options. The paper\ninvestigates how to apply the Fourier approach to the pricing and hedging of\noptions, in particular, the Black-Scholes option pricing model. The paper\nprovides an explanation of how the Fourier approach is applied in the\nBlack-Scholes model and how to incorporate it into the existing Black-Scholes\nframework.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20689655172413793,
          "p": 0.27692307692307694,
          "f": 0.2368421003679018
        },
        "rouge-2": {
          "r": 0.034482758620689655,
          "p": 0.036036036036036036,
          "f": 0.0352422857513252
        },
        "rouge-l": {
          "r": 0.14942528735632185,
          "p": 0.2,
          "f": 0.1710526266836913
        }
      }
    },
    {
      "paper_id": "cs.CY.q-fin/EC/2502.12397v1",
      "true_abstract": "Access to digital information is a driver of economic development. But\nalthough 85% of sub-Saharan Africa's population is covered by mobile broadband\nsignal, only 37% use the internet, and those who do seldom use the web. We\ninvestigate whether AI can bridge this gap by analyzing how 469 teachers use an\nAI chatbot in Sierra Leone. The chatbot, accessible via a common messaging app,\nis compared against traditional web search. Teachers use AI more frequently\nthan web search for teaching assistance. Data cost is the most frequently cited\nreason for low internet usage across Africa. The average web search result\nconsumes 3,107 times more data than an AI response, making AI 87% less\nexpensive than web search. Additionally, only 2% of results for corresponding\nweb searches contain content from Sierra Leone. In blinded evaluations, an\nindependent sample of teachers rate AI responses as more relevant, helpful, and\ncorrect than web search results. These findings suggest that AI-driven\nsolutions can cost-effectively bridge information gaps in low-connectivity\nregions.",
      "generated_abstract": "This study explores the potential of artificial intelligence (AI) in\nthe teaching and learning of mathematics in Sierra Leone, using a nationally\nrepresentative sample of primary school teachers. We examine the relationship\nbetween teachers' subject knowledge and their use of AI in their teaching,\nusing a multivariate statistical analysis of data from the 2021 Sierra Leone\nNational Assessment of Mathematical and Science Achievement. Our findings\nsuggest that teachers with greater subject knowledge were more likely to\ninteract with AI-driven learning tools, particularly in the early stages of the\ncurriculum. These findings highlight the potential of AI to support teachers in\ntheir use of digital technologies, and the need for more rigorous research on\nthe role of AI in mathematics education in developing countries.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20512820512820512,
          "p": 0.3076923076923077,
          "f": 0.24615384135384627
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.05405405405405406,
          "f": 0.04494381536674715
        },
        "rouge-l": {
          "r": 0.19658119658119658,
          "p": 0.2948717948717949,
          "f": 0.23589743109743597
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.10491v1",
      "true_abstract": "While music remains a challenging domain for generative models like\nTransformers, recent progress has been made by exploiting suitable\nmusically-informed priors. One technique to leverage information about musical\nstructure in Transformers is inserting such knowledge into the positional\nencoding (PE) module. However, Transformers carry a quadratic cost in sequence\nlength. In this paper, we propose F-StrIPE, a structure-informed PE scheme that\nworks in linear complexity. Using existing kernel approximation techniques\nbased on random features, we show that F-StrIPE is a generalization of\nStochastic Positional Encoding (SPE). We illustrate the empirical merits of\nF-StrIPE using melody harmonization for symbolic music.",
      "generated_abstract": "r introduces F-StrIPE, a novel positional-encoding framework that\ntransforms music-related symbolic sequences into a structured representation\nusing a positional-encoding-based structure-informed encoder. The proposed\nframework is designed to handle complex and long music-related sequences,\nenabling it to capture the complex and dynamic nature of music. The structure-\ninformed encoder captures the underlying structure of the music-related\nsequence and generates a structured representation, enabling the decoder to\ngenerate the desired music. The positional-encoding-based structure-informed\nencoder utilizes a self-attention mechanism to capture the context of the\nsequence and generates a structured representation, thereby improving the\nperformance of the decoder. The experimental results on the\nMusic21-v1.2-Genre-and-Mood-Representations dataset demonstrate that the\nproposed F",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17647058823529413,
          "p": 0.23809523809523808,
          "f": 0.20270269781318495
        },
        "rouge-2": {
          "r": 0.01020408163265306,
          "p": 0.010752688172043012,
          "f": 0.0104711991919105
        },
        "rouge-l": {
          "r": 0.16470588235294117,
          "p": 0.2222222222222222,
          "f": 0.1891891842996714
        }
      }
    },
    {
      "paper_id": "eess.IV.q-bio/QM/2503.07104v1",
      "true_abstract": "Whole-brain tractography in diffusion MRI is often followed by a parcellation\nin which each streamline is classified as belonging to a specific white matter\nbundle, or discarded as a false positive. Efficient parcellation is important\nboth in large-scale studies, which have to process huge amounts of data, and in\nthe clinic, where computational resources are often limited. TractCloud is a\nstate-of-the-art approach that aims to maximize accuracy with a local-global\nrepresentation. We demonstrate that the local context does not contribute to\nthe accuracy of that approach, and is even detrimental when dealing with\npathological cases. Based on this observation, we propose PETParc, a new method\nfor Parallel Efficient Tractography Parcellation. PETParc is a\ntransformer-based architecture in which the whole-brain tractogram is randomly\npartitioned into sub-tractograms whose streamlines are classified in parallel,\nwhile serving as global context for each other. This leads to a speedup of up\nto two orders of magnitude relative to TractCloud, and permits inference even\non clinical workstations without a GPU. PETParc accounts for the lack of\nstreamline orientation either via a novel flip-invariant embedding, or by\nsimply using flips as part of data augmentation. Despite the speedup, results\nare often even better than those of prior methods. The code and pretrained\nmodel will be made public upon acceptance.",
      "generated_abstract": "zed tractography is a powerful approach for characterizing the\nbrain's anatomy, but traditional segmentation methods often struggle with\nhigh-throughput imaging, requiring manual segmentation of multiple slices.\nThis study presents a novel framework for parallel tractography segmentation that\nutilizes an unsupervised learning approach. Our approach builds a global\ncontext model to learn localized features, enabling a scalable, parallelized\nsegmentation workflow. We developed a custom pre-processing pipeline for\nultra-high-resolution (UHR) MRI scans, integrating diffusion tensor\nimaging (DTI) and diffusion white matter tractography (DTI-WM). We validated\nthe framework by applying it to the 10-sample Human Connectome Project (HCP)\ndataset, demonstrating that it outperforms existing approaches for segmenting\nwhite matter tracts in high-throughput imaging. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1793103448275862,
          "p": 0.3058823529411765,
          "f": 0.22608695186200387
        },
        "rouge-2": {
          "r": 0.024154589371980676,
          "p": 0.045871559633027525,
          "f": 0.0316455651011463
        },
        "rouge-l": {
          "r": 0.15172413793103448,
          "p": 0.25882352941176473,
          "f": 0.19130434316635173
        }
      }
    },
    {
      "paper_id": "cs.NI.cs/NI/2503.07973v1",
      "true_abstract": "The Satellite-Terrestrial Integrated Network (STIN) enhances end-to-end\ntransmission by simultaneously utilizing terrestrial and satellite networks,\noffering significant benefits in scenarios like emergency response and\ncross-continental communication. Low Earth Orbit (LEO) satellite networks offer\nreduced Round Trip Time (RTT) for long-distance data transmission and serve as\na crucial backup during terrestrial network failures. Meanwhile, terrestrial\nnetworks are characterized by ample bandwidth resources and generally more\nstable link conditions. Therefore, integrating Multipath TCP (MPTCP) into STIN\nis vital for optimizing resource utilization and ensuring efficient data\ntransfer by exploiting the complementary strengths of both networks. However,\nthe inherent challenges of STIN, such as heterogeneity, instability, and\nhandovers, pose difficulties for traditional multipath schedulers, which are\ntypically designed for terrestrial networks. We propose a novel multipath data\nscheduling approach for STIN, Adaptive Latency Compensation Scheduler (ALCS),\nto address these issues. ALCS refines transmission latency estimates by\nincorporating RTT, congestion window size, inflight and queuing packets, and\nsatellite trajectory information. It further employs adaptive mechanisms for\nlatency compensation and proactive handover management. It further employs\nadaptive mechanisms for latency compensation and proactive handover management.\nImplemented in the MPTCP Linux Kernel and evaluated in a simulated STIN\ntestbed, ALCS outperforms existing multipath schedulers, delivering faster data\ntransmission and achieving throughput gains of 9.8% to 44.0% compared to\nbenchmark algorithms.",
      "generated_abstract": "r presents a novel Adaptive Latency Compensation Scheduler (ALCS),\nwhich is designed to enhance the performance of multitasking applications on\nsatellite-terrestrial integrated networks (STINets). The proposed scheduler\nadapts its workload distribution strategy based on the real-time latency\nrequirements of individual tasks. Unlike previous work, which relies on\nstatistical measures of latency and workload to decide the workload split, ALCS\nproposes a novel approach that integrates task characteristics, such as\ntask-specific latency, throughput, and workload distribution. The proposed ALCS\nscheduler leverages a novel workload split strategy based on task characteristics\nto improve performance and reliability in STINets. The proposed ALCS scheduler\nis evaluated using a case study of a real-world application, i.e., video\ntransmission, in a STIN",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1568627450980392,
          "p": 0.3076923076923077,
          "f": 0.20779220331927822
        },
        "rouge-2": {
          "r": 0.035175879396984924,
          "p": 0.06542056074766354,
          "f": 0.04575162943889148
        },
        "rouge-l": {
          "r": 0.1568627450980392,
          "p": 0.3076923076923077,
          "f": 0.20779220331927822
        }
      }
    },
    {
      "paper_id": "math.GT.math/GT/2503.06133v1",
      "true_abstract": "We introduce a new PL invariant, called the balanced genus, for balanced\nnormal $d$-pseudomanifolds. As a key result, we establish that for any\n3-manifold $M$ that is not a sphere, the balanced genus satisfies the lower\nbound $\\mathcal{G}_M \\geq m+3$, where $m$ is the rank of its fundamental group.\nFurthermore, we prove that a 3-manifold $M$ is homeomorphic to the 3-sphere if\nand only if its balanced genus $\\mathcal{G}_M$ is at most 3.\n  For 4-manifolds, we establish a similar characterization: if $M$ is not\nhomeomorphic to a sphere, then its balanced genus is bounded below by\n$\\mathcal{G}_M \\geq 2\\chi(M) + 5m + 11$, where $m$ is the rank of $\\pi_1(M)$.\nAdditionally, we prove that a 4-manifold $M$ is PL-homeomorphic to the 4-sphere\nif and only if its balanced genus satisfies $\\mathcal{G}_M \\leq 2\\chi(M) + 10$.\n  We believe that the balanced genus provides a new perspective in\ncombinatorial topology and will inspire further developments in the field. To\nthis end, we outline several research directions for future exploration.",
      "generated_abstract": "In this paper, we prove a lower bound theorem for balanced 3-manifolds, which\nis equivalent to the existence of a minimal pair of non-isotopic\ntetrahedra. We also show that a 4-manifold is balanced if and only if it is\nhomeomorphic to the balanced 3-sphere, and the number of balanced 3-spheres\nwith given genus and number of boundary components is bounded by a constant.\nThese results generalize the results of Bubenik and Kronheimer-Mrowka.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24444444444444444,
          "p": 0.4489795918367347,
          "f": 0.31654675802494703
        },
        "rouge-2": {
          "r": 0.09701492537313433,
          "p": 0.18840579710144928,
          "f": 0.12807881324662104
        },
        "rouge-l": {
          "r": 0.24444444444444444,
          "p": 0.4489795918367347,
          "f": 0.31654675802494703
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2501.01454v2",
      "true_abstract": "Infectious diseases remain a critical global health challenge, and the\nintegration of standardized ontologies plays a vital role in managing related\ndata. The Infectious Disease Ontology (IDO) and its extensions, such as the\nCoronavirus Infectious Disease Ontology (CIDO), are essential for organizing\nand disseminating information related to infectious diseases. The COVID-19\npandemic highlighted the need for updating IDO and its virus-specific\nextensions. There is an additional need to update IDO extensions specific to\nbacteria, fungus, and parasite infectious diseases. We adopt the \"hub and\nspoke\" methodology to generate pathogen-specific extensions of IDO: Virus\nInfectious Disease Ontology (VIDO), Bacteria Infectious Disease Ontology\n(BIDO), Mycosis Infectious Disease Ontology (MIDO), and Parasite Infectious\nDisease Ontology (PIDO). The creation of pathogen-specific reference ontologies\nadvances modularization and reusability of infectious disease data within the\nIDO ecosystem. Future work will focus on further refining these ontologies,\ncreating new extensions, and developing application ontologies based on them,\nin line with ongoing efforts to standardize biological and biomedical\nterminologies for improved data sharing and analysis.",
      "generated_abstract": "old Pathogen Reference Ontology (FP4O) is a consensus-based,\nresource-oriented, and flexible ontology that supports the representation,\nclassification, and integration of pathogenic biological entities. It is the\nfirst pathogen ontology to address the diverse needs of researchers, clinicians,\nand public health professionals. FP4O offers a unified vocabulary for\nclassifying and annotating biological entities such as pathogens, infectious\ndiseases, and associated diseases. The FP4O ontology provides a foundation for\ndeveloping new biomedical resources and tools that address the needs of\nresearchers, clinicians, and public health professionals. This paper presents\nthe FP4O suite, which includes a vocabulary, ontology graph, and ontology\nannotation tools. The suite provides an integrated framework for the representation,\nclass",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18867924528301888,
          "p": 0.2857142857142857,
          "f": 0.2272727224819216
        },
        "rouge-2": {
          "r": 0.01948051948051948,
          "p": 0.030303030303030304,
          "f": 0.023715410256058698
        },
        "rouge-l": {
          "r": 0.16981132075471697,
          "p": 0.2571428571428571,
          "f": 0.20454544975464886
        }
      }
    },
    {
      "paper_id": "math.LO.math/LO/2503.03551v1",
      "true_abstract": "This is the second of three papers motivated by the author's desire to\nunderstand and explain \"algebraically\" one aspect of Dmitriy Zhuk's proof of\nthe CSP Dichotomy Theorem. In this paper we extend Zhuk's \"bridge\" construction\nto arbitrary meet-irreducible congruences of finite algebras in locally finite\nvarieties with a Taylor term. We then connect bridges to centrality and\nsimilarity. In particular, we prove that Zhuk's bridges and our \"proper\nbridges\" (defined in our first paper) convey the same information in locally\nfinite Taylor varieties.",
      "generated_abstract": "We prove a result of Zhuk concerning the number of bridges between two\ngroups of positive rank, which generalizes a result of Pillay, and we use this\nresult to show that a group has a centralizer if and only if it has a bridge to\nany group of positive rank.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18032786885245902,
          "p": 0.3235294117647059,
          "f": 0.23157894277229923
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14754098360655737,
          "p": 0.2647058823529412,
          "f": 0.18947367961440453
        }
      }
    },
    {
      "paper_id": "cs.MM.cs/MA/2503.09448v1",
      "true_abstract": "Proactive virtual reality (VR) streaming requires users to upload\nviewpoint-related information, raising significant privacy concerns. Existing\nstrategies preserve privacy by introducing errors to viewpoints, which,\nhowever, compromises the quality of experience (QoE) of users. In this paper,\nwe first delve into the analysis of the viewpoint leakage probability achieved\nby existing privacy-preserving approaches. We determine the optimal\ndistribution of viewpoint errors that minimizes the viewpoint leakage\nprobability. Our analyses show that existing approaches cannot fully eliminate\nviewpoint leakage. Then, we propose a novel privacy-preserving approach that\nintroduces noise to uploaded viewpoint prediction errors, which can ensure zero\nviewpoint leakage probability. Given the proposed approach, the tradeoff\nbetween privacy preservation and QoE is optimized to minimize the QoE loss\nwhile satisfying the privacy requirement. Simulation results validate our\nanalysis results and demonstrate that the proposed approach offers a promising\nsolution for balancing privacy and QoE.",
      "generated_abstract": "oncerns have become increasingly prevalent in Virtual Reality\n(VR) environments. This paper investigates how to balance Quality of Experience\n(QoE) and Privacy in VR streaming. To this end, we propose a novel privacy\nmanagement strategy that leverages a privacy-preserving QoE model and a\nproactive video streaming algorithm. Our proposed solution allows the user to\nopt for a QoE-privacy tradeoff by selecting a set of video quality levels and\nvideo resolutions based on their preference. Furthermore, we design an\nautonomous algorithm that dynamically adjusts the video streaming parameters\nbased on user interactions, such as viewing behavior, to ensure privacy\nprotection. The proposed solution is evaluated using an extensive set of\nexperiments, including an evaluation of privacy and video quality tradeoffs,\nas well as user interactions. The results demonstrate that our proposed\nsolution",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2916666666666667,
          "p": 0.3111111111111111,
          "f": 0.30107526382240724
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.04065040650406504,
          "f": 0.03875968493329792
        },
        "rouge-l": {
          "r": 0.2708333333333333,
          "p": 0.28888888888888886,
          "f": 0.2795698874783212
        }
      }
    },
    {
      "paper_id": "cs.GR.eess/IV/2503.02218v1",
      "true_abstract": "Purpose: This study proposes a novel anatomically-driven dynamic modeling\nframework for coronary arteries using skeletal skinning weights computation,\naiming to achieve precise control over vessel deformation while maintaining\nreal-time performance for surgical simulation applications. Methods: We\ndeveloped a computational framework based on biharmonic energy minimization for\nskinning weight calculation, incorporating volumetric discretization through\ntetrahedral mesh generation. The method implements temporal sampling and\ninterpolation for continuous vessel deformation throughout the cardiac cycle,\nwith mechanical constraints and volume conservation enforcement. The framework\nwas validated using clinical datasets from 5 patients, comparing interpolated\ndeformation results against ground truth data obtained from frame-by-frame\nsegmentation across cardiac phases. Results: The proposed framework effectively\nhandled interactive vessel manipulation. Geometric accuracy evaluation showed\nmean Hausdorff distance of 4.96 +- 1.78 mm and mean surface distance of 1.78 +-\n0.75 mm between interpolated meshes and ground truth models. The Branch\nCompleteness Ratio achieved 1.82 +- 0.46, while Branch Continuity Score\nmaintained 0.84 +- 0.06 (scale 0-1) across all datasets. The system\ndemonstrated capability in supporting real-time guidewire-vessel collision\ndetection and contrast medium flow simulation throughout the complete coronary\ntree structure. Conclusion: Our skinning weight-based methodology enhances\nmodel interactivity and applicability while maintaining geometric accuracy. The\nframework provides a more flexible technical foundation for virtual surgical\ntraining systems, demonstrating promising potential for both clinical practice\nand medical education applications. The code is available at\nhttps://github.com/ipoirot/DynamicArtery.",
      "generated_abstract": "eld of surgical training, deformable skins play an essential role\nin virtual simulation of surgical procedures. However, most existing deformable\nskins focus on static deformation, which does not account for the dynamic\ndeformation in surgical training. In this paper, we propose a novel dynamic\nskinning framework that can handle the deformation of the human body during\nsurgical training. The proposed method is composed of two key components:\nskinning and deformation. First, the skinning module utilizes a novel deformation\nconstraint to model the dynamic deformation of the human body during surgical\ntraining. Second, the deformation module leverages the skinning result to\ngenerate the deformed skin. Extensive experiments demonstrate that the proposed\nframework can effectively simulate the dynamic deformation of the human body\nduring surgical training. The proposed framework can be applied to various\nsurgical procedures, such as open",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13372093023255813,
          "p": 0.2948717948717949,
          "f": 0.1839999957068801
        },
        "rouge-2": {
          "r": 0.017777777777777778,
          "p": 0.037037037037037035,
          "f": 0.02402401964126368
        },
        "rouge-l": {
          "r": 0.13372093023255813,
          "p": 0.2948717948717949,
          "f": 0.1839999957068801
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.02850v1",
      "true_abstract": "The comparison of different medical treatments from observational studies or\nacross different clinical studies is often biased by confounding factors such\nas systematic differences in patient demographics or in the inclusion criteria\nfor the trials. Propensity score matching is a popular method to adjust for\nsuch confounding. It compares weighted averages of patient responses. The\nweights are calculated from logistic regression models with the intention to\nreduce differences between the confounders in the treatment groups. However,\nthe groups are only \"roughly matched\" with no generally accepted principle to\ndetermine when a match is \"good enough\".\n  In this manuscript, we propose an alternative approach to the matching\nproblem by considering it as a constrained optimization problem. We investigate\nthe conditions for exact matching in the sense that the average values of\nconfounders are identical in the treatment groups after matching. Our approach\nis similar to the matching-adjusted indirect comparison approach by\nSignorovitch et al. (2010) but with two major differences: First, we do not\nimpose any specific functional form on the matching weights; second, the\nproposed approach can be applied to individual patient data from several\ntreatment groups as well as to a mix of individual patient and aggregated data.",
      "generated_abstract": "er matching methods for the purpose of causal inference in\nparticular clinical settings where the outcome of interest is disease\nprogression. This includes the detection of recurrent events, such as\nprogression of breast cancer to metastatic disease or progression of sepsis to\norgan dysfunction. In such settings, it is often unclear whether an event\noccurred due to exposure to a treatment or a different exposure. A widely used\nstrategy in such settings is to use the propensity score as an alternative to\nmatching. In this paper, we propose to use matching in the context of\npropensity score matching, known as matching as an alternative to propensity\nscore matching, where we match on the propensity score and use the resulting\nmatched pairs as the data for the causal inference. This approach is based on\nthe fact that, in the propensity score matching setting, the conditional",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.3684210526315789,
          "f": 0.27722771807861984
        },
        "rouge-2": {
          "r": 0.06349206349206349,
          "p": 0.09523809523809523,
          "f": 0.0761904713904765
        },
        "rouge-l": {
          "r": 0.1984126984126984,
          "p": 0.32894736842105265,
          "f": 0.24752474778159012
        }
      }
    },
    {
      "paper_id": "econ.GN.stat/AP/2502.18253v1",
      "true_abstract": "Participants in online experiments often enroll over time, which can\ncompromise sample representativeness due to temporal shifts in covariates. This\nissue is particularly critical in A/B tests, online controlled experiments\nextensively used to evaluate product updates, since these tests are\ncost-sensitive and typically short in duration. We propose a novel framework\nthat dynamically assesses sample representativeness by dividing the ongoing\nsampling process into three stages. We then develop stage-specific estimators\nfor Population Average Treatment Effects (PATE), ensuring that experimental\nresults remain generalizable across varying experiment durations. Leveraging\nsurvival analysis, we develop a heuristic function that identifies these stages\nwithout requiring prior knowledge of population or sample characteristics,\nthereby keeping implementation costs low. Our approach bridges the gap between\nexperimental findings and real-world applicability, enabling product decisions\nto be based on evidence that accurately represents the broader target\npopulation. We validate the effectiveness of our framework on three levels: (1)\nthrough a real-world online experiment conducted on WeChat; (2) via a synthetic\nexperiment; and (3) by applying it to 600 A/B tests on WeChat in a\nplatform-wide application. Additionally, we provide practical guidelines for\npractitioners to implement our method in real-world settings.",
      "generated_abstract": "We propose a novel approach to enhance the external validity of experiments\nby incorporating ongoing sampling in the experimental design. Our method\nutilizes the structural properties of the experimental design to generate\nsamples from the same distribution as those in the experiment. We show that our\nmethod can enhance the external validity of experiments, while maintaining the\nsampling consistency and efficiency of the experiment. We also provide theoretical\nguarantees on the sample quality of our method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1702127659574468,
          "p": 0.4897959183673469,
          "f": 0.2526315751196676
        },
        "rouge-2": {
          "r": 0.031746031746031744,
          "p": 0.09375,
          "f": 0.04743082626005748
        },
        "rouge-l": {
          "r": 0.16312056737588654,
          "p": 0.46938775510204084,
          "f": 0.242105259330194
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.cond-mat/str-el/2503.09692v1",
      "true_abstract": "We present a model for interacting electrons in a continuum band structure\nthat resembles a ``trashcan'', with a flat bottom of radius $k_b$ beyond which\nthe dispersion increases rapidly with velocity $v$. The form factors of the\nBloch wavefunctions can be well-approximated by the Girvin-MacDonald-Platzman\nalgebra, which encodes the uniform Berry curvature. We demonstrate how this\nmodel captures the salient features of the low-energy Hamiltonian for\nelectron-doped pristine $n$-layer rhombohedral graphene (R$n$G) for appropriate\nvalues of the displacement field, and provide corresponding expressions for\n$k_b$. In the regime where the Fermi wavevector is close to $k_b$, we\nanalytically solve the Hartree-Fock equations for a gapped Wigner crystal in\nseveral limits of the model. We introduce a new method, the sliver-patch\napproximation, which extends the previous few-patch approaches and is crucial\nin both differentiating even vs odd Chern numbers of the ground state and\ngapping the Hartree-Fock solution. A key parameter is the Berry flux\n$\\varphi_{\\text{BZ}}$ enclosed by the (flat) bottom of the band. We\nanalytically show that there is a ferromagnetic coupling between the signs of\n$\\varphi_{\\text{BZ}}$ and the Chern number $C$ of the putative Wigner crystal.\nWe also study the competition between the $C=0$ and $1$ solutions as a function\nof the interaction potential for parameters relevant to R$n$G. By exhaustive\ncomparison to numerical Hartree-Fock calculations, we demonstrate how the\nanalytic results capture qualitative trends of the phase diagram, as well as\nquantitative details such as the enhancement of the effective velocity. Our\nanalysis paves the way for an analytic and numerical examination of the\nstability and competition beyond mean-field theory of the Wigner crystals in\nthis model.",
      "generated_abstract": "is a two-dimensional honeycomb lattice of carbon atoms, with\na flat surface and a hexagonal crystal structure. Graphene is a highly\ndisordered system and exhibits various unusual properties such as the\nexistence of a Berry curvature, which can be used to describe the electronic\nbehavior in this material. The Berry curvature is a manifestation of the\nspin-orbit coupling that arises from the interplay between the spin of the\nelectron and the spin of the carbon atoms. This phenomenon is not observed in\nordinary crystalline materials and can be exploited to construct quantum\ndevices. In this work, we explore the electronic behavior of interacting\nelectrons in the Berry trashcan model, which describes the interaction of the\nelectron with the flat surface of graphene. We show that the Berry curvature\ncan be used to describe the electronic behavior of interacting",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19135802469135801,
          "p": 0.4025974025974026,
          "f": 0.2594142215738521
        },
        "rouge-2": {
          "r": 0.064,
          "p": 0.14035087719298245,
          "f": 0.08791208361007145
        },
        "rouge-l": {
          "r": 0.1728395061728395,
          "p": 0.36363636363636365,
          "f": 0.2343096190633918
        }
      }
    },
    {
      "paper_id": "physics.chem-ph.physics/chem-ph/2503.08435v1",
      "true_abstract": "We show how small molecules in an intense and cold molecular beam can be\nhighly nuclear-spin polarized via microwave or infrared rotational excitation\nschemes, followed by hyperfine-induced quantum beats. Repumping schemes can be\nused to achieve polarization above $90\\%$ in cases where single-pumping schemes\nare insufficient. Projected production rates in excess of $10^{21}$\n${\\text{s}^{-1}}$ allow applications including nuclear-magnetic-resonance\nsignal enhancement, and spin-polarized nuclear fusion, where polarized nuclei\nare known to enhance D-T and D-$^3$He fusion cross sections by $50\\%$.",
      "generated_abstract": "We present a detailed theoretical study of the generation of spin-polarized\nmolecular beams by microwave or infrared excitation. A simple model is used to\nanalyze the excitation of the ground-state rotational levels of a molecule,\nwhich are characterized by angular momentum quantum numbers $J$ and $M$. The\nmodel predicts the formation of a single spin-polarized molecular beam in the\nlimit of zero magnetic field, in which the excitation is polarized in the\n$J_M$ direction. The theoretical predictions are confirmed by experiments on\ntwo molecular beams, and we discuss the possible experimental realizations of\nthese spin-polarized molecular beams.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26865671641791045,
          "p": 0.2857142857142857,
          "f": 0.2769230719278107
        },
        "rouge-2": {
          "r": 0.05194805194805195,
          "p": 0.0449438202247191,
          "f": 0.04819276611046648
        },
        "rouge-l": {
          "r": 0.2537313432835821,
          "p": 0.2698412698412698,
          "f": 0.26153845654319535
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.02285v1",
      "true_abstract": "Monitoring a process/phenomenon of specific interest is prevalent in\nCyber-Physical Systems (CPS), remote healthcare, smart buildings, intelligent\ntransport, industry 4.0, etc. A key building block of the monitoring system is\na sensor sampling the process and communicating the status updates to a monitor\nfor detecting events of interest. Measuring the freshness of the status updates\nis essential for the timely detection of events, and it has received\nsignificant research interest in recent times. In this paper, we propose a new\nfreshness metric, Age of Detection (AoD), for monitoring the state transitions\nof a Discrete Time Markov Chain (DTMC) source over a lossy wireless channel. We\nconsider the pull model where the sensor samples DTMC state whenever the\nmonitor requests a status update. We formulate a Constrained Markov Decision\nProblem (CMDP) for optimising the AoD subject to a constraint on the average\nsampling frequency and solve it using the Lagrangian MDP formulation and\nRelative Value Iteration (RVI) algorithm. Our numerical results show\ninteresting trade-offs between AoD, sampling frequency, and transmission\nsuccess probability. Further, the AoD minimizing policy provides a lower\nestimation error than the Age of Information (AoI) minimizing policy, thus\ndemonstrating the utility of AoD for monitoring DTMC sources.",
      "generated_abstract": "In this work, we consider a source with a Markov structure and a lossy\nchannel. Our goal is to maximize the probability of detection (PoD) while\nminimizing the average age of detection (AOD). A key feature of the source is\nthat it is not entirely known, and the source's state is updated at each\nsuccessive detection. This leads to an infinite-dimensional optimization\nproblem. We derive a necessary and sufficient condition for the PoD to be\nmaximized and show that the optimal PoD value is the minimum AOD value. Our\nfindings highlight the trade-off between detecting the source at a young age\n(AOD) and detecting the source at a later age (PoD).",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2074074074074074,
          "p": 0.4057971014492754,
          "f": 0.2745097994449251
        },
        "rouge-2": {
          "r": 0.03125,
          "p": 0.057692307692307696,
          "f": 0.04054053598246947
        },
        "rouge-l": {
          "r": 0.18518518518518517,
          "p": 0.36231884057971014,
          "f": 0.2450980347390427
        }
      }
    },
    {
      "paper_id": "math.OC.stat/AP/2502.18332v1",
      "true_abstract": "National teams from different continents can play against each other only in\nafew sports competitions. Therefore, a reasonable aim is maximising the number\nof intercontinental games in world cups, as done in basketball and football, in\ncontrast to handball and volleyball. However, this objective requires\nadditional draw constraints that imply the violation of equal treatment. In\naddition, the standard draw mechanism is non-uniformly distributed on the set\nof valid assignments, which may lead to further distortions. Our paper analyses\nthis novel trade-off between attractiveness and fairness through the example of\nthe 2025 World Men's Handball Championship. We introduce a measure of\ninequality, which enables considering 32 sets of reasonable geographical\nrestrictions to determine the Pareto frontier. The proposed methodology can be\nused by policy-makers to select the optimal set of draw constraints.",
      "generated_abstract": "We study a group-draw-based model for attractiveness. Drawing a group from\n$G$ and applying the attractiveness to a single individual in the group yields a\nmeasure of attractiveness for the individual. We characterize the attractiveness\nof a group, and show that it is a linear combination of the attractiveness of\nthe individuals in the group. We then study the effect of equal treatment on\nindividual attractiveness. We show that equal treatment results in a linear\ncombination of the attractiveness of the individuals in the group. This is the\nfirst time that an attractiveness measure has been analyzed in a group-draw\nmodel.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1485148514851485,
          "p": 0.3409090909090909,
          "f": 0.20689654749678962
        },
        "rouge-2": {
          "r": 0.03875968992248062,
          "p": 0.0625,
          "f": 0.04784688522698703
        },
        "rouge-l": {
          "r": 0.13861386138613863,
          "p": 0.3181818181818182,
          "f": 0.1931034440485138
        }
      }
    },
    {
      "paper_id": "nlin.PS.nlin/PS/2503.06289v1",
      "true_abstract": "We study a nonlinear magnetic metamaterial modeled as a split-ring resonator\narray, where the standard discrete laplacian is replaced by its fractional\nform. We find a closed-form expression for the dispersion relation as a\nfunction of the fractional exponent s and the gain/loss parameter {\\gamma} and\nexamine the conditions under which stable magneto-inductive waves exist. The\ndensity of states is computed in closed form and suggests that the main effect\nof fractionality is the flattening of the bands, while gain/loss increase tends\nto reduce the bandgaps. The spatial extent of the modes for a finite array is\ncomputed by means of the participation ratio R, which is also obtained in\nclosed form. For a fixed fractionality exponent, an increase in gain/loss\n{\\gamma} decreases the overall R, from the number of sites N towards N/2 at\nlarge {\\gamma}. The nonlinear dynamics of the average magnetic energy on an\ninitial ring during a cycle shows a monotonic increase with {\\gamma}, and it is\nqualitatively similar for all fractional exponents. This is explained as mainly\ndue to the interplay of nonlinearity and PT symmetry.",
      "generated_abstract": "pt of fractional symmetry is gaining momentum in the physics\ncommunities, as it is closely related to topological phenomena and is the basis\nfor a wealth of new ideas. This work explores the interplay between fractional\nsymmetry and the P\\\"{o}ringer-Tomonberg (PT) symmetry, which is known to\nenhance the magnetic response of magnetic materials. We study the effect of\nfractional symmetry on the properties of a magnetic metamaterial composed of\nFerroelectric-Thermoelectric (FET) layers. The metamaterial consists of\nmagnetic layers that can be switched on and off by applying a voltage, which\nleads to a fractional magnetic response. Our results show that PT symmetry\nallows for a more pronounced fractionalization of the magnetic response,\nincreasing the robustness of the metamaterial against external perturbations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22123893805309736,
          "p": 0.33783783783783783,
          "f": 0.26737967436186344
        },
        "rouge-2": {
          "r": 0.06395348837209303,
          "p": 0.09821428571428571,
          "f": 0.07746478395556468
        },
        "rouge-l": {
          "r": 0.21238938053097345,
          "p": 0.32432432432432434,
          "f": 0.25668448719608805
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.06776v1",
      "true_abstract": "We address safe multi-robot interaction under uncertainty. In particular, we\nformulate a chance-constrained linear quadratic Gaussian game with coupling\nconstraints and system uncertainties. We find a tractable reformulation of the\ngame and propose a dual ascent algorithm. We prove that the algorithm converges\nto a generalized Nash equilibrium of the reformulated game, ensuring the\nsatisfaction of the chance constraints. We test our method in driving\nsimulations and real-world robot experiments. Our method ensures safety under\nuncertainty and generates less conservative trajectories than single-agent\nmodel predictive control.",
      "generated_abstract": "This paper introduces a novel chance-constrained linear quadratic Gaussian\n(LQG) game for multi-robot interaction under uncertainty. The game is\nconstructed by combining a linear LQG game and a chance constrained\nquadratic programming (CCQP) game. To ensure the game's solvability, we\nintroduce a new class of convex problems, namely, chance constrained linear\nquadratic games with bounded uncertainty. The CCQP game is solved via\niterative quadratic programming (IQP). The CCQP game is solved using a\ndecentralized distributed optimization algorithm. The decentralized distributed\noptimization algorithm is derived by combining the IQP algorithm and\ndistributed convex optimization. The proposed algorithms are proven to be\nconvergent and solve the game within the specified constraints. Simulation\nresults demonstrate the effectiveness of the proposed algorithms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2923076923076923,
          "p": 0.2714285714285714,
          "f": 0.28148147648834027
        },
        "rouge-2": {
          "r": 0.10975609756097561,
          "p": 0.08737864077669903,
          "f": 0.09729729236172413
        },
        "rouge-l": {
          "r": 0.2923076923076923,
          "p": 0.2714285714285714,
          "f": 0.28148147648834027
        }
      }
    },
    {
      "paper_id": "math.AP.math/CA/2503.05140v1",
      "true_abstract": "In this paper, we investigate the mixed norm estimates for the operator $ T\n$associated with a dilated plane curve $(ut, u\\gamma(t))$, defined by \\[ Tf(x,\nu) := \\int_{0}^{1} f(x_1 - ut, x_2 - u\\gamma(t)) \\, dt, \\] where $ x := (x_1,\nx_2) $ and $\\gamma $ is a general plane curve satisfying appropriate smoothness\nand curvature conditions. Our results partially address a problem posed by\nHickman [J. Funct. Anal. 2016] in the two-dimensional setting. More precisely,\nwe establish the $ L_x^p(\\mathbb{R}^2) \\rightarrow L_x^q L_u^r(\\mathbb{R}^2\n\\times [1, 2]) $ (space-time) estimates for $ T $, whenever\n$(\\frac{1}{p},\\frac{1}{q})$ satisfy \\[ \\max\\left\\{0, \\frac{1}{2p} -\n\\frac{1}{2r}, \\frac{3}{p} - \\frac{r+2}{r}\\right\\} < \\frac{1}{q} \\leq\n\\frac{1}{p} < \\frac{r+1}{2r} \\] and $$1 + (1 + \\omega)\\left(\\frac{1}{q} -\n\\frac{1}{p}\\right) > 0,$$ where $ r \\in [1, \\infty] $ and $ \\omega :=\n\\limsup_{t \\rightarrow 0^+} \\frac{\\ln|\\gamma(t)|}{\\ln t} $. These results are\nsharp, except for certain borderline cases. Additionally, we examine the $\nL_x^p(\\mathbb{R}^2) \\rightarrow L_u^r L_x^q(\\mathbb{R}^2 \\times [1, 2]) $\n(time-space) estimates for $T $, which are especially almost sharp when $p=2$.",
      "generated_abstract": "We prove mixed norm estimates for dilated averages $a_t(z):=\\int_0^z\na(s)ds$, where $a$ is a holomorphic function defined on a domain $D$ and\n$z\\in D$, and $a_t$ is the dilated version of $a$ defined on a circle of\nradius $t$. The results generalize the estimates in [BM21",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10655737704918032,
          "p": 0.37142857142857144,
          "f": 0.16560509207675775
        },
        "rouge-2": {
          "r": 0.024691358024691357,
          "p": 0.09302325581395349,
          "f": 0.039024386928733175
        },
        "rouge-l": {
          "r": 0.09016393442622951,
          "p": 0.3142857142857143,
          "f": 0.14012738507038836
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2402.14161v1",
      "true_abstract": "We derive the short-maturity asymptotics for option prices in the local\nvolatility model in a new short-maturity limit $T\\to 0$ at fixed $\\rho = (r-q)\nT$, where $r$ is the interest rate and $q$ is the dividend yield. In cases of\npractical relevance $\\rho$ is small, however our result holds for any fixed\n$\\rho$. The result is a generalization of the Berestycki-Busca-Florent formula\nfor the short-maturity asymptotics of the implied volatility which includes\ninterest rates and dividend yield effects of $O(((r-q) T)^n)$ to all orders in\n$n$. We obtain analytical results for the ATM volatility and skew in this\nasymptotic limit. Explicit results are derived for the CEV model. The\nasymptotic result is tested numerically against exact evaluation in the\nsquare-root model model $\\sigma(S)=\\sigma/\\sqrt{S}$, which demonstrates that\nthe new asymptotic result is in very good agreement with exact evaluation in a\nwide range of model parameters relevant for practical applications.",
      "generated_abstract": "This paper studies the short-maturity asymptotics for option prices under\ninterest rate effects, including the case of short-maturity interest rates\npositive or negative. The result shows that, for all maturities in the range\n$[1,2",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17045454545454544,
          "p": 0.5172413793103449,
          "f": 0.2564102526817153
        },
        "rouge-2": {
          "r": 0.06666666666666667,
          "p": 0.2727272727272727,
          "f": 0.10714285398596948
        },
        "rouge-l": {
          "r": 0.17045454545454544,
          "p": 0.5172413793103449,
          "f": 0.2564102526817153
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.06673v1",
      "true_abstract": "We initiate systematic study of EZ-structures (and associated boundaries) of\ngroups acting on spaces that admit consistent and conical (equivalently,\nconsistent and convex) geodesic bicombings. Such spaces recently drew a lot of\nattention due to the fact that many classical groups act `nicely' on them. We\nrigorously construct EZ-structures, discuss their uniqueness (up to\nhomeomorphism), provide examples, and prove some boundary-related features\nanalogous to the ones exhibited by CAT(0) spaces and groups, which form a\nsubclass of the discussed class of spaces and groups.",
      "generated_abstract": "A space is bicombable if every closed bounded subset is compact. In this\npaper we prove that the boundary of a bicombable space is bicombable. In the\nprocess we also show that every bicombable space is homeomorphic to a product of\ntwo finite metric spaces, and we provide examples of bicombable spaces\nwhich are not homeomorphic to products of finite metric spaces.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16129032258064516,
          "p": 0.2631578947368421,
          "f": 0.1999999952880001
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14516129032258066,
          "p": 0.23684210526315788,
          "f": 0.17999999528800012
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.06528v1",
      "true_abstract": "This paper integrates the damped harmonic oscillator into DSGE models to\nbetter capture delayed economic adjustments. By introducing a damping\ncoefficient, I model economic recoveries as under-damped, critically damped, or\nover-damped processes. Numerical simulations illustrate how different damping\nlevels affect recovery speed and stability. This approach enhances DSGE models'\nrealism, offering insights into historical economic crises and improving\nmacroeconomic forecasting.",
      "generated_abstract": "We propose a stochastic differential equation (SDE) model to incorporate\ndamped harmonic oscillator (DHO) in the framework of dynamic stochastic\ngeneral equilibrium (DSGE) models. The DHO is used to model the impact of\nenvironmental factors on the demand for goods and services. The model is\ndeveloped for the period from 2000 to 2023. The results show that DHO can\naccurately represent the impact of environmental factors on the demand for\ngoods and services. The results also show that the DHO can effectively\nrepresent the impact of environmental factors on the demand for goods and\nservices, which is a significant improvement over traditional models that only\nconsider the impact of environmental factors on economic growth.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18867924528301888,
          "p": 0.1694915254237288,
          "f": 0.1785714235857782
        },
        "rouge-2": {
          "r": 0.03389830508474576,
          "p": 0.024390243902439025,
          "f": 0.028368789459283568
        },
        "rouge-l": {
          "r": 0.18867924528301888,
          "p": 0.1694915254237288,
          "f": 0.1785714235857782
        }
      }
    },
    {
      "paper_id": "math.DS.math/CA/2503.07508v1",
      "true_abstract": "This paper relates to the Fourier decay properties of images of self-similar\nmeasures $\\mu$ on $\\mathbb{R}^k$ under nonlinear smooth maps $f \\colon\n\\mathbb{R}^k \\to \\mathbb{R}$. For example, we prove that if the linear parts of\nthe similarities defining $\\mu$ commute and the graph of $f$ has nonvanishing\nGaussian curvature, then the Fourier dimension of the image measure is at least\n$\\max\\left\\{ \\frac{2(2\\kappa_2 - k)}{4 + 2\\kappa_* - k} , 0 \\right\\}$, where\n$\\kappa_2$ is the lower correlation dimension of $\\mu$ and $\\kappa_*$ is the\nAssouad dimension of the support of $\\mu$. Under some additional assumptions on\n$\\mu$, we use recent breakthroughs in the fractal uncertainty principle to\nobtain further improvements for the decay exponents.\n  We give several applications to nonlinear arithmetic of self-similar sets $F$\nin the line. For example, we prove that if $\\dim_{\\mathrm H} F > (\\sqrt{65} -\n5)/4 = 0.765\\dots$ then the arithmetic product set $F \\cdot F = \\{ xy : x,y \\in\nF \\}$ has positive Lebesgue measure, while if $\\dim_{\\mathrm H} F > (-3 +\n\\sqrt{41})/4 = 0.850\\dots$ then $F \\cdot F \\cdot F$ has non-empty interior. One\nfeature of the above results is that they do not require any separation\nconditions on the self-similar sets.",
      "generated_abstract": "We consider the Fourier transform of nonlinear images of self-similar\nmeasures. We show that the Fourier transform of the image of a self-similar\nmeasure is an eigenfunction of the Laplace operator on the unit sphere of\n$\\mathbb{R}^d$. In particular, we prove that the Fourier transform of the image\nof a self-similar measure is a Dirac delta function when the self-similar\nmeasure is a Dirac delta measure. We also show that the Fourier transform of the\nimage of a self-similar measure is a Dirac delta function when the self-similar\nmeasure is a Dirac delta measure on a set of positive measure. This result\nextends a theorem of Kac, which was the first to show that the Fourier\ntransform of the image of a self-similar measure is a Dirac delta function when\nthe self-similar measure is a Dirac delta measure.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14615384615384616,
          "p": 0.4318181818181818,
          "f": 0.21839080081913073
        },
        "rouge-2": {
          "r": 0.061452513966480445,
          "p": 0.16417910447761194,
          "f": 0.08943089034536338
        },
        "rouge-l": {
          "r": 0.14615384615384616,
          "p": 0.4318181818181818,
          "f": 0.21839080081913073
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.15035v1",
      "true_abstract": "Hair follicles constantly cycle through phases of growth, regression and\nrest, as matrix keratinocytes (MKs), the cells producing hair fibers,\nproliferate, and then undergo spontaneous apoptosis. Damage to MKs and\nperturbations in their normal dynamics result in a shortened growth phase,\nleading to hair loss. Two common factors causing such disruption are hormonal\nimbalance and attacks by the immune system. Androgenetic alopecia (AGA) is hair\nloss caused by high sensitivity to androgens, and alopecia areata (AA) is hair\nloss caused by an autoimmune reaction against MKs. In this study, we inform a\nmathematical model for the human hair cycle with experimental data for the\nlengths of hair cycle phases available from male control subjects and subjects\nwith AGA. We also, connect a mathematical model for AA with estimates for the\nduration of hair cycle phases obtained from the literature. Subsequently, with\neach model we perform parameter screening, uncertainty quantification and\nglobal sensitivity analysis and compare the results within and between the\ncontrol and AGA subject groups as well as among AA, control and AGA conditions.\nThe findings reveal that in AGA subjects there is greater uncertainty\nassociated with the duration of hair growth than in control subjects and that,\ncompared to control and AGA conditions, in AA it is more certain that longer\nhair growth phase could not be expected. The comparison of results also\nindicates that in AA lower proliferation of MKs and weaker communication of the\ndermal papilla with MKs via signaling molecules could be expected than in\nnormal and AGA conditions, and in AA stronger inhibition of MK proliferation by\nregulatory molecules could be expected than in AGA. Finally, the global\nsensitivity analysis highlights the process of MK apoptosis as highly impactful\nfor the length of hair growth only in the AA case, but not for control and AGA\nconditions.",
      "generated_abstract": "th is a complex biological process involving many cellular,\ndynamic and nonlinear interactions. In this study, we aim to analyze the\nuncertainty in the hair growth duration of normal and alopecic human scalp\nfollicles, based on an unbiased experimental design. In particular, we focus on\nthe effect of the follicular environment on the hair growth rate and its\nuncertainty. The main contributions of this study include: (1) the development\nof a new experimental design, which allows for the quantification of the\nuncertainty in the hair growth rate; (2) a sensitivity analysis to assess the\neffect of parameters on the calculated uncertainty; (3) a detailed analysis of\nthe hair growth rate in alopecic and normal scalp follicles; and (4) the\ninvestigation of the influence of the follicular environment on the hair\ngrowth rate, including",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15822784810126583,
          "p": 0.33783783783783783,
          "f": 0.21551723703478012
        },
        "rouge-2": {
          "r": 0.0392156862745098,
          "p": 0.09259259259259259,
          "f": 0.05509641455274034
        },
        "rouge-l": {
          "r": 0.14556962025316456,
          "p": 0.3108108108108108,
          "f": 0.1982758577244353
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/RO/2503.10554v1",
      "true_abstract": "The evolution from motion capture and teleoperation to robot skill learning\nhas emerged as a hotspot and critical pathway for advancing embodied\nintelligence. However, existing systems still face a persistent gap in\nsimultaneously achieving four objectives: accurate tracking of full upper limb\nmovements over extended durations (Accuracy), ergonomic adaptation to human\nbiomechanics (Comfort), versatile data collection (e.g., force data) and\ncompatibility with humanoid robots (Versatility), and lightweight design for\noutdoor daily use (Convenience). We present a wearable exoskeleton system,\nincorporating user-friendly immersive teleoperation and multi-modal sensing\ncollection to bridge this gap. Due to the features of a novel shoulder\nmechanism with synchronized linkage and timing belt transmission, this system\ncan adapt well to compound shoulder movements and replicate 100% coverage of\nnatural upper limb motion ranges. Weighing 5.2 kg, NuExo supports backpack-type\nuse and can be conveniently applied in daily outdoor scenarios. Furthermore, we\ndevelop a unified intuitive teleoperation framework and a comprehensive data\ncollection system integrating multi-modal sensing for various humanoid robots.\nExperiments across distinct humanoid platforms and different users validate our\nexoskeleton's superiority in motion range and flexibility, while confirming its\nstability in data collection and teleoperation accuracy in dynamic scenarios.",
      "generated_abstract": "growth of humanoid robotics requires robust and flexible\nhuman-robot interaction (HRI) solutions. Existing exoskeletons can be\noverwhelmingly expensive, limiting their use in commercial applications. We\nintroduce NuExo, a wearable exoskeleton covering all upper limb muscles and\nproviding precise control of the shoulder, elbow, wrist, and finger joints.\nThis exoskeleton integrates a wearable force platform (WFP), a\nmultibody-model-based controller, and a customized software package for\nreal-time human-robot interaction (HRI). The exoskeleton is designed for\ncommercial use and can be used in a wide range of applications, such as\noutdoor data collection, indoor teleoperation, and task execution in dynamic\nenvironments. To demonstrate the NuExo's versatility,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15714285714285714,
          "p": 0.28205128205128205,
          "f": 0.2018348577897484
        },
        "rouge-2": {
          "r": 0.042328042328042326,
          "p": 0.08163265306122448,
          "f": 0.05574912442253797
        },
        "rouge-l": {
          "r": 0.15,
          "p": 0.2692307692307692,
          "f": 0.19266054586314293
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.13868v1",
      "true_abstract": "Policy makers need to decide whether to treat or not to treat heterogeneous\nindividuals. The optimal treatment choice depends on the welfare function that\nthe policy maker has in mind and it is referred to as the policy learning\nproblem. I study a general setting for policy learning with semiparametric\nSocial Welfare Functions (SWFs) that can be estimated by locally\nrobust/orthogonal moments based on U-statistics. This rich class of SWFs\nsubstantially expands the setting in Athey and Wager (2021) and accommodates a\nwider range of distributional preferences. Three main applications of the\ngeneral theory motivate the paper: (i) Inequality aware SWFs, (ii) Inequality\nof Opportunity aware SWFs and (iii) Intergenerational Mobility SWFs. I use the\nPanel Study of Income Dynamics (PSID) to assess the effect of attending\npreschool on adult earnings and estimate optimal policy rules based on parental\nyears of education and parental income.",
      "generated_abstract": "r introduces a locally robust policy learning framework that\nlocally adapts the estimand of interest to the particulars of the data,\nallowing for a flexible and interpretable estimation strategy. The framework\nemploys a novel robust estimand, which we call the Inequality of Opportunity\nEstimand (IOE), to address the challenges posed by the inadequate coverage of\nthe data with respect to the inequality of opportunity. The framework is\nderived from a locally robust estimand and a locally robust identification\nassumption, and it allows for flexible covariate selection. The identification\nassumption is based on the assumption that the treatment effect is a\nfunction of the inequality of opportunity and the covariates, which we call the\nInequality of Opportunity Effect (IOE). We propose a local robust estimator for\nthe IOE estimand, which we show is locally consistent and asymptotically",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21568627450980393,
          "p": 0.3055555555555556,
          "f": 0.25287355836702347
        },
        "rouge-2": {
          "r": 0.05714285714285714,
          "p": 0.06896551724137931,
          "f": 0.06249999504394571
        },
        "rouge-l": {
          "r": 0.18627450980392157,
          "p": 0.2638888888888889,
          "f": 0.21839079974633385
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.10447v1",
      "true_abstract": "In the Feedback Arc Set in Tournaments (Subset-FAST) problem, we are given a\ntournament $D$ and a positive integer $k$, and the objective is to determine\nwhether there exists an arc set $S \\subseteq A(D)$ of size at most $k$ whose\nremoval makes the graph acyclic. This problem is well-known to be equivalent to\na natural tournament ranking problem, whose task is to rank players in a\ntournament such that the number of pairs in which the lower-ranked player\ndefeats the higher-ranked player is no more than $k$. Using the PTAS for\nSubset-FAST [STOC 2007], Bessy et al. [JCSS 2011] present a $(2 +\n\\varepsilon)k$-vertex kernel for this problem, given any fixed $\\varepsilon >\n0$. A generalization of Subset-FAST, called Subset-FAST, further includes an\nadditional terminal subset $T \\subseteq V(D)$ in the input. The goal of\nSubset-FAST is to determine whether there is an arc set $S \\subseteq A(D)$ of\nsize at most $k$ whose removal ensures that no directed cycle passes through\nany terminal in $T$. Prior to our work, no polynomial kernel for Subset-FAST\nwas known. In our work, we show that Subset-FAST admits an $\\mathcal{O}((\\alpha\nk)^{2})$-vertex kernel, provided that Subset-FAST has an approximation\nalgorithm with an approximation ratio $\\alpha$. Consequently, based on the\nknown $\\mathcal{O}(\\log k \\log \\log k)$-approximation algorithm, we obtain an\nalmost quadratic kernel for Subset-FAST.",
      "generated_abstract": "t a new vertex kernel for subset feedback arc set in tournaments,\nwhich is an extension of the kernel for the subgraph problem in tournaments\nintroduced by Cai et al. in 2018. Our kernel is a $O(\\log n)$-time reduction\nfrom the subgraph problem in tournaments to the subset feedback arc set problem\nin tournaments. The kernel is based on the idea of using an auxiliary tournament\nwith a special vertex set for each vertex in the original tournament. The\nauxiliary tournament is used to determine the number of edges in the vertex\nkernel of the original tournament. The vertex kernel of the auxiliary tournament\nis a subset of the original vertex kernel of the original tournament. We show\nthat a tournament with a vertex kernel of size $O(\\log n)$ has a vertex kernel\nof size $O(1)$ if and only if the original tournament has a vertex kernel of\nsize $O",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2074074074074074,
          "p": 0.4745762711864407,
          "f": 0.28865978958178345
        },
        "rouge-2": {
          "r": 0.06666666666666667,
          "p": 0.1262135922330097,
          "f": 0.08724831762420637
        },
        "rouge-l": {
          "r": 0.1925925925925926,
          "p": 0.4406779661016949,
          "f": 0.2680412328807525
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.02477v1",
      "true_abstract": "Two high-level \"pictures\" of probability theory have emerged: one that takes\nas central the notion of random variable, and one that focuses on distributions\nand probability channels (Markov kernels). While the channel-based picture has\nbeen successfully axiomatized, and widely generalized, using the notion of\nMarkov category, the categorical semantics of the random variable picture\nremain less clear. Simpson's probability sheaves are a recent approach, in\nwhich probabilistic concepts like random variables are allowed vary over a site\nof sample spaces. Simpson has identified rich structure on these sites, most\nnotably an abstract notion of conditional independence, and given examples\nranging from probability over databases to nominal sets. We aim bring this\ndevelopment together with the generality and abstraction of Markov categories:\nWe show that for any suitable Markov category, a category of sample spaces can\nbe defined which satisfies Simpson's axioms, and that a theory of probability\nsheaves can be developed purely synthetically in this setting. We recover\nSimpson's examples in a uniform fashion from well-known Markov categories, and\nconsider further generalizations.",
      "generated_abstract": "We study the properties of random variables in a category of abstract sample\nspaces, generalizing the classical notion of independence to the setting of\ncategories. We introduce a notion of conditional independence, and prove that\nthis is a well-defined notion. We show that the classical notion of independence\nis a special case of the new one, and that both are equivalent. We prove that\nif the category is generated by its coproducts, then the new notion is\nequivalent to the classical one. We show that if the category is generated by\nits coproducts and the category of finite sets is generated by its coproducts,\nthen the new notion is equivalent to the classical one.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19642857142857142,
          "p": 0.46808510638297873,
          "f": 0.2767295555840355
        },
        "rouge-2": {
          "r": 0.08125,
          "p": 0.16666666666666666,
          "f": 0.10924369307252331
        },
        "rouge-l": {
          "r": 0.1875,
          "p": 0.44680851063829785,
          "f": 0.26415093923183425
        }
      }
    },
    {
      "paper_id": "stat.ML.cs/AI/2503.10496v1",
      "true_abstract": "Modeling natural phenomena with artificial neural networks (ANNs) often\nprovides highly accurate predictions. However, ANNs often suffer from\nover-parameterization, complicating interpretation and raising uncertainty\nissues. Bayesian neural networks (BNNs) address the latter by representing\nweights as probability distributions, allowing for predictive uncertainty\nevaluation. Latent binary Bayesian neural networks (LBBNNs) further handle\nstructural uncertainty and sparsify models by removing redundant weights. This\narticle advances LBBNNs by enabling covariates to skip to any succeeding layer\nor be excluded, simplifying networks and clarifying input impacts on\npredictions. Ultimately, a linear model or even a constant can be found to be\noptimal for a specific problem at hand. Furthermore, the input-skip LBBNN\napproach reduces network density significantly compared to standard LBBNNs,\nachieving over 99% reduction for small networks and over 99.9% for larger ones,\nwhile still maintaining high predictive accuracy and uncertainty measurement.\nFor example, on MNIST, we reached 97% accuracy and great calibration with just\n935 weights, reaching state-of-the-art for compression of neural networks.\nFurthermore, the proposed method accurately identifies the true covariates and\nadjusts for system non-linearity. The main contribution is the introduction of\nactive paths, enhancing directly designed global and local explanations within\nthe LBBNN framework, that have theoretical guarantees and do not require post\nhoc external tools for explanations.",
      "generated_abstract": "deep learning (BDL) has been a popular method for modeling\nnonlinear data distributions, but its interpretation in terms of Bayesian\ninference has been limited. We propose an input-skip Latent Binary\nBayesian Neural Network (LBN-skip) to facilitate interpretability by leveraging\nthe information contained in the latent space. The LBN-skip network is\nconstructed by encoding the input with a low-dimensional latent space, then\napplying a binary classification layer to extract the binary labels of the\nencoded latent space. These labels are then combined with the raw input to\nobtain a binary label for the output, which is then used as an input for the\noriginal model. Our analysis shows that LBN-skip has similar or better\npredictive performance as the original model, and it also has a lower\nprediction error, which can be interpreted as a lower prediction error on the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1794871794871795,
          "p": 0.30434782608695654,
          "f": 0.22580644694588978
        },
        "rouge-2": {
          "r": 0.0049504950495049506,
          "p": 0.007692307692307693,
          "f": 0.0060240916207034345
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.2608695652173913,
          "f": 0.19354838242976077
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.08244v1",
      "true_abstract": "We establish the existence of intermittent two-point dynamics and infinite\nstationary measures for a class of random circle endomorphisms with zero\nLyapunov exponent, as a dynamical characterisation of the transition from\nsynchronisation (negative Lyapunov exponent) to chaos (positive Lyapunov\nexponent).",
      "generated_abstract": "We study intermittent two-point dynamics for random circle endomorphisms on\n$[0,1",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24242424242424243,
          "p": 0.7272727272727273,
          "f": 0.36363635988636367
        },
        "rouge-2": {
          "r": 0.10526315789473684,
          "p": 0.4,
          "f": 0.1666666633680556
        },
        "rouge-l": {
          "r": 0.24242424242424243,
          "p": 0.7272727272727273,
          "f": 0.36363635988636367
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08546v1",
      "true_abstract": "Positron Emission Tomography (PET) is a functional imaging modality that\nenables the visualization of biochemical and physiological processes across\nvarious tissues. Recently, deep learning (DL)-based methods have demonstrated\nsignificant progress in directly mapping sinograms to PET images. However,\nregression-based DL models often yield overly smoothed reconstructions lacking\nof details (i.e., low distortion, low perceptual quality), whereas GAN-based\nand likelihood-based posterior sampling models tend to introduce undesirable\nartifacts in predictions (i.e., high distortion, high perceptual quality),\nlimiting their clinical applicability. To achieve a robust\nperception-distortion tradeoff, we propose Posterior-Mean Denoising Diffusion\nModel (PMDM-PET), a novel approach that builds upon a recently established\nmathematical theory to explore the closed-form expression of\nperception-distortion function in diffusion model space for PET image\nreconstruction from sinograms. Specifically, PMDM-PET first obtained\nposterior-mean PET predictions under minimum mean square error (MSE), then\noptimally transports the distribution of them to the ground-truth PET images\ndistribution. Experimental results demonstrate that PMDM-PET not only generates\nrealistic PET images with possible minimum distortion and optimal perceptual\nquality but also outperforms five recent state-of-the-art (SOTA) DL baselines\nin both qualitative visual inspection and quantitative pixel-wise metrics PSNR\n(dB)/SSIM/NRMSE.",
      "generated_abstract": "sion model has been widely adopted in image restoration. However,\nit is challenging to reconstruct images with realistic anatomical details,\nparticularly in medical imaging. This paper proposes a novel framework for\nposterior-mean diffusion modeling, enabling realistic reconstruction of PET\nscans. The proposed method employs a two-stage denoising diffusion modeling\napproach, where the prior distribution is conditioned on the posterior\ndistribution to model the anatomical details of the PET scan. The posterior\ndistribution is obtained by the variational inference algorithm and is\nconditioned on the prior distribution. This method is trained with a\nreconstruction loss and an adversarial loss. The adversarial loss encourages\nthe denoising diffusion model to generate images that are consistent with the\nground-truth images, while the reconstruction loss enforces the denoising\ndiffusion model to generate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17687074829931973,
          "p": 0.35135135135135137,
          "f": 0.23529411319260463
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.036036036036036036,
          "f": 0.027118639373973807
        },
        "rouge-l": {
          "r": 0.17687074829931973,
          "p": 0.35135135135135137,
          "f": 0.23529411319260463
        }
      }
    },
    {
      "paper_id": "math.CV.math/CV/2503.05976v1",
      "true_abstract": "We prove that the (hermitian) rank of $QP^d$ is bounded from below by the\nrank of $P^d$ whenever $Q$ is not identically zero and real-analytic in a\nneighborhood of some point on the zero set of $P$ in $\\mathbb{C}^n$ and $P$ is\na polynomial of bidegree at most $(1,1)$. This result generalizes the theorem\nof D'Angelo and the second author which assumed that $P$ was bihomogeneous.\nExamples show that no hypothesis can be dropped.",
      "generated_abstract": "The rank of an ideal $I$ in a Noetherian domain $R$ is defined as the minimal\nnumber of generators of the ideal $I$ over $R$. The rank of $I$ is called the\nHermitian rank of $I$ if it is the minimal positive integer $k$ such that $I^k$\nis an ideal of $R$. In this paper, we investigate the Hermitian rank of ideals\nof rank one. We show that the rank of an ideal of rank one is equal to the\nHermitian rank of the ideal in the case of the polynomial ideal $I(x)$ of a\npolynomial ring $R=k[x",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18867924528301888,
          "p": 0.2222222222222222,
          "f": 0.2040816276863808
        },
        "rouge-2": {
          "r": 0.08333333333333333,
          "p": 0.08,
          "f": 0.08163264806330726
        },
        "rouge-l": {
          "r": 0.18867924528301888,
          "p": 0.2222222222222222,
          "f": 0.2040816276863808
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2411.12010v2",
      "true_abstract": "The advancement of novel combinatorial CRISPR screening technologies enables\nthe identification of synergistic gene combinations on a large scale. This is\ncrucial for developing novel and effective combination therapies, but the\ncombinatorial space makes exhaustive experimentation infeasible. We introduce\nNAIAD, an active learning framework that efficiently discovers optimal gene\npairs capable of driving cells toward desired cellular phenotypes. NAIAD\nleverages single-gene perturbation effects and adaptive gene embeddings that\nscale with the training data size, mitigating overfitting in small-sample\nlearning while capturing complex gene interactions as more data is collected.\nEvaluated on four CRISPR combinatorial perturbation datasets totaling over\n350,000 genetic interactions, NAIAD, trained on small datasets, outperforms\nexisting models by up to 40\\% relative to the second-best. NAIAD's\nrecommendation system prioritizes gene pairs with the maximum predicted\neffects, resulting in the highest marginal gain in each AI-experiment round and\naccelerating discovery with fewer CRISPR experimental iterations. Our NAIAD\nframework (https://github.com/NeptuneBio/NAIAD) improves the identification of\nnovel, effective gene combinations, enabling more efficient CRISPR library\ndesign and offering promising applications in genomics research and therapeutic\ndevelopment.",
      "generated_abstract": "the problem of active learning in the combinatorial perturbation\nspace (CPS), where the goal is to discover a small set of optimal perturbations\nof a target function, with the goal of achieving a certain performance\nbound. This problem arises in various areas, such as drug discovery, biomedical\nstudies, and machine learning. In this paper, we focus on the case where the\ntarget function is a function of a set of variables. We propose a novel\nframework for active learning in the CPS. Our approach is based on the\nrepresentation of the CPS as a tree and introduces a novel technique for\npartitioning the CPS into a set of subtrees, which we call the tree\npartition. We prove that a small set of optimal perturbations of a target\nfunction can be discovered by first performing an active search on the tree\npartition, followed by a greedy step that maximizes the expected performance\nbound.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.3132530120481928,
          "f": 0.24413145064250924
        },
        "rouge-2": {
          "r": 0.03529411764705882,
          "p": 0.047619047619047616,
          "f": 0.04054053565102324
        },
        "rouge-l": {
          "r": 0.18461538461538463,
          "p": 0.2891566265060241,
          "f": 0.22535210791950464
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2501.02214v1",
      "true_abstract": "One approach to estimating the average treatment effect in binary treatment\nwith unmeasured confounding is the proximal causal inference, which assumes the\navailability of outcome and treatment confounding proxies. The key identifying\nresult relies on the existence of a so-called bridge function. A parametric\nspecification of the bridge function is usually postulated and estimated using\nstandard techniques. The estimated bridge function is then plugged in to\nestimate the average treatment effect. This approach may have two efficiency\nlosses. First, the bridge function may not be efficiently estimated since it\nsolves an integral equation. Second, the sequential procedure may fail to\naccount for the correlation between the two steps. This paper proposes to\napproximate the integral equation with increasing moment restrictions and\njointly estimate the bridge function and the average treatment effect. Under\nsufficient conditions, we show that the proposed estimator is efficient. To\nassist implementation, we propose a data-driven procedure for selecting the\ntuning parameter (i.e., number of moment restrictions). Simulation studies\nreveal that the proposed method performs well in finite samples, and\napplication to the right heart catheterization dataset from the SUPPORT study\ndemonstrates its practical value.",
      "generated_abstract": "the problem of estimating the average treatment effect (ATE) under\nunmeasured confounding (UMC) and proxies. We assume that the treatment effect\nis non-negative, and that there are no measurement errors. We consider\nnonparametric and semiparametric estimation methods. We show that the UMC\nproblem can be reduced to a linear regression problem with a single predictor\n(the treatment) and a single outcome. We propose a semiparametric estimation\nmethod based on the maximum likelihood estimator. We establish consistency,\nasymptotic normality, and asymptotic precision of the semiparametric estimator.\nWe derive asymptotic distributional results for the semiparametric estimator\nand the UMC problem. We provide an asymptotic analysis of the unbiased\nsemiparametric estimator, and we derive asymptotic properties of the\nsemip",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2066115702479339,
          "p": 0.36764705882352944,
          "f": 0.2645502599434507
        },
        "rouge-2": {
          "r": 0.06976744186046512,
          "p": 0.11650485436893204,
          "f": 0.08727272258750438
        },
        "rouge-l": {
          "r": 0.19834710743801653,
          "p": 0.35294117647058826,
          "f": 0.2539682493614401
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/DB/2503.06882v1",
      "true_abstract": "Maximum Inner Product Search (MIPS) for high-dimensional vectors is pivotal\nacross databases, information retrieval, and artificial intelligence. Existing\nmethods either reduce MIPS to Nearest Neighbor Search (NNS) while suffering\nfrom harmful vector space transformations, or attempt to tackle MIPS directly\nbut struggle to mitigate redundant computations due to the absence of the\ntriangle inequality. This paper presents a novel theoretical framework that\nequates MIPS with NNS without requiring space transformation, thereby allowing\nus to leverage advanced graph-based indices for NNS and efficient edge pruning\nstrategies, significantly reducing unnecessary computations. Despite a strong\nbaseline set by our theoretical analysis, we identify and address two\npersistent challenges to further refine our method: the introduction of the\nProximity Graph with Spherical Pathway (PSP), designed to mitigate the issue of\nMIPS solutions clustering around large-norm vectors, and the implementation of\nAdaptive Early Termination (AET), which efficiently curtails the excessive\nexploration once an accuracy bottleneck is reached. Extensive experiments\nreveal the superiority of our method over existing state-of-the-art techniques\nin search efficiency, scalability, and practical applicability. Compared with\nstate-of-the-art graph based methods, it achieves an average 35% speed-up in\nquery processing and a 3x reduction in index size. Notably, our approach has\nbeen validated and deployed in the search engines of Shopee, a well-known\nonline shopping platform. Our code and an industrial-scale dataset for offline\nevaluation will also be released to address the absence of e-commerce data in\npublic benchmarks.",
      "generated_abstract": "We study the nearest neighbor problem in the inner product space. We show\nthat there exists a constant $c>0$ such that, for any $0<\\delta<1/2$,\n$n\\geq c\\delta^{-2}\\log(n)$, and any $f$ with $nf\\in[1,2",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.045454545454545456,
          "p": 0.2857142857142857,
          "f": 0.0784313701806998
        },
        "rouge-2": {
          "r": 0.004347826086956522,
          "p": 0.03333333333333333,
          "f": 0.007692305650888115
        },
        "rouge-l": {
          "r": 0.045454545454545456,
          "p": 0.2857142857142857,
          "f": 0.0784313701806998
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2502.19333v1",
      "true_abstract": "Despite advances in methods to interrogate tumor biology, the observational\nand population-based approach of classical cancer research and clinical\noncology does not enable anticipation of tumor outcomes to hasten the discovery\nof cancer mechanisms and personalize disease management. To address these\nlimitations, individualized cancer forecasts have been shown to predict tumor\ngrowth and therapeutic response, inform treatment optimization, and guide\nexperimental efforts. These predictions are obtained via computer simulations\nof mathematical models that are constrained with data from a patient's cancer\nand experiments. This book chapter addresses the validation of these\nmathematical models to forecast tumor growth and treatment response. We start\nwith an overview of mathematical modeling frameworks, model selection\ntechniques, and fundamental metrics. We then describe the usual strategies\nemployed to validate cancer forecasts in preclinical and clinical scenarios.\nFinally, we discuss existing barriers in validating these predictions along\nwith potential strategies to address them.",
      "generated_abstract": "cal models are widely used to understand the dynamics of tumor\ngrowth and response to therapy. However, their predictive accuracy can be\nsubject to substantial uncertainty, particularly in the early stages of\ntreatment, when tumor growth is dominated by non-cancerous factors. Here, we\npropose a novel approach to assess the predictive accuracy of tumor growth and\nresponse to therapy models by comparing the model predictions with clinical\ndata from a large cohort of patients with metastatic cancer. We use the\nEuclidean distance metric to quantify the discrepancy between predicted and\nobserved growth rates, and the Spearman rank correlation coefficient to\nevaluate the strength of the relationship between predicted and observed\nresponse to therapy. The results show that the model predictions have high\ndiscrepancy metrics with observed growth rates, and high Spearman rank\ncorrelation coefficients with response to therapy",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2376237623762376,
          "p": 0.3,
          "f": 0.26519336523305154
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.043478260869565216,
          "f": 0.03921568132256887
        },
        "rouge-l": {
          "r": 0.22772277227722773,
          "p": 0.2875,
          "f": 0.25414364147614554
        }
      }
    },
    {
      "paper_id": "physics.pop-ph.physics/pop-ph/2501.08583v1",
      "true_abstract": "In this work, I propose a way to help high school students and the general\npopulation understand quantum concepts by adopting a new inherently dual\nrepresentation. Major difficulties in explaining to people the basic concepts\nof quantum mechanics reside in the apparent impossibility of representing\nquantum superposition with examples taken from everyday life. In this context,\nI propose a new pictorial paradigm that illustrates a number of quantum\nconcepts by means of optical illusions, potentially without raising\nmisconceptions. The method is based on \"bistable reversible figures\", which\ninduce in the viewer a multistable perception, conveying a direct understanding\nof superposition, random collapse, and observer effect via a sensorial\nexperience. I present the advantages and discuss the limitations of this\nanalogy, and show how it extends to the concepts of complementarity and quantum\nentanglement, also helping to avoiding misconceptions in quantum teleportation.\nFinally, I also address quantum spin and quantum measurement by using different\ntypes of optical illusions.",
      "generated_abstract": "We present a new experimental paradigm for understanding quantum physics,\nwhich enables a direct comparison between classical and quantum models.\nWe introduce a simple optical illusion that can be used to test the validity of\nthe Heisenberg uncertainty principle. We show that the illusion can be used to\nvalidate the assumption that the uncertainty principle is violated at the\nlevel of the individual photons, and we argue that this assumption is\nfundamentally in conflict with the classical Heisenberg uncertainty principle.\nFurthermore, we demonstrate that the illusion can be used to validate the\nassumption that the uncertainty principle is violated at the level of the\nensemble of photons. Finally, we show that the illusion can be used to validate\nthe assumption that the uncertainty principle is violated at the level of the\nensemble of photons, and we argue that this assumption is also in conflict with\nthe classical Heisenberg uncertainty principle.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20192307692307693,
          "p": 0.375,
          "f": 0.26249999545000013
        },
        "rouge-2": {
          "r": 0.020689655172413793,
          "p": 0.03571428571428571,
          "f": 0.026200868717225895
        },
        "rouge-l": {
          "r": 0.17307692307692307,
          "p": 0.32142857142857145,
          "f": 0.2249999954500001
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.04480v1",
      "true_abstract": "Research in adversarial machine learning (AML) has shown that statistical\nmodels are vulnerable to maliciously altered data. However, despite advances in\nBayesian machine learning models, most AML research remains concentrated on\nclassical techniques. Therefore, we focus on extending the white-box model\npoisoning paradigm to attack generic Bayesian inference, highlighting its\nvulnerability in adversarial contexts. A suite of attacks are developed that\nallow an attacker to steer the Bayesian posterior toward a target distribution\nthrough the strategic deletion and replication of true observations, even when\nonly sampling access to the posterior is available. Analytic properties of\nthese algorithms are proven and their performance is empirically examined in\nboth synthetic and real-world scenarios. With relatively little effort, the\nattacker is able to substantively alter the Bayesian's beliefs and, by\naccepting more risk, they can mold these beliefs to their will. By carefully\nconstructing the adversarial posterior, surgical poisoning is achieved such\nthat only targeted inferences are corrupted and others are minimally disturbed.",
      "generated_abstract": "er the problem of poisoning Bayesian inference, where the goal is to\nadversarially modify data in order to generate a surrogate posterior that\nmimics the target posterior as closely as possible. We first introduce a\ngeneralization of the Bayesian information criterion that is more robust to\nout-of-distribution data. We then show that the information-theoretic loss\nfunctions in the literature are sub-optimal when the posterior distribution is\nnon-Gaussian and non-conjugate. To address this, we introduce a novel\ninformation-theoretic loss function that quantifies the difference between the\nposterior and target posteriors. We demonstrate that this loss function is\noptimal when the posterior distribution is conjugate and non-Gaussian. We\nfurther show that the proposed loss function can be implemented by replicating\ndata and performing Bayesian inference with a surrogate posterior obtained by\nrep",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17796610169491525,
          "p": 0.28378378378378377,
          "f": 0.2187499952625869
        },
        "rouge-2": {
          "r": 0.01910828025477707,
          "p": 0.02586206896551724,
          "f": 0.021978017090797955
        },
        "rouge-l": {
          "r": 0.15254237288135594,
          "p": 0.24324324324324326,
          "f": 0.18749999526258695
        }
      }
    },
    {
      "paper_id": "math.ST.cs/CC/2503.02802v1",
      "true_abstract": "In this work, we show the first average-case reduction transforming the\nsparse Spiked Covariance Model into the sparse Spiked Wigner Model and as a\nconsequence obtain the first computational equivalence result between two\nwell-studied high-dimensional statistics models. Our approach leverages a new\nperturbation equivariance property for Gram-Schmidt orthogonalization, enabling\nremoval of dependence in the noise while preserving the signal.",
      "generated_abstract": "We derive a Gram-Schmidt perturbation method to compute the\ncomputational equivalence between spiked covariance and spiked Wigner models. We\nshow that the computation of the covariance kernel and the Wigner density\nfunction are equivalent for spiked covariance, but not for spiked Wigner. The\nGram-Schmidt perturbation method is then used to compute the covariance kernel\nand the Wigner density function of spiked covariance models for a given\nspiked covariance model, and we show that they are computable exactly.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2857142857142857,
          "p": 0.35,
          "f": 0.31460673662416366
        },
        "rouge-2": {
          "r": 0.03636363636363636,
          "p": 0.03333333333333333,
          "f": 0.034782603705104685
        },
        "rouge-l": {
          "r": 0.20408163265306123,
          "p": 0.25,
          "f": 0.22471909617472552
        }
      }
    },
    {
      "paper_id": "math.RT.math/GN/2502.08847v1",
      "true_abstract": "A representation $\\rho$ of a compact group $\\mathbb{G}$ selects eigenvalues\nif there is a continuous circle-valued map on $\\mathbb{G}$ assigning an\neigenvalue of $\\rho(g)$ to every $g\\in \\mathbb{G}$. For every compact connected\n$\\mathbb{G}$, we characterize the irreducible $\\mathbb{G}$-representations\nwhich select eigenvalues as precisely those annihilating the intersection\n$Z_0(\\mathbb{G})\\cap \\mathbb{G}'$ of the connected center of $\\mathbb{G}$ with\nits derived subgroup. The result applies more generally to finite-spectrum\nrepresentations isotypic on $Z_0(\\mathbb{G})$, and recovers as applications\n(noted in prior work) the existence of a continuous eigenvalue selector for the\nnatural representation of $\\mathrm{SU}(n)$ and the non-existence of such a\nselector for $\\mathrm{U}(n)$.",
      "generated_abstract": "e a compact connected Lie group and let $\\pi:G\\to G/H$ be the\ngeneralized quotient map. In this paper, we introduce a new tool for the study\nof the eigenvalues of the Laplacian on a representation of $G$. We show that\nwhen $G$ is a compact connected Lie group and $H$ is a closed subgroup of $G$\ncontaining $G$ as a normal subgroup, the eigenvalue selectors for $H$ on\nrepresentations of $G$ are the eigenvalues of the operator $\\pi_H$ on the\nReeb cohomology $H^*(G/H)$. In particular, if $G$ is a solvable Lie group and\n$H$ is a closed subgroup of $G$ containing $G$ as a normal subgroup, then the\neigenvalue selectors for $H$ on representations of $G$ are the eigenvalues of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2676056338028169,
          "p": 0.34545454545454546,
          "f": 0.3015872966679265
        },
        "rouge-2": {
          "r": 0.0625,
          "p": 0.07692307692307693,
          "f": 0.0689655122948874
        },
        "rouge-l": {
          "r": 0.2112676056338028,
          "p": 0.2727272727272727,
          "f": 0.23809523317586304
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.05418v1",
      "true_abstract": "Integrated sensing and communication (ISAC) has been identified as a\npromising technology for the sixth generation (6G) of communication networks.\nTarget privacy in ISAC is essential to ensure that only legitimate sensors can\ndetect the target while keeping it hidden from malicious ones. In this paper,\nwe consider a downlink reconfigurable intelligent surface (RIS)-assisted ISAC\nsystem capable of protecting a sensing region against an adversarial detector.\nThe RIS consists of both reflecting and sensing elements, adaptively changing\nthe element assignment based on system needs. To achieve this, we minimize the\nmaximum sensing signal-to-interference-plus-noise-ratio (SINR) at the\nadversarial detector within sample points in the sensing region, by optimizing\nthe transmit beamformer at the base station, the RIS phase shift matrix, the\nreceived beamformer at the RIS, and the division between reflecting and\nabsorptive elements at the RIS, where the latter function as sensing elements.\nAt the same time, the system is designed to maintain a minimum sensing SINR at\neach monitored location, as well as minimum communication SINR for each user.\nTo solve this challenging optimization problem, we develop an alternating\noptimization approach combined with a successive convex approximation based\nmethod tailored for each subproblem. Our results show that the proposed\napproach achieves a 25 dB reduction in the maximum sensing SINR at the\nadversarial detector compared to scenarios without sensing area protection.\nAlso, the optimal RIS element assignment can further improve sensing protection\nby 3 dB over RISs with fixed element configuration.",
      "generated_abstract": "Target obfuscation is a critical task in the Internet of Things (IoT), as\ntargets in the real world can be dangerous and pose a security threat. In\nthis paper, we propose a novel approach for target obfuscation, which employs a\nreflecting surface, referred to as the RIS, to conceal a target. We first\nformulate the problem as a three-player game, where a target and two reflecting\nsurfaces compete to evade a transmitter. Then, we propose a distributed\nbeamforming design for the transmitter to maximize its utility. Finally, we\nderive the optimal transmit beamforming design for the two reflecting surfaces\nto maximize the transmit power. Numerical simulations validate the effectiveness\nof the proposed approach in concealing a target in a realistic scenario.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.152317880794702,
          "p": 0.3150684931506849,
          "f": 0.2053571384634089
        },
        "rouge-2": {
          "r": 0.049107142857142856,
          "p": 0.10091743119266056,
          "f": 0.06606606166238341
        },
        "rouge-l": {
          "r": 0.1456953642384106,
          "p": 0.3013698630136986,
          "f": 0.19642856703483746
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.10055v1",
      "true_abstract": "While 3D point clouds are widely utilized across various vision applications,\ntheir irregular and sparse nature make them challenging to handle. In response,\nnumerous encoding approaches have been proposed to capture the rich semantic\ninformation of point clouds. Yet, a critical limitation persists: a lack of\nconsideration for colored point clouds which are more capable 3D\nrepresentations as they contain diverse attributes: color and geometry. While\nexisting methods handle these attributes separately on a per-point basis, this\nleads to a limited receptive field and restricted ability to capture\nrelationships across multiple points. To address this, we pioneer a point cloud\nencoding methodology that leverages 3D Fourier decomposition to disentangle\ncolor and geometric features while extending the receptive field through\nspectral-domain operations. Our analysis confirms that this encoding approach\neffectively separates feature components, where the amplitude uniquely captures\ncolor attributes and the phase encodes geometric structure, thereby enabling\nindependent learning and utilization of both attributes. Furthermore, the\nspectral-domain properties of these components naturally aggregate local\nfeatures while considering multiple points' information. We validate our point\ncloud encoding approach on point cloud classification and style transfer tasks,\nachieving state-of-the-art results on the DensePoint dataset with improvements\nvia a proposed amplitude-based data augmentation strategy.",
      "generated_abstract": "uds are increasingly used for object recognition and analysis in\nmany applications, yet existing methods often rely on pre-computed 3D\nmesh representations, which are computationally expensive and require\nspecialized hardware. This paper introduces a novel method for 3D point cloud\nattribute extraction that leverages Fourier decomposition to transform the\npoint cloud into a linearly separable representation. Specifically, we propose\nan unsupervised approach that first transforms the point cloud into a\nrepresentation space using a pre-trained transformer model and then uses a\nlinear layer to map the 3D point cloud to a 2D attribute representation. This\napproach is highly scalable and can be applied to arbitrary point cloud\ndatasets. Additionally, we introduce a novel metric called the\n2D-3D-3D-point-cloud distance, which captures the distance between 3D\npoint clouds and their linearly separable representations",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17857142857142858,
          "p": 0.2840909090909091,
          "f": 0.21929824087411526
        },
        "rouge-2": {
          "r": 0.04736842105263158,
          "p": 0.07563025210084033,
          "f": 0.058252422448445626
        },
        "rouge-l": {
          "r": 0.15714285714285714,
          "p": 0.25,
          "f": 0.19298245140043102
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06370v1",
      "true_abstract": "This paper presents a method for load balancing and dynamic pricing in\nelectric vehicle (EV) charging networks, utilizing reinforcement learning (RL)\nto enhance network performance. The proposed framework integrates a pre-trained\ngraph neural network to predict demand elasticity and inform pricing decisions.\nThe spatio-temporal EV charging demand prediction (EVCDP) dataset from Shenzhen\nis utilized to capture the geographic and temporal characteristics of the\ncharging stations. The RL model dynamically adjusts prices at individual\nstations based on occupancy, maximum station capacity, and demand forecasts,\nensuring an equitable network load distribution while preventing station\noverloads. By leveraging spatially-aware demand predictions and a carefully\ndesigned reward function, the framework achieves efficient load balancing and\nadaptive pricing strategies that respond to localized demand and global network\ndynamics, ensuring improved network stability and user satisfaction. The\nefficacy of the approach is validated through simulations on the dataset,\nshowing significant improvements in load balancing and reduced overload as the\nRL agent iteratively interacts with the environment and learns to dynamically\nadjust pricing strategies based on real-time demand patterns and station\nconstraints. The findings highlight the potential of adaptive pricing and\nload-balancing strategies to address the complexities of EV infrastructure,\npaving the way for scalable and user-centric solutions.",
      "generated_abstract": "r proposes a reinforcement learning-based Dynamic Load Balancing\n(DLB) framework for energy storage systems (ESSs) to balance the energy\ndemand and minimize energy losses. The proposed DLB system employs a\nreinforcement learning agent to dynamically balance the energy demand between\nthe ESSs, maximizing the system's economic efficiency while ensuring grid\nstability. The agent uses a deep Q-learning algorithm to optimize the\nbalancing policy, and it receives feedback from the ESSs regarding their\ndemand. The agent also predicts the demand in the future to ensure grid\nstability. The DLB agent utilizes a Deep Neural Network (DNN) to predict the\ndemand for the next 24 hours, which is used to update the agent's policy. The\nagent implements the proposed DLB framework in a simulation environment and\nevaluates its performance through",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18604651162790697,
          "p": 0.2962962962962963,
          "f": 0.22857142383265314
        },
        "rouge-2": {
          "r": 0.03125,
          "p": 0.05128205128205128,
          "f": 0.03883494675087247
        },
        "rouge-l": {
          "r": 0.17054263565891473,
          "p": 0.2716049382716049,
          "f": 0.20952380478503413
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.09287v1",
      "true_abstract": "We study the properties of macroeconomic survey forecast response averages as\nthe number of survey respondents grows. Such averages are \"portfolios\" of\nforecasts. We characterize the speed and pattern of the gains from\ndiversification and their eventual decrease with portfolio size (the number of\nsurvey respondents) in both (1) the key real-world data-based environment of\nthe U.S. Survey of Professional Forecasters (SPF), and (2) the theoretical\nmodel-based environment of equicorrelated forecast errors. We proceed by\nproposing and comparing various direct and model-based \"crowd size signature\nplots,\" which summarize the forecasting performance of k-average forecasts as a\nfunction of k, where k is the number of forecasts in the average. We then\nestimate the equicorrelation model for growth and inflation forecast errors by\nchoosing model parameters to minimize the divergence between direct and\nmodel-based signature plots. The results indicate near-perfect equicorrelation\nmodel fit for both growth and inflation, which we explicate by showing\nanalytically that, under conditions, the direct and fitted equicorrelation\nmodel-based signature plots are identical at a particular model parameter\nconfiguration, which we characterize. We find that the gains from\ndiversification are greater for inflation forecasts than for growth forecasts,\nbut that both gains nevertheless decrease quite quickly, so that fewer SPF\nrespondents than currently used may be adequate.",
      "generated_abstract": "In the past, economists have been reluctant to collaborate with non-economists.\nHowever, the advent of the Internet and social media has changed this. Today,\nmany economists engage with the public through online platforms. This paper\nconsiders the case of a model in which several economists collaborate to\nestimate a parameter in a linear model. We show that the model can be\ninformatively equivalent to a model in which a single expert estimates the\nparameter. We further show that, under some assumptions, the best-fitting\nestimator of the parameter in the model with multiple experts is the same as\nthe best-fitting estimator of the parameter in the model with a single expert.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14754098360655737,
          "p": 0.2903225806451613,
          "f": 0.1956521694447071
        },
        "rouge-2": {
          "r": 0.031578947368421054,
          "p": 0.06593406593406594,
          "f": 0.042704621955142866
        },
        "rouge-l": {
          "r": 0.13934426229508196,
          "p": 0.27419354838709675,
          "f": 0.1847826042273158
        }
      }
    },
    {
      "paper_id": "cs.HC.eess/SY/2503.03570v1",
      "true_abstract": "Traditional XR and Metaverse applications prioritize user experience (UX) for\nadoption and success but often overlook a crucial aspect of user interaction:\nemotions. This article addresses this gap by presenting an emotion-aware\nMetaverse application: a Virtual Reality (VR) fire drill simulator designed to\nprepare crews for shipboard emergencies. The simulator detects emotions in real\ntime, assessing trainees responses under stress to improve learning outcomes.\nIts architecture incorporates eye-tracking and facial expression analysis via\nMeta Quest Pro headsets. The system features four levels whose difficulty is\nincreased progressively to evaluate user decision-making and emotional\nresilience. The system was evaluated in two experimental phases. The first\nphase identified challenges, such as navigation issues and lack of visual\nguidance. These insights led to an improved second version with a better user\ninterface, visual cues and a real-time task tracker. Performance metrics like\ncompletion times, task efficiency and emotional responses were analyzed. The\nobtained results show that trainees with prior VR or gaming experience\nnavigated the scenarios more efficiently. Moreover, the addition of\ntask-tracking visuals and navigation guidance significantly improved user\nperformance, reducing task completion times between 14.18\\% and 32.72\\%.\nEmotional responses were captured, revealing that some participants were\nengaged, while others acted indifferently, indicating the need for more\nimmersive elements. Overall, this article provides useful guidelines for\ncreating the next generation of emotion-aware Metaverse applications.",
      "generated_abstract": "development of advanced technology and the expansion of the\nfire drill simulation market have led to an explosion in the number of\navailable simulators. However, the emergence of metaverse technology has\nrevolutionized the simulation industry by integrating human-centric AI-based\nsimulation with immersive virtual environments. This paper introduces a novel\nemotion-aware shipboard fire drill simulation system, which leverages an\nemotion-awareness model to analyze and understand human emotions, enhancing the\nrealism of the simulation and enhancing the user's experience. The system\nintegrates a human-centric emotion-awareness model with an advanced\nfire-simulation engine to capture human emotions, such as anger, fear, and\nsurprise, and generate realistic fire scenarios. Additionally, the system\nintegrates a virtual-reality-based design tool to facilitate the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13333333333333333,
          "p": 0.3013698630136986,
          "f": 0.18487394532695442
        },
        "rouge-2": {
          "r": 0.022935779816513763,
          "p": 0.047619047619047616,
          "f": 0.030959747933940372
        },
        "rouge-l": {
          "r": 0.11515151515151516,
          "p": 0.2602739726027397,
          "f": 0.15966386129334098
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.09565v1",
      "true_abstract": "Despite deep neural networks' powerful representation learning capabilities,\ntheoretical understanding of how networks can simultaneously achieve meaningful\nfeature learning and global convergence remains elusive. Existing approaches\nlike the neural tangent kernel (NTK) are limited because features stay close to\ntheir initialization in this parametrization, leaving open questions about\nfeature properties during substantial evolution. In this paper, we investigate\nthe training dynamics of infinitely wide, $L$-layer neural networks using the\ntensor program (TP) framework. Specifically, we show that, when trained with\nstochastic gradient descent (SGD) under the Maximal Update parametrization\n($\\mu$P) and mild conditions on the activation function, SGD enables these\nnetworks to learn linearly independent features that substantially deviate from\ntheir initial values. This rich feature space captures relevant data\ninformation and ensures that any convergent point of the training process is a\nglobal minimum. Our analysis leverages both the interactions among features\nacross layers and the properties of Gaussian random variables, providing new\ninsights into deep representation learning. We further validate our theoretical\nfindings through experiments on real-world datasets.",
      "generated_abstract": "Under the framework of infinite-width neural networks, we analyze the\nperformance of parametrized $L$-layer neural networks under $\u03bc$P parametrization\nwith $\u03bc > 1$. We establish the global convergence of the network, and show that\nthe feature learning ability of the network is robust to the initial point and\nthe network architecture. Furthermore, we propose a learning rule that\nachieves the same convergence rate as the network in a finite-width setting. We\nalso establish the rich feature learning ability of the network, which can\ndistinguish the input features and learn the feature representations of the\ninput features. This is achieved by using the feature distributions to\nrepresent the input features, which are constructed based on the $\u03bc$P\nparametrization. Theoretical analysis, numerical experiments, and real-world\napplications demonstrate the effectiveness and robustness of our proposed\nframework.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2518518518518518,
          "p": 0.425,
          "f": 0.3162790650946458
        },
        "rouge-2": {
          "r": 0.05389221556886228,
          "p": 0.07758620689655173,
          "f": 0.06360423544506774
        },
        "rouge-l": {
          "r": 0.1925925925925926,
          "p": 0.325,
          "f": 0.241860460443483
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.18647v1",
      "true_abstract": "Urban overheating, exacerbated by climate change, threatens public health and\nurban sustainability. Traditional approaches, such as numerical simulations and\nfield measurements, face challenges due to uncertainties in input data. This\nstudy integrates field measurements with machine learning models to predict the\nduration and severity of future urban overheating events, focusing on the role\nof urban greening under different global warming (GW) scenarios. Field\nmeasurements were conducted in summer 2024 at an office campus in Ottawa, a\ncold-climate city. Microclimate data were collected from four locations with\nvarying levels of greenery: a large lawn without trees (Lawn), a parking lot\nwithout greenery (Parking), an area with sparsely distributed trees (Tree), and\na fully covered forested area (Forest). Machine learning models, including\nArtificial Neural Networks (ANN), Recurrent Neural Networks (RNN), and Long\nShort-Term Memory (LSTM) networks, were trained on local microclimate data,\nwith LSTM achieving the best predictions. Four GW scenarios were analyzed,\ncorresponding to different Shared Socioeconomic Pathways (SSP) for 2050 and\n2090. Results show that the Universal Thermal Climate Index (UTCI) at the\n\"Parking\" location rises from about 27,\\textdegree C under GW1.0 to\n31,\\textdegree C under GW3.5. Moreover, low health risk conditions (UTCI >\n26,\\textdegree C) increase across all locations due to climate change,\nregardless of greenery levels. However, tree-covered areas such as \"Tree\" and\n\"Forest\" effectively prevent extreme heat conditions (UTCI > 38.9,\\textdegree\nC). These findings highlight the crucial role of urban greening in mitigating\nsevere thermal stress and enhancing thermal comfort under future climate\nscenarios.",
      "generated_abstract": "y investigates the relationship between urban land use and\nlong-term urban overheating and proposes a machine learning-based model that\nenables the prediction of urban overheating from nature-based solutions\n(NBS) using field measurements. The model consists of a series of neural\nnetworks, each one of which predicts the probability of overheating at a\nspecific location within a given time window. The neural networks are\ntrained using daily data from 2023 to 2025, including hourly data for 11\ndifferent fields (12,000 observations). The model is evaluated using cross-\nvalidation and the RMSE and AUROC metrics. The results show that the model\nachieves an RMSE of 1.71\u00b0C and an AUROC of 71.9%, outperforming existing\nmethods in both metrics. The model is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12994350282485875,
          "p": 0.2875,
          "f": 0.17898832256052336
        },
        "rouge-2": {
          "r": 0.02092050209205021,
          "p": 0.043859649122807015,
          "f": 0.028328607524978806
        },
        "rouge-l": {
          "r": 0.11299435028248588,
          "p": 0.25,
          "f": 0.15564201905857783
        }
      }
    },
    {
      "paper_id": "math.ST.math/ST/2503.09507v1",
      "true_abstract": "For one dimensional stochastic Burgers equation driven by space-time white\nnoise we consider the problem of estimation of the diffusivity parameter in\nfront of the second-order spatial derivative. Based on local observations in\nspace, we study the estimator derived in [Altmeyer, Rei{\\ss}, Ann. Appl.\nProbab.(2021)] for linear stochastic heat equation that has also been used in\n[Altmeyer, Cialenco, Pasemann, Bernoulli (2023)] to cover large class of\nsemilinear SPDEs and has been examined for the stochastic Burgers equation\ndriven by trace class noise. We extend the achieved results by considering the\nspace-time white noise case which has also relevant physical motivations. After\nwe establish new regularity results for the solution, we are able to show that\nour proposed estimator is strongly consistent and asymptotically normal.",
      "generated_abstract": "We consider the stochastic Burgers equation driven by a white noise, and\ninvestigate the parameter estimation problem for a family of stochastic\nmodels. We propose a Bayesian approach to the estimation problem, where the\nlikelihood function is defined as the product of the likelihood functions of\nthe corresponding deterministic models. We show that the proposed Bayesian\nestimation procedure provides consistent and asymptotically normal estimators\nof the parameter of the stochastic Burgers equation. Numerical examples are\npresented to illustrate the performance of the proposed methodology.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2857142857142857,
          "p": 0.47058823529411764,
          "f": 0.355555550854321
        },
        "rouge-2": {
          "r": 0.09649122807017543,
          "p": 0.1527777777777778,
          "f": 0.11827956514741608
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.4117647058823529,
          "f": 0.3111111064098766
        }
      }
    },
    {
      "paper_id": "q-bio.QM.stat/AP/2503.07664v1",
      "true_abstract": "The Antibiotic Resistance Microbiology Dataset (ARMD) is a de-identified\nresource derived from electronic health records (EHR) that facilitates research\ninto antimicrobial resistance (AMR). ARMD encompasses data from adult patients,\nfocusing on microbiological cultures, antibiotic susceptibilities, and\nassociated clinical and demographic features. Key attributes include organism\nidentification, susceptibility patterns for 55 antibiotics, implied\nsusceptibility rules, and de-identified patient information. This dataset\nsupports studies on antimicrobial stewardship, causal inference, and clinical\ndecision-making. ARMD is designed to be reusable and interoperable, promoting\ncollaboration and innovation in combating AMR. This paper describes the\ndataset's acquisition, structure, and utility while detailing its\nde-identification process.",
      "generated_abstract": "bial resistance (AMR) is a global public health problem that\nis caused by the emergence and spread of bacterial strains that are resistant\nto the antibiotics we use to treat infections. Antibiotic resistance can be\nidentified by a variety of methods, including whole genome sequencing, which\ncan reveal the evolutionary history of a bacterial strain. However, sequencing\nis costly and time-consuming, making it difficult to scale for large-scale\nresearch. The availability of large-scale electronic health records (EHRs)\nprovides a novel approach to AMR research, enabling the identification of\nantibiotic-resistant bacteria through their presence in EHRs. However,\ncurrently available EHR datasets are limited to a few diseases, such as\ninfectious diseases and chronic diseases, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21428571428571427,
          "p": 0.21428571428571427,
          "f": 0.2142857092857144
        },
        "rouge-2": {
          "r": 0.04081632653061224,
          "p": 0.035398230088495575,
          "f": 0.037914686968397594
        },
        "rouge-l": {
          "r": 0.17857142857142858,
          "p": 0.17857142857142858,
          "f": 0.17857142357142872
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/NE/2503.08301v2",
      "true_abstract": "In many-task optimization scenarios, surrogate models are valuable for\nmitigating the computational burden of repeated fitness evaluations across\ntasks. This study proposes a novel meta-surrogate framework to assist many-task\noptimization, by leveraging the knowledge transfer strengths and emergent\ncapabilities of large language models (LLMs). We formulate a unified framework\nfor many-task fitness prediction, by defining a universal model with metadata\nto fit a group of problems. Fitness prediction is performed on metadata and\ndecision variables, enabling efficient knowledge sharing across tasks and\nadaptability to new tasks. The LLM-based meta-surrogate treats fitness\nprediction as conditional probability estimation, employing a unified token\nsequence representation for task metadata, inputs, and outputs. This approach\nfacilitates efficient inter-task knowledge sharing through shared token\nembeddings and captures complex task dependencies via multi-task model\ntraining. Experimental results demonstrate the model's emergent generalization\nability, including zero-shot performance on problems with unseen dimensions.\nWhen integrated into evolutionary transfer optimization (ETO), our framework\nsupports dual-level knowledge transfer -- at both the surrogate and individual\nlevels -- enhancing optimization efficiency and robustness. This work\nestablishes a novel foundation for applying LLMs in surrogate modeling,\noffering a versatile solution for many-task optimization.",
      "generated_abstract": "en many-task optimization (DMT) has gained attention due to its\nlarge-scale performance benefits, yet it faces significant challenges.\nSpecifically, existing methods often struggle to generate interpretable\nhints and do not effectively leverage meta-learning techniques, limiting\ntheir application in practical scenarios. To address these issues, we propose\nMeta-LangModel (MLM), a novel framework that leverages large language models\n(LLMs) as a meta-surrogate to optimize many tasks with a single optimization\nproblem. In particular, we first integrate LLMs into existing DMT algorithms\nto generate meta-hints and extract meta-features. Then, we design a meta-learning\nstrategy to refine the meta-hints into a more interpretable one.\nFurthermore, we propose a meta-learning-based task selection strategy to\nidentify the most informative tasks, which",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.171875,
          "p": 0.24175824175824176,
          "f": 0.20091323715185264
        },
        "rouge-2": {
          "r": 0.027472527472527472,
          "p": 0.04504504504504504,
          "f": 0.03412968812636205
        },
        "rouge-l": {
          "r": 0.1640625,
          "p": 0.23076923076923078,
          "f": 0.1917808170605285
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.15891v1",
      "true_abstract": "We consider the problem of estimating the number of communities in a weighted\nbalanced Stochastic Block Model. We construct hypothesis tests based on\nsemidefinite programming and with a statistic coming from a GOE matrix to\ndistinguish between any two candidate numbers of communities. This is possible\ndue to a universality result for a semidefinite programming-based function that\nwe also prove. The tests are then used to form a sequential test to estimate\nthe number of communities. Furthermore, we also construct estimators of the\ncommunities themselves.",
      "generated_abstract": "We consider the problem of counting communities in a weighted stochastic\nblock model where individuals are assigned to a community by a randomized\npartition. In this setting, the community detection problem is equivalent to\ncounting the number of communities. In the special case of independent\nindependence models, we show that the optimal solution to this problem is\nachieved via a semidefinite program (SDP). We show that this SDP can be\nexpressed as a matrix completion problem, and propose a novel approach for\nsolving this matrix completion problem efficiently. As a result, we obtain an\nefficient algorithm for solving this problem. We further generalize our\napproach to the case where the community detection problem is also a\nweighted stochastic block model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3333333333333333,
          "p": 0.2898550724637681,
          "f": 0.3100775144041825
        },
        "rouge-2": {
          "r": 0.1518987341772152,
          "p": 0.11428571428571428,
          "f": 0.13043477770853043
        },
        "rouge-l": {
          "r": 0.26666666666666666,
          "p": 0.2318840579710145,
          "f": 0.24806201052821356
        }
      }
    },
    {
      "paper_id": "physics.med-ph.eess/SY/2503.06172v1",
      "true_abstract": "Noninvasive brain stimulation can activate neurons in the brain but requires\npower electronics with exceptionally high power in the mega-volt-ampere and\nhigh frequencies in the kilohertz range. Whereas oscillator circuits offered\nonly one or very few pulse shapes, modular power electronics solved a\nlong-standing problem for the first time and enabled arbitrary software-based\ndesign of the temporal shape of stimuli. However, synthesizing arbitrary\nstimuli with a high output quality requires a large number of modules. Systems\nwith few modules and pulse-width modulation may generate apparently smooth\ncurrent shapes in the highly inductive coil, but the stimulation effect of the\nneurons depends on the electric field and the electric field becomes a burst of\nultra-brief rectangular pulses. We propose an alternative solution that\nachieves high-resolution pulse shaping with fewer modules by implementing\nhigh-power wide-bandwidth voltage asymmetry. Rather than equal voltage steps,\nour system strategically assigns different voltages to each module to achieve a\nnear-exponential improvement in resolution. Compared to prior designs, our\nexperimental prototype achieved better output quality, although it uses only\nhalf the number of modules.",
      "generated_abstract": "ial magnetic stimulation (TMS) is a non-invasive and clinically\nfavorable method for enhancing neuronal activation in the brain. However,\ndifferent stimulation patterns can lead to different neural activation\npatterns. Consequently, the stimulation patterns that are most effective for\ndifferent applications may differ from one another. This paper presents a\npulse-shaping modular pulse synthesizer that can be used to create a wide\nvariety of stimulation patterns for TMS. We demonstrate that the modular\npulse synthesizer can be used to create pulse shapes that can be used to\ngenerate stimulation patterns with a wide variety of activation patterns. The\nmodular pulse synthesizer can be used to create pulse shapes that can be used\nto generate stimulation patterns with a wide variety of activation patterns.\nWe demonstrate that the modular pulse synthesizer can be",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17355371900826447,
          "p": 0.375,
          "f": 0.2372881312675158
        },
        "rouge-2": {
          "r": 0.024096385542168676,
          "p": 0.04938271604938271,
          "f": 0.032388659559737694
        },
        "rouge-l": {
          "r": 0.1322314049586777,
          "p": 0.2857142857142857,
          "f": 0.18079095612627288
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2502.13785v2",
      "true_abstract": "mRNA-based vaccines have become a major focus in the pharmaceutical industry.\nThe coding sequence as well as the Untranslated Regions (UTRs) of an mRNA can\nstrongly influence translation efficiency, stability, degradation, and other\nfactors that collectively determine a vaccine's effectiveness. However,\noptimizing mRNA sequences for those properties remains a complex challenge.\nExisting deep learning models often focus solely on coding region optimization,\noverlooking the UTRs. We present Helix-mRNA, a structured state-space-based and\nattention hybrid model to address these challenges. In addition to a first\npre-training, a second pre-training stage allows us to specialise the model\nwith high-quality data. We employ single nucleotide tokenization of mRNA\nsequences with codon separation, ensuring prior biological and structural\ninformation from the original mRNA sequence is not lost. Our model, Helix-mRNA,\noutperforms existing methods in analysing both UTRs and coding region\nproperties. It can process sequences 6x longer than current approaches while\nusing only 10% of the parameters of existing foundation models. Its predictive\ncapabilities extend to all mRNA regions. We open-source the model\n(https://github.com/helicalAI/helical) and model weights\n(https://huggingface.co/helical-ai/helix-mRNA).",
      "generated_abstract": "opment of mRNA therapeutics has the potential to transform the\nfield of oncology, with the potential for personalized therapeutics and\nprecision oncology. However, the development of mRNA therapeutics has\nbecome increasingly challenging, due to the complexities of mRNA structure and\nfunction. This paper introduces Helix-mRNA, a novel framework for the\ndevelopment of mRNA therapeutics that addresses these challenges. Helix-mRNA\nutilizes a hybrid foundation model that integrates sequence-specific and\nstructural information to enhance therapeutic efficacy. This approach\nenables the optimization of therapeutic efficacy, precision, and safety, while\nreducing the time and cost of development. Additionally, Helix-mRNA provides\na framework for the design of mRNA th",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.31746031746031744,
          "f": 0.20725388161292932
        },
        "rouge-2": {
          "r": 0.029069767441860465,
          "p": 0.054945054945054944,
          "f": 0.038022809162486626
        },
        "rouge-l": {
          "r": 0.14615384615384616,
          "p": 0.30158730158730157,
          "f": 0.1968911873124112
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.05908v1",
      "true_abstract": "We explain the fundamental challenges of sampling from multimodal\ndistributions, particularly for high-dimensional problems. We present the major\ntypes of MCMC algorithms that are designed for this purpose, including parallel\ntempering, mode jumping and Wang-Landau, as well as several state-of-the-art\napproaches that have recently been proposed. We demonstrate these methods using\nboth synthetic and real-world examples of multimodal distributions with\ndiscrete or continuous state spaces.",
      "generated_abstract": "A major challenge in Bayesian inference is the need to sample from a\nmulti-modal posterior distribution. In this paper, we propose a novel MCMC\nalgorithm for sampling from such distributions, which is based on a\nrepresentation that combines the full posterior density with a bivariate\napproximation. We derive the corresponding likelihood function, and show that\nour MCMC sampler is well-conditioned. Numerical experiments show that our\napproach can be more efficient than standard Metropolis-Hastings algorithms in\ncases with multiple modes. The proposed approach is implemented in the\nBayesianOptimization.jl package, and is also available as an R package. A\npreprint of the main results is available in the arXiv:2501.05908.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2909090909090909,
          "p": 0.19753086419753085,
          "f": 0.23529411282980112
        },
        "rouge-2": {
          "r": 0.015625,
          "p": 0.009523809523809525,
          "f": 0.011834314820911502
        },
        "rouge-l": {
          "r": 0.2545454545454545,
          "p": 0.1728395061728395,
          "f": 0.20588234812391878
        }
      }
    },
    {
      "paper_id": "math.GT.math/QA/2503.08861v1",
      "true_abstract": "We give invariants of flat bundles over 4-manifolds generalizing a result by\nChaidez, Cotler, and Cui (Alg. \\& Geo. Topology '22). We utilize a structure\ncalled a Hopf $G$-triplet for $G$ a group, which generalizes the notion of a\nHopf triplet by Chaidez, Cotler, and Cui. In our construction, we present flat\nbundles over 4-manifolds using colored trisection diagrams: a direct analogue\nof colored Heegaard diagrams as described by Virelizier. Our main result is\nthat involutory Hopf $G$-triplets of finite type yield well-defined invariants\nof $G$-colored trisection diagrams, and that if the monodromy of a flat bundle\nhas image in $G$ we obtain invariants of flat bundles. We also show that a\nspecial Hopf $G$-triplet yields the invariant from Hopf $G$-algebras described\nby Mochida, thus generalizing the construction.",
      "generated_abstract": "e a 4-manifold with a flat connection and $H$ be an involutory\nHopf group. In this paper, we construct an Hopf-Grothendieck algebroid $H\\rtimes\n\\mathrm{HFp}(M)$ which consists of the Hopf-Grothendieck algebroid associated\nwith the involutory Hopf group $H$ and the flat bundle over $M$. Then we\nconstruct an involutory Hopf-Grothendieck algebroid $H\\rtimes\n\\mathrm{HFp}(M)/\\mathrm{Diff}_+(M)$ which consists of the Hopf-Grothendieck algebrod\nassociated with the involutory Hopf group $H$ and the flat bundle over $M$ with\nrespect to the action of $\\mathrm{Diff}_+(M)$. We show that these algeb",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.189873417721519,
          "p": 0.35714285714285715,
          "f": 0.2479338797650435
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.06896551724137931,
          "f": 0.047058819033910466
        },
        "rouge-l": {
          "r": 0.189873417721519,
          "p": 0.35714285714285715,
          "f": 0.2479338797650435
        }
      }
    },
    {
      "paper_id": "physics.flu-dyn.nlin/CD/2503.08983v1",
      "true_abstract": "We study decaying turbulence in the 1D Burgers equation (Burgulence) and 3D\nNavier-Stokes (NS) turbulence. We first investigate the decay in time $t$ of\nthe energy $E(t)$ in Burgulence, for a fractional Brownian initial potential,\nwith Hurst exponent $H$, and demonstrate rigorously a self-similar time-decay\nof $E(t)$, previously determined heuristically. This is a consequence of the\nnontrivial boundedness of the energy for any positive time. We define a\nspatially forgetful \\textit{oblivious fractional Brownian motion} (OFBM), with\nHurst exponent $H$, and prove that Burgulence, with an OFBM as initial\npotential $\\varphi_0(x)$, is not only intermittent, but it also displays, a\nhitherto unanticipated, large-scale bifractality or multifractality; the latter\noccurs if we combine OFBMs, with different values of $H$. This is the first\nrigorous proof of genuine multifractality for turbulence in a nonlinear\nhydrodynamical partial differential equation. We then present direct numerical\nsimulations (DNSs) of freely decaying turbulence, capturing some aspects of\nthis multifractality. For Burgulence, we investigate such decay for two cases:\n(A) $\\varphi_0(x)$ a multifractal random walk that crosses over to a fractional\nBrownian motion beyond a crossover scale $\\mathcal{L}$, tuned to go from small-\nto large-scale multifractality; (B) initial energy spectra $E_0(k)$, with\nwavenumber $k$, having one or more power-law regions, which lead, respectively,\nto self-similar and non-self-similar energy decay. Our analogous DNSs of the 3D\nNS equations also uncover self-similar and non-self-similar energy decay.\nChallenges confronting the detection of genuine large-scale multifractality, in\nnumerical and experimental studies of NS and MHD turbulence, are highlighted.",
      "generated_abstract": "large-scale multifractality in three-dimensional turbulence using a\nlarge-eddy simulation approach. The turbulence is described by the Burgers\nequation, and the turbulent transport is dominated by large-scale fluctuations.\nIn the absence of dissipation, the Kolmogorov lengthscale is approximately\ntwo times the mean-free-path, which is approximately the mean-free-path divided\nby the mean-free-time. As a result, large-scale fluctuations decay on a timescale\napproximately equal to the inverse of the inverse of the Kolmogorov lengthscale\n(or the inverse of the mean-free-time). In the presence of dissipation, the\nKolmogorov lengthscale is reduced and the timescale for large-scale\ndecay is reduced as well. We show that in the absence of diss",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11764705882352941,
          "p": 0.32727272727272727,
          "f": 0.1730769191868529
        },
        "rouge-2": {
          "r": 0.00881057268722467,
          "p": 0.024096385542168676,
          "f": 0.012903221885328976
        },
        "rouge-l": {
          "r": 0.10457516339869281,
          "p": 0.2909090909090909,
          "f": 0.15384614995608367
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.09383v2",
      "true_abstract": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality.",
      "generated_abstract": "termath of the Covid-19 pandemic, this study explores how the UK's\nmillion-plus small and medium-sized enterprises (SMEs) responded to the\nemergency, with a focus on the impact on employment, turnover, and productivity\nduring the first year of the crisis. Our findings highlight the diversity of\nresponse across industries and regions, with the highest levels of turnover and\nmost rapid declines in employment recorded in London. We also find evidence of\na \"recession gap,\" with firms in the North of England experiencing the\nlargest declines in employment and turnover. This study provides a first\ncomprehensive assessment of how firms across the UK responded to the\nCovid-19 crisis, offering valuable insights for policymakers seeking to\naddress the economic fallout from the pandemic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16363636363636364,
          "p": 0.35064935064935066,
          "f": 0.22314049152892573
        },
        "rouge-2": {
          "r": 0.023255813953488372,
          "p": 0.04424778761061947,
          "f": 0.030487800361578603
        },
        "rouge-l": {
          "r": 0.14545454545454545,
          "p": 0.3116883116883117,
          "f": 0.19834710309917364
        }
      }
    },
    {
      "paper_id": "cs.CE.econ/GN/2502.12966v1",
      "true_abstract": "Ethereum has adopted a rollup-centric roadmap to scale by making rollups\n(layer 2 scaling solutions) the primary method for handling transactions. The\nfirst significant step towards this goal was EIP-4844, which introduced blob\ntransactions that are designed to meet the data availability needs of layer 2\nprotocols. This work constitutes the first rigorous and comprehensive empirical\nanalysis of transaction- and mempool-level data since the institution of blobs\non Ethereum on March 13, 2024. We perform a longitudinal study of the early\ndays of the blob fee market analyzing the landscape and the behaviors of its\nparticipants. We identify and measure the inefficiencies arising out of\nsuboptimal block packing, showing that at times it has resulted in up to 70%\nrelative fee loss. We hone in and give further insight into two (congested)\npeak demand periods for blobs. Finally, we document a market design issue\nrelating to subset bidding due to the inflexibility of the transaction\nstructure on packing data as blobs and suggest possible ways to fix it. The\nlatter market structure issue also applies more generally for any discrete\nobjects included within transactions.",
      "generated_abstract": "The Ethereum blockchain was designed to handle a large number of transactions\nin a very short period of time. To keep the network running, it introduced\nfees on transactions that are not included in the block. These fees are called\n\"blobs\". Early on, the fee structure was flexible and the market was not\nwell-developed. The introduction of the Ethereum 2.0 chain changed the fee\nstructure. This paper explores the evolution of the blob market and the\nlessons learned from the experience.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18604651162790697,
          "p": 0.42857142857142855,
          "f": 0.25945945523798397
        },
        "rouge-2": {
          "r": 0.03314917127071823,
          "p": 0.07792207792207792,
          "f": 0.0465116237194283
        },
        "rouge-l": {
          "r": 0.17054263565891473,
          "p": 0.39285714285714285,
          "f": 0.23783783361636238
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.16142v1",
      "true_abstract": "In this study, we investigate the integration of a large language model (LLM)\nwith an automatic speech recognition (ASR) system, specifically focusing on\nenhancing rare word recognition performance. Using a 190,000-hour dataset\nprimarily sourced from YouTube, pre-processed with Whisper V3 pseudo-labeling,\nwe demonstrate that the LLM-ASR architecture outperforms traditional\nZipformer-Transducer models in the zero-shot rare word recognition task, after\ntraining on a large dataset. Our analysis reveals that the LLM contributes\nsignificantly to improvements in rare word error rate (R-WER), while the speech\nencoder primarily determines overall transcription performance (Orthographic\nWord Error Rate, O-WER, and Normalized Word Error Rate, N-WER). Through\nextensive ablation studies, we highlight the importance of adapter integration\nin aligning speech encoder outputs with the LLM's linguistic capabilities.\nFurthermore, we emphasize the critical role of high-quality labeled data in\nachieving optimal performance. These findings provide valuable insights into\nthe synergy between LLM-based ASR architectures, paving the way for future\nadvancements in large-scale LLM-based speech recognition systems.",
      "generated_abstract": "rare word recognition (ZSRWR) is a challenging task where a LLM\nis only asked to recognize a limited number of rare words, which is different\nfrom the common practice of asking a LLM to recognize all the words in a\ncollection. In this paper, we propose a novel method for integrating a large\nlanguage model (LLM) with ZSRWR, where the LLM is fine-tuned on a subset of\nrare words in the training set and asked to recognize the remaining rare\nwords. We introduce a novel LLM-based pre-training method, named\nZero-Shot Rare Word Recognition Integration (ZSRWI), to achieve this goal.\nFurthermore, we propose a few fine-tuning strategies to improve the\nperformance of ZSRWR. Experimental results demonstrate that the proposed\nmethod can effectively improve the performance of ZSRWR by integrating a L",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24561403508771928,
          "p": 0.345679012345679,
          "f": 0.2871794823226825
        },
        "rouge-2": {
          "r": 0.09395973154362416,
          "p": 0.12389380530973451,
          "f": 0.10687022410203392
        },
        "rouge-l": {
          "r": 0.24561403508771928,
          "p": 0.345679012345679,
          "f": 0.2871794823226825
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.02381v1",
      "true_abstract": "We propose a new approach to estimating the random coefficient logit demand\nmodel for differentiated products when the vector of market-product level\nshocks is sparse. Assuming sparsity, we establish nonparametric identification\nof the distribution of random coefficients and demand shocks under mild\nconditions. Then we develop a Bayesian procedure, which exploits the sparsity\nstructure using shrinkage priors, to conduct inference about the model\nparameters and counterfactual quantities. Comparing to the standard BLP (Berry,\nLevinsohn, & Pakes, 1995) method, our approach does not require demand\ninversion or instrumental variables (IVs), thus provides a compelling\nalternative when IVs are not available or their validity is questionable. Monte\nCarlo simulations validate our theoretical findings and demonstrate the\neffectiveness of our approach, while empirical applications reveal evidence of\nsparse demand shocks in well-known datasets.",
      "generated_abstract": "In this paper, we extend the existing literature on discrete choice demand\nmodels by introducing a novel approach that enables us to estimate such models\nusing market-product shocks. Our approach is based on the assumption that\nmarket-product shocks are only present during the time periods when demand\nconditions are met. In this paper, we present a simple method for estimating\ndiscrete choice demand models with sparse market-product shocks. Our method\nrelies on the assumption that the market-product shocks are time-varying and\nnon-stationary. We provide theoretical results that provide insights into the\neffect of market-product shocks on the estimated parameters of the discrete\nchoice demand model. Our results highlight the importance of using market-product\nshocks during the time periods when demand conditions are met.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22448979591836735,
          "p": 0.3492063492063492,
          "f": 0.27329192070213343
        },
        "rouge-2": {
          "r": 0.0234375,
          "p": 0.031914893617021274,
          "f": 0.027027022144307352
        },
        "rouge-l": {
          "r": 0.1836734693877551,
          "p": 0.2857142857142857,
          "f": 0.2236024797083447
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.17271v1",
      "true_abstract": "In the context of scientific policy and science management, this study\nexamines the system of nonuniform wage distribution for researchers. A\nnonlinear mathematical model of optimal remuneration for scientific workers has\nbeen developed, considering key and additive aspects of scientific activity:\nbasic qualifications, research productivity, collaborative projects, skill\nenhancement, distinctions, and international collaborations. Unlike traditional\nlinear schemes, the proposed approach is based on exponential and logarithmic\ndependencies, allowing for the consideration of saturation effects and\npreventing artificial wage growth due to mechanical increases in scientific\nproductivity indicators.\n  The study includes detailed calculations of optimal, minimum, and maximum\nwages, demonstrating a fair distribution of remuneration on the basis of\nresearcher productivity. A linear increase in publication activity or grant\nfunding should not lead to uncontrolled salary growth, thus avoiding\ndistortions in the motivational system. The results of this study can be used\nto reform and modernize the wage system for researchers in Kazakhstan and other\ncountries, as well as to optimize grant-based science funding mechanisms. The\nproposed methodology fosters scientific motivation, long-term productivity, and\nthe internationalization of research while also promoting self-actualization\nand ultimately forming an adequate and authentic reward system for the research\ncommunity.\n  Specifically, in resource-limited scientific systems, science policy should\nfocus on the qualitative development of individual researchers rather than\nquantitative expansion (e.g., increasing the number of scientists). This can be\nachieved through the productive progress of their motivation and\nself-actualization.",
      "generated_abstract": "We consider the problem of optimal salaries for researchers with motivational\nevolution. We show that in the presence of externalities and a mechanism that\nleverages the motivational effects of scientists, optimal salaries depend on the\nsize of the externality and the time elapsed since the externality was\nintroduced. Our results are general, and the conclusions hold even if the\nscientists have no incentive to engage in research. The optimal salary is\nindependent of the time elapsed since the externality was introduced.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.3333333333333333,
          "f": 0.16666666291666676
        },
        "rouge-2": {
          "r": 0.02643171806167401,
          "p": 0.08695652173913043,
          "f": 0.0405405369651665
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.3333333333333333,
          "f": 0.16666666291666676
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.14984v1",
      "true_abstract": "We examine the growing gender gap in venture capital funding, focusing on\naccelerator programs in the U.S. We collect a unique dataset with detailed\ninformation on accelerators and startups. Using a two-stage methodology, we\nfirst estimate a matching model between startups and accelerators, and then use\nits output to analyze the gender gap in post-graduation outcomes through a\ncontrol function approach. Our results show that female-founded startups face a\nsignificant funding disadvantage, primarily due to relocation challenges tied\nto family obligations. However, larger cohorts and higher-quality accelerators\nhelp reduce this gap by offering female founders better networking\nopportunities and mentorship.",
      "generated_abstract": "the gender gap in venture capital (VC) funding by examining the\nfunding history of venture-backed startups, which are defined as companies\nwith at least one female founder. Our analysis reveals that the gender gap in\nVC funding has grown over time, with the gap widening over the past 10 years.\nWe demonstrate that this trend is driven by the increasing share of venture\ncapital funding allocated to firms with at least one female founder, which\nincreased from 33% to 43% over the past 10 years. We further show that the\ngender gap is worsened by the fact that venture capital firms are less likely\nto invest in startups with at least one female founder. Our findings suggest\nthat increasing the proportion of venture capital funding allocated to\nventures with at least one female founder can significantly narrow the gender",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21518987341772153,
          "p": 0.2463768115942029,
          "f": 0.22972972475255674
        },
        "rouge-2": {
          "r": 0.061224489795918366,
          "p": 0.05825242718446602,
          "f": 0.059701487540407834
        },
        "rouge-l": {
          "r": 0.21518987341772153,
          "p": 0.2463768115942029,
          "f": 0.22972972475255674
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.19006v2",
      "true_abstract": "In this note, I introduce a novel performance rating system called\nPerformance Rating Equilibrium (PRE). A PRE is a vector of hypothetical ratings\nfor each player, such that if these ratings were each player's initial rating\nat the start of a tournament, scoring the same points against the same\nopponents would leave each player's initial rating unchanged. In other words,\nall players' initial ratings perfectly predict their actual scores in the\ntournament. This property, however, does not hold for the well-known Tournament\nPerformance Rating. PRE is defined as a fixed point of a multidimensional\nrating function. I show that such a fixed point, and hence a PRE, exists under\nmild conditions. I provide an implementation of PRE along with several\nempirical applications. PREs have broad applicability, from sports competitions\nto the evaluation of large language models.",
      "generated_abstract": "We study the problem of assigning individuals to firms to maximize total\nperformance. The performance of a firm is determined by the performance of its\nemployees, which are assigned to it in a rationing-based matching process. We\nconsider the case where the firm is a market maker, and we show that if the\nfirm is well-motivated and employs a performance-based matching method, the\nperformance rating of the firm is the performance rating of its employees. We\nshow that, under a suitable condition, the performance rating of the firm is\ndetermined by the performance rating of its employees. We then provide a\nconsistent formulation of the performance rating equilibrium of the firm. We\nalso show that the performance rating equilibrium is a Nash equilibrium.\nFinally, we show that the performance rating equilibrium is also the\nequilibrium of the market maker.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.24561403508771928,
          "f": 0.18064515664016662
        },
        "rouge-2": {
          "r": 0.03937007874015748,
          "p": 0.053763440860215055,
          "f": 0.04545454057396747
        },
        "rouge-l": {
          "r": 0.12244897959183673,
          "p": 0.21052631578947367,
          "f": 0.15483870502726338
        }
      }
    },
    {
      "paper_id": "cs.GT.cs/GT/2503.07558v1",
      "true_abstract": "We introduce the first formal model capturing the elicitation of unverifiable\ninformation from a party (the \"source\") with implicit signals derived by other\nplayers (the \"observers\"). Our model is motivated in part by applications in\ndecentralized physical infrastructure networks (a.k.a. \"DePIN\"), an emerging\napplication domain in which physical services (e.g., sensor information,\nbandwidth, or energy) are provided at least in part by untrusted and\nself-interested parties. A key challenge in these signal network applications\nis verifying the level of service that was actually provided by network\nparticipants.\n  We first establish a condition called source identifiability, which we show\nis necessary for the existence of a mechanism for which truthful signal\nreporting is a strict equilibrium. For a converse, we build on techniques from\npeer prediction to show that in every signal network that satisfies the source\nidentifiability condition, there is in fact a strictly truthful mechanism,\nwhere truthful signal reporting gives strictly higher total expected payoff\nthan any less informative equilibrium. We furthermore show that this truthful\nequilibrium is in fact the unique equilibrium of the mechanism if there is\npositive probability that any one observer is unconditionally honest (e.g., if\nan observer were run by the network owner). Also, by extending our condition to\ncoalitions, we show that there are generally no collusion-resistant mechanisms\nin the settings that we consider.\n  We apply our framework and results to two DePIN applications: proving\nlocation, and proving bandwidth. In the location-proving setting observers\nlearn (potentially enlarged) Euclidean distances to the source. Here, our\ncondition has an appealing geometric interpretation, implying that the source's\nlocation can be truthfully elicited if and only if it is guaranteed to lie\ninside the convex hull of the observers.",
      "generated_abstract": "aper, we present a framework for incentive-compatible recovery from\nmanipulated signals, with applications to decentralized physical infrastructure\n(DPI). Specifically, we consider a scenario where a malicious entity,\n$M$, wishes to cause a malicious change in the value of a signal, $S$, which\nis being observed by a legitimate agent, $L$. In this scenario, the malicious\nentity may be able to alter $S$ without being detected by $L$, which can\nsignificantly affect the legitimate agent's decision-making process. To\naddress this, we propose a two-agent protocol that enables $L$ to recover the\noriginal value of $S$ from the manipulated signal. The protocol is designed to\nensure that $L$ is incentivized to recover $S$ correctly. Our protocol is\ncomplemented by a novel analysis of the incentive-compatibility",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15028901734104047,
          "p": 0.32098765432098764,
          "f": 0.2047244051047803
        },
        "rouge-2": {
          "r": 0.02214022140221402,
          "p": 0.05128205128205128,
          "f": 0.030927830839223663
        },
        "rouge-l": {
          "r": 0.1329479768786127,
          "p": 0.2839506172839506,
          "f": 0.1811023578606858
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2411.04694v1",
      "true_abstract": "Introduction: Circadian rhythm disruption has garnered significant attention\nfor its adverse effects on human health, particularly in reproductive medicine\nand fetal well-being. Assessing pregnancy health often relies on diagnostic\nmarkers such as the labyrinth zone (LZ) proportion within the placenta. This\nstudy aimed to investigate the impact of disrupted circadian rhythms on\nplacental health and fetal development using animal models. Methods and\nResults: Employing unstained photo-acoustic microscopy (PAM) and hematoxylin\nand eosin (HE)-stained images, we found them mutually reinforcing. Our images\nrevealed the role of MCRD on the LZ and fetus weight: a decrease in LZ area\nfrom 5.01-HE(4.25-PAM) mm2 to 3.58-HE (2.62-PAM) mm2 on day 16 and\n6.48-HE(5.16-PAM) mm2 to 4.61-HE (3.03-PAM) mm2 on day 18, resulting in 0.71\ntimes lower fetus weights. We have discriminated a decrease in the mean LZ to\nplacenta area ratio from 64% to 47% on day 18 in mice with disrupted circadian\nrhythms with PAM. Discussion: The study highlights the negative influence of\ncircadian rhythm disruption on placental development and fetal well-being.\nReduced LZ area and fetal weights in the MCRD group suggest compromised\nplacental function under disrupted circadian rhythms. PAM imaging proved to be\nan efficient technique for assessing placental development, offering advantages\nover traditional staining methods. These findings contribute to understanding\nthe mechanisms underlying circadian disruption's effects on reproductive health\nand fetal development, emphasizing the importance of maintaining normal\ncircadian rhythms for optimal pregnancy outcomes. Further research is needed to\nexplore interventions to mitigate these effects and improve pregnancy outcomes.",
      "generated_abstract": "rhythm (CR) is a biological system that regulates biological\nclocks and drives homeostasis through the expression of genes and proteins that\nundergo rhythmic changes. Photo-acoustic imaging (PAI) is a non-invasive\ntechnique that captures the ultrasonic signal emitted by tissues during the\nexcitation process. This study aimed to investigate the effects of CR disruption\non placental development in mice using photo-acoustic imaging (PAI).\nCircadian-disrupted (CD) mice were exposed to a 24-hour light/dark cycle\n(L/D) for 14 days. A light exposure at the optimal time of 9 h was applied to\nthe mice. The placentas of CD mice were examined using PAI. The results showed\nthat",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15337423312883436,
          "p": 0.32051282051282054,
          "f": 0.2074688752900261
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.09,
          "f": 0.053892211373660195
        },
        "rouge-l": {
          "r": 0.15337423312883436,
          "p": 0.32051282051282054,
          "f": 0.2074688752900261
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/OT/2406.10612v1",
      "true_abstract": "A key output of network meta-analysis (NMA) is the relative ranking of the\ntreatments; nevertheless, it has attracted a lot of criticism. This is mainly\ndue to the fact that ranking is an influential output and prone to\nover-interpretations even when relative effects imply small differences between\ntreatments. To date, common ranking methods rely on metrics that lack a\nstraightforward interpretation, while it is still unclear how to measure their\nuncertainty. We introduce a novel framework for estimating treatment\nhierarchies in NMA. At first, we formulate a mathematical expression that\ndefines a treatment choice criterion (TCC) based on clinically important\nvalues. This TCC is applied to the study treatment effects to generate paired\ndata indicating treatment preferences or ties. Then, we synthesize the paired\ndata across studies using an extension of the so-called \"Bradley-Terry\" model.\nWe assign to each treatment a latent variable interpreted as the treatment\n\"ability\" and we estimate the ability parameters within a regression model.\nHigher ability estimates correspond to higher positions in the final ranking.\nWe further extend our model to adjust for covariates that may affect treatment\nselection. We illustrate the proposed approach and compare it with alternatives\nin two datasets: a network comparing 18 antidepressants for major depression\nand a network comparing 6 antihypertensives for the incidence of diabetes. Our\napproach provides a robust and interpretable treatment hierarchy which accounts\nfor clinically important values and is presented alongside with uncertainty\nmeasures. Overall, the proposed framework offers a novel approach for ranking\nin NMA based on concrete criteria and preserves from over-interpretation of\nunimportant differences between treatments.",
      "generated_abstract": "Network meta-analysis is a method for pooling evidence from different\ntreatment studies to address the question of which treatment is best. In this\narticle, we introduce a probabilistic framework for producing treatment\nhierarchies in network meta-analysis, and we apply it to a real-world example\nfrom the field of clinical trials. We show how the framework can be used to\nproduce a hierarchy of treatment options based on probability estimates, and we\nillustrate this with a simulation study.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16981132075471697,
          "p": 0.48214285714285715,
          "f": 0.25116278684521365
        },
        "rouge-2": {
          "r": 0.024193548387096774,
          "p": 0.08,
          "f": 0.03715169922073475
        },
        "rouge-l": {
          "r": 0.15723270440251572,
          "p": 0.44642857142857145,
          "f": 0.23255813568242298
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.04929v1",
      "true_abstract": "Planning and control for high-dimensional robot manipulators in cluttered,\ndynamic environments require both computational efficiency and robust safety\nguarantees. Inspired by recent advances in learning configuration-space\ndistance functions (CDFs) as robot body representations, we propose a unified\nframework for motion planning and control that formulates safety constraints as\nCDF barriers. A CDF barrier approximates the local free configuration space,\nsubstantially reducing the number of collision-checking operations during\nmotion planning. However, learning a CDF barrier with a neural network and\nrelying on online sensor observations introduce uncertainties that must be\nconsidered during control synthesis. To address this, we develop a\ndistributionally robust CDF barrier formulation for control that explicitly\naccounts for modeling errors and sensor noise without assuming a known\nunderlying distribution. Simulations and hardware experiments on a 6-DoF xArm\nmanipulator show that our neural CDF barrier formulation enables efficient\nplanning and robust real-time safe control in cluttered and dynamic\nenvironments, relying only on onboard point-cloud observations.",
      "generated_abstract": "Manipulation planning and control are fundamental tasks in robotics, yet\nthere is no clear theoretical understanding of the underlying optimality\nconstraints. This limits the design of optimal control and planning algorithms,\nwhich can be unstable and slow to converge. We present a novel neural\nreinforcement learning framework for manipulation planning and control that\naddresses these limitations. Our approach is based on neural configurations,\na generalization of neural representations that can represent a wide range of\ncomplex patterns. We propose a neural configuration-space barrier method to\nminimize the barrier, thereby improving stability and convergence. We demonstrate\nthe effectiveness of our method through experiments on both simulated and real\nrobotic manipulation tasks. Our method improves performance in both the\nsimulated and real-world robotic manipulation tasks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20952380952380953,
          "p": 0.2716049382716049,
          "f": 0.23655913486819294
        },
        "rouge-2": {
          "r": 0.04794520547945205,
          "p": 0.05982905982905983,
          "f": 0.053231934224291684
        },
        "rouge-l": {
          "r": 0.18095238095238095,
          "p": 0.2345679012345679,
          "f": 0.20430107035206396
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/DB/2503.10036v1",
      "true_abstract": "Concurrency control (CC) algorithms are important in modern transactional\ndatabases, as they enable high performance by executing transactions\nconcurrently while ensuring correctness. However, state-of-the-art CC\nalgorithms struggle to perform well across diverse workloads, and most do not\nconsider workload drifts.\n  In this paper, we propose CCaaLF (Concurrency Control as a Learnable\nFunction), a novel learned concurrency control algorithm designed to achieve\nhigh performance across varying workloads. The algorithm is quick to optimize,\nmaking it robust against dynamic workloads. CCaaLF learns an agent function\nthat captures a large number of design choices from existing CC algorithms. The\nfunction is implemented as an efficient in-database lookup table that maps\ndatabase states to concurrency control actions. The learning process is based\non a combination of Bayesian optimization and a novel graph reduction\nalgorithm, which converges quickly to a function that achieves high transaction\nthroughput. We compare CCaaLF against five state-of-the-art CC algorithms and\nshow that our algorithm consistently outperforms them in terms of transaction\nthroughput and optimization time.",
      "generated_abstract": "cy control is a fundamental design principle in database systems,\nplaying a crucial role in preventing data corruption, providing fault\ntolerance, and ensuring data consistency. However, the existing concurrency\ncontrol models are often impractical due to their complex, hard-to-understand\nformulas and their high computational complexity. To address these issues, we\npropose Concurrent-Aware Learnable Function (CCaLF), a learnable concurrency\ncontrol model that incorporates the concept of concurrency into the\nformula. CCaLF consists of two key components: the concurrency control formula\nand the concurrency control model. The concurrency control formula represents\nthe concurrency control objective, which determines the trade-off between the\nprobability of data corruption and the probability of data inconsistency. The\nconcurrency control model is responsible for learning the optimal concurrency\ncontrol formula given a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19130434782608696,
          "p": 0.2857142857142857,
          "f": 0.2291666618625218
        },
        "rouge-2": {
          "r": 0.01282051282051282,
          "p": 0.01904761904761905,
          "f": 0.015325665688996093
        },
        "rouge-l": {
          "r": 0.17391304347826086,
          "p": 0.2597402597402597,
          "f": 0.20833332852918848
        }
      }
    },
    {
      "paper_id": "physics.acc-ph.physics/plasm-ph/2503.09557v1",
      "true_abstract": "We present a novel approach for generating collider-quality electron bunches\nusing a plasma photoinjector. The approach leverages recently developed\ntechniques for the spatiotemporal control of laser pulses to produce a moving\nionization front in a nonlinear plasma wave. The moving ionization front\ngenerates an electron bunch with a current profile that balances the\nlongitudinal electric field of an electron beam-driven plasma wave, creating a\nuniform accelerating field across the bunch. Particle-in-cell (PIC) simulations\nof the ionization stage show the formation of an electron bunch with 220 pC\ncharge and low emittance ($\\epsilon_x = 171$ nm-rad, $\\epsilon_y = 76$ nm-rad).\nQuasistatic PIC simulations of the acceleration stage show that the bunch is\nefficiently accelerated to 20 GeV over 2 meters with a final energy spread of\nless than 1\\% and emittances of $\\epsilon_x = 177$ nm-rad and $\\epsilon_y = 82$\nnm-rad. This high-quality electron bunch meets the requirements outlined by the\nSnowmass process for intermediate-energy colliders and compares favorably to\nthe beam quality of proposed and existing accelerator facilities. The results\nestablish the feasibility of plasma photoinjectors for future collider\napplications making a significant step towards the realization of\nhigh-luminosity, compact accelerators for particle physics research.",
      "generated_abstract": "We present a novel all-optical plasma photoinjector that generates high-energy\nelectron bunches suitable for future electron-positron colliders. The photoinjector\nutilizes a low-power, single-mode, all-fiber laser to produce a continuous\nstream of electron bunches with energies above 100 MeV, while the main\nfeed-in-line is coated with an ultra-thin metallic film to prevent the\nphotoinjector from heating up the electron bunches. The photoinjector is\ncharacterized by a low power consumption, a high-efficiency laser, and a\nhigh-performance fiber optics. The experimental results show that the\nphotoinjector can generate electron bunches with a typical energy of 150 MeV\nand a bunch length of 400 $\\mu$m.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23015873015873015,
          "p": 0.4084507042253521,
          "f": 0.29441623904455155
        },
        "rouge-2": {
          "r": 0.06111111111111111,
          "p": 0.11827956989247312,
          "f": 0.08058607609386975
        },
        "rouge-l": {
          "r": 0.2222222222222222,
          "p": 0.39436619718309857,
          "f": 0.28426395478059213
        }
      }
    },
    {
      "paper_id": "astro-ph.HE.astro-ph/HE/2503.09562v1",
      "true_abstract": "The prompt emission of Gamma-Ray Bursts (GRBs) could be composed of different\nspectral components, such as a dominant non-thermal Band component in the\nkeV-MeV range, a subdominant quasi-thermal component, and an additional hard\nnon-thermal component extending into the GeV range. The existence and\nevolutionary behaviors of these components could place strong implication on\nphysical models, such as ejecta composition and dissipation processes. Although\nnumerous GRBs have been found to exhibit one or two spectral components,\nreports of GRBs containing all three components remain rare. In this letter,\nbased on the joint observations of GRB 240825A from multiple gamma-ray\ntelescopes, we conduct a comprehensive temporal and spectral analysis to\nidentify the presence and evolution of all three components. The bulk Lorentz\nfactor of this bright and relatively short-duration burst is independently\ncalculated using the thermal and hard non-thermal components, supporting a jet\npenetration scenario. The multi-segment broken powerlaw feature observed in the\nflux light curves suggests the presence of an early afterglow in the keV-MeV\nband and hints at a possible two-jet structure. Furthermore, the observed\ntransition from positive to negative on the spectral lag can be interpreted as\nan independent evolution of the soft and hard components, leading to\nmisalignment in the cross-correlation function (CCF) analysis of pulses.",
      "generated_abstract": "t an analysis of the prompt emission of the gravitational\nramp-up GRB 240825A, based on the analysis of the data collected during the\nfirst 100 s after the trigger. We focus on the spectral evolution of the\nprompt emission, from the prompt-burst spectral shape to the afterglow\nspectral shape, and on the evolution of the prompt emission compared to the\nafterglow emission. We also discuss the possible contribution of the\nbackground-subtracted point-like source to the observed spectrum. Our\nfindings reveal that the prompt-emission spectrum evolves from a flat\nspectrum to a steep spectrum, with a power-law index of -1.5$\\pm$0.2 at\n$\\nu_{\\rm p,obs}$ = 100 Hz, consistent with the model of a k-cored power-law\nspectrum.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14074074074074075,
          "p": 0.2835820895522388,
          "f": 0.18811880744779932
        },
        "rouge-2": {
          "r": 0.04639175257731959,
          "p": 0.09,
          "f": 0.06122448530704832
        },
        "rouge-l": {
          "r": 0.1037037037037037,
          "p": 0.208955223880597,
          "f": 0.1386138569527499
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.17072v2",
      "true_abstract": "General Equilibrium Theory is the benchmark of economics, especially its\nresults concerning the efficient allocation of resources, known as the First\nand Second Welfare Theorems. Yet, General Equilibrium Theory is beyond the\nscope of most economists. This paper is pitched as the first entry point into\nthe theory. General Equilibrium Theory proves that at least one state of\nequilibrium always exists. In its most general approach, it uses fixed-point\ntheorems to this end. This paper discusses the assumptions on individuals'\nbehaviour and the structure of the system of exchange that guarantee that the\nconditions of the fixed-point theorems are satisfied. The purpose is to lay\nbare the role each plays in proving the existence of equilibrium and provide a\nclear picture of the relationship between the assumptions and the result. The\ndiscussion is presented in the simplest possible setting that captures the\nfundamental features of commodity exchange.",
      "generated_abstract": "We consider a market for a commodity which is traded between a buyer and a\nsupplier. The buyer is a utility maximizer, the supplier a subjective\nutility maximizer. The buyer has a set of preferences over the commodity.\nBoth preferences and the price are observed. The supplier has a set of\npreferences over the commodity. The supplier has observed the buyer's\npreferences, and he may choose to adjust his preferences to match the buyer's\npreferences. We show that in a general equilibrium, the supplier's preferences\nmust be consistent with the buyer's preferences. We show that the supplier's\npreferences must be either subjective or objectively optimal. The supplier\npreferences may be either a subset of the buyer's preferences or a subset of\nthe market's preferences.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14130434782608695,
          "p": 0.24528301886792453,
          "f": 0.17931034018929853
        },
        "rouge-2": {
          "r": 0.022556390977443608,
          "p": 0.03125,
          "f": 0.02620086849297396
        },
        "rouge-l": {
          "r": 0.14130434782608695,
          "p": 0.24528301886792453,
          "f": 0.17931034018929853
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.02075v1",
      "true_abstract": "Aligning a lens system relative to an imager is a critical challenge in\ncamera manufacturing. While optimal alignment can be mathematically computed\nunder ideal conditions, real-world deviations caused by manufacturing\ntolerances often render this approach impractical. Measuring these tolerances\ncan be costly or even infeasible, and neglecting them may result in suboptimal\nalignments. We propose a reinforcement learning (RL) approach that learns\nexclusively in the pixel space of the sensor output, eliminating the need to\ndevelop expert-designed alignment concepts. We conduct an extensive benchmark\nstudy and show that our approach surpasses other methods in speed, precision,\nand robustness. We further introduce relign, a realistic, freely explorable,\nopen-source simulation utilizing physically based rendering that models optical\nsystems with non-deterministic manufacturing tolerances and noise in robotic\nalignment movement. It provides an interface to popular machine learning\nframeworks, enabling seamless experimentation and development. Our work\nhighlights the potential of RL in a manufacturing environment to enhance\nefficiency of optical alignments while minimizing the need for manual\nintervention.",
      "generated_abstract": "r proposes a reinforcement learning-based approach to align lens\nsystems with a given optical axis. The approach is based on a generative\nmodel that estimates the position of the optical axis using the position of\nthe lens's focus and the position of the lens's image plane. The model is trained\non synthetic images, and it is tested on real images from the LSST survey and\nthe Sloan Digital Sky Survey (SDSS). The proposed method is compared with a\nconventional method based on the least squares approach. The results show that\nthe reinforcement learning approach performs better, with a mean absolute error\nof 0.0011 and a median of 0.0005. The algorithm is simple, requiring only\nestimating the position of the focus. The approach can be applied to other\nlens systems, such as the 8-m Keck Teles",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15702479338842976,
          "p": 0.24050632911392406,
          "f": 0.18999999522050015
        },
        "rouge-2": {
          "r": 0.0375,
          "p": 0.05042016806722689,
          "f": 0.04301074779614912
        },
        "rouge-l": {
          "r": 0.1487603305785124,
          "p": 0.22784810126582278,
          "f": 0.17999999522050011
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.00450v1",
      "true_abstract": "We propose and study three confidence intervals (CIs) centered at an\nestimator that is intentionally biased to reduce mean squared error. The first\nCI simply uses an unbiased estimator's standard error; compared to centering at\nthe unbiased estimator, this CI has higher coverage probability for confidence\nlevels above 91.7%, even if the biased and unbiased estimators have equal mean\nsquared error. The second CI trades some of this \"excess\" coverage for shorter\nlength. The third CI is centered at a convex combination of the two estimators\nto further reduce length. Practically, these CIs apply broadly and are simple\nto compute.",
      "generated_abstract": "er the problem of constructing confidence intervals for the mean of\na biased estimator, and propose an estimator that improves on the standard\n$\\alpha$-confidence interval. Our estimator is based on the fact that the bias\nof the estimator is controlled by a function of the mean and variance of the\ntrue distribution. We show that this estimator is asymptotically efficient and\ngives an $\\alpha$-confidence interval with probability tending to one. We also\nprove that the estimator is asymptotically efficient and asymptotically\n$\\alpha$-consistent, and that the confidence interval has asymptotic\n$\\alpha$-consistency and asymptotic efficiency. We show that the estimator is\nalso asymptotically efficient and asymptotically $\\alpha$-consistent when the\nestimator is replaced by a randomized estimator that is consistent with the\nsame distributional assumptions as the estimator. We provide an illustr",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3,
          "p": 0.3387096774193548,
          "f": 0.31818181320018374
        },
        "rouge-2": {
          "r": 0.052083333333333336,
          "p": 0.04950495049504951,
          "f": 0.05076141632301836
        },
        "rouge-l": {
          "r": 0.24285714285714285,
          "p": 0.27419354838709675,
          "f": 0.25757575259412313
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2410.03976v1",
      "true_abstract": "Asynchronous Boolean networks are a type of discrete dynamical system in\nwhich each variable can take one of two states, and a single variable state is\nupdated in each time step according to pre-selected rules. Boolean networks are\npopular in systems biology due to their ability to model long-term biological\nphenotypes within a qualitative, predictive framework. Boolean networks model\nphenotypes as attractors, which are closely linked to minimal trap spaces\n(inescapable hypercubes in the system's state space). In biological\napplications, attractors and minimal trap spaces are typically in one-to-one\ncorrespondence. However, this correspondence is not guaranteed: motif-avoidant\nattractors (MAAs) that lie outside minimal trap spaces are possible.\n  MAAs are rare and (despite recent efforts) poorly understood. Here we\nsummarize the current state of knowledge regarding MAAs and present several\nnovel observations regarding their response to node deletion reductions and\nlinear extensions of edges. We conduct large-scale computational studies on an\nensemble of 14,000 models derived from published Boolean models of biological\nsystems, and more than 100 million Random Boolean Networks. Our findings\nquantify the rarity of MAAs (in particular, we found no MAAs in the biological\nmodels), but highlight the role of network reduction in introducing MAAs into\nthe dynamics. We also show that MAAs are fragile to linear extensions: in\nsparse networks, even a single linear node can disrupt virtually all MAAs.\nMotivated by this observation, we improve the upper bound on the number of\ndelays needed to disrupt a motif-avoidant attractor.",
      "generated_abstract": "idance is a well-known property of Boolean networks that prevents\nthe emergence of attractors with local neighborhoods that consist of elements\nof the same or similar motifs. We study a generalization of motif-avoidance\nwhich we call motif-avoidant attractors, and investigate their existence in\nasynchronous Boolean networks. We find that motif-avoidant attractors are\nrare in asynchronous Boolean networks, but not only because of the presence of\nmotifs. We show that the prevalence of motif-avoidant attractors in asynchronous\nBoolean networks is due to the fact that asynchronous Boolean networks are\nstructurally different from synchronous Boolean networks. We discuss a\npossibility to improve the detection of motif-avoidant attractors in asynchronous\nBoolean networks by using their local neighborhoods, and conclude with a\ndiscussion on how the lack of motif-avoidant attractors",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1830065359477124,
          "p": 0.417910447761194,
          "f": 0.25454545030950415
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.0673076923076923,
          "f": 0.04179104049472087
        },
        "rouge-l": {
          "r": 0.1568627450980392,
          "p": 0.3582089552238806,
          "f": 0.2181818139458678
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2412.17354v3",
      "true_abstract": "In this study, we introduce a novel methodological framework called Bayesian\nPenalized Empirical Likelihood (BPEL), designed to address the computational\nchallenges inherent in empirical likelihood (EL) approaches. Our approach has\ntwo primary objectives: (i) to enhance the inherent flexibility of EL in\naccommodating diverse model conditions, and (ii) to facilitate the use of\nwell-established Markov Chain Monte Carlo (MCMC) sampling schemes as a\nconvenient alternative to the complex optimization typically required for\nstatistical inference using EL. To achieve the first objective, we propose a\npenalized approach that regularizes the Lagrange multipliers, significantly\nreducing the dimensionality of the problem while accommodating a comprehensive\nset of model conditions. For the second objective, our study designs and\nthoroughly investigates two popular sampling schemes within the BPEL context.\nWe demonstrate that the BPEL framework is highly flexible and efficient,\nenhancing the adaptability and practicality of EL methods. Our study highlights\nthe practical advantages of using sampling techniques over traditional\noptimization methods for EL problems, showing rapid convergence to the global\noptima of posterior distributions and ensuring the effective resolution of\ncomplex statistical inference challenges.",
      "generated_abstract": "penalized empirical likelihood (PEL) has emerged as a powerful\nmethod for Bayesian inference in survival analysis. Despite its advantages,\nPEL faces two key challenges: (1) the computationally expensive computation of\nthe empirical likelihood function, and (2) the lack of explicit expressions\nfor the asymptotic distribution of the Markov chain Monte Carlo (MCMC) sampler\nthat is used to generate the posterior. We address these issues by proposing a\nnovel Markov chain sampler that combines a novel algorithm for computing the\npenalized empirical likelihood, the so-called Penalized Penalized Empirical\nLikelihood (PPEL), with a new MCMC sampler that we develop for the\nnonparametric Bayesian inference in survival analysis. We show that the\nPPEL-based sampler provides an efficient and robust algorithm for the MCMC",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2396694214876033,
          "p": 0.3717948717948718,
          "f": 0.2914572816656146
        },
        "rouge-2": {
          "r": 0.05172413793103448,
          "p": 0.08571428571428572,
          "f": 0.06451612433807412
        },
        "rouge-l": {
          "r": 0.2066115702479339,
          "p": 0.32051282051282054,
          "f": 0.25125627664048894
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.12024v1",
      "true_abstract": "Mean field equilibrium (MFE) has emerged as a computationally tractable\nsolution concept for large dynamic games. However, computing MFE remains\nchallenging due to nonlinearities and the absence of contraction properties,\nlimiting its reliability for counterfactual analysis and comparative statics.\nThis paper focuses on MFE in dynamic models where agents interact through a\nscalar function of the population distribution, referred to as the\n\\textit{scalar interaction function}. Such models naturally arise in a wide\nrange of applications in operations and economics, including quality ladder\nmodels, inventory competition, online marketplaces, and heterogeneous-agent\nmacroeconomic models. The main contribution of this paper is to introduce\niterative algorithms that leverage the scalar interaction structure and are\nguaranteed to converge to the MFE under mild assumptions. Unlike existing\napproaches, our algorithms do not rely on monotonicity or contraction\nproperties, significantly broadening their applicability. Furthermore, we\nprovide a model-free algorithm that learns the MFE by employing simulation and\nreinforcement learning techniques such as Q-learning and policy gradient\nmethods without requiring prior knowledge of payoff or transition functions. We\nestablish finite-time performance bounds for this algorithm under technical\nLipschitz continuity assumptions. We apply our algorithms to classic models of\ndynamic competition, such as capacity competition, and to competitive models\nmotivated by online marketplaces, including ridesharing, dynamic reputation,\nand inventory competition, as well as to social learning models. Using our\nalgorithms, we derive reliable comparative statics results that illustrate how\nkey market parameters influence equilibrium outcomes in these stylized models,\nproviding insights that could inform the design of competitive systems in these\ncontexts.",
      "generated_abstract": "Mean field equilibrium (MFE) is a key concept in game theory, and the\nunderlying ideas of MFE are fundamental to many applications in economics and\nother fields. In this paper, we consider the mean field equilibrium problem\nwith scalar interactions and introduce a novel algorithm to compute it. The\nalgorithm is based on a novel approximation of the expectation of the\ninteraction vector, which is based on the convexity of the function whose\nexpectation is approximated. The convergence of the algorithm is guaranteed by\nan analysis of the convergence of the approximation. We also develop an\nalgorithm to compute the mean field equilibrium when interactions are\nnon-negativity constrained. This algorithm is based on the convexity of the\nfunction whose expectation is approximated. We prove its convergence and\nestablish the convergence rate of the algorithm. We then use our algorithms to\nsolve problems of learning MFE and MFE with non-negativity constraints.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21212121212121213,
          "p": 0.4605263157894737,
          "f": 0.29045642721716225
        },
        "rouge-2": {
          "r": 0.03278688524590164,
          "p": 0.06779661016949153,
          "f": 0.044198890633375496
        },
        "rouge-l": {
          "r": 0.18787878787878787,
          "p": 0.40789473684210525,
          "f": 0.25726140647027435
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.15655v1",
      "true_abstract": "We study the local geometry of empirical risks in high dimensions via the\nspectral theory of their Hessian and information matrices. We focus on settings\nwhere the data, $(Y_\\ell)_{\\ell =1}^n\\in \\mathbb R^d$, are i.i.d. draws of a\n$k$-component Gaussian mixture model, and the loss depends on the projection of\nthe data into a fixed number of vectors, namely $\\mathbf{x}^\\top Y$, where\n$\\mathbf{x}\\in \\mathbb{R}^{d\\times C}$ are the parameters, and $C$ need not\nequal $k$. This setting captures a broad class of problems such as\nclassification by one and two-layer networks and regression on multi-index\nmodels. We prove exact formulas for the limits of the empirical spectral\ndistribution and outlier eigenvalues and eigenvectors of such matrices in the\nproportional asymptotics limit, where the number of samples and dimension\n$n,d\\to\\infty$ and $n/d=\\phi \\in (0,\\infty)$. These limits depend on the\nparameters $\\mathbf{x}$ only through the summary statistic of the $(C+k)\\times\n(C+k)$ Gram matrix of the parameters and class means, $\\mathbf{G} =\n(\\mathbf{x},\\mathbf{\\mu})^\\top(\\mathbf{x},\\mathbf{\\mu})$. It is known that\nunder general conditions, when $\\mathbf{x}$ is trained by stochastic gradient\ndescent, the evolution of these same summary statistics along training\nconverges to the solution of an autonomous system of ODEs, called the effective\ndynamics. This enables us to connect the spectral theory to the training\ndynamics. We demonstrate our general results by analyzing the effective\nspectrum along the effective dynamics in the case of multi-class logistic\nregression. In this setting, the empirical Hessian and information matrices\nhave substantially different spectra, each with their own static and even\ndynamical spectral transitions.",
      "generated_abstract": "of high-dimensional mixture models (HDMMs) is a key topic in\napplied statistics. These models arise in various applications, such as\nclassification, clustering, and modeling. In this work, we study the geometry\nof HDMMs. Our main result is a new proof of the existence of an effective\nspectral theorem for HDMMs. This result provides a rigorous foundation for the\ndynamical behavior of HDMMs, including the existence of a unique invariant\nmeasure, the existence of invariant subspaces, and the existence of an\nattracting fixed point for the dynamics. In addition, we provide a proof of a\ncharacterization result that characterizes the transition between two\nattracting invariant subspaces. This result extends the known results for\nnearly-isotropic Gaussian HDMMs. Our approach unifies existing methods,\nenabling a more unified understanding of the dynamics of HDMMs.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.175,
          "p": 0.35443037974683544,
          "f": 0.2343096190052696
        },
        "rouge-2": {
          "r": 0.03829787234042553,
          "p": 0.07964601769911504,
          "f": 0.051724133545548
        },
        "rouge-l": {
          "r": 0.14375,
          "p": 0.2911392405063291,
          "f": 0.19246861482116917
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2503.07811v2",
      "true_abstract": "The theory of optimal transportation has developed into a powerful and\nelegant framework for comparing probability distributions, with wide-ranging\napplications in all areas of science. The fundamental idea of analyzing\nprobabilities by comparing their underlying state space naturally aligns with\nthe core idea of causal inference, where understanding and quantifying\ncounterfactual states is paramount. Despite this intuitive connection, explicit\nresearch at the intersection of optimal transport and causal inference is only\nbeginning to develop. Yet, many foundational models in causal inference have\nimplicitly relied on optimal transport principles for decades, without\nrecognizing the underlying connection. Therefore, the goal of this review is to\noffer an introduction to the surprisingly deep existing connections between\noptimal transport and the identification of causal effects with observational\ndata -- where optimal transport is not just a set of potential tools, but\nactually builds the foundation of model assumptions. As a result, this review\nis intended to unify the language and notation between different areas of\nstatistics, mathematics, and econometrics, by pointing out these existing\nconnections, and to explore novel problems and directions for future work in\nboth areas derived from this realization.",
      "generated_abstract": "r presents a comprehensive introduction to optimal transport (OT) for\ncausal inference with observational data, focusing on the applications of OT to\nestimation and inference in causal mediation analysis. We first review the\nbasics of OT, including its definition, properties, and the optimal transport\nmap. Next, we introduce the causal mediation framework for causal inference\nwith observational data, and discuss how OT can be used to estimate and\ninference the causal mediation function. We then discuss the properties of the\noptimal transport map, and provide a brief introduction to its connection to\nvarious machine learning algorithms, such as neural networks, kernel machines,\nand deep learning. We also provide a brief introduction to the optimization\nframework of OT for the estimation of causal mediation function, and the\nestimation of causal mediation function using the optimal transport map. We\nthen discuss some of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16129032258064516,
          "p": 0.2898550724637681,
          "f": 0.20725388141641396
        },
        "rouge-2": {
          "r": 0.05113636363636364,
          "p": 0.08181818181818182,
          "f": 0.06293705820333549
        },
        "rouge-l": {
          "r": 0.14516129032258066,
          "p": 0.2608695652173913,
          "f": 0.1865284928153777
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/DC/2503.08976v1",
      "true_abstract": "Federated Ranking Learning (FRL) is a state-of-the-art FL framework that\nstands out for its communication efficiency and resilience to poisoning\nattacks. It diverges from the traditional FL framework in two ways: 1) it\nleverages discrete rankings instead of gradient updates, significantly reducing\ncommunication costs and limiting the potential space for malicious updates, and\n2) it uses majority voting on the server side to establish the global ranking,\nensuring that individual updates have minimal influence since each client\ncontributes only a single vote. These features enhance the system's scalability\nand position FRL as a promising paradigm for FL training.\n  However, our analysis reveals that FRL is not inherently robust, as certain\nedges are particularly vulnerable to poisoning attacks. Through a theoretical\ninvestigation, we prove the existence of these vulnerable edges and establish a\nlower bound and an upper bound for identifying them in each layer. Based on\nthis finding, we introduce a novel local model poisoning attack against FRL,\nnamely the Vulnerable Edge Manipulation (VEM) attack. The VEM attack focuses on\nidentifying and perturbing the most vulnerable edges in each layer and\nleveraging an optimization-based approach to maximize the attack's impact.\nThrough extensive experiments on benchmark datasets, we demonstrate that our\nattack achieves an overall 53.23% attack impact and is 3.7x more impactful than\nexisting methods. Our findings highlight significant vulnerabilities in\nranking-based FL systems and underline the urgency for the development of new\nrobust FL frameworks.",
      "generated_abstract": "learning (FL) has been extensively studied for its ability to\nprotect privacy and securely share large datasets across heterogeneous\nclouds. However, the robustness of FL algorithms is a critical issue, as\nfailures in federated learning can lead to privacy leaks, resulting in the loss\nof data privacy and the inability to conduct fair and accurate training. In\nthis paper, we propose a novel framework for evaluating the robustness of\nFederated Learning (FL) algorithms. Specifically, we introduce a novel\nmeasure, robustness index, that evaluates the robustness of an FL algorithm\nagainst various attack types. To demonstrate the effectiveness of the proposed\nindex, we conduct a comprehensive evaluation of 30 state-of-the-art FL\nalgorithms. Our experimental results show that our proposed robustness index\nis able to effectively assess the robustness of FL algorithms,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.175,
          "p": 0.32941176470588235,
          "f": 0.2285714240399834
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.058823529411764705,
          "f": 0.03999999551200051
        },
        "rouge-l": {
          "r": 0.15625,
          "p": 0.29411764705882354,
          "f": 0.2040816281216161
        }
      }
    },
    {
      "paper_id": "math.NT.cs/DS/2503.10158v1",
      "true_abstract": "Integral linear systems $Ax=b$ with matrices $A$, $b$ and solutions $x$ are\nalso required to be in integers, can be solved using invariant factors of $A$\n(by computing the Smith Canonical Form of $A$). This paper explores a new\nproblem which arises in applications, that of obtaining conditions for solving\nthe Modular Linear System $Ax=b\\rem n$ given $A,b$ in $\\zz_n$ for $x$ in\n$\\zz_n$ along with the constraint that the value of the linear function\n$\\phi(x)=<w,x>$ is coprime to $n$ for some solution $x$. In this paper we\ndevelop decomposition of the system to coprime moduli $p^{r(p)}$ which are\ndivisors of $n$ and show how such a decomposition simplifies the computation of\nSmith form. This extends the well known index calculus method of computing the\ndiscrete logarithm where the moduli over which the linear system is reduced\nwere assumed to be prime (to solve the reduced systems over prime fields) to\nthe case when the factors of the modulus are prime powers $p^{r(p)}$. It is\nshown how this problem can be addressed effciently using the invariant factors\nand Smith form of the augmented matrix $[A,-p^{r(p)}I]$ and conditions modulo\n$p$ satisfied by $w$, where $p^{r(p)}$ vary over all divisors of $n$ with $p$\nprime.",
      "generated_abstract": "r presents a novel approach for solving linear systems with a\nconstraint, namely the Modular Linear System (MLS) problem. The approach\ninvolves a decomposition of the MLS into a Smith form and a set of\nextended Euclidean division (EED) problems modulo powers of prime divisors. In\nthis paper, we propose a parallel decomposition approach, where the\ndecomposition of the MLS into a Smith form and EED problems is performed in\nparallel using the MPI library. We further propose a novel parallel EED\nsolver, which is implemented using the OpenMP library. We compare the\nperformance of our approach with existing parallel EED solvers. We further\ndemonstrate the efficacy of our approach on real-world problems, including\nsolving the MLS problem with a constraint on the number of days between two\nevents. Our approach is implemented using the open-source MPI library\nMPI",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2543859649122807,
          "p": 0.37662337662337664,
          "f": 0.30366491665360057
        },
        "rouge-2": {
          "r": 0.06878306878306878,
          "p": 0.11206896551724138,
          "f": 0.0852458969257729
        },
        "rouge-l": {
          "r": 0.23684210526315788,
          "p": 0.35064935064935066,
          "f": 0.28272250827663725
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.15346v1",
      "true_abstract": "Drug discovery remains a slow and expensive process that involves many steps,\nfrom detecting the target structure to obtaining approval from the Food and\nDrug Administration (FDA), and is often riddled with safety concerns. Accurate\nprediction of how drugs interact with their targets and the development of new\ndrugs by using better methods and technologies have immense potential to speed\nup this process, ultimately leading to faster delivery of life-saving\nmedications. Traditional methods used for drug-target interaction prediction\nshow limitations, particularly in capturing complex relationships between drugs\nand their targets. As an outcome, deep learning models have been presented to\novercome the challenges of interaction prediction through their precise and\nefficient end results. By outlining promising research avenues and models, each\nwith a different solution but similar to the problem, this paper aims to give\nresearchers a better idea of methods for even more accurate and efficient\nprediction of drug-target interaction, ultimately accelerating the development\nof more effective drugs. A total of 180 prediction methods for drug-target\ninteractions were analyzed throughout the period spanning 2016 to 2025 using\ndifferent frameworks based on machine learning, mainly deep learning and graph\nneural networks. Additionally, this paper discusses the novelty, architecture,\nand input representation of these models.",
      "generated_abstract": "ew summarizes the current state-of-the-art in drug-target\ninteraction (DTI) and drug-target affinity prediction (DTAP). The focus is on\nthe application of deep learning (DL) models in DTI and DTAP. DL models for\nDTI are typically based on neural networks (NNs), but they may be applied to\na variety of DTAP tasks. The most widely used DL model for DTI is the\nResNet50, which has been extensively studied in the literature. However, it\nremains unclear if and how the performance of ResNet50 differs from that of\nother DL models, such as the VGG16. The most widely used DL model for DTAP is\nthe Long-Short-Term-Memory (LSTM) network, which has been extensively studied\nin the literature. However, it remains unclear if and how",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1925925925925926,
          "p": 0.3611111111111111,
          "f": 0.2512077249317371
        },
        "rouge-2": {
          "r": 0.02072538860103627,
          "p": 0.041237113402061855,
          "f": 0.02758620244447159
        },
        "rouge-l": {
          "r": 0.15555555555555556,
          "p": 0.2916666666666667,
          "f": 0.20289854618777578
        }
      }
    },
    {
      "paper_id": "cs.AI.nlin/CD/2503.09858v1",
      "true_abstract": "This paper investigates the complex interplay between AI developers,\nregulators, users, and the media in fostering trustworthy AI systems. Using\nevolutionary game theory and large language models (LLMs), we model the\nstrategic interactions among these actors under different regulatory regimes.\nThe research explores two key mechanisms for achieving responsible governance,\nsafe AI development and adoption of safe AI: incentivising effective regulation\nthrough media reporting, and conditioning user trust on commentariats'\nrecommendation. The findings highlight the crucial role of the media in\nproviding information to users, potentially acting as a form of \"soft\"\nregulation by investigating developers or regulators, as a substitute to\ninstitutional AI regulation (which is still absent in many regions). Both\ngame-theoretic analysis and LLM-based simulations reveal conditions under which\neffective regulation and trustworthy AI development emerge, emphasising the\nimportance of considering the influence of different regulatory regimes from an\nevolutionary game-theoretic perspective. The study concludes that effective\ngovernance requires managing incentives and costs for high quality\ncommentaries.",
      "generated_abstract": "The emergence of artificial intelligence (AI) has sparked significant\npromises and concerns. These concerns include the potential for AI to replace\nhuman decision-making, undermine human rights, and worsen inequality. While\nthese risks are real, there is limited consensus on how to address them. This\nstudy explores the role of game theory and large language models (LLMs) in\naddressing these concerns. Specifically, we apply a game-theoretic framework to\nanalyze the tradeoffs between AI-related policies. We demonstrate that AI-related\ngovernance policies can be balanced between minimizing harm and maximizing\nbenefits, provided they are informed by the appropriate use of LLMs. Our\nfindings highlight the potential of LLMs to enhance policy formation, and\ndemonstrate that the right combination of policies can balance harm minimization\nwith human rights protection.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2543859649122807,
          "p": 0.3118279569892473,
          "f": 0.2801932317664356
        },
        "rouge-2": {
          "r": 0.052980132450331126,
          "p": 0.06666666666666667,
          "f": 0.05904058547133114
        },
        "rouge-l": {
          "r": 0.24561403508771928,
          "p": 0.3010752688172043,
          "f": 0.2705313960176434
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06070v1",
      "true_abstract": "This paper bridges optimization and control, and presents a novel closed-loop\ncontrol framework based on natural gradient descent, offering a\ntrajectory-oriented alternative to traditional cost-function tuning. By\nleveraging the Fisher Information Matrix, we formulate a preconditioned\ngradient descent update that explicitly shapes system trajectories. We show\nthat, in sharp contrast to traditional controllers, our approach provides\nflexibility to shape the system's low-level behavior. To this end, the proposed\nmethod parameterizes closed-loop dynamics in terms of stationary covariance and\nan unknown cost function, providing a geometric interpretation of control\nadjustments. We establish theoretical stability conditions. The simulation\nresults on a rotary inverted pendulum benchmark highlight the advantages of\nnatural gradient descent in trajectory shaping.",
      "generated_abstract": "This paper introduces a natural gradient descent (NGD) method for control\nlimiting the gradient magnitude, enabling the algorithm to find the control\nthat minimizes the expected cost. The method is inspired by the NGD of\ndistributions, where the gradient is replaced by the empirical distribution. We\ndemonstrate that NGD can be extended to control, by applying it to the\nconvolutional neural network (CNN) used to generate the control. The method is\nvalidated through simulation and numerical experiments with a nonlinear\ndifferential-difference system.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21348314606741572,
          "p": 0.3333333333333333,
          "f": 0.26027396784293494
        },
        "rouge-2": {
          "r": 0.027522935779816515,
          "p": 0.039473684210526314,
          "f": 0.03243242759152739
        },
        "rouge-l": {
          "r": 0.20224719101123595,
          "p": 0.3157894736842105,
          "f": 0.24657533770594867
        }
      }
    },
    {
      "paper_id": "cs.CG.cs/CG/2503.08863v1",
      "true_abstract": "We study three fundamental three-dimensional (3D) geometric packing problems:\n3D (Geometric) Bin Packing (3D-BP), 3D Strip Packing (3D-SP), and Minimum\nVolume Bounding Box (3D-MVBB), where given a set of 3D (rectangular) cuboids,\nthe goal is to find an axis-aligned nonoverlapping packing of all cuboids. In\n3D-BP, we need to pack the given cuboids into the minimum number of unit cube\nbins. In 3D-SP, we need to pack them into a 3D cuboid with a unit square base\nand minimum height. Finally, in 3D-MVBB, the goal is to pack into a cuboid box\nof minimum volume.\n  It is NP-hard to even decide whether a set of rectangles can be packed into a\nunit square bin -- giving an (absolute) approximation hardness of 2 for 3D-BP\nand 3D-SP. The previous best (absolute) approximation for all three problems is\nby Li and Cheng (SICOMP, 1990), who gave algorithms with approximation ratios\nof 13, $46/7$, and $46/7+\\varepsilon$, respectively, for 3D-BP, 3D-SP, and\n3D-MVBB. We provide improved approximation ratios of 6, 6, and $3+\\varepsilon$,\nrespectively, for the three problems, for any constant $\\varepsilon > 0$.\n  For 3D-BP, in the asymptotic regime, Bansal, Correa, Kenyon, and Sviridenko\n(Math.~Oper.~Res., 2006) showed that there is no asymptotic polynomial-time\napproximation scheme (APTAS) even when all items have the same height. Caprara\n(Math.~Oper.~Res., 2008) gave an asymptotic approximation ratio of\n$T_{\\infty}^2 + \\varepsilon\\approx 2.86$, where $T_{\\infty}$ is the well-known\nHarmonic constant in Bin Packing. We provide an algorithm with an improved\nasymptotic approximation ratio of $3 T_{\\infty}/2 +\\varepsilon \\approx 2.54$.\nFurther, we show that unlike 3D-BP (and 3D-SP), 3D-MVBB admits an APTAS.",
      "generated_abstract": "-dimensional (3D) bin packing problem is a fundamental combinatorial\nproblem that is widely used in many areas such as manufacturing, supply\nchain management, and logistics. The 3D bin packing problem can be formulated\nas a mixed-integer linear programming (MILP) problem and has been studied for\nmany years. In this paper, we present improved approximation algorithms for the\n3D bin packing problem, which can significantly improve the approximation\nratios. Our algorithms are based on the idea of using a binary search tree to\nconstruct a dynamic partition of the input space. The proposed algorithms\nconsiderably outperform the state-of-the-art algorithms in terms of the\napproximation ratio. The proposed algorithms can also handle more general\ninputs and can be easily integrated into existing software packages. In the\nexperiments, our algorithms show superior performance compared to the state-of-the-art",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16774193548387098,
          "p": 0.28888888888888886,
          "f": 0.21224489331112048
        },
        "rouge-2": {
          "r": 0.020491803278688523,
          "p": 0.04065040650406504,
          "f": 0.02724795194678183
        },
        "rouge-l": {
          "r": 0.15483870967741936,
          "p": 0.26666666666666666,
          "f": 0.19591836269887558
        }
      }
    },
    {
      "paper_id": "physics.geo-ph.physics/geo-ph/2503.06490v1",
      "true_abstract": "Seismic data acquisition is often affected by various types of noise, which\ndegrade data quality and hinder subsequent interpretation. Recovery of seismic\ndata becomes particularly challenging in the presence of strong noise, which\nsignificantly impacts both data accuracy and geological analysis. This study\nproposes a novel single-encoder, multiple-decoder network based on Nash\nequalization (SEMD-Nash) for effective strong noise attenuation in seismic\ndata. The main contributions of this method are as follows: First, we design a\nshared encoder-multi-decoder architecture, where an improved encoder extracts\nkey features from the noisy data, and three parallel decoders reconstruct the\ndenoised seismic signal from different perspectives. Second, we develop a\nmulti-objective optimization system that integrates three loss functions-Mean\nSquared Error (MSE), Perceived Loss, and Structural Similarity Index (SSIM)-to\nensure effective signal reconstruction, high-order feature preservation, and\nstructural integrity. Third, we introduce the Nash Equalization Weight\nOptimizer, which dynamically adjusts the weights of the loss functions,\nbalancing the optimization objectives to improve the models robustness and\ngeneralization. Experimental results demonstrate that the proposed method\neffectively suppresses strong noise while preserving the geological\ncharacteristics of the seismic data.",
      "generated_abstract": "r presents a new method for modeling and characterizing strong\nseismic noise. The method is based on the Nash equilibrium principle, which\nallows one to solve the so-called Nash problem in a manner that is robust to\nimperfections. The Nash equilibrium principle was introduced in 1939 by the\nmathematical economist John Nash and has been extensively applied to various\nfields, including the field of control theory and robotics. The principle has\nbeen applied in the field of robotics to develop the Nash equilibrium control\n(NEC) method, which is a method for controlling robot movements by solving\nequilibrium problems in a way that is robust to imperfections. This paper\nintroduces the NEC model for solving the problem of modeling and characterizing\nseismic noise. The NEC model provides a framework for modeling and characterizing\nseismic noise in the context of the field",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.3188405797101449,
          "f": 0.21890546812801673
        },
        "rouge-2": {
          "r": 0.022857142857142857,
          "p": 0.034782608695652174,
          "f": 0.027586202110583467
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.3188405797101449,
          "f": 0.21890546812801673
        }
      }
    },
    {
      "paper_id": "cs.NI.math/NA/2503.09869v1",
      "true_abstract": "A well-known expression for the saturation throughput of heterogeneous\ntransmitting nodes in a wireless network using p-CSMA, derived from Renewal\nTheory, implicitly assumes that all transmitting nodes are in range of, and\ntherefore conflicting with, each other. This expression, as well as simple\nmodifications of it, does not correctly capture the saturation throughput\nvalues when an arbitrary topology is specified for the conflict graph between\ntransmitting links. For example, we show numerically that calculations based on\nrenewal theory can underestimate throughput by 48-62% for large packet sizes\nwhen the conflict graph is represented by a star topology. This is problematic\nbecause real-world wireless networks, such as wireless IoT mesh networks, are\noften deployed over a large area, resulting in non-complete conflict graphs.\n  To address this gap, we present a computational approach based on a novel\nMarkov chain formulation that yields the exact saturation throughput for each\nnode in the general network case for any given set of access probabilities, as\nwell as a more compact expression for the special case where the packet length\nis twice the slot length. Using our approach, we show how the transmit\nprobabilities could be optimized to maximize weighted utility functions of the\nsaturation throughput values. This would allow a wireless system designer to\nset transmit probabilities to achieve desired throughput trade-offs in any\ngiven deployment.",
      "generated_abstract": "This paper studies the saturation throughput of a heterogeneous p-CSMA network,\nwhere nodes can be classified into three types: primary, secondary, and tertiary.\nIn this paper, we focus on the case where the primary nodes can transmit in\nboth the primary and secondary channels, while the secondary and tertiary\nnodes can transmit in only the primary channel. We formulate the problem as a\nmarkov chain with two states, primary and secondary, and two rates per state.\nWe characterize the saturation throughput of the network and derive the\nsolution of the problem using the method of moments. We prove that the\nsolution has a closed-form expression, which enables us to evaluate the\nsaturation throughput numerically. We also provide a numerical simulation\nstudy to illustrate the effectiveness of the solution derived in this paper.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19424460431654678,
          "p": 0.35064935064935066,
          "f": 0.24999999541195136
        },
        "rouge-2": {
          "r": 0.03482587064676617,
          "p": 0.0625,
          "f": 0.04472842990905333
        },
        "rouge-l": {
          "r": 0.15827338129496402,
          "p": 0.2857142857142857,
          "f": 0.20370369911565508
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.12141v2",
      "true_abstract": "The FAO-GAEZ crop productivity data are widely used in Economics. However,\nthe existence of measurement error is rarely recognized in the empirical\nliterature. We propose a novel method to partially identify the effect of\nagricultural productivity, deriving bounds that allow for nonclassical\nmeasurement error by leveraging two proxies. These bounds exhaust all the\ninformation contained in the first two moments of the data. We reevaluate three\ninfluential studies, documenting that measurement error matters and that the\nimpact of agricultural productivity on economic outcomes may be smaller than\npreviously reported. Our methodology has broad applications in empirical\nresearch involving mismeasured variables.",
      "generated_abstract": "We investigate the measurement error associated with the FAO-GAEZ data\ndistribution. We show that the FAO-GAEZ data distribution is nonclassical and\nthat it is possible to construct a new FAO-GAEZ data distribution that\navoids the measurement error. We illustrate the results of our study by\nintroducing a new FAO-GAEZ data distribution that is based on the FAO-GAEZ\nproductivity measures and the FAO-GAEZ total factor productivity. This new FAO-\nGAEZ data distribution is useful for analyzing the production and productivity\nof the FAO-GAEZ countries.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22666666666666666,
          "p": 0.3953488372093023,
          "f": 0.28813558858804944
        },
        "rouge-2": {
          "r": 0.042105263157894736,
          "p": 0.06153846153846154,
          "f": 0.04999999517578172
        },
        "rouge-l": {
          "r": 0.17333333333333334,
          "p": 0.3023255813953488,
          "f": 0.22033897841855798
        }
      }
    },
    {
      "paper_id": "cs.CL.cs/CL/2503.10515v1",
      "true_abstract": "Discourse understanding is essential for many NLP tasks, yet most existing\nwork remains constrained by framework-dependent discourse representations. This\nwork investigates whether large language models (LLMs) capture discourse\nknowledge that generalizes across languages and frameworks. We address this\nquestion along two dimensions: (1) developing a unified discourse relation\nlabel set to facilitate cross-lingual and cross-framework discourse analysis,\nand (2) probing LLMs to assess whether they encode generalizable discourse\nabstractions. Using multilingual discourse relation classification as a\ntestbed, we examine a comprehensive set of 23 LLMs of varying sizes and\nmultilingual capabilities. Our results show that LLMs, especially those with\nmultilingual training corpora, can generalize discourse information across\nlanguages and frameworks. Further layer-wise analyses reveal that language\ngeneralization at the discourse level is most salient in the intermediate\nlayers. Lastly, our error analysis provides an account of challenging relation\nclasses.",
      "generated_abstract": "-level generalization refers to the ability of language models (LLMs) to\nproperly handle multilingual text when presented with unseen languages.\nPrevious studies have focused on unified language models (ULMs) that are\ncapable of handling text from multiple languages, but have not explored the\nability of LLMs to generalize to new languages within their own language\nfamilies. To address this gap, we propose a multilingual discourse-level\ngeneralization framework that extends the unified language model paradigm to\nthe discourse-level. We develop a label set for our framework, which consists\nof a set of predefined labels and a set of user-defined labels. We then\nevaluate the framework through a series of experiments on the WMT2018\nmultilingual-English-to-German dataset. Our experiments demonstrate that our\nframework can generalize discourse-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24528301886792453,
          "p": 0.325,
          "f": 0.27956988757081747
        },
        "rouge-2": {
          "r": 0.05223880597014925,
          "p": 0.06086956521739131,
          "f": 0.056224894627506454
        },
        "rouge-l": {
          "r": 0.2358490566037736,
          "p": 0.3125,
          "f": 0.26881719939877446
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.03131v1",
      "true_abstract": "There is growing recognition in both the experimental and modelling\nliterature of the importance of spatial structure to the dynamics of viral\ninfections in tissues. Aided by the evolution of computing power and motivated\nby recent biological insights, there has been an explosion of new,\nspatially-explicit models for within-host viral dynamics in recent years. This\ndevelopment has only been accelerated in the wake of the COVID-19 pandemic.\nSpatially-structured models offer improved biological realism and can account\nfor dynamics which cannot be well-described by conventional, mean-field\napproaches. However, despite their growing popularity, spatially-structured\nmodels of viral dynamics are underused in biological applications. One major\nobstacle to the wider application of such models is the huge variety in\napproaches taken, with little consensus as to which features should be included\nand how they should be implemented for a given biological context. Previous\nreviews of the field have focused on specific modelling frameworks or on models\nfor particular viral species. Here, we instead apply a scoping review approach\nto the literature of spatially-structured viral dynamics models as a whole to\nprovide an exhaustive update of the state of the field. Our analysis is\nstructured along two axes, methodology and viral species, in order to examine\nthe breadth of techniques used and the requirements of different biological\napplications. We then discuss the contributions of mathematical and\ncomputational modelling to our understanding of key spatially-structured\naspects of viral dynamics, and suggest key themes for future model development\nto improve robustness and biological utility.",
      "generated_abstract": "ections can be considered as spatially-structured systems,\nwith both spatial and temporal aspects being important for disease dynamics.\nThe spatial structure of viral populations can be either local or global,\ndepending on the spatial organization of the virus. For local spatial\nstructures, a viral population can be modeled by a single host, while for\nglobal viral populations, a population is modeled by the entire host population.\nIn this paper, we review the literature on spatially-structured models of\nviral dynamics, focusing on the mathematical formulation of such models and\ntheir mathematical properties. We review the mathematical formulation of\nspatially-structured models of viral dynamics, focusing on the mathematical\nformulation of such models and the their mathematical properties. We also\nconsider the role of the viral population size, host population size, and\nepidemiological parameters in such models. We",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2054794520547945,
          "p": 0.4411764705882353,
          "f": 0.28037382743995115
        },
        "rouge-2": {
          "r": 0.05172413793103448,
          "p": 0.11538461538461539,
          "f": 0.07142856715419528
        },
        "rouge-l": {
          "r": 0.17123287671232876,
          "p": 0.36764705882352944,
          "f": 0.23364485547733432
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06411v1",
      "true_abstract": "This paper examines the intricate interplay among AI safety, security, and\ngovernance by integrating technical systems engineering with principles of\nmoral imagination and ethical philosophy. Drawing on foundational insights from\nWeapons of Math Destruction and Thinking in Systems alongside contemporary\ndebates in AI ethics, we develop a comprehensive multi-dimensional framework\ndesigned to regulate AI technologies deployed in high-stakes domains such as\ndefense, finance, healthcare, and education. Our approach combines rigorous\ntechnical analysis, quantitative risk assessment, and normative evaluation to\nexpose systemic vulnerabilities inherent in opaque, black-box models. Detailed\ncase studies, including analyses of Microsoft Tay (2016) and the UK A-Level\nGrading Algorithm (2020), demonstrate how security lapses, bias amplification,\nand lack of accountability can precipitate cascading failures that undermine\npublic trust. We conclude by outlining targeted strategies for enhancing AI\nresilience through adaptive regulatory mechanisms, robust security protocols,\nand interdisciplinary oversight, thereby advancing the state of the art in\nethical and technical AI governance.",
      "generated_abstract": "r introduces the concept of moral imagination to address the\ntechnical AI governance challenge of developing AI systems that can successfully\nintegrate the ethical and moral dimensions of human decision-making. We argue\nthat the integration of moral imagination into technical AI governance is a\npivotal step in mitigating the risks of AI-driven technological unemployment,\nenvironmental degradation, and social injustice. To that end, we present a\nframework that integrates moral imagination with technical AI governance.\nSpecifically, we integrate moral imagination into the AI governance cycle by\nfirst defining a set of core values and principles for AI governance, followed\nby the development of a decision-making framework for the AI governance\ncycle. The framework guides the development of an AI governance cycle that\nincorporates moral imagination as a key component. We validate the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18548387096774194,
          "p": 0.32857142857142857,
          "f": 0.23711339744925078
        },
        "rouge-2": {
          "r": 0.032679738562091505,
          "p": 0.044642857142857144,
          "f": 0.037735844176291125
        },
        "rouge-l": {
          "r": 0.1532258064516129,
          "p": 0.2714285714285714,
          "f": 0.19587628404718896
        }
      }
    },
    {
      "paper_id": "cs.IT.math/IT/2503.08451v1",
      "true_abstract": "Early neural channel coding approaches leveraged dense neural networks with\none-hot encodings to design adaptive encoder-decoder pairs, improving block\nerror rate (BLER) and automating the design process. However, these methods\nstruggled with scalability as the size of message sets and block lengths\nincreased. TurboAE addressed this challenge by focusing on bit-sequence inputs\nrather than symbol-level representations, transforming the scalability issue\nassociated with large message sets into a sequence modeling problem. While\nrecurrent neural networks (RNNs) were a natural fit for sequence processing,\ntheir reliance on sequential computations made them computationally expensive\nand inefficient for long sequences. As a result, TurboAE adopted convolutional\nnetwork blocks, which were faster to train and more scalable, but lacked the\nsequential modeling advantages of RNNs. Recent advances in efficient RNN\narchitectures, such as minGRU and minLSTM, and structured state space models\n(SSMs) like S4 and S6, overcome these limitations by significantly reducing\nmemory and computational overhead. These models enable scalable sequence\nprocessing, making RNNs competitive for long-sequence tasks. In this work, we\nrevisit RNNs for Turbo autoencoders by integrating the lightweight minGRU model\nwith a Mamba block from SSMs into a parallel Turbo autoencoder framework. Our\nresults demonstrate that this hybrid design matches the performance of\nconvolutional network-based Turbo autoencoder approaches for short sequences\nwhile significantly improving scalability and training efficiency for long\nblock lengths. This highlights the potential of efficient RNNs in advancing\nneural channel coding for long-sequence scenarios.",
      "generated_abstract": "oencoders (TAEs) have recently emerged as a powerful tool for\nreducing the computational complexity of signal processing and image\ncompression. However, conventional TAE frameworks typically require\ncomputationally-expensive matrix-vector multiplications, which are often\ninefficient in high-dimensional settings. To address this, we propose\nMinGRU-Based Encoder (MiBE), a novel encoder that integrates a MinGRU-based\nencoder with a residual encoder to significantly reduce the computational\nrequirements of TAE. Specifically, MiBE introduces a MinGRU-based encoder to\nmodel complex time-frequency (T-F) dependencies, while a residual encoder is\nemployed to reduce the number of model parameters and further enhance the\nperformance of TAEs. To evaluate the performance of MiBE, we implement two\nstate-of-the-art",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1282051282051282,
          "p": 0.2597402597402597,
          "f": 0.17167381531728354
        },
        "rouge-2": {
          "r": 0.013392857142857142,
          "p": 0.03125,
          "f": 0.01874999580000094
        },
        "rouge-l": {
          "r": 0.11538461538461539,
          "p": 0.23376623376623376,
          "f": 0.15450643334303465
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2501.16522v1",
      "true_abstract": "With limited resources, competition is widespread, yet cooperation persists\nacross taxa, from microorganisms to large mammals. Recent observations reveal\ncontingent factors often drive cooperative interactions, with the intensity\nheterogeneously distributed within species. While cooperation has beneficial\noutcomes, it may also incur significant costs, largely depending on species\ndensity. This creates a dilemma that is pivotal in shaping sustainable\ncooperation strategies. Understanding how cooperation intensity governs the\ncost-benefit balance, and whether an optimal strategy exists for species\nsurvival, is a fundamental question in ecological research, and the focus of\nthis study. We develop a novel mathematical model within the Lotka-Volterra\nframework to explore the dynamics of cost-associated partial cooperation, which\nremains relatively unexplored in ODE model-based studies. Our findings\ndemonstrate that partial cooperation benefits ecosystems up to a certain\nintensity, beyond which costs become dominant, leading to system collapse via\nheteroclinic bifurcation. This outcome captures the cost-cooperation dilemma,\nproviding insights for adopting sustainable strategies and resource management\nfor species survival. We propose a novel mathematical approach to detect and\ntrack heteroclinic orbits in predator-prey systems. Moreover, we show that\nintroducing fear of predation can protect the regime shift, even with a type-I\nfunctional response, challenging traditional ecological views. Although fear is\nknown to resolve the \"paradox of enrichment,\" our results suggest that certain\nlevels of partial cooperation can reestablish this dynamic even at higher fear\nintensity. Finally, we validate the system's dynamical robustness across\nfunctional responses through structural sensitivity analysis.",
      "generated_abstract": "on is a key mechanism in evolutionary biology, where individuals\ncooperate to benefit each other. However, cooperation can lead to a\ndisadvantageous trade-off in the fitness of the individuals, which is\nknown as the cost-associated cooperation. In this work, we investigate the\ncost-associated cooperation (CAC) by modeling it as a dilemma of species\ninteraction. CAC is a dilemma of species interaction because it is a\ncompeting cooperation versus competition situation, and the evolutionary\nefficiency of cooperation is a key question. In our model, the species interact\nwith a common resource, and there are two species, each of which has its own\nreproductive strategy. The species interact with the resource and compete for\nreproduction. In the CAC, the species with cooperative reproductive strategy\ncompetes with the species",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15789473684210525,
          "p": 0.3698630136986301,
          "f": 0.22131147121640693
        },
        "rouge-2": {
          "r": 0.03418803418803419,
          "p": 0.07339449541284404,
          "f": 0.04664722598475166
        },
        "rouge-l": {
          "r": 0.12280701754385964,
          "p": 0.2876712328767123,
          "f": 0.17213114334755453
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.08991v1",
      "true_abstract": "We prove that cw-hyperbolic homeomorphisms with jointly continuous\nstable/unstable holonomies satisfy the periodic shadowing property and, if they\nare topologically mixing, the periodic specification property. We discuss\ndifficulties to adapt Bowen's techniques to obtain a measure of maximal entropy\nfor cw-hyperbolic homeomorphisms, exhibit the unique measure of maximal entropy\nfor Walter's pseudo-Anosov diffeomorphism of $\\mathbb{S}^2$, and prove it can\nbe obtained, as in the expansive case, as the weak* limit of an average of\nDirac measures on periodic orbits. As an application, we exhibit the unique\nmeasure of maximal entropy for the homeomorphism on the Sierpi\\'nski Carpet\ndefined in [12], which does not satisfy the specification property.",
      "generated_abstract": "This paper explores the connection between continuum-wise hyperbolicity and\nperiodic points in the dynamics of a single continuum. We show that a\none-dimensional flow with no periodic points is hyperbolic if and only if it\nis periodic. In particular, the dynamics of a periodic point on a one-dimensional\nflow are determined by its orbit in the unit interval. We also show that a\none-dimensional flow with a single fixed point is hyperbolic if and only if it\nhas a periodic point. These results have important applications in the\nunderstanding of chaotic dynamics. In particular, we obtain a characterization\nof the dynamics of a periodic point on a one-dimensional flow which is\nhyperbolic and does not have a periodic point.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2571428571428571,
          "p": 0.32142857142857145,
          "f": 0.2857142807760142
        },
        "rouge-2": {
          "r": 0.03260869565217391,
          "p": 0.03614457831325301,
          "f": 0.0342857092989395
        },
        "rouge-l": {
          "r": 0.2571428571428571,
          "p": 0.32142857142857145,
          "f": 0.2857142807760142
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2407.21190v2",
      "true_abstract": "Predictive values are measures of the clinical accuracy of a binary\ndiagnostic test, and depend on the sensitivity and the specificity of the test\nand on the disease prevalence among the population being studied. This article\nstudies hypothesis tests to simultaneously compare the predictive values of two\nbinary diagnostic tests in the presence of missing data. The hypothesis tests\nwere solved applying two computational methods: the EM and SEM algorithms and\nmultiple imputation. Simulation experiments were carried out to study the sizes\nand the power of the hypothesis tests, giving some general rules of\napplication. Two R programmes were written to apply each method, and they are\navailable as supplementary material for the manuscript. The results were\napplied to the diagnosis of Alzheimer's disease.",
      "generated_abstract": "ata is a common problem in clinical diagnostic studies.\nMany existing methods to handle missing data rely on the assumption of\nmissing-at-random (MAR) missingness. However, this assumption may be violated\nin clinical diagnostic studies, particularly when the likelihood of missing\ndata is high. As a result, the existing methods may not be applicable.\nAdditionally, existing methods are computationally expensive, particularly when\nthe number of missing data points is large. To address these limitations, we\npropose a novel method to handle missing data in diagnostic studies that\nincorporates both the MAP (maximum a posteriori) and EM (expectation-maximization)\nalgorithms. Our proposed method, called EM-SEM (EM-Stratified Extreme\nMethod), extends the EM algorithm with a new strategy that uses the\nextreme values of the posterior predictive distribution. We compare the\neffect",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2375,
          "p": 0.2235294117647059,
          "f": 0.23030302530762176
        },
        "rouge-2": {
          "r": 0.05982905982905983,
          "p": 0.061946902654867256,
          "f": 0.06086956021890401
        },
        "rouge-l": {
          "r": 0.2125,
          "p": 0.2,
          "f": 0.20606060106519758
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.00564v1",
      "true_abstract": "For a many-to-one market with substitutable preferences on the firm's side,\nbased on the Aizerman-Malishevski decomposition, we define an associated\none-to-one market. Given that the usual notion of stability for a one-to-one\nmarket does not fit well for this associated one-to-one market, we introduce a\nnew notion of stability. This notion allows us to establish an isomorphism\nbetween the set of stable matchings in the many-to-one market and the matchings\nin the associated one-to-one market that meet this new stability criterion.\nFurthermore, we present an adaptation of the well-known deferred acceptance\nalgorithm to compute a matching that satisfies this new notion of stability for\nthe associated one-to-one market.",
      "generated_abstract": "er a market with $n$ goods and $m$ buyers where each buyer has a\nchoice of $k$ goods and each seller has a choice of $l$ buyers. The buyer-good\nand seller-buyer pairings are chosen by a matching mechanism. We derive a\ndecomposition between the $n\\times k$ buyer-good matrix and the $m\\times l$\nseller-buyer matrix such that each row of the buyer-good matrix is a\nsubstitutable many-to-one match, while each row of the seller-buyer matrix is\na one-to-one match. In particular, the buyer-good matrix is a one-to-one\ndecomposition of the buyer-good matrix. We show that this decomposition\nconverges in probability as the number of buyers, $m$, tends to infinity. We\nshow that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25862068965517243,
          "p": 0.2459016393442623,
          "f": 0.25210083533931227
        },
        "rouge-2": {
          "r": 0.06741573033707865,
          "p": 0.06741573033707865,
          "f": 0.06741572533707901
        },
        "rouge-l": {
          "r": 0.20689655172413793,
          "p": 0.19672131147540983,
          "f": 0.20168066727208542
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.00254v1",
      "true_abstract": "Growth curve analysis (GCA) has a wide range of applications in various\nfields where growth trajectories need to be modeled. Heteroscedasticity is\noften present in the error term, which can not be handled with sufficient\nflexibility by standard linear fixed or mixed-effects models. One situation\nthat has been addressed is where the error variance is characterized by a\nlinear predictor with certain covariates. A frequently encountered scenario in\nGCA, however, is one in which the variance is a smooth function of the mean\nwith known shape restrictions. A naive application of standard linear\nmixed-effects models would underestimate the variance of the fixed effects\nestimators and, consequently, the uncertainty of the estimated growth curve. We\npropose to model the variance of the response variable as a shape-restricted\n(increasing/decreasing; convex/concave) function of the marginal or conditional\nmean using shape-restricted splines. A simple iteratively reweighted fitting\nalgorithm that takes advantage of existing software for linear mixed-effects\nmodels is developed. For inference, a parametric bootstrap procedure is\nrecommended. Our simulation study shows that the proposed method gives\nsatisfactory inference with moderate sample sizes. The utility of the method is\ndemonstrated using two real-world applications.",
      "generated_abstract": "This paper introduces a new model for the estimation of shape-restricted\ncurve-based growth curves. We propose a novel approach to modeling the\ndirectional variation of the growth curve. Our approach is based on a\nGaussian process framework, which incorporates shape-restricted splines for\nmodeling the shape function. We illustrate the proposed model through an\napplication to the estimation of the daily price change of the S&P 500 stock\nindex.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15702479338842976,
          "p": 0.40425531914893614,
          "f": 0.22619047216057264
        },
        "rouge-2": {
          "r": 0.028735632183908046,
          "p": 0.08064516129032258,
          "f": 0.04237287748204574
        },
        "rouge-l": {
          "r": 0.1487603305785124,
          "p": 0.3829787234042553,
          "f": 0.2142857102558107
        }
      }
    },
    {
      "paper_id": "math.GT.math/GT/2503.05151v1",
      "true_abstract": "In a previous note, it is claimed that every surface-link consisting of\ntrivial components and having at most one non-sphere component is a ribbon\nsurface-link, but it was false. In this revised note, this claim is replaced by\nthe claim that a surface-link $L$ with trivial components is a ribbon\nsurface-link if and only if the surface-link obtained from $L$ by every fusion\nis a ribbon surface-link if and only if the surface-link obtained from $L$ by\nany one fusion is a ribbon surface-link. For any closed oriented disconnected\nsurface ${\\mathbf F}$ containing at least two non-sphere components, there is a\npair of a ribbon ${\\mathbf F}$-link $L$ consisting of trivial components and a\nnon-ribbon ${\\mathbf F}$-link $L'$ consisting of trivial components such that\nthe fundamental groups of $L$ and $L'$ are the same group up to\nmeridian-preserving isomorphisms and the pair of the ${\\mathbf F}'$-links $K$\nand $K'$ obtained from $L$ and $L'$ by every corresponding fusion is a pair of\na ribbon surface-link and a non-ribbon surface-link such that the fundamental\ngroups of $K$ and $K'$ are the same group up to meridian-preserving\nisomorphisms.",
      "generated_abstract": "We revisit the notion of surface-link of a surface $S$ in the presence of a\nlittle group $G$ acting on $S$ and a trivial component $C$ of $S \\setminus\nC(G)$. We show that the group $H_1(S \\setminus C(G)) / H_1(C(G))$ is\nisomorphic to the fundamental group of $S$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18571428571428572,
          "p": 0.3939393939393939,
          "f": 0.2524271801112264
        },
        "rouge-2": {
          "r": 0.031746031746031744,
          "p": 0.08695652173913043,
          "f": 0.046511623988642836
        },
        "rouge-l": {
          "r": 0.12857142857142856,
          "p": 0.2727272727272727,
          "f": 0.17475727719860504
        }
      }
    },
    {
      "paper_id": "hep-ph.nucl-ex/2503.07055v1",
      "true_abstract": "The valence quark parton distribution functions (PDFs) of all ground state\nheavy mesons that composed of $b$ or $c$ quarks, are discussed; namely, the\npseudoscalar $\\eta_c(1S)$, $\\eta_b(1S)$ and $B_c$, together with the\ncorresponding vector ones, $J/\\psi$, $\\Upsilon(1S)$ and $B_c^\\ast$. We use a\nQCD-inspired constituent quark model, which has been applied with success to\nconventional heavy mesons, so that one advantage here is that all parameters\nhave already been fixed by previous studies. The wave functions of the heavy\nmesons in the rest frame are obtained by solving the Schr\\\"odinger equation,\nthen boosted to its light-front based on Susskind's Lorentz transformation. The\nPDFs at the hadron scale, are then obtained by integrating out the transverse\nmomenta of the modulus square of the light-front wave function. Our study shows\nhow the valence quark distributions differ between pseudoscalar and vector\nmesons, as well as among charmonia, bottomonia and bottom-charmed mesons.\nComparisons with other theoretical calculations demonstrate that the PDFs\nobtained herein are in general narrower but align well with the expected\npatterns. Moreover, each PDF's point-wise behavior is squeezed with respect to\nthe scale-free parton-like PDF.",
      "generated_abstract": "the parton distribution functions (PDFs) of ground state mesons\ncompounded of $c$- or $b$-quarks. We extend our previous analysis of the\n$J/\\psi$ and $\\psi(2S)$ to include the $J/\\psi(3770)$, $J/\\psi(4040)$,\n$\\psi(4160)$, and $\\psi(4415)$ mesons. We perform the fits to the inclusive\n$pp\\to\\psi(2S)\\ell^+\\ell^-$ and $pp\\to\\psi(3770)\\ell^+\\ell^-$ production data,\nand use the results to constrain the PDFs of the $c$- and $b$-quark\ncontributions. We find that the $c$-quark PDFs are significantly broader than\nthose of the $b$-quark, while the $b$-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.144,
          "p": 0.35294117647058826,
          "f": 0.20454545042936476
        },
        "rouge-2": {
          "r": 0.05113636363636364,
          "p": 0.1232876712328767,
          "f": 0.07228915248205697
        },
        "rouge-l": {
          "r": 0.144,
          "p": 0.35294117647058826,
          "f": 0.20454545042936476
        }
      }
    },
    {
      "paper_id": "physics.med-ph.q-bio/TO/2502.20406v1",
      "true_abstract": "In computational modelling of coronary haemodynamics, imposing\npatient-specific flow conditions is paramount, yet often impractical due to\nresource and time constraints, limiting the ability to perform a large number\nof simulations particularly for diseased cases. We aimed to compare coronary\nhaemodynamics quantified using a simplified flow-split strategy with varying\nexponents against the clinically verified but computationally intensive\nmultiscale simulations under both resting and hyperaemic conditions in arteries\nwith varying degrees of stenosis.\n  Six patient-specific left coronary artery trees were segmented and\nreconstructed, including three with severe (>70%) and three with mild (<50%)\nfocal stenoses. Simulations were performed for the entire coronary tree to\naccount for the flow-limiting effects from epicardial artery stenoses. Both a\n0D-3D coupled multiscale model and a flow-split approach with four different\nexponents (2.0, 2.27, 2.33, and 3.0) were used. The resulting prominent\nhaemodynamic metrics were statistically compared between the two methods.\n  Flow-split and multiscale simulations did not significantly differ under\nresting conditions regardless of the stenosis severity. However, under\nhyperaemic conditions, the flow-split method significantly overestimated the\ntime-averaged wall shear stress by up to 16.8 Pa (p=0.031) and underestimate\nthe fractional flow reserve by 0.327 (p=0.043), with larger discrepancies\nobserved in severe stenoses than in mild ones. Varying the exponent from 2.0 to\n3.0 within the flow-split methods did not significantly affect the haemodynamic\nresults (p>0.141).\n  Flow-split strategies with exponents between 2.0 and 3.0 are appropriate for\nmodelling stenosed coronaries under resting conditions. Multiscale simulations\nare recommended for accurate modelling of hyperaemic conditions, especially in\nseverely stenosed arteries.",
      "generated_abstract": "split outflow strategy has been widely used to characterise flow\ncoronary arteries. However, it has not been thoroughly validated. This study\naims to evaluate the reliability of the flow-split outflow strategy in\ncharacterising coronary artery flow, using a large dataset. The dataset\ncomprises 1232 coronary artery images, obtained from 42 patients undergoing\ncardiac catheterisation. The flow-split outflow strategy was used to analyse\nthe mean flow velocity and flow gradient along the left coronary artery. A\nlarge number of outliers were identified, with the highest values being found\nin the left anterior descending artery. The flow-split outflow strategy was\nfound to be unreliable for characterising flow in the left coronary artery,\nwith a high proportion of outliers. These findings highlight the need for\nreli",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1509433962264151,
          "p": 0.32,
          "f": 0.20512820077251814
        },
        "rouge-2": {
          "r": 0.032,
          "p": 0.07476635514018691,
          "f": 0.044817922973111206
        },
        "rouge-l": {
          "r": 0.14465408805031446,
          "p": 0.30666666666666664,
          "f": 0.19658119222550965
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2410.21295v1",
      "true_abstract": "Gene regulatory networks (GRNs) play a crucial role in the control of\ncellular functions. Numerous methods have been developed to infer GRNs from\ngene expression data, including mechanism-based approaches, information-based\napproaches, and more recent deep learning techniques, the last of which often\noverlooks the underlying gene expression mechanisms. In this work, we introduce\nTRENDY, a novel GRN inference method that integrates transformer models to\nenhance the mechanism-based WENDY approach. Through testing on both simulated\nand experimental datasets, TRENDY demonstrates superior performance compared to\nexisting methods. Furthermore, we apply this transformer-based approach to\nthree additional inference methods, showcasing its broad potential to enhance\nGRN inference.",
      "generated_abstract": "latory networks (GRNs) are a fundamental class of biological\nsystems, describing the relationship between genes and their interactions.\nHowever, existing methods for GRN inference face several challenges: (1) they\noften lack interpretability, making it difficult to understand the underlying\nmechanisms; (2) they suffer from high computational costs, limiting their\napplication in large-scale datasets; (3) they are limited to specific types of\nGRNs, making it difficult to generalize their results. To address these\nchallenges, we propose TRENDY, a novel framework that leverages Transformers to\nenhance GRN inference. TRENDY integrates three key modules: (1) a\nmulti-headed Transformer encoder that captures the complex interactions among\ngenes, (2) a bi-directional Transformer decoder that extends the inference\nprocess, and (3)",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2804878048780488,
          "p": 0.26744186046511625,
          "f": 0.2738095188123583
        },
        "rouge-2": {
          "r": 0.08,
          "p": 0.07272727272727272,
          "f": 0.0761904712018144
        },
        "rouge-l": {
          "r": 0.24390243902439024,
          "p": 0.23255813953488372,
          "f": 0.23809523309807268
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.15173v1",
      "true_abstract": "Stable and efficient food markets are crucial for global food security, yet\ninternational staple food markets are increasingly exposed to complex risks,\nincluding intensified risk contagion and escalating external uncertainties.\nThis paper systematically investigates risk spillovers in global staple food\nmarkets and explores the key determinants of these spillover effects, combining\ninnovative decomposition-reconstruction techniques, risk connectedness\nanalysis, and random forest models. The findings reveal that short-term\ncomponents exhibit the highest volatility, with futures components generally\nmore volatile than spot components. Further analysis identifies two main risk\ntransmission patterns, namely cross-grain and cross-timescale transmission, and\nclarifies the distinct roles of each component in various net risk spillover\nnetworks. Additionally, price drivers, external uncertainties, and core\nsupply-demand indicators significantly influence these spillover effects, with\nheterogeneous importance of varying factors in explaining different risk\nspillovers. This study provides valuable insights into the risk dynamics of\nstaple food markets, offers evidence-based guidance for policymakers and market\nparticipants to enhance risk warning and mitigation efforts, and supports the\nstabilization of international food markets and the safeguarding of global food\nsecurity.",
      "generated_abstract": "We study the multiscale spillover dynamics of the futures and spot markets\nfor staple foods. We combine multiscale spillover analysis with\ndynamic-factor models, allowing for cross-sectional and multiscale spillovers\nbetween futures and spot markets. Our results suggest that multiscale spillovers\nare significantly greater in the spot market than in the futures market.\nMultiscale spillovers are also associated with higher volatility, greater\ncorrelation between spot and futures prices, and greater variability in spot\nspot market prices. Spot market prices exhibit greater multiscale spillover\ndynamics than futures market prices. Our findings provide new insights into the\ninterplay between spot and futures markets, highlighting the importance of\nmultiscale spillover dynamics in driving volatility and price variation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23529411764705882,
          "p": 0.49122807017543857,
          "f": 0.3181818138022986
        },
        "rouge-2": {
          "r": 0.024539877300613498,
          "p": 0.043010752688172046,
          "f": 0.03124999537384102
        },
        "rouge-l": {
          "r": 0.21008403361344538,
          "p": 0.43859649122807015,
          "f": 0.28409090471138954
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.cond-mat/other/2503.06672v1",
      "true_abstract": "Ferroelastic materials (materials with switchable spontaneous strain) often\nare centrosymmetric, but their domain walls are always polar, as their internal\nstrain gradients cause polarization via flexoelectricity. This polarization is\ngenerally not switchable by an external electric field, because reversing the\ndomain wall polarity would require reversing the strain gradient, which in turn\nwould require switching the spontaneous strain of the adjacent domains,\ndestroying the domain wall in the process. However, domain wall polarization\ncan also arise from biquadratic coupling between polar and non-polar order\nparameters (e.g. octahedral tilts in perovskites). Such coupling is independent\nof the sign of the polarization and thus allows switching between +P and -P. In\nthis work, we seek to answer the question of whether the polarization of domain\nwalls in ferroelastic perovskites is switchable, as per the symmetric\nbiquadratic term, or non-switchable due to the unipolar flexoelectric bias.\nUsing perovskite calcium titanate (CaTiO3) as a paradigm, molecular dynamics\ncalculations indicate that high electric fields broaden the ferroelastic domain\nwalls, thereby reducing flexoelectricity (as the domain wall strain gradient is\ninversely proportional to the wall width), eventually enabling switching. The\npolarization switching, however, is not ferroelectric-like with a simple\nhysteresis loop, but antiferroelectric-like with a double hysteresis loop.\nFerroelastic domain walls thus behave as functional antiferroelectric elements,\nand also as nucleation points for a bulk phase transition to a polar state.",
      "generated_abstract": "tric domain walls (FDWs) can be switched from a polar to a\nelectrically insulating phase by mechanical perturbations. The FDWs have been\nstudied as switching devices for ferroelectric polarizer (FEP) and ferroelectric\nlinearisers (FLIs) in various systems. The switching of FDWs, however, has\nnot been studied so far within the framework of the antiferroelectric phase,\nwhich is also known as the polarization switching phase. Here, we present a\ntheoretical study of the switching of FDWs within the antiferroelectric phase\nusing a first-principles approach. The antiferroelectric domain wall is\nmodeled as a periodic array of polarization atoms that are shifted along the\nFDW axis, which allows us to control the polarization direction of the\nFDW. We investigate the switching of the polar",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20422535211267606,
          "p": 0.37662337662337664,
          "f": 0.26484017808886395
        },
        "rouge-2": {
          "r": 0.03317535545023697,
          "p": 0.06422018348623854,
          "f": 0.043749995508008274
        },
        "rouge-l": {
          "r": 0.18309859154929578,
          "p": 0.33766233766233766,
          "f": 0.23744291781489135
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2502.11449v3",
      "true_abstract": "We study Walrasian economies (or general equilibrium models) and their\nsolution concept, the Walrasian equilibrium. A key challenge in this domain is\nidentifying price-adjustment processes that converge to equilibrium. One such\nprocess, t\\^atonnement, is an auction-like algorithm first proposed in 1874 by\nL\\'eon Walras. While continuous-time variants of t\\^atonnement are known to\nconverge to equilibrium in economies satisfying the Weak Axiom of Revealed\nPreferences (WARP), the process fails to converge in a pathological Walrasian\neconomy known as the Scarf economy. To address these issues, we analyze\nWalrasian economies using variational inequalities (VIs), an optimization\nframework. We introduce the class of mirror extragradient algorithms, which,\nunder suitable Lipschitz-continuity-like assumptions, converge to a solution of\nany VI satisfying the Minty condition in polynomial time. We show that the set\nof Walrasian equilibria of any balanced economy-which includes among others\nArrow-Debreu economies-corresponds to the solution set of an associated VI that\nsatisfies the Minty condition but is generally discontinuous. Applying the\nmirror extragradient algorithm to this VI we obtain a class of\nt\\^atonnement-like processes, which we call the mirror extrat\\^atonnement\nprocess. While our VI formulation is generally discontinuous, it is\nLipschitz-continuous in variationally stable Walrasian economies with bounded\nelasticity-including those satisfying WARP and the Scarf economy-thus\nestablishing the polynomial-time convergence of mirror extrat\\^atonnement in\nthese economies. We validate our approach through experiments on large\nArrow-Debreu economies with Cobb-Douglas, Leontief, and CES consumers, as well\nas the Scarf economy, demonstrating fast convergence in all cases without\nfailure.",
      "generated_abstract": "We introduce a general equilibrium framework for modeling complex\nsystems with many players and complex interactions. We consider a network of\nplayers interacting through a set of strategic actions. Each player can choose\nfrom a set of strategies. The strategies are characterized by a set of\npreferences over the players. We consider both symmetric and asymmetric\ninteractions. We define a notion of tractability for equilibrium strategies\nand show that the problem of deciding whether a given equilibrium strategy is\ntractable is polynomial-time solvable.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1258741258741259,
          "p": 0.33962264150943394,
          "f": 0.18367346544200341
        },
        "rouge-2": {
          "r": 0.02262443438914027,
          "p": 0.06578947368421052,
          "f": 0.033670029861805915
        },
        "rouge-l": {
          "r": 0.11888111888111888,
          "p": 0.32075471698113206,
          "f": 0.17346938380935034
        }
      }
    },
    {
      "paper_id": "stat.ML.q-fin/ST/2411.16666v2",
      "true_abstract": "We introduce CatNet, an algorithm that effectively controls False Discovery\nRate (FDR) and selects significant features in LSTM with the Gaussian Mirror\n(GM) method. To evaluate the feature importance of LSTM in time series, we\nintroduce a vector of the derivative of the SHapley Additive exPlanations\n(SHAP) to measure feature importance. We also propose a new kernel-based\ndependence measure to avoid multicollinearity in the GM algorithm, to make a\nrobust feature selection with controlled FDR. We use simulated data to evaluate\nCatNet's performance in both linear models and LSTM models with different link\nfunctions. The algorithm effectively controls the FDR while maintaining a high\nstatistical power in all cases. We also evaluate the algorithm's performance in\ndifferent low-dimensional and high-dimensional cases, demonstrating its\nrobustness in various input dimensions. To evaluate CatNet's performance in\nreal world applications, we construct a multi-factor investment portfolio to\nforecast the prices of S\\&P 500 index components. The results demonstrate that\nour model achieves superior predictive accuracy compared to traditional LSTM\nmodels without feature selection and FDR control. Additionally, CatNet\neffectively captures common market-driving features, which helps informed\ndecision-making in financial markets by enhancing the interpretability of\npredictions. Our study integrates of the Gaussian Mirror algorithm with LSTM\nmodels for the first time, and introduces SHAP values as a new feature\nimportance metric for FDR control methods, marking a significant advancement in\nfeature selection and error control for neural networks.",
      "generated_abstract": "st few years, LSTM networks have achieved remarkable performance in\nmany domains, including financial time series forecasting. However, it remains\nchallenging to identify the causal influences on the target financial time\nseries. This paper proposes CatNet, an LSTM model with Gaussian Mirror\ndescent (GMD) and SHAP feature importance, which is effective for FDR control\nand interpretable. First, we propose a novel Gaussian Mirror descent loss\nfunction based on Gaussian processes to control FDR, and further enhance it by\nusing SHAP feature importance to obtain more interpretable results. Second, we\ndevelop a new LSTM architecture with GMD and SHAP, enabling us to balance\ncausal inference and FDR control. Third, we provide theoretical guarantees for\nCatNet's effectiveness in FDR control. Finally, we conduct extensive experiments\non three financial datasets, demonstrating the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2112676056338028,
          "p": 0.32967032967032966,
          "f": 0.25751072485328524
        },
        "rouge-2": {
          "r": 0.04245283018867924,
          "p": 0.0743801652892562,
          "f": 0.05405404942744586
        },
        "rouge-l": {
          "r": 0.19718309859154928,
          "p": 0.3076923076923077,
          "f": 0.24034334287903636
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/OT/2411.19902v1",
      "true_abstract": "We propose a pair of completely data-driven algorithms for unsupervised\nclassification and dimension reduction, and we empirically study their\nperformance on a number of data sets, both simulated data in three-dimensions\nand images from the COIL-20 data set. The algorithms take as input a set of\npoints sampled from a uniform distribution supported on a metric space, the\nlatter embedded in an ambient metric space, and they output a clustering or\nreduction of dimension of the data. They work by constructing a natural family\nof graphs from the data and selecting the graph which maximizes the relative\nvon Neumann entropy of certain normalized heat operators constructed from the\ngraphs. Once the appropriate graph is selected, the eigenvectors of the graph\nLaplacian may be used to reduce the dimension of the data, and clusters in the\ndata may be identified with the kernel of the associated graph Laplacian.\nNotably, these algorithms do not require information about the size of a\nneighborhood or the desired number of clusters as input, in contrast to popular\nalgorithms such as $k$-means, and even more modern spectral methods such as\nLaplacian eigenmaps, among others.\n  In our computational experiments, our clustering algorithm outperforms\n$k$-means clustering on data sets with non-trivial geometry and topology, in\nparticular data whose clusters are not concentrated around a specific point,\nand our dimension reduction algorithm is shown to work well in several simple\nexamples.",
      "generated_abstract": "This paper studies the problem of model selection for clustering and\ndimension reduction in noncommutative data. We introduce a novel notion of\nrelative von Neumann entropy (RVNE) that measures the similarity of two\nprobability measures, and we show that the relative von Neumann entropy is a\nnoncommutative analog of the relative entropy. Based on this, we define a\nmodel selection criterion based on RVNE, which is shown to be equivalent to\nthe conditional mutual information (CMI) under the relative von Neumann\nentropy. We then propose two methods for noncommutative model selection: the\nfirst uses the relative von Neumann entropy to identify a set of hypotheses,\nand the second uses the relative von Neumann entropy to obtain an\noptimization problem that can be solved using standard noncommutative\noptimization algorithms. We illustrate the proposed methods using two\nsimulated datasets and two real data examples.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.38961038961038963,
          "f": 0.28301886329877185
        },
        "rouge-2": {
          "r": 0.05555555555555555,
          "p": 0.09836065573770492,
          "f": 0.07100591254647977
        },
        "rouge-l": {
          "r": 0.2074074074074074,
          "p": 0.36363636363636365,
          "f": 0.26415093877046997
        }
      }
    },
    {
      "paper_id": "math.CA.math/SP/2503.06957v1",
      "true_abstract": "Volterra integral and integro-differential equations have been extensively\nstudied in both pure mathematics and applied science. In one direction,\ndevelopments in analysis have yielded far-ranging existence, uniqueness, and\nregularity results. In the other, applications in science have inspired a\nsubstantial library of practical techniques to deal with such equations.\n  The present work connects these research areas by examining five large\nclasses of linear Volterra equations: integral and integro-differential\nequations with completely monotone (CM) kernels, corresponding to linear\nviscoelastic models; those with positive definite (PD) kernels, corresponding\nto partially-observed quantum systems; difference equations with PD kernels; a\nclass of generalized delay differential equations; and a class of generalized\nfractional differential equations. We develop a system of correspondences\nbetween these problems, showing that all five can be understood within the\nsame, spectral theory. We leverage this theory to recover practical,\nclosed-form solutions of all five classes, and we show that interconversion\nyields a natural, continuous involution within each class. Our work unifies\nseveral results from science: the interconversion formula of Gross, recent\nresults in viscoelasticity and operator theory for integral equations of the\nsecond type, classical formulas for Prony series and fractional differential\nequations, and the convergence of Prony series to CM kernels. Finally, our\ntheory yields a novel, geometric construction of the regularized Hilbert\ntransform, extends it to a wide class of infinite measures, and reveals a\nnatural connection to delay and fractional differential equations.\n  We leverage our theory to develop a powerful, spectral method to handle\nscalar Volterra equations numerically, and illustrate it with a number of\npractical examples.",
      "generated_abstract": "We study the spectral theory of scalar Volterra operators, which are defined\nby the linear equations\n  \\begin{equation*}\n    \\int_0^t f(t) \\, \\mathrm{d} s + g(t) = h(t),\n  \\end{equation*}\nand the associated Hilbert spaces\n  \\begin{equation*}\n    \\mathcal{H} = L^2([0,T",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0784313725490196,
          "p": 0.375,
          "f": 0.12972972686866333
        },
        "rouge-2": {
          "r": 0.01282051282051282,
          "p": 0.08571428571428572,
          "f": 0.02230483045010457
        },
        "rouge-l": {
          "r": 0.06535947712418301,
          "p": 0.3125,
          "f": 0.10810810524704172
        }
      }
    },
    {
      "paper_id": "math.QA.math/QA/2503.06280v1",
      "true_abstract": "Hopf braces are the quantum analogues of skew braces and, as such, their\ncocommutative counterparts provide solutions to the quantum Yang-Baxter\nequation. We investigate various properties of categories related to Hopf\nbraces. In particular, we prove that the category of Hopf braces is accessible\nwhile the category of cocommutative Hopf braces is even locally presentable. We\nalso show that functors forgetting multiple antipodes and/or multiplications\ndown to coalgebras are monadic. Colimits in the category of cocommutative Hopf\nbraces are described explicitly and a free cocommutative Hopf brace on an\narbitrary cocommutative Hopf algebra is constructed.",
      "generated_abstract": "In this paper we investigate the structure of the category of Hopf braces. We\nshow that the category of Hopf braces is equivalent to the category of\n$K(\\mathbb{Z}/2\\mathbb{Z})$-graded Hopf algebras with coproducts. We also\nobtain a partial list of Hopf algebras which are not in this category.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2786885245901639,
          "p": 0.5666666666666667,
          "f": 0.3736263692066175
        },
        "rouge-2": {
          "r": 0.11392405063291139,
          "p": 0.23684210526315788,
          "f": 0.1538461494601506
        },
        "rouge-l": {
          "r": 0.2459016393442623,
          "p": 0.5,
          "f": 0.3296703252505736
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.02242v1",
      "true_abstract": "Approaches for improving generative adversarial networks (GANs) training\nunder a few samples have been explored for natural images. However, these\nmethods have limited effectiveness for synthetic aperture radar (SAR) images,\nas they do not account for the unique electromagnetic scattering properties of\nSAR. To remedy this, we propose a physics-inspired regularization method dubbed\n$\\Phi$-GAN, which incorporates the ideal point scattering center (PSC) model of\nSAR with two physical consistency losses. The PSC model approximates SAR\ntargets using physical parameters, ensuring that $\\Phi$-GAN generates SAR\nimages consistent with real physical properties while preventing discriminator\noverfitting by focusing on PSC-based decision cues. To embed the PSC model into\nGANs for end-to-end training, we introduce a physics-inspired neural module\ncapable of estimating the physical parameters of SAR targets efficiently. This\nmodule retains the interpretability of the physical model and can be trained\nwith limited data. We propose two physical loss functions: one for the\ngenerator, guiding it to produce SAR images with physical parameters consistent\nwith real ones, and one for the discriminator, enhancing its robustness by\nbasing decisions on PSC attributes. We evaluate $\\Phi$-GAN across several\nconditional GAN (cGAN) models, demonstrating state-of-the-art performance in\ndata-scarce scenarios on three SAR image datasets.",
      "generated_abstract": "aging, the image quality strongly depends on the data availability,\nwhich is often limited by the scanning rate and the number of antennas. In\nthis paper, we propose a novel $\\mathbfPhi$-GAN (Generative Adversarial Network)\nto generate SAR images under limited data, where the generator $G$ aims to\ngenerate synthetic SAR images with the same distribution as the real ones, while\nthe discriminator $D$ is used to distinguish between real and generated\nimages. To solve the challenging data sparsity problem, we propose a\n$\\mathbfPhi$-regularization (or $\\mathbfPhi$-GAN) method that utilizes the\n$\\mathbfPhi$-vector to guide the generator $G$. In addition, to enhance the\nimage quality, we propose a $\\mathbfPhi$-constraining regularization (or\n$\\mathbfPhi$-GAN) method to enhance the contrast of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19852941176470587,
          "p": 0.36486486486486486,
          "f": 0.25714285257868486
        },
        "rouge-2": {
          "r": 0.02717391304347826,
          "p": 0.04950495049504951,
          "f": 0.03508771472231516
        },
        "rouge-l": {
          "r": 0.19117647058823528,
          "p": 0.35135135135135137,
          "f": 0.24761904305487537
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2502.20877v1",
      "true_abstract": "Quantitative magnetic resonance imaging (qMRI) requires multi-phase\nacqui-sition, often relying on reduced data sampling and reconstruction\nalgorithms to accelerate scans, which inherently poses an ill-posed inverse\nproblem. While many studies focus on measuring uncertainty during this process,\nfew explore how to leverage it to enhance reconstruction performance. In this\npaper, we in-troduce PUQ, a novel approach that pioneers the use of uncertainty\ninfor-mation for qMRI reconstruction. PUQ employs a two-stage reconstruction\nand parameter fitting framework, where phase-wise uncertainty is estimated\nduring reconstruction and utilized in the fitting stage. This design allows\nuncertainty to reflect the reliability of different phases and guide\ninformation integration during parameter fitting. We evaluated PUQ on in vivo\nT1 and T2 mapping datasets from healthy subjects. Compared to existing qMRI\nreconstruction methods, PUQ achieved the state-of-the-art performance in\nparameter map-pings, demonstrating the effectiveness of uncertainty guidance.\nOur code is available at https://anonymous.4open.science/r/PUQ-75B2/.",
      "generated_abstract": "vances in deep learning have led to remarkable progress in quantitative\nMRI (qMRI), but existing methods often rely on a single set of ground truth\nreconstruction masks, which are typically obtained through the use of an\nexpert annotator. In this work, we propose a novel approach that leverages\nmultiple ground truth reconstruction masks and a neural network to guide the\nreconstruction process. We propose a novel method for guiding the reconstruction\nprocess based on the phase uncertainty. The phase uncertainty is estimated\nthrough the phase-sensitive wavelet transform. The phase uncertainty is then\nused as a prior for a neural network that predicts the reconstruction mask. This\nprior-guided reconstruction mask is then used to guide the reconstruction\nprocess. Our method provides a method to provide quantitative guidance for\nguiding the reconstruction process and improves the reconstruction accuracy.\nThe proposed method is evaluated",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26851851851851855,
          "p": 0.35802469135802467,
          "f": 0.30687830198034777
        },
        "rouge-2": {
          "r": 0.055944055944055944,
          "p": 0.07017543859649122,
          "f": 0.062256804402186644
        },
        "rouge-l": {
          "r": 0.25925925925925924,
          "p": 0.345679012345679,
          "f": 0.29629629139833713
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2408.01185v1",
      "true_abstract": "We introduce a new class of anticipative backward stochastic differential\nequations with a dependence of McKean type on the law of the solution, that we\nname MKABSDE. We provide existence and uniqueness results in a general\nframework with relatively general regularity assumptions on the coefficients.\nWe show how such stochastic equations arise within the modern paradigm of\nderivative pricing where a central counterparty (CCP) requires the members to\ndeposit variation and initial margins to cover their exposure. In the case when\nthe initial margin is proportional to the Conditional Value-at-Risk (CVaR) of\nthe contract price, we apply our general result to define the price as a\nsolution of a MKABSDE. We provide several linear and non-linear simpler\napproximations, which we solve using different numerical (deterministic and\nMonte-Carlo) methods.",
      "generated_abstract": "We study the numerical approximation of McKean-Anticipative Backward\nStochastic Differential Equations (MABSDEs) arising in the context of initial\nmargin requirements. We develop numerical schemes for the case of constant\nvolatility and for the case of time-varying volatility. We prove that the\nsolutions of the approximated MABSDEs converge to the corresponding solutions of\nthe corresponding MABSDEs. We also propose a numerical method for the case of\ntime-varying volatility. We compare our numerical methods with those developed\nin [1,2,3",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16853932584269662,
          "p": 0.3333333333333333,
          "f": 0.22388059255402099
        },
        "rouge-2": {
          "r": 0.032520325203252036,
          "p": 0.0625,
          "f": 0.04278074416082864
        },
        "rouge-l": {
          "r": 0.14606741573033707,
          "p": 0.28888888888888886,
          "f": 0.19402984628536432
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/AS/2502.14893v1",
      "true_abstract": "Symbolic music is represented in two distinct forms: two-dimensional,\nvisually intuitive score images, and one-dimensional, standardized text\nannotation sequences. While large language models have shown extraordinary\npotential in music, current research has primarily focused on unimodal symbol\nsequence text. Existing general-domain visual language models still lack the\nability of music notation understanding. Recognizing this gap, we propose NOTA,\nthe first large-scale comprehensive multimodal music notation dataset. It\nconsists of 1,019,237 records, from 3 regions of the world, and contains 3\ntasks. Based on the dataset, we trained NotaGPT, a music notation visual large\nlanguage model. Specifically, we involve a pre-alignment training phase for\ncross-modal alignment between the musical notes depicted in music score images\nand their textual representation in ABC notation. Subsequent training phases\nfocus on foundational music information extraction, followed by training on\nmusic notation analysis. Experimental results demonstrate that our NotaGPT-7B\nachieves significant improvement on music understanding, showcasing the\neffectiveness of NOTA and the training pipeline. Our datasets are open-sourced\nat https://huggingface.co/datasets/MYTH-Lab/NOTA-dataset.",
      "generated_abstract": "ation (MN) is a widely used notation system for musical\nmusic, representing melodies, harmonies, and rhythms in a consistent and\nsystematic manner. However, existing music Notation understanding models\noften struggle with challenging tasks, including understanding music Notation\nin non-English languages, and understanding MNs in complex musical contexts. In\nthis paper, we propose a novel music Notation understanding framework,\nNotation-Aware Visual Large Language Model (NOTA). Our NOTA framework\nintegrates music Notation and visual images into a unified multimodal\nframework, leveraging the powerful capabilities of a large language model. We\nemploy a two-stage training strategy to optimize the multimodal alignment\nbetween music Notation and visual images. In the first stage, we first train\nNOTA on the Music21 dataset, which contains both music Notation and\nvisual images",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.256198347107438,
          "p": 0.36470588235294116,
          "f": 0.30097086893910835
        },
        "rouge-2": {
          "r": 0.0379746835443038,
          "p": 0.05405405405405406,
          "f": 0.044609660580147294
        },
        "rouge-l": {
          "r": 0.24793388429752067,
          "p": 0.35294117647058826,
          "f": 0.2912621310750307
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/SC/2502.03290v1",
      "true_abstract": "Flagellar motors enable bacteria to navigate their environments by switching\nrotation direction in response to external cues with high sensitivity. Previous\nwork suggested that ultrasensitivity of the flagellar motor originates from\nconformational spread, in which subunits of the switching complex are strongly\ncoupled to their neighbors as in an equilibrium Ising model. However, dynamic\nsingle-motor measurements indicated that rotation switching is driven out of\nequilibrium, and the mechanism for this dissipative driving remains unknown.\nHere, based on recent cryo-EM structures, we propose that local mechanical\ntorques on motor subunits can affect their conformation dynamics. This gives\nrise to a tug of war between stator-associated subunits, which produces\ncooperative, non-equilibrium switching responses without requiring\nnearest-neighbor interactions. Since subunits are effectively coupled at a\ndistance, we call this mechanism ``Global Mechanical Coupling.\" Our model makes\na qualitatively new prediction that the motor response cooperativity grows with\nthe number of stators driving rotation. Re-analyzing published motor\ndose-response curves in varying load conditions, we find tentative experimental\nevidence for this prediction. Finally, we show that operating out of\nequilibrium enables motors to achieve high cooperativity with faster responses\ncompared to equilibrium motors. Our results suggest a general role for\nmechanics in sensitive chemical regulation.",
      "generated_abstract": "llar motor is a key component of bacterial motility and is\ndiscovered to possess remarkable properties, including a highly dynamic\nmechanical response to perturbations and a non-equilibrium cooperative\nphenomenon, known as flagellar beat. Here, we use a mechanically driven\nswitching model to analyze the flagellar motor's response to perturbations,\nidentifying a striking non-equilibrium phase transition in the dynamics of the\nswitch. We show that, under conditions of sufficient force, the motor can\nexhibit an ultrasensitive, flagellar beat response to a single perturbation.\nThis response is sensitive to the initial angular position of the motor, and\nto the magnitude of the perturbation. This response is not driven by conformational\nspread, but is instead driven by the motor's mechanical response to the\nperturbation. This non-equilibrium",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20437956204379562,
          "p": 0.3888888888888889,
          "f": 0.2679425792156773
        },
        "rouge-2": {
          "r": 0.030456852791878174,
          "p": 0.056074766355140186,
          "f": 0.03947367964876265
        },
        "rouge-l": {
          "r": 0.1897810218978102,
          "p": 0.3611111111111111,
          "f": 0.24880382323481606
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.05664v1",
      "true_abstract": "In financial applications, we often observe both global and local factors\nthat are modeled by a multi-level factor model. When detecting unknown local\ngroup memberships under such a model, employing a covariance matrix as an\nadjacency matrix for local group memberships is inadequate due to the\npredominant effect of global factors. Thus, to detect a local group structure\nmore effectively, this study introduces an inverse covariance matrix-based\nfinancial adjacency matrix (IFAM) that utilizes negative values of the inverse\ncovariance matrix. We show that IFAM ensures that the edge density between\ndifferent groups vanishes, while that within the same group remains\nnon-vanishing. This reduces falsely detected connections and helps identify\nlocal group membership accurately. To estimate IFAM under the multi-level\nfactor model, we introduce a factor-adjusted GLASSO estimator to address the\nprevalent global factor effect in the inverse covariance matrix. An empirical\nstudy using returns from international stocks across 20 financial markets\ndemonstrates that incorporating IFAM effectively detects latent local groups,\nwhich helps improve the minimum variance portfolio allocation performance.",
      "generated_abstract": "data is one of the most important sources for detecting local\ngroups, which are defined as small groups of related assets. Financial\ncorporations can use financial data to identify local groups, which is a\nvaluable asset for investors. The inverse covariance matrix (ICM) is a\ncomplementary measure of the correlation between two variables in the\ncontext of the covariance matrix. This paper investigates the ICM-based\nfinancial adjacency matrix, which is a matrix that reflects the relationship\nbetween financial variables. This paper uses the ICM as the basis for a\nfinancial adjacency matrix, which can be used to detect local groups. This paper\nalso proposes a method to calculate the ICM-based financial adjacency matrix,\nand it shows that the ICM-based financial adjacency matrix is similar to the\ngeneralized adjacency matrix and is similar",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22321428571428573,
          "p": 0.36764705882352944,
          "f": 0.27777777307654333
        },
        "rouge-2": {
          "r": 0.07643312101910828,
          "p": 0.11009174311926606,
          "f": 0.09022555907258775
        },
        "rouge-l": {
          "r": 0.21428571428571427,
          "p": 0.35294117647058826,
          "f": 0.26666666196543215
        }
      }
    },
    {
      "paper_id": "eess.SP.cs/ET/2503.08062v1",
      "true_abstract": "Orthogonal frequency division multiplexing (OFDM), which has been the\ndominating waveform for contemporary wireless communications, is also regarded\nas a competitive candidate for future integrated sensing and communication\n(ISAC) systems. Existing works on OFDM-ISAC usually assume that the maximum\nsensing range should be limited by the cyclic prefix (CP) length since\ninter-symbol interference (ISI) and inter-carrier interference (ICI) should be\navoided. However, in this paper, we provide rigorous analysis to reveal that\nthe random data embedded in OFDM-ISAC signal can actually act as a free ``mask\"\nfor ISI, which makes ISI/ICI random and hence greatly attenuated after radar\nsignal processing. The derived signal-to-interference-plus-noise ratio (SINR)\nin the range profile demonstrates that the maximum sensing range of OFDM-ISAC\ncan greatly exceed the ISI-free distance that is limited by the CP length,\nwhich is validated by simulation results. To further mitigate power degradation\nfor long-range targets, a novel sliding window sensing method is proposed,\nwhich iteratively detects and cancels short-range targets before shifting the\ndetection window. The shifted detection window can effectively compensate the\npower degradation due to insufficient CP length for long-range targets. Such\nresults provide valuable guidance for the CP length design in OFDM-ISAC\nsystems.",
      "generated_abstract": "SAC, the minimum sensing range (MSR) is a critical parameter for\nfine sensing of the wireless channel. In this paper, we investigate the\neffect of the CP length on the MSR for OFDM-ISAC. We first derive an analytical\nexpression for the MSR as a function of the number of symbols $K$ and the\nnumber of subcarriers $M$, assuming that the CP length is fixed and the number\nof symbols $K$ is varied. The analysis reveals that the MSR is insensitive to\nthe CP length as long as $M \\leq 4K$. We then present a heuristic algorithm to\napproximate the MSR for OFDM-ISAC, and demonstrate its effectiveness by\nsimulation. The results show that the heuristic algorithm provides a\nreasonable approximation to the MSR for OFDM-ISAC. In addition, the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19047619047619047,
          "p": 0.34285714285714286,
          "f": 0.24489795459183678
        },
        "rouge-2": {
          "r": 0.05027932960893855,
          "p": 0.08823529411764706,
          "f": 0.0640569348772182
        },
        "rouge-l": {
          "r": 0.1746031746031746,
          "p": 0.3142857142857143,
          "f": 0.22448979132653069
        }
      }
    },
    {
      "paper_id": "cs.AR.cs/AR/2503.06862v1",
      "true_abstract": "Weight-only quantization has emerged as a promising solution to the\ndeployment challenges of large language models (LLMs). However, it necessitates\nFP-INT operations, which make implementation on general-purpose hardware like\nGPUs difficult. In this paper, we propose FIGLUT, an efficient look-up table\n(LUT)-based GEMM accelerator architecture. Instead of performing traditional\narithmetic operations, FIGLUT retrieves precomputed values from an LUT based on\nweight patterns, significantly reducing the computational complexity. We also\nintroduce a novel LUT design that addresses the limitations of conventional\nmemory architectures. To further improve LUT-based operations, we propose a\nhalf-size LUT combined with a dedicated decoding and multiplexing unit. FIGLUT\nefficiently supports different bit precisions and quantization methods using a\nsingle fixed hardware configuration. For the same 3-bit weight precision,\nFIGLUT demonstrates 59% higher TOPS/W and 20% lower perplexity than\nstate-of-the-art accelerator designs. When targeting the same perplexity,\nFIGLUT achieves 98% higher TOPS/W by performing 2.4-bit operations.",
      "generated_abstract": "on-intensive matrix-matrix multiplication (GEMM) is a fundamental\ntask in modern computing, particularly in large-scale data analytics and\ndeep learning applications. In recent years, FP-INT GEMM accelerators have\nbeen introduced to improve performance, efficiency, and energy consumption.\nHowever, due to the significant number of multiplications, it is challenging to\ndesign FP-INT GEMM accelerators using lookup tables (LUTs) efficiently. In this\nwork, we propose a novel FP-INT GEMM accelerator, named FIGLUT, which uses\nlook-up tables to accelerate the computation of the inner product between two\nFP-INT matrices. We first design a novel FIGLUT accelerator, named FIG, to\nperform the inner product computation efficiently. The FIG accelerator uses\nlook-up tables to store the values of the inner product, which allows the\nFIGL",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.211864406779661,
          "p": 0.3125,
          "f": 0.2525252477094174
        },
        "rouge-2": {
          "r": 0.034482758620689655,
          "p": 0.04716981132075472,
          "f": 0.03984063257091217
        },
        "rouge-l": {
          "r": 0.2033898305084746,
          "p": 0.3,
          "f": 0.24242423760840742
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2503.05185v1",
      "true_abstract": "Finance decision-making often relies on in-depth data analysis across various\ndata sources, including financial tables, news articles, stock prices, etc. In\nthis work, we introduce FinTMMBench, the first comprehensive benchmark for\nevaluating temporal-aware multi-modal Retrieval-Augmented Generation (RAG)\nsystems in finance. Built from heterologous data of NASDAQ 100 companies,\nFinTMMBench offers three significant advantages. 1) Multi-modal Corpus: It\nencompasses a hybrid of financial tables, news articles, daily stock prices,\nand visual technical charts as the corpus. 2) Temporal-aware Questions: Each\nquestion requires the retrieval and interpretation of its relevant data over a\nspecific time period, including daily, weekly, monthly, quarterly, and annual\nperiods. 3) Diverse Financial Analysis Tasks: The questions involve 10\ndifferent tasks, including information extraction, trend analysis, sentiment\nanalysis and event detection, etc. We further propose a novel TMMHybridRAG\nmethod, which first leverages LLMs to convert data from other modalities (e.g.,\ntabular, visual and time-series data) into textual format and then incorporates\ntemporal information in each node when constructing graphs and dense indexes.\nIts effectiveness has been validated in extensive experiments, but notable gaps\nremain, highlighting the challenges presented by our FinTMMBench.",
      "generated_abstract": "vancements in financial modelling have demonstrated the potential of\nRAGs to capture temporal patterns in financial time series. However, existing\nRAG models often struggle to capture the complexity of financial data,\nparticularly when dealing with multi-modal data, where the temporal and modal\ndimensions are often intertwined. To address this, we introduce FinTMMBench,\nthe first benchmarking suite to evaluate multi-modal RAGs in financial\napplications. Our benchmarking methodology leverages the MIMIC-III dataset to\nprovide an objective evaluation of the performance of state-of-the-art\nmulti-modal RAGs, including multi-modal RAGs with a single transformer, a\nmulti-modal RAG with a multi-modal transformer, and a multi-modal RAG with a\nmulti-modal transformer with an additional memory block. Our benchmarking\nmethodology also includes a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11920529801324503,
          "p": 0.2647058823529412,
          "f": 0.16438355736202342
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.04950495049504951,
          "f": 0.03558718400729534
        },
        "rouge-l": {
          "r": 0.11920529801324503,
          "p": 0.2647058823529412,
          "f": 0.16438355736202342
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.05300v1",
      "true_abstract": "This article introduces a subbagging (subsample aggregating) approach for\nvariable selection in regression within the context of big data. The proposed\nsubbagging approach not only ensures that variable selection is scalable given\nthe constraints of available computational resources, but also preserves the\nstatistical efficiency of the resulting estimator. In particular, we propose a\nsubbagging loss function that aggregates the least-squares approximations of\nthe loss function for each subsample. Subsequently, we penalize the subbagging\nloss function via an adaptive LASSO-type regularizer, and obtain a regularized\nestimator to achieve variable selection. We then demonstrate that the\nregularized estimator exhibits $\\sqrt{N}$-consistency and possesses the oracle\nproperties, where $N$ represents the size of the full sample in the big data.\nIn addition, we propose a subbagging Bayesian information criterion to select\nthe regularization parameter, ensuring that the regularized estimator achieves\nselection consistency. Simulation experiments are conducted to demonstrate the\nnumerical performance. A U.S. census dataset is analyzed to illustrate the\nusefulness and computational scalability of the subbagging variable selection\nmethod.",
      "generated_abstract": "Bagging is a popular feature selection method that employs a random\nsampling approach to reduce the variance of the predictive model. However,\nbagging often produces models with limited interpretability, making it difficult\nto assess the model's predictive accuracy. In this paper, we propose a novel\nsubbagging approach for variable selection in big data. We introduce the\nvariable selection index (VSI) as a novel metric for variable selection,\nenabling us to select the subset of variables that most influence the model.\nWe demonstrate the effectiveness of our approach through simulations and an\napplication to the World Bank loan dataset.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23529411764705882,
          "p": 0.3380281690140845,
          "f": 0.2774566425593906
        },
        "rouge-2": {
          "r": 0.08843537414965986,
          "p": 0.13829787234042554,
          "f": 0.10788381266920356
        },
        "rouge-l": {
          "r": 0.20588235294117646,
          "p": 0.29577464788732394,
          "f": 0.24277456163453512
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.09541v1",
      "true_abstract": "The paper studies the problem of detecting and locating change points in\nmultivariate time-evolving data. The problem has a long history in statistics\nand signal processing and various algorithms have been developed primarily for\nsimple parametric models. In this work, we focus on modeling the data through\nfeed-forward neural networks and develop a detection strategy based on the\nfollowing two-step procedure. In the first step, the neural network is trained\nover a prespecified window of the data, and its test error function is\ncalibrated over another prespecified window. Then, the test error function is\nused over a moving window to identify the change point. Once a change point is\ndetected, the procedure involving these two steps is repeated until all change\npoints are identified. The proposed strategy yields consistent estimates for\nboth the number and the locations of the change points under temporal\ndependence of the data-generating process. The effectiveness of the proposed\nstrategy is illustrated on synthetic data sets that provide insights on how to\nselect in practice tuning parameters of the algorithm and in real data sets.\nFinally, we note that although the detection strategy is general and can work\nwith different neural network architectures, the theoretical guarantees\nprovided are specific to feed-forward neural architectures.",
      "generated_abstract": "int detection (CPD) is a fundamental problem in data science,\nrepresenting the temporal evolution of an unknown event in time-evolving data.\nIn this paper, we propose a novel neural network-based CPD approach for large-\nscale time-evolving data, where the change point is identified by a sequence of\nneural network outputs. We introduce a novel model-free algorithm that\nautomatically selects the number of hidden layers and the number of nodes in\neach layer. The proposed approach has a simple and fast implementation,\nrequires no pre-training or adaptation, and is applicable to both continuous and\ndiscrete time-evolving data. We also propose a novel extension of the\nmodel-free algorithm to enable a direct optimization of the CPD parameters\nthrough an unsupervised training procedure, without requiring access to the\ntraining data. Extensive experiments on synthetic and real-world data demonstrate\nthe effect",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2786885245901639,
          "p": 0.37362637362637363,
          "f": 0.3192488213969892
        },
        "rouge-2": {
          "r": 0.06417112299465241,
          "p": 0.09375,
          "f": 0.07619047136588593
        },
        "rouge-l": {
          "r": 0.23770491803278687,
          "p": 0.31868131868131866,
          "f": 0.2723004645894775
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.02030v1",
      "true_abstract": "We study policy evaluation problems in multi-task reinforcement learning (RL)\nunder a low-rank representation setting. In this setting, we are given $N$\nlearning tasks where the corresponding value function of these tasks lie in an\n$r$-dimensional subspace, with $r<N$. One can apply the classic\ntemporal-difference (TD) learning method for solving these problems where this\nmethod learns the value function of each task independently. In this paper, we\nare interested in understanding whether one can exploit the low-rank structure\nof the multi-task setting to accelerate the performance of TD learning. To\nanswer this question, we propose a new variant of TD learning method, where we\nintegrate the so-called truncated singular value decomposition step into the\nupdate of TD learning. This additional step will enable TD learning to exploit\nthe dominant directions due to the low rank structure to update the iterates,\ntherefore, improving its performance. Our empirical results show that the\nproposed method significantly outperforms the classic TD learning, where the\nperformance gap increases as the rank $r$ decreases.\n  From the theoretical point of view, introducing the truncated singular value\ndecomposition step into TD learning might cause an instability on the updates.\nWe provide a theoretical result showing that the instability does not happen.\nSpecifically, we prove that the proposed method converges at a rate\n$\\mathcal{O}(\\frac{\\ln(t)}{t})$, where $t$ is the number of iterations. This\nrate matches that of the standard TD learning.",
      "generated_abstract": "ding how low-rank representations can be leveraged to accelerate\nlow-rank representation-based multi-task learning (MTL) has received significant\nattention in recent years. However, existing approaches typically focus on\nlearning a single task-specific low-rank representation, while ignoring the\ninteraction between different tasks. To address this limitation, we propose\na two-stage MTL framework, named Multi-Task Rank (MTR), which learns a\nlow-rank representation for all tasks simultaneously. We first design a\nmulti-task learning (MTL) loss that explicitly encourages the shared low-rank\nrepresentation to capture the commonalities across tasks, while also\nincorporating the diversity of representations across tasks to promote the\nrepresentation learning. Then, we develop a MTL-based training strategy to\noptimize the shared low-rank representation, thereby facilitating the\ntransfer learning between tasks. Ext",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16153846153846155,
          "p": 0.25301204819277107,
          "f": 0.19718309383499757
        },
        "rouge-2": {
          "r": 0.024390243902439025,
          "p": 0.044642857142857144,
          "f": 0.031545736755267416
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.24096385542168675,
          "f": 0.18779342247349526
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2503.02923v1",
      "true_abstract": "Diverse organisms exploit the geomagnetic field (GMF) for migration.\nMigrating birds employ an intrinsically quantum mechanical mechanism for\ndetecting the geomagnetic field: absorption of a blue photon generates a\nradical pair whose two electrons precess at different rates in the magnetic\nfield, thereby sensitizing cells to the direction of the GMF. In this work,\nusing an in vitro injury model, we discovered a quantum-based mechanism of\ncellular migration. Specifically, we show that migrating cells detect the GMF\nvia an optically activated, electron spin-based mechanism. Cell injury provokes\nacute emission of blue photons, and these photons sensitize muscle progenitor\ncells to the magnetic field. We show that the magnetosensitivity of muscle\nprogenitor cells is (a) activated by blue light, but not by green or red light,\nand (b) disrupted by the application of an oscillatory field at the frequency\ncorresponding to the energy of the electron-spin/magnetic field interaction. A\ncomprehensive analysis of protein expression reveals that the ability of blue\nphotons to promote cell motility is mediated by activation of calmodulin\ncalcium sensors. Collectively, these data suggest that cells possess a\nlight-dependent magnetic compass driven by electron spin dynamics.",
      "generated_abstract": "ation is a fundamental process of cellular behavior. The cell\ncells respond to their environment by changing their shape and movement.\nCellular motility is a key feature of cellular behavior and has been extensively\ninvestigated. We show that the dynamics of the spin of the electron, a key\ncomponent of the cellular magnetic field, can be used to guide cell\nmotility. Our study focuses on the spin dynamics of the electron in the\ncytoplasm of a cell. The electron spins are in a precessing state, oscillating\nwith a frequency that depends on the cellular velocity and magnetic field. We\nshow that this precession can guide the cell to follow the magnetic field\nlines, with the cell following the field lines faster when the electron\nprecesses faster. We also show that the magnetic field can guide the cell\nmotility by driving the precession of the electron. Our study reveals a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.3380281690140845,
          "f": 0.2513088958526357
        },
        "rouge-2": {
          "r": 0.08,
          "p": 0.11570247933884298,
          "f": 0.0945945897610028
        },
        "rouge-l": {
          "r": 0.19166666666666668,
          "p": 0.323943661971831,
          "f": 0.24083769166415403
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.00358v1",
      "true_abstract": "This paper considers nonparametric estimation and inference in first-order\nautoregressive (AR(1)) models with deterministically time-varying parameters. A\nkey feature of the proposed approach is to allow for time-varying stationarity\nin some time periods, time-varying nonstationarity (i.e., unit root or\nlocal-to-unit root behavior) in other periods, and smooth transitions between\nthe two. The estimation of the AR parameter at any time point is based on a\nlocal least squares regression method, where the relevant initial condition is\nendogenous. We obtain limit distributions for the AR parameter estimator and\nt-statistic at a given point $\\tau$ in time when the parameter exhibits unit\nroot, local-to-unity, or stationary/stationary-like behavior at time $\\tau$.\nThese results are used to construct confidence intervals and median-unbiased\ninterval estimators for the AR parameter at any specified point in time. The\nconfidence intervals have correct asymptotic coverage probabilities with the\ncoverage holding uniformly over stationary and nonstationary behavior of the\nobservations.",
      "generated_abstract": "In this paper, we consider the stationary/nonstationary autoregressive\ntime-varying parameter model with a single exogenous variable, where the\ndynamic parameters of the exogenous variable are time-varying. The parameters\nof the model are estimated using the maximum likelihood estimator. We show\nthat the proposed estimator is consistent and asymptotically normal, and that\nthe asymptotic distribution of the estimator is the same as the one of the\nstandard autoregressive model. We derive the asymptotic distribution of the\nestimator and show that it is equivalent to the one of the standard autoregressive\nmodel.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18,
          "p": 0.3829787234042553,
          "f": 0.24489795483363413
        },
        "rouge-2": {
          "r": 0.028368794326241134,
          "p": 0.056338028169014086,
          "f": 0.037735844601727124
        },
        "rouge-l": {
          "r": 0.18,
          "p": 0.3829787234042553,
          "f": 0.24489795483363413
        }
      }
    },
    {
      "paper_id": "math.CO.math/AT/2503.05385v1",
      "true_abstract": "A Bier sphere is a simplicial sphere obtained as the deleted join of a\nsimplicial complex and its combinatorial Alexander dual. We focus on particular\nclasses of full subcomplexes of Bier spheres, and determine their topological\ntypes. As applications, we explicitly describe the cohomology of real toric\nspaces associated with Bier spheres.",
      "generated_abstract": "a number of theorems concerning the full subcomplexes of\n$B(n,1)$, a $C^*$-algebra that arises in the theory of Bier spheres. We show\nthat the full subcomplex of $B(n,1)$ consists of compactly supported\n$C^*$-algebras and that the inclusion $B(n,1) \\to B(n+1,1)$ induces a\nhomeomorphism between the full subcomplex and the full subcomplex of\n$B(n+1,1)$. We also prove that the full subcomplex of $B(n,1)$ consists of\nclosed $C^*$-subalgebras. We also prove that the full subcomplex of $B(n,1)$\nconsists of compactly supported $C^*$-subalgebras and that the inclusion $B(n,1)\n\\to B(n+1,1)$ induces a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21428571428571427,
          "p": 0.24324324324324326,
          "f": 0.22784809628585176
        },
        "rouge-2": {
          "r": 0.08,
          "p": 0.07407407407407407,
          "f": 0.0769230719304737
        },
        "rouge-l": {
          "r": 0.21428571428571427,
          "p": 0.24324324324324326,
          "f": 0.22784809628585176
        }
      }
    },
    {
      "paper_id": "math.AG.math/AG/2503.10564v1",
      "true_abstract": "We prove that holomorphic maps from an open subset of a complex smooth\nprojective curve to a complex smooth projective rationally simply connected\nvariety can be approximated by algebraic maps for the compact-open topology.\nThis theorem can be applied in particular when the target is a smooth\nhypersurface of degree d in P^n with n greater than or equal to d^2-1. We\ndeduce it from a more general result: the tight approximation property holds\nfor rationally simply connected varieties over function fields of complex\ncurves.",
      "generated_abstract": "In this paper, we study the problem of constructing a compact, rational\nsymplectic manifold of dimension $d$ for any $d \\geq 2$. We prove that the\nproblem is NP-hard even for $d=2$. In this context, we prove that there exists\na sufficiently large constant $C$ such that the problem is NP-hard for $d \\geq\nC$. We also present a proof that the problem is NP-hard for $d \\geq 10$. Our\nproofs rely on the construction of a tetrahedron in a hyperplane section of\nthe sphere. We give an alternative proof of the fact that the problem is\nNP-hard for $d \\geq 10$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16129032258064516,
          "p": 0.18181818181818182,
          "f": 0.17094016595806866
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.03896103896103896,
          "f": 0.0387096724195636
        },
        "rouge-l": {
          "r": 0.16129032258064516,
          "p": 0.18181818181818182,
          "f": 0.17094016595806866
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.09155v1",
      "true_abstract": "We consider time-invariant nonlinear $n$-dimensional strongly $2$-cooperative\nsystems, that is, systems that map the set of vectors with up to weak sign\nvariation to its interior. Strongly $2$-cooperative systems enjoy a strong\nPoincare-Bendixson property: bounded solutions that maintain a positive\ndistance from the set of equilibria converge to a periodic solution. For\nstrongly $2$-cooperative systems whose trajectories evolve in a bounded and\ninvariant set that contains a single unstable equilibrium, we provide a simple\ncriterion for the existence of periodic trajectories. Moreover, we explicitly\ncharacterize a positive-measure set of initial conditions which yield solutions\nthat asymptotically converge to a periodic trajectory. We demonstrate our\ntheoretical results using two models from systems biology, the $n$-dimensional\nGoodwin oscillator and a $4$-dimensional biomolecular oscillator with\nRNA-mediated regulation, and provide numerical simulations that verify the\ntheoretical results.",
      "generated_abstract": "We study the dynamics of a strongly 2-cooperative system with two types of\nparticipants, and we show that the system undergoes a period-doubling\ntransformation, and that the period of the periodic orbits depends on the\nparticipants' strengths. Furthermore, we show that the system converges to a\nperiodic orbit if and only if the participant's strength is sufficiently large.\nThese results generalize previous findings for the 1-cooperative case.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.30434782608695654,
          "f": 0.21538461081183444
        },
        "rouge-2": {
          "r": 0.02459016393442623,
          "p": 0.04918032786885246,
          "f": 0.03278688080145779
        },
        "rouge-l": {
          "r": 0.15476190476190477,
          "p": 0.2826086956521739,
          "f": 0.19999999542721902
        }
      }
    },
    {
      "paper_id": "cs.LG.q-fin/CP/2502.17518v1",
      "true_abstract": "This paper presents a comprehensive study on the use of ensemble\nReinforcement Learning (RL) models in financial trading strategies, leveraging\nclassifier models to enhance performance. By combining RL algorithms such as\nA2C, PPO, and SAC with traditional classifiers like Support Vector Machines\n(SVM), Decision Trees, and Logistic Regression, we investigate how different\nclassifier groups can be integrated to improve risk-return trade-offs. The\nstudy evaluates the effectiveness of various ensemble methods, comparing them\nwith individual RL models across key financial metrics, including Cumulative\nReturns, Sharpe Ratios (SR), Calmar Ratios, and Maximum Drawdown (MDD). Our\nresults demonstrate that ensemble methods consistently outperform base models\nin terms of risk-adjusted returns, providing better management of drawdowns and\noverall stability. However, we identify the sensitivity of ensemble performance\nto the choice of variance threshold {\\tau}, highlighting the importance of\ndynamic {\\tau} adjustment to achieve optimal performance. This study emphasizes\nthe value of combining RL with classifiers for adaptive decision-making, with\nimplications for financial trading, robotics, and other dynamic environments.",
      "generated_abstract": "RL is a state-of-the-art approach in reinforcement learning that\nutilizes multiple agents to improve generalization and robustness. However,\nexisting ensemble RL methods often rely on a single, pre-trained agent that\nleads to a limited ability to adapt to changing market conditions. In this\nwork, we propose an ensemble RL method that leverages multiple classifier\nmodels to adapt to market changes, thereby enhancing risk-return trade-offs.\nSpecifically, we develop a multi-agent RL framework that employs a\nthree-level ensemble architecture. The first level comprises a base agent,\nwhich uses a pre-trained base agent to obtain the initial state of the\nmarket. The second level comprises a set of ensemble agents, each of which\nuses a classifier model to predict the next state of the market. Finally, the\nthird level comprises a risk-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.25316455696202533,
          "f": 0.20100502033787038
        },
        "rouge-2": {
          "r": 0.03125,
          "p": 0.04310344827586207,
          "f": 0.03623187918504582
        },
        "rouge-l": {
          "r": 0.15833333333333333,
          "p": 0.24050632911392406,
          "f": 0.19095476908158895
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.06441v1",
      "true_abstract": "Company financial risks pose a significant threat to personal wealth and\nnational economic stability, stimulating increasing attention towards the\ndevelopment of efficient andtimely methods for monitoring them. Current\napproaches tend to use graph neural networks (GNNs) to model the momentum\nspillover effect of risks. However, due to the black-box nature of GNNs, these\nmethods leave much to be improved for precise and reliable explanations towards\ncompany risks. In this paper, we propose CF3, a novel Counterfactual and\nFactual learning method for company Financial risk detection, which generates\nevidence subgraphs on company knowledge graphs to reliably detect and explain\ncompany financial risks. Specifically, we first propose a meta-path attribution\nprocess based on Granger causality, selecting the meta-paths most relevant to\nthe target node labels to construct an attribution subgraph. Subsequently, we\npropose anedge-type-aware graph generator to identify important edges, and we\nalso devise a layer-based feature masker to recognize crucial node features.\nFinally, we utilize counterfactual-factual reasoning and a loss function based\non attribution subgraphs to jointly guide the learning of the graph generator\nand feature masker. Extensive experiments on three real-world datasets\ndemonstrate the superior performance of our method compared to state-of-the-art\napproaches in the field of financial risk detection.",
      "generated_abstract": "aper, we present a novel methodology for identifying evidence\nsubgraphs, i.e., subgraphs that can be used as explanations for a given\nfinancial risk. To this end, we propose two graph-based methods that are\nefficient, computationally-efficient, and can be used to identify subgraphs in\nthe context of a financial risk detection scenario. The first method,\ncounterfactual-based, employs the concept of counterfactual reasoning, which\nallows us to construct subgraphs that are hypothetical but explain the\nobserved financial risk. The second method, factual-based, employs the\nconcept of factual reasoning, which allows us to construct subgraphs that\nexplain the observed financial risk and provide evidence to support the\nexplanation. We demonstrate the effectiveness of the proposed methods by\nanalyzing a dataset of financial risk prediction scenarios,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19083969465648856,
          "p": 0.3424657534246575,
          "f": 0.2450980346198578
        },
        "rouge-2": {
          "r": 0.05181347150259067,
          "p": 0.09803921568627451,
          "f": 0.06779660564527465
        },
        "rouge-l": {
          "r": 0.19083969465648856,
          "p": 0.3424657534246575,
          "f": 0.2450980346198578
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2411.07674v2",
      "true_abstract": "We prove that a two-cycle equilibrium in a general equilibrium model with\ninfinitely-lived agents also constitutes an equilibrium in an overlapping\ngenerations (OLG) model. Conversely, an equilibrium in an OLG model that\nsatisfies additional conditions is part of an equilibrium in a general\nequilibrium model with infinitely-lived agents. Applying this result, we\ndemonstrate that equilibrium indeterminacy and rational asset price bubbles may\narise in both types of models.",
      "generated_abstract": "This paper addresses the relationship between general equilibrium models\nwith infinite-lived agents and overlapping generations models. First, we\nconsider a model where the state is a random variable representing the number\nof agents in the system. Second, we consider a model where the state is the\nnumber of generations in the system. The model with infinite-lived agents has\na unique equilibrium solution, whereas the model with overlapping generations\nhas multiple equilibria. We study the relationship between the two models,\nincluding the existence of a unique equilibrium solution for the model with\ninfinite-lived agents and the existence of multiple equilibria for the model\nwith overlapping generations. Finally, we apply our results to the model with\noverlapping generations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.35714285714285715,
          "p": 0.29411764705882354,
          "f": 0.3225806402081166
        },
        "rouge-2": {
          "r": 0.05555555555555555,
          "p": 0.0379746835443038,
          "f": 0.04511277713155119
        },
        "rouge-l": {
          "r": 0.35714285714285715,
          "p": 0.29411764705882354,
          "f": 0.3225806402081166
        }
      }
    },
    {
      "paper_id": "nlin.CD.nlin/CD/2503.05462v1",
      "true_abstract": "The wave kinetic equation has become an important tool in different fields of\nphysics. In particular, for surface gravity waves, it is the backbone of wave\nforecasting models. Its derivation is based on the Hamiltonian dynamics of\nsurface gravity waves. Only at the end of the derivation are the\nnon-conservative effects, such as forcing and dissipation, included as\nadditional terms to the collision integral. In this paper, we present a first\nattempt to derive the wave kinetic equation when the dissipation/forcing is\nincluded in the deterministic dynamics. If, in the dynamical equations, the\ndissipation/forcing is one order of magnitude smaller than the nonlinear\neffect, then the classical wave action balance equation is obtained and the\nkinetic time scale corresponds to the dissipation/forcing time scale. However,\nif we assume that the nonlinearity and the dissipation/forcing act on the same\ndynamical time scale, we find that the dissipation/forcing dominates the\ndynamics and the resulting collision integral appears in a modified form, at a\nhigher order.",
      "generated_abstract": "igate the wave kinetic equation for a self-gravitating fluid in\nthe presence of a forcing and dissipation. We find that the wave equation\nincorporating dissipation, which is a generalization of the standard Boltzmann\nequation, is not equivalent to the standard Boltzmann equation. Our findings\nhave implications for the dynamics of turbulent flows in the presence of\nnon-Newtonian forces. We show that the velocity and energy spectra of the\nturbulence can be reconstructed from the solution of the wave kinetic equation.\nWe find that the dissipation term is responsible for the enhancement of the\nvelocity spectrum. We also show that the dissipation term can alter the\ndissipation rate of the fluid, which can affect the behavior of turbulence. We\nshow that for a homogeneous fluid, the dissipation term can lead to\nnon",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1702127659574468,
          "p": 0.26229508196721313,
          "f": 0.20645160812986485
        },
        "rouge-2": {
          "r": 0.07534246575342465,
          "p": 0.11,
          "f": 0.08943088948377315
        },
        "rouge-l": {
          "r": 0.1595744680851064,
          "p": 0.2459016393442623,
          "f": 0.19354838232341323
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2502.16108v1",
      "true_abstract": "The binary expansions of irrational algebraic numbers can serve as\nhigh-quality pseudorandom binary sequences. This study presents an efficient\nmethod for computing the exact binary expansions of real quadratic algebraic\nintegers using Newton's method. To this end, we clarify conditions under which\nthe first $N$ bits of the binary expansion of an irrational number match those\nof its upper rational approximation. Furthermore, we establish that the\nworst-case time complexity of generating a sequence of length $N$ with the\nproposed method is equivalent to the complexity of multiplying two $N$-bit\nintegers, showing its efficiency compared to a previously proposed true orbit\ngenerator. We report the results of numerical experiments on computation time\nand memory usage, highlighting in particular that the proposed method\nsuccessfully accelerates true orbit pseudorandom number generation. We also\nconfirm that a generated pseudorandom sequence successfully passes all the\nstatistical tests included in RabbitFile of TestU01.",
      "generated_abstract": "dom number generators (PRNGs) are essential for numerous applications,\nincluding machine learning, simulation, and cryptography. However, the\ncomputationally expensive process of generating PRNGs is a major bottleneck in\ntheir widespread adoption. In this work, we propose a novel approach to\nreducing the computational cost of PRNGs, using a Newton method. Our method\nincorporates the concept of a \"pseudorandomness exponent\", a new measure that\ntells us how hard it is to generate a PRNG that is not random. By exploiting\nthis exponent, we design an accelerated PRNG that uses only a fraction of the\ncomputational resources needed by current state-of-the-art PRNGs. We evaluate\nthe performance of our accelerated PRNG in a number of simulation-based and\nreal-world scenarios, demonstrating that it can achieve comparable or",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.20224719101123595,
          "f": 0.19148935671627446
        },
        "rouge-2": {
          "r": 0.014388489208633094,
          "p": 0.01680672268907563,
          "f": 0.015503870999040112
        },
        "rouge-l": {
          "r": 0.1717171717171717,
          "p": 0.19101123595505617,
          "f": 0.18085105884393404
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/ET/2503.07599v1",
      "true_abstract": "Generative AI is transforming education by enabling personalized, on-demand\nlearning experiences. However, AI tutors lack the ability to assess a learner's\ncognitive state in real time, limiting their adaptability. Meanwhile,\nelectroencephalography (EEG)-based neuroadaptive systems have successfully\nenhanced engagement by dynamically adjusting learning content. This paper\npresents NeuroChat, a proof-of-concept neuroadaptive AI tutor that integrates\nreal-time EEG-based engagement tracking with generative AI. NeuroChat\ncontinuously monitors a learner's cognitive engagement and dynamically adjusts\ncontent complexity, response style, and pacing using a closed-loop system. We\nevaluate this approach in a pilot study (n=24), comparing NeuroChat to a\nstandard LLM-based chatbot. Results indicate that NeuroChat enhances cognitive\nand subjective engagement but does not show an immediate effect on learning\noutcomes. These findings demonstrate the feasibility of real-time cognitive\nfeedback in LLMs, highlighting new directions for adaptive learning, AI\ntutoring, and human-AI interaction.",
      "generated_abstract": "Neuroadaptive AI (NAA) systems are designed to adapt to individual\nchats in real-time, leveraging neural network-based models to understand and\nrespond to user behavior. This study introduces NeuroChat, a NAA chatbot\ndesigned to support students with learning disabilities. NeuroChat leverages\ndeep learning models to analyze chat logs and respond with tailored suggestions\nbased on user responses. This paper examines NeuroChat's architecture and\nperformance, demonstrating how the system adapts to individual chat conversations\nand responds to user needs. We evaluate NeuroChat's effectiveness in enhancing\nstudent engagement, supporting self-regulation, and promoting student\nself-awareness. Our findings highlight the potential of NAA systems to\naddress student needs and enhance learning experiences.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20952380952380953,
          "p": 0.29333333333333333,
          "f": 0.2444444395833334
        },
        "rouge-2": {
          "r": 0.02962962962962963,
          "p": 0.039603960396039604,
          "f": 0.033898300188524126
        },
        "rouge-l": {
          "r": 0.20952380952380953,
          "p": 0.29333333333333333,
          "f": 0.2444444395833334
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2412.17005v1",
      "true_abstract": "This study examines the phytochemical characteristics of Ayurvedic products.\nAn analysis was performed on Kottakkal Ayurveda Triphala (T), Kottakkal\nAyurveda Hinguvachadi Churnam (H), and Kottakkal Ayurveda Jirakadyarishtam (J)\nusing GC-MS and LC-MS techniques to determine their bioactive constituents,\nwhile also assessing their antimicrobial, docking, anticancer, and\nanti-diabetic activities. The GC-MS analysis identified 30, 45, and 8 chemical\ncomponents in Kottakkal Ayurveda Triphala (T), Kottakkal Ayurveda Hinguvachadi\nChurnam (H), and Kottakkal Ayurveda Jirakadyarishtam (J), respectively. The\nLC-MS analysis produced 15, 20, and 16 peaks for Kottakkal Ayurveda Triphala\n(T), Kottakkal Ayurveda Hinguvachadi Churnam (H), and Kottakkal Ayurveda\nJirakadyarishtam (J), with m/z values of 982, 981, 972, and 933; 987, 985, 974,\nand 945; and 969, 965, 951, and 941, respectively, confirming their precision.\nMoreover, characterization of the Ayurvedic products was carried out using\nFT-IR, UV-vis, and 1H-NMR spectroscopy to identify significant functional\ngroups and chemical substances. Kottakkal Ayurveda Triphala (T) was evaluated\nfor antibacterial activity against Gram-positive bacteria (Streptococcus\npneumoniae and Staphylococcus aureus) along with Gram-negative bacteria\n(Escherichia coli and Klebsiella pneumoniae), yielding a P value of 0.0650 (P <\n0.0001). Both Kottakkal Ayurveda Hinguvachadi Churnam (H) and Kottakkal\nAyurveda Jirakadyarishtam (J) were subjected to analysis for their\neffectiveness against Aspergillus niger and Aspergillus fumigatus, also\nrevealing a P value within the acceptable range of 0.0650 (P < 0.0001). The\nanti-diabetic properties of Kottakkal Ayurveda Triphala (T) were assessed using\nthe {\\alpha}-glucosidase inhibitory method, which exhibited a significant\ninhibitory effect on {\\alpha}-glucosidase, resulting in an average P value of\n0.001 (P < 0.0001).",
      "generated_abstract": "nt study aimed to investigate the phytochemicals, spectral\nproperties, anticancer, antidiabetic, and antimicrobial activities of\nselected Ayurvedic medicinal plants. A total of 27 Ayurvedic medicinal\nplants were selected for the present study. The phytochemical screening was\ncarried out by using standard procedures, and the anticancer, antidiabetic, and\nantimicrobial activities were determined using the well-established\nstandard-test-methods. The results revealed that the selected plants exhibited\nsignificant anticancer activity, especially in the case of Panax ginseng and\nTinospora crispa. The antidiabetic and antimicrobial activities of the\nremedies were also observed, with the most potent activity being observed for\nTinospora crispa. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15827338129496402,
          "p": 0.3548387096774194,
          "f": 0.21890546837058494
        },
        "rouge-2": {
          "r": 0.019704433497536946,
          "p": 0.046511627906976744,
          "f": 0.02768165671914911
        },
        "rouge-l": {
          "r": 0.14388489208633093,
          "p": 0.3225806451612903,
          "f": 0.19900497085814717
        }
      }
    },
    {
      "paper_id": "cond-mat.supr-con.cond-mat/stat-mech/2503.10146v1",
      "true_abstract": "We find nonequilibrium phase transitions accompanied by multiple (nested)\nhysteresis behaviors in superconductors coupled to baths under a time-periodic\nlight driving.The transitions are demonstrated with a full phase diagram in the\ndomain of the driving amplitude and frequency by means of the Floquet many-body\ntheory. In the weak driving regime with a frequency smaller than half of the\nsuperconducting gap, excited quasiparticles are accumulated at the far edges of\nthe bands, realizing a distribution reminiscent of the Eliashberg effect, which\nsuddenly becomes unstable in the strong driving regime due to\nmulti-photon-assisted tunneling across the gap mediated by the in-gap Floquet\nsidebands. We also show that superconductivity is enhanced in the weak driving\nregime without effective cooling, which is attributed to the modulation of the\nspectrum due to Floquet sidebands.",
      "generated_abstract": "t a theory for nonequilibrium hysteretic phase transitions in\nperiodically light-driven superconductors, where a time-dependent magnetic\nfield drives a phase transition between two superconducting phases. We first\nderive the equations of motion for the superconducting order parameter and the\nmagnetic field, which are coupled via a time-dependent Zeeman energy. The\nequations of motion are solved analytically for the special case of a weak\nmagnetic field, where the superconducting order parameter acquires a\ntime-dependent phase. In the weak-field limit, the equations of motion are\nreduced to the standard BCS-BEC crossover theory, where the phase transition\nis driven by a time-dependent magnetic field. However, we show that, in the\npresence of a strong magnetic field, the phase transition can be driven by the\ntime-dependent",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2682926829268293,
          "p": 0.3333333333333333,
          "f": 0.2972972923557342
        },
        "rouge-2": {
          "r": 0.05217391304347826,
          "p": 0.061224489795918366,
          "f": 0.056338023200864465
        },
        "rouge-l": {
          "r": 0.23170731707317074,
          "p": 0.2878787878787879,
          "f": 0.25675675181519364
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.00326v1",
      "true_abstract": "BART (Bayesian additive regression trees) has been established as a leading\nsupervised learning method, particularly in the field of causal inference. This\npaper explores the use of BART models for learning conditional average\ntreatment effects (CATE) from regression discontinuity designs, where treatment\nassignment is based on whether an observed covariate (called the running\nvariable) exceeds a pre-specified threshold. A purpose-built version of BART\nthat uses linear regression leaf models (of the running variable and treatment\nassignment dummy) is shown to out-perform off-the-shelf BART implementations as\nwell as a local polynomial regression approach and a CART-based approach. The\nnew method is evaluated in thorough simulation studies as well as an empirical\napplication looking at the effect of academic probation on student performance.",
      "generated_abstract": "e a Bayesian additive regression trees (BART) framework for\nestimating the conditional average treatment effect (CATE) in regression\ndiscontinuity (RD) designs. This approach is particularly useful in cases where\nthe treatment and outcome are binary, such as in a RD study of a healthcare\nintervention. The BART model accounts for the confounders that influence both\nthe treatment and the outcome simultaneously, and it provides a Bayesian\nframework for estimating the conditional CATE in RD designs. We show that the\nBART model can be used to estimate the conditional CATE when the binary\noutcome is observed, even when the binary treatment is not observed. We also\nprovide a theoretical bound on the bias and variance of the estimated\nconditional CATE. This bound is obtained using the theory of the BART model,\nand it is shown that the bound holds under mild assumptions. We demonstrate the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3103448275862069,
          "p": 0.3333333333333333,
          "f": 0.32142856643494905
        },
        "rouge-2": {
          "r": 0.043859649122807015,
          "p": 0.03968253968253968,
          "f": 0.041666661679167265
        },
        "rouge-l": {
          "r": 0.26436781609195403,
          "p": 0.2839506172839506,
          "f": 0.27380951881590143
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.18662v1",
      "true_abstract": "The peer-review process is broken and the problem is getting worse,\nespecially in AI: large conferences like NeurIPS increasingly struggle to\nadequately review huge numbers of paper submissions. I propose a scalable\nsolution that, foremost, recognizes reviewing as important, necessary,\n\\emph{work} and rewards it with crypto-coins owned and managed by the\nconferences themselves. The idea is at its core quite simple: paper submissions\nrequire work (reviews, meta-reviews, etc.) to be done, and therefore the\nsubmitter must pay for that work. Each reviewer submits their review to be\napproved by some designated conference officer (e.g. PC chair, Area Chair,\netc.), and upon approval is paid a single coin for a single review. If three\nreviews are required, the cost of submission should be three coins + a tax that\ncovers payments to all the volunteers who organize the conference. After some\none-time startup costs to fairly distribute coins, the process should be\nrelatively stable with new coins minted only when a conference grows.",
      "generated_abstract": "n is a novel peer-to-peer (P2P) payment mechanism that uses\ncoins to compensate reviewers for their efforts in providing feedback on\nproducts. Reviewers are rewarded with ReviewCoins to incentivize them to\nprovide reviews, which is crucial for building a trustworthy marketplace.\nFurthermore, the platform enables reviewers to earn additional ReviewCoins by\nreferring their friends to join the platform. This paper reviews existing\np2p payment mechanisms, including review coins, and presents a detailed\nanalysis of the unique features of ReviewCoin, such as the mechanics of\nrewarding reviewers and the possibility of earning additional ReviewCoins by\nreferring friends to join the platform. Additionally, this paper explores the\nimpact of the COVID-19 pandemic on ReviewCoin, highlighting the challenges\nassociated with scaling the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15833333333333333,
          "p": 0.24050632911392406,
          "f": 0.19095476908158895
        },
        "rouge-2": {
          "r": 0.00625,
          "p": 0.009174311926605505,
          "f": 0.0074349394176452255
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.20253164556962025,
          "f": 0.16080401531274477
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.02118v2",
      "true_abstract": "An increase in availability of Software Defined Radios (SDRs) has caused a\ndramatic shift in the threat landscape of legacy satellite systems, opening\nthem up to easy spoofing attacks by low-budget adversaries. Physical-layer\nauthentication methods can help improve the security of these systems by\nproviding additional validation without modifying the space segment. This paper\nextends previous research on Radio Frequency Fingerprinting (RFF) of satellite\ncommunication to the Orbcomm satellite formation. The GPS and Iridium\nconstellations are already well covered in prior research, but the feasibility\nof transferring techniques to other formations has not yet been examined, and\nraises previously undiscussed challenges.\n  In this paper, we collect a novel dataset containing 8992474 packets from the\nOrbcom satellite constellation using different SDRs and locations. We use this\ndataset to train RFF systems based on convolutional neural networks. We achieve\nan ROC AUC score of 0.53 when distinguishing different satellites within the\nconstellation, and 0.98 when distinguishing legitimate satellites from SDRs in\na spoofing scenario. We also demonstrate the possibility of mixing datasets\nusing different SDRs in different physical locations.",
      "generated_abstract": "s a constellation of low Earth orbit (LEO) satellites with an\nexponential growth trajectory. Their unique frequency and spectral characteristics\npresent unique challenges in satellite communications. This paper presents\nOrbID, a novel technique for identifying and characterizing Orbcomm's unique\nradio frequency (RF) fingerprints. By using spectral analysis, OrbID identifies\nand categorizes Orbcomm satellites based on their signal strength and\ncharacteristics. This information is then used to enhance the system's\nperformance, reducing latency and increasing reliability. The technique is\napplied to Orbcomm's upcoming constellation, Orbital ATK's Cygnus OA-5\nmission, which is scheduled to launch in 2025. By identifying and\ncharacterizing Orbcomm's unique RF fingerprints,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.2564102564102564,
          "f": 0.19230768762019243
        },
        "rouge-2": {
          "r": 0.017241379310344827,
          "p": 0.030612244897959183,
          "f": 0.0220588189197674
        },
        "rouge-l": {
          "r": 0.14615384615384616,
          "p": 0.24358974358974358,
          "f": 0.18269230300480782
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.05915v1",
      "true_abstract": "Multilevel regression and poststratification (MRP) is a computationally\nefficient indirect estimation method that can quickly produce improved\npopulation-adjusted estimates with limited data. Recent computational\nadvancements allow efficient, relatively simple, and quick approximate Bayesian\nestimation for MRP. As population health outcomes of interest including\nvaccination uptake are known to have spatial structure, precision may be gained\nby including space in the model. We test a recently proposed spatial MRP method\nthat includes a BYM2 spatial term that smooths across demographics and\ngeographic areas using a large, unrepresentative survey. We produce California\ncounty-level estimates of first-dose COVID-19 vaccination up to June 2021 using\nclassic and spatial MRP models, and poststratify using data from the American\nCommunity Survey (US Census Bureau). We assess validity using reported\nfirst-dose vaccination counts from the Centers for Disease Control (CDC).\nNeither classic nor spatial MRP models performed well, highlighting: 1. spatial\nMRP may be most appropriate for richer data contexts, 2. some demographics in\nthe survey data are over-sampled and -aggregated, producing model\nover-smoothing, and 3. a need for survey producers to share user-representative\nmetrics to better benchmark estimates.",
      "generated_abstract": "l regression and poststratification (MRP) is an estimator of\nthe average treatment effect (ATT) for a multivariate outcome that is\nproportional to the sum of the individual ATTs. However, MRP can be difficult\nto interpret and can lead to overestimates of the ATT. We propose a new MRP\nestimator, Multilevel MRP, that uses spatial priors to account for spatial\ncorrelation in the outcome and MRP weights. We apply Multilevel MRP to a\nbehavioural survey dataset of over 40,000 individuals living in the UK. The\ndataset includes information on the presence of a condition, which is treated as\na binary outcome. The survey includes a set of questions about the behavioural\ntrajectories of the individuals, and the ATT is estimated for each question.\nWe compare the performance of Multilevel MR",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16129032258064516,
          "p": 0.2631578947368421,
          "f": 0.1999999952880001
        },
        "rouge-2": {
          "r": 0.034482758620689655,
          "p": 0.04878048780487805,
          "f": 0.040404035551474926
        },
        "rouge-l": {
          "r": 0.16129032258064516,
          "p": 0.2631578947368421,
          "f": 0.1999999952880001
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.05758v1",
      "true_abstract": "Lipreading is an important technique for facilitating human-computer\ninteraction in noisy environments. Our previously developed self-supervised\nlearning method, AV2vec, which leverages multimodal self-distillation, has\ndemonstrated promising performance in speaker-independent lipreading on the\nEnglish LRS3 dataset. However, AV2vec faces challenges such as high training\ncosts and a potential scarcity of audio-visual data for lipreading in languages\nother than English, such as Chinese. Additionally, most studies concentrate on\nspeakerindependent lipreading models, which struggle to account for the\nsubstantial variation in speaking styles across di?erent speakers. To address\nthese issues, we propose a comprehensive approach. First, we investigate\ncross-lingual transfer learning, adapting a pre-trained AV2vec model from a\nsource language and optimizing it for the lipreading task in a target language.\nSecond, we enhance the accuracy of lipreading for specific target speakers\nthrough a speaker adaptation strategy, which is not extensively explored in\nprevious research. Third, after analyzing the complementary performance of\nlipreading with lip region-of-interest (ROI) and face inputs, we introduce a\nmodel ensembling strategy that integrates both, signi?cantly boosting model\nperformance. Our method achieved a character error rate (CER) of 77.3% on the\nevaluation set of the ChatCLR dataset, which is lower than the top result from\nthe 2024 Chat-scenario Chinese Lipreading Challenge.",
      "generated_abstract": "g is a crucial task in spoken language understanding. Despite its\nchallenges, it has significant practical applications, such as speech\ninterpretation and speech enhancement. The problem of lipreading in noisy\nenvironments is particularly challenging. While self-distillation has been\nproposed for large language models (LLMs), it has not been applied to lipreading\nmodels. To address this gap, we propose Audio-Visual Self-Distillation\nPretraining and Speaker Adaptation (AVSD-PASA). AVSD-PASA uses self-distillation\nto improve the performance of a lipreading model trained on public speech\ndatasets, while simultaneously adapting the model to the characteristics of\nspeaker-specific datasets. To this end, we propose a two-stage self-distillation\napproach, where the first stage distills the model using a small-scale\ntraining dataset",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1958041958041958,
          "p": 0.345679012345679,
          "f": 0.24999999538305173
        },
        "rouge-2": {
          "r": 0.050761421319796954,
          "p": 0.09174311926605505,
          "f": 0.06535947253769951
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.32098765432098764,
          "f": 0.23214285252590888
        }
      }
    },
    {
      "paper_id": "cs.AI.econ/GN/2502.16879v1",
      "true_abstract": "This paper pioneers a novel approach to economic and public policy analysis\nby leveraging multiple Large Language Models (LLMs) as heterogeneous artificial\neconomic agents. We first evaluate five LLMs' economic decision-making\ncapabilities in solving two-period consumption allocation problems under two\ndistinct scenarios: with explicit utility functions and based on intuitive\nreasoning. While previous research has often simulated heterogeneity by solely\nvarying prompts, our approach harnesses the inherent variations in analytical\ncapabilities across different LLMs to model agents with diverse cognitive\ntraits. Building on these findings, we construct a Multi-LLM-Agent-Based (MLAB)\nframework by mapping these LLMs to specific educational groups and\ncorresponding income brackets. Using interest-income taxation as a case study,\nwe demonstrate how the MLAB framework can simulate policy impacts across\nheterogeneous agents, offering a promising new direction for economic and\npublic policy analysis by leveraging LLMs' human-like reasoning capabilities\nand computational power.",
      "generated_abstract": "uce a multi-agent framework for economics and public policy\nanalysis, designed to integrate large language models (LLMs) with a\ncomputational framework that can efficiently evaluate and compare multiple\npolicy options. Our framework integrates a set of agents with varying\nobjectives and capabilities, each operating in a separate LLM-based environment.\nThese agents can collaborate to jointly evaluate and compare alternative\npolicies, providing insights into the optimal trade-offs among multiple\nobjectives. Our framework can be used to evaluate and compare multiple policy\noptions, and it can be applied to various economic and public policy\ncontexts, such as healthcare, housing, and education. The framework is\ninterpretable and accessible, making it suitable for use in both academic and\npolicy contexts. This work provides a novel framework for economics and\npublic policy analysis, offering a scalable and interpretable solution for\nevaluating and comparing multiple policy options",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20754716981132076,
          "p": 0.2619047619047619,
          "f": 0.23157894243545718
        },
        "rouge-2": {
          "r": 0.05185185185185185,
          "p": 0.05737704918032787,
          "f": 0.05447470318400022
        },
        "rouge-l": {
          "r": 0.18867924528301888,
          "p": 0.23809523809523808,
          "f": 0.2105263108565098
        }
      }
    },
    {
      "paper_id": "cs.IT.math/AC/2503.03421v1",
      "true_abstract": "In this paper, we study the unit graph $ G(\\mathbb{Z}_n) $, where $ n $ is of\nthe form $n = p_1^{n_1} p_2^{n_2} \\dots p_r^{n_r}$, with $ p_1, p_2, \\dots, p_r\n$ being distinct prime numbers and $ n_1, n_2, \\dots, n_r $ being positive\nintegers. We establish the connectivity of $ G(\\mathbb{Z}_n) $, show that its\ndiameter is at most three, and analyze its edge connectivity. Furthermore, we\nconstruct $ q $-ary linear codes from the incidence matrix of $ G(\\mathbb{Z}_n)\n$, explicitly determining their parameters and duals. A primary contribution of\nthis work is the resolution of two conjectures from \\cite{Jain2023} concerning\nthe structural and coding-theoretic properties of $ G(\\mathbb{Z}_n) $. These\nresults extend the study of algebraic graph structures and highlight the\ninterplay between number theory, graph theory, and coding theory.",
      "generated_abstract": "We consider a graph over $\\mathbb{Z}_n$ with no isolated vertices and show\nthat the edge weights can be described by a linear code. In particular, this\ncode can be used to encode a unit graph and hence the underlying graph. We\npresent a method to generate such codes and show how the weights of these codes\ncan be used to encode unit graphs.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16091954022988506,
          "p": 0.35,
          "f": 0.22047243662967328
        },
        "rouge-2": {
          "r": 0.016,
          "p": 0.03571428571428571,
          "f": 0.02209944324043915
        },
        "rouge-l": {
          "r": 0.14942528735632185,
          "p": 0.325,
          "f": 0.20472440513361037
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/DC/2503.09799v1",
      "true_abstract": "As we scale to more massive machine learning models, the frequent\nsynchronization demands inherent in data-parallel approaches create significant\nslowdowns, posing a critical challenge to further scaling. Recent work develops\nan approach (DiLoCo) that relaxes synchronization demands without compromising\nmodel quality. However, these works do not carefully analyze how DiLoCo's\nbehavior changes with model size. In this work, we study the scaling law\nbehavior of DiLoCo when training LLMs under a fixed compute budget. We focus on\nhow algorithmic factors, including number of model replicas, hyperparameters,\nand token budget affect training in ways that can be accurately predicted via\nscaling laws. We find that DiLoCo scales both predictably and robustly with\nmodel size. When well-tuned, DiLoCo scales better than data-parallel training\nwith model size, and can outperform data-parallel training even at small model\nsizes. Our results showcase a more general set of benefits of DiLoCo than\npreviously documented, including increased optimal batch sizes, improved\ndownstream generalization with scale, and improved evaluation loss for a fixed\ntoken budget.",
      "generated_abstract": "aper, we establish a new scalability law for DiLoCo, a novel\nscalable and reliable language modeling framework that scales reliably to\nmillions of tokens, achieving state-of-the-art performance on benchmarks such\nas GLUE and WMT. DiLoCo is the first language modeling framework to fully\noptimize for memory efficiency by leveraging a novel memory-efficient\nmemory-efficient framework, which we call DiMem, and we present a detailed\nanalysis of DiMem's memory efficiency. DiLoCo also achieves a new state-of-the-art\nperformance on the WMT-19 benchmark. We also provide the first scalability law\nfor DiLoCo, which demonstrates that DiLoCo scales reliably to millions of\ntokens, achieving state-of-the-art performance on benchmarks such as GLUE and\nWMT. DiLoCo is the first language",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11016949152542373,
          "p": 0.22413793103448276,
          "f": 0.1477272683083679
        },
        "rouge-2": {
          "r": 0.012738853503184714,
          "p": 0.024691358024691357,
          "f": 0.01680671819892783
        },
        "rouge-l": {
          "r": 0.11016949152542373,
          "p": 0.22413793103448276,
          "f": 0.1477272683083679
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.11462v1",
      "true_abstract": "Deep learning based end-to-end multi-channel speech enhancement methods have\nachieved impressive performance by leveraging sub-band, cross-band, and spatial\ninformation. However, these methods often demand substantial computational\nresources, limiting their practicality on terminal devices. This paper presents\na lightweight multi-channel speech enhancement network with decoupled fully\nconnected attention (LMFCA-Net). The proposed LMFCA-Net introduces time-axis\ndecoupled fully-connected attention (T-FCA) and frequency-axis decoupled\nfully-connected attention (F-FCA) mechanisms to effectively capture long-range\nnarrow-band and cross-band information without recurrent units. Experimental\nresults show that LMFCA-Net performs comparably to state-of-the-art methods\nwhile significantly reducing computational complexity and latency, making it a\npromising solution for practical applications.",
      "generated_abstract": "nnel speech enhancement (MSE) is a fundamental task in speech\nconversation, where speech signals from multiple speakers are combined into a\nsingle high-quality output signal. In this paper, we propose LMFCA-Net, a\nlightweight model for multi-channel speech enhancement with efficient narrow-\nband and cross-band attention. The proposed model is composed of a narrow-band\nattention module and a cross-band attention module, enabling the model to\nperform both narrow-band and cross-band processing. Furthermore, a novel\nattention-based cross-channel fusion method is proposed to enhance the\nspeech enhancement performance. The proposed model is trained with a loss\nfunction based on the mean square error (MSE) between the input signal and the\noutput signal. The experimental results on the ASVspoof2022 and ESC-50\ndatasets demonstrate that LM",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23170731707317074,
          "p": 0.24358974358974358,
          "f": 0.23749999500312513
        },
        "rouge-2": {
          "r": 0.06315789473684211,
          "p": 0.05405405405405406,
          "f": 0.05825242221462952
        },
        "rouge-l": {
          "r": 0.21951219512195122,
          "p": 0.23076923076923078,
          "f": 0.22499999500312512
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.hep-ex/2503.09420v1",
      "true_abstract": "Diamond's exceptional properties make it highly suited for applications in\nchallenging radiation environments. Understanding radiation-induced damage in\ndiamond is crucial for enabling its practical applications and advancing\nmaterials science. However, direct imaging of radiation-induced crystal defects\nat the atomic scale remains rare due to diamond's compact lattice structure.\nHere, we report the atomic-level characterization of crystal defects induced by\nhigh-flux fast neutron radiation (up to $3 \\times10^{17}$ n/$cm^2$) in\nsingle-crystal chemical vapor deposition diamonds. Through Raman spectroscopy,\nthe phase transition from carbon $sp^3$ to $sp^2$ hybridization was identified,\nprimarily associated with the formation of dumbbell-shaped interstitial\ndefects. Using electron energy loss spectroscopy and aberration-corrected\ntransmission electron microscopy, we observed a clustering trend in defect\ndistribution, where $sp^2$ rich clusters manifested as dislocation structures\nwith a density up to $10^{14}$ $cm^{-2}$. Lomer-Cottrell junctions were\nidentified, offering a possible explanation for defect cluster formation.\nRadiation-induced point defects were found to be dispersed throughout the\ndiamond lattice, highlighting the widespread nature of primary defect\nformation. Vacancy defects, along with $\\langle 111 \\rangle$ and $\\langle 100\n\\rangle$ oriented dumbbell-shaped interstitial defects induced by high-dose\nneutron irradiation, were directly imaged, providing microscopic structural\nevidence that complements spectroscopic studies of point defects. Dynamical\nsimulations combined with an adiabatic recombination-based damage model\nprovided insights into the correlation between irradiation dose and resulting\ncrystal damage. These findings advance our understanding of neutron-induced\ndamage mechanisms in diamond and contribute to the development of\nradiation-resistant diamond materials.",
      "generated_abstract": "of radiation damage in chemical vapor deposition (CVD) diamond\n(CVD-D) is explored in detail using neutron diffraction and electron\nmicroscopy. The defects formed by electron irradiation are investigated in\ndetail. The defects produced by ionizing radiation can be classified into\ndendrites, which are the most common defects in CVD-D, and the more\nuncommon, but important, spheroidal defects. The defects produced by neutron\nirradiation are not observed in CVD-D. The spheroidal defects produced by\nneutron irradiation are the result of the breakage of the atomic lattice of\nCVD-D during the neutron irradiation. The defects produced by electron\nirradiation are the result of the breakage of the atomic lattice of CVD-D. The\ndendrites",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1242603550295858,
          "p": 0.40384615384615385,
          "f": 0.19004524527016242
        },
        "rouge-2": {
          "r": 0.017467248908296942,
          "p": 0.05194805194805195,
          "f": 0.026143787083387278
        },
        "rouge-l": {
          "r": 0.1242603550295858,
          "p": 0.40384615384615385,
          "f": 0.19004524527016242
        }
      }
    },
    {
      "paper_id": "cs.DL.stat/OT/2405.20156v1",
      "true_abstract": "This paper seeks to bridge the gap between archival text analysis and network\nanalysis by applying network clustering methods to analyze the coverage of\nBulgaria in 123 issues of the newspaper Osservatore Romano published between\nJanuary and May 1877. Utilizing optical character recognition and generalized\nhomogeneity blockmodeling, the study constructs networks of relevant keywords.\nThose including the sets Bulgaria and Russia are rather isomorphic and they\nlargely overlap with those for Germany, Britain, and War. In structural terms,\nthe blockmodel of the two networks exhibits a clear\ncore-semiperiphery-periphery structure that reflects relations between concepts\nin the newpaper's coverage. The newspaper's lexical choices effectively\ndelegitimised the Bulgarian national revival, highlighting the influence of the\nHoly See on the newspaper's editorial line.",
      "generated_abstract": "val text analysis of the journal Osservatore Romano, published by the\nbishops of the Catholic Church in Italy, is a unique dataset that provides a\nunique insight into the historical and political context of the Church in the\nfirst half of the twentieth century. The purpose of this paper is to present\nthe blockmodeling approach applied to the Osservatore Romano dataset, aimed at\nscaling up the analysis of historical texts. We focus on the analysis of the\n1877-1912 period, focusing on the relationship between the archival texts and\nthe political, social, and cultural context of the time. We propose a\ncomputational framework for blockmodeling that allows to model the interplay\nbetween textual data and the contextual information, and that provides a\nrobust and scalable solution for large-scale text analysis. We demonstrate the\nefficiency of our approach through several case studies",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2111111111111111,
          "p": 0.23170731707317074,
          "f": 0.2209302275689563
        },
        "rouge-2": {
          "r": 0.042735042735042736,
          "p": 0.04065040650406504,
          "f": 0.04166666166979227
        },
        "rouge-l": {
          "r": 0.18888888888888888,
          "p": 0.2073170731707317,
          "f": 0.19767441361546798
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/SC/2312.05876v3",
      "true_abstract": "Some proteins can find their targets on DNA faster than by pure diffusion in\nthe three-dimensional cytoplasm, through the process of facilitated diffusion:\nThey can loosely bind to DNA and temporarily slide along it, thus being guided\nby the DNA molecule itself to the target. This chapter examines this process in\nmathematical detail with a focus on including the effect of DNA coiling on the\nsearch process.",
      "generated_abstract": "to develop an efficient tool for targeting nucleic acids, we\ntarget the DNA molecule, which is the genetic material of all living organisms.\nThis molecule is known to be very flexible and is able to fold itself into many\ndifferent shapes. The search of a specific sequence within a DNA molecule is\ncalled target search. This paper presents a novel approach to target search\nbased on coiling, where the DNA is folded into a series of coils and a target\nsequence is located by finding the closest coil to the target sequence. We\npropose an efficient and effective algorithm to search for a target sequence in\na DNA molecule. This algorithm is based on a coiling process. This process\ninvolves the coiling of DNA, which makes the DNA flexible and allows us to\nfind the closest coil to a target sequence. The proposed algorithm is compared\nwith several other target search",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.36,
          "p": 0.23376623376623376,
          "f": 0.2834645621551244
        },
        "rouge-2": {
          "r": 0.06060606060606061,
          "p": 0.030534351145038167,
          "f": 0.04060913260017056
        },
        "rouge-l": {
          "r": 0.32,
          "p": 0.2077922077922078,
          "f": 0.2519684991629984
        }
      }
    },
    {
      "paper_id": "cs.AR.eess/SY/2503.04581v1",
      "true_abstract": "Most Wearable Ultrasound (WUS) devices lack the computational power to\nprocess signals at the edge, instead relying on remote offload, which\nintroduces latency, high power consumption, and privacy concerns. We present\nMaestro, a RISC-V SoC with unified Vector-Tensor Unit (VTU) and memory-coupled\nFast Fourier Transform (FFT) accelerators targeting edge processing for\nwearable ultrasound devices, fabricated using low-cost TSMC 65nm CMOS\ntechnology. The VTU achieves peak 302GFLOPS/W and 19.8GFLOPS at FP16, while the\nmulti-precision 16/32-bit floating-point FFT accelerator delivers peak\n60.6GFLOPS/W and 3.6GFLOPS at FP16, We evaluate Maestro on a US-based gesture\nrecognition task, achieving 1.62GFLOPS in signal processing at 26.68GFLOPS/W,\nand 19.52GFLOPS in Convolutional Neural Network (CNN) workloads at\n298.03GFLOPS/W. Compared to a state-of-the-art SoC with a similar mission\nprofile, Maestro achieves a 5x speedup while consuming only 12mW, with an\nenergy consumption of 2.5mJ in a wearable US channel preprocessing and ML-based\npostprocessing pipeline.",
      "generated_abstract": "ultrasound (WUS) is an emerging medical technology with significant\ndifferences from traditional ultrasound systems. While traditional ultrasound\nsystems focus on the diagnostic or therapeutic aspects of WUS, the hardware\nrequirements are different. In this paper, we propose Maestro, a 302 GFLOPS/W\nand 19.8GFLOPS/W RISC-V Vector-Tensor Architecture (VTA) for WUS. Maestro\noffers 302 GFLOPS/W for image processing, 19.8 GFLOPS/W for inference, and 12.9\nGFLOPS/W for vector-tensor matrix multiplication. This architecture offers\ncompetitive performance and energy efficiency compared to existing WUS\narchitectures. Maestro also offers a flexible and energy-efficient VTA\narchitecture",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15,
          "p": 0.2571428571428571,
          "f": 0.18947367955678682
        },
        "rouge-2": {
          "r": 0.013422818791946308,
          "p": 0.023255813953488372,
          "f": 0.017021271955094064
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.22857142857142856,
          "f": 0.16842104797783947
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ME/2503.09494v2",
      "true_abstract": "In the era of big data, large-scale, multi-modal datasets are increasingly\nubiquitous, offering unprecedented opportunities for predictive modeling and\nscientific discovery. However, these datasets often exhibit complex\nheterogeneity, such as covariate shift, posterior drift, and missing\nmodalities, that can hinder the accuracy of existing prediction algorithms. To\naddress these challenges, we propose a novel Representation Retrieval ($R^2$)\nframework, which integrates a representation learning module (the representer)\nwith a sparsity-induced machine learning model (the learner). Moreover, we\nintroduce the notion of \"integrativeness\" for representers, characterized by\nthe effective data sources used in learning representers, and propose a\nSelective Integration Penalty (SIP) to explicitly improve the property.\nTheoretically, we demonstrate that the $R^2$ framework relaxes the conventional\nfull-sharing assumption in multi-task learning, allowing for partially shared\nstructures, and that SIP can improve the convergence rate of the excess risk\nbound. Extensive simulation studies validate the empirical performance of our\nframework, and applications to two real-world datasets further confirm its\nsuperiority over existing approaches.",
      "generated_abstract": "Representation learning has been widely used in heterogeneous data integration\n(HDI), yet its effectiveness in integrating heterogeneous data remains\nlimited. In this paper, we propose a novel heterogeneous data integration\nframework based on representation retrieval learning. Specifically, we design a\ntwo-stage retrieval-driven integration framework, and propose a novel\nrepresentation retrieval method to select relevant representations for each\ndata. In the first stage, the retrieved representations are used to refine the\noriginal data. Then, in the second stage, these refined data are used to\nintegrate heterogeneous data. Experiments on the UCI Repository and TREC 2024\nHDI dataset demonstrate that the proposed framework outperforms existing\nmethods in both accuracy and computational efficiency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19834710743801653,
          "p": 0.3380281690140845,
          "f": 0.24999999533908426
        },
        "rouge-2": {
          "r": 0.056962025316455694,
          "p": 0.09,
          "f": 0.06976743711315458
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.30985915492957744,
          "f": 0.22916666200575095
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.07934v1",
      "true_abstract": "Counterfactual explanations indicate the smallest change in input that can\ntranslate to a different outcome for a machine learning model. Counterfactuals\nhave generated immense interest in high-stakes applications such as finance,\neducation, hiring, etc. In several use-cases, the decision-making process often\nrelies on an ensemble of models rather than just one. Despite significant\nresearch on counterfactuals for one model, the problem of generating a single\ncounterfactual explanation for an ensemble of models has received limited\ninterest. Each individual model might lead to a different counterfactual,\nwhereas trying to find a counterfactual accepted by all models might\nsignificantly increase cost (effort). We propose a novel strategy to find the\ncounterfactual for an ensemble of models using the perspective of entropic risk\nmeasure. Entropic risk is a convex risk measure that satisfies several\ndesirable properties. We incorporate our proposed risk measure into a novel\nconstrained optimization to generate counterfactuals for ensembles that stay\nvalid for several models. The main significance of our measure is that it\nprovides a knob that allows for the generation of counterfactuals that stay\nvalid under an adjustable fraction of the models. We also show that a limiting\ncase of our entropic-risk-based strategy yields a counterfactual valid for all\nmodels in the ensemble (worst-case min-max approach). We study the trade-off\nbetween the cost (effort) for the counterfactual and its validity for an\nensemble by varying degrees of risk aversion, as determined by our risk\nparameter knob. We validate our performance on real-world datasets.",
      "generated_abstract": "We consider the problem of generating counterfactual explanations for\nmodel ensembles, which is a key challenge for ensembling methods that\nincorporate model uncertainty. A common approach is to use the entropic\nrisk measures, which can be interpreted as an extension of the Kullback-Leibler\ndivergence. However, existing entropic risk measures are not directly\napplicable to ensembles due to the additional uncertainty in the ensemble. To\naddress this, we propose a novel entropic risk measure that can be used with\nensembles, called the ensemble entropic risk measure. We evaluate the\nperformance of the proposed entropic risk measure on synthetic data and\nclassification tasks. The results demonstrate that the proposed entropic risk\nmeasure significantly outperforms the existing entropic risk measures and the\nKullback-Leibler divergence.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22794117647058823,
          "p": 0.4246575342465753,
          "f": 0.29665071315766584
        },
        "rouge-2": {
          "r": 0.0593607305936073,
          "p": 0.12745098039215685,
          "f": 0.08099688039945287
        },
        "rouge-l": {
          "r": 0.20588235294117646,
          "p": 0.3835616438356164,
          "f": 0.26794257918637404
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.04941v2",
      "true_abstract": "Assessing the economic impacts of artificial intelligence requires\nintegrating insights from both computer science and economics. We present the\nGrowth and AI Transition Endogenous model (GATE), a dynamic integrated\nassessment model that simulates the economic effects of AI automation. GATE\ncombines three key ingredients that have not been brought together in previous\nwork: (1) a compute-based model of AI development, (2) an AI automation\nframework, and (3) a semi-endogenous growth model featuring endogenous\ninvestment and adjustment costs. The model allows users to simulate the\neconomic effects of the transition to advanced AI across a range of potential\nscenarios. GATE captures the interactions between economic variables, including\ninvestment, automation, innovation, and growth, as well as AI-related inputs\nsuch as compute and algorithms. This paper explains the model's structure and\nfunctionality, emphasizing AI development for economists and economic modeling\nfor the AI community. The model is implemented in an interactive sandbox,\nenabling users to explore the impact of AI under different parameter choices\nand policy interventions. The modeling sandbox is available at:\nwww.epoch.ai/GATE.",
      "generated_abstract": "l intelligence (AI) and automation are rapidly transforming\nthe world economy, raising critical questions about the potential impacts on\njobs and the economy. However, existing models lack a comprehensive framework\nthat integrates economic, technological, and social factors, making it\ndifficult to understand how these forces interact. This paper presents GATE,\nan integrated assessment model that integrates economic, technological, and\nsocial factors to analyze the impacts of AI on employment, wages, and\nincome inequality. By considering the interplay between these factors, GATE\nprovides a more nuanced understanding of the relationship between AI and the\neconomy. The model is built on a three-level framework that incorporates\ntechnology, economic, and social factors, enabling more precise predictions of\nAI's impacts on employment, wages, and income inequality. The model's\ninteractive features",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20512820512820512,
          "p": 0.2962962962962963,
          "f": 0.24242423758953177
        },
        "rouge-2": {
          "r": 0.05521472392638037,
          "p": 0.08256880733944955,
          "f": 0.06617646578530527
        },
        "rouge-l": {
          "r": 0.18803418803418803,
          "p": 0.2716049382716049,
          "f": 0.22222221738751158
        }
      }
    },
    {
      "paper_id": "math.FA.math/FA/2503.08590v1",
      "true_abstract": "In this paper we study the essential spectra of the Toeplitz operator on the\nHardy space $H^1$. We give a counterexample to show that the Toeplitz operator\nwith symbol is not Fredholm, which gives a counterexample to the conjecture by\nJ.A. Virtanen J A in 2006.",
      "generated_abstract": "In this paper, we show that the Fredholm of Toeplitz operator is a\ncharacteristic of the Hilbert space, and it cannot be replaced by the\nHermitian operator. For this reason, we give an example that the Fredholm\noperator is not a characteristic of the Hilbert space. Moreover, we give a\ncounterexample that the Hermitian operator is a characteristic of the Hilbert\nspace.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.4444444444444444,
          "p": 0.5,
          "f": 0.4705882303114187
        },
        "rouge-2": {
          "r": 0.1951219512195122,
          "p": 0.18604651162790697,
          "f": 0.1904761854790251
        },
        "rouge-l": {
          "r": 0.4444444444444444,
          "p": 0.5,
          "f": 0.4705882303114187
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.07940v1",
      "true_abstract": "Recent advances in deep learning-based point cloud registration have improved\ngeneralization, yet most methods still require retraining or manual parameter\ntuning for each new environment. In this paper, we identify three key factors\nlimiting generalization: (a) reliance on environment-specific voxel size and\nsearch radius, (b) poor out-of-domain robustness of learning-based keypoint\ndetectors, and (c) raw coordinate usage, which exacerbates scale discrepancies.\nTo address these issues, we present a zero-shot registration pipeline called\nBUFFER-X by (a) adaptively determining voxel size/search radii, (b) using\nfarthest point sampling to bypass learned detectors, and (c) leveraging\npatch-wise scale normalization for consistent coordinate bounds. In particular,\nwe present a multi-scale patch-based descriptor generation and a hierarchical\ninlier search across scales to improve robustness in diverse scenes. We also\npropose a novel generalizability benchmark using 11 datasets that cover various\nindoor/outdoor scenarios and sensor modalities, demonstrating that BUFFER-X\nachieves substantial generalization without prior information or manual\nparameter tuning for the test datasets. Our code is available at\nhttps://github.com/MIT-SPARK/BUFFER-X.",
      "generated_abstract": "ud (PC) registration is crucial for precise 3D object alignment and\npoint-based reconstruction. Recent advancements in zero-shot learning (ZSL)\nhave shown promising performance in addressing this challenge, yet existing\nmethods often rely on large-scale supervised datasets and pre-trained models\nthat often suffer from data bias and model bias. To address these limitations,\nthis work introduces BUFFER-X, a novel ZSL-based framework that achieves\nzero-shot registration in diverse scenes. Specifically, we introduce a novel\nBidirectional Feature Transformer (BFT) that effectively captures semantic\ninformation from both the point cloud and the semantic maps, enabling\ncross-modal reasoning and adaptive feature fusion. Additionally, we introduce\na novel Zero-shot Prototype Registration (ZPR) task to further improve the\ngeneralization capability of the BFT. Extensive experiments",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2421875,
          "p": 0.32978723404255317,
          "f": 0.27927927439655875
        },
        "rouge-2": {
          "r": 0.045454545454545456,
          "p": 0.061946902654867256,
          "f": 0.052434452046739785
        },
        "rouge-l": {
          "r": 0.2421875,
          "p": 0.32978723404255317,
          "f": 0.27927927439655875
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PR/2409.06289v1",
      "true_abstract": "Despite significant progress in deep learning for financial trading, existing\nmodels often face instability and high uncertainty, hindering their practical\napplication. Leveraging advancements in Large Language Models (LLMs) and\nmulti-agent architectures, we propose a novel framework for quantitative stock\ninvestment in portfolio management and alpha mining. Our framework addresses\nthese issues by integrating LLMs to generate diversified alphas and employing a\nmulti-agent approach to dynamically evaluate market conditions. This paper\nproposes a framework where large language models (LLMs) mine alpha factors from\nmultimodal financial data, ensuring a comprehensive understanding of market\ndynamics. The first module extracts predictive signals by integrating numerical\ndata, research papers, and visual charts. The second module uses ensemble\nlearning to construct a diverse pool of trading agents with varying risk\npreferences, enhancing strategy performance through a broader market analysis.\nIn the third module, a dynamic weight-gating mechanism selects and assigns\nweights to the most relevant agents based on real-time market conditions,\nenabling the creation of an adaptive and context-aware composite alpha formula.\nExtensive experiments on the Chinese stock markets demonstrate that this\nframework significantly outperforms state-of-the-art baselines across multiple\nfinancial metrics. The results underscore the efficacy of combining\nLLM-generated alphas with a multi-agent architecture to achieve superior\ntrading performance and stability. This work highlights the potential of\nAI-driven approaches in enhancing quantitative investment strategies and sets a\nnew benchmark for integrating advanced machine learning techniques in financial\ntrading can also be applied on diverse markets.",
      "generated_abstract": "t a proof of concept for the integration of LLM-based automated\nmachine learning (ML) strategies into Quantitative investment workflows.\nAutomated trading platforms have been rapidly adopted by investors,\ndemocratizing access to trading tools for all. The ability to integrate\nLLM-based automated trading strategies with Quant platforms, however, has\nremained a critical gap. This work presents a proof of concept for the\nintegration of LLM-based automated machine learning (ML) strategies into\nQuantitative investment workflows. Automated trading platforms have been\nrapidly adopted by investors, democratizing access to trading tools for all.\nThe ability to integrate LLM-based automated trading strategies with Quant\nplatforms, however, has remained a critical gap. This work presents a proof\nof concept for the integration of LLM-based automated machine learning\nstrateg",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09090909090909091,
          "p": 0.3191489361702128,
          "f": 0.14150943051130305
        },
        "rouge-2": {
          "r": 0.00847457627118644,
          "p": 0.03636363636363636,
          "f": 0.013745701401731888
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.3191489361702128,
          "f": 0.14150943051130305
        }
      }
    },
    {
      "paper_id": "q-bio.SC.q-bio/SC/2308.13877v2",
      "true_abstract": "In the modern world, technology is at its peak. Different avenues in\nprogramming and technology have been explored for data analysis, automation,\nand robotics. Machine learning is key to optimize data analysis, make accurate\npredictions, and hasten/improve existing functions. Thus, presently, the field\nof machine learning in artificial intelligence is being developed and its uses\nin varying fields are being explored. One field in which its uses stand out is\nthat of microbial biosynthesis. In this paper, a comprehensive overview of the\ndiffering machine learning programs used in biosynthesis is provided, alongside\nbrief descriptions of the fields of machine learning and microbial biosynthesis\nseparately. This information includes past trends, modern developments, future\nimprovements, explanations of processes, and current problems they face. Thus,\nthis paper's main contribution is to distill developments in, and provide a\nholistic explanation of, 2 key fields and their applicability to improve\nindustry/research. It also highlights challenges and research directions,\nacting to instigate more research and development in the growing fields.\nFinally, the paper aims to act as a reference for academics performing\nresearch, industry professionals improving their processes, and students\nlooking to understand the concept of machine learning in biosynthesis.",
      "generated_abstract": "biosynthetic pathways are crucial for the production of many\napplications in the biotechnology and pharmaceutical industries. These\npathways are highly complex, and their optimization is challenging. Machine\nlearning (ML) can be used to enhance the efficiency and performance of\nmicrobial biosynthesis, by providing a deeper understanding of the system and\nits dynamics. ML techniques can be applied to various steps of the biosynthetic\nprocess, including pre-processing, substrate selection, and reaction\noptimization. In this review, we explore the current state-of-art techniques\nused in the biosynthetic optimization, focusing on the application of machine\nlearning in these fields. We discuss the advantages and limitations of each\ntechnique, and provide a comprehensive overview of the current state of\nart. We conclude by discussing the potential of ML in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1885245901639344,
          "p": 0.2839506172839506,
          "f": 0.22660098042563528
        },
        "rouge-2": {
          "r": 0.08839779005524862,
          "p": 0.13793103448275862,
          "f": 0.10774410298359599
        },
        "rouge-l": {
          "r": 0.1885245901639344,
          "p": 0.2839506172839506,
          "f": 0.22660098042563528
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2502.19213v1",
      "true_abstract": "We consider an optimal investment-consumption problem for a\nutility-maximizing investor who has access to assets with different liquidity\nand whose consumption rate as well as terminal wealth are subject to\nlower-bound constraints. Assuming utility functions that satisfy standard\nconditions, we develop a methodology for deriving the optimal strategies in\nsemi-closed form. Our methodology is based on the generalized martingale\napproach and the decomposition of the problem into subproblems. We illustrate\nour approach by deriving explicit formulas for agents with power-utility\nfunctions and discuss potential extensions of the proposed framework.",
      "generated_abstract": "-liability management (ALM) framework is a central component of\nfundamental financial modeling. Traditional ALM frameworks focus on the\nfundamental analysis of companies and their assets, with fixed-term securities\nbeing a typical example. However, for companies that are in the early stage of\ndevelopment, their assets and liabilities are likely to be short-term in nature,\nand they may be unable to generate sufficient profits from their fixed-term\nsecurities. In this paper, we propose a novel fixed-term ALM framework that\nfocuses on the fixed-term securities of early-stage companies. The framework\nincludes two steps: (1) the determination of the optimal fixed-term securities\nunder the ALM framework and (2) the adjustment of the optimal fixed-term\nsecurities in the market. The first step is based on the fixed-term ALM model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2463768115942029,
          "p": 0.22972972972972974,
          "f": 0.23776223276835065
        },
        "rouge-2": {
          "r": 0.05747126436781609,
          "p": 0.04504504504504504,
          "f": 0.050505045578512875
        },
        "rouge-l": {
          "r": 0.21739130434782608,
          "p": 0.20270270270270271,
          "f": 0.20979020479632268
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2412.08342v1",
      "true_abstract": "We consider an economic environment where a seller wants to sell an\nindivisible unit of good to a buyer. We show that revenue from any\nstrategy-proof and individually rational mechanism defined on closed intervals\nof rich single crossing domains considered in \\citep{Goswami1}, can be\napproximated by the revenue from a sequence of strategy-proof and individually\nrational mechanisms with finite range. Thus while studying optimal mechanisms\nwithout loss of generality we can study mechanisms with finite range.",
      "generated_abstract": "We consider the problem of maximizing the total revenue of a finite range\nmechanism over a set of possible choices. We show that the problem is NP-hard\nand present a polynomial-time approximation scheme for this problem. We then\nprove that a finite range mechanism achieves the optimal revenue if and only if\nit is an approximate revenue mechanism. We also present a polynomial-time\napproximation scheme for approximate revenue mechanisms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26785714285714285,
          "p": 0.39473684210526316,
          "f": 0.31914893135355366
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.06896551724137931,
          "f": 0.06349205852355796
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.3684210526315789,
          "f": 0.29787233560887283
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.cond-mat/str-el/2503.10330v1",
      "true_abstract": "Motivated by the appearance of Majorana fermions in a broad range of\ncorrelated and topological electronic systems, we develop a general method to\ncompute the dynamical response of interacting Majorana fermions in the\nrandom-phase approximation (RPA). This can be applied self-consistently on top\nof Majorana mean-field theory (MFT) backgrounds, thereby in particular\nproviding a powerful tool to analyse $\\textit{generic}$ behaviour in the\nvicinity of (various heavily studied) exactly soluble models. Prime examples\nare quantum spin liquids (QSL) with emergent Majorana excitations, with the\ncelebrated exact solution of Kitaev. We employ the RPA to study in considerable\ndetail phase structure and dynamics of the extended Kitaev honeycomb\n$KJ\\Gamma$-model, with and without an applied field. First, we benchmark our\nmethod with Kitaev's exactly soluble model, finding a remarkable agreement. The\ninteractions between Majorana fermions even turn out to mimic the effect of\nlocal $\\mathbb{Z}_2$ flux excitations, which we explain analytically. Second,\nwe show how small non-Kitaev couplings $J$ and $\\Gamma$ induce Majorana bound\nstates, resulting in sharp features in the dynamical structure factor in the\npresence of fractionalisation: such 'spinon excitons' naturally appear, and can\ncoexist and interact with the broad Majorana continuum. Third, for increasing\ncouplings or field, our theory predicts instabilities of the KQSL triggered by\nthe condensation of the sharp modes. From the high symmetry momenta of the\ncondensation we can deduce which magnetically ordered phases surround the KQSL,\nin good agreement with previous finite-size numerics. We discuss implications\nfor experiments and the broad range of applicability of our method to other QSL\nand Majorana systems.",
      "generated_abstract": "t a dynamical response theory for interacting Majorana fermions\nin the context of a Kitaev model in a magnetic field. We employ a\nphenomenological approach based on the renormalization group theory and\ninvestigate the dynamical properties of the system in the strong-coupling limit\nand beyond. We obtain a self-consistent field equation for the effective\nLagrange multiplier field, which can be used to determine the critical\nfield strength, $H_c$. We also investigate the dependence of the critical\nfield strength on the strength of the magnetic field and the interaction\nstrength. Furthermore, we use our theory to analyze the Kitaev spin liquid\n(KSL) in the presence of a magnetic field. We find that the KSL phase\ntransitions to a chiral state and a chiral superconductor occur at different\nvalues of the magnetic field, and the critical field strengths are different\nfrom",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1728395061728395,
          "p": 0.3684210526315789,
          "f": 0.2352941132999083
        },
        "rouge-2": {
          "r": 0.059322033898305086,
          "p": 0.11764705882352941,
          "f": 0.0788732349797265
        },
        "rouge-l": {
          "r": 0.16049382716049382,
          "p": 0.34210526315789475,
          "f": 0.2184873906108326
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/CP/2501.05975v1",
      "true_abstract": "In energy markets, joint historical and implied calibration is of paramount\nimportance for practitioners yet notoriously challenging due to the need to\nalign historical correlations of futures contracts with implied volatility\nsmiles from the option market. We address this crucial problem with a\nparsimonious multiplicative multi-factor Heath-Jarrow-Morton (HJM) model for\nforward curves, combined with a stochastic volatility factor coming from the\nLifted Heston model. We develop a sequential fast calibration procedure\nleveraging the Kemna-Vorst approximation of futures contracts: (i) historical\ncorrelations and the Variance Swap (VS) volatility term structure are captured\nthrough Level, Slope, and Curvature factors, (ii) the VS volatility term\nstructure can then be corrected for a perfect match via a fixed-point\nalgorithm, (iii) implied volatility smiles are calibrated using Fourier-based\ntechniques. Our model displays remarkable joint historical and implied\ncalibration fits - to both German power and TTF gas markets - and enables\nrealistic interpolation within the implied volatility hypercube.",
      "generated_abstract": "We study the joint historical and implied calibration of the Heath-Jarrow-Morton\n(HJM) model and the lifted Heston model in the context of energy markets.\nSpecifically, we identify a set of assumptions on the dynamics of the\nHeston-based model that ensures joint historical and implied calibration. The\nresults show that the joint historical and implied calibration of the Heston\nmodel is incompatible with the lifted Heston model and is consistent with the\nHJM model only if the Heston-based model dynamics are highly non-linear.\nFinally, we propose an approximation scheme for the Heston-based model that\nmaintains joint historical and implied calibration.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19230769230769232,
          "p": 0.40816326530612246,
          "f": 0.2614379041428511
        },
        "rouge-2": {
          "r": 0.05755395683453238,
          "p": 0.11267605633802817,
          "f": 0.07619047171473951
        },
        "rouge-l": {
          "r": 0.16346153846153846,
          "p": 0.3469387755102041,
          "f": 0.2222222178683413
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10472v1",
      "true_abstract": "In this letter, we propose to deploy rotatable antennas (RAs) at the base\nstation (BS) to enhance both communication and sensing (C&S) performances, by\nexploiting a new spatial degree-of-freedom (DoF) offered by array rotation.\nSpecifically, we formulate a multi-objective optimization problem to\nsimultaneously maximize the sum-rate of multiple communication users and\nminimize the Cram\\'er-Rao bound (CRB) for target angle estimation, by jointly\noptimizing the transmit beamforming vectors and the array rotation angle at the\nBS. To solve this problem, we first equivalently decompose it into two\nsubproblems, corresponding to an inner problem for beamforming optimization and\nan outer problem for array rotation optimization. Although these two\nsubproblems are non-convex, we obtain their high-quality solutions by applying\nthe block coordinate descent (BCD) technique and one-dimensional exhaustive\nsearch, respectively. Moreover, we show that for the communication-only case,\nRAs provide an additional rotation gain to improve communication performance;\nwhile for the sensing-only case, the equivalent spatial aperture can be\nenlarged by RAs for achieving higher sensing accuracy. Finally, numerical\nresults are presented to showcase the performance gains of RAs over\nfixed-rotation antennas in integrated sensing and communications (ISAC).",
      "generated_abstract": "aper, we propose a novel antenna system that can be rotated around the\nmechanical axes to achieve the desired spatial beamforming direction. The\nproposed system has two main contributions: (1) a novel antenna array with\nflexible beamforming direction, and (2) a novel method for designing the\ntransmit and receive arrays. The proposed system consists of a rotatable\nantennas array, which can be used for both sensing and communication. The\ntransmit array consists of a single element and can be used for both\nsensing and communication. The receive array consists of two elements, one\nelement for sensing and the other for communication. The proposed system\nachieves a 100\\% sensing capability with a 360\\degree\\ beamforming direction\nand a 100\\% communication capability with a 90\\degree\\ beamforming direction.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1693548387096774,
          "p": 0.35,
          "f": 0.2282608651701324
        },
        "rouge-2": {
          "r": 0.0335195530726257,
          "p": 0.0625,
          "f": 0.04363635909183519
        },
        "rouge-l": {
          "r": 0.1693548387096774,
          "p": 0.35,
          "f": 0.2282608651701324
        }
      }
    },
    {
      "paper_id": "physics.flu-dyn.cond-mat/soft/2503.10261v1",
      "true_abstract": "Flow birefringence measurement is an emerging technique for visualizing\nstress fields in fluid flows. This study investigates flow birefringence in the\nsteady radial Hele-Shaw flow of a shear-thinning fluid. In this flow\nconfiguration, stress is dominant along the optical axis, challenging the\napplicability of the conventional stress-optic law (SOL). We conduct flow\nbirefringence measurements at various flow rates and compare the results with\ntheoretical predictions. The observed phase retardation cannot be\nquantitatively explained using the conventional SOL, but is successfully\ndescribed using the second-order SOL, which accounts for stress along the\noptical direction, using the results of rheo-optical measurements. Furthermore,\nwe investigate the shear-thinning effects on phase retardation from two\nperspectives: (i) stress changes resulting from viscosity variations and (ii)\nthe variation of the shear-dependent stress-optic coefficient in the\nsecond-order SOL. Our findings indicate that the latter is more significant and\nshear-thinning behavior suppresses radial variations in phase retardation. This\nstudy demonstrates that the combination of the second-order SOL and\nrheo-optical measurements is essential for an accurate interpretation of flow\nbirefringence in Hele-Shaw flow, providing a noninvasive approach for stress\nfield analysis in high-aspect-ratio geometries.",
      "generated_abstract": "birefringence of shear-thinning fluids in a Hele-Shaw cell is\ninvestigated using a microfluidic optical system that allows the flow field to\nbe monitored at different locations inside the cell. A shear-thinning fluid is\nused to study the flow birefringence, where the shear stress is negligible,\nmaking the flow birefringence an excellent tool for investigating the effects\nof shear stress. A Hele-Shaw cell is used to study the flow birefringence for\ndifferent flow speeds and different concentrations of the shear-thinning fluid.\nThe results show that the flow birefringence increases with the flow speed and\ndecreases with the concentration of the shear-thinning fluid. A simple\nparameterization is presented that can be used to quantify the flow bire",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23148148148148148,
          "p": 0.423728813559322,
          "f": 0.2994011930352469
        },
        "rouge-2": {
          "r": 0.03680981595092025,
          "p": 0.06451612903225806,
          "f": 0.046874995373840794
        },
        "rouge-l": {
          "r": 0.19444444444444445,
          "p": 0.3559322033898305,
          "f": 0.25149700141848047
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.06743v2",
      "true_abstract": "Structural changes in main retinal blood vessels serve as critical biomarkers\nfor the onset and progression of glaucoma. Identifying these vessels is vital\nfor vascular modeling yet highly challenging. This paper proposes X-GAN, a\ngenerative AI-powered unsupervised segmentation model designed for extracting\nmain blood vessels from Optical Coherence Tomography Angiography (OCTA) images.\nThe process begins with the Space Colonization Algorithm (SCA) to rapidly\ngenerate a skeleton of vessels, featuring their radii. By synergistically\nintegrating generative adversarial networks (GANs) with biostatistical modeling\nof vessel radii, X-GAN enables a fast reconstruction of both 2D and 3D\nrepresentations of the vessels. Based on this reconstruction, X-GAN achieves\nnearly 100\\% segmentation accuracy without relying on labeled data or\nhigh-performance computing resources. Also, to address the Issue, data scarity,\nwe introduce GSS-RetVein, a high-definition mixed 2D and 3D glaucoma retinal\ndataset. GSS-RetVein provides a rigorous benchmark due to its exceptionally\nclear capillary structures, introducing controlled noise for testing model\nrobustness. Its 2D images feature sharp capillary boundaries, while its 3D\ncomponent enhances vascular reconstruction and blood flow prediction,\nsupporting glaucoma progression simulations. Experimental results confirm\nGSS-RetVein's superiority in evaluating main vessel segmentation compared to\nexisting datasets. Code and dataset are here:\nhttps://github.com/VikiXie/SatMar8.",
      "generated_abstract": "is a group of eye diseases characterized by elevated intraocular\nline pressures (IOPs). A growing body of literature has revealed that\nretinal-based imaging can be a promising approach to early disease detection.\nHowever, current techniques remain limited in their ability to accurately\nsegment the retinal main vessels (RMVs), especially in light of the complex\nstructural variations that occur in glaucomatous eyes. To address this\nchallenge, we introduce X-GAN, a novel unsupervised deep learning model that\nleverages the power of Generative Adversarial Networks (GANs) to generate high\nquality retinal segmentations. Our approach integrates the retinal image\npreprocessing pipeline, which comprises a series of image enhancement and\ncolorization steps, with a generative model that outputs a series of\npseudo-labels for each retinal",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15862068965517243,
          "p": 0.24731182795698925,
          "f": 0.1932773061630536
        },
        "rouge-2": {
          "r": 0.020618556701030927,
          "p": 0.034782608695652174,
          "f": 0.02588996296436024
        },
        "rouge-l": {
          "r": 0.13793103448275862,
          "p": 0.21505376344086022,
          "f": 0.16806722212944017
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2502.08443v1",
      "true_abstract": "The use of valid surrogate endpoints is an important stake in clinical\nresearch to help reduce both the duration and cost of a clinical trial and\nspeed up the evaluation of interesting treatments. Several methods have been\nproposed in the statistical literature to validate putative surrogate\nendpoints. Two main approaches have been proposed: the meta-analytic approach\nand the mediation analysis approach. The former uses data from meta-analyses to\nderive associations measures between the surrogate and the final endpoint at\nthe individual and trial levels. The latter rather uses the proportion of the\ntreatment effect on the final endpoint through the surrogate as a measure of\nsurrogacy in a causal inference framework. Both approaches have remained\nseparated as the meta-analytic approach does not estimate the treatment effect\non the final endpoint through the surrogate while the mediation analysis\napproach have been limited to single-trial setting. However, these two\napproaches are complementary. In this work we propose an approach that combines\nthe meta-analytic and mediation analysis approaches using joint modeling for\nsurrogate validation. We focus on the cases where the final endpoint is a\ntime-to-event endpoint (such as time-to-death) and the surrogate is either a\ntime-to-event or a longitudinal biomarker. Two new joint models were proposed\ndepending on the nature of the surrogate. These model are implemented in the R\npackage frailtypack. We illustrate the developed approaches in three\napplications on real datasets in oncology.",
      "generated_abstract": "This tutorial explores the use of a joint modeling approach and\nmediation analysis to determine if a surrogate endpoint is a valid surrogate\nfor the primary endpoint. We first discuss the potential benefits of joint\nmodeling and mediation analysis. Then, we review the case study of a\npost-market study of a drug used for the treatment of an autoimmune\ndisease. We provide a detailed overview of the data and the modeling approach.\nFinally, we present the results of the analysis. We discuss the strengths and\nlimitations of the approach.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1640625,
          "p": 0.4375,
          "f": 0.23863635966942154
        },
        "rouge-2": {
          "r": 0.06060606060606061,
          "p": 0.15584415584415584,
          "f": 0.08727272324072746
        },
        "rouge-l": {
          "r": 0.1328125,
          "p": 0.3541666666666667,
          "f": 0.19318181421487612
        }
      }
    },
    {
      "paper_id": "cs.HC.stat/OT/2407.14072v2",
      "true_abstract": "Psychological research often involves understanding psychological constructs\nthrough conducting factor analysis on data collected by a questionnaire, which\ncan comprise hundreds of questions. Without interactive systems for\ninterpreting factor models, researchers are frequently exposed to subjectivity,\npotentially leading to misinterpretations or overlooked crucial information.\nThis paper introduces FAVis, a novel interactive visualization tool designed to\naid researchers in interpreting and evaluating factor analysis results. FAVis\nenhances the understanding of relationships between variables and factors by\nsupporting multiple views for visualizing factor loadings and correlations,\nallowing users to analyze information from various perspectives. The primary\nfeature of FAVis is to enable users to set optimal thresholds for factor\nloadings to balance clarity and information retention. FAVis also allows users\nto assign tags to variables, enhancing the understanding of factors by linking\nthem to their associated psychological constructs. Our user study demonstrates\nthe utility of FAVis in various tasks.",
      "generated_abstract": "alysis (FA) is a widely-used method in psychological research for\nanalysis of personality traits. However, the visualization of FA results is\noften limited to statistical indicators. This paper proposes FAVis, a visual\nanalysis tool that provides an intuitive interface to visualize the FA model\nand its estimated parameters. It offers a variety of visualization methods\nincluding histograms, scatter plots, and line plots, and provides intuitive\ncontrols to manipulate these visualizations. The tool can be used for\ninteractive exploration of FA results, and can be integrated into existing\nanalytical workflows. We demonstrate FAVis on two real-world datasets, one\nfrom a study of cognitive flexibility and the other from a study of gender\ndifferences in cognitive flexibility. The results show that FAVis provides a\nmore comprehensive and interactive view of FA results, enhancing the\nunderstanding and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26732673267326734,
          "p": 0.3103448275862069,
          "f": 0.2872340375809191
        },
        "rouge-2": {
          "r": 0.029197080291970802,
          "p": 0.03225806451612903,
          "f": 0.03065133600857379
        },
        "rouge-l": {
          "r": 0.24752475247524752,
          "p": 0.28735632183908044,
          "f": 0.2659574418362382
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/CR/2503.10171v1",
      "true_abstract": "Graph databases have garnered extensive attention and research due to their\nability to manage relationships between entities efficiently. Today, many graph\nsearch services have been outsourced to a third-party server to facilitate\nstorage and computational support. Nevertheless, the outsourcing paradigm may\ninvade the privacy of graphs. PeGraph is the latest scheme achieving encrypted\nsearch over social graphs to address the privacy leakage, which maintains two\ndata structures XSet and TSet motivated by the OXT technology to support\nencrypted conjunctive search. However, PeGraph still exhibits limitations\ninherent to the underlying OXT. It does not provide transparent search\ncapabilities, suffers from expensive computation and result pattern leakages,\nand it fails to support search over dynamic encrypted graph database and\nresults verification. In this paper, we propose SecGraph to address the first\ntwo limitations, which adopts a novel system architecture that leverages an\nSGX-enabled cloud server to provide users with secure and transparent search\nservices since the secret key protection and computational overhead have been\noffloaded to the cloud server. Besides, we design an LDCF-encoded XSet based on\nthe Logarithmic Dynamic Cuckoo Filter to facilitate efficient plaintext\ncomputation in trusted memory, effectively mitigating the risks of result\npattern leakage and performance degradation due to exceeding the limited\ntrusted memory capacity. Finally, we design a new dynamic version of TSet named\nTwin-TSet to enable conjunctive search over dynamic encrypted graph database.\nIn order to support verifiable search, we further propose VSecGraph, which\nutilizes a procedure-oriented verification method to verify all data structures\nloaded into the trusted memory, thus bypassing the computational overhead\nassociated with the client's local verification.",
      "generated_abstract": "rch is a fundamental problem in knowledge graphs, where a user\nquerying for a graph is provided with a set of pre-defined candidate nodes and\nedges, and the user is interested in finding a graph that satisfies the query\nand satisfies the transparency property: no node is chosen more than once.\nAlthough existing graph search algorithms have high efficiency and\ntransparency, they are not verifiable and have low efficiency and\nconfidentiality. To address these limitations, we propose a new graph search\nalgorithm with high efficiency and transparency, and a novel verification\nmethod that verifies the transparency of the graph search result. The\ntransparency verification method is based on a new method that allows a\nqueryer to prove a graph satisfies the transparency property. We show that the\nverification method can verify the transparency of a graph search result and\nthat the proposed algorithm is efficient and has high efficiency",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16875,
          "p": 0.34615384615384615,
          "f": 0.22689075189605257
        },
        "rouge-2": {
          "r": 0.02531645569620253,
          "p": 0.048,
          "f": 0.033149166749336725
        },
        "rouge-l": {
          "r": 0.15,
          "p": 0.3076923076923077,
          "f": 0.20168066786243918
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.08458v1",
      "true_abstract": "The information criterion AIC has been used successfully in many areas of\nstatistical modeling, and since it is derived based on the Taylor expansion of\nthe log-likelihood function and the asymptotic distribution of the maximum\nlikelihood estimator, it is not directly justified for likelihood functions\nthat include non-differentiable points such as the Laplace distribution. In\nfact, it is known to work effectively in many such cases. In this paper, we\nattempt to evaluate the bias correction directly for the case where the true\nmodel or the model to be estimated is a simple Laplace distribution model. As a\nresult, an approximate expression for the bias correction term was obtained.\nNumerical results show that the AIC approximations are relatively good except\nwhen the Gauss distribution model is fitted to data following the Laplace\ndistribution.",
      "generated_abstract": "igate the consistency of the Information Criterion for Gaussian and\nLaplace\n    distributions. We use a new approach to derive asymptotic properties of the\ninformation criterion, which is applied to the Gaussian and Laplace\ndistributions. We show that the information criterion is asymptotically\nconsistent in the Gaussian case if the sample size is large enough. This\nresult is generalized to the case where the data are generated from the\nLaplace distribution. The consistency of the information criterion for the\nLaplace distribution is established in the asymptotic regime. We also examine\nthe finite sample performance of the information criterion in the Laplace\ndistribution. The finite sample performance of the information criterion is\nstudied for the Gaussian and Laplace distributions. The finite sample\nperformance of the information criterion is analyzed for the Gaussian and Laplace\ndistributions. We also examine the finite sample performance of the\ninformation criterion",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22988505747126436,
          "p": 0.36363636363636365,
          "f": 0.28169013609898835
        },
        "rouge-2": {
          "r": 0.09090909090909091,
          "p": 0.12941176470588237,
          "f": 0.10679611165755513
        },
        "rouge-l": {
          "r": 0.20689655172413793,
          "p": 0.32727272727272727,
          "f": 0.2535211220144813
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.20067v1",
      "true_abstract": "The emergence of audio language models is empowered by neural audio codecs,\nwhich establish critical mappings between continuous waveforms and discrete\ntokens compatible with language model paradigms. The evolutionary trends from\nmulti-layer residual vector quantizer to single-layer quantizer are beneficial\nfor language-autoregressive decoding. However, the capability to handle\nmulti-domain audio signals through a single codebook remains constrained by\ninter-domain distribution discrepancies. In this work, we introduce UniCodec, a\nunified audio codec with a single codebook to support multi-domain audio data,\nincluding speech, music, and sound. To achieve this, we propose a partitioned\ndomain-adaptive codebook method and domain Mixture-of-Experts strategy to\ncapture the distinct characteristics of each audio domain. Furthermore, to\nenrich the semantic density of the codec without auxiliary modules, we propose\na self-supervised mask prediction modeling approach. Comprehensive objective\nand subjective evaluations demonstrate that UniCodec achieves excellent audio\nreconstruction performance across the three audio domains, outperforming\nexisting unified neural codecs with a single codebook, and even surpasses\nstate-of-the-art domain-specific codecs on both acoustic and semantic\nrepresentation capabilities.",
      "generated_abstract": "ecs are often designed to be highly flexible, adapting to\nvarious hardware and software environments. However, most existing codecs\nrequire the user to define a number of domain-specific parameters, such as\nfilter order, filter bandwidth, and the number of frame-level codebook\nelements. These parameters often vary across codec implementations, making it\nchallenging to implement a single codec across diverse platforms and\nenvironments. In this paper, we propose UniCodec, a unified audio codec that\nreduces the need for domain-specific codebook configuration by introducing a\nsingle codebook element. UniCodec defines a common codebook structure that\nincludes a number of domain-specific codebook elements, allowing users to\nconfigure a codec for a specific platform without the need to define\nspecific codebook elements. We demonstrate the effectiveness of UniCodec in\nterms of both compression performance and implementation complexity",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2396694214876033,
          "p": 0.3372093023255814,
          "f": 0.2801932318579197
        },
        "rouge-2": {
          "r": 0.05,
          "p": 0.06504065040650407,
          "f": 0.05653709755896608
        },
        "rouge-l": {
          "r": 0.21487603305785125,
          "p": 0.3023255813953488,
          "f": 0.2512077246115429
        }
      }
    },
    {
      "paper_id": "math.AC.math/RA/2503.07271v1",
      "true_abstract": "A module $M$ is said to be stable if it has no nonzero projective direct\nsummand. For a ring $ R $, we study the conditions under which every $R$-module\n$M$ within a specific class can be decomposed into a direct sum of a projective\nmodule and a stable module, focusing on identifying the types of rings and the\nclass of $R$-modules where this property holds. Some well-known classes of\nrings over which every finitely presented module can be decomposed into a\ndirect sum of a projective submodule and a stable submodule are semiperfect\nrings or semilocal rings or rings satisfying some finiteness conditions like\nhaving finite uniform dimension or hollow dimension or being Noetherian or\nArtinian. By using the Auslander-Bridger transpose of modules, we prove that\nevery finitely presented right $R$-module over a left semihereditary ring $R$\nhas such a decomposition; note that the semihereditary condition is on the\nopposite side. Our main focus in this article is to give examples where such a\ndecomposition fails. We give some ring examples over which there exists an\ninfinitely generated or finitely generated or cyclic module or finitely\npresented module or cyclically presented module where such a decomposition\nfails. Our main example is a cyclically presented module $M$ over a commutative\nring such that~$M$ has no such decomposition and $M$ is not projectively\nequivalent to a stable module.",
      "generated_abstract": "In this paper, we study the decompositions of a complex abelian group $G$\ninto a direct sum of projective and stable submodules. We show that if $G$ is\nabelian, then it decomposes into a direct sum of projective and stable submodules\nonly if it is cyclic. In particular, if $G$ is cyclic, then it decomposes into a\ndirect sum of projective and stable submodules. We also prove that if $G$ is\nnot cyclic, then it decomposes into a direct sum of projective and stable\nsubmodules only if $G$ is a direct sum of projective and stable submodules. We\nalso give a complete characterization of the cyclic cases.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18584070796460178,
          "p": 0.525,
          "f": 0.27450980005980613
        },
        "rouge-2": {
          "r": 0.0582010582010582,
          "p": 0.19642857142857142,
          "f": 0.08979591484081645
        },
        "rouge-l": {
          "r": 0.168141592920354,
          "p": 0.475,
          "f": 0.2483660092101329
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.05880v1",
      "true_abstract": "Likelihood inference for max-stable random fields is in general impossible\nbecause their finite-dimensional probability density functions are unknown or\ncannot be computed efficiently. The weighted composite likelihood approach that\nutilizes lower dimensional marginal likelihoods (typically pairs or triples of\nsites that are not too distant) is rather favored. In this paper, we consider\nthe family of spatial max-stable Brown-Resnick random fields associated with\nisotropic fractional Brownian fields. We assume that the sites are given by\nonly one realization of a homogeneous Poisson point process restricted to\n$\\mathbf{C}=(-1/2,1/2]^{2}$ and that the random field is observed at these\nsites. As the intensity increases, we study the asymptotic properties of the\ncomposite likelihood estimators of the scale and Hurst parameters of the\nfractional Brownian fields using different weighting strategies: we exclude\neither pairs that are not edges of the Delaunay triangulation or triples that\nare not vertices of triangles.",
      "generated_abstract": "the asymptotic properties of the maximum composite likelihood\n(MCL) estimator for the mean function of a max-stable Brown-Resnick random\nfield over a fixed-domain. The MCL estimator is based on a generalized\nHansen-Kolmogorov-Smirnov (HKS) test statistic, which is asymptotically\nconsistent and asymptotically normal under certain conditions. We show that the\nMCL estimator asymptotically follows a normal distribution with a density that\nis asymptotically the product of a density with a log-normal distribution and\na density with a Gaussian density. We further show that the MCL estimator is\nasymptotically normal and has a finite variance under certain conditions. We\nalso establish that the MCL estimator is asymptotically normal and has a finite\nvariance under the more general setting where the underlying max-stable\nBrown-Resnick",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20202020202020202,
          "p": 0.3448275862068966,
          "f": 0.2547770654046818
        },
        "rouge-2": {
          "r": 0.07575757575757576,
          "p": 0.11235955056179775,
          "f": 0.09049773274584905
        },
        "rouge-l": {
          "r": 0.20202020202020202,
          "p": 0.3448275862068966,
          "f": 0.2547770654046818
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/CB/2502.18947v1",
      "true_abstract": "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
      "generated_abstract": "We investigate the Keller-Segel model of chemotaxis, a system of coupled\nparameters describing the response of a population of receptors to\nchemical cues, with special attention to the role of ligand-receptor\ndynamics. We develop a tractable numerical scheme to solve the system in\ndifferent parameter regimes and study the dynamics of a travelling wave. We\nfind that the presence of a positive feedback between the two parameters\ndetermines the stability of the wave. Additionally, we show that the\ntravelling wave can be driven either by a positive or a negative feedback and\nthat the transition between these two regimes depends on the initial\nconditions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18840579710144928,
          "p": 0.41935483870967744,
          "f": 0.25999999572200005
        },
        "rouge-2": {
          "r": 0.06349206349206349,
          "p": 0.12371134020618557,
          "f": 0.08391607943346886
        },
        "rouge-l": {
          "r": 0.15942028985507245,
          "p": 0.3548387096774194,
          "f": 0.21999999572200007
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.12752v1",
      "true_abstract": "Using novel establishment-level observational data from Switzerland, we\nempirically examine whether the usage of key technologies of Industry 4.0\ndistinguishes across firms with different types of organizational culture.\nBased on the Technology-Organization-Environment and the Competing Values\nframework, we hypothesize that the developmental culture has the greatest\npotential to promote the usage of Industry 4.0 technologies. We also\nhypothesize that companies with a hierarchical or rational culture are\nespecially likely to make use of automation technologies, such as AI and\nrobotics. By means of descriptive statistics and multiple regression analysis,\nwe find empirical support for our first hypothesis, while we cannot con-firm\nour second hypothesis. Our empirical results provide important implications for\nmanagerial decision-makers. Specifically, the link between organizational\nculture and the implementation of Industry 4.0 technologies is relevant for\nmanagers, as this knowledge helps them to cope with digital transformation in\nturbulent times and keep their businesses competitive.",
      "generated_abstract": "ration of Industry 4.0 technologies into companies is essential to\nreach the goal of Industry 4.0. Despite the importance of this integration,\nresearch on its adoption is scarce. The aim of this study is to analyze the\nimpact of organizational culture on the usage of Industry 4.0 technologies. We\nutilize a panel data set of 500 Swiss manufacturing companies, and analyze the\neffects of organizational culture on the adoption of Industry 4.0 technologies\nand on the usage of these technologies. Our findings show that firms with\nhigher levels of organizational culture are more likely to adopt Industry 4.0\ntechnologies, and they are also more likely to use Industry 4.0 technologies. In\naddition, the adoption of Industry 4.0 technologies is positively associated\nwith the use",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2523364485981308,
          "p": 0.421875,
          "f": 0.31578946900037624
        },
        "rouge-2": {
          "r": 0.10144927536231885,
          "p": 0.14736842105263157,
          "f": 0.12017166899003501
        },
        "rouge-l": {
          "r": 0.22429906542056074,
          "p": 0.375,
          "f": 0.2807017497021306
        }
      }
    },
    {
      "paper_id": "math-ph.math/DS/2503.08412v1",
      "true_abstract": "The article presents the concept of a cumulant representation for\ndistribution functions describing the states of many-particle systems with\ntopological nearest-neighbor interaction. A solution to the Cauchy problem for\nthe hierarchy of nonlinear evolution equations for the cumulants of\ndistribution functions of such systems is constructed. The connection between\nthe constructed solution and the series expansion structure for a solution to\nthe Cauchy problem of the BBGKY hierarchy has been established. Furthermore,\nthe expansion structure for a solution to the Cauchy problem of the hierarchy\nof evolution equations for reduced observables of topologically interacting\nparticles is established.",
      "generated_abstract": "aper we introduce a novel cluster expansion technique for the\nparticle system state of the interacting $1D$ fermionic model with\ntopological nearest-neighbor interaction. The cluster expansion allows us to\ncompute the mean-field energy and correlation functions of the particle system\nin closed form. Moreover, we present an exact analytical expression for the\npartition function of the cluster expansion. We compare the results obtained\nusing the cluster expansion with the results obtained using the mean-field\napproximation. In particular, we show that the mean-field approximation\npredicts the correct order of magnitude of the mean-field energy, but it\noverestimates the correlation function. We also compare the results obtained\nusing the cluster expansion with the results obtained using the mean-field\napproximation for the correlation function of the system with two-particle\ninteraction. In particular, we show that the cluster expansion predicts the\ncorrect order of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2857142857142857,
          "p": 0.2222222222222222,
          "f": 0.24999999507812506
        },
        "rouge-2": {
          "r": 0.07894736842105263,
          "p": 0.06666666666666667,
          "f": 0.07228915166207031
        },
        "rouge-l": {
          "r": 0.2653061224489796,
          "p": 0.20634920634920634,
          "f": 0.23214285222098224
        }
      }
    },
    {
      "paper_id": "physics.chem-ph.physics/chem-ph/2503.10538v1",
      "true_abstract": "Given the power of large language and large vision models, it is of profound\nand fundamental interest to ask if a foundational model based on data and\nparameter scaling laws and pre-training strategies is possible for learned\nsimulations of chemistry and materials. The scaling of large and diverse\ndatasets and highly expressive architectures for chemical and materials\nsciences should result in a foundation model that is more efficient and broadly\ntransferable, robust to out-of-distribution challenges, and easily fine-tuned\nto a variety of downstream observables, when compared to specific training from\nscratch on targeted applications in atomistic simulation. In this Perspective\nwe aim to cover the rapidly advancing field of machine learned interatomic\npotentials (MLIP), and to illustrate a path to create chemistry and materials\nMLIP foundation models at larger scale.",
      "generated_abstract": "t a new foundation model for atomistic simulations of chemistry and\nmaterials, based on the Density Functional Theory (DFT) and the\nLangevin-Gaussian (LG) method. The foundation model is based on the\npseudo-potential method (PPM), which was originally developed to study\nelectronic structure in atoms and molecules. In this approach, the potential\nenergy function is constructed from the gradient of a generalized\npseudo-potential, which is a smooth function of the coordinates and the\nelectric field. This method allows for a direct mapping of the energy of a\nparticle (a particle in the PPM model) to its coordinates and electric field.\nIn the PPM model, the coordinates and electric field are used as input data for\nthe pseudo potential, which is then used to generate the potential energy\nfunction. This approach is particularly well suited for simulations of\nmolecules",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23333333333333334,
          "p": 0.28,
          "f": 0.25454544958677694
        },
        "rouge-2": {
          "r": 0.048,
          "p": 0.05084745762711865,
          "f": 0.04938271105353233
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.24,
          "f": 0.21818181322314062
        }
      }
    },
    {
      "paper_id": "physics.app-ph.eess/SP/2503.07239v1",
      "true_abstract": "We present the \"Virtual VNA 3.0\" technique for estimating the scattering\nmatrix of a \\textit{non-reciprocal}, linear, passive, time-invariant device\nunder test (DUT) with $N$ monomodal ports using a single measurement setup\ninvolving a vector network analyzer (VNA) with only $N_\\mathrm{A}<N$ ports --\nthus eliminating the need for any reconnections. We partition the DUT ports\ninto $N_\\mathrm{A}$ \"accessible\" and $N_\\mathrm{S}$ \"not-directly-accessible\"\n(NDA) ports. We connect the accessible ports to the VNA and the NDA ports to\nthe \"virtual VNA ports\" of a VNA Extension Kit. This kit enables each NDA port\nto be terminated with three distinct individual loads or connected to\nneighboring DUT ports via coupled loads. We derive both a closed-form and a\ngradient-descent method to estimate the complete scattering matrix of the\nnon-reciprocal DUT from measurements conducted with the $N_\\mathrm{A}$-port VNA\nunder various NDA-port terminations. We validate both methods experimentally\nfor $N_\\mathrm{A}=N_\\mathrm{S}=4$, where our DUT is a complex eight-port\ntransmission-line network comprising circulators. Altogether, the presented\n\"Virtual VNA 3.0\" technique constitutes a scalable approach to unambiguously\ncharacterize a many-port \\textit{non-reciprocal} DUT with a few-port VNA (only\n$N_\\mathrm{A}>1$ is required) -- without any tedious and error-prone manual\nreconnections susceptible to inaccuracies. The VNA Extension Kit requirements\nmatch those for the \"Virtual VNA 2.0\" technique that was limited to reciprocal\nDUTs.",
      "generated_abstract": "NA (VVA) is a powerful tool for non-reciprocal systems, but\ncurrently, there is no established method for estimating the scattering\nmatrix from VVAs. This paper introduces a novel approach for estimating the\nscattering matrix from VVAs. The proposed method relies on the coupling of a\ntunable and coupled load with a VVA and utilizes the method of moments to\nestimate the scattering matrix. By utilizing the proposed method, the scattering\nmatrix can be estimated even for non-reciprocal systems. In addition, the\nproposed method provides a method for estimating the scattering matrix from\nVVAs. The proposed method is validated through a simulation study and an\nexperimental study. The simulation study demonstrates the effectiveness of the\nproposed method, while the experimental study validates the proposed method's\neffectiveness for non-recipro",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15151515151515152,
          "p": 0.3076923076923077,
          "f": 0.20304568085753316
        },
        "rouge-2": {
          "r": 0.050505050505050504,
          "p": 0.10204081632653061,
          "f": 0.06756756313823988
        },
        "rouge-l": {
          "r": 0.14393939393939395,
          "p": 0.2923076923076923,
          "f": 0.19289339659357377
        }
      }
    },
    {
      "paper_id": "cs.IT.math/IT/2503.09991v1",
      "true_abstract": "A finite-field multiple-access (FFMA) system separates users within a finite\nfield by utilizing different element-pairs (EPs) as virtual resources. The\nCartesian product of distinct EPs forms an EP code, which serves as the input\nto a finite-field multiplexing module (FF-MUX), allowing the FFMA technique to\ninterchange the order of channel coding and multiplexing. This flexibility\nenables the FFMA system to support a large number of users with short packet\ntraffic, addressing the finite block length (FBL) problem in multiuser reliable\ntransmission. Designing EP codes is a central challenge in FFMA systems. In\nthis paper, we construct EP codes based on a bit(s)-to-codeword transformation\napproach and define the corresponding EP code as a codeword-wise EP (CWEP)\ncode. We then investigate the encoding process of EP codes, and propose unique\nsum-pattern mapping (USPM) structural property constraints to design uniquely\ndecodable CWEP codes. Next, we present the \\(\\kappa\\)-fold ternary orthogonal\nmatrix \\({\\bf T}_{\\rm o}(2^{\\kappa}, 2^{\\kappa})\\) over GF\\((3^m)\\), where \\(m\n= 2^{\\kappa}\\), and the ternary non-orthogonal matrix \\({\\bf T}_{\\rm no}(3,2)\\)\nover GF\\((3^2)\\), for constructing specific CWEP codes. Based on the proposed\nCWEP codes, we introduce three FFMA modes: channel codeword multiple access\n(FF-CCMA), code division multiple access (FF-CDMA), and non-orthogonal multiple\naccess (FF-NOMA). Simulation results demonstrate that all three modes\neffectively support massive user transmissions with strong error performance.",
      "generated_abstract": "t progress in the study of multi-antenna multiple access systems\nwith finite fields has been motivated by the potential of leveraging the\nstructure of the channel in the encoding and decoding of the data. In this\narticle, we revisit the problem of symbol-wise multiple access and then\nextend it to codeword-wise multiple access. The focus is on the case of\ncontinuous-variable channels. In this setting, the channel is modeled by a\ndiscrete-time convolutional code, and the channel is assumed to be an\n$n$-point discrete Fourier transform of a discrete Gaussian channel. We show\nthat the optimal capacity of the continuous-variable case is achieved by\nusing a two-stage coding scheme with a binary block code and a binary convolutional\ncode. The first stage is used to construct a block code that is used in the\nsecond stage, where a binary convolutional code is used. This block code is\nconstruct",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22,
          "p": 0.4177215189873418,
          "f": 0.288209602467535
        },
        "rouge-2": {
          "r": 0.024154589371980676,
          "p": 0.0390625,
          "f": 0.02985074154671494
        },
        "rouge-l": {
          "r": 0.19333333333333333,
          "p": 0.3670886075949367,
          "f": 0.25327510465094116
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2407.00022v1",
      "true_abstract": "Entropy is a very useful concept from physics that tries to explain how a\nsystem behaves from a point of view of the thermodynamics. However, there are\ntwo ways to explain entropy, and it depends on if we are studying a microsystem\nor a microsystem. From a macroscopically point of view, it is important to\ndescribe if the system is a reversible system or not. However, form the\nmicroscopically point of view, the concept of chaos is related to entropy. In\nsuch case, entropy measures the amount of disorder into the system. Otherwise,\nthe idea of connecting at the same time the analysis of the macro and micro\nsystem with the use of entropy it is not very common.",
      "generated_abstract": "We show that entropy is a universal measure of the disorder of a system. We\nentertain the possibility that entropy, as a measure of disorder, may be a\ngeneral principle in economics, and we explore this possibility by showing that\nthe theory of financial economics is consistent with the general principle of\nentropy being a measure of disorder.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.34285714285714286,
          "f": 0.23762375784726997
        },
        "rouge-2": {
          "r": 0.05504587155963303,
          "p": 0.11320754716981132,
          "f": 0.07407406967154424
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.3142857142857143,
          "f": 0.21782177764925018
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SP/2503.10345v1",
      "true_abstract": "Online conformal prediction enables the runtime calibration of a pre-trained\nartificial intelligence model using feedback on its performance. Calibration is\nachieved through set predictions that are updated via online rules so as to\nensure long-term coverage guarantees. While recent research has demonstrated\nthe benefits of incorporating prior knowledge into the calibration process,\nthis has come at the cost of replacing coverage guarantees with less tangible\nregret guarantees based on the quantile loss. This work introduces intermittent\nmirror online conformal prediction (IM-OCP), a novel runtime calibration\nframework that integrates prior knowledge, while maintaining long-term coverage\nand achieving sub-linear regret. IM-OCP features closed-form updates with\nminimal memory complexity, and is designed to operate under potentially\nintermittent feedback.",
      "generated_abstract": "We present a novel, online conformal prediction method for intermittent\nfeedback control (IFC), a recently proposed framework for modeling uncertain\nsystems in real-time control. The method relies on a novel approach to\nestimating the system's confidence interval, which is based on a stochastic\ncontinuation algorithm that provides an estimate of the system's expected\nconformal confidence interval, and a dynamic model that uses the estimated\nconfidence interval to predict the system's output. We prove the consistency\nand asymptotic normality of the predicted system's output under the\nintermittent feedback framework, and we demonstrate the performance of the\nprediction method by applying it to a model of a quadrotor system.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22093023255813954,
          "p": 0.2835820895522388,
          "f": 0.2483660081490026
        },
        "rouge-2": {
          "r": 0.05454545454545454,
          "p": 0.061855670103092786,
          "f": 0.05797100951247447
        },
        "rouge-l": {
          "r": 0.19767441860465115,
          "p": 0.2537313432835821,
          "f": 0.22222221729932945
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.08795v1",
      "true_abstract": "We propose a stochastic Model Predictive Control (MPC) framework that ensures\nclosed-loop chance constraint satisfaction for linear systems with general\nsub-Gaussian process and measurement noise. By considering sub-Gaussian noise,\nwe can provide guarantees for a large class of distributions, including\ntime-varying distributions. Specifically, we first provide a new\ncharacterization of sub-Gaussian random vectors using matrix variance proxies,\nwhich can more accurately represent the predicted state distribution. We then\nderive tail bounds under linear propagation for the new characterization,\nenabling tractable computation of probabilistic reachable sets of linear\nsystems. Lastly, we utilize these probabilistic reachable sets to formulate a\nstochastic MPC scheme that provides closed-loop guarantees for general\nsub-Gaussian noise. We further demonstrate our approach in simulations,\nincluding a challenging task of surgical planning from image observations.",
      "generated_abstract": "This paper addresses the design of stochastic model predictive control (SMPC)\nfor an uncertain system with sub-Gaussian noise. The control objective is to\nminimize a cost function that combines the expected cost with the variance of\nthe system's noise. We first derive a closed-form solution for the SMPC\ncontrol, and then provide a high-accuracy numerical method to solve it. The\nresults show that the SMPC control with the optimal cost function can achieve\noptimal convergence rates, which are comparable to the optimal rates for the\nstandard deterministic control problem. Furthermore, we provide a theoretical\njustification of the convergence rate of the proposed SMPC control.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21978021978021978,
          "p": 0.30303030303030304,
          "f": 0.25477706519047433
        },
        "rouge-2": {
          "r": 0.03361344537815126,
          "p": 0.0425531914893617,
          "f": 0.03755868051488967
        },
        "rouge-l": {
          "r": 0.1978021978021978,
          "p": 0.2727272727272727,
          "f": 0.22929935818410493
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.10524v1",
      "true_abstract": "We discuss invariants which are helpful for the computation of the vanishing\nlocus of a finitely presented functor $\\mathcal{G}$, i.e., the set of points in\nthe Ziegler spectrum on which $\\mathcal{G}$ vanishes. These invariants are: the\nrank of $\\mathcal{G}$, the supports of its co- and contravariant defect, and\nthe class of $\\mathcal{G}$ in the Grothendieck group of the category of\nfinitely presented functors. We show that these invariants determine the\nvanishing locus in the case of a finitely presented functor over a Dedekind\ndomain.",
      "generated_abstract": "that if a functor $F: \\mathcal{C} \\to \\mathcal{D}$ is finitely\npresented, then the vanishing locus of $F$ in the category $\\mathcal{C}/\\mathcal{D}$\nis the set of objects of $\\mathcal{C}$ that are not in the image of $F$. This\nproposition generalizes the result of Lurie \\cite{lurie-hilbert} for presentable\ncategories, where the functor is assumed to be a full embedding.\n  We apply our result to the functor $F$ that is the coproduct of a family of\nfunctors, and we prove that this functor is injective on objects and surjective\non morphisms in the category $\\mathcal{C}/\\mathcal{D}$. We also prove that\n$F$ is injective on objects and surjective on morphisms in the category\n$\\mathcal{",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.32,
          "p": 0.2807017543859649,
          "f": 0.29906541558214694
        },
        "rouge-2": {
          "r": 0.10666666666666667,
          "p": 0.0898876404494382,
          "f": 0.097560970646193
        },
        "rouge-l": {
          "r": 0.24,
          "p": 0.21052631578947367,
          "f": 0.22429906044196007
        }
      }
    },
    {
      "paper_id": "cs.MM.eess/AS/2502.03897v2",
      "true_abstract": "As a natural multimodal content, audible video delivers an immersive sensory\nexperience. Consequently, audio-video generation systems have substantial\npotential. However, existing diffusion-based studies mainly employ relatively\nindependent modules for generating each modality, which lack exploration of\nshared-weight generative modules. This approach may under-use the intrinsic\ncorrelations between audio and visual modalities, potentially resulting in\nsub-optimal generation quality. To address this, we propose UniForm, a unified\ndiffusion transformer designed to enhance cross-modal consistency. By\nconcatenating auditory and visual information, UniForm learns to generate audio\nand video simultaneously within a unified latent space, facilitating the\ncreation of high-quality and well-aligned audio-visual pairs. Extensive\nexperiments demonstrate the superior performance of our method in joint\naudio-video generation, audio-guided video generation, and video-guided audio\ngeneration tasks. Our demos are available at https://uniform-t2av.github.io/.",
      "generated_abstract": "ual content has become increasingly common in media, with\ncreative uses in both audio and video. However, existing methods for audio-to-\nvideo generation often struggle with unbalanced data, leading to inconsistent\nresults. To address this issue, we propose UniForm, a novel framework that\nintegrates a diffusion transformer with an autoregressive model. UniForm\npreserves the temporal structure of the audio-visual data while leveraging\nthe power of diffusion transformers. Our approach generates high-quality audio\nand video with unbalanced data by explicitly controlling the diffusion\nprocess. Additionally, we introduce a novel audio-visual contrastive loss to\npromote the alignment between the audio and video representations. Experiments\non audio-to-video generation demonstrate that UniForm achieves state-of-the-art\nperformance, with a mean absolute error (MAE) of 1.29% on",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25471698113207547,
          "p": 0.3103448275862069,
          "f": 0.27979274116244734
        },
        "rouge-2": {
          "r": 0.064,
          "p": 0.07017543859649122,
          "f": 0.0669456017051526
        },
        "rouge-l": {
          "r": 0.24528301886792453,
          "p": 0.2988505747126437,
          "f": 0.26943004686192923
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/RM/2503.08693v1",
      "true_abstract": "We construct liquidity-adjusted return and volatility using purposely\ndesigned liquidity metrics (liquidity jump and liquidity diffusion) that\nincorporate additional liquidity information. Based on these measures, we\nintroduce a liquidity-adjusted ARMA-GARCH framework to address the limitations\nof traditional ARMA-GARCH models, which are not effectively in modeling\nilliquid assets with high liquidity variability, such as cryptocurrencies. We\ndemonstrate that the liquidity-adjusted model improves model fit for\ncryptocurrencies, with greater volatility sensitivity to past shocks and\nreduced volatility persistence of erratic past volatility. Our model is\nvalidated by the empirical evidence that the liquidity-adjusted mean-variance\n(LAMV) portfolios outperform the traditional mean-variance (TMV) portfolios.",
      "generated_abstract": "This paper explores liquidity-adjusted return and volatility (LARV) as a\nnew\n  alternative to traditional measures of volatility and return. We develop\nan autoregressive model to predict LARV and its forecasting accuracy is\nevaluated using the M2 real-time data. We find that LARV is more volatile\nthan traditional measures of volatility and return, and the model performs\nbetter in predicting LARV than in predicting return. Our findings suggest that\nLARV can provide a more accurate measure of the market's volatility and\nreturn, and can be a useful tool for investors, regulators, and policymakers.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.3103448275862069,
          "f": 0.2769230719810652
        },
        "rouge-2": {
          "r": 0.030927835051546393,
          "p": 0.036585365853658534,
          "f": 0.0335195481077377
        },
        "rouge-l": {
          "r": 0.2222222222222222,
          "p": 0.27586206896551724,
          "f": 0.24615384121183442
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/TH/2503.03206v1",
      "true_abstract": "We developed an analytical framework for understanding how the learned\ndistribution evolves during diffusion model training. Leveraging the Gaussian\nequivalence principle, we derived exact solutions for the gradient-flow\ndynamics of weights in one- or two-layer linear denoiser settings with\narbitrary data. Remarkably, these solutions allowed us to derive the generated\ndistribution in closed form and its KL divergence through training. These\nanalytical results expose a pronounced power-law spectral bias, i.e., for\nweights and distributions, the convergence time of a mode follows an inverse\npower law of its variance. Empirical experiments on both Gaussian and image\ndatasets demonstrate that the power-law spectral bias remains robust even when\nusing deeper or convolutional architectures. Our results underscore the\nimportance of the data covariance in dictating the order and rate at which\ndiffusion models learn different modes of the data, providing potential\nexplanations for why earlier stopping could lead to incorrect details in image\ngenerative models.",
      "generated_abstract": "We derive a general analytical theory for the learning dynamics of diffusion\nmodel ensembles with power law-distributed weights. Our analysis is based on\nthe spectral bias of the diffusion operator, and focuses on the case where the\nspectral gap is small compared to the average diffusion time scale. We derive\nclosed-form solutions for the diffusion operator and show that the\npower-law-distributed weights induce a spectral bias that is strongly\nnon-Gaussian. Our results provide a mathematical foundation for the\nphenomenon of power-law-distributed weights in diffusion models, and we\ndemonstrate its relevance for the learning dynamics of these models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2543859649122807,
          "p": 0.4915254237288136,
          "f": 0.33526011111229914
        },
        "rouge-2": {
          "r": 0.059602649006622516,
          "p": 0.10714285714285714,
          "f": 0.0765957400872796
        },
        "rouge-l": {
          "r": 0.24561403508771928,
          "p": 0.4745762711864407,
          "f": 0.3236994174706807
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SP/2503.03338v1",
      "true_abstract": "We offer a new in-depth investigation of global path planning (GPP) for\nunmanned ground vehicles, an autonomous mining sampling robot named ROMIE. GPP\nis essential for ROMIE's optimal performance, which is translated into solving\nthe traveling salesman problem, a complex graph theory challenge that is\ncrucial for determining the most effective route to cover all sampling\nlocations in a mining field. This problem is central to enhancing ROMIE's\noperational efficiency and competitiveness against human labor by optimizing\ncost and time. The primary aim of this research is to advance GPP by\ndeveloping, evaluating, and improving a cost-efficient software and web\napplication. We delve into an extensive comparison and analysis of Google\noperations research (OR)-Tools optimization algorithms. Our study is driven by\nthe goal of applying and testing the limits of OR-Tools capabilities by\nintegrating Reinforcement Learning techniques for the first time. This enables\nus to compare these methods with OR-Tools, assessing their computational\neffectiveness and real-world application efficiency. Our analysis seeks to\nprovide insights into the effectiveness and practical application of each\ntechnique. Our findings indicate that Q-Learning stands out as the optimal\nstrategy, demonstrating superior efficiency by deviating only 1.2% on average\nfrom the optimal solutions across our datasets.",
      "generated_abstract": "This paper presents a survey of the OR-Tools (Open Robotics) and Deep\nmotor (DM) libraries for global path planning in autonomous vehicles. OR-Tools\nis a widely used open-source library for efficient and scalable path planning\nin autonomous vehicles, while Deep Motor is a deep learning-based approach that\nenables path planning with real-time navigation. The paper explores the\ndifferences and similarities between the two libraries, focusing on their\ncomputational complexity, performance, and scalability. Additionally, the paper\naddresses challenges and opportunities for integrating OR-Tools and Deep\nMotor, highlighting the benefits of a unified approach for global path\nplanning in autonomous vehicles. The paper concludes with a discussion of\npotential future directions for research in path planning, including the\npotential for using deep learning-based methods to enhance path planning\nperformance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18439716312056736,
          "p": 0.34210526315789475,
          "f": 0.2396313318541486
        },
        "rouge-2": {
          "r": 0.015151515151515152,
          "p": 0.02702702702702703,
          "f": 0.01941747112451801
        },
        "rouge-l": {
          "r": 0.14893617021276595,
          "p": 0.27631578947368424,
          "f": 0.19354838254539286
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2503.10117v1",
      "true_abstract": "Using introduced concept of the exchange and inflation rates adequacy, the\nrelevance of them to the determining factors is found. We established close\npositive relation between hryvnia / dollar exchange and inflation rates, fiscal\ndeficit, price level of energy sources, and money supply. On this basis, we\ngive proposals for state macroeconomic policy to stabilize Ukrainian economy.",
      "generated_abstract": "This study focuses on the problem of the exchange rate adequacy to determining\nfactors in the context of the exchange rate and inflation rates. The paper\nproposes a method of using the Kalman filter in the exchange rate and inflation\nrates adequacy to determining factors problem. The results of the study\nindicate that the exchange rate adequacy to determining factors method is\neffective in the problem of the exchange rate and inflation rates adequacy to\ndetermining factors in the context of the exchange rate and inflation rates.\nThe paper presents a theoretical basis for the proposed method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22916666666666666,
          "p": 0.3055555555555556,
          "f": 0.2619047570068028
        },
        "rouge-2": {
          "r": 0.09259259259259259,
          "p": 0.09615384615384616,
          "f": 0.09433961764328969
        },
        "rouge-l": {
          "r": 0.20833333333333334,
          "p": 0.2777777777777778,
          "f": 0.23809523319727902
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.09172v1",
      "true_abstract": "Low Ambiguity Zone (LAZ) sequences play a pivotal role in modern integrated\nsensing and communication (ISAC) systems. Recently, Wang et al.[1] proposed a\ndefinition of locally perfect nonlinear functions (LPNFs) and constructed three\nclasses of both periodic and aperiodic LAZ sequence sets with flexible\nparameters by applying such functions and interleaving method. Some of these\nLAZ sequence sets are asymptotically optimal with respect to the\nYe-Zhou-Liu-Fan-Lei-Tang bounds undercertain conditions. In this paper, we\nproceed with the construction of LPNFs with new parameters. By using these\nLPNFs, we also present a series of LAZ sequence sets with more flexible\nparameters, addressing the limitations of existing parameter choices.\nFurthermore, our results show that one of these classes is asymptotically\noptimal in both the periodic and aperiodic cases, respectively.",
      "generated_abstract": "r presents a new construction of locally perfect nonlinear\nfunctions for sequences sets with low ambiguity zone. The construction is\nbased on a new concept of locally perfect nonlinear functions. It is shown that\nthe construction of locally perfect nonlinear functions is based on the\nconstruction of locally perfect nonlinear functions. It is shown that the\nconstruction of locally perfect nonlinear functions is based on the\nconstruction of locally perfect nonlinear functions. The construction of\nlocally perfect nonlinear functions is based on the construction of locally\nperfect nonlinear functions. The construction of locally perfect nonlinear\nfunctions is based on the construction of locally perfect nonlinear functions.\nThe construction of locally perfect nonlinear functions is based on the\nconstruction of locally perfect nonlinear functions. The construction of\nlocally perfect nonlinear functions is based on the construction of locally\nperfect nonlinear functions. The construction of locally perfect nonlinear\nfunctions is based on the construction of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15730337078651685,
          "p": 0.5384615384615384,
          "f": 0.24347825737013232
        },
        "rouge-2": {
          "r": 0.05982905982905983,
          "p": 0.21212121212121213,
          "f": 0.09333332990133347
        },
        "rouge-l": {
          "r": 0.14606741573033707,
          "p": 0.5,
          "f": 0.22608695302230625
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.21141v1",
      "true_abstract": "How do transport infrastructures shape economic transformation and social\nchange? We examine the impact of railway expansion in nineteenth-century\nDenmark on local population growth, occupational shifts, and the diffusion of\nideas. Using a historical panel dataset and a difference-in-differences\napproach, we document that railway access significantly increased population\ngrowth and accelerated structural change. Moreover, railway-connected areas\nwere more likely to establish key institutions linked to civic engagement and\nthe cooperative movement. These findings suggest that improved market access\nwas not only a driver of economic modernization but also a catalyst for\ninstitutional and cultural transformation.",
      "generated_abstract": "y examines the relationship between railroads and the emergence of\nmodernity in Denmark. We show that the expansion of railroads coincided with\nthe creation of the modern political parties, which facilitated the growth of\nmass organizations, such as trade unions. This process was facilitated by the\nlack of alternative forms of transportation. By analyzing the growth of the\ntrade unions and political parties, we can trace the emergence of the\ntrade-union movement, which eventually became a political movement in its own\nright, and the rise of the Social Democratic Party. The rise of the\ntrade-union movement and the Social Democratic Party are closely linked. The\ntrade-union movement was a counter-movement to the traditional parties,\nrepresenting the interests of the working class. The Social Democratic Party\nwas a counter-movement to the traditional parties, representing the interests\nof the middle class",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19736842105263158,
          "p": 0.21739130434782608,
          "f": 0.20689654673579083
        },
        "rouge-2": {
          "r": 0.010752688172043012,
          "p": 0.009433962264150943,
          "f": 0.010050246277621716
        },
        "rouge-l": {
          "r": 0.18421052631578946,
          "p": 0.2028985507246377,
          "f": 0.19310344328751497
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.05695v1",
      "true_abstract": "We develop a Functional Augmented Vector Autoregression (FunVAR) model to\nexplicitly incorporate firm-level heterogeneity observed in more than one\ndimension and study its interaction with aggregate macroeconomic fluctuations.\nOur methodology employs dimensionality reduction techniques for tensor data\nobjects to approximate the joint distribution of firm-level characteristics.\nMore broadly, our framework can be used for assessing predictions from\nstructural models that account for micro-level heterogeneity observed on\nmultiple dimensions. Leveraging firm-level data from the Compustat database, we\nuse the FunVAR model to analyze the propagation of total factor productivity\n(TFP) shocks, examining their impact on both macroeconomic aggregates and the\ncross-sectional distribution of capital and labor across firms.",
      "generated_abstract": "We develop a Functional Vector Auto-Regressive (FVAR) model that allows for\nparticular functional forms of the error terms, i.e., heterogeneity. We show\nthat the model captures heterogeneity in the variance and the mean of the\nerror terms. Furthermore, we show that the model can be used to estimate\ndifferent types of heterogeneity. We illustrate our results using empirical\nevidence from Germany.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23529411764705882,
          "p": 0.43478260869565216,
          "f": 0.30534350689353773
        },
        "rouge-2": {
          "r": 0.05825242718446602,
          "p": 0.10714285714285714,
          "f": 0.0754716935500972
        },
        "rouge-l": {
          "r": 0.23529411764705882,
          "p": 0.43478260869565216,
          "f": 0.30534350689353773
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.04453v1",
      "true_abstract": "Given the need to elucidate the mechanisms underlying illnesses and their\ntreatment, as well as the lack of harmonization of acquisition and\npost-processing protocols among different magnetic resonance system vendors,\nthis work is to determine if metabolite concentrations obtained from different\nsessions, machine models and even different vendors of 3 T scanners can be\nhighly reproducible and be pooled for diagnostic analysis, which is very\nvaluable for the research of rare diseases. Participants underwent magnetic\nresonance imaging (MRI) scanning once on two separate days within one week (one\nsession per day, each session including two proton magnetic resonance\nspectroscopy (1H-MRS) scans with no more than a 5-minute interval between scans\n(no off-bed activity)) on each machine. were analyzed for reliability of\nwithin- and between- sessions using the coefficient of variation (CV) and\nintraclass correlation coefficient (ICC), and for reproducibility of across the\nmachines using correlation coefficient. As for within- and between- session,\nall CV values for a group of all the first or second scans of a session, or for\na session were almost below 20%, and most of the ICCs for metabolites range\nfrom moderate (0.4-0.59) to excellent (0.75-1), indicating high data\nreliability. When it comes to the reproducibility across the three scanners,\nall Pearson correlation coefficients across the three machines approached 1\nwith most around 0.9, and majority demonstrated statistical significance\n(P<0.01). Additionally, the intra-vendor reproducibility was greater than the\ninter-vendor ones.",
      "generated_abstract": "anterior cingulate cortex (pACC) has been associated with\nprocessing speed, executive function, and mental health. However, the\nextent to which the pACC is affected in different populations is still\nunclear. Therefore, the aim of this study is to assess the reproducibility of\nmagnetic resonance spectroscopy (MRS) of the pACC across sessions and\nvendors using the CloudBrain-MRS cloud computing platform. To evaluate\nreproducibility, 14 participants (9 male, 5 female) underwent 5-minute\nMRS scans of the pACC. Each scan was acquired at a 1000 Hz sampling rate, with\n300 scans per session and 40 scans per vendor. The scans were analyzed using\nthe CloudBrain-MRS platform, and the reproduci",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16774193548387098,
          "p": 0.325,
          "f": 0.22127659125396115
        },
        "rouge-2": {
          "r": 0.039301310043668124,
          "p": 0.09090909090909091,
          "f": 0.05487804456592091
        },
        "rouge-l": {
          "r": 0.14193548387096774,
          "p": 0.275,
          "f": 0.18723403806247182
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/SC/2309.06907v3",
      "true_abstract": "Plasma membrane calcium influx through ion channels is crucial for many\nevents in cellular physiology. Cell surface stimuli lead to the production of\ninositol 1,4,5-trisphosphate (IP3), which binds to IP3 receptors in the\nendoplasmic reticulum (ER) to release calcium pools from the ER lumen. This\nleads to depletion of ER calcium pools which has been termed store-depletion.\nStore-depletion leads the dissociation of calcium ions from the EF-hand motif\nof the ER calcium sensor Stromal Interaction Molecule 1 (STIM1). This leads to\na conformational change in STIM1 which helps it to interact with a plasma\nmembrane (PM) at ER:PM junctions. At these ER:PM junctions, STIM1 binds to and\nactivates a calcium channel known as Orai1 to form calcium-release activated\ncalcium (CRAC) channels. Activation of Orai1 leads to calcium influx, known as\nstore-operated calcium entry (SOCE). In addition to Orai1 and STIM1, the\nhomologs of Orai1 and STIM1, such as Orai2/3 and STIM2 also play a crucial role\nin calcium homeostasis. The influx of calcium through the Orai channel\nactivates a calcium current that has been termed CRAC currents. CRAC channels\nform multimers and cluster together in large macromolecular assemblies termed\npuncta. How these CRAC channels form puncta has been contentious since their\ndiscovery. In this review, we will outline the history of SOCE, the molecular\nplayers involved in this process (Orai and STIM proteins, TRP channels,\nSOCE-associated regulatory factor etc.), as well as the models that have been\nproposed to explain this important mechanism in cellular physiology.",
      "generated_abstract": "activated K+ channel (CAK) is a key regulator of intracellular\ncalcium levels. It is essential for regulating the cellular response to\nextracellular calcium and is also involved in Ca2+-dependent signaling. However,\nthe molecular mechanisms regulating the Ca2+-activation kinetics and channel\nformation of the CAK have not been fully elucidated. In this study, we\ndemonstrated that the cAMP-dependent protein kinase (PKA) phosphorylates the\nC-terminal domain of the CAK (CAK-CTD) at Ser444 and Ser446, which can\npotentially modulate the Ca2+-activation kinetics of the CAK. In addition, we\nproposed a model for the interaction between PKA and the C-terminal domain of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.36363636363636365,
          "f": 0.22857142426122457
        },
        "rouge-2": {
          "r": 0.017857142857142856,
          "p": 0.04597701149425287,
          "f": 0.025723468639075913
        },
        "rouge-l": {
          "r": 0.1597222222222222,
          "p": 0.3484848484848485,
          "f": 0.21904761473741502
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/PR/2410.04748v2",
      "true_abstract": "We introduce a fairly general, recombining trinomial tree model in the\nnatural world. Market-completeness is ensured by considering a market\nconsisting of two risky assets, a riskless asset, and a European option. The\ntwo risky assets consist of a stock and a perpetual derivative of that stock.\nThe option has the stock and its derivative as its underlying. Using a\nreplicating portfolio, we develop prices for European options and generate the\nunique relationships between the risk-neutral and real-world parameters of the\nmodel. We discuss calibration of the model to empirical data in the cases in\nwhich the risky asset returns are treated as either arithmetic or logarithmic.\nFrom historical price and call option data for select large cap stocks, we\ndevelop implied parameter surfaces for the real-world parameters in the model.",
      "generated_abstract": "We introduce a novel methodology for hedging fixed-income derivatives via\nperpetual options. Our method relies on a decomposition of the Black-Scholes\nequations into two systems, one for pricing perpetuals and one for implied\nparameters. We then derive a closed-form expression for the implied parameter\nsurface in terms of the price surface of the perpetuals. This enables us to\nderive the hedging strategy and evaluate the effectiveness of the strategy. We\ndemonstrate the applicability of our methodology through numerical examples and\ncompare it with a popular strategy based on the Black-Litterman model. Our\nresults demonstrate that our methodology outperforms the Black-Litterman\nstrategy.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.2903225806451613,
          "f": 0.25174824683652014
        },
        "rouge-2": {
          "r": 0.041666666666666664,
          "p": 0.05319148936170213,
          "f": 0.04672896703642291
        },
        "rouge-l": {
          "r": 0.20987654320987653,
          "p": 0.27419354838709675,
          "f": 0.23776223285050624
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.04129v1",
      "true_abstract": "This work aims to synthesize a controller that ensures that an unknown\ndiscrete-time system is incrementally input-to-state stable ($\\delta$-ISS). In\nthis work, we introduce the notion of $\\delta$-ISS control Lyapunov function\n($\\delta$-ISS-CLF), which, in conjunction with the controller, ensures that the\nclosed-loop system is incrementally ISS. To address the unknown dynamics of the\nsystem, we parameterize the controller as well as the $\\delta$-ISS-CLF as\nneural networks and learn them by utilizing the sampled data from the state\nspace of the unknown system. To formally verify the obtained $\\delta$-ISS-CLF,\nwe develop a validity condition and incorporate the condition into the training\nframework to ensure a provable correctness guarantee at the end of the training\nprocess. Finally, the usefulness of the proposed approach is proved using\nmultiple case studies - the first one is a scalar system with a non-affine\nnon-polynomial structure, the second example is a one-link manipulator system,\nthe third system is a nonlinear Moore-Grietzer model of the jet engine and the\nfinal one is a rotating rigid spacecraft model.",
      "generated_abstract": "al Input-to-State Stability (IISS) is a powerful tool for\nmanaging state changes in discrete-time systems. However, the formal verification\nof IISS controllers in terms of the input-to-state stability (ISS)\nframework is still an open problem. In this paper, we formalize IISS controllers\nfor unknown discrete-time systems in the context of the ISS framework. To\nachieve this, we first extend the classical IISS framework to include the\npossibility of imperfect state measurements and non-stationary disturbances.\nNext, we design a novel verification theorem for IISS controllers, which\nprovides conditions for the verification of IISS controllers. Finally, we\npresent a formalization of IISS controllers for unknown discrete-time\nsystems using the new verification theorem. The formalization is validated\nthrough the analysis of several examples. The results highlight the effectiveness",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19811320754716982,
          "p": 0.28,
          "f": 0.23204419404169604
        },
        "rouge-2": {
          "r": 0.03225806451612903,
          "p": 0.04504504504504504,
          "f": 0.03759398009921483
        },
        "rouge-l": {
          "r": 0.1792452830188679,
          "p": 0.25333333333333335,
          "f": 0.20994474652788386
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.09998v1",
      "true_abstract": "We numerically investigate the sensitivity of the scattered wave field to\nperturbations in the shape of a scattering body illuminated by an incident\nplane wave. This study is motivated by recent work on the inverse problem of\nreconstructing a scatterer shape from measurements of the scattered wave at\nlarge distances from the scatterer. For this purpose we consider star-shaped\nscatterers represented using cubic splines, and our approach is based on a\nNystr\\\"om method-based discretisation of the shape derivative. Using the\nsingular value decomposition, we identify fundamental geometric modes that most\nstrongly influence the scattered wave, providing insight into the most visible\nboundary features in scattering data.",
      "generated_abstract": "aper, we investigate the sensitivity of the far-field pattern of\neither the scattering object or the incident wavefield to local boundary\nperturbations in the two-dimensional (2D) wave scattering. We first consider a\nwave-like scatterer with a fixed boundary, and show that the far-field pattern\nis sensitive to the boundary perturbations, in which the boundary is viewed as\na perturbation on the wavefront. In the case of the incident wavefield, we\nconsider a source with a fixed boundary and show that the far-field pattern is\nsensitive to the boundary perturbations, in which the boundary is viewed as a\nperturbation on the wavefront. In both cases, we provide a perturbation\nexpansion of the far-field pattern for the scatterer and the source, and\nderive the conditions for the perturbation to lead to a significant change of\nthe far-field pattern.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2631578947368421,
          "p": 0.3448275862068966,
          "f": 0.2985074577767878
        },
        "rouge-2": {
          "r": 0.09090909090909091,
          "p": 0.09473684210526316,
          "f": 0.09278350015676509
        },
        "rouge-l": {
          "r": 0.23684210526315788,
          "p": 0.3103448275862069,
          "f": 0.26865671150813103
        }
      }
    },
    {
      "paper_id": "math.OA.math/OA/2503.03498v1",
      "true_abstract": "In this paper, we provide a comprehensive analysis of involutive quantales,\nwith a particular focus on quantic frames. We extend the axiomatic foundations\nof quantale-enriched topological spaces to include closure under the\nanti-homomorphic involution, facilitating a balanced topologization of the\nspectrum of unital $C^*$-algebras that encompasses both closed right and left\nideals through the concept of quantic frames. Specifically, certain subspaces\nof pure states are identified as strongly Hausdorff separated quantale-enriched\ninvolutive topological spaces.",
      "generated_abstract": "Quantale-enriched spaces are a class of topological spaces that have been\ndesigned as a generalization of topological spaces. Involutive quantale-enriched\nspaces are a particular class of such spaces. We show that they are\nequivalent to the class of involutive topological spaces. We also provide a\ncomprehensive characterization of involutive quantale-enriched spaces. In\nparticular, we show that any involutive quantale-enriched space is a\nquantale-enriched topological space. Furthermore, we show that any involutive\nquantale-enriched space is a topological space. We also provide a characterization\nof involutive quantale-enriched spaces that are not topological.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.29310344827586204,
          "p": 0.4722222222222222,
          "f": 0.36170212293345405
        },
        "rouge-2": {
          "r": 0.09859154929577464,
          "p": 0.11666666666666667,
          "f": 0.10687022404288817
        },
        "rouge-l": {
          "r": 0.27586206896551724,
          "p": 0.4444444444444444,
          "f": 0.3404255271887732
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/MN/2410.02463v1",
      "true_abstract": "In this study, the predominant lactic acid bacteria (LAB) isolates were\nobtained from Gouda, Jack, Cheddar, and Parmesan cheeses produced in Uganda.\nThe isolates were identified through Gram staining, catalase and oxidase tests,\nand 16S rDNA sequencing. Approximately 90% of the isolates were cocci (n=192),\nincluding Streptococcus, Enterococcus, and Lactococcus. The remaining 10% were\nidentified as rod-shaped bacteria, primarily belonging to the Lactobacillus\nspecies (n=23). BLAST analysis revealed that Pediococcus pentosaceus dominated\nin all cheese samples (23.7%, of the total 114 isolates). This was followed by\nuncultured bacterium (15.8%), uncultured Pediococcus species (13.2%),\nLacticaseibacillus rhamnosus (8.8%) among others",
      "generated_abstract": "y aimed to identify and characterize dominant microflora isolated\nfrom selected ripened cheese varieties produced in Uganda. The study used\nmicrobial culture collections of ripened cheese varieties, namely; Kigali,\nKibugu, and Bukusu, produced in Uganda. The microbial cultures were isolated\nfrom the cheese by using two methods: (1) the selective culture technique and\n(2) the enrichment culture technique. The isolates were identified using\nstandard biochemical tests. The microbial cultures were characterized using\nbiochemical tests, and the species of the isolates were identified. The\nmicrobial cultures were characterized using biochemical tests, and the species\nof the isolates were identified. The results of the selective culture technique\nindicated that the isolates belonged to the following species: Lact",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20238095238095238,
          "p": 0.30357142857142855,
          "f": 0.24285713805714293
        },
        "rouge-2": {
          "r": 0.10309278350515463,
          "p": 0.12195121951219512,
          "f": 0.11173183861053049
        },
        "rouge-l": {
          "r": 0.20238095238095238,
          "p": 0.30357142857142855,
          "f": 0.24285713805714293
        }
      }
    },
    {
      "paper_id": "physics.acc-ph.physics/acc-ph/2503.05192v1",
      "true_abstract": "Symplectic integrator plays a pivotal role in the long-term tracking of\ncharged particles within accelerators. To get symplectic maps in accurate\nsimulation of single-particle trajectories, two key components are addressed:\nprecise analytical expressions for arbitrary electromagnetic fields and a\nrobust treatment of the equations of motion. In a source-free region, the\nelectromagnetic fields can be decomposed into harmonic functions, applicable to\nboth scalar and vector potentials, encompassing both straight and curved\nreference trajectories. These harmonics are constructed according to the\nboundary surface's field data due to uniqueness theorem. Finding generating\nfunctions to meet the Hamilton-Jacobi equation via a perturbative ansatz, we\nderive symplectic maps that are independent of the expansion order. This method\nyields a direct mapping from initial to final coordinates in a large step,\nbypassing the transition of intermediate coordinates. Our developed\nparticle-tracking algorithm translates the field harmonics into beam\ntrajectories, offering a high-resolution integration method in accelerator\nsimulations.",
      "generated_abstract": "We introduce a new method of generating functions for the fields of a\nsystem of coupled harmonic oscillators in curved three-dimensional fields,\nbased on a symplectic form. The method is applied to the case of two\nharmonic oscillators in a uniform magnetic field. The new generating functions\nare obtained by using the symplectic form in a way similar to the\ngenerating functions for the fields of a system of coupled harmonic oscillators\nin flat three-dimensional fields. We show that the generating functions of the\nfields of a system of coupled harmonic oscillators in a uniform magnetic\nfield can be obtained by means of the new generating functions. We also show\nthat the new generating functions can be used to derive the generating\nfunctions for the fields of a system of coupled harmonic oscillators in curved\nthree-dimensional fields.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18018018018018017,
          "p": 0.425531914893617,
          "f": 0.25316455278240674
        },
        "rouge-2": {
          "r": 0.034013605442176874,
          "p": 0.06493506493506493,
          "f": 0.04464285263113885
        },
        "rouge-l": {
          "r": 0.16216216216216217,
          "p": 0.3829787234042553,
          "f": 0.2278480970862042
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.10230v1",
      "true_abstract": "Lucatelli Nunes obtained a 2-categorical version of the adjoint triangle\ntheorem of Dubuc using the descent object of a specific diagram. In some cases,\nsuch a diagram can be filled with an extra cell. We show then how to obtain a\nbiadjoint as an inverter of this additional datum (under suitable hypotheses).\nThe problem addressed here is slightly different however: we still have a\ntriangle of pseudofunctors but the lifted biadjoint is not the same. The\nconstruction is simplified when the pseudofunctor whose left biadjoint is\nsought is fully faithful. As an example, we get the biadjoint of the inclusion\npseudofunctor of a bicategory associated to a KZ-monad preserving\npseudomonicity.",
      "generated_abstract": "We study the biadjoint triangles of biadditive functions. In this paper, we\nadd two modifications to the definition of biadjoint triangle, which are\nessentially the biadditive versions of the two modifications studied by\nFan-Hou-Sun in [FHS19",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1038961038961039,
          "p": 0.2962962962962963,
          "f": 0.1538461500018492
        },
        "rouge-2": {
          "r": 0.018867924528301886,
          "p": 0.058823529411764705,
          "f": 0.02857142489387803
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.25925925925925924,
          "f": 0.13461538077108
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.02156v1",
      "true_abstract": "WiFi-based mobility monitoring in urban environments can provide valuable\ninsights into pedestrian and vehicle movements. However, MAC address\nrandomization introduces a significant obstacle in accurately estimating\ncongestion levels and path trajectories. To this end, we consider radio\nfrequency fingerprinting and re-identification for attributing WiFi traffic to\nemitting devices without the use of MAC addresses.\n  We present MobRFFI, an AI-based device fingerprinting and re-identification\nframework for WiFi networks that leverages an encoder deep learning model to\nextract unique features based on WiFi chipset hardware impairments. It is\nentirely independent of frame type. When evaluated on the WiFi fingerprinting\ndataset WiSig, our approach achieves 94% and 100% device accuracy in multi-day\nand single-day re-identification scenarios, respectively.\n  We also collect a novel dataset, MobRFFI, for granular multi-receiver WiFi\ndevice fingerprinting evaluation. Using the dataset, we demonstrate that the\ncombination of fingerprints from multiple receivers boosts re-identification\nperformance from 81% to 100% on a single-day scenario and from 41% to 100% on a\nmulti-day scenario.",
      "generated_abstract": "Intelligence (MI) plays a crucial role in the urban transportation\nsystem, providing crucial data for traffic management, vehicle routing, and\noperational optimization. However, collecting accurate mobility data in real\nworld scenarios remains challenging due to the high cost of sensors and the\nlimited mobility of users. This paper presents MobRFFI, a non-cooperative\ndevice-to-device mobility data fusion method that overcomes the above\nchallenges by leveraging the unique mobility patterns of users to enhance the\nquality of collected mobility data. First, we develop a novel mobility\nre-identification model to analyze the unique mobility patterns of users.\nSecond, we propose a device-to-device mobility fusion algorithm based on\ngeographic proximity and non-cooperative data fusion. Third, we integrate\nMobRFFI into a mobility-aware smart",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1981981981981982,
          "p": 0.2857142857142857,
          "f": 0.234042548355025
        },
        "rouge-2": {
          "r": 0.01948051948051948,
          "p": 0.028037383177570093,
          "f": 0.022988500909265415
        },
        "rouge-l": {
          "r": 0.1981981981981982,
          "p": 0.2857142857142857,
          "f": 0.234042548355025
        }
      }
    },
    {
      "paper_id": "physics.geo-ph.physics/geo-ph/2503.02815v1",
      "true_abstract": "Full waveform inversion (FWI) plays an important role in velocity modeling\ndue to its high-resolution advantages. However, its highly non-linear\ncharacteristic leads to numerous local minimums, which is known as the\ncycle-skipping problem. Therefore, effectively addressing the cycle-skipping\nissue is crucial to the success of FWI. Well-log data contain rich information\nabout subsurface medium parameters, providing inherent advantages for velocity\nmodeling. Traditional well-log data interpolation methods to build velocity\nmodels have limited accuracy and poor adaptability to complex geological\nstructures. This study introduces a well interpolation algorithm based on a\ngenerative diffusion model (GDM) to generate initial models for FWI, addressing\nthe cycle-skipping problem. Existing convolutional neural network (CNN)-based\nmethods face difficulties in handling complex feature distributions and lack\neffective uncertainty quantification, limiting the reliability of their\noutputs. The proposed GDM-based approach overcomes these challenges by\nproviding geologically consistent well interpolation while incorporating\nuncertainty assessment. Numerical experiments demonstrate that the method\nproduces accurate and reliable initial models, enhancing FWI performance and\nmitigating cycle-skipping issues.",
      "generated_abstract": "uce a method to generate initial velocity models (IVMs) for\nfull-waveform inversion (FWI) that are consistent with well and structural\nconstraints. The approach relies on the inversion of a time-dependent seismic\nwavefield using a Gaussian process (GP) interpolation model. The GP model is\nestimated from data using a variational Bayesian approach and is parameterized\nby a set of well and structural constraints. We show that the initial\nvelocity model can be accurately estimated from seismic data when the well\nconstraints are accurate. We validate our method by comparing the initial\nvelocity model with an IVM generated by the Kernel Interpolation Method (KIM)\nwith a GP model. The results demonstrate that the initial velocity model\ngenerates a consistent velocity model with the well and structural constraints,\nand it is accurate compared to the KIM-generated initial velocity model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.216,
          "p": 0.38571428571428573,
          "f": 0.27692307232084157
        },
        "rouge-2": {
          "r": 0.044585987261146494,
          "p": 0.061946902654867256,
          "f": 0.05185184698463694
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.35714285714285715,
          "f": 0.25641025180802113
        }
      }
    },
    {
      "paper_id": "cs.CR.q-bio/GN/2411.16744v1",
      "true_abstract": "Counting distinct permutations with replacement, especially when involving\nmultiple subwords, is a longstanding challenge in combinatorial analysis, with\ncritical applications in cryptography, bioinformatics, and statistical\nmodeling. This paper introduces a novel framework that presents closed-form\nformulas for calculating distinct permutations with replacement, fundamentally\nreducing the time complexity from exponential to linear relative to the\nsequence length for single-subword calculations. We then extend our\nfoundational formula to handle multiple subwords through the development of an\nadditional formula. Unlike traditional methods relying on brute-force\nenumeration or recursive algorithms, our approach leverages novel combinatorial\nconstructs and advanced mathematical techniques to achieve unprecedented\nefficiency. This comprehensive advancement in reducing computational complexity\nnot only simplifies permutation counting but also establishes a new benchmark\nfor scalability and versatility. We also demonstrate the practical utility of\nour formulas through diverse applications, including the simultaneous\nidentification of multiple genetic motifs in DNA sequences and complex pattern\nanalysis in cryptographic systems, using a computer program that runs the\nproposed formulae.",
      "generated_abstract": "on counting is the problem of counting the number of ways to\npermute a collection of objects. This problem arises in a wide range of\napplications, including word-sized image compression, counting the number of\nways to permute a collection of points in a graph, and counting the number of\nways to permute a collection of patterns in a system. These problems are\nfundamentally NP-hard, and it has been shown that the problem is NP-hard when\nthe input is a finite sequence of symbols. In this paper, we present a new\napproach to permutation counting that is NP-hard only when the input is a\nfinite sequence of symbols. Our approach is based on the idea that if a\nsubword of a word can be expressed as a product of subwords, then the number of\nways to permute the subword is equal to the number of ways to permute the\nsubwords.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19327731092436976,
          "p": 0.3382352941176471,
          "f": 0.2459893001847351
        },
        "rouge-2": {
          "r": 0.03184713375796178,
          "p": 0.04672897196261682,
          "f": 0.03787878305813881
        },
        "rouge-l": {
          "r": 0.15966386554621848,
          "p": 0.27941176470588236,
          "f": 0.20320855152163353
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.09325v1",
      "true_abstract": "We initiate the study of $\\lambda$-fold near-factorizations of groups with\n$\\lambda > 1$. While $\\lambda$-fold near-factorizations of groups with $\\lambda\n= 1$ have been studied in numerous papers, this is the first detailed treatment\nfor $\\lambda > 1$. We establish fundamental properties of $\\lambda$-fold\nnear-factorizations and introduce the notion of equivalence. We prove various\nnecessary conditions of $\\lambda$-fold near-factorizations, including upper\nbounds on $\\lambda$. We present three constructions of infinite families of\n$\\lambda$-fold near-factorizations, highlighting the characterization of two\nsubfamilies of $\\lambda$-fold near-factorizations. We discuss a computational\napproach to $\\lambda$-fold near-factorizations and tabulate computational\nresults for abelian groups of small order.",
      "generated_abstract": "We introduce the concept of a $\u03bb$-fold near-factorization of a group $G$,\nand show that this concept is equivalent to the existence of a universal\nnear-factorization of $G$. We also introduce the notion of a $\u03bb$-fold\nnear-factorization of a group $G$, and show that this concept is equivalent to\nthe existence of a universal near-factorization of $G$. Finally, we consider\nthe concept of a $\u03bb$-fold near-factorization of a group $G$, and show that\nthis concept is equivalent to the existence of a universal near-factorization\nof $G$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16129032258064516,
          "p": 0.4,
          "f": 0.2298850533756111
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.09090909090909091,
          "f": 0.05128204723208447
        },
        "rouge-l": {
          "r": 0.14516129032258066,
          "p": 0.36,
          "f": 0.2068965476284847
        }
      }
    },
    {
      "paper_id": "math.CO.math/RT/2503.06975v1",
      "true_abstract": "Given two affine permutations, some results of Lascoux and Deodhar, and\nindependently Jacon-Lecouvey, allow to decide if they are comparable for the\nstrong Bruhat order. These permutations are associated with tuples of core\npartitions, and the preceding problem is equivalent to compare the Young\ndiagrams in each components for the inclusion. Using abaci, we give an easy\nrule to compute these Young diagrams one another. We deduce a procedure to\ncompare, for the Bruhat order, two affine permutations in the window notation.",
      "generated_abstract": "The Bruhat order is a well-known order on the set of finite subsets of a\ngeneralized symmetric space. In this paper we show that the Bruhat order on\naffine symmetric groups is a suborder of the Bruhat order on the symmetric\nspace. This result extends a result of Ginzburg, who showed that the Bruhat\norder on the symmetric space is the unique order on the set of finite subsets\nof the symmetric space that satisfies a number of axioms. In addition, we show\nthat the Bruhat order on affine symmetric groups can be defined by a\ngeneralization of the notion of a suborder. We also show that the Bruhat order\non affine symmetric groups is a suborder of the Coxeter order on the group.\nFinally, we show that the Bruhat order on affine symmetric groups can be\ndefined by a generalization of the notion of a suborder.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14754098360655737,
          "p": 0.1956521739130435,
          "f": 0.16822429416368254
        },
        "rouge-2": {
          "r": 0.025974025974025976,
          "p": 0.0273972602739726,
          "f": 0.02666666167022316
        },
        "rouge-l": {
          "r": 0.14754098360655737,
          "p": 0.1956521739130435,
          "f": 0.16822429416368254
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.04565v1",
      "true_abstract": "Panoramic imagery, with its 360{\\deg} field of view, offers comprehensive\ninformation to support Multi-Object Tracking (MOT) in capturing spatial and\ntemporal relationships of surrounding objects. However, most MOT algorithms are\ntailored for pinhole images with limited views, impairing their effectiveness\nin panoramic settings. Additionally, panoramic image distortions, such as\nresolution loss, geometric deformation, and uneven lighting, hinder direct\nadaptation of existing MOT methods, leading to significant performance\ndegradation. To address these challenges, we propose OmniTrack, an\nomnidirectional MOT framework that incorporates Tracklet Management to\nintroduce temporal cues, FlexiTrack Instances for object localization and\nassociation, and the CircularStatE Module to alleviate image and geometric\ndistortions. This integration enables tracking in large field-of-view\nscenarios, even under rapid sensor motion. To mitigate the lack of panoramic\nMOT datasets, we introduce the QuadTrack dataset--a comprehensive panoramic\ndataset collected by a quadruped robot, featuring diverse challenges such as\nwide fields of view, intense motion, and complex environments. Extensive\nexperiments on the public JRDB dataset and the newly introduced QuadTrack\nbenchmark demonstrate the state-of-the-art performance of the proposed\nframework. OmniTrack achieves a HOTA score of 26.92% on JRDB, representing an\nimprovement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing the\nbaseline by 6.81%. The dataset and code will be made publicly available at\nhttps://github.com/xifen523/OmniTrack.",
      "generated_abstract": "This paper presents a multi-object tracking framework that enables real-time\ntracking of both 2D and 3D objects. The proposed framework consists of a\nmulti-object detector and an adaptive multistage tracking algorithm that\nintegrates multi-object feature extraction, object classification, and\ntracking. The proposed framework is designed to support both omnidirectional\nand horizontal-vertical object tracking, enabling tracking of both 2D and 3D\nobjects in complex scenes. The proposed framework is implemented using\nPyTorch, with pre-trained detection models and multi-object feature extraction\nmodels. The framework is evaluated using the Real-Time Object Tracking\nBenchmark (RTOTB) dataset. The results demonstrate that the proposed framework\nis capable of tracking both 2D and 3D objects with high precision and\nefficiency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14556962025316456,
          "p": 0.3709677419354839,
          "f": 0.20909090504297526
        },
        "rouge-2": {
          "r": 0.019138755980861243,
          "p": 0.043478260869565216,
          "f": 0.026578068845156913
        },
        "rouge-l": {
          "r": 0.13291139240506328,
          "p": 0.3387096774193548,
          "f": 0.1909090868611571
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.07997v1",
      "true_abstract": "Autonomous stores leverage advanced sensing technologies to enable\ncashier-less shopping, real-time inventory tracking, and seamless customer\ninteractions. However, these systems face significant challenges, including\nocclusion in vision-based tracking, scalability of sensor deployment, theft\nprevention, and real-time data processing. To address these issues, researchers\nhave explored multi-modal sensing approaches, integrating computer vision,\nRFID, weight sensing, vibration-based detection, and LiDAR to enhance accuracy\nand efficiency. This survey provides a comprehensive review of sensing\ntechnologies used in autonomous retail environments, highlighting their\nstrengths, limitations, and integration strategies. We categorize existing\nsolutions across inventory tracking, environmental monitoring, people-tracking,\nand theft detection, discussing key challenges and emerging trends. Finally, we\noutline future directions for scalable, cost-efficient, and privacy-conscious\nautonomous store systems.",
      "generated_abstract": "s retail systems (ARS) are emerging technologies that enable\nretailers to monitor and control the physical environment of their stores,\nenhancing store operations and customer experience. These systems are\nincorporating various sensing technologies, such as cameras, sensors, and\nradars, to provide accurate and real-time information. However, they face\nchallenges such as limited sensing range, low sensitivity, and high energy\nconsumption. This paper presents a comprehensive review of ARS sensing\ntechnologies and their challenges. It explores the current status of\nsensing technologies and the limitations of traditional sensors in ARS. It also\ndiscusses the integration of sensing technologies in ARS systems and their\neffects on energy efficiency, cost, and data storage. This study provides\ninsights into the technological advancements and future directions for ARS\nsystem",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2872340425531915,
          "p": 0.32142857142857145,
          "f": 0.3033707815326348
        },
        "rouge-2": {
          "r": 0.07079646017699115,
          "p": 0.06896551724137931,
          "f": 0.06986899063404624
        },
        "rouge-l": {
          "r": 0.26595744680851063,
          "p": 0.2976190476190476,
          "f": 0.28089887142027525
        }
      }
    },
    {
      "paper_id": "cs.SC.cs/SC/2503.07342v1",
      "true_abstract": "In this work, we introduce a novel variant of the multivariate quadratic\nproblem, which is at the core of one of the most promising post-quantum\nalternatives: multivariate cryptography. In this variant, the solution of a\ngiven multivariate quadratic system must also be regular, i.e. if it is split\ninto multiple blocks of consecutive entries with the same fixed length, then\neach block has only one nonzero entry. We prove the NP-completeness of this\nvariant and show similarities and differences with other computational problems\nused in cryptography. Then we analyze its hardness by reviewing the most common\nsolvers for polynomial systems over finite fields, derive asymptotic formulas\nfor the corresponding complexities and compare the different approaches.",
      "generated_abstract": "The regular multivariate quadratic problem is a problem in which the objective\nfunction is a quadratic function of several variables and the constraint\nfunctions are polynomials of the same degree. In this paper, we investigate the\nproblem of finding an optimal solution for the regular multivariate quadratic\nproblem. First, we present a linear programming formulation of the regular\nmultivariate quadratic problem and derive the necessary and sufficient\nconditions for the linear program to have an optimal solution. Then, we\npropose a polynomial-time algorithm to solve the linear program and show that\nthe resulting solution is unique. Moreover, we prove that the optimal solution\nis a linear combination of the constraints. Finally, we provide some\nillustrative examples to demonstrate the effectiveness of our proposed\nalgorithms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20930232558139536,
          "p": 0.26865671641791045,
          "f": 0.235294112724166
        },
        "rouge-2": {
          "r": 0.05405405405405406,
          "p": 0.05825242718446602,
          "f": 0.05607476136212815
        },
        "rouge-l": {
          "r": 0.18604651162790697,
          "p": 0.23880597014925373,
          "f": 0.2091503218744928
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.06638v1",
      "true_abstract": "In the paper the joint optimization of uplink multiuser power and resource\nblock (RB) allocation are studied, where each user has quality of service (QoS)\nconstraints on both long- and short-blocklength transmissions. The objective is\nto minimize the consumption of RBs for meeting the QoS requirements, leading to\na mixed-integer nonlinear programming (MINLP) problem. We resort to deep\nlearning to solve the problem with low inference complexity. To provide a\nperformance benchmark for learning based methods, we propose a hierarchical\nalgorithm to find the global optimal solution in the single-user scenario,\nwhich is then extended to the multiuser scenario. The design of the learning\nmethod, however, is challenging due to the discrete policy to be learned, which\nresults in either vanishing or exploding gradient during neural network\ntraining. We introduce two types of smoothing functions to approximate the\ninvolved discretizing processes and propose a smoothing parameter adaption\nmethod. Another critical challenge lies in guaranteeing the QoS constraints. To\naddress it, we design a nonlinear function to intensify the penalties for minor\nconstraint violations. Simulation results demonstrate the advantages of the\nproposed method in reducing the number of occupied RBs and satisfying QoS\nconstraints reliably.",
      "generated_abstract": "This paper presents a learning-based uplink resource allocation algorithm\nfor multiuser communications with QoS constraints. The proposed method is\nbased on a reinforcement learning approach and is designed to balance\nresource allocation efficiency and QoS guarantee. The algorithm is designed to\naddress the challenges of dynamic channel states and interference contamination\nin uplink multiuser communications. The proposed approach is based on a\nreinforcement learning method, and it is designed to balance resource allocation\nefficiency and QoS guarantee. The algorithm is designed to address the\nchallenges of dynamic channel states and interference contamination in uplink\nmultiuser communications. The proposed algorithm is applied to a multiuser\nchannel with interference contamination, and it achieves a throughput of 2.1\nBPSK symbols/s/Hz.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19230769230769232,
          "p": 0.5,
          "f": 0.27777777376543217
        },
        "rouge-2": {
          "r": 0.026595744680851064,
          "p": 0.06666666666666667,
          "f": 0.03802280961124247
        },
        "rouge-l": {
          "r": 0.17692307692307693,
          "p": 0.46,
          "f": 0.2555555515432099
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/PL/2503.07328v1",
      "true_abstract": "Reachability Types (RT) are a qualified type system for tracking aliasing and\nseparation in functional and higher-order programming. By formalizing resource\nreachability with a sound static type system, RT enable higher-order\nprogramming patterns with runtime safety and non-interference guarantees.\nHowever, previous RT systems have been based on calculi that restrict cyclic\ndependencies and are shown to be terminating in the absence of built-in\nrecursive constructs. While termination is sometimes a desirable property,\nsimplifying reasoning and ensuring predictable behavior, it implies an\ninability to encode expressive programs involving non-termination and advanced\nrecursive patterns, such as mutual recursion and various fixed-point\ncombinators.\n  In this paper, we address this limitation by extending RT with an expressive\ncyclic reference type that permits the formation of cyclic dependencies through\nthe store, thereby allowing the system to encode recursive programming patterns\nwithout relying on extra built-in constructs. In addition, we redesign\nqualifier typing in the reference introduction rule, allowing separate\nreferences to point to a shared and tracked referent. We formalize the system\nas the $\\lambda^{\\circ}_{<:}$-calculus, with a mechanized soundness proof via\nthe standard progress and preservation lemmas. As a demonstration, we implement\na well-typed fixpoint operator, proving that recursive patterns can be encoded\nusing the novel cyclic reference type.",
      "generated_abstract": "The Cyclic Reference Problem (CRP) is a classical algorithmic problem,\ndesigned to analyze the behavior of programs with circular references.\nSince the 1980s, researchers have been studying different approaches to solve\nthis problem, focusing on the search algorithm and the design of more efficient\nalgorithms. We present a novel approach to CRP that uses reachability types to\nsolve the problem. In this approach, we represent the behavior of a program by\na set of reachability types, and we solve the CRP by checking whether these\ntypes are consistent. We prove that the solution of our approach is\ndeterministic and polynomial-time. We also show that our approach can be used\nto solve the more general Problem with Circular Reference (PCR), where the\nreference is not circular, but may be cyclic. In both problems, we show that\nour approach is also consistent and polynomial-time.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18518518518518517,
          "p": 0.2976190476190476,
          "f": 0.22831049755426294
        },
        "rouge-2": {
          "r": 0.020618556701030927,
          "p": 0.031007751937984496,
          "f": 0.024767797060070516
        },
        "rouge-l": {
          "r": 0.14814814814814814,
          "p": 0.23809523809523808,
          "f": 0.18264839709764197
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.08311v1",
      "true_abstract": "Inference in linear panel data models is complicated by the presence of fixed\neffects when (some of) the regressors are not strictly exogenous. Under\nasymptotics where the number of cross-sectional observations and time periods\ngrow at the same rate, the within-group estimator is consistent but its limit\ndistribution features a bias term. In this paper we show that a panel version\nof the moving block bootstrap, where blocks of adjacent cross-sections are\nresampled with replacement, replicates the limit distribution of the\nwithin-group estimator. Confidence ellipsoids and hypothesis tests based on the\nreverse-percentile bootstrap are thus asymptotically valid without the need to\ntake the presence of bias into account.",
      "generated_abstract": "r presents a novel procedure for the estimation and inference in\ndynamic panel models using the moving block bootstrap. The method is designed\nto handle the presence of serial correlation and the non-identifiability of\nthe covariance matrix of the time-varying parameters in the model. The key\ningredients of the method are the construction of two moving blocks, each\nconsisting of a set of blocks that are conditionally independent given the\ncurrent block and a set of blocks that are conditionally independent given the\nprevious block. The resulting moving block bootstrap is a generalization of the\nmoving block bootstrap proposed by Lanski and Van Der Linden (2005). The\nproposed method is applied to a dynamic panel data model where the\ntime-varying parameters are the average hourly wage of the panelists over the\ntime period of the observations. The proposed method is compared with the\nproposed method of Lans",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.275,
          "p": 0.3013698630136986,
          "f": 0.28758169435687136
        },
        "rouge-2": {
          "r": 0.06930693069306931,
          "p": 0.059322033898305086,
          "f": 0.06392693566939844
        },
        "rouge-l": {
          "r": 0.2125,
          "p": 0.2328767123287671,
          "f": 0.22222221723268842
        }
      }
    },
    {
      "paper_id": "eess.IV.q-bio/TO/2411.00749v1",
      "true_abstract": "Accurate survival prediction is essential for personalized cancer treatment.\nHowever, genomic data - often a more powerful predictor than pathology data -\nis costly and inaccessible. We present the cross-modal genomic feature\ntranslation and alignment network for enhanced survival prediction from\nhistopathology images (PathoGen-X). It is a deep learning framework that\nleverages both genomic and imaging data during training, relying solely on\nimaging data at testing. PathoGen-X employs transformer-based networks to align\nand translate image features into the genomic feature space, enhancing weaker\nimaging signals with stronger genomic signals. Unlike other methods, PathoGen-X\ntranslates and aligns features without projecting them to a shared latent space\nand requires fewer paired samples. Evaluated on TCGA-BRCA, TCGA-LUAD, and\nTCGA-GBM datasets, PathoGen-X demonstrates strong survival prediction\nperformance, emphasizing the potential of enriched imaging models for\naccessible cancer prognosis.",
      "generated_abstract": "survival prediction for cancer patients is a critical challenge in\nchallenging clinical settings, such as the National Cancer Institute's\nMulti-Institutional Cancer Prediction Consortium (MIPCC). However, traditional\ndeep learning-based methods often struggle with the inherent challenges of\nhistopathology image data, such as image variability, low resolution, and\nsmaller spatial resolution. To address this, we propose a novel genomic feature\ntrans-align network (PathoGen-X) for enhanced survival prediction from\nhistopathology images. PathoGen-X leverages the patho-genomic signature to\nalign the genomic features with the histopathology image features, enhancing\nthe prediction accuracy. Our method employs a multi-layer perceptron (MLP) to\nencode the genomic features, followed by a convolutional neural network (",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.29896907216494845,
          "p": 0.3625,
          "f": 0.3276836108653325
        },
        "rouge-2": {
          "r": 0.078125,
          "p": 0.1,
          "f": 0.08771929332102213
        },
        "rouge-l": {
          "r": 0.25773195876288657,
          "p": 0.3125,
          "f": 0.2824858707523381
        }
      }
    },
    {
      "paper_id": "math.AP.math/AP/2503.09307v1",
      "true_abstract": "We consider a broad class of nonlinear integro-differential equations with a\nkernel whose differentiability order is described by a general function $\\phi$.\nThis class includes not only the fractional $p$-Laplace equations, but also\nborderline cases when the fractional order approaches $1$. Under mild\nassumptions on $\\phi$, we establish sharp Sobolev-Poincar\\'e type inequalities\nfor the associated Sobolev spaces, which are connected to a question raised by\nBrezis (Russian Math. Surveys 57:693--708, 2002). Using these inequalities, we\nprove H\\\"older regularity and Harnack inequalities for weak solutions to such\nnonlocal equations. All the estimates in our results remain stable as the\nassociated nonlocal energy functional approaches its local counterpart.",
      "generated_abstract": "We study nonlocal equations with kernels of general order. We show that the\nnonlocal equation is equivalent to the system of coupled ordinary differential\nequations with kernels of general order. In particular, we consider the case of\nnonlocal equation with kernels of order two. We prove that the solution of the\nnonlocal equation is given by the solution of the system of coupled ordinary\ndifferential equations with kernels of order two. We also prove that the\nsolution of the nonlocal equation is given by the solution of the system of\ncoupled ordinary differential equations with kernels of general order.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1724137931034483,
          "p": 0.4838709677419355,
          "f": 0.25423728426170644
        },
        "rouge-2": {
          "r": 0.0196078431372549,
          "p": 0.043478260869565216,
          "f": 0.027027022742878694
        },
        "rouge-l": {
          "r": 0.1724137931034483,
          "p": 0.4838709677419355,
          "f": 0.25423728426170644
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.cond-mat/mtrl-sci/2503.10373v1",
      "true_abstract": "Topotactic reduction of perovskite oxides offers a powerful approach for\ndiscovering novel phenomena, such as superconducting infinite-layer nickelates\nand polar metallicity, and is commonly accompanied by the emergence of multiple\nvalence states and/or complex crystal fields of transition metals. However,\nunderstanding the complex interplay between crystal chemistry, electronic\nstructure, and physical properties at the spin- and orbital-resolved levels in\nthese reduced systems remains elusive. Here, we combine x-ray absorption\nspectroscopy, resonant inelastic x-ray scattering (RIXS), and density\nfunctional theory calculations to uncover topotactic metal-insulator transition\nand orbital-specific crystal field excitations in brownmillerite\nLa0.67Ca0.33MnO2.5 thin films. We reveal the Mn valence states to be\npredominantly Mn2+/Mn3+, along with their corresponding populations at\noctahedral and tetrahedral sites, which effectively weaken the Mn-O\nhybridization compared to the parent perovskite phase. As a result,\nLa0.67Ca0.33MnO2.5 films exhibit an antiferromagnetic insulating ground state.\nMoreover, by combining the RIXS measurements on selected single-valence\nmanganites, specifically MnO, LaMnO3, and CaMnO3, with orbital- and\nspin-resolved density-of-states calculations, we identify the dd excitations of\noctahedrally and tetrahedrally coordinated Mn2+/Mn3+ ions, directly linking the\nmicroscopic electronic structure to the macroscopic magnetic/electrical\nproperties.",
      "generated_abstract": "nsic excitonic properties of brownmillerite manganite thin films\nare expected to exhibit a rich variety of topological excitations, including\nbifurcations of the electronic band structure and novel crystal field\nexcitations. We demonstrate that these excitations can be observed experimentally\nusing a topotactic reduction-driven spin-polarized transport method. By\ncomparing the topotactic reduction-driven transport data of a\n\\textbf{La}Mn$_{1-x}$Sr$_x$CuO$_4$ thin film with the transport data of a\n\\textbf{La}Mn$_{1-x}$Sr$_x$CuO$_4$ bulk sample, we identify the existence of\ntopotactic excitations and reveal the role of the crystal field in their\nformation. We also show that the topotactic excitations of the thin film",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18309859154929578,
          "p": 0.43333333333333335,
          "f": 0.2574257383981963
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.08333333333333333,
          "f": 0.05263157462603914
        },
        "rouge-l": {
          "r": 0.15492957746478872,
          "p": 0.36666666666666664,
          "f": 0.2178217780021567
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.physics/comp-ph/2503.09328v1",
      "true_abstract": "A Schr\\\"odinger bridge is the most probable time-dependent probability\ndistribution that connects an initial probability distribution $w_{i}$ to a\nfinal one $w_{f}$. The problem has been solved and widely used for the case of\nsimple Brownian evolution (non-interacting particles). It is related to the\nproblem of entropy regularized Wasserstein optimal transport. In this article,\nwe generalize Brownian bridges to systems of interacting particles. We derive\nsome equations for the forward and backward single particle ``wave-functions''\nwhich allow to compute the most probable evolution of the single-particle\nprobability between the initial and final distributions.",
      "generated_abstract": "t a general method to construct quantum circuits that mimic the\nSchr\u00f6dinger dynamics of interacting particles. The method relies on the\nconstruction of bridges, which can be seen as a type of quantum circuit that\ncombines gates with a non-trivial time-dependent boundary condition. We apply\nthe method to a model of interacting particles in a harmonic potential,\ndemonstrating its use in simulating the dynamics of a model of interacting\nquantum particles. We find that the quantum circuits that we construct have a\nsimilar structure to the Schr\u00f6dinger bridge, with a time-dependent boundary\ncondition and a boundary layer at the middle of the bridge. We also demonstrate\nthat the quantum circuits that we construct can be used to simulate quantum\ndynamics that are not directly accessible to classical simulators, such as\ndynamics that involve a jump between two quantum states, or a non",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23880597014925373,
          "p": 0.21621621621621623,
          "f": 0.2269503496222525
        },
        "rouge-2": {
          "r": 0.06818181818181818,
          "p": 0.05,
          "f": 0.057692302810651304
        },
        "rouge-l": {
          "r": 0.19402985074626866,
          "p": 0.17567567567567569,
          "f": 0.18439715813289087
        }
      }
    },
    {
      "paper_id": "cs.NI.cs/NI/2503.06883v1",
      "true_abstract": "Semantic communication has emerged as a transformative paradigm in\nnext-generation communication systems, leveraging advanced artificial\nintelligence (AI) models to extract and transmit semantic representations for\nefficient information exchange. Nevertheless, the presence of unpredictable\nsemantic noise, such as ambiguity and distortions in transmitted\nrepresentations, often undermines the reliability of received information.\nConventional approaches primarily adopt adversarial training with noise\ninjection to mitigate the adverse effects of noise. However, such methods\nexhibit limited adaptability to varying noise levels and impose additional\ncomputational overhead during model training. To address these challenges, this\npaper proposes Noise-Resilient \\textbf{Se}mantic Communication with\n\\textbf{Hi}gh-and-\\textbf{Lo}w Frequency Decomposition (Se-HiLo) for image\ntransmission. The proposed Se-HiLo incorporates a Finite Scalar Quantization\n(FSQ) based noise-resilient module, which bypasses adversarial training by\nenforcing encoded representations within predefined spaces to enhance noise\nresilience. While FSQ improves robustness, it compromise representational\ndiversity. To alleviate this trade-off, we adopt a transformer-based\nhigh-and-low frequency decomposition module that decouples image\nrepresentations into high-and-low frequency components, mapping them into\nseparate FSQ representation spaces to preserve representational diversity.\nExtensive experiments demonstrate that Se-HiLo achieves superior noise\nresilience and ensures accurate semantic communication across diverse noise\nenvironments.",
      "generated_abstract": "t a novel method for noise-resilient communication in dense\ncommunication networks. The method leverages the power of a novel\ncompression-based decomposition of the input signal into three components\n(high-frequency, medium-frequency, and low-frequency) and combines them in a\nsemantic manner, thus providing higher resilience against noise. Our method\nemploys a novel formulation for the problem of semantic communication, where\nthe noise is assumed to be additive and has a low-frequency component. We\npresent an efficient method for decomposing the signal into high-frequency,\nmedium-frequency, and low-frequency components. We then introduce a novel\nmethod for noise-resilient communication, which is based on the decomposition\nof the input signal. We show that the noise-resilient communication can be\nachieved by transmitting a subset of the high-frequency and medium",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15827338129496402,
          "p": 0.3142857142857143,
          "f": 0.21052631133444755
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14388489208633093,
          "p": 0.2857142857142857,
          "f": 0.19138755535358634
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/MF/2411.11522v1",
      "true_abstract": "This paper presents comparison results and establishes risk bounds for credit\nportfolios within classes of Bernoulli mixture models, assuming conditionally\nindependent defaults that are stochastically increasing with a common risk\nfactor. We provide simple and interpretable conditions for conditional default\nprobabilities that imply a comparison of credit portfolio losses in convex\norder. In the case of threshold models, the ranking of portfolio losses is\nbased on a pointwise comparison of the underlying copulas. Our setting includes\nas special case the well-known Gaussian copula model but allows for general\ntail dependencies, which are crucial for modeling credit portfolio risks.\nMoreover, our results extend the classical parameterized models, such as the\nindustry models CreditMetrics and KMV Portfolio Manager, to a robust setting\nwhere individual parameters or the copula modeling the dependence structure can\nbe ambiguous. A simulation study and a real data example under model\nuncertainty offer evidence supporting the effectiveness of our approach.",
      "generated_abstract": "We consider the estimation of a risk-neutral Bernoulli mixture model\nwith parameterized transition functions for the credit portfolio. We first\nintroduce a class of parametric and non-parametric estimators for the model\nparameters and show that the maximum likelihood estimators are consistent and\nhave finite asymptotic variances. We then propose a robust estimator that\nconsiders the effect of out-of-sample data, and we establish its consistency\nand asymptotic normality. Finally, we apply our methodology to a real-world\ndataset from the European Banking Authority.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1743119266055046,
          "p": 0.3220338983050847,
          "f": 0.22619047163336178
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.05063291139240506,
          "f": 0.03524228621087214
        },
        "rouge-l": {
          "r": 0.1559633027522936,
          "p": 0.288135593220339,
          "f": 0.20238094782383798
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.19947v1",
      "true_abstract": "Motivated by studying the effects of marriage prospects on students' college\nmajor choices, this paper develops a new econometric test for analyzing the\neffects of an unobservable factor in a setting where this factor potentially\ninfluences both agents' decisions and a binary outcome variable. Our test is\nbuilt upon a flexible copula-based estimation procedure and leverages the\nordered nature of latent utilities of the polychotomous choice model. Using the\nproposed method, we demonstrate that marriage prospects significantly influence\nthe college major choices of college graduates participating in the National\nLongitudinal Study of Youth (97) Survey. Furthermore, we validate the\nrobustness of our findings with alternative tests that use stated marriage\nexpectation measures from our data, thereby demonstrating the applicability and\nvalidity of our testing procedure in real-life scenarios.",
      "generated_abstract": "This study examines the impact of a prospective partner's characteristics on\ncollege major choice using a large nationally representative sample of U.S.\nundergraduates. Using data from the National Longitudinal Survey of Youth,\nfollow-up data from 2009-2013, we estimate a dynamic model that incorporates\nthe partner's characteristics at time 1, allowing for the possibility of\npotential outcomes. We find that marriage prospects are significantly associated\nwith college major choice, with the effect being stronger for women and\nearlier graduation. Marriage prospects also significantly influence the\npreference for the major of their partner. This study is the first to examine\nthe impact of a prospective partner's characteristics on college major\nchoice using a longitudinal survey data set.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25274725274725274,
          "p": 0.3108108108108108,
          "f": 0.2787878738409551
        },
        "rouge-2": {
          "r": 0.05737704918032787,
          "p": 0.07142857142857142,
          "f": 0.06363635869586816
        },
        "rouge-l": {
          "r": 0.2087912087912088,
          "p": 0.25675675675675674,
          "f": 0.23030302535610664
        }
      }
    },
    {
      "paper_id": "math.SG.math/SG/2503.10283v1",
      "true_abstract": "Given a closed connected symplectic manifold $(M,\\omega)$, we construct an\nalternating $\\mathbb{R}$-bilinear form\n$\\mathfrak{b}=\\mathfrak{b}_{\\mu_{\\mathrm{Sh}}}$ on the real first cohomology of\n$M$ from Shelukhin's quasimorphism $\\mu_{\\mathrm{Sh}}$. Here\n$\\mu_{\\mathrm{Sh}}$ is defined on the universal cover of the group of\nHamiltonian diffeomorphisms on $(M,\\omega)$. This bilinear form is invariant\nunder the symplectic mapping class group action, and $\\mathfrak{b}$ yields a\nconstraint on the fluxes of commuting two elements in the group of\nsymplectomorphisms on $(M,\\omega)$. These results might be seen as an analog of\nRousseau's result for an open connected symplectic manifold, where he recovered\nthe symplectic pairing from the Calabi homomorphism. Furthermore,\n$\\mathfrak{b}$ controls the extendability of Shelukhin's quasimorphisms, as\nwell as the triviality of a characteristic class of Reznikov. To construct\n$\\mathfrak{b}$, we build general machinery for a group $G$ of producing a\nreal-valued $\\mathbb{Z}$-bilinear form $\\mathfrak{b}_{\\mu}$ from a\n$G$-invariant quasimorphism $\\mu$ on the commutator subgroup of $G$.",
      "generated_abstract": "uct the Shelukhin's quasimorphism and its induced bilinear form\n$(\\cdot, \\cdot)_S$, which is a bilinear form on $H_2(Y;\\mathbb{Z}_2)$ defined by\nthe following formula:\n$$(\\alpha,\\beta)_S=\\sum_{n=1}^\\infty (-1)^n\\langle \\alpha,\\beta\\rangle\n\\cdot\\sum_{e\\in E_n}\\mu(e)c_e,$$\nwhere $c_e$ is the characteristic function of the edge $e$ of $Y$, and\n$\\mu(e)$ is the sign of the orientation of the edge $e$ of $Y$.\n  We also introduce a new quasimorphism on $H_2(Y;\\mathbb{Z}_2)$ constructed\nfrom the Shelukhin's quasimorphism by adding the sign $\\mp1$ to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.30434782608695654,
          "f": 0.20437955758324908
        },
        "rouge-2": {
          "r": 0.028985507246376812,
          "p": 0.06557377049180328,
          "f": 0.040201000773718285
        },
        "rouge-l": {
          "r": 0.13186813186813187,
          "p": 0.2608695652173913,
          "f": 0.17518247729127828
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.09767v1",
      "true_abstract": "Classical unsupervised learning methods like clustering and linear\ndimensionality reduction parametrize large-scale geometry when it is discrete\nor linear, while more modern methods from manifold learning find low\ndimensional representation or infer local geometry by constructing a graph on\nthe input data. More recently, topological data analysis popularized the use of\nsimplicial complexes to represent data topology with two main methodologies:\ntopological inference with geometric complexes and large-scale topology\nvisualization with Mapper graphs -- central to these is the nerve construction\nfrom topology, which builds a simplicial complex given a cover of a space by\nsubsets. While successful, these have limitations: geometric complexes scale\npoorly with data size, and Mapper graphs can be hard to tune and only contain\nlow dimensional information. In this paper, we propose to study the problem of\nlearning covers in its own right, and from the perspective of optimization. We\ndescribe a method for learning topologically-faithful covers of geometric\ndatasets, and show that the simplicial complexes thus obtained can outperform\nstandard topological inference approaches in terms of size, and Mapper-type\nalgorithms in terms of representation of large-scale topology.",
      "generated_abstract": "has emerged as a key component in various scientific fields,\nfrom physics and engineering to computer science and biology. While\nrepresentations of topology, such as the Euclidean lattice and the graph, have\nbeen extensively studied, their relationships remain unclear, hindering\ncomparisons of their properties and identifying new topological structures.\nExisting methods for learning topological representations are limited in\neffectiveness and generalizability, often requiring specific data distributions\nand training procedures. To address these limitations, we introduce\nCoverLearn, a novel method for learning topological representations that\noptimizes a metric-based cover learner. CoverLearn utilizes an iterative\nbackpropagation process that leverages the cover learner to refine topological\nrepresentations. The algorithm is highly efficient, with a computational\ncomplexity of O(N log N), where N is the number of data points. We demonstrate\nthe effect",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19827586206896552,
          "p": 0.23711340206185566,
          "f": 0.21596243635433898
        },
        "rouge-2": {
          "r": 0.017341040462427744,
          "p": 0.024390243902439025,
          "f": 0.02027026541293944
        },
        "rouge-l": {
          "r": 0.1810344827586207,
          "p": 0.21649484536082475,
          "f": 0.19718309363133432
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.04488v1",
      "true_abstract": "In a recent article [13], G. Janelidze introduced the concept of ideally\nexact categories as a generalization of semi-abelian categories, aiming to\nincorporate relevant examples of non-pointed categories, such as the categories\n$\\textbf{Ring}$ and $\\textbf{CRing}$ of unitary (commutative) rings. He also\nextended the notion of action representability to this broader framework,\nproving that both $\\textbf{Ring}$ and $\\textbf{CRing}$ are action\nrepresentable.\n  This article investigates the representability of actions of unitary\nnon-associative algebras. After providing a detailed description of the monadic\nadjunction associated with any category of unitary algebra, we use the\nconstruction of the external weak actor [4] in order to prove that the\ncategories of unitary (commutative) associative algebras and that of unitary\nalternative algebras are action representable. The result is then extended for\nunitary (commutative) Poisson algebras, where the explicit construction of the\nuniversal strict general actor is employed.",
      "generated_abstract": "We introduce the notion of action representability for actions of unital\nalgebraic groups on topological spaces and prove that it is a stable property\nunder arbitrary homotopy. We show that for a large class of examples of\nalgebraic groups, the action representations are in particular representable\nalgebras and characterize which are the groups that have action representations\nas modules. As an application, we show that the group of $C^*$-dynamics of a\nHilbert space is representable and that any $C^*$-dynamics on a Hilbert space\nis given by a finitely generated representation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23255813953488372,
          "p": 0.35714285714285715,
          "f": 0.2816901360682405
        },
        "rouge-2": {
          "r": 0.08,
          "p": 0.11904761904761904,
          "f": 0.09569377509672422
        },
        "rouge-l": {
          "r": 0.19767441860465115,
          "p": 0.30357142857142855,
          "f": 0.23943661494147994
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.cond-mat/str-el/2503.09717v1",
      "true_abstract": "We clarify the lore that anomaly-free symmetries are either on-site or can be\ntransformed into on-site symmetries. We prove that any finite, internal,\nanomaly-free symmetry in a 1+1d lattice Hamiltonian system can be disentangled\ninto an on-site symmetry by introducing ancillas and applying conjugation via a\nfinite-depth quantum circuit. We provide an explicit construction of the\ndisentangling circuit using Gauss's law operators and emphasize the necessity\nof adding ancillas. Our result establishes the converse to a generalized\nLieb-Schultz-Mattis theorem by demonstrating that any anomaly-free symmetry\nadmits a trivially gapped Hamiltonian.",
      "generated_abstract": "um spin-chain model has long been recognized as a valuable platform\nfor studying the physics of complex systems, and has been extensively studied in\nthe literature. In recent years, the study of symmetry-protected topological\n(SPT) phases of quantum spin chains has become increasingly important. One of\nthe main objectives of this study is to identify the SPT phases of quantum\nspin chains, and the identification of SPT phases of quantum spin chains is\nusually achieved by analyzing their spectral properties. However, in the\nclassification of SPT phases, one often faces the problem of identifying\nanomaly-free symmetries. Anomaly-free symmetries are important because they\ncan be used to reduce the complexity of the classification of SPT phases. In\nthis paper, we identify a class of SPT phases of quantum spin chains, which are\nnot identified by any symmetry. We show that this class",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27419354838709675,
          "p": 0.20238095238095238,
          "f": 0.23287670744229697
        },
        "rouge-2": {
          "r": 0.046511627906976744,
          "p": 0.03361344537815126,
          "f": 0.039024385373468785
        },
        "rouge-l": {
          "r": 0.27419354838709675,
          "p": 0.20238095238095238,
          "f": 0.23287670744229697
        }
      }
    },
    {
      "paper_id": "cs.CY.econ/GN/2501.19407v2",
      "true_abstract": "Surnames often convey implicit markers of social status, wealth, and lineage,\nshaping perceptions in ways that can perpetuate systemic biases and\nintergenerational inequality. This study is the first of its kind to\ninvestigate whether and how surnames influence AI-driven decision-making,\nfocusing on their effects across key areas such as hiring recommendations,\nleadership appointments, and loan approvals. Using 72,000 evaluations of 600\nsurnames from the United States and Thailand, two countries with distinct\nsociohistorical contexts and surname conventions, we classify names into four\ncategories: Rich, Legacy, Normal, and phonetically similar Variant groups. Our\nfindings show that elite surnames consistently increase AI-generated\nperceptions of power, intelligence, and wealth, which in turn influence\nAI-driven decisions in high-stakes contexts. Mediation analysis reveals\nperceived intelligence as a key mechanism through which surname biases\ninfluence AI decision-making process. While providing objective qualifications\nalongside surnames mitigates most of these biases, it does not eliminate them\nentirely, especially in contexts where candidate credentials are low. These\nfindings highlight the need for fairness-aware algorithms and robust policy\nmeasures to prevent AI systems from reinforcing systemic inequalities tied to\nsurnames, an often-overlooked bias compared to more salient characteristics\nsuch as race and gender. Our work calls for a critical reassessment of\nalgorithmic accountability and its broader societal impact, particularly in\nsystems designed to uphold meritocratic principles while counteracting the\nperpetuation of intergenerational privilege.",
      "generated_abstract": "ng use of Artificial Intelligence (AI) in decision-making systems\nencourages us to consider their potential to address long-standing\ninequalities. However, existing research typically ignores the potential for\nalgorithmic inheritance, a process in which an individual's surnames are used to\ndetermine the surnames of their children. We develop a theoretical framework to\nanalyze the impact of algorithmic inheritance on intergenerational inequality,\nand empirically assess its impact on the U.S. Census Bureau's American Fact\nFinder data. Using data from 1990 to 2020, we find that algorithmic\ninheritance exacerbates intergenerational inequality by amplifying the gender\ngap in occupational surnames and by exacerbating the racial wealth gap. The\nimpact of algorithmic inheritance on intergenerational inequality varies\nsignificantly across",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16071428571428573,
          "p": 0.3375,
          "f": 0.21774193111342363
        },
        "rouge-2": {
          "r": 0.00909090909090909,
          "p": 0.018691588785046728,
          "f": 0.012232411499220693
        },
        "rouge-l": {
          "r": 0.1488095238095238,
          "p": 0.3125,
          "f": 0.2016128988553591
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ph/2503.10299v1",
      "true_abstract": "In this work, we systematically study the interactions of the $S$-wave\n$D^{(*)}\\bar{B}^{(*)}$ systems within the framework of chiral effective field\ntheory in heavy hadron formalism. We calculate the $D^{(*)}\\bar{B}^{(*)}$\neffective potentials up to next-to-leading order, explore the bound state\nformations, and investigate the $D^{(*)}\\bar{B}^{(*)}$ scattering properties\nsuch as scattering rate, scattering length, and effective range. Our results\nshow that all $I=1$ $D^{(*)}\\bar{B}^{(*)}$ potentials are repulsive, preventing\nthe formation of bound states, while the $I=0$ potentials are generally\nattractive. Specifically, we get two important observations: first, the shallow\nbound state is more likely to exist in the $D\\bar{B}[I(J^{P})=0(0^{+})]$ system\nthan in the $D\\bar{B}^{*}[I(J^{P})=0(1^{+})]$ system; second,\n$D^{*}\\bar{B}^{*}[I(J^{P})=0(0^{+})]$ and $D^{*}\\bar{B}^{*}[I(J^{P})=0(1^{+})]$\nsystems possess relatively large binding energies and positive scattering\nlengths, which suggests strong bound state formations in these channels. So the\nattractions in the $D^{*}\\bar{B}^{*}[I=0]$ systems are deeper than those in the\n$D\\bar{B}^{(*)}[I=0]$ systems, thus we strongly recommend the future experiment\nto search for the $D^{*}\\bar{B}^{*}[I=0]$ tetraquark systems. In addition, we\nalso investigate the dependencies of the $D\\bar{B}^{(*)}$ binding energies on\nthe contact low-energy coupling constants (LECs).",
      "generated_abstract": "the dynamics of $D^*$ and $\\bar{B}^*$ mesons in chiral effective\nfield theory. We construct the effective lagrangian by incorporating the\nintrinsic heavy-quark symmetry (IHQS) into the effective field theory. We\ncalculate the tree-level scattering amplitudes of $D^*$, $\\bar{B}^*$, and\n$D^0$, $\\bar{D}^0$ mesons. We then apply the IHQS to the effective lagrangian\nconstructed above. The results of the tree-level scattering amplitudes are\ncompared with the numerical results. We find that the IHQS can enhance the\ntree-level scattering amplitudes of $D^*$, $\\bar{B}^*$, and $D^0$,\n$\\bar{D}^0$ mesons, which are useful for the future precision studies of these",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.35185185185185186,
          "f": 0.22619047182823135
        },
        "rouge-2": {
          "r": 0.04938271604938271,
          "p": 0.10810810810810811,
          "f": 0.06779660586469433
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.3333333333333333,
          "f": 0.2142857099234695
        }
      }
    },
    {
      "paper_id": "quant-ph.stat/TH/2502.14950v1",
      "true_abstract": "Inferring causal models from observed correlations is a challenging task,\ncrucial to many areas of science. In order to alleviate the effort, it is\nimportant to know whether symmetries in the observations correspond to\nsymmetries in the underlying realization. Via an explicit example, we answer\nthis question in the negative. We use a tripartite probability distribution\nover binary events that is realized by using three (different) independent\nsources of classical randomness. We prove that even removing the condition that\nthe sources distribute systems described by classical physics, the requirements\nthat i) the sources distribute the same physical systems, ii) these physical\nsystems respect relativistic causality, and iii) the correlations are the\nobserved ones, are incompatible.",
      "generated_abstract": "the problem of estimating a symmetric observable $X$ without\nany knowledge of the causal structure of the world. This is a generalization of\nthe so-called observational bias problem. We show that in the case where $X$\nrepresents the marginal distribution of a random variable $Y$, the problem\nbecomes equivalent to estimating the causal graph of $Y$. We show that this is\na well-defined estimator, and it can be used to recover causal structures of\nthe observable $X$. We also show that if the causal graph of $Y$ is known, the\nproblem can be solved by estimating $Y$ using the estimator proposed. We show\nthat in this case, the estimator is consistent and asymptotically normal. We\nalso show that the estimator is asymptotically normal when the causal graph of\n$Y$ is unknown. Finally, we show that the estim",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19753086419753085,
          "p": 0.25,
          "f": 0.2206896502411416
        },
        "rouge-2": {
          "r": 0.027522935779816515,
          "p": 0.02912621359223301,
          "f": 0.028301881796458695
        },
        "rouge-l": {
          "r": 0.1728395061728395,
          "p": 0.21875,
          "f": 0.19310344334458993
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2501.00002v2",
      "true_abstract": "In this paper we present a QUBO formulation for the Takuzu game (or Binairo),\nfor the most recent LinkedIn game, Tango, and for its generalizations. We\noptimize the number of variables needed to solve the combinatorial problem,\nmaking it suitable to be solved by quantum devices with fewer resources.",
      "generated_abstract": "The Takuzu/LinkedIn Tango game is a dynamic, competitive game where players\nare assigned tasks to fulfill on their LinkedIn profiles. The players'\nstrategies are modeled as quadratically constrained Boolean functions. This\npaper presents a QUBO-based formulation for the Takuzu/LinkedIn Tango game.\nThe QUBO formulation allows for a more efficient use of computing resources and\noffers a mathematical foundation for game theory. We demonstrate the\ncomputational feasibility of the QUBO-based game by solving it using\noptimization software.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3488372093023256,
          "p": 0.2631578947368421,
          "f": 0.299999995098
        },
        "rouge-2": {
          "r": 0.06382978723404255,
          "p": 0.0410958904109589,
          "f": 0.04999999523472268
        },
        "rouge-l": {
          "r": 0.32558139534883723,
          "p": 0.24561403508771928,
          "f": 0.27999999509800005
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.12935v1",
      "true_abstract": "In this review, we examine computational models that explore the role of\nneural oscillations in speech perception, spanning from early auditory\nprocessing to higher cognitive stages. We focus on models that use rhythmic\nbrain activities, such as gamma, theta, and delta oscillations, to encode\nphonemes, segment speech into syllables and words, and integrate linguistic\nelements to infer meaning. We analyze the mechanisms underlying these models,\ntheir biological plausibility, and their potential applications in processing\nand understanding speech in real time, a computational feature that is achieved\nby the human brain but not yet implemented in speech recognition models.\nReal-time processing enables dynamic adaptation to incoming speech, allowing\nsystems to handle the rapid and continuous flow of auditory information\nrequired for effective communication, interactive applications, and accurate\nspeech recognition in a variety of real-world settings. While significant\nprogress has been made in modeling the neural basis of speech perception,\nchallenges remain, particularly in accounting for the complexity of semantic\nprocessing and the integration of contextual influences. Moreover, the high\ncomputational demands of biologically realistic models pose practical\ndifficulties for their implementation and analysis. Despite these limitations,\nthese models provide valuable insights into the neural mechanisms of speech\nperception. We conclude by identifying current limitations, proposing future\nresearch directions, and suggesting how these models can be further developed\nto achieve a more comprehensive understanding of speech processing in the human\nbrain.",
      "generated_abstract": "rception is a complex, nonlinear process, with multiple\nevents taking place in parallel. The neural mechanisms underlying this\ncomplexity are poorly understood, and it is not clear which components of the\nbrain are involved in the processing of speech. Here, we present a new class\nof models that explore the processing of speech in the human brain. These\nmodels use neural oscillations to model the neural dynamics underlying\nspeech processing. By coupling the oscillatory dynamics with the spiking\ndynamics, we can simulate a variety of different neural mechanisms, including\nspiking neurons, GABAergic interneurons, and dopaminergic neurons. These\nmodels demonstrate that oscillatory dynamics can provide a powerful framework\nfor modeling complex speech processing, and can be used to study the\ndynamics underlying speech perception and speech production. These models\nprovide a new approach for understanding the neural mechan",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2108843537414966,
          "p": 0.38271604938271603,
          "f": 0.27192981998037863
        },
        "rouge-2": {
          "r": 0.08294930875576037,
          "p": 0.14516129032258066,
          "f": 0.10557184287923241
        },
        "rouge-l": {
          "r": 0.19047619047619047,
          "p": 0.345679012345679,
          "f": 0.24561403050669442
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.06431v1",
      "true_abstract": "The kidney paired donation (KPD) program provides an innovative solution to\novercome incompatibility challenges in kidney transplants by matching\nincompatible donor-patient pairs and facilitating kidney exchanges. To address\nunequal access to transplant opportunities, there are two widely used fairness\ncriteria: group fairness and individual fairness. However, these criteria do\nnot consider protected patient features, which refer to characteristics legally\nor ethically recognized as needing protection from discrimination, such as race\nand gender. Motivated by the calibration principle in machine learning, we\nintroduce a new fairness criterion: the matching outcome should be\nconditionally independent of the protected feature, given the sensitization\nlevel. We integrate this fairness criterion as a constraint within the KPD\noptimization framework and propose a computationally efficient solution.\nTheoretically, we analyze the associated price of fairness using random graph\nmodels. Empirically, we compare our fairness criterion with group fairness and\nindividual fairness through both simulations and a real-data example.",
      "generated_abstract": "ation is a life-saving procedure that transfers organs from one\nperson to another, helping patients recover from organ failure. In the United\nStates, kidney paired donation (KPD) is one of the most widely used organ\nexchange methods, which is beneficial for both donors and recipients. However,\nthe fairness of KPD has been widely criticized due to the unequal distribution\nof organs among donors and recipients. In this paper, we propose a fairness-\naware KPD scheme to enhance the fairness of KPD. Our proposed scheme is based\non a fairness-aware matching mechanism that assesses the fairness of each\nmatching process and weights the matching process according to the fairness\nof the match. In the fairness-aware matching mechanism, the fairness of the\nmatching process is evaluated using a weighted sum of fairness metrics. The\npro",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21238938053097345,
          "p": 0.3076923076923077,
          "f": 0.25130889569145587
        },
        "rouge-2": {
          "r": 0.05517241379310345,
          "p": 0.07079646017699115,
          "f": 0.06201549895288784
        },
        "rouge-l": {
          "r": 0.19469026548672566,
          "p": 0.28205128205128205,
          "f": 0.2303664873144926
        }
      }
    },
    {
      "paper_id": "physics.soc-ph.econ/TH/2501.09778v2",
      "true_abstract": "We propose a disaggregated representation of production through an\nagent-based fund-flow model (NGR-ADAPT) within which inefficiencies, such as\nfactor idleness and production instability, emerge from endogenous frictions.\nThe model incorporates productivity dynamics (learning and depreciation) and is\nextended with time-saving process innovations. Specifically, we assume that\nworkers possess inherent creativity that flourishes during idle periods. The\nfirm, rather than laying off idle workers, is assumed to exploit this potential\nby involving them in the innovation process. Results show that a firm's\norganizational and managerial decisions, the temporal structure of the\nproduction system, the speed at which workers learn and forget, and the pace of\ninnovation are critical factors influencing production efficiency in both the\nshort and long run. The co-evolution of production and innovation processes\nemerges in our model through the two-sided effects of idleness: whereas it\ndrives skill decay it is also a condition for creative thinking that can be\nleveraged for innovation. In doing so, we question the utilization of labour as\nan adjustment variable in a productive organisation. The paper concludes by\ndiscussing potential solutions to this issue and suggesting avenues for future\nresearch.",
      "generated_abstract": "r investigates the emergence of instability in the system of\nproduction and innovation. We consider a market economy with two main\ncomponents: production and innovation. The production component is characterized\nby a dynamic allocation of the labor force, whereas the innovation component is\ndescribed by the allocation of the capital stock. We investigate how the\nlabor supply dynamics evolve in response to the production dynamics, and how\nthe capital stock dynamics evolve in response to the labor supply dynamics. We\npropose a co-evolutionary model that captures the interaction between these\ntwo processes. We find that the production dynamics is driven by the\noptimization of the labor supply, which is driven by the competition between\nthe capital stock and the labor force. The competition between the capital\nstock and the labor force is driven by the production dynamics, which in turn\nis driven by the optimization of the labor supply. The capital stock dynamics\nis driven by",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15503875968992248,
          "p": 0.3448275862068966,
          "f": 0.2139037390362894
        },
        "rouge-2": {
          "r": 0.05405405405405406,
          "p": 0.09803921568627451,
          "f": 0.06968640656800526
        },
        "rouge-l": {
          "r": 0.13953488372093023,
          "p": 0.3103448275862069,
          "f": 0.1925133647047386
        }
      }
    },
    {
      "paper_id": "cs.NE.cs/NE/2503.10387v1",
      "true_abstract": "Progress in neuromorphic computing requires efficient implementation of\nstandard computational problems, like adding numbers. Here we implement one\nsequential and two parallel binary adders in the Lava software framework, and\ndeploy them to the neuromorphic chip Loihi 2. We describe the time complexity,\nneuron and synaptic resources, as well as constraints on the bit width of the\nnumbers that can be added with the current implementations. Further, we measure\nthe time required for the addition operation on-chip. Importantly, we encounter\ntrade-offs in terms of time complexity and required chip resources for the\nthree considered adders. While sequential adders have linear time complexity\n$\\bf\\mathcal{O}(n)$ and require a linearly increasing number of neurons and\nsynapses with number of bits $n$, the parallel adders have constant time\ncomplexity $\\bf\\mathcal{O}(1)$ and also require a linearly increasing number of\nneurons, but nonlinearly increasing synaptic resources (scaling with $\\bf n^2$\nor $\\bf n \\sqrt{n}$). This trade-off between compute time and chip resources\nmay inform decisions in application development, and the implementations we\nprovide may serve as a building block for further progress towards efficient\nneuromorphic algorithms.",
      "generated_abstract": "uce the concept of spiking neural circuits (SNCs) as a framework for\nadding and subtracting arbitrary numbers on neuromorphic hardware. In the SNC,\nneurons are biologically inspired digital logic gates that generate spikes when\ntheir input voltages exceed their threshold voltage. This allows us to add or\nsubtract numbers by controlling the threshold voltages of the neurons. We\ndemonstrate that SNCs can be implemented on the SpiNNaker neuromorphic\nhardware, which is a grid-shaped array of neuromorphic devices. By applying\nadditive and multiplicative operators to SNCs, we demonstrate that SNCs can\nperform addition and multiplication. We also show that SNCs can be used to\nimplement simple addition and multiplication operations on a grid-shaped array\nof neuromorphic devices. Our experiments show that SNCs can be implemented on\nSpiNN",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1981981981981982,
          "p": 0.275,
          "f": 0.23036648727830938
        },
        "rouge-2": {
          "r": 0.023952095808383235,
          "p": 0.03773584905660377,
          "f": 0.029304024553664446
        },
        "rouge-l": {
          "r": 0.17117117117117117,
          "p": 0.2375,
          "f": 0.19895287471286438
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.10496v1",
      "true_abstract": "Modeling natural phenomena with artificial neural networks (ANNs) often\nprovides highly accurate predictions. However, ANNs often suffer from\nover-parameterization, complicating interpretation and raising uncertainty\nissues. Bayesian neural networks (BNNs) address the latter by representing\nweights as probability distributions, allowing for predictive uncertainty\nevaluation. Latent binary Bayesian neural networks (LBBNNs) further handle\nstructural uncertainty and sparsify models by removing redundant weights. This\narticle advances LBBNNs by enabling covariates to skip to any succeeding layer\nor be excluded, simplifying networks and clarifying input impacts on\npredictions. Ultimately, a linear model or even a constant can be found to be\noptimal for a specific problem at hand. Furthermore, the input-skip LBBNN\napproach reduces network density significantly compared to standard LBBNNs,\nachieving over 99% reduction for small networks and over 99.9% for larger ones,\nwhile still maintaining high predictive accuracy and uncertainty measurement.\nFor example, on MNIST, we reached 97% accuracy and great calibration with just\n935 weights, reaching state-of-the-art for compression of neural networks.\nFurthermore, the proposed method accurately identifies the true covariates and\nadjusts for system non-linearity. The main contribution is the introduction of\nactive paths, enhancing directly designed global and local explanations within\nthe LBBNN framework, that have theoretical guarantees and do not require post\nhoc external tools for explanations.",
      "generated_abstract": "e a novel Bayesian deep learning framework that integrates\nexplainability and interpretability by leveraging latent binary neural\nnetworks (LBNNs) as a surrogate model for generating explainable and\ninterpretable outputs. The LBNNs are trained to predict the label for each\nsample based on the outputs from a deep neural network. The proposed\nframework extends the standard deep learning framework by introducing an\nexplainability component that generates a high-level explanation of the model's\ndecision. We show that the LBNNs can learn a model that performs well on a\ncertain task, even when the data is not fully labeled. Moreover, we demonstrate\nthat LBNNs can generate a higher-level explanation of the model's decision.\nThese explanations can be used to guide the training process and to help\nunderstand the model's decision-making process. Additionally, we show that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17307692307692307,
          "p": 0.32142857142857145,
          "f": 0.2249999954500001
        },
        "rouge-2": {
          "r": 0.009900990099009901,
          "p": 0.01639344262295082,
          "f": 0.012345674317179046
        },
        "rouge-l": {
          "r": 0.14743589743589744,
          "p": 0.27380952380952384,
          "f": 0.19166666211666677
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2502.21276v1",
      "true_abstract": "Boosting has emerged as a useful machine learning technique over the past\nthree decades, attracting increased attention. Most advancements in this area,\nhowever, have primarily focused on numerical implementation procedures, often\nlacking rigorous theoretical justifications. Moreover, these approaches are\ngenerally designed for datasets with fully observed data, and their validity\ncan be compromised by the presence of missing observations. In this paper, we\nemploy semiparametric estimation approaches to develop boosting prediction\nmethods for data with missing responses. We explore two strategies for\nadjusting the loss functions to account for missingness effects. The proposed\nmethods are implemented using a functional gradient descent algorithm, and\ntheir theoretical properties, including algorithm convergence and estimator\nconsistency, are rigorously established. Numerical studies demonstrate that the\nproposed methods perform well in finite sample settings.",
      "generated_abstract": "The problem of boosting prediction with data missing not at random (DnNR)\nhas been studied in a number of ways. In the literature, a common assumption\nis that the data generating process (DGP) is a Gaussian process. However,\nthe Gaussian assumption is restrictive as it limits the flexibility of the\nmodeling framework. We propose a method for DnNR based on a\nprobabilistic-linear-quadratic Gaussian process model. We show that our\napproach is able to capture the Gaussian process structure of the DGP while\nproviding more flexibility than the Gaussian assumption. Our method is\ncomputationally efficient, and it is demonstrated through several simulation\nstudies and an application in biomedical research.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18867924528301888,
          "p": 0.2777777777777778,
          "f": 0.22471909630602208
        },
        "rouge-2": {
          "r": 0.016,
          "p": 0.02,
          "f": 0.017777772839507544
        },
        "rouge-l": {
          "r": 0.18867924528301888,
          "p": 0.2777777777777778,
          "f": 0.22471909630602208
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.07924v1",
      "true_abstract": "We study a fundamental challenge in the economics of innovation: an inventor\nmust reveal details of a new idea to secure compensation or funding, yet such\ndisclosure risks expropriation. We present a model in which a seller (inventor)\nand buyer (investor) bargain over an information good under the threat of\nhold-up. In the classical setting, the seller withholds disclosure to avoid\nmisappropriation, leading to inefficiency. We show that trusted execution\nenvironments (TEEs) combined with AI agents can mitigate and even fully\neliminate this hold-up problem. By delegating the disclosure and payment\ndecisions to tamper-proof programs, the seller can safely reveal the invention\nwithout risking expropriation, achieving full disclosure and an efficient ex\npost transfer. Moreover, even if the invention's value exceeds a threshold that\nTEEs can fully secure, partial disclosure still improves outcomes compared to\nno disclosure. Recognizing that real AI agents are imperfect, we model \"agent\nerrors\" in payments or disclosures and demonstrate that budget caps and\nacceptance thresholds suffice to preserve most of the efficiency gains.\n  Our results imply that cryptographic or hardware-based solutions can function\nas an \"ironclad NDA,\" substantially mitigating the fundamental\ndisclosure-appropriation paradox first identified by Arrow (1962) and Nelson\n(1959). This has far-reaching policy implications for fostering R&D, technology\ntransfer, and collaboration.",
      "generated_abstract": "t a novel approach to establishing NDAIs through a novel mechanism\nfor distributing NDAIs. The mechanism is a bidding auction that allows each\nbidder to bid on multiple NDAIs, and the bidder with the highest bid wins the\nright to negotiate. We prove that the NDAI auction is Nash-equilibrium\nstable. We also prove that the NDAI auction is strictly sub-game-perfect. We\nalso prove that the NDAI auction is strictly Nash-equilibrium-feasible,\nmeaning that each bidder is able to achieve the highest possible payoff.\nAdditionally, we show that the NDAI auction is strictly social-dilemma-feasible,\nmeaning that the auction does not lead to any bidder having a negative payoff.\nWe also show that the NDAI auction is strictly Nash-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06711409395973154,
          "p": 0.18181818181818182,
          "f": 0.09803921174788559
        },
        "rouge-2": {
          "r": 0.004901960784313725,
          "p": 0.011764705882352941,
          "f": 0.00692041107266685
        },
        "rouge-l": {
          "r": 0.06040268456375839,
          "p": 0.16363636363636364,
          "f": 0.08823529017925817
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.06389v1",
      "true_abstract": "Network estimation has been a critical component of single-cell\ntranscriptomic data analysis, which can provide crucial insights into the\ncomplex interplay among genes, facilitating uncovering the biological basis of\nhuman life at single-cell resolution. Despite notable achievements, existing\nmethodologies often falter in their practicality, primarily due to their narrow\nfocus on simplistic linear relationships and inadequate handling of cellular\nheterogeneity. To bridge these gaps, we propose a joint regularized deep neural\nnetwork method incorporating a Mahalanobis distance-based K-means clustering\n(JRDNN-KM) to estimate multiple networks for various cell subgroups\nsimultaneously, accounting for both unknown cellular heterogeneity and\nzero-inflation and, more importantly, complex nonlinear relationships among\ngenes. We innovatively introduce a selection layer for network construction and\ndevelop homogeneous and heterogeneous hidden layers to accommodate commonality\nand specificity across multiple networks. Through simulations and applications\nto real single-cell transcriptomic data for multiple tissues and species, we\nshow that JRDNN-KM constructs networks with more accuracy and biological\ninterpretability and, meanwhile, identifies more accurate cell subgroups\ncompared to the state-of-the-art methods in the literature. Building on the\nnetwork construction, we further find hub genes with important biological\nimplications and modules with statistical enrichment of biological processes.",
      "generated_abstract": "ll transcriptomic data exhibit significant heterogeneity in the\nsingle-cell gene expression levels, posing a significant challenge in\nestimating the network structure. To address this issue, we propose a novel\nframework that leverages a joint regularized deep neural network (JDNN) to\nestimate the gene network structure from single-cell transcriptomic data. The\nkey idea of the JDNN is to incorporate both the global connectivity and the\nlocal connectivity of the network into the optimization process. We first\nderive a novel regularization term that enhances the global connectivity of the\nnetwork. To overcome the challenges associated with the local connectivity\nestimation, we introduce a novel deep neural network architecture that\nrepresents the local connectivity. Furthermore, we propose an efficient\noptimization method that enables us to estimate the global and local network\nstructures simultaneously, and the proposed method is validated through",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19852941176470587,
          "p": 0.35526315789473684,
          "f": 0.2547169765325739
        },
        "rouge-2": {
          "r": 0.07526881720430108,
          "p": 0.12280701754385964,
          "f": 0.09333332862133359
        },
        "rouge-l": {
          "r": 0.19117647058823528,
          "p": 0.34210526315789475,
          "f": 0.245283014268423
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.11552v1",
      "true_abstract": "We explore the interplay between sovereign debt default/renegotiation and\nenvironmental factors (e.g., pollution from land use, natural resource\nexploitation). Pollution contributes to the likelihood of natural disasters and\ninfluences economic growth rates. The country can default on its debt at any\ntime while also deciding whether to invest in pollution abatement. The\nframework provides insights into the credit spreads of sovereign bonds and\nexplains the observed relationship between bond spread and a country's climate\nvulnerability. Through calibration for developing and low-income countries, we\ndemonstrate that there is limited incentive for these countries to address\nclimate risk, and the sensitivity of bond spreads to climate vulnerability\nremains modest. Climate risk does not play a relevant role on the decision to\ndefault on sovereign debt. Financial support for climate abatement expenditures\ncan effectively foster climate adaptation actions, instead renegotiation\nconditional upon pollution abatement does not produce any effect.",
      "generated_abstract": "We analyze the effects of sovereign debt default on climate risk using a\nparticularly simple model that includes both weather and climate shocks. We\nfind that climate risk is amplified by default. This amplification is driven by\nthe fact that climate shocks increase the probability of default, while\nweather shocks reduce the probability of default. We find that climate risk\nincreases by 50% when the probability of default increases by 5%. This result\nis consistent with previous empirical studies that show that climate risk\nincreases as the probability of default increases.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1346153846153846,
          "p": 0.2978723404255319,
          "f": 0.1854304592886278
        },
        "rouge-2": {
          "r": 0.02112676056338028,
          "p": 0.04285714285714286,
          "f": 0.02830188236917122
        },
        "rouge-l": {
          "r": 0.1346153846153846,
          "p": 0.2978723404255319,
          "f": 0.1854304592886278
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2410.21090v1",
      "true_abstract": "One of goals in soft robotics is to achive spontaneous behavior like real\norganisms. To gain a clue to achieve this, we examined the long (16-hour)\nspontaneous exploratory locomotion of snails. The active forager snail, Tegula\nnigerrima, from an intertidal rocky shore was selected to test the general\nhypothesis that nervous systems are inherently near a critical state, which is\nself-organized to drive spontaneous animal behavior. This hypothesis, known as\nthe critical brain hypothesis, was originally proposed for vertebrate species,\nbut it might be applicable to other invertebrate species as well. We first\ninvestigated the power spectra of the speed of locomotion of the snails\n($N=39$). The spectra showed $1/{f^\\alpha}$ fluctuation, which is one of the\nsignatures of self-organized criticality. The $\\alpha$ was estimated to be\nabout 0.9. We further examined whether the spatial and temporal quantities show\nmultiple power-laws and scaling relations, which are rigorous criteria of\ncriticality. Although the satisfaction of these criteria is limited to a\ntruncated region and provides limited evidence to demonstrate the aspect of\nself-organization, the multiple power-laws and the scaling relations were\noverall satisfied. Therefore, these results additionally support the generality\nof the critical brain hypothesis.",
      "generated_abstract": "Tegula nigrima is known for its exploratory locomotion, in which\nthe animal moves through the environment using its four limbs. In this study,\nwe present a systematic analysis of the behavior of this species, focusing on\nthe role of the mechanical properties of its body in the evolution of its\nlocomotion. We describe the system by a multi-body dynamics model, and we\ninvestigate the effects of various parameters, such as the radius of gyration\nof the body, the length of the body, and the ratio of the mass of the animal\nto its body radius, on the kinematics and the power laws of locomotion. We\nidentify a regime of locomotion where the power law of the force-speed curve\nexhibits two distinct regimes, depending on the ratio of the mass of the animal\nto its body radius. We show that this is due to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1746031746031746,
          "p": 0.2894736842105263,
          "f": 0.21782177748456044
        },
        "rouge-2": {
          "r": 0.02717391304347826,
          "p": 0.04310344827586207,
          "f": 0.0333333285902229
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.27631578947368424,
          "f": 0.20792078738555053
        }
      }
    },
    {
      "paper_id": "cs.CV.q-bio/GN/2411.16793v1",
      "true_abstract": "Spatial transcriptomics (ST) provides high-resolution pathological images and\nwhole-transcriptomic expression profiles at individual spots across whole-slide\nscales. This setting makes it an ideal data source to develop multimodal\nfoundation models. Although recent studies attempted to fine-tune visual\nencoders with trainable gene encoders based on spot-level, the absence of a\nwider slide perspective and spatial intrinsic relationships limits their\nability to capture ST-specific insights effectively. Here, we introduce\nST-Align, the first foundation model designed for ST that deeply aligns\nimage-gene pairs by incorporating spatial context, effectively bridging\npathological imaging with genomic features. We design a novel pretraining\nframework with a three-target alignment strategy for ST-Align, enabling (1)\nmulti-scale alignment across image-gene pairs, capturing both spot- and\nniche-level contexts for a comprehensive perspective, and (2) cross-level\nalignment of multimodal insights, connecting localized cellular characteristics\nand broader tissue architecture. Additionally, ST-Align employs specialized\nencoders tailored to distinct ST contexts, followed by an Attention-Based\nFusion Network (ABFN) for enhanced multimodal fusion, effectively merging\ndomain-shared knowledge with ST-specific insights from both pathological and\ngenomic data. We pre-trained ST-Align on 1.3 million spot-niche pairs and\nevaluated its performance through two downstream tasks across six datasets,\ndemonstrating superior zero-shot and few-shot capabilities. ST-Align highlights\nthe potential for reducing the cost of ST and providing valuable insights into\nthe distinction of critical compositions within human tissue.",
      "generated_abstract": "Image-gene alignment plays a crucial role in understanding the spatial\ntranscriptomics (ST) data. However, the existing methods mainly focus on\naligning image features, which neglect the spatial relationship between\nimage-gene pairs. To address this limitation, we propose ST-Align, a novel\nframework that aligns image features and gene expression features. Specifically,\nthe proposed method first aligns image features through a pre-trained\ntransformer-based image-alignment module, and then aligns gene expression\nfeatures using a GNN-based gene-alignment module. In addition, we introduce a\nnovel attention mechanism to integrate image and gene features, ensuring that\nimage features are highly considered during the gene alignment process.\nExperimental results on the ST dataset demonstrate that our method achieves\nstate-of-the-art performance in image-gene alignment.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16981132075471697,
          "p": 0.34177215189873417,
          "f": 0.22689075186745292
        },
        "rouge-2": {
          "r": 0.018433179723502304,
          "p": 0.037383177570093455,
          "f": 0.02469135360101436
        },
        "rouge-l": {
          "r": 0.16981132075471697,
          "p": 0.34177215189873417,
          "f": 0.22689075186745292
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.physics/space-ph/2503.00705v1",
      "true_abstract": "The first severe (G4) geomagnetic storm of Solar Cycle 25 occurred on 23-24\nApril 2023, following the arrival of a Coronal Mass Ejection (CME) on 23 April.\nThe characteristics of this CME, measured from coronagraphs (speed and mass),\ndid not indicate that it would trigger such an intense geomagnetic storm. In\nthis work, our aim is to understand why this CME led to such a geoeffective\noutcome. Our analysis spans from the source active region to the corona and\ninner heliosphere through 1 au using multiwavelength, multi-viewpoint remote\nsensing observations and in situ data. We find that rotation and possibly\ndeflection of the CME resulted in an axial magnetic field nearly parallel to\nthe ecliptic plane during the Earth encounter, which might explain the storm's\nseverity. Additionally, we find that imaging away from the Sun-Earth line is\ncrucial in hindcasting the CME Time-of-Arrival at Earth. The position (0.39 au)\nand detailed images from the SoloHI telescope onboard the Solar Orbiter\nmission, in combination with SOHO and STEREO images, helped decisively with the\nthree-dimensional (3D) reconstruction of the CME.",
      "generated_abstract": "The Coronal Mass Ejection (CME) on 21 April 2023, which was classified as a\nHypersonic CME, triggered the first severe geomagnetic storm of Solar Cycle\n25. The geomagnetic storm affected the Earth's magnetic field, causing a\nrapid decrease in geomagnetic indices. The CME was also responsible for\nintermittent geomagnetic storms on 22 April 2023. This study analyzes the CME\nand its impact on the geomagnetic field and solar wind to understand how the\nCME triggered geomagnetic storms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2109375,
          "p": 0.4909090909090909,
          "f": 0.29508196300874917
        },
        "rouge-2": {
          "r": 0.07647058823529412,
          "p": 0.1780821917808219,
          "f": 0.10699588057037393
        },
        "rouge-l": {
          "r": 0.171875,
          "p": 0.4,
          "f": 0.2404371542655798
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.16524v2",
      "true_abstract": "This study demonstrates the persistent dominance of identity based voting\nacross democratic systems, using the United States as a primary case and\ncomparative analyses of 19 other democracies as counterfactuals. Drawing solely\non election data from the Roper Center (1976 through recent cycles), we employ\nOLS regression, ANOVA, and correlation tests to show that race remains the\nstrongest predictor of party affiliation in the US (p < 0.001), with White\nvoters favoring Republicans and Black voters consistently supporting Democrats\n(85% since 1988). Income, education, and gender exemplified by gaps like 10\npoints in 2020 further shape voting patterns, yet racial identity predominates.\nComparative evidence from majoritarian (e.g., India), proportional (e.g.,\nGermany through 2025), and hybrid (e.g., South Korea with a 25 point gender\ngap) systems reveals no democracy where issue based voting fully supplants\nidentity based voting. Digital mobilization amplifies this trend globally.\nThese findings underscore identity enduring role in electoral behavior,\nchallenging assumptions of policy driven democratic choice.",
      "generated_abstract": "Identity-based voting is the practice of selecting a candidate based on their\nidentity rather than their merits. This phenomenon is widespread, yet its\nevolution and consequences remain poorly understood. This study examines the\nevolution of identity-based voting in the United States, France, and Canada. We\nfind that the practice of identity-based voting evolved slowly in the United\nStates, but it increased rapidly in France and Canada. We find that identity-\nbased voting is associated with lower support for democracy and more polarized\nsocieties. We also find that identity-based voting is associated with higher\nincidents of racial discrimination. These findings suggest that identity-based\nvoting is a symptom of more fundamental problems with democracy.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1450381679389313,
          "p": 0.2878787878787879,
          "f": 0.19289339655956103
        },
        "rouge-2": {
          "r": 0.03164556962025317,
          "p": 0.054945054945054944,
          "f": 0.04016063793229195
        },
        "rouge-l": {
          "r": 0.13740458015267176,
          "p": 0.2727272727272727,
          "f": 0.18274111229560164
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2409.00780v1",
      "true_abstract": "The main purpose of this work is the derivation of a functional partial\ndifferential equation (FPDE) for the calculations of equity-linked insurance\npolicies, where the payment stream may depend on the whole past history of the\nfinancial asset. To this end, we employ variational techniques from the theory\nof functional It\\^o calculus.",
      "generated_abstract": "We develop a functional variational approach to pricing path-dependent\ninsurance policies in continuous-time. This approach combines the\npath-dependent pricing framework with the functional variational approach to\nstochastic differential equations. The functional variational approach provides\na principled way to model the stochastic insurance dynamics, and it allows to\nformulate the optimal control problem as a variational problem. We then\nderive the optimal insurance policies using the functional variational\napproach. The optimal insurance policies are characterized by the solution to\nthe functional variational problem. We also derive the optimal insurance\npolicies using the variational approach, and we compare the two approaches. We\nalso provide a simple numerical example to illustrate the approach.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1951219512195122,
          "p": 0.1509433962264151,
          "f": 0.1702127610389318
        },
        "rouge-2": {
          "r": 0.0196078431372549,
          "p": 0.011627906976744186,
          "f": 0.014598535472322876
        },
        "rouge-l": {
          "r": 0.17073170731707318,
          "p": 0.1320754716981132,
          "f": 0.14893616529425097
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2502.21106v1",
      "true_abstract": "Recent advancements in AI and medical imaging offer transformative potential\nin emergency head CT interpretation for reducing assessment times and improving\naccuracy in the face of an increasing request of such scans and a global\nshortage in radiologists. This study introduces a 3D foundation model for\ndetecting diverse neuro-trauma findings with high accuracy and efficiency.\nUsing large language models (LLMs) for automatic labeling, we generated\ncomprehensive multi-label annotations for critical conditions. Our approach\ninvolved pretraining neural networks for hemorrhage subtype segmentation and\nbrain anatomy parcellation, which were integrated into a pretrained\ncomprehensive neuro-trauma detection network through multimodal fine-tuning.\nPerformance evaluation against expert annotations and comparison with CT-CLIP\ndemonstrated strong triage accuracy across major neuro-trauma findings, such as\nhemorrhage and midline shift, as well as less frequent critical conditions such\nas cerebral edema and arterial hyperdensity. The integration of neuro-specific\nfeatures significantly enhanced diagnostic capabilities, achieving an average\nAUC of 0.861 for 16 neuro-trauma conditions. This work advances foundation\nmodels in medical imaging, serving as a benchmark for future AI-assisted\nneuro-trauma diagnostics in emergency radiology.",
      "generated_abstract": "sive neuro-trauma triage is critical for rapid decision-making to\nprevent delays in patient care and preventable deaths. Traditional\ncomputerized neuro-trauma assessment systems are limited by their reliance on\ncontrast-enhanced CT scans, which can cause severe adverse effects in patients\nwith traumatic brain injuries (TBI). This study introduces a non-contrast head\nCT foundation model (NHCTFM) for comprehensive neuro-trauma triage. The NHCTFM\nis designed to address the limitations of existing CT-based systems. First, we\nextended the model to include head CT scans to evaluate the feasibility of\ncomprehensive neuro-trauma triage using a head CT scan as an alternative to\ncontrast-enhanced CT scans. Second, we implemented a deep learning approach to\naugment the model with",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21428571428571427,
          "p": 0.34177215189873417,
          "f": 0.26341462940916127
        },
        "rouge-2": {
          "r": 0.03508771929824561,
          "p": 0.05825242718446602,
          "f": 0.04379561574591131
        },
        "rouge-l": {
          "r": 0.18253968253968253,
          "p": 0.2911392405063291,
          "f": 0.22439023916525888
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2503.02389v1",
      "true_abstract": "We propose a method for accurately detecting bioacoustic sound events that is\nrobust to overlapping events, a common issue in domains such as ethology,\necology and conservation. While standard methods employ a frame-based,\nmulti-label approach, we introduce an onset-based detection method which we\nname Voxaboxen. It takes inspiration from object detection methods in computer\nvision, but simultaneously takes advantage of recent advances in\nself-supervised audio encoders. For each time window, Voxaboxen predicts\nwhether it contains the start of a vocalization and how long the vocalization\nis. It also does the same in reverse, predicting whether each window contains\nthe end of a vocalization, and how long ago it started. The two resulting sets\nof bounding boxes are then fused using a graph-matching algorithm. We also\nrelease a new dataset designed to measure performance on detecting overlapping\nvocalizations. This consists of recordings of zebra finches annotated with\ntemporally-strong labels and showing frequent overlaps. We test Voxaboxen on\nseven existing data sets and on our new data set. We compare Voxaboxen to\nnatural baselines and existing sound event detection methods and demonstrate\nSotA results. Further experiments show that improvements are robust to frequent\nvocalization overlap.",
      "generated_abstract": "udio-based applications, such as audio assistants, we often need to\nrecognize multiple sound events, and then selectively output the relevant\ninformation. In this paper, we propose an algorithm that can detect and select\nthe most relevant sound events. We focus on the overlapping detection problem\nand propose a novel strategy, called the interference-aware detection strategy.\nThe main idea of the strategy is to reduce the interference of the target\nsound events by reducing the interference of the other sound events. To\nachieve this, we first construct a noise model for the interference and then\nestimate the interference of the target sound event. We then design a\nrepresentative selection strategy, which selects the most relevant sound event\nbased on the estimated interference. In addition, we propose a robust estimation\nof the interference and a novel algorithm for calculating the interference of\nthe target sound event. We also introduce a novel experimental",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2230769230769231,
          "p": 0.3625,
          "f": 0.27619047147392295
        },
        "rouge-2": {
          "r": 0.026881720430107527,
          "p": 0.042735042735042736,
          "f": 0.03300329558932199
        },
        "rouge-l": {
          "r": 0.2076923076923077,
          "p": 0.3375,
          "f": 0.2571428524263039
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.06790v1",
      "true_abstract": "Recent research applying text-to-image (T2I) diffusion models to real-world\nsuper-resolution (SR) has achieved remarkable success. However, fundamental\nmisalignments between T2I and SR targets result in a dilemma between inference\nspeed and detail fidelity. Specifically, T2I tasks prioritize multi-step\ninversion to synthesize coherent outputs aligned with textual prompts and\nshrink the latent space to reduce generating complexity. Contrariwise, SR tasks\npreserve most information from low-resolution input while solely restoring\nhigh-frequency details, thus necessitating sufficient latent space and fewer\ninference steps. To bridge the gap, we present a one-step diffusion model for\ngenerative detail restoration, GenDR, distilled from a tailored diffusion model\nwith larger latent space. In detail, we train a new SD2.1-VAE16 (0.9B) via\nrepresentation alignment to expand latent space without enlarging the model\nsize. Regarding step-distillation, we propose consistent score identity\ndistillation (CiD) that incorporates SR task-specific loss into score\ndistillation to leverage more SR priors and align the training target.\nFurthermore, we extend CiD with adversarial learning and representation\nalignment (CiDA) to enhance perceptual quality and accelerate training. We also\npolish the pipeline to achieve a more efficient inference. Experimental results\ndemonstrate that GenDR achieves state-of-the-art performance in both\nquantitative metrics and visual fidelity.",
      "generated_abstract": "years, Deep Learning (DL) has demonstrated strong potential in\naddressing various tasks in the image domain. However, the DL-based approaches\noften encounter challenges in image restoration, due to the intrinsic\nimperfections of the input images. In this paper, we propose GenDR, a novel\nlightning generator that restores the image quality by generating high-quality\nand high-quality details. Specifically, we first introduce a lightning generator\nwith a single lightning head to generate high-quality details, which can\ndistinguish the details in the input images. Then, we introduce a lightning\nrestorer to restore the image quality after generating high-quality details. We\nintroduce the novel details consistency loss and detail consistency loss to\nenhance the performance of restoration. Experiments demonstrate that our\nframework achieves competitive performance with state-of-the-art methods. The\ncode",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19718309859154928,
          "p": 0.35443037974683544,
          "f": 0.25339366056468954
        },
        "rouge-2": {
          "r": 0.020942408376963352,
          "p": 0.035398230088495575,
          "f": 0.02631578480284882
        },
        "rouge-l": {
          "r": 0.19014084507042253,
          "p": 0.34177215189873417,
          "f": 0.24434388680903343
        }
      }
    },
    {
      "paper_id": "physics.ed-ph.physics/ed-ph/2503.03850v1",
      "true_abstract": "Although physics has become increasingly computational, with computing even\nbeing considered the third pillar of physics [1], it is still not well\nintegrated into physics education [2]. Research suggests that integrating\nComputational Thinking (CT) into physics enhances conceptual understanding and\nstrengthens students ability to model and analyze phenomena [3]. Building on\nthis, we designed a didactic sequence for K9 students to foster specific CT\npractices while reinforcing fundamental kinematics concepts. Assessments\nhighlight student's ability to apply CT skills to analyze accelerated motion.\nThis activity can be seamlessly integrated into introductory kinematics\ncourses.",
      "generated_abstract": "This article explores computational thinking in science education through the\nIntegrated Physics and Mathematics Program (IPMP) at the University of\nCalifornia, Santa Cruz. The IPMP integrates computational thinking with\nkinematics exploration, using a framework that links kinematics to the\ncomputational thinking framework of the Scientific Process. This approach\nallows students to apply computational thinking to kinematics problems while\nfurthering their understanding of the Scientific Process. The article includes\nan overview of the IPMP program, highlights key aspects of the computational\nthinking framework, and provides examples of kinematics problems and\nsolutions explored using the framework. The article also includes a discussion\nof the benefits of integrating computational thinking with kinematics\nexploration, including increased student engagement, greater mathematical\nunderstanding, and improved problem-solving skills.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2077922077922078,
          "p": 0.22535211267605634,
          "f": 0.21621621122443405
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.019417475728155338,
          "f": 0.0209424034078025
        },
        "rouge-l": {
          "r": 0.16883116883116883,
          "p": 0.18309859154929578,
          "f": 0.1756756706838935
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.04337v1",
      "true_abstract": "In this paper, a compressor system is analyzed in order to show its\ncharacteristics and design a control scheme to improve its efficiency. A\nmathematical technique has been created to forecast the onset of surge and\ninstability in a compressor chart, drawing from the nonlinear Greitzer and\nMoore model. This approach employs the phase plane and Jacobian matrix to\nidentify both stable and unstable regions within the compressor, as well as to\ncapture the limit cycle within the unstable region. A predictive analytical\napproach for anticipating compressor surge and instability is of great\nimportance in system instrumentation and control. State space model is built up\nby nonlinear Greitzer equations. Validation from previous study about especial\ncompressor will be considered for evaluation of mathematic method. Upstream\nflow acts as a disturbance to control loop and controller cannot satisfy\ndesired requirements with flow variances, ergo it is essential that controller\nis adapted to new conditions. Since control signal is linearly related to\nsystem output, a PD controller is used to control compressor system. An\nadaptive PD controller is designed with MRAS method based on a reference model.\nAdaptive controller can stabilize compressor and increase its efficiency in the\npresence of any disturbances. Simulation results shows that an adaptive\ncontroller can provide good performance and convergence in case of speed\nchanges by adapting gain parameters, and adaptive will be compared with normal\nPID. Finally, controller stability is investigated.",
      "generated_abstract": "This paper proposes a novel active adaptive control strategy for nonlinear\ncompressor system with surge and antisurge valve to prevent the system from\nsurge and antisurge. The control strategy is based on the nonlinear state\nfeedback control of the active adaptive controller and the feedback control\nof the passive adaptive controller. The surge is prevented by the surge\ncontrol and the antisurge is prevented by the antisurge control. The stability\nof the system is analyzed by using Lyapunov function approach. The analysis\nshows that the surge control and the antisurge control are stable in the\nregion of parameter space and the system is asymptotically stable in the\nregion of parameter space.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20279720279720279,
          "p": 0.5918367346938775,
          "f": 0.3020833295317925
        },
        "rouge-2": {
          "r": 0.04072398190045249,
          "p": 0.1125,
          "f": 0.059800660549000816
        },
        "rouge-l": {
          "r": 0.1888111888111888,
          "p": 0.5510204081632653,
          "f": 0.28124999619845925
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.15104v2",
      "true_abstract": "In both artificial and biological systems, the centered kernel alignment\n(CKA) has become a widely used tool for quantifying neural representation\nsimilarity. While current CKA estimators typically correct for the effects of\nfinite stimuli sampling, the effects of sampling a subset of neurons are\noverlooked, introducing notable bias in standard experimental scenarios. Here,\nwe provide a theoretical analysis showing how this bias is affected by the\nrepresentation geometry. We then introduce a novel estimator that corrects for\nboth input and feature sampling. We use our method for evaluating both\nbrain-to-brain and model-to-brain alignments and show that it delivers reliable\ncomparisons even with very sparsely sampled neurons. We perform within-animal\nand across-animal comparisons on electrophysiological data from visual cortical\nareas V1, V4, and IT data, and use these as benchmarks to evaluate\nmodel-to-brain alignment. We also apply our method to reveal how object\nrepresentations become progressively disentangled across layers in both\nbiological and artificial systems. These findings underscore the importance of\ncorrecting feature-sampling biases in CKA and demonstrate that our\nbias-corrected estimator provides a more faithful measure of representation\nalignment. The improved estimates increase our understanding of how neural\nactivity is structured across both biological and artificial systems.",
      "generated_abstract": "ation learning is a fundamental task in neuroscience and other\nfields. It aims to learn a mapping between an input and a representation of\nthe input, such as a neuron's firing rate, that is invariant to changes in the\ninput. One way to achieve this is by learning a mapping between the input and\nthe representation using sparse data. However, training a neural network\nrequires a large number of samples, which is prohibitively expensive in\nhigh-dimensional spaces, particularly when the input dimensions are high. In\nthis paper, we introduce a new method to estimate the alignment between\nrepresentations and inputs using a sparse data set. The idea is to use a\nsmall subset of the input to train a neural network that learns to map the\ninput to a representation. We show that this method can be used to estimate the\nalignment between representations and inputs, even when the input dimension is\nvery high. We validate our",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23484848484848486,
          "p": 0.3563218390804598,
          "f": 0.2831050180421593
        },
        "rouge-2": {
          "r": 0.031746031746031744,
          "p": 0.044444444444444446,
          "f": 0.03703703217592656
        },
        "rouge-l": {
          "r": 0.2196969696969697,
          "p": 0.3333333333333333,
          "f": 0.2648401778595109
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/SY/2503.10419v1",
      "true_abstract": "In motion simulation, motion cueing algorithms are used for the trajectory\nplanning of the motion simulator platform, where workspace limitations prevent\ndirect reproduction of reference trajectories. Strategies such as motion\nwashout, which return the platform to its center, are crucial in these\nsettings. For serial robotic MSPs with highly nonlinear workspaces, it is\nessential to maximize the efficient utilization of the MSPs kinematic and\ndynamic capabilities. Traditional approaches, including classical washout\nfiltering and linear model predictive control, fail to consider\nplatform-specific, nonlinear properties, while nonlinear model predictive\ncontrol, though comprehensive, imposes high computational demands that hinder\nreal-time, pilot-in-the-loop application without further simplification. To\novercome these limitations, we introduce a novel approach using deep\nreinforcement learning for motion cueing, demonstrated here for the first time\nin a 6-degree-of-freedom setting with full consideration of the MSPs kinematic\nnonlinearities. Previous work by the authors successfully demonstrated the\napplication of DRL to a simplified 2-DOF setup, which did not consider\nkinematic or dynamic constraints. This approach has been extended to all 6 DOF\nby incorporating a complete kinematic model of the MSP into the algorithm, a\ncrucial step for enabling its application on a real motion simulator. The\ntraining of the DRL-MCA is based on Proximal Policy Optimization in an\nactor-critic implementation combined with an automated hyperparameter\noptimization. After detailing the necessary training framework and the\nalgorithm itself, we provide a comprehensive validation, demonstrating that the\nDRL MCA achieves competitive performance against established algorithms.\nMoreover, it generates feasible trajectories by respecting all system\nconstraints and meets all real-time requirements with low...",
      "generated_abstract": "es are widely used in industrial applications, including\nautomated assembly lines and medical imaging. A key challenge in motion cueing\nis the dynamic response of the cueing system, which can be challenging to\ncontrol in real-time due to the high stochasticity and unpredictability of the\nsystem's dynamics. This paper presents a deep reinforcement learning-based\nalgorithm for motion cueing, which is capable of adapting to different\nsystem configurations and changing task requirements in real-time. Our\napproach combines a high-level policy with a low-level controller, enabling a\nsystematic approach to cueing. We demonstrate that our approach is capable of\nmaintaining a stable cueing system in a variety of dynamic conditions. The\nresults demonstrate that our approach achieves high-quality cueing,\nconsistently responding to the motion commands and maintaining stable\nperformance",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18497109826589594,
          "p": 0.3902439024390244,
          "f": 0.2509803877936179
        },
        "rouge-2": {
          "r": 0.028455284552845527,
          "p": 0.059322033898305086,
          "f": 0.03846153407982178
        },
        "rouge-l": {
          "r": 0.1791907514450867,
          "p": 0.3780487804878049,
          "f": 0.24313725053871596
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/EC/2502.17906v2",
      "true_abstract": "In econophysics, there are several enigmatic empirical laws: (i)~the\nmarket-order flow has strong persistence (long-range order-sign correlation),\nwell formulated as the Lillo-Mike-Farmer model. This phenomenon seems\nparadoxical given the diffusive and unpredictable price dynamics; (ii)~the\nprice impact $I(Q)$ of a large metaorder $Q$ follows the square-root law,\n$I(Q)\\propto \\sqrt{Q}$. In this Letter, we propose an exactly solvable model of\nthe nonlinear price-impact dynamics that unifies these enigmas. We generalize\nthe Lillo-Mike-Farmer model to nonlinear price-impact dynamics, which is mapped\nto an exactly solvable L\\'evy-walk model. Our exact solution and numerical\nsimulations reveal three important points: First, the price dynamics remains\ndiffusive under the square-root law, even under the long-range correlation.\nSecond, price-movement statistics follows truncated power laws with typical\nexponent around three. Third, volatility has long memory. While this simple\nmodel lacks adjustable free parameters, it naturally aligns even with other\nenigmatic empirical laws, such as (iii)~the inverse-cubic law for price\nstatistics and (iv)~volatility clustering. This work illustrates the crucial\nrole of the square-root law in understanding rich and complex financial price\ndynamics from a single coherent viewpoint.",
      "generated_abstract": "aper, we propose an exactly solvable model of the square-root price\nimpact dynamics, which is a stochastic model of the financial markets. The\nsquare-root price impact dynamics are the time evolution of the price impact\nunder the long-range market-order correlation. Our model can be viewed as a\ngeneralization of the classical model of the price impact dynamics. In our\nmodel, the price impact is determined by the square-root of the price impact\ndynamics. We derive the explicit expression of the price impact dynamics in\nterms of the square-root of the price impact dynamics. Then, we use the\nexact solution to numerically study the price impact dynamics in the model.\nNumerical results show that our model can well reproduce the price impact\ndynamics. Moreover, we use the numerical results to analyze the price impact\ndynamics. Finally, we derive the exact analytical solution to the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2578125,
          "p": 0.515625,
          "f": 0.3437499955555556
        },
        "rouge-2": {
          "r": 0.09090909090909091,
          "p": 0.15625,
          "f": 0.1149425240850841
        },
        "rouge-l": {
          "r": 0.234375,
          "p": 0.46875,
          "f": 0.3124999955555556
        }
      }
    },
    {
      "paper_id": "math.CV.math/CV/2503.05855v1",
      "true_abstract": "We first study subextensions of m-subharmonic functions in weighted energy\nclasses with given boundary values. The results are used to approximate an\nm-subharmonic function in weighted energy classes with given boundary values by\nan increasing sequence of m-subharmonic functions defined on larger domains.",
      "generated_abstract": "We study the existence and approximation properties of subextension and\napproximation of $m$-subharmonic functions in weighted energy classes with\ngiven boundary values. The problem is formulated in terms of the subextension\nof $m$-subharmonic functions with respect to the boundary values. We show that\nsubextension is a non-trivial property of $m$-subharmonic functions in such\nweighted energy classes. Moreover, we obtain the existence of a subextension\nmatrix of the corresponding subharmonic functions, and we show that the\nsubextension matrix is nonsingular in the sense of the Banach-Kukavica theorem\n(cf. [Banach-Kukavica",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.4666666666666667,
          "p": 0.3111111111111111,
          "f": 0.3733333285333334
        },
        "rouge-2": {
          "r": 0.2727272727272727,
          "p": 0.125,
          "f": 0.17142856711836746
        },
        "rouge-l": {
          "r": 0.43333333333333335,
          "p": 0.28888888888888886,
          "f": 0.34666666186666667
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.07615v1",
      "true_abstract": "Climate change is increasing the frequency and severity of natural disasters\nworldwide. Media coverage of these events may be vital to generate empathy and\nmobilize global populations to address the common threat posed by climate\nchange. Using a dataset of 466 news sources from 123 countries, covering 135\nmillion news articles since 2016, we apply an event study framework to measure\ncross-border media activity following natural disasters. Our results shows that\nwhile media attention rises after disasters, it is heavily skewed towards\ncertain events, notably earthquakes, accidents, and wildfires. In contrast,\nclimatologically salient events such as floods, droughts, or extreme\ntemperatures receive less coverage. This cross-border disaster reporting is\nstrongly related to the number of deaths associated with the event, especially\nwhen the affected populations share strong social ties or genetic similarities\nwith those in the reporting country. Achieving more balanced media coverage\nacross different types of natural disasters may be essential to counteract\nskewed perceptions. Further, fostering closer social connections between\ncountries may enhance empathy and mobilize the resources necessary to confront\nthe global threat of climate change.",
      "generated_abstract": "r examines how social and genetic ties influence the coverage of\ndisasters by news outlets across the globe. Using a novel dataset that\nquantifies the social ties between news outlets and individuals, we examine\nthe influence of social ties on coverage by news outlets in 130 countries from\n1970 to 2019. We find that news outlets with stronger social ties to their\ntarget communities are more likely to cover disasters and that this is more\npronounced in countries with higher levels of social inequality. We also find\nthat coverage by news outlets with stronger social ties is more likely to\ninclude stories from people with the same social ties as the news outlet,\nhighlighting the potential for news outlets to reinforce social norms. Our\nfindings highlight the potential for news outlets to use their social and\ngenetic ties to reinforce social",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.38235294117647056,
          "f": 0.26262625811651885
        },
        "rouge-2": {
          "r": 0.023391812865497075,
          "p": 0.03669724770642202,
          "f": 0.028571423816582425
        },
        "rouge-l": {
          "r": 0.16923076923076924,
          "p": 0.3235294117647059,
          "f": 0.22222221771247844
        }
      }
    },
    {
      "paper_id": "math.PR.math/PR/2503.09550v1",
      "true_abstract": "We prove that the limit profile of a sequence of reversible Markov chains\nexhibiting total variation cutoff is a continuous function, under a computable\ncondition involving the spectrum of the transition matrix and the cutoff\nwindow.",
      "generated_abstract": "We study the continuity of the limit profile of a reversible Markov chain\nwith a discrete state space. We prove that, for any initial probability\ndistribution, the limit profile of a reversible Markov chain with a discrete\nstate space is in general continuous and, under certain assumptions, Lipschitz.\nMoreover, we show that the limit profile of a reversible Markov chain with a\ncontinuous state space is in general Lipschitz.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.4642857142857143,
          "p": 0.37142857142857144,
          "f": 0.41269840776014116
        },
        "rouge-2": {
          "r": 0.2571428571428571,
          "p": 0.20930232558139536,
          "f": 0.23076922582182782
        },
        "rouge-l": {
          "r": 0.42857142857142855,
          "p": 0.34285714285714286,
          "f": 0.38095237601410936
        }
      }
    },
    {
      "paper_id": "math.NT.math/NT/2503.10443v1",
      "true_abstract": "We prove a completely explicit and effective upper bound for the\nN\\'eron--Tate height of rational points of curves of genus at least $2$ over\nnumber fields, provided that they have enough automorphisms with respect to the\nMordell--Weil rank of their jacobian. Our arguments build on Arakelov theory\nfor arithmetic surfaces. Our bounds are practical, and we illustrate this by\nexplicitly computing the rational points of a certain genus $2$ curve whose\njacobian has Mordell--Weil rank $2$.",
      "generated_abstract": "that, for any smooth projective curve $C$ of genus $g\\ge 2$ and any\nalgebraic group $G$ of rank at most $16$ acting effectively on $C$, there is an\ninfinite family of $G$-equivariant line bundles $L_1, \\ldots, L_n$ such that\nthe canonical map $H^1(C, \\Omega_C^1) \\to H^0(C, L_1)$ is surjective and the\nrestriction map $H^0(C, L_i) \\to H^0(C, L_j)$ is an isomorphism for $i \\ne\nj$. More generally, if $G$ is a $G$-equivariant linear algebraic group acting\neffectively on $C$, we prove that there is a $G$-equivariant line bundle $L$\nsuch that the canonical map $H",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22413793103448276,
          "p": 0.20634920634920634,
          "f": 0.214876028066389
        },
        "rouge-2": {
          "r": 0.013888888888888888,
          "p": 0.0125,
          "f": 0.01315788975069441
        },
        "rouge-l": {
          "r": 0.1896551724137931,
          "p": 0.1746031746031746,
          "f": 0.18181817682671964
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.10846v1",
      "true_abstract": "The digitalization of public administration has advanced significantly on a\nglobal scale. Many governments now view digital platforms as essential for\nimproving the delivery of public services and fostering direct communication\nbetween citizens and public institutions. However, this view overlooks the role\nplayed by digital intermediaries significantly shape the provision of\ne-government services. Using Chile as a case study, we analyze these\nintermediaries through a national survey on digitalization, we find five types\nof intermediaries: family members, peers, political figures, bureaucrats, and\ncommunity leaders. The first two classes comprise close intermediaries, while\nthe latter three comprise hierarchical intermediaries. Our findings suggest\nthat all these intermediaries are a critical but underexplored element in the\ndigitalization of public administration.",
      "generated_abstract": "-19 pandemic prompted governments around the world to accelerate\nthe implementation of e-government programs. However, the literature on e-\ngovernment adoption and usage often overlooks the role of digital intermediaries\n(DI) in facilitating government services. This study explores how DI influence\ne-government adoption and usage by examining the relationship between DI and\ne-government usage. We utilize panel data from 116 countries, and employ\nmultiple regression and difference-in-differences (DID) methods to test the\neffects of DI on e-government usage. Our findings indicate that DI positively\ninfluence e-government usage, with a dose-response relationship. Furthermore,\nDI positively influence e-government adoption, with a strong effect in high\nincome countries. The results also",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23255813953488372,
          "p": 0.2702702702702703,
          "f": 0.24999999502812506
        },
        "rouge-2": {
          "r": 0.04504504504504504,
          "p": 0.050505050505050504,
          "f": 0.04761904263537467
        },
        "rouge-l": {
          "r": 0.23255813953488372,
          "p": 0.2702702702702703,
          "f": 0.24999999502812506
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/CB/2407.11453v1",
      "true_abstract": "Telomeres are repetitive sequences of nucleotides at the end of chromosomes,\nwhose evolution over time is intrinsically related to biological ageing. In\nmost cells, with each cell division, telomeres shorten due to the so-called end\nreplication problem, which can lead to replicative senescence and a variety of\nage-related diseases. On the other hand, in certain cells, the presence of the\nenzyme telomerase can lead to the lengthening of telomeres, which may delay or\nprevent the onset of such diseases but can also increase the risk of cancer.In\nthis article, we propose a stochastic representation of this biological model,\nwhich takes into account multiple chromosomes per cell, the effect of\ntelomerase, different cell types and the dependence of the distribution of\ntelomere length on the dynamics of the process. We study theoretical properties\nof this model, including its long-term behaviour. In addition, we investigate\nnumerically the impact of the model parameters on biologically relevant\nquantities, such as the Hayflick limit and the Malthusian parameter of the\npopulation of cells.",
      "generated_abstract": "are repetitive DNA sequences that regulate the replication of DNA\nand the maintenance of chromosomal stability. They are composed of 30 base pairs\nof DNA that can be extended indefinitely by a process called telomerase\nreplication. The telomeres are considered to be a hallmark of biological\naging, and the stability of telomeres is crucial for maintaining genomic\nintegrity and cellular proliferation. Telomeres are considered to be a hallmark\nof biological aging, and the stability of telomeres is crucial for maintaining\ngenomic integrity and cellular proliferation. In this work, we present a\nstochastic model for telomeres that incorporates the telomerase activity. We\nalso extend the model to include the effect of a random perturbation of the\ntelomeres. We find that the stochastic effects of the telomerase",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21238938053097345,
          "p": 0.39344262295081966,
          "f": 0.2758620644120756
        },
        "rouge-2": {
          "r": 0.05660377358490566,
          "p": 0.09183673469387756,
          "f": 0.07003890578752171
        },
        "rouge-l": {
          "r": 0.19469026548672566,
          "p": 0.36065573770491804,
          "f": 0.2528735586649492
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/OT/2407.05572v2",
      "true_abstract": "This study addresses important issues of traffic congestion and vehicle\nemissions in urban areas by developing a comprehensive mathematical framework\nto evaluate Park-and-Ride (PnR) systems. The proposed approach integrates\nqueueing theory and emissions modeling to simultaneously assess waiting times,\ntravel times, and vehicle emissions under various PnR usage scenarios. The\nmethodology employs a novel combination of Monte Carlo simulation and matrix\ngeometric analytic methods to analyze a queueing network representing PnR\nfacilities and road traffic. A case study of Tsukuba, Japan demonstrates the\nmodel's applicability, revealing potential reductions in social costs related\nto total trip time and emissions through optimized PnR policies. Specifically,\nthe study found that implementing optimal bus frequency and capacity policies\ncould reduce total social costs by up to 30\\% compared to current conditions.\nThis research contributes to the literature by providing a unified framework\nfor evaluating PnR systems that considers both time and environmental costs,\noffering valuable insights for urban planners and policymakers seeking to\nimprove transportation sustainability. The proposed model utilizes a single\nserver queue with a deterministic service time and multiple arrival streams to\nrepresent traffic flow, incorporating both private cars and public buses.\nEmissions are calculated using the Methodologies for Estimating Air Pollutant\nEmissions from Transport (MEET) framework. The social cost of emissions and\ntotal trip time (SCETT) is introduced as a comprehensive metric for evaluating\nPnR system performance.",
      "generated_abstract": "ride is a common urban transportation strategy that can significantly\nreduced total trip time and improve the environmental performance of the\nurban transportation system. However, it is challenging to optimize the\nlocation of the parking facilities and the trip routing for the optimal\nreduction of total trip time and environmental impact. This paper presents an\nanalysis of the total trip time and emissions in the parking-free Park-and-Ride\nsystem and the optimal parking location and trip routing for urban transport.\nThe proposed model integrates the dynamic parking demand and vehicle emission\nmodels to capture the dynamic behavior of the parking demand and emissions,\nwhile considering the heterogeneity of parking spaces and vehicle types. The\nmodel is implemented in the Python package ParkAndRide, which provides a\ncomprehensive set of tools to facilitate the analysis and optimization of Park-\nand-Ride systems. The case studies of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20666666666666667,
          "p": 0.4025974025974026,
          "f": 0.2731277488210523
        },
        "rouge-2": {
          "r": 0.06103286384976526,
          "p": 0.10743801652892562,
          "f": 0.07784430675660682
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.38961038961038963,
          "f": 0.2643171761338276
        }
      }
    },
    {
      "paper_id": "cs.IR.cs/IR/2503.10166v1",
      "true_abstract": "With the proliferation of images in online content, language-guided image\nretrieval (LGIR) has emerged as a research hotspot over the past decade,\nencompassing a variety of subtasks with diverse input forms. While the\ndevelopment of large multimodal models (LMMs) has significantly facilitated\nthese tasks, existing approaches often address them in isolation, requiring the\nconstruction of separate systems for each task. This not only increases system\ncomplexity and maintenance costs, but also exacerbates challenges stemming from\nlanguage ambiguity and complex image content, making it difficult for retrieval\nsystems to provide accurate and reliable results. To this end, we propose\nImageScope, a training-free, three-stage framework that leverages collective\nreasoning to unify LGIR tasks. The key insight behind the unification lies in\nthe compositional nature of language, which transforms diverse LGIR tasks into\na generalized text-to-image retrieval process, along with the reasoning of LMMs\nserving as a universal verification to refine the results. To be specific, in\nthe first stage, we improve the robustness of the framework by synthesizing\nsearch intents across varying levels of semantic granularity using\nchain-of-thought (CoT) reasoning. In the second and third stages, we then\nreflect on retrieval results by verifying predicate propositions locally, and\nperforming pairwise evaluations globally. Experiments conducted on six LGIR\ndatasets demonstrate that ImageScope outperforms competitive baselines.\nComprehensive evaluations and ablation studies further confirm the\neffectiveness of our design.",
      "generated_abstract": "rieval has become a critical task in digital image analysis,\nrequiring the retrieval of similar images that capture the content of interest.\nIn this paper, we propose ImageScope, a unified framework for image retrieval\nthat integrates language-guided retrieval with collective reasoning in a\nmultimodal context. Our model leverages large language models (LLMs) to capture\nthe content of interest, enabling multimodal language-guided retrieval. To\naccommodate diverse collections of images, we introduce a novel multimodal\ncollective reasoning module that enables the model to collectively reason over\nimages across multiple modalities. We then propose an attention-guided\nretrieval module that leverages the model's collective reasoning to enhance\nretrieval performance. Experiments on three datasets demonstrate the effectiveness\nof our model, demonstrating a 12.9% improvement",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23780487804878048,
          "p": 0.4875,
          "f": 0.31967212674012363
        },
        "rouge-2": {
          "r": 0.0547945205479452,
          "p": 0.10714285714285714,
          "f": 0.07250754839258522
        },
        "rouge-l": {
          "r": 0.21951219512195122,
          "p": 0.45,
          "f": 0.29508196280569743
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2503.06251v1",
      "true_abstract": "Short-term patterns in financial time series form the cornerstone of many\nalgorithmic trading strategies, yet extracting these patterns reliably from\nnoisy market data remains a formidable challenge. In this paper, we propose an\nentropy-assisted framework for identifying high-quality, non-overlapping\npatterns that exhibit consistent behavior over time. We ground our approach in\nthe premise that historical patterns, when accurately clustered and pruned, can\nyield substantial predictive power for short-term price movements. To achieve\nthis, we incorporate an entropy-based measure as a proxy for information gain.\nPatterns that lead to high one-sided movements in historical data, yet retain\nlow local entropy, are more informative in signaling future market direction.\nCompared to conventional clustering techniques such as K-means and Gaussian\nMixture Models (GMM), which often yield biased or unbalanced groupings, our\napproach emphasizes balance over a forced visual boundary, ensuring that\nquality patterns are not lost due to over-segmentation. By emphasizing both\npredictive purity (low local entropy) and historical profitability, our method\nachieves a balanced representation of Buy and Sell patterns, making it better\nsuited for short-term algorithmic trading strategies.",
      "generated_abstract": "data is usually noisy and nonstationary. Aiming at improving\nthe robustness of the quality patterns, we propose an entropy-assisted quality\npattern identification method in finance. We first apply the entropy-based\nmethod to the quality patterns, which can effectively suppress the noise and\nreduce the nonstationarity in the data. Then, we use the information entropy\nwith a weighting function to determine the significance of the quality patterns\nand construct the quality patterns. Finally, we use the entropy-based method to\nidentify the quality patterns and construct the quality pattern sets. The\nproposed method effectively identifies the quality patterns and the quality\npattern sets in the data, and the quality patterns are more stable and\nreliable than the traditional quality patterns. The performance of the\nproposed method is evaluated based on the quality patterns and the quality\npattern sets. Experimental results show that the proposed method can effectively\nident",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1865671641791045,
          "p": 0.36231884057971014,
          "f": 0.24630541423184263
        },
        "rouge-2": {
          "r": 0.04046242774566474,
          "p": 0.0673076923076923,
          "f": 0.05054151155573556
        },
        "rouge-l": {
          "r": 0.17164179104477612,
          "p": 0.3333333333333333,
          "f": 0.22660098073430568
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2501.17096v1",
      "true_abstract": "Estimating market impact and transaction costs of large trades (metaorders)\nis a very important topic in finance. However, using models of price and trade\nbased on public market data provide average price trajectories which are\nqualitatively different from what is observed during real metaorder executions:\nthe price increases linearly, rather than in a concave way, during the\nexecution and the amount of reversion after its end is very limited. We claim\nthat this is a generic phenomenon due to the fact that even sophisticated\nstatistical models are unable to correctly describe the origin of the\nautocorrelation of the order flow. We propose a modified Transient Impact Model\nwhich provides more realistic trajectories by assuming that only a fraction of\nthe metaorder trading triggers market order flow. Interestingly, in our model\nthere is a critical condition on the kernels of the price and order flow\nequations in which market impact becomes permanent.",
      "generated_abstract": "r investigates the estimation of metaorder impact on the NYSE and\nthe NASDAQ stock exchanges using public market data. We present an empirical\nstudy based on the FIN5303 and FIN5304 course of the Master's degree in Finance\nat the University of Bologna, Italy. The study aims to assess the accuracy of\nthe estimation of metaorder impact on the NYSE and NASDAQ stock exchanges using\nthe public market data. We used 150,000 metaorder executions, recorded from the\nNYSE and NASDAQ stock exchanges, and 3,000 metaorder executions, recorded from\nthe London Stock Exchange. We employed different estimation methods, including\na regression model, a support vector machine model, and a neural network model.\nThe results show that the neural network model performs the best. The accuracy\nof the model is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20618556701030927,
          "p": 0.29411764705882354,
          "f": 0.24242423757869613
        },
        "rouge-2": {
          "r": 0.04285714285714286,
          "p": 0.061224489795918366,
          "f": 0.05042016322293671
        },
        "rouge-l": {
          "r": 0.15463917525773196,
          "p": 0.22058823529411764,
          "f": 0.1818181769726356
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.physics/bio-ph/2503.06239v1",
      "true_abstract": "Interactions between crawling cells, which are essential for many biological\nprocesses, can be quantified by measuring cell-cell collisions. Conventionally,\nexperiments of cell-cell collisions are conducted on two-dimensional flat\nsubstrates, where colliding cells repolarize and move away upon contact with\none another in \"contact inhibition of locomotion\" (CIL). Inspired by recent\nexperiments that show cells on suspended nanofibers have qualitatively\ndifferent CIL behaviors than those on flat substrates, we develop a phase field\nmodel of cell motility and two-cell collisions in fiber geometries. Our model\nincludes cell-cell and cell-fiber adhesion, and a simple positive feedback\nmechanism of cell polarity. We focus on cell collisions on two parallel fibers,\nfinding that larger cell deformability (lower membrane tension), larger\npositive feedback of polarization, and larger fiber spacing promote more\noccurrences of cells walking past one another. We can capture this behavior\nusing a simple linear stability analysis on the cell-cell interface upon\ncollision.",
      "generated_abstract": "Cell-cell interactions are ubiquitous and central to cellular processes such\nas self-organization, migration, and cancer progression. Yet, the underlying\nmechanisms remain poorly understood. This review examines the effects of\ncell-cell interactions on cell-cell contact geometry and its influence on cell\npolarity. We highlight how cell-cell contact geometry and polarity influence\ncell-cell collision outcomes and how cell-cell interactions shape cellular\nproperties such as the growth of tumors. We then discuss the potential role of\ncell-cell interactions in cancer progression and how the interplay between cell\nmechanics and polarity control cell-cell collision outcomes shapes tumor\nbehavior. We conclude by discussing future research directions in the field of\ncell-cell interactions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.25396825396825395,
          "f": 0.19161676176843928
        },
        "rouge-2": {
          "r": 0.027972027972027972,
          "p": 0.04395604395604396,
          "f": 0.03418802943494843
        },
        "rouge-l": {
          "r": 0.14423076923076922,
          "p": 0.23809523809523808,
          "f": 0.17964071386424768
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.15422v1",
      "true_abstract": "We study the classical object reallocation problem under strict preferences,\nwith a focus on characterizing \"TTC domains\" -- preference domains on which the\nTop Trading Cycles (TTC) mechanism is the unique mechanism satisfying\nindividual rationality, Pareto efficiency, and strategyproofness. We introduce\na sufficient condition for a domain to be a TTC domain, which we call the\ntop-two condition. This condition requires that, within any subset of objects,\nif two objects can each be most-preferred, they can also be the top-two\nmost-preferred objects (in both possible orders). A weaker version of this\ncondition, applying only to subsets of size three, is shown to be necessary.\nThese results provide a complete characterization of TTC domains for the case\nof three objects, unify prior studies on specific domains such as single-peaked\nand single-dipped preferences, and classify several previously unexplored\ndomains as TTC domains or not.",
      "generated_abstract": "This paper introduces the TTC domains, which are a novel framework for\ndomains that are more general than the TTC (Time Travel Compatibility) domain.\nWe propose a generic approach to construct TTC domains, which are defined as\nequivalence classes of sets of time travels that are compatible with a\ngiven time travel. We then use this approach to construct TTC domains for the\nclasses of time travels defined by the Cauchy time travel and the\ncontinuation-return time travel. In particular, we show that the Cauchy time\ntravel is a TTC domain. We further show that the continuation-return time\ntravel is a TTC domain when the continuation is a time travel and the\nreturn is a non-time travel.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16831683168316833,
          "p": 0.30357142857142855,
          "f": 0.21656050496490742
        },
        "rouge-2": {
          "r": 0.036231884057971016,
          "p": 0.058823529411764705,
          "f": 0.04484304460978553
        },
        "rouge-l": {
          "r": 0.15841584158415842,
          "p": 0.2857142857142857,
          "f": 0.20382165146172265
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2412.18563v3",
      "true_abstract": "Artificial intelligence is transforming financial investment decision-making\nframeworks, with deep reinforcement learning demonstrating substantial\npotential in robo-advisory applications. This paper addresses the limitations\nof traditional portfolio optimization methods in dynamic asset weight\nadjustment through the development of a deep reinforcement learning-based\ndynamic optimization model grounded in practical trading processes. The\nresearch advances two key innovations: first, the introduction of a novel\nSharpe ratio reward function engineered for Actor-Critic deep reinforcement\nlearning algorithms, which ensures stable convergence during training while\nconsistently achieving positive average Sharpe ratios; second, the development\nof an innovative comprehensive approach to portfolio optimization utilizing\ndeep reinforcement learning, which significantly enhances model optimization\ncapability through the integration of random sampling strategies during\ntraining with image-based deep neural network architectures for\nmulti-dimensional financial time series data processing, average Sharpe ratio\nreward functions, and deep reinforcement learning algorithms. The empirical\nanalysis validates the model using randomly selected constituent stocks from\nthe CSI 300 Index, benchmarking against established financial econometric\noptimization models. Backtesting results demonstrate the model's efficacy in\noptimizing portfolio allocation and mitigating investment risk, yielding\nsuperior comprehensive performance metrics.",
      "generated_abstract": "This study employs a deep reinforcement learning (DRL) approach to investigate\nthe impact of market dynamics on dynamic portfolio optimization. The framework\ncombines a neural network model with a value-at-risk (VaR) metric to model the\nrisk-return relationship. A reinforcement learning agent is trained to select\nthe optimal portfolio weight at each trading round, while considering the\nvolatility of the stock market. The agent's decisions are backpropagated to\nimprove the model's performance. The results demonstrate that the DRL framework\nachieves superior performance compared to traditional methods, particularly in\nthe early stages of trading. It also identifies key market dynamics that\ninfluence the portfolio's performance, such as stock market volatility,\ninterest rate, and economic growth.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23622047244094488,
          "p": 0.36585365853658536,
          "f": 0.28708133494471283
        },
        "rouge-2": {
          "r": 0.04878048780487805,
          "p": 0.07339449541284404,
          "f": 0.05860805381100014
        },
        "rouge-l": {
          "r": 0.2047244094488189,
          "p": 0.3170731707317073,
          "f": 0.2488038229829904
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/ST/2412.04263v1",
      "true_abstract": "A simple model-free and distribution-free statistic, the functional\nrelationship between the number of \"effective\" degrees of freedom and portfolio\nsize, or N*(N), is used to discriminate between two alternative models for the\ncorrelation of daily cryptocurrency returns within a retail universe of defined\nby the list of tradable assets available to account holders at the Robinhood\nbrokerage. The average pairwise correlation between daily cryptocurrency\nreturns is found to be high (of order 60%) and the data collected supports\ndescription of the cross-section of returns by a simple isotropic correlation\nmodel distinct from a decomposition into a linear factor model with additive\nnoise with high confidence. This description appears to be relatively stable\nthrough time.",
      "generated_abstract": "This study examines the lack of cross-market correlation in retail cryptocurrency\nmarket prices using a large panel of cryptocurrencies from 2017 to 2024. Our\nfindings show that factor-based approaches are unable to capture the\ncorrelation between cryptocurrencies. In contrast, a novel multivariate\nfactor-free model captures significant cross-market correlations, offering\nvaluable insights into the volatile dynamics of cryptocurrencies. This study\noffers a novel perspective on cryptocurrency market dynamics and provides\ninsights into the influence of cryptocurrencies on each other.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.23636363636363636,
          "f": 0.19548871695403935
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.02857142857142857,
          "f": 0.02222221746913682
        },
        "rouge-l": {
          "r": 0.1282051282051282,
          "p": 0.18181818181818182,
          "f": 0.15037593499915217
        }
      }
    },
    {
      "paper_id": "stat.ML.q-fin/ST/2502.11310v1",
      "true_abstract": "We tackle the challenges of modeling high-dimensional data sets, particularly\nthose with latent low-dimensional structures hidden within complex, non-linear,\nand noisy relationships. Our approach enables a seamless integration of\nconcepts from non-parametric regression, factor models, and neural networks for\nhigh-dimensional regression. Our approach introduces PCA and Soft PCA layers,\nwhich can be embedded at any stage of a neural network architecture, allowing\nthe model to alternate between factor modeling and non-linear transformations.\nThis flexibility makes our method especially effective for processing\nhierarchical compositional data. We explore ours and other techniques for\nimposing low-rank structures on neural networks and examine how architectural\ndesign impacts model performance. The effectiveness of our method is\ndemonstrated through simulation studies, as well as applications to forecasting\nfuture price movements of equity ETF indices and nowcasting with macroeconomic\ndata.",
      "generated_abstract": "We propose a generalized factor neural network (FNN) model to perform high-dimensional\nregression. The FNN is based on the factor neural network, which is a neural\nnetwork that maps a high-dimensional input to a factor, which is a linear\ncombination of low-dimensional features. We show that FNNs are special cases\nof factor neural networks. We also show that the FNN can be viewed as a\nspecial case of the factor neural network when the factor neural network is\napplied to a regression problem. We further show that the FNN can be viewed as a\nspecial case of the factor neural network when the factor neural network is\napplied to a regression problem with a general target variable. We provide a\nneural network implementation of the FNN model and provide theoretical\nresults on the convergence of the FNN model. We illustrate the model on the\napplication of FNNs to high-dimensional regression problems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22,
          "p": 0.3728813559322034,
          "f": 0.27672955508089087
        },
        "rouge-2": {
          "r": 0.05426356589147287,
          "p": 0.06930693069306931,
          "f": 0.06086956029149379
        },
        "rouge-l": {
          "r": 0.21,
          "p": 0.3559322033898305,
          "f": 0.2641509387286896
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.07527v1",
      "true_abstract": "This paper presents a novel method for real-time lifting-load estimation to\nenhance the control strategies of upper-limb assistive exoskeletons. By\nleveraging cost-effective insole pressure sensors, the proposed system extracts\ndifferential pressure data that minimizes disturbances from variations in body\nweight and sensor placement. Two modeling approaches are explored: a\nchannel-based method that employs traditional regression techniques-Elastic\nNet, Support Vector Regression (SVR), and Multi-Layer Perceptron (MLP)-and a\nmap-based method that utilizes transfer learning with a pre-trained MobileNetV2\nmodel. The experiment is in the preliminary test stage, covering load ranges\nfrom 2 kg to 10 kg in increments of 0.5 kg, and collecting data from three\nsubjects to test the approach. In the Channel-based method, the average\nWeighted Mean Absolute Percentage Error(WMAPE) for three subjects showed that\nthe SVR achieved 13.46%, with the MLP performing similarly. In the Map-based\nmethod, using data from one subject, the Fully Fine-Tuned MobileNetV2 model\nreached a WMAPE of 9.74%. The results indicate that the integration of insole\nsensor technology with advanced machine learning models provides an effective\nsolution for dynamic load estimation, potentially reducing the risks of over-\nand under-compensation in exoskeleton control.",
      "generated_abstract": "ons are being developed to enhance human performance in work\nexercises, enhancing work capacity and efficiency, and reducing the burden of\nphysical labor. Exoskeletons provide a variety of benefits, including\nreducing muscle fatigue and improving work performance, but their effectiveness\nis dependent on accurate load estimation. In this study, we propose a method\nto estimate the workload of an exoskeleton user using insole pressure sensors\nand machine learning. We first propose a method to convert insole pressure\nsensors to workload estimates using a neural network. We then apply a\nsupervised learning algorithm to train the neural network using the\ninsole pressure sensor measurements and the user's workload. The accuracy of\nthe model is assessed by comparing the estimated workload to the actual workload\nmeasured through the accelerometer in the exoskeleton. Our results show that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19548872180451127,
          "p": 0.3023255813953488,
          "f": 0.23744291760472058
        },
        "rouge-2": {
          "r": 0.0273224043715847,
          "p": 0.03968253968253968,
          "f": 0.032362454717064844
        },
        "rouge-l": {
          "r": 0.18045112781954886,
          "p": 0.27906976744186046,
          "f": 0.2191780774220722
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09919v1",
      "true_abstract": "We provide a family of $5$-dimensional prismatoids whose width grows linearly\nin the number of vertices. This provides a new infinite family of\ncounter-examples to the Hirsch conjecture whose excess width grows linearly in\nthe number of vertices, and answers a question of Matschke, Santos and Weibel.",
      "generated_abstract": "aper, we introduce a new class of $k$-th roots of unity, which are\ndrums of width $k$. The main result is that every $k$-th root of unity is a\ndrum of width $k$. Moreover, we prove that every $k$-th root of unity is a\ndrum of width $k$ if and only if it is a primitive $(k+1)$-th root of unity.\nWe also prove that every drum of width $k$ is a $k$-th root of unity. As a\nconsequence, we show that the class of $k$-th roots of unity forms a group under\nmultiplication, and we obtain a proof of the existence of a primitive $(k+1)$-th\nroot of unity. We also study the group structure of the class of $k$-th roots\nof unity, and we give a characterization of the $k$-th roots of unity which are",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22580645161290322,
          "p": 0.14285714285714285,
          "f": 0.17499999525312515
        },
        "rouge-2": {
          "r": 0.02564102564102564,
          "p": 0.011904761904761904,
          "f": 0.016260158270871666
        },
        "rouge-l": {
          "r": 0.22580645161290322,
          "p": 0.14285714285714285,
          "f": 0.17499999525312515
        }
      }
    },
    {
      "paper_id": "hep-ex.physics/data-an/2503.06727v1",
      "true_abstract": "The Precision Reactor Oscillation and Spectrum Experiment, PROSPECT, was a\nsegmented antineutrino detector that successfully operated at the High Flux\nIsotope Reactor in Oak Ridge, TN, during its 2018 run. Despite challenges with\nphotomultiplier tube base failures affecting some segments, innovative machine\nlearning approaches were employed to perform position and energy\nreconstruction, and particle classification. This work highlights the\neffectiveness of convolutional neural networks and graph convolutional networks\nin enhancing data analysis. By leveraging these techniques, a 3.3% increase in\neffective statistics was achieved compared to traditional methods, showcasing\ntheir potential to improve performance. Furthermore, these machine learning\nmethodologies offer promising applications for other segmented particle\ndetectors, underscoring their versatility and impact.",
      "generated_abstract": "ded Electromagnetic Calorimeters (SEEC) are an important tool for\nsearches for new physics. In the PROSPECT experiment, an electronics board is\nused to digitize the EM calorimeter readout, and the data are reconstructed\nusing a particle-flow algorithm. This paper presents a machine learning\nmethodology that improves the performance of the particle-flow algorithm in the\nPROSPECT experiment. The algorithm is trained on a subset of the dataset,\ncomprising a sample of 1000 events. The training dataset is then used to\nevaluate the performance of the trained algorithm. The results show that the\nmachine learning-based algorithm achieves a performance improvement of 2.9\\% in\nthe mean reconstruction uncertainty in the muon channel. This improvement is\ndue to a reduction in the number of calibration uncertainties. The machine\nlearning-based algorithm",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15053763440860216,
          "p": 0.1891891891891892,
          "f": 0.16766466572340363
        },
        "rouge-2": {
          "r": 0.009009009009009009,
          "p": 0.008771929824561403,
          "f": 0.00888888388978059
        },
        "rouge-l": {
          "r": 0.15053763440860216,
          "p": 0.1891891891891892,
          "f": 0.16766466572340363
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.09794v1",
      "true_abstract": "As Augmented Reality (AR) and Artificial Intelligence (AI) continue to\nconverge, new opportunities emerge for AI agents to actively support human\ncollaboration in immersive environments. While prior research has primarily\nfocused on dyadic human-AI interactions, less attention has been given to\nHuman-AI Teams (HATs) in AR, where AI acts as an adaptive teammate rather than\na static tool. This position paper takes the perspective of team dynamics and\nwork organization to propose that AI agents in AR should not only interact with\nindividuals but also recognize and respond to team-level needs in real time. We\nargue that spatially aware AI agents should dynamically generate the resources\nnecessary for effective collaboration, such as virtual blackboards for\nbrainstorming, mental map models for shared understanding, and memory recall of\nspatial configurations to enhance knowledge retention and task coordination.\nThis approach moves beyond predefined AI assistance toward context-driven AI\ninterventions that optimize team performance and decision-making.",
      "generated_abstract": "We propose a novel approach for enhancing the collaborative capabilities of\nteamwork within distributed systems, leveraging AI agents that actively\nparticipate in the team's decision-making processes. Our approach integrates\nthe AI agents with the team's existing tools, enabling them to collaborate\neffectively through distributed teamwork. We demonstrate our approach through\ntwo case studies, where we investigate the effectiveness of our approach in\nsimulated and real-world scenarios. In the simulated scenarios, our approach\neffectively enhances the team's collaborative capabilities, resulting in\nimproved decision-making and team performance. In the real-world scenario, our\napproach enhances the team's collaboration capabilities, resulting in improved\nperformance. Our approach provides a scalable and efficient way to enhance\ndistributed teamwork capabilities through AI agents, offering potential\nbenefits for teams across various domains.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.175,
          "p": 0.2916666666666667,
          "f": 0.21874999531250008
        },
        "rouge-2": {
          "r": 0.020134228187919462,
          "p": 0.027522935779816515,
          "f": 0.02325580907367448
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.2777777777777778,
          "f": 0.20833332864583345
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.cond-mat/soft/2503.09564v1",
      "true_abstract": "Under an externally applied load, granular packings form force chains that\ndepend on the contact network and moduli of the grains. In this work, we\ninvestigate packings of variable modulus (VM) particles, where we can direct\nforce chains by changing the Young's modulus of individual particles within the\npacking on demand. Each VM particle is made of a silicone shell that\nencapsulates a core made of a low-melting-point metallic alloy (Field's metal).\nBy sending an electric current through a co-located copper heater, the Field's\nmetal internal to each particle can be melted via Joule heating, which softens\nthe particle. As the particle cools to room temperature, the alloy solidifies\nand the particle recovers its original modulus. To optimize the mechanical\nresponse of granular packings containing both soft and stiff particles, we\nemploy an evolutionary algorithm coupled with discrete element method\nsimulations to predict the patterns of particle moduli that will yield specific\nforce outputs on the assembly boundaries. The predicted patterns of particle\nmoduli from the simulations were realized in experiments using 2D assemblies of\nVM particles and the force outputs on the assembly boundaries were measured\nusing photoelastic techniques. These studies represent a step towards making\nrobotic granular metamaterials that can dynamically adapt their mechanical\nproperties in response to different environmental conditions or perform\nspecific tasks on demand.",
      "generated_abstract": "t a theoretical framework to study the evolution of adaptive\nforce chains in reconfigurable granular metamaterials (RGMs). We employ the\ntime-dependent Ginzburg-Landau (TG-L) theory to describe the response of a\nforce chain to external perturbations. The TG-L theory provides a\nself-consistent description of the force chain in the limit of slow\nperturbations. In the limit of fast perturbations, the TG-L theory predicts a\nphase transition from a stable to an unstable phase, where the chain\ndiverges. This phase transition is attributed to the spontaneous formation of\na metastable phase, where the chain relaxes to a new equilibrium. We show that\nthis metastable phase can be stabilized by introducing a force-changing\nmaterial, which is responsible for the slow and fast perturbations. The\nstability of this",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17266187050359713,
          "p": 0.3333333333333333,
          "f": 0.227488147162912
        },
        "rouge-2": {
          "r": 0.03482587064676617,
          "p": 0.061946902654867256,
          "f": 0.04458598265386068
        },
        "rouge-l": {
          "r": 0.1510791366906475,
          "p": 0.2916666666666667,
          "f": 0.19905212820556603
        }
      }
    },
    {
      "paper_id": "cs.GT.stat/OT/2502.11645v1",
      "true_abstract": "Many real-world multi-agent or multi-task evaluation scenarios can be\nnaturally modelled as normal-form games due to inherent strategic (adversarial,\ncooperative, and mixed motive) interactions. These strategic interactions may\nbe agentic (e.g. players trying to win), fundamental (e.g. cost vs quality), or\ncomplementary (e.g. niche finding and specialization). In such a formulation,\nit is the strategies (actions, policies, agents, models, tasks, prompts, etc.)\nthat are rated. However, the rating problem is complicated by redundancy and\ncomplexity of N-player strategic interactions. Repeated or similar strategies\ncan distort ratings for those that counter or complement them. Previous work\nproposed ``clone invariant'' ratings to handle such redundancies, but this was\nlimited to two-player zero-sum (i.e. strictly competitive) interactions. This\nwork introduces the first N-player general-sum clone invariant rating, called\ndeviation ratings, based on coarse correlated equilibria. The rating is\nexplored on several domains including LLMs evaluation.",
      "generated_abstract": "re critical for assessing the quality of items or services. They\nrepresent a user's evaluation of the features of a product, service, or\nperson. However, ratings often contain inaccurate or biased information due to\nincomplete information, biases, or inconsistent ratings. For example, a\nconsumer may rate a product as 5 stars but later change their mind and rate it\nas 3 stars. This can lead to misleading or incomplete information, such as\ninconsistent ratings or missing ratings.\n  We propose a novel rating method, called deviation ratings, to overcome these\nchallenges. Our method takes a set of ratings and generates new ratings by\nincorporating the deviation between the original ratings and the most\nrecently observed ratings. We evaluate our method on a variety of datasets,\nincluding ratings for music, movies, and games. Our method outperforms existing\nmethods in terms",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22608695652173913,
          "p": 0.27956989247311825,
          "f": 0.24999999505593573
        },
        "rouge-2": {
          "r": 0.027972027972027972,
          "p": 0.030303030303030304,
          "f": 0.02909090409890995
        },
        "rouge-l": {
          "r": 0.21739130434782608,
          "p": 0.26881720430107525,
          "f": 0.24038461044055115
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/NI/2503.06785v1",
      "true_abstract": "As reliance on space systems continues to increase, so does the need to\nensure security for them. However, public work in space standards have\nstruggled with defining security protocols that are well tailored to the domain\nand its risks. In this work, we investigate various space networking paradigms\nand security approaches, and identify trade-offs and gaps. Furthermore, we\ndescribe potential existing security protocol approaches that fit well into the\nspace network paradigm in terms of both functionality and security. Finally, we\nestablish future directions for enabling strong security for space\ncommunication.",
      "generated_abstract": "lishment is a fundamental security requirement for the\nsecurity of space-based systems. This study presents a key establishment\narchitecture based on the use of an authentication token, a secure key\ndistribution protocol, and a cryptographic signature protocol. The\narchitecture provides a secure and scalable solution for key establishment in\nthe space environment. The key establishment process is divided into three\nphases: key exchange, key establishment, and key revocation. Key exchange\ninvolves the exchange of authentication tokens between the key establishment\npoint (KEP) and the target system, while key establishment and key revocation\nare performed at the KEP. The key establishment protocol is based on the\nSecure Key Exchange Protocol (SKEP) for the KEP, and the key establishment\nprotocol for the target system. The key establishment protocol uses the\nSignature-based Message Authentication Code (SMAC) to generate the\nsignature, and the cryptographic signature protocol is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19402985074626866,
          "p": 0.17567567567567569,
          "f": 0.18439715813289087
        },
        "rouge-2": {
          "r": 0.011363636363636364,
          "p": 0.008547008547008548,
          "f": 0.009756092661037559
        },
        "rouge-l": {
          "r": 0.16417910447761194,
          "p": 0.14864864864864866,
          "f": 0.15602836380664972
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.01298v2",
      "true_abstract": "Measuring inequality of opportunities has long been a challenging and open\nproblem, primarily due to the limitations associated with individual-level\ndata. In this study, we utilize data obtained from vehicle license plates in a\ncomprehensive survey (17258 vehicles from 6 major cities in China) to evaluate\nthe inequality of opportunities in the country. In our context, we define\ninequality of opportunity as the scenario where relatively expensive vehicles\nhave a higher likelihood of being paired with license plates featuring 'Lucky\nNumbers'. To quantify this, we propose a lucky-number-based opportunity Gini\ncoefficient. Through the calculation of the opportunity Gini coefficient, we\nobserve a significant and positive correlation between opportunity inequality\nand income inequality. Particularly noteworthy is our finding that the\nadvancement of technology, exemplified by the widespread adoption of new energy\nvehicles, can substantially reduce the inequality of opportunity. Taking\nincorporation of a random lottery process before acquiring a motor vehicle in\nBeijing and Shanghai as a natural experiment, our empirical results support the\nargument that, in terms of equality, employing random drawing is a fair and\nequitable approach for allocating scarce resources.",
      "generated_abstract": "the license plate policy, which provides licenses to owners of\nattached vehicles, has become a major source of inequality in urban areas.\nThis paper analyzes the effects of the license plate policy on income and\nwealth inequality in Shanghai, China. We find that the license plate policy\nsignificantly decreases the income and wealth of the poorest 20 percent of the\npopulation and increases the wealth of the richest 20 percent. These effects\nare more pronounced for low-income and middle-income households. The\ninequality is driven by the difference in the wealth of the poorest 20 percent\nof the population and the richest 20 percent. The effect of the license plate\npolicy on inequality is larger in cities than in towns. In cities, the\ninequality is driven by the difference in the wealth of the poorest 20 percent\nof the population and the richest",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15,
          "p": 0.2857142857142857,
          "f": 0.19672130696049456
        },
        "rouge-2": {
          "r": 0.028901734104046242,
          "p": 0.05555555555555555,
          "f": 0.03802280918619667
        },
        "rouge-l": {
          "r": 0.14166666666666666,
          "p": 0.2698412698412698,
          "f": 0.1857923452118607
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2410.01378v1",
      "true_abstract": "This paper studies robust forward investment and consumption preferences and\noptimal strategies for a risk-averse and ambiguity-averse agent in an\nincomplete financial market with drift and volatility uncertainties. We focus\non non-zero volatility and constant relative risk aversion (CRRA) forward\npreferences. Given the non-convexity of the Hamiltonian with respect to\nuncertain volatilities, we first construct robust randomized forward\npreferences through endogenous randomization in an auxiliary market. We derive\nthe corresponding optimal and robust investment and consumption strategies.\nFurthermore, we show that such forward preferences and strategies, developed in\nthe auxiliary market, remain optimal and robust in the physical market,\noffering a comprehensive framework for forward investment and consumption under\nmodel uncertainty.",
      "generated_abstract": "This paper examines the effects of drift and volatility uncertainties on\nforward investment and consumption decisions in a dynamic stochastic\ngeneral equilibrium (DSGE) model. We introduce a novel randomization method,\nwhich allows us to investigate the effects of drift and volatility uncertainty\non investment decisions through a simple Monte Carlo simulation. The\nsimulation results indicate that drift uncertainty has a significant impact on\nforward investment decisions, while volatility uncertainty has a more subtle\neffect on investment decisions. These findings provide valuable insights into\nthe impact of drift and volatility uncertainty on forward investment and\nconsumption decisions in a DSGE model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3088235294117647,
          "p": 0.3620689655172414,
          "f": 0.3333333283648275
        },
        "rouge-2": {
          "r": 0.07142857142857142,
          "p": 0.09333333333333334,
          "f": 0.0809248505797056
        },
        "rouge-l": {
          "r": 0.2647058823529412,
          "p": 0.3103448275862069,
          "f": 0.28571428074577987
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2412.00986v1",
      "true_abstract": "We study a problem of optimal irreversible investment and emission reduction\nformulated as a nonzero-sum dynamic game between an investor with environmental\npreferences and a firm. The game is set in continuous time on an infinite-time\nhorizon. The firm generates profits with a stochastic dynamics and may spend\npart of its revenues towards emission reduction (e.g., renovating the\ninfrastructure). The firm's objective is to maximize the discounted expectation\nof a function of its profits. The investor participates in the profits and may\ndecide to invest to support the firm's production capacity. The investor uses a\nprofit function which accounts for both financial and environmental factors.\nNash equilibria of the game are obtained via a system of variational\ninequalities. We formulate a general verification theorem for this system in a\ndiffusive setup and construct an explicit solution in the zero-noise limit. Our\nexplicit results and numerical approximations show that both the investor's and\nthe firm's optimal actions are triggered by moving boundaries that increase\nwith the total amount of emission abatement.",
      "generated_abstract": "uce a model of strategic sustainable investment where agents\ninvestigate their investment opportunities based on their investment\nobjectives and risk tolerance. The investment objectives of the agents are\nconsidered as a stochastic process, and the investment risks are modeled by a\nstochastic process. We derive the investment strategy of the investor that\nmaximizes the expected utility. We derive the optimal portfolio strategy, and\nshow that the optimal portfolio strategy is the sum of the optimal investment\nstrategy and the optimal holding strategy. We also show that the optimal\ninvestment strategy is the expected utility of the optimal holding strategy,\nwhich is the expected utility of the optimal investment strategy. We use\nsimulations to study the properties of the investment strategy. We also use\nsimulations to study the properties of the optimal investment strategy. We\nfind that the optimal investment strategy has the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19047619047619047,
          "p": 0.37735849056603776,
          "f": 0.25316455250360526
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.05813953488372093,
          "f": 0.03984063294550932
        },
        "rouge-l": {
          "r": 0.19047619047619047,
          "p": 0.37735849056603776,
          "f": 0.25316455250360526
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2404.07658v1",
      "true_abstract": "This paper extends the valuation and optimal surrender framework for variable\nannuities with guaranteed minimum benefits in a L\\'evy equity market\nenvironment by incorporating a stochastic interest rate described by the\nHull-White model. This approach frames a more dynamic and realistic financial\nsetting compared to previous literature. We exploit a robust valuation\nmechanism employing a hybrid numerical method that merges tree methods for\ninterest rate modeling with finite difference techniques for the underlying\nasset price. This method is particularly effective for addressing the\ncomplexities of variable annuities, where periodic fees and mortality risks are\nsignificant factors. Our findings reveal the influence of stochastic interest\nrates on the strategic decision-making process concerning the surrender of\nthese financial instruments. Through comprehensive numerical experiments, and\nby comparing our results with those obtained through the Longstaff-Schwartz\nMonte Carlo method, we illustrate how our refined model can guide insurers in\ndesigning contracts that equitably balance the interests of both parties. This\nis particularly relevant in discouraging premature surrenders while adapting to\nthe realistic fluctuations of financial markets. Lastly, a comparative statics\nanalysis with varying interest rate parameters underscores the impact of\ninterest rates on the cost of the optimal surrender strategy, emphasizing the\nimportance of accurately modeling stochastic interest rates.",
      "generated_abstract": "r studies the valuation of variable annuities in the presence of\nl\u00e9vy noise. We introduce the notion of stochastic interest rate and the\nstochastic interest rate model (SIRM), which captures the interaction of\nvolatility and interest rate in the annuity's price. We derive the\nrepresentative value (RV) formula for annuities in the SIRM, and derive the\nformula for the annuity's payoff in the SIRM using the Black-Scholes formula.\nThen, we extend the RV formula to the stochastic interest rate model. Finally,\nwe show that the Black-Scholes formula is the limiting case of the\nstochastic interest rate model under a mild condition. We show that the Black-Scholes\nformula provides a valid approximation of the stochastic interest rate model\nwhen the volatility is low and the interest rate is stable",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14074074074074075,
          "p": 0.3275862068965517,
          "f": 0.19689118750570495
        },
        "rouge-2": {
          "r": 0.04639175257731959,
          "p": 0.09375,
          "f": 0.06206896108822862
        },
        "rouge-l": {
          "r": 0.14074074074074075,
          "p": 0.3275862068965517,
          "f": 0.19689118750570495
        }
      }
    },
    {
      "paper_id": "math.QA.math/OA/2502.19876v3",
      "true_abstract": "This paper explores Frobenius subalgebra posets within abelian monoidal\ncategories and shows that they form lattices under certain conditions,\nincluding all semisimple tensor categories. Furthermore, it extends Watatani's\ntheorem on the finiteness of intermediate subfactors, proving that these\nlattices are finite under weak positivity constraints, encompassing all\nsemisimple tensor categories as well. The primary tools employed in this paper\nare semisimplification and a concept of formal angle. Additionally, we have\nbroadened several intermediate results, such as the exchange relation theorem\nand Landau's theorem, to apply to abelian monoidal categories. Key applications\nof our findings include a stronger version of the Ino-Watatani result: we show\nthat the finiteness of intermediate C*-algebras holds in a finite-index unital\nirreducible inclusion of C*-algebras without requiring the simple assumption.\nMoreover, for a finite-dimensional semisimple Hopf algebra H, we demonstrate\nthat H* contains a finite number of Frobenius subalgebra objects in Rep(H).\nFinally, we explore a range of applications, including abstract spin chains,\nvertex operator algebras, and speculations on quantum arithmetic involving the\ngeneralization of Ore's theorem, Euler's totient and sigma functions, and RH.",
      "generated_abstract": "Frobenius subalgebras are a natural generalization of subalgebras of\nalgebraic groups, and they are of central importance in algebraic geometry. In\nthis paper, we introduce Frobenius subalgebra lattices, which are a new\ngeneralization of Frobenius subalgebra lattices, and show that Frobenius\nsubalgebra lattices in tensor categories are Frobenius subalgebra lattices in\ntheir own right. We also investigate Frobenius subalgebra lattices in\ntensor-like categories, which are categorical generalizations of tensor\ncategories. We show that Frobenius subalgebra lattices in tensor-like categories\nare Frobenius subalgebra lattices in the tensor-like categories of their own\nright.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14049586776859505,
          "p": 0.425,
          "f": 0.2111801204891787
        },
        "rouge-2": {
          "r": 0.02976190476190476,
          "p": 0.07936507936507936,
          "f": 0.0432900393231015
        },
        "rouge-l": {
          "r": 0.14049586776859505,
          "p": 0.425,
          "f": 0.2111801204891787
        }
      }
    },
    {
      "paper_id": "cs.GL.cs/GL/2403.05592v1",
      "true_abstract": "As we keep rapidly advancing toward an era where artificial intelligence is a\nconstant and normative experience for most of us, we must also be aware of what\nthis vision and this progress entail. By first approximating neural connections\nand activities in computer circuits and then creating more and more\nsophisticated versions of this crude approximation, we are now facing an age to\ncome where modern deep learning-based artificial intelligence systems can\nrightly be called thinking machines, and they are sometimes even lauded for\ntheir emergent behavior and black-box approaches. But as we create more\npowerful electronic brains, with billions of neural connections and parameters,\ncan we guarantee that these mammoths built of artificial neurons will be able\nto forget the data that we store in them? If they are at some level like a\nbrain, can the right to be forgotten still be protected while dealing with\nthese AIs? The essential gap between machine learning and the RTBF is explored\nin this article, with a premonition of far-reaching conclusions if the gap is\nnot bridged or reconciled any time soon. The core argument is that deep\nlearning models, due to their structure and size, cannot be expected to forget\nor delete a data as it would be expected from a tabular database, and they\nshould be treated more like a mechanical brain, albeit still in development.",
      "generated_abstract": "the European Court of Justice ruled that Google's removal of a\nGoogle Maps listing for a place called 'Little Orphan Annie's Caf\u00e9' violated\nthe right to be forgotten. Since then, the court has made similar rulings\nregarding other 'rights to be forgotten,' including rulings on Google's\nremoval of personal details from online profiles of deceased people. This work\nrevisits the rulings of the European Court of Justice, focusing on the rulings\nregarding Google's removal of personal details from online profiles of deceased\npeople. We show that these rulings, which focus on Google's removal of\npersonal details from online profiles of deceased people, are inconsistent with\ntheir rulings on other 'rights to be forgotten' rulings, and with the rulings\nof the European Court of Human",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.25,
          "f": 0.15384614958579892
        },
        "rouge-2": {
          "r": 0.022935779816513763,
          "p": 0.058823529411764705,
          "f": 0.033003296293392215
        },
        "rouge-l": {
          "r": 0.10416666666666667,
          "p": 0.234375,
          "f": 0.14423076497041432
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2502.10631v1",
      "true_abstract": "Large Language Models (LLMs) employ three popular training approaches: Masked\nLanguage Models (MLM), Causal Language Models (CLM), and Sequence-to-Sequence\nModels (seq2seq). However, each approach has its strengths and limitations, and\nfaces challenges in addressing specific tasks that require controllable and\nbidirectional generation, such as drug optimization. To address this challenge,\ninspired by the biological processes of growth and evolution, which involve the\nexpansion, shrinking, and mutation of sequences, we introduce ControllableGPT.\nThis initiative represents the first effort to combine the advantages of MLM,\nCLM, and seq2seq into a single unified, controllable GPT framework. It enables\nthe precise management of specific locations and ranges within a sequence,\nallowing for expansion, reduction, or mutation over chosen or random lengths,\nwhile maintaining the integrity of any specified positions or subsequences. In\nthis work, we designed ControllableGPT for drug optimization from the ground\nup, which included proposing the Causally Masked Seq2seq (CMS) objective,\ndeveloping the training corpus, introducing a novel pre-training approach, and\ndevising a unique generation process. We demonstrate the effectiveness and\ncontrollability of ControllableGPT by conducting experiments on drug\noptimization tasks for both viral and cancer benchmarks, surpassing competing\nbaselines.",
      "generated_abstract": "ble generation of molecules is a crucial task in chemical\nreinforcement learning (CRL), but existing methods face significant challenges\ndue to their lack of attention to molecular properties. To address this, we\npropose ControllableGPT, a novel approach that integrates molecular\nproperties into controllable prompts to improve generation quality. ControllableGPT\nemploys a transformer-based model with two decoder layers, a control block,\nand a decoder tail. It also incorporates a molecular property decoder, which\nis a graph neural network (GNN), to explicitly capture the molecular properties\nof generated molecules. The model is trained on a large-scale dataset that\ncontains molecular properties, including chemical structures, molecular\nproperties, and reaction potentials. Experimental results show that ControllableGPT\noutperforms state-of-the-art methods in generating cont",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.23529411764705882,
          "f": 0.1818181770764464
        },
        "rouge-2": {
          "r": 0.01092896174863388,
          "p": 0.017699115044247787,
          "f": 0.013513508793144455
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.21176470588235294,
          "f": 0.16363635889462821
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/DB/2503.08717v1",
      "true_abstract": "The ability of tracing states of logistic transportations requires an\nefficient storage and retrieval of the state of logistic transportations and\nlocations of logistic objects. However, the restriction of sharing states and\nlocations of logistic objects across organizations from different countries\nmakes it hard to deploy a centralized database for implementing the\ntraceability in a cross-border logistic system. This paper proposes a semantic\ndata model on Blockchain to represent a logistic process based on the Semantic\nLink Network model where each semantic link represents a logistic\ntransportation of a logistic object between two parties. A state representation\nmodel is designed to represent the states of a logistic transportation with\nsemantic links. It enables the locations of logistic objects to be derived from\nthe link states. A mapping from the semantic links to the blockchain\ntransactions is designed to enable schema of semantic links and states of\nsemantic links to be published in blockchain transactions. To improve the\nefficiency of tracing a path of semantic links on blockchain platform, an\nalgorithm is designed to build shortcuts along the path of semantic links to\nenable a query on the path of a logistic object to reach the target in\nlogarithmic steps on the blockchain platform. A reward-penalty policy is\ndesigned to allow participants to confirm the state of links on blockchain.\nAnalysis and simulation demonstrate the flexibility, effectiveness and the\nefficiency of Semantic Link Network on immutable blockchain for implementing\nlogistic traceability.",
      "generated_abstract": "r presents a semantic link network model for supporting traceability\nof logistics on blockchain. Our work aims to enhance the efficiency and\nsustainability of supply chains by integrating blockchain with logistics. The\nsemantic link network model enables a more robust and reliable supply chain\nthrough blockchain. The model integrates semantic links between various\nblockchain-enabled supply chain elements and their corresponding physical\nlogistics elements. The semantic links provide a more accurate and reliable\nsupply chain traceability. Additionally, the model supports the development of\nblockchain-based supply chain applications. We further introduce the blockchain\nsemantic link network model, including the definition of semantic link and the\nlinking model, and the semantic link network. The semantic link network model\nenables a more robust and reliable supply chain through blockchain. The model\nintegrates semantic links between various blockchain-enabled supply chain\nelements and their corresponding physical logistics",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17307692307692307,
          "p": 0.3103448275862069,
          "f": 0.2222222176253621
        },
        "rouge-2": {
          "r": 0.04945054945054945,
          "p": 0.0967741935483871,
          "f": 0.06545454097824824
        },
        "rouge-l": {
          "r": 0.16346153846153846,
          "p": 0.29310344827586204,
          "f": 0.20987653861301642
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.07896v2",
      "true_abstract": "The effect of a negative sectoral shock on GDP depends on how important the\nshocked sector is as a direct and indirect supplier and how easily sectors can\nsubstitute inputs. Past estimates of the parameters that determine these\nqualities in the US have been restrictive: they have not been allowed to vary\nacross industries or across time. This paper uses a novel empirical strategy to\nrelax those restrictions, by exploiting variation in input expenditure share\nshifts within industries rather than across industries. The resulting estimates\nexhibit significant sectoral and temporal heterogeneity, and are dynamically\ncorrelated with weighted patents. In a calibrated GE model of multi-sector\nproduction, this heterogeneity (1) raises[lowers] the GDP effect of negative\nshocks to sectors whose customers are less[more] able to substitute inputs\n(e.g. the GDP effect of \"Chemical products\" shocks rises), (2) raises[lowers]\nthe GDP effect of negative sectoral shocks in years where sectors are\nless[more] able to substitute inputs, and (3) raises[lowers] the GDP effect of\nnegative shocks to sectors as they become more[less] central input suppliers\n(e.g. between 1997 and 2023 the GDP effect of \"Paper products\" shocks fell and\nthe GDP effect of \"Computer and electronic products\" shocks rose due to changes\nin their importance as input suppliers).",
      "generated_abstract": "We estimate a dynamic general equilibrium model of sectoral production with\nthe presence of heterogeneous firms and heterogeneous shocks. The model\nincludes a general sectoral production function that captures both monopoly\nand oligopoly production structures. We show that the model can be\ninverted to estimate the equilibrium sectoral production function using\naggregate production data. We show that the model captures the heterogeneous\neffects of sectoral shocks on aggregate production and that the aggregate\nproduction function can be inverted to estimate the sectoral production function\nusing aggregate production data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1092436974789916,
          "p": 0.3611111111111111,
          "f": 0.16774193191758593
        },
        "rouge-2": {
          "r": 0.011560693641618497,
          "p": 0.03125,
          "f": 0.016877633188414457
        },
        "rouge-l": {
          "r": 0.1092436974789916,
          "p": 0.3611111111111111,
          "f": 0.16774193191758593
        }
      }
    },
    {
      "paper_id": "cs.NI.eess/SY/2503.07935v1",
      "true_abstract": "Unmanned aerial vehicles (UAVs) enhance coverage and provide flexible\ndeployment in 5G and next-generation wireless networks. The performance of such\nwireless networks can be improved by developing new navigation and wireless\nadaptation approaches in digital twins (DTs). However, challenges such as\ncomplex propagation conditions and hardware complexities in real-world\nscenarios introduce a realism gap with the DTs. Moreover, while using real-time\nfull-stack protocols in DTs enables subsequent deployment and testing in a\nreal-world environment, development in DTs requires high computational\ncomplexity and involves a long development time. In this paper, to accelerate\nthe development cycle, we develop a measurement-calibrated Matlab-based\nsimulation framework to replicate performance in a full-stack UAV wireless\nnetwork DT. In particular, we use the DT from the NSF AERPAW platform and\ncompare its reports with those generated by our developed simulation framework\nin wireless networks with similar settings. In both environments, we observe\ncomparable results in terms of RSRP measurement, hence motivating iterative use\nof the developed simulation environment with the DT.",
      "generated_abstract": "years, the development of UAV (unmanned aerial vehicle) networks has\nbecome increasingly crucial. However, the development process of UAV networks\nrequires extensive effort, which is a complex and time-consuming process,\nespecially for those who are new to UAV network development. To address this\nissue, this paper proposes a dynamic simulation framework that can be used to\ndevelop UAV networks. This framework uses dynamic simulation to accelerate the\ndevelopment process, allowing UAV network developers to focus on the development\nof network technologies and network applications rather than the simulation\nprocess itself. This paper introduces a dynamic simulation framework that can\naccelerate the development of UAV network digital twins. This framework uses\ndynamic simulation to accelerate the development process, allowing UAV network\ndevelopers to focus on the development of network technologies and network\napplications rather than the simulation process itself. This paper introdu",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22018348623853212,
          "p": 0.38095238095238093,
          "f": 0.27906976279948625
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.06666666666666667,
          "f": 0.04878048316478331
        },
        "rouge-l": {
          "r": 0.2018348623853211,
          "p": 0.3492063492063492,
          "f": 0.2558139488459979
        }
      }
    },
    {
      "paper_id": "cs.DS.stat/ML/2503.01766v1",
      "true_abstract": "We provide the first $\\widetilde{\\mathcal{O}}\\left(d\\right)$-sample algorithm\nfor sampling from unbounded Gaussian distributions under the constraint of\n$\\left(\\varepsilon, \\delta\\right)$-differential privacy. This is a quadratic\nimprovement over previous results for the same problem, settling an open\nquestion of Ghazi, Hu, Kumar, and Manurangsi.",
      "generated_abstract": "achine learning applications, it is desirable to obtain a sample\nfrom a unbounded Gaussian distribution with as little information about the\nparameters as possible. In particular, it is often useful to obtain a sample\nfrom the population distribution, which is much harder to obtain. In this\npaper, we address the problem of obtaining a sample from a Gaussian distribution\nwith as little information about the parameters as possible. Our main result is\nan optimal differentially private algorithm that achieves this goal. Our\nalgorithm is based on a novel randomized estimator of the Gaussian mean and\nvariance, which we call the Khatri-Rao estimator. We establish a connection\nbetween our estimator and the sample mean of a sub-gaussian distribution,\nestimating the mean of the sub-gaussian distribution can be achieved by the\nKhatri-Rao estimator. We further establish that our estimator achieves the\noptimal differential priv",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2972972972972973,
          "p": 0.1506849315068493,
          "f": 0.1999999955355373
        },
        "rouge-2": {
          "r": 0.025,
          "p": 0.008620689655172414,
          "f": 0.012820509007233218
        },
        "rouge-l": {
          "r": 0.2972972972972973,
          "p": 0.1506849315068493,
          "f": 0.1999999955355373
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/PM/2501.06701v2",
      "true_abstract": "This paper investigates the investment problem of constructing an optimal\nno-short sequential portfolio strategy in a market with a latent dependence\nstructure between asset prices and partly unobservable side information, which\nis often high-dimensional. The results demonstrate that a dynamic strategy,\nwhich forms a portfolio based on perfect knowledge of the dependence structure\nand full market information over time, may not grow at a higher rate infinitely\noften than a constant strategy, which remains invariant over time.\nSpecifically, if the market is stationary, implying that the dependence\nstructure is statistically stable, the growth rate of an optimal dynamic\nstrategy, utilizing the maximum capacity of the entire market information,\nalmost surely decays over time into an equilibrium state, asymptotically\nconverging to the growth rate of a constant strategy.\n  Technically, this work reassesses the common belief that a constant strategy\nonly attains the optimal limiting growth rate of dynamic strategies when the\nmarket process is identically and independently distributed. By analyzing the\ndynamic log-optimal portfolio strategy as the optimal benchmark in a stationary\nmarket with side information, we show that a random optimal constant strategy\nalmost surely exists, even when a limiting growth rate for the dynamic strategy\ndoes not. Consequently, two approaches to learning algorithms for portfolio\nconstruction are discussed, demonstrating the safety of removing side\ninformation from the learning process while still guaranteeing an asymptotic\ngrowth rate comparable to that of the optimal dynamic strategy.",
      "generated_abstract": "This paper addresses the sequential portfolio selection problem under the\nlatter-day information-dependency structure, where the investor obtains\ninformation about the future distribution of dividends through a latent\nside-information, which is shared with the portfolio manager. The problem is\nformulated as a non-convex optimal allocation problem under the\n$k$-step-ahead regret framework. The regret is a function of the conditional\ndistribution of future dividends given the side information, which is\nintractable to evaluate. We address this challenge by deriving a new regret\ndefinition that accounts for the information-dependency structure. We then\nintroduce a novel universal learning algorithm, which is shown to achieve\noptimal regret under a mild assumption. Our numerical results demonstrate that\nthe proposed algorithm outperforms other state-of-the-art algorithms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.216,
          "p": 0.34177215189873417,
          "f": 0.26470587760717035
        },
        "rouge-2": {
          "r": 0.055,
          "p": 0.09821428571428571,
          "f": 0.07051281591058543
        },
        "rouge-l": {
          "r": 0.208,
          "p": 0.3291139240506329,
          "f": 0.25490195603854293
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/AS/2503.06805v1",
      "true_abstract": "Emotion recognition and sentiment analysis are pivotal tasks in speech and\nlanguage processing, particularly in real-world scenarios involving\nmulti-party, conversational data. This paper presents a multimodal approach to\ntackle these challenges on a well-known dataset. We propose a system that\nintegrates four key modalities/channels using pre-trained models: RoBERTa for\ntext, Wav2Vec2 for speech, a proposed FacialNet for facial expressions, and a\nCNN+Transformer architecture trained from scratch for video analysis. Feature\nembeddings from each modality are concatenated to form a multimodal vector,\nwhich is then used to predict emotion and sentiment labels. The multimodal\nsystem demonstrates superior performance compared to unimodal approaches,\nachieving an accuracy of 66.36% for emotion recognition and 72.15% for\nsentiment analysis.",
      "generated_abstract": "y explores the integration of multimodal emotion recognition and\nsentiment\n  analysis in multi-party conversations. We propose a novel framework that\ncombines emotion recognition and sentiment analysis for multi-party conversations\nin a multi-party conversation context. We introduce a novel emotion recognition\nsystem that incorporates a multi-modal feature fusion method to detect emotion\nusing both audio and visual features. This system is integrated with a\nsentiment analysis system that uses a text-based sentiment analysis method\nthat combines a large-scale text-based emotion lexicon and an emotion\nclassifier. We then propose a novel method for sentiment analysis within the\nmulti-party conversation context. Our experiments demonstrate that our\nproposed framework is capable of detecting emotion and sentiment in multi-party\nconversations. We also show that our proposed framework is capable of\nintegrating emotion and sentiment analysis within a multi-party",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2413793103448276,
          "p": 0.35,
          "f": 0.28571428088296547
        },
        "rouge-2": {
          "r": 0.08108108108108109,
          "p": 0.0891089108910891,
          "f": 0.08490565538848374
        },
        "rouge-l": {
          "r": 0.22988505747126436,
          "p": 0.3333333333333333,
          "f": 0.27210883870609476
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/RM/2502.13148v1",
      "true_abstract": "This paper explores key theoretical frameworks instrumental in understanding\nthe relationship between sustainability and institutional investment decisions.\nThe study identifies and analyzes various theories, including Behavioral\nFinance Theory, Modern Portfolio Theory, Risk Management Theory, and others, to\nexplain how sustainability considerations increasingly influence investment\nchoices. By examining these frameworks, the paper highlights how investors\nintegrate Environmental, Social, and Governance (ESG) factors to optimize\nfinancial outcomes and align with broader societal goals.",
      "generated_abstract": "This paper introduces a novel theoretical framework for integrating\nsustainability factors into institutional investment decision-making. By\nfocusing on the role of institutional investors in the capital allocation\nprocess, the framework proposes a series of measures to assess their\nsustainability practices and strategies. The proposed framework is applied to\nthe capital allocation of five leading institutional investors, including the\nGlobal Investment House, the State Administration of Foreign Exchange, the\nChina Investment Corporation, the China Investment Corporation, and the\nChina Development Bank. The results demonstrate that the investment strategies\nof these five institutions are aligned with the sustainability goals of the\ngovernment, and that they have made significant progress in integrating\nsustainability factors into their investment processes.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3050847457627119,
          "p": 0.2571428571428571,
          "f": 0.27906976247821647
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.01904761904761905,
          "f": 0.022857138057143865
        },
        "rouge-l": {
          "r": 0.2711864406779661,
          "p": 0.22857142857142856,
          "f": 0.24806201054023205
        }
      }
    },
    {
      "paper_id": "cs.AR.cs/AR/2503.07778v1",
      "true_abstract": "The future of artificial intelligence (AI) acceleration demands a paradigm\nshift beyond the limitations of purely electronic or photonic architectures.\nPhotonic analog computing delivers unmatched speed and parallelism but\nstruggles with data movement, robustness, and precision. Electronic\nprocessing-in-memory (PIM) enables energy-efficient computing by co-locating\nstorage and computation but suffers from endurance and reconfiguration\nconstraints, limiting it to static weight mapping. Neither approach alone\nachieves the balance needed for adaptive, efficient AI. To break this impasse,\nwe study a hybrid electronic-photonic-PIM computing architecture and introduce\nH3PIMAP, a heterogeneity-aware mapping framework that seamlessly orchestrates\nworkloads across electronic and optical tiers. By optimizing workload\npartitioning through a two-stage multi-objective exploration method, H3PIMAP\nharnesses light speed for high-throughput operations and PIM efficiency for\nmemory-bound tasks. System-level evaluations on language and vision models show\nH3PIMAP achieves a 2.74x energy efficiency improvement and a 3.47x latency\nreduction compared to homogeneous architectures and naive mapping strategies.\nThis proposed framework lays the foundation for hybrid AI accelerators,\nbridging the gap between electronic and photonic computation for\nnext-generation efficiency and scalability.",
      "generated_abstract": "r presents a novel heterogeneity-aware multi-objective deep neural\nnetwork mapping framework, H3PIMAP, for the design of electronic-photonic\nprocessing-in-memory (e-PIM) architectures. The proposed framework leverages\nmulti-objective evolutionary algorithms (MAEAs) to optimize the\narchitectural trade-offs between the digital-to-analog and analog-to-digital\nconversions, the analog-to-digital conversion, and the digital-to-analog\nconversion. The proposed framework is designed to be scalable across\nvarious electronic-photonic processing-in-memory (e-PIM) architectures,\nincluding heterogeneous e-PIM architectures that combine multiple e-PIM\ncomponents, including multiple digital-to-analog converters, multiple analog-to\ndigital converters, and multiple digital-to-analog",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13740458015267176,
          "p": 0.32727272727272727,
          "f": 0.19354838293155288
        },
        "rouge-2": {
          "r": 0.005813953488372093,
          "p": 0.0136986301369863,
          "f": 0.008163261122534422
        },
        "rouge-l": {
          "r": 0.12213740458015267,
          "p": 0.2909090909090909,
          "f": 0.17204300658746685
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.16214v1",
      "true_abstract": "This paper analyzes nonlinearities in the international transmission of\nfinancial shocks originating in the US. To do so, we develop a flexible\nnonlinear multi-country model. Our framework is capable of producing\nasymmetries in the responses to financial shocks for shock size and sign, and\nover time. We show that international reactions to US-based financial shocks\nare asymmetric along these dimensions. Particularly, we find that adverse\nshocks trigger stronger declines in output, inflation, and stock markets than\nbenign shocks. Further, we investigate time variation in the estimated dynamic\neffects and characterize the responsiveness of three major central banks to\nfinancial shocks.",
      "generated_abstract": "This paper develops a novel theoretical framework for asymmetries in\nspillover effects and applies it to analyze financial spillovers between\ncountries and firms. We characterize the magnitude of spillover effects and\ndemonstrate that they can be asymmetric, with the largest spillover effects\noccurring in the short-run and the smallest spillover effects occurring in the\nlong-run. Using a micro-founded stochastic volatility model, we find that\nspillover effects are positively correlated with firm size and negatively\ncorrelated with the level of trade and foreign direct investment. Our results\nare consistent with the view that firms are more sensitive to shocks from\ntrade and foreign direct investment than domestic firms, while the effect of\ntrade on spillovers is neutral.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3333333333333333,
          "p": 0.3287671232876712,
          "f": 0.3310344777588586
        },
        "rouge-2": {
          "r": 0.08695652173913043,
          "p": 0.08080808080808081,
          "f": 0.08376962851456952
        },
        "rouge-l": {
          "r": 0.2916666666666667,
          "p": 0.2876712328767123,
          "f": 0.28965516741403097
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PM/2502.00415v1",
      "true_abstract": "MarketSenseAI is a novel framework for holistic stock analysis which\nleverages Large Language Models (LLMs) to process financial news, historical\nprices, company fundamentals and the macroeconomic environment to support\ndecision making in stock analysis and selection. In this paper, we present the\nlatest advancements on MarketSenseAI, driven by rapid technological expansion\nin LLMs. Through a novel architecture combining Retrieval-Augmented Generation\nand LLM agents, the framework processes SEC filings and earnings calls, while\nenriching macroeconomic analysis through systematic processing of diverse\ninstitutional reports. We demonstrate a significant improvement in fundamental\nanalysis accuracy over the previous version. Empirical evaluation on S\\&P 100\nstocks over two years (2023-2024) shows MarketSenseAI achieving cumulative\nreturns of 125.9% compared to the index return of 73.5%, while maintaining\ncomparable risk profiles. Further validation on S\\&P 500 stocks during 2024\ndemonstrates the framework's scalability, delivering a 33.8% higher Sortino\nratio than the market. This work marks a significant advancement in applying\nLLM technology to financial analysis, offering insights into the robustness of\nLLM-driven investment strategies.",
      "generated_abstract": "This paper introduces MarketSenseAI 2.0, a novel deep learning (DL) model\nthat leverages large language models (LLMs) to improve stock analysis and\nforecasting. MarketSenseAI 2.0 incorporates the latest advancements in LLMs,\nincluding transformer-based models and pre-trained language models, to enhance\nstock analysis and forecasting. The model is designed to process large volumes\nof text data and generate relevant insights and predictions, enabling\nprofessionals to make informed decisions and optimize investment strategies.\nThe model is also compatible with various data formats, making it easy to integrate\ninto existing workflows and enhancing its usability. MarketSenseAI 2.0 is\navailable for download on GitHub at https://github.com/market-sense-ai/market-sense-ai-2-0.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18461538461538463,
          "p": 0.3037974683544304,
          "f": 0.22966506706806175
        },
        "rouge-2": {
          "r": 0.048484848484848485,
          "p": 0.08247422680412371,
          "f": 0.0610686976268869
        },
        "rouge-l": {
          "r": 0.17692307692307693,
          "p": 0.2911392405063291,
          "f": 0.22009568907763108
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.q-bio/SC/2412.20570v1",
      "true_abstract": "Characterizing the local voltage distribution within nanophysiological\ndomains, driven by ionic currents through membrane channels, is crucial for\nstudying cellular activity in modern biophysics, yet it presents significant\nexperimental and theoretical challenges. Theoretically, the complexity arises\nfrom the difficulty of solving electro-diffusion equations in three-dimensional\ndomains. Currently, there are no methods available for obtaining asymptotic\ncomputations or approximated solutions of nonlinear equations, and numerically,\nit is challenging to explore solutions across both small and large spatial\nscales. In this work, we develop a method to solve the Poisson-Nernst-Planck\nequations with ionic currents entering and exiting through two narrow, circular\nwindow channels located on the boundary. The inflow through the first window is\ncomposed of a single cation, while the outflow maintains a constant ionic\ndensity satisfying local electro-neutrality conditions. Employing regular\nexpansions and Green's function representations, we derive the ionic profiles\nand voltage drops in both small and large charge regimes. We explore how local\nsurface curvature and window channels size influence voltage dynamics and\nvalidate our theoretical predictions through numerical simulations, assessing\nthe accuracy of our asymptotic computations. These novel relationships between\ncurrent, voltage, concentrations and geometry can enhance the characterization\nof physiological behaviors of nanodomains.",
      "generated_abstract": "domains, with length on the order of tens of nanometers, are\npresented as a physical system of interest in a variety of fields, including\nnanomechanics, electro-dynamics, and biology. However, their behavior is\ncontested, as the theoretical understanding of their dynamics remains unclear.\nHere, we present a unified theoretical framework to study the dynamics of\nnanodomains by combining electro-dynamics with diffusion equations. By\nanalyzing the behavior of the solution in the asymptotic regime, we derive\ntheir voltage law and provide a complete explanation of their behavior. Our\nresults are supported by numerical simulations, which reveal the existence of\noscillations in the domain's voltage, in agreement with our theoretical\npredictions. These oscillations are attributed to the influence of the\nelectrostatic field on the diffusion coefficient of the domain, which is\ndirectly related to the size of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.34146341463414637,
          "f": 0.25225224759353954
        },
        "rouge-2": {
          "r": 0.031578947368421054,
          "p": 0.047619047619047616,
          "f": 0.03797467874939974
        },
        "rouge-l": {
          "r": 0.17142857142857143,
          "p": 0.2926829268292683,
          "f": 0.21621621155750356
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.00467v1",
      "true_abstract": "Recent advancements in convolutional neural network (CNN)-based techniques\nfor remote sensing pansharpening have markedly enhanced image quality. However,\nconventional convolutional modules in these methods have two critical\ndrawbacks. First, the sampling positions in convolution operations are confined\nto a fixed square window. Second, the number of sampling points is preset and\nremains unchanged. Given the diverse object sizes in remote sensing images,\nthese rigid parameters lead to suboptimal feature extraction. To overcome these\nlimitations, we introduce an innovative convolutional module, Adaptive\nRectangular Convolution (ARConv). ARConv adaptively learns both the height and\nwidth of the convolutional kernel and dynamically adjusts the number of\nsampling points based on the learned scale. This approach enables ARConv to\neffectively capture scale-specific features of various objects within an image,\noptimizing kernel sizes and sampling locations. Additionally, we propose ARNet,\na network architecture in which ARConv is the primary convolutional module.\nExtensive evaluations across multiple datasets reveal the superiority of our\nmethod in enhancing pansharpening performance over previous techniques.\nAblation studies and visualization further confirm the efficacy of ARConv.",
      "generated_abstract": "ning is a critical technique in remote sensing imaging for\nreducing the impact of noise in the image and enhancing image quality. However,\npansharpening has a significant impact on the performance of neural network\nmodels, especially when processing remote sensing images. This paper proposes a\nrectangular convolution that adaptively adjusts the number of convolution\nlayers based on the input image. The proposed rectangular convolution can\nachieve better image quality while preserving the information in the image,\nenhancing the accuracy of pansharpening, and reducing the impact of the\npansharpening model on neural network models. The experimental results show\nthat the proposed rectangular convolution can achieve better image quality and\nreduced impact on neural network models than traditional rectangular convolution\nand traditional rectangular convolution with a higher number of layers.\nAdditionally, the results show that the proposed rectangular convolution can\nachieve",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22131147540983606,
          "p": 0.4090909090909091,
          "f": 0.2872340379968312
        },
        "rouge-2": {
          "r": 0.0658682634730539,
          "p": 0.10377358490566038,
          "f": 0.08058607583571525
        },
        "rouge-l": {
          "r": 0.20491803278688525,
          "p": 0.3787878787878788,
          "f": 0.2659574422521504
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.02726v1",
      "true_abstract": "Deep learning scaling laws predict how performance improves with increased\nmodel and dataset size. Here we identify measurement noise in data as another\nperformance scaling axis, governed by a distinct logarithmic law. We focus on\nrepresentation learning models of biological single cell genomic data, where a\ndominant source of measurement noise is due to molecular undersampling. We\nintroduce an information-theoretic metric for cellular representation model\nquality, and find that it scales with sampling depth. A single quantitative\nrelationship holds across several model types and across several datasets. We\nshow that the analytical form of this relationship can be derived from a simple\nGaussian noise model, which in turn provides an intuitive interpretation for\nthe scaling law. Finally, we show that the same relationship emerges in image\nclassification models with respect to two types of imaging noise, suggesting\nthat measurement noise scaling may be a general phenomenon. Scaling with noise\ncan serve as a guide in generating and curating data for deep learning models,\nparticularly in fields where measurement quality can vary dramatically between\ndatasets.",
      "generated_abstract": "years, learning cellular representations has emerged as a\nmechanism for modeling and understanding cellular processes. This work focuses\non the problem of learning cellular representations for time series data,\nspecifically focusing on a recent method that utilizes gradient descent to\nlearn the cellular representations. The method uses a modified version of the\nconvolutional neural network (CNN) model, with a hidden layer that is\nconstrained to be a convolutional layer, and uses a similar objective function\nto train the parameters of the CNN model. The method has two distinct\nadvantages compared to other methods: 1) it is a fully convolutional method,\nand 2) it can learn cellular representations in an unsupervised manner. The\nstudy evaluates the performance of the method in terms of classification\naccuracy. The method is evaluated on a dataset of time series data for the\ncell cycle. The results show",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25663716814159293,
          "p": 0.3372093023255814,
          "f": 0.291457281524204
        },
        "rouge-2": {
          "r": 0.023809523809523808,
          "p": 0.03076923076923077,
          "f": 0.026845632665196163
        },
        "rouge-l": {
          "r": 0.22123893805309736,
          "p": 0.29069767441860467,
          "f": 0.2512562764990784
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.10653v1",
      "true_abstract": "This paper proposes a framework for selecting policies that maximize expected\nbenefit in the presence of estimation uncertainty, by controlling for\nestimation risk and incorporating risk aversion. The proposed method explicitly\nbalances the size of the estimated benefit against the uncertainty inherent in\nits estimation, ensuring that chosen policies meet a reporting guarantee,\nnamely that the actual benefit of the implemented policy is guaranteed not to\nfall below the reported estimate with a pre-specified confidence level. This\napproach applies to a variety of settings, including the selection of policy\nrules that allocate individuals to treatments based on observed\ncharacteristics, using both experimental and non-experimental data; and the\nallocation of limited budgets among competing social programs; as well as many\nothers. Across these applications, the framework offers a principled and robust\nmethod for making data-driven policy choices under uncertainty. In broader\nterms, it focuses on policies that are on the efficient decision frontier,\ndescribing policies that offer maximum estimated benefit for a given acceptable\nlevel of estimation risk.",
      "generated_abstract": "uce a novel class of policy learning models that explicitly\nmodel uncertainty in the data generating process. Uncertainty arises from both\nobserved and unknown sources, including randomness in the underlying model and\nthe uncertainty of the data collection process. We show that in many cases,\nuncertainty in the data generating process is negligible relative to the\nuncertainty in the model, and as a result, the data generating process is\nsufficiently well-specified. We propose a Bayesian policy learning model that\ncombines a simple prior on the unknown model with a posterior distribution\nover the unknown uncertainty. This approach addresses the issue of model\nuncertainty in a principled way, while allowing for flexible uncertainty\ndecomposition. We provide a closed-form solution for the posterior distribution\nand show that the policy learning problem is tractable for large enough data\nsizes. Finally, we apply the proposed model to the case of a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2072072072072072,
          "p": 0.3026315789473684,
          "f": 0.24598929998798938
        },
        "rouge-2": {
          "r": 0.0625,
          "p": 0.08064516129032258,
          "f": 0.07042253029160916
        },
        "rouge-l": {
          "r": 0.16216216216216217,
          "p": 0.23684210526315788,
          "f": 0.1925133641591125
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2501.17490v1",
      "true_abstract": "Leveraging a unique dataset of carbon futures option prices traded on the ICE\nmarket from December 2015 until December 2020, we present the results from an\nunprecedented calibration exercise. Within a multifactor stochastic volatility\nframework with jumps, we employ a three-dimensional pricing kernel compensating\nfor equity and variance components' risk to derive an analytically tractable\nand numerically practical approach to pricing. To the best of our knowledge, we\nare the first to provide an estimate of the equity and variance risk premia for\nthe carbon futures option market. We gain insights into daily option and\nfutures dynamics by exploiting the information from tick-by-tick futures trade\ndata. Decomposing the realized measure of futures volatility into continuous\nand jump components, we employ them as auxiliary variables for estimating\nfutures dynamics via indirect inference. Our approach provides a realistic\ndescription of carbon futures price, volatility, and jump dynamics and an\ninsightful understanding of the carbon option market.",
      "generated_abstract": "lowances are a key commodity in the transition to low-carbon\nmarket-based instruments. This paper introduces a novel model that incorporates\nhigh-frequency data into the pricing of carbon allowance options. The model\ncaptures the price sensitivity of futures prices to the underlying price of\ncarbon allowances and the impact of trading volumes on the price of carbon\nallowances. We use the model to estimate the price sensitivity of carbon\nallowance futures to price changes in carbon prices, and the impact of\nvolatility in carbon prices on the futures price. We also examine the\ninterdependence of futures prices and spot prices, and the impact of\nmarket-impacted futures prices on spot prices. Finally, we study the impact of\nmarket impacts on the futures price, and examine the implications of\nmarket-impacted futures prices on the spot price",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20212765957446807,
          "p": 0.3275862068965517,
          "f": 0.24999999528047093
        },
        "rouge-2": {
          "r": 0.028368794326241134,
          "p": 0.04040404040404041,
          "f": 0.03333332848645905
        },
        "rouge-l": {
          "r": 0.1702127659574468,
          "p": 0.27586206896551724,
          "f": 0.21052631106994468
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2502.11199v1",
      "true_abstract": "Although the methodology of Design Science Research (DSR) is playing an\nincreasingly important role with the emergence of the \"sciences of the\nartificial\", the validity of the resulting artifacts is occasionally\nquestioned. This paper compares three influential DSR frameworks to assess\ntheir support for artifact validity. Using five essential validity types\n(instrument validity, technical validity, design validity, purpose validity and\ngeneralization), the qualitative analysis reveals that while purpose validity\nis explicitly emphasized, instrument and design validity remain the least\ndeveloped. Their implicit treatment in all frameworks poses a risk of\noverlooked validation, and the absence of mandatory instrument validity can\nlead to invalid artifacts, threatening research credibility. Beyond these\nfindings, the paper contributes (a) a comparative overview of each framework's\nstrengths and weaknesses and (b) a revised DSR framework incorporating all five\nvalidity types with definitions and examples. This ensures systematic artifact\nevaluation and improvement, reinforcing the rigor of DSR.",
      "generated_abstract": "The concept of artifact validity has gained prominence in design science\nresearch (DSR), yet there is a lack of consensus on its definition. This\narticle aims to provide a comprehensive review of the concept and provide\ninsights into how the concept has evolved over time. The article first\nintroduces the concept of artifact validity and discusses how it has evolved\nover time, before examining the three most influential frameworks that have\nconsistently focused on artifact validity: the DSR Framework, the Design\nInspection Framework, and the Design Validity Framework. The article then\noutlines the key concepts in these frameworks, including artifact,\nartifactuality, and artifact validity. The article also examines the\ndiscrepancies between these frameworks and highlights the limitations of each\nframework, offering recommendations for future research in DSR.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21,
          "p": 0.2625,
          "f": 0.23333332839506182
        },
        "rouge-2": {
          "r": 0.034482758620689655,
          "p": 0.043478260869565216,
          "f": 0.03846153352810714
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.25,
          "f": 0.22222221728395072
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.physics/bio-ph/2503.09319v1",
      "true_abstract": "We present PolyMorph, a lightweight standalone C++ program that extends its\npredecessor PolyHoop by a finite-difference solver for multi-component\nreaction-advection-diffusion equations. PolyMorph simulates two integral parts\nof tissue morphogenesis in two dimensions: 1) the mechanics of cellular\ndeformation, growth and proliferation, and 2) transport and reaction of an\narbitrary number of chemical species. Both of these components are\nbidirectionally coupled, allowing cells to base their behavior on local\ninformation on concentrations and flow, and allowing the chemical transport and\nreaction kinetics to depend on spatial information such as the local cell type.\nThis bidirectional feedback makes PolyMorph a versatile tool to study a variety\nof cellular morphogenetic processes such as chemotaxis, cell sorting, tissue\npatterning with morphogen gradients, Turing patterning, and diffusion- or\nsupply-limited growth with sub-cellular resolution.",
      "generated_abstract": "t PolyMorph, a novel extension of PolyHoop designed to model\npolymeric tissue formation. PolyMorph is based on a three-dimensional\ntetrahedral cell model that allows for the computation of biomolecular\ninteractions. In addition to the traditional tetrahedral cells, PolyMorph\nconsiders polydispersed particles, which significantly enhance the modeling\ncapabilities. We demonstrate the versatility of PolyMorph through an\napplication of PolyMorph to the modeling of cell-cell interactions in\ntissue formation. We compare the results obtained with PolyMorph to those\nobtained with PolyHoop, a model that is widely used for tissue morphogenesis.\nThe results of the comparison highlight the ability of PolyMorph to\naccurately capture cell-cell interactions, which are crucial for tissue\nmorphogenesis. We also present a novel method for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20430107526881722,
          "p": 0.27941176470588236,
          "f": 0.23602483984105563
        },
        "rouge-2": {
          "r": 0.016260162601626018,
          "p": 0.019417475728155338,
          "f": 0.017699110083406515
        },
        "rouge-l": {
          "r": 0.17204301075268819,
          "p": 0.23529411764705882,
          "f": 0.19875775909571403
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.09740v1",
      "true_abstract": "This paper addresses the challenge of forecasting corporate distress, a\nproblem marked by three key statistical hurdles: (i) right censoring, (ii)\nhigh-dimensional predictors, and (iii) mixed-frequency data. To overcome these\ncomplexities, we introduce a novel high-dimensional censored MIDAS (Mixed Data\nSampling) logistic regression. Our approach handles censoring through inverse\nprobability weighting and achieves accurate estimation with numerous\nmixed-frequency predictors by employing a sparse-group penalty. We establish\nfinite-sample bounds for the estimation error, accounting for censoring, the\nMIDAS approximation error, and heavy tails. The superior performance of the\nmethod is demonstrated through Monte Carlo simulations. Finally, we present an\nextensive application of our methodology to predict the financial distress of\nChinese-listed firms. Our novel procedure is implemented in the R package\n'Survivalml'.",
      "generated_abstract": "This study develops a MIDAS logistic regression model for corporate survival\nforecasting with high-dimensional censored data. Unlike traditional\nlogistic regression models, which are often inappropriate for high-dimensional\ndata, MIDAS is a flexible and effective model that enables the analysis of\ncensored data without the need to preprocess the data. The proposed model is\nbased on a novel MIDAS-type estimator that is efficient and computationally\nsimple. Theoretical results establish the consistency and asymptotic normality\nof the proposed estimator, and the finite sample performance is evaluated using\na simulation study and an empirical application to the forecasting of the\nannual revenue of the largest 200 companies in the United States. The results\nshow that the proposed model is more stable and accurate than traditional\nlogistic regression models in high-dimensional censored data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2604166666666667,
          "p": 0.3246753246753247,
          "f": 0.28901733610077185
        },
        "rouge-2": {
          "r": 0.041666666666666664,
          "p": 0.042735042735042736,
          "f": 0.04219408782780597
        },
        "rouge-l": {
          "r": 0.20833333333333334,
          "p": 0.2597402597402597,
          "f": 0.23121386789267945
        }
      }
    },
    {
      "paper_id": "cs.LO.cs/LO/2503.06036v1",
      "true_abstract": "Consistent Hoare, Smyth and Plotkin power domains are introduced and\ndiscussed by Yuan and Kou. The consistent algebraic operation $+$ defined by\nthem is a binary partial Scott continuous operation satisfying the requirement:\n$a+b$ exists whenever there exists a $c$ which is greater than $a$ and $b$. We\nextend the consistency to be a categorical concept and obtain an approach to\ngenerating consistent monads from monads on dcpos whose images equipped with\nsome algebraic operations. Then we provide two new power constructions over\ndomains: the consistent Plotkin index power domain and the consistent\nprobabilistic power domain. Moreover, we verify these power constructions are\nfree.",
      "generated_abstract": "In this paper, we prove that some consistent power constructions are\ncongruent to the power constructions of the form $\\langle G,C,Q,P\\rangle$ where\n$G$ is a graph, $C$ is a clique, and $P$ is a power set. This includes the\npower construction of the form $\\langle G,Q,P\\rangle$ where $G$ is a graph and\n$Q$ is a quorum. This is the first congruence property for power constructions\nof this form that has been proved for all graphs.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14666666666666667,
          "p": 0.25,
          "f": 0.18487394491914425
        },
        "rouge-2": {
          "r": 0.03,
          "p": 0.04918032786885246,
          "f": 0.03726707603873367
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.22727272727272727,
          "f": 0.16806722223006862
        }
      }
    },
    {
      "paper_id": "quant-ph.cs/CC/2503.03600v1",
      "true_abstract": "Bosonic quantum systems operate in an infinite-dimensional Hilbert space,\nunlike discrete-variable quantum systems. This distinct mathematical structure\nleads to fundamental differences in quantum information processing, such as an\nexponentially greater complexity of state tomography [MMB+24] or a factoring\nalgorithm in constant space [BCCRK24]. Yet, it remains unclear whether this\nstructural difference of bosonic systems may also translate to a practical\ncomputational advantage over finite-dimensional quantum computers. Here we take\na step towards answering this question by showing that universal bosonic\nquantum computations can be simulated in exponential time on a classical\ncomputer, significantly improving the best previous upper bound requiring\nexponential memory [CJMM24]. In complexity-theoretic terms, we improve the best\nupper bound on $\\textsf{CVBQP}$ from $\\textsf{EXPSPACE}$ to $\\textsf{EXP}$.\nThis result is achieved using a simulation strategy based on finite energy\ncutoffs and approximate coherent state decompositions. While we propose ways to\npotentially refine this bound, we also present arguments supporting the\nplausibility of an exponential computational advantage of bosonic quantum\ncomputers over their discrete-variable counterparts. Furthermore, we emphasize\nthe role of circuit energy as a resource and discuss why it may act as the\nfundamental bottleneck in realizing this advantage in practical\nimplementations.",
      "generated_abstract": "st decade, it was shown that bosonic systems can outperform their\nfermionic counterparts in many quantum information protocols. However, the\nexact computational power of bosonic systems remains unknown. In this work, we\npropose a new framework for studying the computational power of bosonic\nsystems. In particular, we introduce a new computational unit called a\n$\\Lambda$-qubit that serves as a unifying framework for bosonic systems. We\ndemonstrate that, when $\\Lambda$-qubits are used, the computational power of a\nbosonic system is bounded by the number of $\\Lambda$-qubits. We further\ndemonstrate that, when the number of bosonic modes is large compared to the\nnumber of $\\Lambda$-qubits, the computational power of a bosonic system is\nexponentially smaller than that of a fermionic system. In the case where the\nbosonic modes are evenly distributed",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18461538461538463,
          "p": 0.34782608695652173,
          "f": 0.24120602562056523
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.04950495049504951,
          "f": 0.0349650303963525
        },
        "rouge-l": {
          "r": 0.16153846153846155,
          "p": 0.30434782608695654,
          "f": 0.21105527185172104
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.06711v1",
      "true_abstract": "This paper touches on several interaction points of semigroups and\nconstructions from category theory: An adjunction is established between\ncategories with selected arrows and semigroups. Regular semigroups are\ncharacterized by split epi - split mono factorization of the Karoubi envelope.\nWe investigate how semigroupads (monads without requirement of unit\ntransformation) map semigroups to semigroups and ensure certain properties\nprovided they hold on meta level.",
      "generated_abstract": "r discusses the interplay between categories and semigroups, focusing\non the case of semigroups of finite type. We provide a categorical framework\nthat allows us to define a notion of morphisms between semigroups that are\ndefined over the same category, using the theory of colimits. We then extend\nthis approach to categories that are not necessarily present in the language\nused to define the semigroup. In particular, we consider the categories of\nsemigroups, rings, and fields, and prove that, in each of these cases, there\nis a natural way of defining a semigroup that is defined over the category.\nWe then show that, in the case of rings and fields, these categories are\nequivalent to the category of categories. In particular, we prove that the\ncategory of categories is a full subcategory of the category of semigroups,\nand that the category of categories is a full subcategory",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.17647058823529413,
          "f": 0.19672130654125247
        },
        "rouge-2": {
          "r": 0.04838709677419355,
          "p": 0.025423728813559324,
          "f": 0.03333332881728457
        },
        "rouge-l": {
          "r": 0.2037037037037037,
          "p": 0.16176470588235295,
          "f": 0.18032786391830166
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.06327v1",
      "true_abstract": "We investigate the return-forecasting and volatility-forecasting power of\nintraday on-chain flow data for BTC, ETH, and USDT, and the associated option\nstrategies. First, we find that USDT net inflow into cryptocurrency exchanges\npositively forecasts future returns of both BTC and ETH, with the strongest\neffect at the 1-hour frequency. Second, we find that ETH net inflow into\ncryptocurrency exchanges negatively forecasts future returns of ETH. Third, we\nfind that BTC net inflow into cryptocurrency exchanges does not significantly\nforecast future returns of BTC. Finally, we confirm that selling 0DTE ETH call\noptions is a profitable trading strategy when the net inflow into\ncryptocurrency exchanges is high. Our study lends new insights into the\nemerging literature that studies the on-chain activities and their\nasset-pricing impact in the cryptocurrency market.",
      "generated_abstract": "We investigate the power of on-chain activities in predicting future market\nactions, focusing on predicting the return of cryptocurrency assets. We\nintegrate daily on-chain activities with daily returns from financial markets\nand financial news to obtain the so-called return-forecasting power and\nvolatility-forecasting power. We construct two benchmark models for\nprediction accuracy by considering only the first and second moments of the\nactivities, and a more complex model with the third and fourth moments of\nactivities. We use the performance of these models to evaluate the\npredictive power of on-chain activities. Our empirical findings indicate that\nthe on-chain activities have a predictive power of around 60\\% for future\nreturns of cryptocurrencies. We also find that the predictive power of on-chain\nactivities is higher for assets with a higher correlation with the market.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2894736842105263,
          "p": 0.3142857142857143,
          "f": 0.30136985802214306
        },
        "rouge-2": {
          "r": 0.10377358490566038,
          "p": 0.09565217391304348,
          "f": 0.09954750632050965
        },
        "rouge-l": {
          "r": 0.2894736842105263,
          "p": 0.3142857142857143,
          "f": 0.30136985802214306
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/ME/2503.06401v1",
      "true_abstract": "Distribution-as-response regression problems are gaining wider attention,\nespecially within biomedical settings where observation-rich patient specific\ndata sets are available, such as feature densities in CT scans (Petersen et\nal., 2021) actigraphy (Ghosal et al., 2023), and continuous glucose monitoring\n(Coulter et al., 2024; Matabuena et al., 2021). To accommodate the complex\nstructure of such problems, Petersen and M\\\"uller (2019) proposed a regression\nframework called Fr\\'echet regression which allows non-Euclidean responses,\nincluding distributional responses. This regression framework was further\nextended for variable selection by Tucker et al. (2023), and Coulter et al.\n(2024) (arXiv:2403.00922 [stat.AP]) developed a fast variable selection\nalgorithm for the specific setting of univariate distributional responses\nequipped with the 2-Wasserstein metric (2-Wasserstein space). We present\n\"fastfrechet\", an R package providing fast implementation of these Fr\\'echet\nregression and variable selection methods in 2-Wasserstein space, with\nresampling tools for automatic variable selection. \"fastfrechet\" makes\ndistribution-based Fr\\'echet regression with resampling-supplemented variable\nselection readily available and highly scalable to large data sets, such as the\nUK Biobank (Doherty et al., 2017).",
      "generated_abstract": "This paper presents a fast implementation of the Fr\u00e9chet regression model\nwith distributional responses. The proposed method is based on the\ndistribution-specific generalized regression approach (DGRA), which\nsimplifies the computation of the Fr\u00e9chet regression model. The implementation\nis tailored for use with the R statistical software environment. The paper\nexplains the theoretical basis of the DGRA approach and its implementation\nwithin R, as well as its practical advantages in comparison to the classical\nFr\u00e9chet regression model. The implementation of the DGRA method is tested\nthrough two simulation studies, and a case study is also conducted to\ndemonstrate the applicability of the proposed approach. The results show that\nthe proposed method is able to perform comparably to existing methods in terms\nof computational efficiency and the ability to handle large datasets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17796610169491525,
          "p": 0.27631578947368424,
          "f": 0.21649484059517496
        },
        "rouge-2": {
          "r": 0.03870967741935484,
          "p": 0.05357142857142857,
          "f": 0.04494381535440303
        },
        "rouge-l": {
          "r": 0.1694915254237288,
          "p": 0.2631578947368421,
          "f": 0.20618556224465948
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.09127v1",
      "true_abstract": "This research presents Spiritus, an AI-assisted creation tool designed to\nstreamline 2D character animation creation while enhancing creative\nflexibility. By integrating natural language processing and diffusion models,\nusers can efficiently transform natural language descriptions into personalized\n2D characters and animations. The system employs automated segmentation,\nlayered costume techniques, and dynamic mesh-skeleton binding solutions to\nsupport flexible adaptation of complex costumes and additional components.\nSpiritus further achieves real-time animation generation and efficient\nanimation resource reuse between characters through the integration of BVH data\nand motion diffusion models. Experimental results demonstrate Spiritus's\neffectiveness in reducing technical barriers, enhancing creative freedom, and\nsupporting resource universality. Future work will focus on optimizing user\nexperience and further exploring the system's human-computer collaboration\npotential.",
      "generated_abstract": "In the field of animation, character design and animation production are two\nparticularly challenging aspects. Character design is a process that requires\nskills in multiple areas, such as creativity, problem-solving, and\ninterpersonal communication. Animation production, on the other hand, is a\nprocess that requires creative thinking, technical expertise, and strong\nmotivation to overcome obstacles. This paper describes Spiritus, an AI-based\ntool that offers a simple yet effective solution to these challenges.\nSpiritus combines machine learning and human feedback to generate 2D\ncharacters and animations. It is designed to be used by artists to generate\ncharacter designs and animations with minimal training. This paper introduces\nSpiritus and describes its functionality and usability.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19791666666666666,
          "p": 0.2289156626506024,
          "f": 0.21229049781966866
        },
        "rouge-2": {
          "r": 0.043478260869565216,
          "p": 0.04854368932038835,
          "f": 0.0458715546481783
        },
        "rouge-l": {
          "r": 0.1875,
          "p": 0.21686746987951808,
          "f": 0.20111731346212675
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2410.12648v1",
      "true_abstract": "Fingerprints, otherwise known as dermatoglyphs, are most commonly thought of\nin the context of identification, but have myriad other roles in human biology.\nThey are formed by the restricted ability of ridges and furrows of the\nepidermis to flatten. The patterns these ridges and furrows make can be\nrepresented as 2D fingerprints, but also as 3D structures with cross-sectional\nshapes that may add new levels of detail to identification, forensic, and\nbehavioral uses/studies. Surface metrology techniques better allow for the\nquantification of these features, though it is unclear what tool and what scale\nis most appropriate. A Sensofar S Neox white light reflectance confocal\nmicroscope and a Gelsight Mobile 2 were used to independently measure the\nsurface roughness of the fingerprints of four individuals from preserved\ncadaveric remains. Scale-sensitive fractal analyses (SSFA) were performed on\nthe data from the S Neox (a small area), Gelsight (a larger area), and the same\nGelsight datasets cropped down to the size of the S Neox scan size. Though\nfewer SSFA parameters identified differences between individuals from the\nsmaller, extracted Gelsight area, all three forms of measurement found\nsignificant differences between some individuals from the study. No significant\ndifferences were found that differ between fingers themselves. Though only an\ninitial step, these data suggest that a variety of surface metrology techniques\nmay be useful in differentiating individuals.",
      "generated_abstract": "etrology techniques, such as contactless hand-held and hand-based\nstylus profilometry, have been widely adopted in research and industry for\ndetailed 3D fingerprint imaging and analysis. However, these methods often\nexhibit varying degrees of accuracy, with inconsistent results and\nuncertainty, especially in challenging cases where fingerprints are poorly\npreserved or inaccurately positioned. In this work, we explore the potential of\ndeep learning and machine learning approaches to improve the performance of\nsurface metrology methods. We propose a novel approach to predict the 3D shape\nof a fingerprint using deep neural networks. Our method is based on a\nconvolutional neural network (CNN) architecture and leverages the\nintrinsic 3D shape features of the fingerprint images to predict the shape. We\nevaluate the performance of this approach using two different",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1310344827586207,
          "p": 0.21348314606741572,
          "f": 0.16239315767952386
        },
        "rouge-2": {
          "r": 0.014423076923076924,
          "p": 0.025,
          "f": 0.018292678286735264
        },
        "rouge-l": {
          "r": 0.11724137931034483,
          "p": 0.19101123595505617,
          "f": 0.14529914058550678
        }
      }
    },
    {
      "paper_id": "cs.CG.cs/DS/2503.07769v1",
      "true_abstract": "We study the problem of computing the diameter and the mean distance of a\ncontinuous graph, i.e., a connected graph where all points along the edges,\ninstead of only the vertices, must be taken into account. It is known that for\ncontinuous graphs with $m$ edges these values can be computed in roughly\n$O(m^2)$ time. In this paper, we use geometric techniques to obtain\nsubquadratic time algorithms to compute the diameter and the mean distance of a\ncontinuous graph for two well-established classes of sparse graphs. We show\nthat the diameter and the mean distance of a continuous graph of treewidth at\nmost $k$ can be computed in $O(n\\log^{O(k)} n)$ time, where $n$ is the number\nof vertices in the graph. We also show that computing the diameter and mean\ndistance of a continuous planar graph with $n$ vertices and $F$ faces takes\n$O(n F \\log n)$ time.",
      "generated_abstract": "We present a unified framework for studying distance problems in continuous\ngraphs. We define a continuous graph model, which generalizes the graph\nmodel, and we define two distance problems: the $l_1$ and the $l_2$ distance.\nWe then define a number of distance problems that can be expressed as\napplications of these two. We also introduce a number of distance problems that\ncan be expressed as applications of the triangle inequality. We then show that\nthese distance problems can be solved using polynomial-time algorithms. We then\ngeneralize our results to the case of graphs with weights.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27710843373493976,
          "p": 0.46,
          "f": 0.34586465696195373
        },
        "rouge-2": {
          "r": 0.09090909090909091,
          "p": 0.14473684210526316,
          "f": 0.1116751221644466
        },
        "rouge-l": {
          "r": 0.25301204819277107,
          "p": 0.42,
          "f": 0.315789468992029
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/CP/2412.16067v1",
      "true_abstract": "We use modifications of the Adams method and very fast and accurate\nsinh-acceleration method of the Fourier inversion (iFT) (S.Boyarchenko and\nLevendorski\\u{i}, IJTAF 2019, v.22) to evaluate prices of vanilla options; for\noptions of moderate and long maturities and strikes not very far from the spot,\nthousands of prices can be calculated in several msec. with relative errors of\nthe order of 0.5\\% and smaller running Matlab on a Mac with moderate\ncharacteristics. We demonstrate that for the calibrated set of parameters in\nEuch and Rosenbaum, Math. Finance 2019, v. 29, the correct implied volatility\nsurface is significantly flatter and fits the data very poorly, hence, the\ncalibration results in op.cit. is an example of the {\\em ghost calibration}\n(M.Boyarchenko and Levendorki\\u{i}, Quantitative Finance 2015, v. 15): the\nerrors of the model and numerical method almost cancel one another. We explain\nhow calibration errors of this sort are generated by each of popular versions\nof numerical realizations of iFT (Carr-Madan, Lipton-Lewis and COS methods)\nwith prefixed parameters of a numerical method, resulting in spurious\nvolatility smiles and skews. We suggest a general {\\em Conformal Bootstrap\nprinciple} which allows one to avoid ghost calibration errors. We outline\nschemes of application of Conformal Bootstrap principle and the method of the\npaper to the design of accurate and fast calibration procedures.",
      "generated_abstract": "the rough Heston model, which is a generalization of the Heston model\nwith an additional rough term. This model is often used for volatility\nrepresentation and pricing, but its pricing method is not yet widely\nadopted. In this paper, we propose a new pricing method based on the rough\nHeston model, which is called the rough Heston-Heston (RHHH) method. This\nmethod is different from the conventional method in that it uses the rough\nHeston model as a surrogate model and estimates the rough Heston model by\nempirical methods. The RHHH method is applied to pricing American options in the\nrough Heston model. We show that the RHHH method is more accurate and\nefficient than the conventional method. This paper is an extension of the\nconcept of rough Heston model, which is a generalization of the Heston model.\nThe rough",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17777777777777778,
          "p": 0.34782608695652173,
          "f": 0.23529411317041526
        },
        "rouge-2": {
          "r": 0.023809523809523808,
          "p": 0.04807692307692308,
          "f": 0.031847129327762395
        },
        "rouge-l": {
          "r": 0.15555555555555556,
          "p": 0.30434782608695654,
          "f": 0.205882348464533
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.00818v1",
      "true_abstract": "Sample size determination is crucial in experimental design, especially in\ntraffic and transport research. Frequentist statistics require a fixed sample\nsize determined by power analysis, which cannot be adjusted once the experiment\nstarts. Bayesian sample size determination, with proper priors, offers an\nalternative. Bayesian optional stopping (BOS) allows experiments to stop when\nstatistical targets are met. We introduce predictive Bayesian optional stopping\n(pBOS), combining BOS with Bayesian rehearsal simulations to predict future\ndata and stop experiments if targets are unlikely to be met within resource\nconstraints. We identified and corrected a bias in predictions using multiple\nlinear regression. pBOS shows up to 118% better cost benefit than traditional\nBOS and is more efficient than frequentist methods. pBOS allows researchers to,\nunder certain conditions, stop experiments when resources are insufficient or\nwhen enough data is collected, optimizing resource use and cost savings.",
      "generated_abstract": "points (DPs) are critical decision points in experimental\nreports, where the experimental protocol is either terminated or extended.\nStopping decisions are often made based on observational data, which may be\nlimited in sample size and variance. A predictive Bayesian optional stopping\nmethod (PB-OSM) is proposed to predict DPs. PB-OSM incorporates a\nBayesian-optimal control framework, which considers both the sampling\nuncertainty and the decision uncertainty. This framework enables PB-OSM to\nestimate DPs by integrating the information of sampling uncertainty and\ndecision uncertainty. A simulation study is conducted to evaluate the\nperformance of PB-OSM. The results show that PB-OSM outperforms the\nexisting approaches, and its prediction accuracy is comparable to that of the\nexisting methods. Furthermore, the proposed PB-OSM can",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19801980198019803,
          "p": 0.25,
          "f": 0.2209944702054273
        },
        "rouge-2": {
          "r": 0.044444444444444446,
          "p": 0.05309734513274336,
          "f": 0.04838709181354109
        },
        "rouge-l": {
          "r": 0.18811881188118812,
          "p": 0.2375,
          "f": 0.20994474644852124
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2502.17455v1",
      "true_abstract": "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
      "generated_abstract": "oma multiforme (GBM) is a deadly brain tumor that is characterized\nby a poor prognosis and the immune system's inability to eradicate it.\nGlioblastoma stem cells (GSCs) are thought to be responsible for\nproliferation and growth of GBM cells, making them a prime target for\ntherapeutic intervention. While several strategies have been attempted\nwith limited success, the development of a more effective therapeutic\napproach is urgently needed. This study proposes a simplified model of\nimmunotherapy for GBM based on the hypothesis that GSCs are the main\ncontributor to tumor growth. The model is based on the idea that GSCs can\nbe identified through their ability to grow in response to immunomodulatory\ncytokines, and that immunotherapy can be used to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.29213483146067415,
          "p": 0.32098765432098764,
          "f": 0.3058823479522492
        },
        "rouge-2": {
          "r": 0.08108108108108109,
          "p": 0.07894736842105263,
          "f": 0.0799999950008892
        },
        "rouge-l": {
          "r": 0.25842696629213485,
          "p": 0.2839506172839506,
          "f": 0.2705882303051904
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2502.19397v1",
      "true_abstract": "In chemical reaction network theory, ordinary differential equations are used\nto model the temporal change of chemical species concentration. As the\nfunctional form of these ordinary differential equations systems is derived\nfrom an empirical model of the reaction network, it may be incomplete. Our\napproach aims to elucidate these hidden insights in the reaction network by\ncombining dynamic modelling with deep learning in the form of neural ordinary\ndifferential equations. Our contributions not only help to identify the\nshortcomings of existing empirical models but also assist the design of future\nreaction networks.",
      "generated_abstract": "reaction networks (CRNs) are models of biological systems that\ndepict the chemical reactions that occur within cells and organisms. These\nnetworks are fundamental to understanding the molecular mechanisms that underlie\nbiological processes such as cellular metabolism and protein synthesis. CRNs\ncan be modelled using a variety of mathematical approaches, including\ndifferential equations, stochastic models, and neural networks. In this paper,\nwe propose a neural Ordinary Differential Equation (ODE) model for CRNs, which\nwe call the Neural Ordinary Differential Equation (N-ODE). The N-ODE is a\nspecial type of neural network that is capable of modelling the dynamics of\nCRNs. By using the N-ODE, we can generate solutions to the ODE that capture the\nbehaviour of CRNs. We demonstrate the application of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27419354838709675,
          "p": 0.2073170731707317,
          "f": 0.23611110620756184
        },
        "rouge-2": {
          "r": 0.03614457831325301,
          "p": 0.02564102564102564,
          "f": 0.029999995144500784
        },
        "rouge-l": {
          "r": 0.24193548387096775,
          "p": 0.18292682926829268,
          "f": 0.20833332842978408
        }
      }
    },
    {
      "paper_id": "math.PR.q-fin/MF/2412.16436v1",
      "true_abstract": "We consider a microstructure foundation for rough volatility models driven by\nPoisson random measures. In our model the volatility is driven by self-exciting\narrivals of market orders as well as self-exciting arrivals of limit orders and\ncancellations. The impact of market order on future order arrivals is captured\nby a Hawkes kernel with power law decay, and is hence persistent. The impact of\nlimit orders on future order arrivals is temporary, yet possibly long-lived.\nAfter suitable scaling the volatility process converges to a fractional Heston\nmodel driven by an additional Poisson random measure. The random measure\ngenerates occasional spikes and clusters of spikes in the volatility process.\nOur results are based on novel existence and uniqueness of solutions results\nfor stochastic path-dependent Volterra equations driven by Poisson random\nmeasures.",
      "generated_abstract": "aper, we derive path-dependent fractional Volterra equations from a\npopular rough volatility model, which is motivated by the analysis of\nvolatility in the rough path theory. The model is constructed by replacing the\nordinary Volterra equation with a fractional one and introducing a new\nmeasure, which is the Poisson random measure on the real line. As a result, we\nobtain a set of path-dependent fractional Volterra equations. The main\ncontribution of this paper is to show that the new measure can be seen as the\ndiffusion process on the rough path space. We establish the existence of the\nsolution for the original fractional Volterra equation and the uniqueness of\nthe solution under the condition that the initial condition is non-negative and\nconcave. The solution is expressed as the solution to a system of ODEs and the\nstochastic integral. The solution is also expressed as the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.37333333333333335,
          "p": 0.36363636363636365,
          "f": 0.36842104763244465
        },
        "rouge-2": {
          "r": 0.07547169811320754,
          "p": 0.064,
          "f": 0.06926406429789583
        },
        "rouge-l": {
          "r": 0.3466666666666667,
          "p": 0.33766233766233766,
          "f": 0.3421052581587605
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.03648v1",
      "true_abstract": "The work aims to propose a new nonlinear characteristics model for a wideband\nradio amplifier of variable supply voltage. An extended Rapp model proposal is\npresented. The proposed model has been verified by measurements of three\ndifferent amplifiers. This model can be used to design frontend-aware 6G\nsystems.\n  --\n  Praca ma na celu zaproponowanie nowego modelu dla nieliniowej charakterystyki\nwzmacniacza radiowego ze zmiennym napi\\k{e}ciem zasilania pracuj\\k{a}cym w\nszerokim zakresie cz\\k{e}stotliwo\\'sci. Przedstawiona zosta{\\l}a propozycja\nrozszerzonego modelu Rappa. Zaproponowany model zweryfikowano na podstawie\npomiar\\'ow charakterystyk trzech r\\'o\\.znych wzmacniaczy. Model ten mo\\.ze\nby\\'c wykorzystany do projektowania system\\'ow 6G \"\\'swiadomych\"\nniedoskona{\\l}o\\'sci uk{\\l}ad\\'ow wej\\'sciowo-wyj\\'sciowych.",
      "generated_abstract": "aper, we study wideband radio frequency amplifiers with variable\nsupply voltage (VSV) and nonlinear characteristics. The VSV is modeled as a\nnonlinear function of the VSV and the baseband signal amplitude, and the\namplifier characteristics are modeled as a nonlinear function of the VSV and\nthe baseband signal amplitude. The main contribution of this paper is to\nderive the closed-form expressions for the VSV-dependent nonlinear\ncharacteristics of the amplifiers. The closed-form expressions for the\ncharacteristics are derived using the Laplace transform and the Bessel\nfunction, which is a powerful technique for transforming complex functions into\nreal functions. The expressions are then used to derive closed-form expressions\nfor the VSV-dependent nonlinear characteristics of the amplifiers. The\nderivation of the closed-form expressions for the VSV-dependent nonlinear\ncharacteristics of the amplifiers",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1797752808988764,
          "p": 0.2807017543859649,
          "f": 0.21917807743197606
        },
        "rouge-2": {
          "r": 0.05,
          "p": 0.05813953488372093,
          "f": 0.05376343588854249
        },
        "rouge-l": {
          "r": 0.16853932584269662,
          "p": 0.2631578947368421,
          "f": 0.20547944729498976
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.06376v1",
      "true_abstract": "Over-the-air federated learning (OTA-FL) offers an exciting new direction\nover classical FL by averaging model weights using the physics of analog signal\npropagation. Since each participant broadcasts its model weights concurrently\nin time and frequency, this paradigm conserves communication bandwidth and\nmodel upload latency. Despite its potential, there is no prior large-scale\ndemonstration on a real-world experimental platform. This paper proves for the\nfirst time that OTA-FL can be deployed in a cellular network setting within the\nconstraints of a 5G compliant waveform. To achieve this, we identify challenges\ncaused by multi-path fading effects, thermal noise at the radio devices, and\nmaintaining highly precise synchronization across multiple clients to perform\ncoherent OTA combining. To address these challenges, we propose a unified\nframework for real-time channel estimation, model weight to OFDM symbol mapping\nand dual-layer synchronization interface to perform OTA model training. We\nexperimentally validate OTA-FL using two relevant applications - Channel\nEstimation and Object Classification, at a large-scale on ORBIT Testbed and a\nportable setup respectively, along with analyzing the benefits from the\nperspective of a telecom operator. Under specific experimental conditions,\nOTA-FL achieves equivalent model performance, supplemented with 43 times\nimprovement in spectrum utilization and 7 times improvement in energy\nefficiency over classical FL when considering 5 nodes.",
      "generated_abstract": "Federated learning (FL) has emerged as a powerful method for training large\nnetwork models in a decentralized manner. However, the data collected by each\nnode is often sensitive and may pose a threat to network security. To address\nthis, we propose a novel approach for FL in cellular networks, where each node\nis required to send a small amount of sensitive data to a central server for\ntraining. We first present a novel framework for training a large network model\nin a federated manner, where each node is required to send a small amount of\nsensitive data to a central server for training. Then, we propose a novel\nprotocol for data transmission in the centralized FL scheme, which ensures the\nsecurity of the data. Finally, we demonstrate the experimental feasibility of\nour proposed scheme in a simulated network.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16560509554140126,
          "p": 0.3561643835616438,
          "f": 0.2260869521886579
        },
        "rouge-2": {
          "r": 0.03482587064676617,
          "p": 0.06542056074766354,
          "f": 0.045454540920265254
        },
        "rouge-l": {
          "r": 0.15286624203821655,
          "p": 0.3287671232876712,
          "f": 0.20869564784083186
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.04924v2",
      "true_abstract": "The integration of artificial intelligence (AI) into the workplace is\nadvancing rapidly, necessitating robust metrics to evaluate its tangible impact\non the labour market. Existing measures of AI occupational exposure largely\nfocus on AI's theoretical potential to substitute or complement human labour on\nthe basis of technical feasibility, providing limited insight into actual\nadoption and offering inadequate guidance for policymakers. To address this\ngap, we introduce the AI Startup Exposure (AISE) index-a novel metric based on\noccupational descriptions from O*NET and AI applications developed by startups\nfunded by the Y Combinator accelerator. Our findings indicate that while\nhigh-skilled professions are theoretically highly exposed according to\nconventional metrics, they are heterogeneously targeted by startups. Roles\ninvolving routine organizational tasks-such as data analysis and office\nmanagement-display significant exposure, while occupations involving tasks that\nare less amenable to AI automation due to ethical or high-stakes, more than\nfeasibility, considerations -- such as judges or surgeons -- present lower AISE\nscores. By focusing on venture-backed AI applications, our approach offers a\nnuanced perspective on how AI is reshaping the labour market. It challenges the\nconventional assumption that high-skilled jobs uniformly face high AI risks,\nhighlighting instead the role of today's AI players' societal\ndesirability-driven and market-oriented choices as critical determinants of AI\nexposure. Contrary to fears of widespread job displacement, our findings\nsuggest that AI adoption will be gradual and shaped by social factors as much\nas by the technical feasibility of AI applications. This framework provides a\ndynamic, forward-looking tool for policymakers and stakeholders to monitor AI's\nevolving impact and navigate the changing labour landscape.",
      "generated_abstract": "The rapid growth of artificial intelligence (AI) presents significant\ntechnological, economic, and social challenges. This study uses a dataset\ncapturing the employment of startups in the United States to investigate how\nAI technologies are being adopted across occupations, industries, and regions.\nWe find that AI adoption is unevenly distributed, with a greater focus on\nemerging technologies in more urban areas and in industries with higher\nlevels of employment. In addition, we find that firms specializing in AI\ntechnologies are more likely to be located in large metropolitan areas. These\nfindings highlight the need for targeted policies to address the\ndisparities in AI adoption across occupations, industries, and regions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16111111111111112,
          "p": 0.38666666666666666,
          "f": 0.2274509762399078
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.07142857142857142,
          "f": 0.039999995968000406
        },
        "rouge-l": {
          "r": 0.15555555555555556,
          "p": 0.37333333333333335,
          "f": 0.2196078389850058
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/PF/2503.08973v1",
      "true_abstract": "Reducing the memory footprint of Machine Learning (ML) models, especially\nDeep Neural Networks (DNNs), is imperative to facilitate their deployment on\nresource-constrained edge devices. However, a notable drawback of DNN models\nlies in their susceptibility to adversarial attacks, wherein minor input\nperturbations can deceive them. A primary challenge revolves around the\ndevelopment of accurate, resilient, and compact DNN models suitable for\ndeployment on resource-constrained edge devices. This paper presents the\noutcomes of a compact DNN model that exhibits resilience against both black-box\nand white-box adversarial attacks. This work has achieved this resilience\nthrough training with the QKeras quantization-aware training framework. The\nstudy explores the potential of QKeras and an adversarial robustness technique,\nJacobian Regularization (JR), to co-optimize the DNN architecture through\nper-layer JR methodology. As a result, this paper has devised a DNN model\nemploying this co-optimization strategy based on Stochastic Ternary\nQuantization (STQ). Its performance was compared against existing DNN models in\nthe face of various white-box and black-box attacks. The experimental findings\nrevealed that, the proposed DNN model had small footprint and on average, it\nexhibited better performance than Quanos and DS-CNN MLCommons/TinyML (MLC/T)\nbenchmarks when challenged with white-box and black-box attacks, respectively,\non the CIFAR-10 image and Google Speech Commands audio datasets.",
      "generated_abstract": "aper, we present a quantitative analysis of deeply quantized\ntinny neural networks (DQTNNs), which is the first of its kind. DQTNNs are\nquantization-aware training algorithms that train a deep neural network in a\nway that preserves its performance, while reducing its energy consumption.\nDQTNNs are trained with an adversarial attack, which aims to increase the\nefficiency of the quantized network by introducing small perturbations to the\ninput data. To the best of our knowledge, this is the first study that analyzes\nthe impact of adversarial attacks on DQTNNs. We conducted a series of\nexperiments to assess the effect of adversarial attacks on DQTNNs, including\nthe size of the attack, the amount of data corruption, and the strength of the\nattack. The results show that DQTNNs are robust to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15328467153284672,
          "p": 0.26582278481012656,
          "f": 0.19444443980495552
        },
        "rouge-2": {
          "r": 0.015544041450777202,
          "p": 0.025423728813559324,
          "f": 0.0192925997923937
        },
        "rouge-l": {
          "r": 0.145985401459854,
          "p": 0.25316455696202533,
          "f": 0.18518518054569627
        }
      }
    },
    {
      "paper_id": "physics.space-ph.physics/space-ph/2503.07905v1",
      "true_abstract": "We present the results of the first multi-event study of the normalized\nreconnection rate integrating events spanning the three primary regimes of\nreconnection observed by the Magnetospheric Multiscale (MMS) mission. We\nutilize a new method for determining the normalized reconnection rate with\nfewer sources of uncertainty by estimating the diffusion region aspect ratio\nwith magnetic field gradients, which are very well measured by MMS. After\ndemonstrating our technique is valid in the guide field and asymmetric regimes\nof reconnection, we investigate any relationships between the normalized rate\non guide field, upstream magnetic field variability, and magnetic field and\ndensity asymmetry. Our results suggest that under typical magnetospheric\nconditions, the normalized reconnection rate is constant, which may be\nsignificant in predicting the terrestrial effects of space weather by providing\ninsight into the efficiency of solar wind-magnetospheric coupling.",
      "generated_abstract": "We study the magnetic reconnection rate, defined as the rate of change of the\nmagnetic field with respect to time, as a function of the local plasma\nparameters, including electron density, electron temperature, and plasma\nshear. Using a novel modeling approach, we find that the magnetic reconnection\nrate scales linearly with the ratio of the electron density to the\ntemperature. We find that the magnetic reconnection rate is sensitive to the\nratio of the background plasma shear to the local shear. The magnetic\nreconnection rate is a function of the ratio of the background plasma shear\nto the local shear, and the ratio of the electron density to the temperature.\nThis scaling is different from previous theoretical and numerical studies,\nwhich assumed that the magnetic reconnection rate scales linearly with the\nratio of the background plasma shear to the local shear.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18478260869565216,
          "p": 0.3148148148148148,
          "f": 0.2328767076674799
        },
        "rouge-2": {
          "r": 0.032520325203252036,
          "p": 0.04878048780487805,
          "f": 0.03902438544390304
        },
        "rouge-l": {
          "r": 0.17391304347826086,
          "p": 0.2962962962962963,
          "f": 0.21917807753049362
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2410.13265v2",
      "true_abstract": "An automated market maker where the price can cross the zero bound into the\nnegative price domain with applications in electricity, energy, and derivatives\nmarkets is presented. A unique feature involves the ability to swap both\nnegatively and positively priced assets between one another, which unlike\ntraditional markets requires a numeraire in the form of a currency. Model\nextensions to skew and concentrate liquidity are shown. The liquidity\nfingerprint, payoff, and invariant are compared to the Black-Scholes covered\ncall and the Logarithmic Market Scoring Rule invariants.",
      "generated_abstract": "Traders often rely on a market maker to execute their trades. While the\nmarket maker may not always be the best bid/ask spread, it is likely to be\nconcentrated, with a small number of orders dominating the market. This paper\nexamines how this concentration influences the price and the trading costs. We\nshow that concentrated markets are prone to arbitrage and liquidity\nprivileges, and we derive conditions under which these phenomena can occur. We\nalso introduce a measure of concentration, the concentration ratio, and\nderive the conditions under which the concentration ratio is maximized by\nconcentrated markets. Finally, we show how concentration can affect the\nprice and trading costs. The result is a tighter constraint on the profit\nability of a market maker. We apply our results to a real-world exchange.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.20987654320987653,
          "f": 0.2281879145011487
        },
        "rouge-2": {
          "r": 0.047058823529411764,
          "p": 0.03361344537815126,
          "f": 0.0392156814133993
        },
        "rouge-l": {
          "r": 0.22058823529411764,
          "p": 0.18518518518518517,
          "f": 0.2013422769172561
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2403.03862v1",
      "true_abstract": "In December 2023 the Florida State Seminoles became the first Power 5 school\nto have an undefeated season and miss selection for the College Football\nPlayoff. In order to assess this decision, we employed an Elo ratings model to\nrank the teams and found that the selection committee's decision was justified\nand that Florida State were not one of the four best teams in college football\nin that season (ranking only 11th!). We extended this analysis to all other\nyears of the CFP and found that the top four teams by Elo ratings differ\ngreatly from the four teams selected in almost every year of the CFP's\nexistence. Furthermore, we found that there have been more egregious\nnon-selections including when Alabama was ranked first by Elo ratings in 2022\nand were not selected. The analysis suggests that the current criteria are too\nsubjective and a ratings model should be implemented to provide transparency\nfor the sport, its teams, and its fans.",
      "generated_abstract": "College Football Playoff (CFP) has become a popular NCAA postseason\nevent. The CFP selects the four teams that compete in the CFP Championship Game\nto determine the national champion. The CFP selection process is based on the\nElo ratings model, which assigns ratings to each team based on the team's\nhistorical performance. This model provides a way to compare teams' strength\nof schedule and assess their strength relative to other teams. This paper\nexplores the application of the Elo ratings model to the CFP selection process\nand the potential of using it to improve the selection process. The paper\nreviews the application of the Elo ratings model to the CFP selection process,\nand discusses the potential benefits of using the Elo ratings model to improve\nthe selection process. The paper concludes with a summary of the findings and\nrecommendations for future research. The findings highlight the potential of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21212121212121213,
          "p": 0.28,
          "f": 0.24137930543995254
        },
        "rouge-2": {
          "r": 0.06944444444444445,
          "p": 0.08849557522123894,
          "f": 0.07782100674590109
        },
        "rouge-l": {
          "r": 0.1919191919191919,
          "p": 0.25333333333333335,
          "f": 0.21839079969282613
        }
      }
    },
    {
      "paper_id": "cs.NI.cs/NI/2503.07935v1",
      "true_abstract": "Unmanned aerial vehicles (UAVs) enhance coverage and provide flexible\ndeployment in 5G and next-generation wireless networks. The performance of such\nwireless networks can be improved by developing new navigation and wireless\nadaptation approaches in digital twins (DTs). However, challenges such as\ncomplex propagation conditions and hardware complexities in real-world\nscenarios introduce a realism gap with the DTs. Moreover, while using real-time\nfull-stack protocols in DTs enables subsequent deployment and testing in a\nreal-world environment, development in DTs requires high computational\ncomplexity and involves a long development time. In this paper, to accelerate\nthe development cycle, we develop a measurement-calibrated Matlab-based\nsimulation framework to replicate performance in a full-stack UAV wireless\nnetwork DT. In particular, we use the DT from the NSF AERPAW platform and\ncompare its reports with those generated by our developed simulation framework\nin wireless networks with similar settings. In both environments, we observe\ncomparable results in terms of RSRP measurement, hence motivating iterative use\nof the developed simulation environment with the DT.",
      "generated_abstract": "advancements in UAVs have enabled the development of advanced\ntechnologies, such as aerial intelligence and robotics, that significantly\nenhance the efficiency and safety of various fields. However, these advancements\nrequire the development of digital twins that closely replicate the real-world\nenvironment, which can facilitate the creation of virtual prototypes. In this\npaper, we present a simulation framework designed to accelerate the development\nof digital twins for UAV networks, which is based on the TensorFlow Protocol\n(TFP) for simulation. The framework features an advanced optimization framework\nthat can dynamically adjust simulation settings to meet specific performance\nrequirements. Additionally, it supports dynamic and adaptive simulation\nsettings, which enables the system to adapt to changing conditions and\ndemand. The framework also includes a flexible data exchange mechanism that\nallows for the easy integration of external data sources. The framework has been\nvalidated",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.29357798165137616,
          "p": 0.3368421052631579,
          "f": 0.3137254852196271
        },
        "rouge-2": {
          "r": 0.0641025641025641,
          "p": 0.07692307692307693,
          "f": 0.0699300649713926
        },
        "rouge-l": {
          "r": 0.25688073394495414,
          "p": 0.29473684210526313,
          "f": 0.2745097989451173
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/IT/2503.08632v1",
      "true_abstract": "This study investigates secret-key generation for device authentication using\nphysical identifiers, such as responses from physical unclonable functions\n(PUFs). The system includes two legitimate terminals (encoder and decoder) and\nan eavesdropper (Eve), each with access to different measurements of the\nidentifier. From the device identifier, the encoder generates a secret key,\nwhich is securely stored in a private database, along with helper data that is\nsaved in a public database accessible by the decoder for key reconstruction.\nEve, who also has access to the public database, may use both her own\nmeasurements and the helper data to attempt to estimate the secret key and\nidentifier. Our setup focuses on authentication scenarios where channel\nstatistics are uncertain, with the involved parties employing multiple antennas\nto enhance signal reception. Our contributions include deriving inner and outer\nbounds on the optimal trade-off among secret-key, storage, and privacy-leakage\nrates for general discrete sources, and showing that these bounds are tight for\nGaussian sources.",
      "generated_abstract": "aper, we study the problem of secret key generation from private\nidentifiers in a channel with uncertainty. We consider the case where a\ncommunicating party can only obtain a private identifier for the sender. The\nkey generation problem is formulated as a decision problem in which the\ncommunicating party is required to select a private identifier based on the\nchannel uncertainty. The key generation problem is a special case of the\nmulti-party secret key generation problem, where the number of parties is\nunbounded. We propose a polynomial-time algorithm that achieves near-optimal\nsecurity under a weaker assumption on the channel uncertainty. Our security\nguarantee is based on a novel and simple lower bound on the entropy of the\nchannel noise. The security proof leverages the fact that the key generation\nproblem can be transformed into a problem in which the number of parties is\nbounded. Our security result is independent of the number of parties and\ncommunicating parties,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21739130434782608,
          "p": 0.3333333333333333,
          "f": 0.2631578899584488
        },
        "rouge-2": {
          "r": 0.03225806451612903,
          "p": 0.04,
          "f": 0.035714280771684356
        },
        "rouge-l": {
          "r": 0.21739130434782608,
          "p": 0.3333333333333333,
          "f": 0.2631578899584488
        }
      }
    },
    {
      "paper_id": "math.LO.math/LO/2503.05360v1",
      "true_abstract": "Sandqvist's base-extension semantics (B-eS) for intuitionistic sentential\nlogic grounds meaning relative to bases (rather than, say, models), which are\narbitrary sets of permitted inferences over sentences. While his soundness\nproof is standard, his completeness proof, is quite unusual. It closely\nparallels a method introduced much earlier by Mints, who developed a\nresolution-based approach to intuitionistic logic using a systematic\ntranslation of formulas into sentential counterparts. In this short note, we\nhighlight the connection between these two approaches and show that the\nsoundness and completeness of B-eS follow directly from Mints' theorem. While\nthe result is modest, it reinforces the relevance of proof-search to\nproof-theoretic semantics and suggests that resolution methods have a deeper\nconceptual role in constructive reasoning than is often acknowledged.",
      "generated_abstract": "uce a new logic for inferential reasoning, inspired by intuitionistic\nlogics and the modal logic of first-order logic. The logic combines the\nintuitionistic modalities of the standard modal logic of first-order logic,\nwith a form of inferential semantics based on the structure of the formulae.\nThis semantics allows us to capture the structure of inferential processes in\nthe logic, with the modal operators expressing the possibility of various\noutcomes. We show that the logic admits a first-order inference rule,\ncharacterized by a certain form of \"indirect\" necessity. We also show that the\nlogic admits a first-order formula for \"inference,\" which expresses the\npossibility of a sequence of formulae, given a sequence of propositions. We\ndiscuss several applications of the logic, including the theory of\nprobabilistic inference, the theory of stochastic processes, and the\nfoundations",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14432989690721648,
          "p": 0.2028985507246377,
          "f": 0.16867469393743664
        },
        "rouge-2": {
          "r": 0.01652892561983471,
          "p": 0.018518518518518517,
          "f": 0.01746724392441171
        },
        "rouge-l": {
          "r": 0.14432989690721648,
          "p": 0.2028985507246377,
          "f": 0.16867469393743664
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.05807v1",
      "true_abstract": "This paper introduces a novel multi-stage decision-making model that\nintegrates hypothesis testing and dynamic programming algorithms to address\ncomplex decision-making scenarios.Initially,we develop a sampling inspection\nscheme that controls for both Type I and Type II errors using a simple random\nsampling method without replacement,ensuring the randomness and\nrepresentativeness of the sample while minimizing selection bias.Through the\napplication of hypothesis testing theory,a hypothesis testing model concerning\nthe defect rate is established,and formulas for the approximate distribution of\nthe sample defect rate and the minimum sample size required under two different\nscenarios are derived. Subsequently,a multi-stage dynamic programming decision\nmodel is constructed.This involves defining the state transition functions and\nstage-specific objective functions,followed by obtaining six optimal decision\nstrategies under various conditions through backward recursion.The results\ndemonstrate the model's potent capability for multi-stage decision-making and\nits high interpretability,offering significant advantages in practical\napplications.",
      "generated_abstract": "In this paper, a decision model for the multi-stage optimization problem is\ndeveloped based on the hypothesis testing and dynamic programming algorithm.\nThis model is designed to solve the multi-stage optimization problem in\naccordance with the characteristics of the optimization problem, such as the\nnumber of stages, the degree of independence, the degree of randomness, and the\ncomplexity of the objective function. The method of the hypothesis testing is\nused to establish the reliability of the solution of the model, and the\nalgorithm of the dynamic programming is used to solve the optimization problem\nin a stable manner. Finally, simulation results show that the proposed model\nhas better solution quality than the original model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2079207920792079,
          "p": 0.3442622950819672,
          "f": 0.2592592545640909
        },
        "rouge-2": {
          "r": 0.0661764705882353,
          "p": 0.09782608695652174,
          "f": 0.07894736360726406
        },
        "rouge-l": {
          "r": 0.2079207920792079,
          "p": 0.3442622950819672,
          "f": 0.2592592545640909
        }
      }
    },
    {
      "paper_id": "eess.SP.math/FA/2503.10274v1",
      "true_abstract": "This paper devotes to combine the chirp basis function transformation and\nsymplectic coordinates transformation to yield a novel Wigner distribution (WD)\nassociated with the linear canonical transform (LCT), named as the symplectic\nWD in the LCT domain (SWDL). It incorporates the merits of the symplectic WD\n(SWD) and the WD in the LCT domain (WDL), achieving stronger capability in the\nlinear frequency-modulated (LFM) signal frequency rate feature extraction while\nmaintaining the same level of computational complexity. Some essential\nproperties of the SWDL are derived, including marginal distributions, energy\nconservations, unique reconstruction, Moyal formula, complex conjugate\nsymmetry, time reversal symmetry, scaling property, time translation property,\nfrequency modulation property, and time translation and frequency modulation\nproperty. Heisenberg's uncertainty principles of the SWDL are formulated,\ngiving rise to three kinds of lower bounds attainable respectively by Gaussian\nenveloped complex exponential signal, Gaussian signal and Gaussian enveloped\nchirp signal. The optimal symplectic matrices corresponding to the highest\ntime-frequency resolution are generated by solving the lower bound optimization\n(minimization) problem. The time-frequency resolution of the SWDL is compared\nwith those of the SWD and WDL to demonstrate its superiority in LFM signals\ntime-frequency energy concentration. A synthesis example is also carried out to\nverify the feasibility and reliability of the theoretical analysis.",
      "generated_abstract": "We introduce a new generalization of the Wigner distribution, the\nsymplectic Wigner distribution, in the linear canonical transform domain.\nThis is a generalization of the classical Wigner distribution, and it is\ndesigned to be invariant under the canonical transformation. In this paper, we\ngive a rigorous derivation of the symplectic Wigner distribution and show that\nit is a special case of the Wigner distribution. We also present an\napplication of the symplectic Wigner distribution in quantum mechanics. We\ndemonstrate that the symplectic Wigner function is related to the Wigner\nfunction by a unitary transformation, and this relation allows us to derive a\nquantum version of the symplectic Wigner function.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15267175572519084,
          "p": 0.36363636363636365,
          "f": 0.21505375927563886
        },
        "rouge-2": {
          "r": 0.043010752688172046,
          "p": 0.09523809523809523,
          "f": 0.05925925497283981
        },
        "rouge-l": {
          "r": 0.13740458015267176,
          "p": 0.32727272727272727,
          "f": 0.19354838293155288
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2502.09860v1",
      "true_abstract": "Molecular discovery has brought great benefits to the chemical industry.\nVarious molecule design techniques are developed to identify molecules with\ndesirable properties. Traditional optimization methods, such as genetic\nalgorithms, continue to achieve state-of-the-art results across multiple\nmolecular design benchmarks. However, these techniques rely solely on random\nwalk exploration, which hinders both the quality of the final solution and the\nconvergence speed. To address this limitation, we propose a novel approach\ncalled Gradient Genetic Algorithm (Gradient GA), which incorporates gradient\ninformation from the objective function into genetic algorithms. Instead of\nrandom exploration, each proposed sample iteratively progresses toward an\noptimal solution by following the gradient direction. We achieve this by\ndesigning a differentiable objective function parameterized by a neural network\nand utilizing the Discrete Langevin Proposal to enable gradient guidance in\ndiscrete molecular spaces. Experimental results demonstrate that our method\nsignificantly improves both convergence speed and solution quality,\noutperforming cutting-edge techniques. For example, it achieves up to a 25%\nimprovement in the top-10 score over the vanilla genetic algorithm. The code is\npublicly available at https://github.com/debadyuti23/GradientGA.",
      "generated_abstract": "We present a novel methodology for the molecular design of small molecule\ndrug candidates that combines the advantages of gradient-based evolutionary\nalgorithms with the flexibility of genetic programming. This approach is\nspecifically tailored to the design of drug candidates with a specific target\nfunction, and we demonstrate that it achieves significantly better results\nthan existing methods in a variety of benchmark problems. In particular, we\nobtain improved performance in the HIV-1 V3 loop targeting problem,\ndemonstrating the potential of our methodology to address challenging drug\ndesign problems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1865671641791045,
          "p": 0.3968253968253968,
          "f": 0.25380710224844755
        },
        "rouge-2": {
          "r": 0.03468208092485549,
          "p": 0.07142857142857142,
          "f": 0.04669260260352204
        },
        "rouge-l": {
          "r": 0.14925373134328357,
          "p": 0.31746031746031744,
          "f": 0.20304568092865066
        }
      }
    },
    {
      "paper_id": "physics.gen-ph.physics/gen-ph/2503.09615v1",
      "true_abstract": "We proceed to the canonical quantization of the complex scalar field without\nmaking use of its real and imaginary parts. Our motivation is to formally\nconnect, as tightly as possible, the quantum-field notions of particle and\nantiparticle - most prominently represented, formally, by the creation and\nannihilation operators - to the initial classical field theory - whose main\nformal object is the field amplitude at a given spacetime point. Our point of\nview is that doing this via the use of the real and imaginary parts of the\nfield is not satisfying. The derivation demands to consider, just before\nquantization, the field and its complex conjugate as independent fields, which\nyields a system of two copies of independent complex scalar fields. One then\nproceeds to quantization with these two copies, which leads to the introduction\nof two families of creation and annihilation operators, corresponding to\nparticles on the one hand, and antiparticles on the other hand. One realizes\nthat having two such families is the only hope for being able to \"invert\" the\ndefinitions of the creation and annihilation in terms of the Fourier quantized\nfields, so as to obtain an expression of the direct-space fields in terms of\nthese creation and annihilation operators, because the real-field condition\nused in the case of a real scalar field does not hold for a complex scalar\nfield. This hope is then met by introducing the complex-conjugate constraint at\nthe quantum level, that is, that the second independent field copy is actually\nthe complex conjugate of the first. All standard results are then recovered in\na rigorous and purely deductive way. While we reckon our derivation exists in\nthe literature, we have not found it.",
      "generated_abstract": "This paper shows that the canonical quantization of the complex scalar\nfield $\\phi(x)$ is a generalization of the canonical quantization of its\nreal and imaginary parts $\\phi(x)$ and $\\overline{\\phi(x)}$. The former is a\ndirect consequence of the fact that $\\phi(x)$ is a holomorphic function and\ntherefore a member of the complex Hilbert space, and the latter is a consequence\nof the fact that $\\phi(x)$ is a complex function and therefore a member of the\ncomplex Hilbert space. The latter is a generalization of the canonical\nquantization of a complex function, which is used in the literature to study\nthe canonical quantization of the complex field. We also discuss some\napplications of the canonical quantization of the complex scalar field in\nphysics and discuss some limitations of the quantization of the complex scalar\nfield.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1476510067114094,
          "p": 0.4583333333333333,
          "f": 0.22335025012136364
        },
        "rouge-2": {
          "r": 0.06882591093117409,
          "p": 0.22077922077922077,
          "f": 0.1049382679814435
        },
        "rouge-l": {
          "r": 0.14093959731543623,
          "p": 0.4375,
          "f": 0.21319796585740422
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2502.20852v1",
      "true_abstract": "Magnetic Resonance Imaging (MRI) Super-Resolution (SR) addresses the\nchallenges such as long scan times and expensive equipment by enhancing image\nresolution from low-quality inputs acquired in shorter scan times in clinical\nsettings. However, current SR techniques still have problems such as limited\nability to capture both local and global static patterns effectively and\nefficiently. To address these limitations, we propose Delta-WKV, a novel MRI\nsuper-resolution model that combines Meta-in-Context Learning (MiCL) with the\nDelta rule to better recognize both local and global patterns in MRI images.\nThis approach allows Delta-WKV to adjust weights dynamically during inference,\nimproving pattern recognition with fewer parameters and less computational\neffort, without using state-space modeling. Additionally, inspired by\nReceptance Weighted Key Value (RWKV), Delta-WKV uses a quad-directional\nscanning mechanism with time-mixing and channel-mixing structures to capture\nlong-range dependencies while maintaining high-frequency details. Tests on the\nIXI and fastMRI datasets show that Delta-WKV outperforms existing methods,\nimproving PSNR by 0.06 dB and SSIM by 0.001, while reducing training and\ninference times by over 15\\%. These results demonstrate its efficiency and\npotential for clinical use with large datasets and high-resolution imaging.",
      "generated_abstract": "olution (SR) is a fundamental task in medical imaging. However,\ndue to the lack of ground truth, existing SR methods often struggle with\ndata imbalance. Moreover, most existing methods only focus on specific\nconditions, which are often insufficient for real-world applications. In this\npaper, we propose a novel Meta-in-Context (Meta-in-Context) learner, which\nintroduces the Meta-in-Context-Decoupling (MID) block to enhance the\ninterpretability of the Meta-Decoder. In addition, we introduce a Delta-WKV\nloss function to reduce the uncertainty of the Meta-Decoder. Extensive\nexperiments show that our method can achieve state-of-the-art performance in\nboth synthetic and real-world scenarios, with a mean SRD of 1.83 and 1.56 for\nsyn",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14788732394366197,
          "p": 0.25925925925925924,
          "f": 0.1883408025490158
        },
        "rouge-2": {
          "r": 0.016853932584269662,
          "p": 0.028846153846153848,
          "f": 0.021276591088980445
        },
        "rouge-l": {
          "r": 0.13380281690140844,
          "p": 0.2345679012345679,
          "f": 0.17040358281807408
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2408.06683v1",
      "true_abstract": "The mechanical properties within living cells play a critical role in the\nadaptive regulation of their biological functions upon environmental and\ninternal stimuli. While these properties exhibit nonequilibrium dynamics due to\nthe thermal and nonthermal forces that universally coexist in\nactin-myosin-active proliferative cells, quantifying them within such complex\nsystems remains challenging. Here, we develop a nonequilibrium framework that\ncombines fluorescence correlation spectroscopy (FCS) measurements of\nintracellular diffusion with nonequilibrium theory to quantitatively analyze\ncell-specific nonthermal driving forces and cellular adaptability. Our results\nreveal that intracellular particle diffusion is influenced not only by common\nthermal forces but also by nonthermal forces generated by approximately 10-100\nmotor proteins. Furthermore, we derive a physical parameter that quantitatively\nassesses the sensitivity of intracellular particle responses to these\nnonthermal forces, showing that systems with more active diffusion exhibit\nhigher response sensitivity. Our work highlights the biological fluctuations\narising from multiple interacting elements, advancing the understanding of the\ncomplex mechanical properties within living cells.",
      "generated_abstract": "e a general method to measure the nonthermal driving forces acting\non cells in the nonequilibrium steady state. This approach relies on the\nexistence of nonequilibrium fluctuations and can be applied to both the\nmicroscopic and macroscopic levels of organization. We show that the nonequilibrium\nstochastic force can be obtained from the nonequilibrium fluctuations by\nconsidering only the effects of thermal fluctuations. This is true if the\nthermal fluctuations are small compared to the nonequilibrium fluctuations. We\ndemonstrate that the nonequilibrium stochastic force can be obtained from the\nnonequilibrium fluctuations by considering only the effects of thermal\nfluctuations. This is true if the thermal fluctuations are small compared to\nthe nonequilibrium fluctuations. We apply this",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16981132075471697,
          "p": 0.33962264150943394,
          "f": 0.22641508989517822
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.0684931506849315,
          "f": 0.04484304492348573
        },
        "rouge-l": {
          "r": 0.16037735849056603,
          "p": 0.32075471698113206,
          "f": 0.21383647354297702
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.03860v1",
      "true_abstract": "Covariate balancing is a popular technique for controlling confounding in\nobservational studies. It finds weights for the treatment group which are close\nto uniform, but make the group's covariate means (approximately) equal to those\nof the entire sample. A crucial question is: how approximate should the\nbalancing be, in order to minimize the error of the final estimate? Current\nguidance is derived from heuristic or asymptotic analyses, which are\nuninformative when the size of the sample is small compared to the number of\ncovariates. This paper presents the first rigorous, nonasymptotic analysis of\ncovariate balancing; specifically, we use PAC-Bayesian techniques to derive\nvalid, finite-sample confidence intervals for the treatment effect. More\ngenerally, we prove these guarantees for a flexible form of covariate balancing\nwhere the regularization parameters weighting the tradeoff between bias\n(imbalance) and variance (divergence from uniform) are optimized, not fixed.\nThis gives rise to a new balancing algorithm which empirically delivers\nsuperior adaptivity. Our overall contribution is to make covariate balancing a\nmore reliable method for causal inference.",
      "generated_abstract": "inference, causal estimands are often defined using a specific\ncounterfactual that is unobservable. If causal estimands are defined using a\nspecific counterfactual, then causal parameters will be estimated using the\ncounterfactual. This may not be the most natural counterfactual, and may not\nbe realistic. In this paper, we consider a causal estimand that is defined by a\ncounterfactual that is partially observed. We characterize the causal\nestimand in terms of the probability distribution of this partially observed\ncounterfactual, and we show that the distribution of the counterfactual can be\nobtained by solving a Markov Chain Monte Carlo (MCMC) algorithm. We show that\nthe distribution of the counterfactual is in some sense balanced. The\ncharacterization of the causal estimand is motivated by the balancedness of\ncounterfactuals. This motivates",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1092436974789916,
          "p": 0.20634920634920634,
          "f": 0.14285713833051578
        },
        "rouge-2": {
          "r": 0.006172839506172839,
          "p": 0.009900990099009901,
          "f": 0.007604558006624404
        },
        "rouge-l": {
          "r": 0.1092436974789916,
          "p": 0.20634920634920634,
          "f": 0.14285713833051578
        }
      }
    },
    {
      "paper_id": "cs.CY.q-fin/EC/2503.00632v1",
      "true_abstract": "Improving social welfare is a complex challenge requiring policymakers to\noptimize objectives across multiple time horizons. Evaluating the impact of\nsuch policies presents a fundamental challenge, as those that appear suboptimal\nin the short run may yield significant long-term benefits. We tackle this\nchallenge by analyzing the long-term dynamics of two prominent policy\nframeworks: Rawlsian policies, which prioritize those with the greatest need,\nand utilitarian policies, which maximize immediate welfare gains. Conventional\nwisdom suggests these policies are at odds, as Rawlsian policies are assumed to\ncome at the cost of reducing the average social welfare, which their\nutilitarian counterparts directly optimize. We challenge this assumption by\nanalyzing these policies in a sequential decision-making framework where\nindividuals' welfare levels stochastically decay over time, and policymakers\ncan intervene to prevent this decay. Under reasonable assumptions, we prove\nthat interventions following Rawlsian policies can outperform utilitarian\npolicies in the long run, even when the latter dominate in the short run. We\ncharacterize the exact conditions under which Rawlsian policies can outperform\nutilitarian policies. We further illustrate our theoretical findings using\nsimulations, which highlight the risks of evaluating policies based solely on\ntheir short-term effects. Our results underscore the necessity of considering\nlong-term horizons in designing and evaluating welfare policies; the true\nefficacy of even well-established policies may only emerge over time.",
      "generated_abstract": "This paper investigates the optimal design of long-run fiscal and monetary\npolicy in a framework where the government's and central bank's preferences are\ndynamic. We introduce a new model for long-run policy design that takes into\naccount the government's and central bank's preferences, and show that\npolicy-makers can achieve long-run welfare optimality under a broad class of\npreferences. We also develop a model of central bank design, where the central\nbank's preferences are also dynamic, and show that policy-makers can achieve\nlong-run welfare optimality under a broad class of preferences. We illustrate\nthe results with two applications: one to the United States, and another to\nBrazil.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14074074074074075,
          "p": 0.3333333333333333,
          "f": 0.19791666249186207
        },
        "rouge-2": {
          "r": 0.009900990099009901,
          "p": 0.024390243902439025,
          "f": 0.014084502934934743
        },
        "rouge-l": {
          "r": 0.11851851851851852,
          "p": 0.2807017543859649,
          "f": 0.16666666249186207
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.04908v1",
      "true_abstract": "This paper presents a novel dissipativity-based distributed droop-free\ncontrol approach for voltage regulation and current sharing in DC microgrids\n(MGs) comprised of an interconnected set of distributed generators (DGs),\nloads, and power lines. First, we describe the closed-loop DC MG as a networked\nsystem where the DGs and lines (i.e., subsystems) are interconnected via a\nstatic interconnection matrix. This interconnection matrix demonstrates how the\ninputs, outputs, and disturbances of DGs and lines are connected in a DC MG.\nEach DG has a local controller and a distributed global controller. To design\nthe controller, we use the dissipativity properties of the subsystems and\nformulate a linear matrix inequality (LMI) problem. To support the feasibility\nof this problem, we identify a set of necessary local and global conditions\nthat we then enforce in a specifically developed LMI-based controller design\nprocess. In contrast to existing DC MG control solutions, our approach proposes\na unified framework for co-designing the distributed controller and\ncommunication topology. As the co-design process is LMI-based, it can be\nefficiently implemented and evaluated. The effectiveness of the proposed\nsolution can be verified by simulating an islanded DC MG in a MATLAB/Simulink\nenvironment under different scenarios, such as load changes and topological\nconstraint changes, and then comparing the performance with a recent droop\ncontrol algorithm.",
      "generated_abstract": "r investigates distributed control and communication topology\nco-design for voltage regulation and current sharing in decentralized\nmicrogrids (DcMg). Specifically, we formulate the problem of maximizing the\naverage power flow in DcMg as a linear program, and propose a distributed\ncontroller and communication topology for DcMg that achieves optimal power\nflow and minimizes communication overhead. The optimal solution is characterized\nby a distributed controller topology with a specific communication topology.\nThe optimal distributed controller topology is constructed by solving a\nlinear program that optimizes the communication topology and the average power\nflow. We prove the convergence of the optimal distributed controller\ntopology and the communication topology to the optimal solution in a\nLipschitz-continuous sense. Furthermore, we prove that the optimal solution\nsatisfies the dissipativity-based communication topology co-design property,\nwhich guarantees that the communication overhead is minim",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22962962962962963,
          "p": 0.49206349206349204,
          "f": 0.31313130879247014
        },
        "rouge-2": {
          "r": 0.07881773399014778,
          "p": 0.15384615384615385,
          "f": 0.10423452320724907
        },
        "rouge-l": {
          "r": 0.2074074074074074,
          "p": 0.4444444444444444,
          "f": 0.2828282784894399
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.01080v1",
      "true_abstract": "We introduce a new dynamic factor correlation model with a novel\nvariation-free parametrization of factor loadings. The model is applicable to\nhigh dimensions and can accommodate time-varying correlations, heterogeneous\nheavy-tailed distributions, and dependent idiosyncratic shocks, such as those\nobserved in returns on stocks in the same subindustry. We apply the model to a\n\"small universe\" with 12 asset returns and to a \"large universe\" with 323 asset\nreturns. The former facilitates a comprehensive empirical analysis and\ncomparisons and the latter demonstrates the flexibility and scalability of the\nmodel.",
      "generated_abstract": "We introduce a novel dynamic factor correlation model that captures the\nexistence of persistent and non-stationary cross-sectional and dynamic\nrelationships between two or more factors. The model can be characterized by a\nsingle stochastic process that describes the evolution of the correlations\nbetween the two or more factors. The model is characterized by a set of\ndynamical equations that govern the evolution of the correlation structure,\nincluding the possibility of non-stationary cross-sectional correlations and\ndynamic dependencies between the correlations. We develop the theory of the\nmodel, and derive a simple analytical solution for the model. We also provide\nan empirical illustration of the model using a data set of factor\ncorrelations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2542372881355932,
          "p": 0.25862068965517243,
          "f": 0.25641025141062174
        },
        "rouge-2": {
          "r": 0.13253012048192772,
          "p": 0.11956521739130435,
          "f": 0.12571428072751042
        },
        "rouge-l": {
          "r": 0.23728813559322035,
          "p": 0.2413793103448276,
          "f": 0.2393162343166047
        }
      }
    },
    {
      "paper_id": "hep-th.math/QA/2503.10469v1",
      "true_abstract": "We introduce a novel machine learning based framework for discovering\nintegrable models. Our approach first employs a synchronized ensemble of neural\nnetworks to find high-precision numerical solution to the Yang-Baxter equation\nwithin a specified class. Then, using an auxiliary system of algebraic\nequations, [Q_2, Q_3] = 0, and the numerical value of the Hamiltonian obtained\nvia deep learning as a seed, we reconstruct the entire Hamiltonian family,\nforming an algebraic variety. We illustrate our presentation with three- and\nfour-dimensional spin chains of difference form with local interactions.\nRemarkably, all discovered Hamiltonian families form rational varieties.",
      "generated_abstract": "e a deep learning approach for the discovery of integrable systems,\nusing the neural network framework. We introduce a novel architecture\nconstructed from a group of two-dimensional neural networks that are\nspecialized to learn the structure of various integrable systems. Our method\nemploys the MLP-based back-propagation algorithm and is based on a variational\napproximation to the partition function of integrable systems. We demonstrate\nthe method's effectiveness in the context of a simple one-dimensional\nBethe-like equations of motion. The method is also applied to the 1D\nHirota-type Bethe Ansatz, which provides a simple, efficient framework for\nexploring integrable systems. In addition, we present a simple neural network\narchitecture that learns the Bethe ansatz solution directly from the\nhigh-energy behavior of the Bethe-like equations of motion. This architecture\nis also applied to the 1",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.29333333333333333,
          "p": 0.275,
          "f": 0.28387096274713847
        },
        "rouge-2": {
          "r": 0.0851063829787234,
          "p": 0.06779661016949153,
          "f": 0.07547169317728762
        },
        "rouge-l": {
          "r": 0.25333333333333335,
          "p": 0.2375,
          "f": 0.24516128532778367
        }
      }
    },
    {
      "paper_id": "physics.geo-ph.physics/ao-ph/2503.04227v1",
      "true_abstract": "We present the first extensive analysis of K/Ka-band ranging post-fit\nresiduals of an official Level-2 product, characterised as Line-of-Sight\nGravity Differences (LGD), which exhibit and showcase interesting sub-monthly\ngeophysical signals. These residuals, provided by CSR, were derived from the\ndifference between spherical harmonic coefficient least-squares fits and\nreduced Level-1B range-rate observations. We classified the geophysical signals\ninto four distinct categories: oceanic, meteorological, hydrological, and solid\nEarth, focusing primarily on the first three categories in this study. In our\nexamination of oceanic processes, we identified notable mass anomalies in the\nArgentine basin, specifically within the Zapiola Rise, where persistent\nremnants of the rotating dipole-like modes are evident in the LGD post-fit\nresiduals. Our analysis extended to the Gulf of Carpentaria and Australia\nduring the 2013 Oswald cyclone, revealing significant LGD residual anomalies\nthat correlate with cyclone tracking and precipitation data. Additionally, we\ninvestigated the monsoon seasons in Bangladesh, particularly from June to\nSeptember 2007, where we observed peaks in sub-monthly variability. These\nfindings were further validated by demonstrating high spatial and temporal\ncorrelations between gridded LGD residuals and ITSG-Grace2018 daily solutions.\nGiven that these anomalies are associated with significant mass change\nphenomena, it is essential to integrate the post-fit residuals into a\nhigh-frequency mass change framework, with the purpose of providing enhanced\nspatial resolution compared to conventional Kalman-filtered methods.",
      "generated_abstract": "A new technique for estimating the mass change rate of a geostationary\nthorium-230 satellite from its Ka-band range rate is presented. The technique\ncombines a post-fit approach with the use of Ka-band range rate residuals\nfollowing the post-fit method. This approach provides a more accurate\nestimation of the mass change rate than the traditional method of using only\nthe Ka-band range rate. The technique is validated through a detailed\nsimulation of the Sputnik 5 satellite. Results show that the technique\nsignificantly improves the accuracy of the mass change rate estimation by\nreducing the mass change rate uncertainty by 65% compared to the traditional\nmethod. This technique provides a more accurate mass change rate estimate for\ngeostationary satellites, especially those with significant mass changes. The\nresults highlight the potential of this technique for high-frequency mass\nchange applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11612903225806452,
          "p": 0.2647058823529412,
          "f": 0.16143497333950022
        },
        "rouge-2": {
          "r": 0.04265402843601896,
          "p": 0.08256880733944955,
          "f": 0.05624999550800818
        },
        "rouge-l": {
          "r": 0.10967741935483871,
          "p": 0.25,
          "f": 0.15246636347402936
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.10009v1",
      "true_abstract": "We introduce a double/debiased machine learning (DML) estimator for the\nimpulse response function (IRF) in settings where a time series of interest is\nsubjected to multiple discrete treatments, assigned over time, which can have a\ncausal effect on future outcomes. The proposed estimator can rely on fully\nnonparametric relations between treatment and outcome variables, opening up the\npossibility to use flexible machine learning approaches to estimate IRFs. To\nthis end, we extend the theory of DML from an i.i.d. to a time series setting\nand show that the proposed DML estimator for the IRF is consistent and\nasymptotically normally distributed at the parametric rate, allowing for\nsemiparametric inference for dynamic effects in a time series setting. The\nproperties of the estimator are validated numerically in finite samples by\napplying it to learn the IRF in the presence of serial dependence in both the\nconfounder and observation innovation processes. We also illustrate the\nmethodology empirically by applying it to the estimation of the effects of\nmacroeconomic shocks.",
      "generated_abstract": "p a semiparametric inference framework for impulse response\nfunctions (IRFs) using double/debiasing machine learning. The proposed\nframework is applicable to a wide range of IRFs, including functional IRFs,\nwhich are a type of nonparametric IRFs. Our analysis extends previous methods\nto address the challenges of double/debiasing machine learning. First, we\nintroduce a double machine learning estimator for the ARMA-GARCH(1,1) model\nwith an IRF, and we further introduce a debiased machine learning estimator\nfor the ARMA-GARCH(1,1) model with an IRF. Second, we extend the classical\ndouble/debiasing machine learning methodology to functional IRFs. Third, we\ndevelop a semiparametric bootstrap inference method for the ARMA-GARCH(1,1)",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23853211009174313,
          "p": 0.43333333333333335,
          "f": 0.3076923031126362
        },
        "rouge-2": {
          "r": 0.06493506493506493,
          "p": 0.11904761904761904,
          "f": 0.08403360887790434
        },
        "rouge-l": {
          "r": 0.22935779816513763,
          "p": 0.4166666666666667,
          "f": 0.29585798358600895
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/MF/2411.12375v3",
      "true_abstract": "In this paper, we introduce a novel pricing model for Uniswap V3, built upon\nstochastic processes and the Martingale Stopping Theorem. This model\ninnovatively frames the valuation of positions within Uniswap V3. We further\nconduct a numerical analysis and examine the sensitivities through Greek risk\nmeasures to elucidate the model's implications. The results underscore the\nmodel's significant academic contribution and its practical applicability for\nUniswap liquidity providers, particularly in assessing risk exposure and\nguiding hedging strategies.",
      "generated_abstract": "aper, we propose a novel approach to the risk-neutral pricing of\nliquidity positions in the Uniswap liquidity provider market. We establish the\nstochastic differential equations of the probability measures of the\nliquidity provider's stock price, and then derive the stochastic differential\nequations of the stock price for the liquidity provider. Based on the\nstochastic differential equations of the liquidity provider's stock price, we\npropose a model for the stock price of the liquidity provider, which can be\nequivalent to the position stock price of the liquidity provider. We then\npropose a model for the position stock price of the liquidity provider.\nBased on the model for the position stock price of the liquidity provider, we\nestablish the stochastic differential equations of the position stock price of\nthe liquidity provider. In this way, we can obtain the stochastic differential\nequations",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3064516129032258,
          "p": 0.4222222222222222,
          "f": 0.3551401820420998
        },
        "rouge-2": {
          "r": 0.0547945205479452,
          "p": 0.057971014492753624,
          "f": 0.05633802317298199
        },
        "rouge-l": {
          "r": 0.3064516129032258,
          "p": 0.4222222222222222,
          "f": 0.3551401820420998
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/RO/2503.10484v1",
      "true_abstract": "Existing quadrupedal locomotion learning paradigms usually rely on extensive\ndomain randomization to alleviate the sim2real gap and enhance robustness. It\ntrains policies with a wide range of environment parameters and sensor noises\nto perform reliably under uncertainty. However, since optimal performance under\nideal conditions often conflicts with the need to handle worst-case scenarios,\nthere is a trade-off between optimality and robustness. This trade-off forces\nthe learned policy to prioritize stability in diverse and challenging\nconditions over efficiency and accuracy in ideal ones, leading to overly\nconservative behaviors that sacrifice peak performance. In this paper, we\npropose a two-stage framework that mitigates this trade-off by integrating\npolicy learning with imagined transitions. This framework enhances the\nconventional reinforcement learning (RL) approach by incorporating imagined\ntransitions as demonstrative inputs. These imagined transitions are derived\nfrom an optimal policy and a dynamics model operating within an idealized\nsetting. Our findings indicate that this approach significantly mitigates the\ndomain randomization-induced negative impact of existing RL algorithms. It\nleads to accelerated training, reduced tracking errors within the distribution,\nand enhanced robustness outside the distribution.",
      "generated_abstract": "This paper presents a novel approach to learning robust, adaptive robotic\npolicy. We propose a method that utilizes imagined transitions, a technique\nfirst introduced in the field of reinforcement learning, to mitigate the\nover-reliance on robustness and to improve performance under uncertainty. By\nimagining a sequence of possible actions, the robot is able to learn to\nmanipulate objects in new configurations with minimal information. This approach\nenables the robot to adapt to novel scenarios with minimal training and\nrequires minimal human intervention. We demonstrate that imagined transitions\nenhance robustness and improve performance under uncertainty, particularly\nwithin the context of unstructured environments.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1935483870967742,
          "p": 0.34782608695652173,
          "f": 0.24870465861848645
        },
        "rouge-2": {
          "r": 0.03428571428571429,
          "p": 0.06315789473684211,
          "f": 0.044444439883402395
        },
        "rouge-l": {
          "r": 0.1693548387096774,
          "p": 0.30434782608695654,
          "f": 0.2176165757169321
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2411.07986v2",
      "true_abstract": "A fundamental question in the field of molecular computation is what\ncomputational tasks a biochemical system can carry out. In this work, we focus\non the problem of finding the maximum likelihood estimate (MLE) for log-affine\nmodels. We revisit a construction due to Gopalkrishnan of a mass-action system\nwith the MLE as its unique positive steady state, which is based on choosing a\nbasis for the kernel of the design matrix of the model. We extend this\nconstruction to allow for any finite spanning set of the kernel, and explore\nhow the choice of spanning set influences the dynamics of the resulting\nnetwork, including the existence of boundary steady states, the deficiency of\nthe network, and the rate of convergence. In particular, we prove that using a\nMarkov basis as the spanning set guarantees global stability of the MLE steady\nstate.",
      "generated_abstract": "t a maximum likelihood estimation (MLE) method for log-affine\nmodels, which is based on a novel combination of detailed-balanced reaction\nnetworks (DBRNs) and maximum likelihood estimation (MLE) techniques. This\nmethod extends the maximum likelihood estimation (MLE) methodology for\nlog-linear models to log-affine models. We show that the MLE methodology for\nlog-affine models can be extended to incorporate DBRNs, which are\ndetailed-balanced models that represent the equilibrium distribution of\nreaction-rate parameters. We apply the MLE method to the log-affine model for\nthe catalytic cycle of cytochrome b6f complex. Our results demonstrate that the\nMLE methodology for log-affine models can be applied to other log-affine\nmodels, and that the MLE methodology is suitable for applications in\nbi",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23863636363636365,
          "p": 0.3442622950819672,
          "f": 0.28187918979505433
        },
        "rouge-2": {
          "r": 0.07575757575757576,
          "p": 0.10869565217391304,
          "f": 0.08928570944515334
        },
        "rouge-l": {
          "r": 0.2159090909090909,
          "p": 0.3114754098360656,
          "f": 0.2550335522111617
        }
      }
    },
    {
      "paper_id": "physics.app-ph.physics/app-ph/2503.10139v1",
      "true_abstract": "We present an optomechanical device platform for characterization of optical,\nthermal, and rheological properties of fluids on the micron scale. A suspended\nsilicon microdisk resonator with a vibrating mass of 100 fg and an effective\nmeasurement volume of less than a pL is used to monitor properties of different\nfluids at rest. By employing analytical models for thermo-optical effects,\nthermal diffusion and fluid-structure interactions, our platform determines the\nrefractive index, thermal conductivity, viscosity, density and compressibility\nof the fluid, in a compact measurement setup. A single measurement takes as\nshort as 70 microseconds, and the employed power can be less than 100\nmicrowatts, guaranteeing measurement at rest and in thermal equilibrium.",
      "generated_abstract": "The potential of optomechanical sensing for micron-scale sensing has been\nresearched in the literature, however, the coupling between optical and mechanical\nresonances is not fully understood. Here, we demonstrate the potential of\nmultiphysics optomechanical sensing using a liquid as a mechanical resonator\nwith a micron-scale sensing element. The sensing element comprises a\nmulti-colored liquid that is a tunable optical cavity for the optomechanical\nsystem. We demonstrate the sensing principle, and the sensing performance\nreaching a sensing resolution of 200 $\\mu$m. This sensing principle can be\napplied to a variety of applications including sensing of nanoparticles in\nliquids, sensing of droplets in liquids, and sensing of suspensions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19230769230769232,
          "p": 0.2459016393442623,
          "f": 0.2158273332042856
        },
        "rouge-2": {
          "r": 0.037383177570093455,
          "p": 0.041666666666666664,
          "f": 0.03940886200975578
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.21311475409836064,
          "f": 0.18705035478701942
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.04265v1",
      "true_abstract": "Regression discontinuity (RD) designs typically identify the treatment effect\nat a single cutoff point. But when and how can we learn about treatment effects\naway from the cutoff? This paper addresses this question within a\nmultiple-cutoff RD framework. We begin by examining the plausibility of the\nconstant bias assumption proposed by Cattaneo, Keele, Titiunik, and\nVazquez-Bare (2021) through the lens of rational decision-making behavior,\nwhich suggests that a kind of similarity between groups and whether individuals\ncan influence the running variable are important factors. We then introduce an\nalternative set of assumptions and propose a broadly applicable partial\nidentification strategy. The potential applicability and usefulness of the\nproposed bounds are illustrated through two empirical examples.",
      "generated_abstract": "r develops a methodology for extrapolating treatment effects from\nintervention and control groups in multiple-cutoff regression discontinuity\ndesigns. The methodology combines a theoretical analysis with an empirical\napproach. The theoretical analysis provides a theoretical foundation for\nextrapolating treatment effects from cutoff values that differ from those used\nin the intervention and control groups. The empirical approach uses\ncross-sectional data from the 2018-2019 National Health Interview Survey to\nestimate the extrapolated treatment effect from cutoff values that differ from\nthose used in the intervention and control groups. The estimated extrapolated\ntreatment effect is compared with the extrapolated treatment effect from the\ncutoff values used in the intervention and control groups. The results suggest\nthat extrapolating treatment effects from cutoff values that differ from those\nused in the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17582417582417584,
          "p": 0.3076923076923077,
          "f": 0.2237762191481247
        },
        "rouge-2": {
          "r": 0.02654867256637168,
          "p": 0.04054054054054054,
          "f": 0.03208555671480526
        },
        "rouge-l": {
          "r": 0.16483516483516483,
          "p": 0.28846153846153844,
          "f": 0.20979020516211072
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.03114v1",
      "true_abstract": "This study constructs a novel analytical general equilibrium model to compare\nenvironmental policies in a setting where oligopolistic energy firms engage in\nthird-degree price discrimination across residential consumers and industrial\nfirms. Closed-form solutions demonstrate the impact on prices and quantities.\nThe resulting welfare change is decomposed across three distortions: output,\nprice discrimination, and externality. This study finds that the output\ndistortion and price discrimination welfare effects generally move in opposite\ndirections under policies such as an emission tax or a two-part instrument.\nNumerical analysis compares policies and finds scenarios where the output\ndistortion and price discrimination welfare changes fully offset and thus\nleaves the net welfare gain of the externality correction. In this way,\nenvironmental policy can be designed to mitigate output distortion welfare\nconcerns when firms have market power.",
      "generated_abstract": "I study the effects of environmental policy on industrial activity in a\nenvironmentally friendly economy. I consider the market power of a dominant\nfirm and the price discrimination of a monopolist, and allow for imperfect\ninformation and imperfect competition. I find that both policies have an\neffect on industrial activity, as they reduce the number of firms. However,\nthe effect of the dominant firm is stronger than the effect of the monopolist,\nand the effect of the dominant firm is stronger when the price discrimination\nis greater. In addition, the dominant firm has a larger effect on\nproduction-technology spillovers than the monopolist does.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26373626373626374,
          "p": 0.42857142857142855,
          "f": 0.3265306075283447
        },
        "rouge-2": {
          "r": 0.041666666666666664,
          "p": 0.06172839506172839,
          "f": 0.04975123896933291
        },
        "rouge-l": {
          "r": 0.25274725274725274,
          "p": 0.4107142857142857,
          "f": 0.31292516535147397
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2503.02642v1",
      "true_abstract": "In both machine learning and in computational neuroscience, plasticity in\nfunctional neural networks is frequently expressed as gradient descent on a\ncost. Often, this imposes symmetry constraints that are difficult to reconcile\nwith local computation, as is required for biological networks or neuromorphic\nhardware. For example, wake-sleep learning in networks characterized by\nBoltzmann distributions builds on the assumption of symmetric connectivity.\nSimilarly, the error backpropagation algorithm is notoriously plagued by the\nweight transport problem between the representation and the error stream.\nExisting solutions such as feedback alignment tend to circumvent the problem by\ndeferring to the robustness of these algorithms to weight asymmetry. However,\nthey are known to scale poorly with network size and depth. We introduce\nspike-based alignment learning (SAL), a complementary learning rule for spiking\nneural networks, which uses spike timing statistics to extract and correct the\nasymmetry between effective reciprocal connections. Apart from being\nspike-based and fully local, our proposed mechanism takes advantage of noise.\nBased on an interplay between Hebbian and anti-Hebbian plasticity, synapses can\nthereby recover the true local gradient. This also alleviates discrepancies\nthat arise from neuron and synapse variability -- an omnipresent property of\nphysical neuronal networks. We demonstrate the efficacy of our mechanism using\ndifferent spiking network models. First, we show how SAL can significantly\nimprove convergence to the target distribution in probabilistic spiking\nnetworks as compared to Hebbian plasticity alone. Second, in neuronal\nhierarchies based on cortical microcircuits, we show how our proposed mechanism\neffectively enables the alignment of feedback weights to the forward pathway,\nthus allowing the backpropagation of correct feedback errors.",
      "generated_abstract": "ansport is a fundamental process in neural networks that converts\nweighted activations into unweighted activations. When weight transport is\ncontrolled by a spike signal, it can be used to improve the robustness of\ngradient estimates in neural networks. However, this process is highly\ntime-consuming, and existing methods often have weak convergence guarantees.\nIn this paper, we propose a novel algorithm to solve the weight transport problem\nin a stochastic gradient setting, which we call the Stochastic Weight\nTransport (SWT). Our algorithm is a variant of the Stochastic Gradient Descent\n(SGD) algorithm and it can be interpreted as a continuous-time version of the\nalgorithm proposed by Vempala et al. (2013). We derive a convergence rate for\nour algorithm, and show that it is consistent and asymptotically normal under\ncertain conditions. We demonstrate the effectiveness of our algorithm in a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1952662721893491,
          "p": 0.3707865168539326,
          "f": 0.2558139489691125
        },
        "rouge-2": {
          "r": 0.03937007874015748,
          "p": 0.078125,
          "f": 0.05235601648639055
        },
        "rouge-l": {
          "r": 0.17159763313609466,
          "p": 0.3258426966292135,
          "f": 0.22480619703112803
        }
      }
    },
    {
      "paper_id": "physics.soc-ph.physics/soc-ph/2503.08418v1",
      "true_abstract": "Human mobility, a pivotal aspect of urban dynamics, displays a profound and\nmultifaceted relationship with urban sustainability. Despite considerable\nefforts analyzing mobility patterns over decades, the ranking dynamics of urban\nmobility has received limited attention. This study aims to contribute to the\nfield by investigating changes in rank and size of hourly inflows to various\nlocations across 60 Chinese cities throughout the day. We find that the\nrank-size distribution of hourly inflows over the course of the day is stable\nacross cities. To uncover the microdynamics beneath the stable aggregate\ndistribution amidst shifting location inflows, we analyzed consecutive-hour\ninflow size and ranking variations. Our findings reveal a dichotomy: locations\nwith higher daily average inflow display a clear monotonic trend, with more\npronounced increases or decreases in consecutive-hour inflow. In contrast,\nranking variations exhibit a non-monotonic pattern, distinguished by the\nstability of not only the top and bottom rankings but also those in\nmoderately-inflowed locations. Finally, we compare ranking dynamics across\ncities using a ranking metric, the rank turnover. The results advance our\nunderstanding of urban mobility dynamics, providing a basis for applications in\nurban planning and traffic engineering.",
      "generated_abstract": "of urban mobility dynamics has been of fundamental importance for\nmany disciplines, ranging from transport planning to public policy. However,\nthe complexity of these dynamics remains difficult to capture. The main\nchallenge is to capture the non-linear interactions between vehicles and the\ncity, which are usually modeled as deterministic systems. However, in reality,\nmobility patterns and interactions are highly non-deterministic, driven by\nspatial, temporal and environmental factors, as well as by human behavior. In\nthis paper, we introduce the concept of ranking dynamics to capture this\nnon-linearity. We show that this concept allows to model mobility dynamics as\na stochastic differential equation with a continuous-time Markov chain\n(CTMC) of rank-ordered states. The rank-ordered state space is a compact\nanalogue of the original state space, allowing to efficiently handle large\nsystems. We then apply this concept",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17886178861788618,
          "p": 0.2391304347826087,
          "f": 0.20465115789464586
        },
        "rouge-2": {
          "r": 0.02824858757062147,
          "p": 0.038461538461538464,
          "f": 0.03257328501947044
        },
        "rouge-l": {
          "r": 0.17073170731707318,
          "p": 0.22826086956521738,
          "f": 0.19534883231325054
        }
      }
    },
    {
      "paper_id": "math.AP.math/AP/2503.09924v1",
      "true_abstract": "This paper discusses the possibility of applying the velocity averaging\ntheorems in [F. Golse, P.-L. Lions, B. Perthame, R. Sentis: J. Funct. Anal.\n76(1):110--125, 1988] to the Wigner equation governing the quantum evolution of\nthe Wigner transform of quantum density operators. Our first main results\naddress the case of the Wigner function of a special class of density operators\nassociated to mixed states, whose Hilbert-Schmidt norm is of order\n$\\hbar^{d/2}$, where $d$ is the space dimension and $\\hbar$ the reduced Planck\nconstant. In space dimension $d=1$, we prove that the density function belongs\nto the Sobolev space $H^s(\\mathbb R)$ for some $s>0$. In the case of pure\nstates, we first obtain a characterization of the Wigner transform of rank-one\nquantum density operators, and apply this characterization (1) to analyze a\nrather general setting in which velocity averaging cannot apply to the Wigner\nfunctions of a family of rank-one density operators whose evolution is governed\nby the von Neumann equation, and (2) to obtain a quick derivation of Madelung's\nsystem of quantum hydrodynamic equations. This derivation provides a physical\nexplanation of one key assumption used in the proof of the negative result (1)\ndescribed above.",
      "generated_abstract": "We study the Wigner kinetic equation for the velocity averaged density\n$\\langle \\rho(\\vec{x},t)\\rangle$ in the semiclassical regime. We obtain an\nexplicit formula for the velocity averaged density in the limit of large\nsemiclassical times $t\\gg 1/\\kappa$ where $\\kappa$ is the semiclassical\nwave-number. We prove that the velocity averaged density is exponentially\ndecaying in the limit $t\\gg 1/\\kappa$. We also show that the velocity\naveraged density is proportional to the semiclassical velocity squared. We\ninvestigate the velocity averaged density in the limit $t\\ll 1/\\kappa$. We\nobtain an explicit formula for the velocity averaged density in the limit of\nlarge semiclassical times $t\\ll 1/\\kappa$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12280701754385964,
          "p": 0.35,
          "f": 0.18181817797267674
        },
        "rouge-2": {
          "r": 0.040697674418604654,
          "p": 0.11864406779661017,
          "f": 0.06060605680253393
        },
        "rouge-l": {
          "r": 0.11403508771929824,
          "p": 0.325,
          "f": 0.16883116498566378
        }
      }
    },
    {
      "paper_id": "math.NA.cs/NA/2503.10194v1",
      "true_abstract": "This paper describes novel algorithms for the identification of\n(almost-)resonant behavior in scattering problems. Our methods, relying on\nrational approximation, aim at building surrogate models of what we call \"field\namplification\", defined as the norm of the solution operator of the scattering\nproblem, which we express through boundary-integral equations. To provide our\ntechniques with theoretical foundations, we first derive results linking the\nfield amplification to the spectral properties of the operator that defines the\nscattering problem. Such results are then used to justify the use of rational\napproximation in the surrogate-modeling task. Some of our proposed methods\napply rational approximation in a \"standard\" way, building a rational\napproximant for either the solution operator directly or, in the interest of\ncomputational efficiency, for a randomly \"sketched\" version of it. Our other\n\"hybrid\" approaches are more innovative, combining\nrational-approximation-assisted root-finding with approximation using radial\nbasis functions. Three key features of our methods are that (i) they are\nagnostic of the strategy used to discretize the scattering problem, (ii) they\ndo not require any computations involving non-real wavenumbers, and (iii) they\ncan adjust to different settings through the use of adaptive sampling\nstrategies. We carry out some numerical experiments involving 2D scatterers to\ncompare our approaches. In our tests, two of our approaches (one standard, one\nhybrid) emerge as the best performers, with one or the other being preferable,\ndepending on whether emphasis is placed on accuracy or efficiency.",
      "generated_abstract": "r introduces a novel approach to surrogate modeling in scattering\nproblems, enabling efficient and accurate prediction of scattering properties\nunder varying conditions. The approach leverages rational approximation to\nconstruct a rational function that approximates the scattering problem. The\nrational function is then used as a basis for constructing a basis of\nhigh-order polynomials, which is used to construct a surrogate model for the\nscattering problem. The proposed approach is demonstrated through a series of\nnumerical examples, including the K-space scattering problem, the\nscattering-by-projection problem, and the scattering-by-computation problem.\nThe proposed approach demonstrates significant computational efficiency,\nespecially for large-scale scattering problems. The results demonstrate that\nthe proposed approach is effective in capturing the complex resonant behavior\nin scattering problems, and can be used to improve computational efficiency in\nscattering",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2077922077922078,
          "p": 0.4507042253521127,
          "f": 0.2844444401248396
        },
        "rouge-2": {
          "r": 0.055299539170506916,
          "p": 0.10810810810810811,
          "f": 0.07317072722951397
        },
        "rouge-l": {
          "r": 0.18831168831168832,
          "p": 0.4084507042253521,
          "f": 0.2577777734581729
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.08026v1",
      "true_abstract": "A principal uses payments conditioned on stochastic outcomes of a team\nproject to elicit costly effort from the team members. We develop a multi-agent\ngeneralization of a classic first-order approach to contract optimization by\nleveraging methods from network games. The main results characterize the\noptimal allocation of incentive pay across agents and outcomes. Incentive\noptimality requires equalizing, across agents, a product of (i) individual\nproductivity (ii) organizational centrality and (iii) responsiveness to\nmonetary incentives.",
      "generated_abstract": "r develops a theory of incentive design with spillovers, where\neither the design or the payoff may depend on the actions of others. We\nintroduce a class of incentive-compatible designs and derive the conditions\nunder which the payoff is independent of the actions of others. We show that\nthe design is incentive-compatible if and only if the payoff is independent of\nthe actions of others, and we characterize this class of designs in terms of\nthe incentive-compatible design. We also study the case where the payoff is\nindependent of the actions of others and the design is not incentive-compatible.\nWe further show that when the payoff is incentive-compatible, the design is\nalways incentive-compatible. We then derive a general characterization of the\nclass of incentive-compatible designs in terms of the payoff and show that this\nclass",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13333333333333333,
          "p": 0.15384615384615385,
          "f": 0.14285713788265325
        },
        "rouge-2": {
          "r": 0.013888888888888888,
          "p": 0.010869565217391304,
          "f": 0.012195117025582
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.11538461538461539,
          "f": 0.10714285216836758
        }
      }
    },
    {
      "paper_id": "math.RA.math/RA/2503.08288v1",
      "true_abstract": "We study numerical regularities for complexes over noncommutative noetherian\nlocally finite $\\mathbb{N}$-graded algebras $A$ such as CM (cm)-regularity, Tor\n(tor)-regularity (Ext (ext)-regularity) and Ex (ex)-regularity, which are the\nsupremum or infimum degrees of some associated canonical complexes. We show\nthat for any right bounded complex $X$ with finitely generated cohomologies,\nthe supremum degree of $R\\underline{\\text{Hom}}_A(X, A_0)$ coincides with the\nopposite of the infimum degree of $X$ if $A_0$ is semisimple. If $A$ has a\nbalanced dualizing complex and $A_0$ is semisimple, we prove that the\nCM-regularity of $X$ coincides with the supremum degree of\n$R\\underline{\\text{Hom}}_A(A_0,X)$ for any left bounded complex $X$ with\nfinitely generated cohomologies.\n  Several inequalities concerning the numerical regularities and the supremum\nor infimum degree of derived Hom or derived tensor complexes are given for\nnoncommutative noetherian locally finite $\\mathbb{N}$-graded algebras. Some of\nthese are generalizations of J\\o rgensen's results on the inequalities between\nthe CM-regularity and Tor-regularity, some are new even in the connected graded\ncase. Conditions are given under which the inequalities become equalities by\nestablishing two technical lemmas.\n  Following Kirkman, Won and Zhang, we also use the numerical AS-regularity\n(resp. little AS-regularity) to study Artin-Schelter regular property\n(finite-dimensional property) for noetherian $\\mathbb{N}$-graded algebras. We\nprove that the numerical AS-regularity of $A$ is zero if and only if that $A$\nis an $\\mathbb{N}$-graded AS-regular algebra under some mild conditions, which\ngeneralizes a result of Dong-Wu and a result of Kirkman-Won-Zhang. If $A$ has a\nbalanced dualizing complex and $A_0$ is semisimple, we prove that the little\nAS-regularity of $A$ is zero if and only if $A$ is finite-dimensional.",
      "generated_abstract": "We establish a regularity result for the homology of the algebra of formal\nchains on a positively graded algebra, in the case that the grading is given by\nthe number of generators of the algebra. This regularity is also applied to the\nhomology of the algebra of differential forms on a positively graded algebra,\nshowing that the homology of the algebra of differential forms is naturally\nisomorphic to the homology of the algebra of formal chains.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13385826771653545,
          "p": 0.5,
          "f": 0.21118012089194096
        },
        "rouge-2": {
          "r": 0.015,
          "p": 0.0625,
          "f": 0.024193545265349
        },
        "rouge-l": {
          "r": 0.11811023622047244,
          "p": 0.4411764705882353,
          "f": 0.18633540039504654
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.09647v1",
      "true_abstract": "This paper introduces a methodology leveraging Large Language Models (LLMs)\nfor sector-level portfolio allocation through systematic analysis of\nmacroeconomic conditions and market sentiment. Our framework emphasizes\ntop-down sector allocation by processing multiple data streams simultaneously,\nincluding policy documents, economic indicators, and sentiment patterns.\nEmpirical results demonstrate superior risk-adjusted returns compared to\ntraditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and\nportfolio return of 8.79% versus -0.61 and -1.39% respectively. These results\nsuggest that LLM-based systematic macro analysis presents a viable approach for\nenhancing automated portfolio allocation decisions at the sector level.",
      "generated_abstract": "aper, we leverage Large Language Models (LLMs) to automate the\nanalysis of large financial datasets. We propose a novel approach to\nidentify sector-specific top-down strategies and their associated risk\nprofiles. Specifically, we use the LLMs to extract features from company\nfinancial data, then apply a machine learning model to identify patterns in\nthese features. Our methodology is particularly useful in automated trading\nenvironments, where data analysis is often a time-consuming manual process.\nHowever, we find that automated trading systems often struggle to interpret\nhigh-level patterns in the data. We address this by incorporating human-curated\nfeatures into the machine learning model, allowing the system to understand\nthe context of the data. Our approach achieves higher accuracy and\nsignificantly reduces the computational time required for sector-specific\nstrategies compared to traditional machine learning approaches.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.275,
          "p": 0.24719101123595505,
          "f": 0.26035502459997906
        },
        "rouge-2": {
          "r": 0.06315789473684211,
          "p": 0.04838709677419355,
          "f": 0.05479451563562105
        },
        "rouge-l": {
          "r": 0.2625,
          "p": 0.23595505617977527,
          "f": 0.24852070507335192
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.10447v1",
      "true_abstract": "Audio-visual speech recognition (AVSR) has become critical for enhancing\nspeech recognition in noisy environments by integrating both auditory and\nvisual modalities. However, existing AVSR systems struggle to scale up without\ncompromising computational efficiency. In this study, we introduce MoHAVE\n(Mixture of Hierarchical Audio-Visual Experts), a novel robust AVSR framework\ndesigned to address these scalability constraints. By leveraging a\nMixture-of-Experts (MoE) architecture, MoHAVE activates modality-specific\nexpert groups, ensuring dynamic adaptation to various audio-visual inputs with\nminimal computational overhead. Key contributions of MoHAVE include: (1) a\nsparse MoE framework that efficiently scales AVSR model capacity, (2) a\nhierarchical gating mechanism that dynamically utilizes the expert groups based\non input context, enhancing adaptability and robustness, and (3) remarkable\nperformance across robust AVSR benchmarks, including LRS3 and MuAViC\ntranscription and translation tasks, setting a new standard for scalable speech\nrecognition systems.",
      "generated_abstract": "r introduces a novel audio-visual speaker recognition method,\nMoHAVE, which leverages a hierarchy of audio-visual experts to enhance robust\nspeaker recognition. MoHAVE employs a two-stage training process. In the\nfirst stage, MoHAVE employs a three-layer audio encoder, a three-layer visual\nencoder, and a two-layer attention module to extract audio and visual features\nfrom the audio-visual input, respectively. In the second stage, MoHAVE employs\na two-layer audio-visual attention module to fuse the audio and visual\ninformation, and a two-layer speaker embedding module to generate a\ntwo-dimensional speaker embedding, which is then used for speaker recognition.\nThe results show that MoHAVE outperforms state-of-the-art methods, including\nHFCC-MFCC, BHMFCC",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13761467889908258,
          "p": 0.25,
          "f": 0.17751478831973683
        },
        "rouge-2": {
          "r": 0.015037593984962405,
          "p": 0.022988505747126436,
          "f": 0.018181813400414477
        },
        "rouge-l": {
          "r": 0.12844036697247707,
          "p": 0.23333333333333334,
          "f": 0.1656804687931096
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2502.01992v1",
      "true_abstract": "In response to Task II of the FinRL Challenge at ACM ICAIF 2024, this study\nproposes a novel prompt framework for fine-tuning large language models (LLM)\nwith Reinforcement Learning from Market Feedback (RLMF). Our framework\nincorporates market-specific features and short-term price dynamics to generate\nmore precise trading signals. Traditional LLMs, while competent in sentiment\nanalysis, lack contextual alignment for financial market applications. To\nbridge this gap, we fine-tune the LLaMA-3.2-3B-Instruct model using a custom\nRLMF prompt design that integrates historical market data and reward-based\nfeedback. Our evaluation shows that this RLMF-tuned framework outperforms\nbaseline methods in signal consistency and achieving tighter trading outcomes;\nawarded as winner of Task II. You can find the code for this project on GitHub.",
      "generated_abstract": "Contest 2024 was a challenge organized by the FinRL Lab at the\nStanford University to demonstrate the potential of Large Language Models (LLMs)\nin financial markets. The contest asked participants to predict the price of\nseveral stocks using a combination of LLM-engineered signals. The problem was\ndivided into two parts: (1) predicting the price of a single stock and (2)\npredicting the average price of several stocks. The first part is a standard\nfinancial time series prediction problem, while the second is a stock price\nprediction problem with an additional time component. We present our\nfinancial time series prediction approach and highlight its key advantages. We\nalso describe the challenges we faced and how we addressed them. Finally, we\ndiscuss the results of the contest and identify the key takeaways for future\nLLM-based financial applications",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17346938775510204,
          "p": 0.19540229885057472,
          "f": 0.18378377880146107
        },
        "rouge-2": {
          "r": 0.025423728813559324,
          "p": 0.024193548387096774,
          "f": 0.024793383432826634
        },
        "rouge-l": {
          "r": 0.16326530612244897,
          "p": 0.1839080459770115,
          "f": 0.17297296799065026
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.09177v1",
      "true_abstract": "We generalize the notions of composition series and composition factors for\nprofinite groups, and prove a profinite version of the Jordan-Holder Theorem.\nWe apply this to prove a Galois Theorem for infinite prosolvable extensions. In\naddition, we investigate the connection between the abstract and topological\ncomposition factors of a nonstrongly complete profinite group.",
      "generated_abstract": "In this article, we prove the Jordan-Holder type theorem for finite\nprofinite groups and its application to the study of the representation\ndimension of the unitary group of a Hilbert space. In addition, we consider\nthe Jordan-Holder type theorem for finite groups and their application to the\ncharacteristic function of a set of the unitary group of a Hilbert space.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.4,
          "p": 0.4375,
          "f": 0.4179104427712186
        },
        "rouge-2": {
          "r": 0.1,
          "p": 0.11904761904761904,
          "f": 0.10869564721172045
        },
        "rouge-l": {
          "r": 0.37142857142857144,
          "p": 0.40625,
          "f": 0.3880596965025619
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.12638v2",
      "true_abstract": "3D molecule generation is crucial for drug discovery and material design.\nWhile prior efforts focus on 3D diffusion models for their benefits in modeling\ncontinuous 3D conformers, they overlook the advantages of 1D SELFIES-based\nLanguage Models (LMs), which can generate 100% valid molecules and leverage the\nbillion-scale 1D molecule datasets. To combine these advantages for 3D molecule\ngeneration, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D\nLanguage Modeling for 3D Molecule Generation. NExT-Mol uses an extensively\npretrained molecule LM for 1D molecule generation, and subsequently predicts\nthe generated molecule's 3D conformers with a 3D diffusion model. We enhance\nNExT-Mol's performance by scaling up the LM's model size, refining the\ndiffusion neural architecture, and applying 1D to 3D transfer learning.\nNotably, our 1D molecule LM significantly outperforms baselines in\ndistributional similarity while ensuring validity, and our 3D diffusion model\nachieves leading performances in conformer prediction. Given these improvements\nin 1D and 3D modeling, NExT-Mol achieves a 26% relative improvement in 3D FCD\nfor de novo 3D generation on GEOM-DRUGS, and a 13% average relative gain for\nconditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are\navailable at https://github.com/acharkq/NExT-Mol.",
      "generated_abstract": "design is crucial in drug discovery and chemical research, yet\nexisting approaches are limited by their reliance on outdated 2D structures and\ntext-based chemical descriptions. To address these limitations, we introduce\nNExT-Mol, a new model that uses 1D language models to generate 3D molecular\nstructures. Unlike previous 1D language models, which generate 2D structures\ndirectly, NExT-Mol first converts 1D structures into 3D molecular clouds and\nthen uses diffusion models to reconstruct the cloud into a 3D structure. The\nmodel leverages 3D diffusion maps to guide the generation of molecular\nstructures, ensuring that the generated molecules satisfy the input\ndescriptions while respecting the 3D structure constraints. Additionally, we\nintroduce a new 3D-to-1D language modeling framework that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.264,
          "p": 0.42857142857142855,
          "f": 0.326732668549652
        },
        "rouge-2": {
          "r": 0.03867403314917127,
          "p": 0.06542056074766354,
          "f": 0.048611106441213796
        },
        "rouge-l": {
          "r": 0.224,
          "p": 0.36363636363636365,
          "f": 0.27722771805460256
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.02383v1",
      "true_abstract": "This paper investigates strategic investments needed to mitigate transition\nrisks, particularly focusing on sectors significantly impacted by the shift to\na low-carbon economy. It emphasizes the importance of tailored sector-specific\nstrategies and the role of government interventions, such as carbon taxes and\nsubsidies, in shaping corporate behavior. In providing a multi-period\nframework, this paper evaluates the economic and operational trade-offs\ncompanies face under four various decarbonization scenarios: immediate, quick,\nslow, and no transitions. The analysis provides practical insights for both\npolicymakers and business leaders, demonstrating how regulatory frameworks and\nstrategic investments can be aligned to manage transition risks while\noptimizing long-term sustainability effectively. The findings contribute to a\ndeeper understanding of the economic impacts of regulatory policies and offer a\ncomprehensive framework to navigate the complexities of transitioning to a\nlow-carbon economy.",
      "generated_abstract": "r examines the role of strategic investment in mitigating\ntransition risks associated with rapid technological change. We introduce\ntransition risks as a new concept, and argue that these risks arise from\ntransition processes in both technologies and markets, and that they may be\nmitigated by investing in technologies that are more likely to succeed than\ntheir competitors. We develop a dynamic model of technology adoption and\nmarket integration to evaluate the effects of strategic investment on\ntransition risks. Our results show that strategic investment can significantly\nreduce transition risks, particularly in technologies that are more likely to\nsuccessfully mature. We find that strategic investment is particularly\neffective in mitigating transition risks in industries that face high levels of\ntechnological uncertainty, such as electric vehicles. Our findings emphasize the\nimportance of strategic investment in mitigating",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23232323232323232,
          "p": 0.2875,
          "f": 0.25698323527979783
        },
        "rouge-2": {
          "r": 0.064,
          "p": 0.07079646017699115,
          "f": 0.06722688576901385
        },
        "rouge-l": {
          "r": 0.21212121212121213,
          "p": 0.2625,
          "f": 0.23463686656471408
        }
      }
    },
    {
      "paper_id": "physics.ins-det.physics/ins-det/2503.10383v1",
      "true_abstract": "In the DEAP-3600 dark matter search experiment, precise reconstruction of the\npositions of scattering events in liquid argon is key for background rejection\nand defining a fiducial volume that enhances dark matter candidate events\nidentification. This paper describes three distinct position reconstruction\nalgorithms employed by DEAP-3600, leveraging the spatial and temporal\ninformation provided by photomultipliers surrounding a spherical liquid argon\nvessel. Two of these methods are maximum-likelihood algorithms: the first uses\nthe spatial distribution of detected photoelectrons, while the second\nincorporates timing information from the detected scintillation light.\nAdditionally, a machine learning approach based on the pattern of photoelectron\ncounts across the photomultipliers is explored.",
      "generated_abstract": "Matter Search experiment (DEAP-3600) is an upcoming dark matter\ninvestigation that aims to search for dark matter particles using cosmic-ray\nneutralino (CN) annihilation in the atmosphere. To reconstruct the position of\nCN particles, a calibration technique that uses the energy deposited by CN\nparticles in the detector is proposed. The reconstruction algorithm is\nimplemented in the event generator and used to perform a Monte-Carlo (MC)\nsimulation of the data collection and the reconstruction process. The algorithm\nis tested on simulated data, and the results are compared with the position\nreconstruction obtained by the particle-tracking method. The MC simulation and\nthe particle-tracking method show that the reconstruction algorithm has a\nhigher precision compared to the particle-tracking method. The MC simulation\nshows that the reconstruction algorithm has a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21794871794871795,
          "p": 0.23943661971830985,
          "f": 0.2281879144741229
        },
        "rouge-2": {
          "r": 0.039603960396039604,
          "p": 0.038461538461538464,
          "f": 0.03902438524497387
        },
        "rouge-l": {
          "r": 0.1794871794871795,
          "p": 0.19718309859154928,
          "f": 0.187919458098284
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.03312v1",
      "true_abstract": "In this paper, we conduct a large-scale field experiment to investigate the\nmanipulability of prediction markets. The main experiment involves randomly\nshocking prices across 817 separate markets; we then collect hourly price data\nto examine whether the effects of these shocks persist over time. We find that\nprediction markets can be manipulated: the effects of our trades are visible\neven 60 days after they have occurred. However, as predicted by our model, the\neffects of the manipulations somewhat fade over time. Markets with more\ntraders, greater trading volume, and an external source of probability\nestimates are harder to manipulate.",
      "generated_abstract": "We show that the manipulability of prediction markets can be quantified by\ntheir price of information. This price of information is a lower bound on\nmanipulability and is a function of the number of participants, the market's\ninformation content and the market's information processing capacity. We further\nshow that the manipulability of prediction markets can be controlled by a\nsingle manipulable parameter, the number of participants. Our results provide\ninsight into the manipulability of prediction markets and open new avenues for\nfurther research.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16455696202531644,
          "p": 0.28888888888888886,
          "f": 0.2096774147307493
        },
        "rouge-2": {
          "r": 0.07608695652173914,
          "p": 0.1111111111111111,
          "f": 0.09032257582018756
        },
        "rouge-l": {
          "r": 0.1518987341772152,
          "p": 0.26666666666666666,
          "f": 0.19354838247268483
        }
      }
    },
    {
      "paper_id": "math.RT.math/QA/2503.10394v1",
      "true_abstract": "This article investigates the two-parameter quantum matrix algebra at roots\nof unity. In the roots of unity setting, this algebra becomes a Polynomial\nIdentity (PI) algebra and it is known that simple modules over such algebra are\nfinite-dimensional with dimension at most the PI degree. We determine the\ncenter, compute the PI degree, and classify simple modules for two-parameter\nquantum matrix algebra, up to isomorphism, over an algebraically closed field\nof arbitrary characteristics.",
      "generated_abstract": "In this work, we prove the existence of simple modules for the quantum\nmatrix algebra over the field of complex numbers, over the field of complex\nnumbers with a non-trivial element and over the field of complex numbers with\nall elements in the unit circle. The main tools for proving the existence of\nsimple modules are the results of the main authors for the quantum group\nalgebra over the complex numbers. Our proof is based on the method of\ntransferring the results of the main authors to the context of the quantum\nmatrix algebra. The main new result of the paper is the proof of the existence\nof simple modules over the quantum matrix algebra at roots of unity.\nMoreover, we prove the existence of simple modules over the quantum matrix\nalgebra for the case of the field of complex numbers with a non-trivial\nelement.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.39622641509433965,
          "p": 0.3888888888888889,
          "f": 0.3925233594864181
        },
        "rouge-2": {
          "r": 0.15151515151515152,
          "p": 0.11904761904761904,
          "f": 0.13333332840533352
        },
        "rouge-l": {
          "r": 0.37735849056603776,
          "p": 0.37037037037037035,
          "f": 0.37383177070137136
        }
      }
    },
    {
      "paper_id": "math.RA.math/RA/2503.05337v2",
      "true_abstract": "We classify all two-dimensional simple algebras over an algebraically closed\nfield. For each two-dimensional algebra $\\mathcal{A}$ with an infinite group of\nautomorphisms we describe a minimal (with respect to inclusion) generating set\nfor the algebra of invariants of the $m$-tuples of $\\mathcal{A}$ in\ncharacteristic zero case. As a consequence, we show that in characteristic zero\ncase Artin-Procesi-Iltyakov Equality holds for all two-dimensional simple\nalgebras with an infinite group of automorphisms. We also consider\nnondegenerate invariant bilinear forms over two-dimensional algebras.",
      "generated_abstract": "In this paper, we provide a systematic approach to computing the\npolynomial invariants of two-dimensional (2D) polynomial algebras. We also\nintroduce a novel class of 2D polynomial algebras which can be viewed as the\nquantum counterpart of classical polynomial algebras. We discuss some\napplications of these invariants to quantum algorithms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19607843137254902,
          "p": 0.2777777777777778,
          "f": 0.22988505261989708
        },
        "rouge-2": {
          "r": 0.029411764705882353,
          "p": 0.043478260869565216,
          "f": 0.03508771448445741
        },
        "rouge-l": {
          "r": 0.19607843137254902,
          "p": 0.2777777777777778,
          "f": 0.22988505261989708
        }
      }
    },
    {
      "paper_id": "econ.EM.q-fin/ST/2502.15458v1",
      "true_abstract": "Network connections, both across and within markets, are central in countless\neconomic contexts. In recent decades, a large literature has developed and\napplied flexible methods for measuring network connectedness and its evolution,\nbased on variance decompositions from vector autoregressions (VARs), as in\nDiebold and Yilmaz (2014). Those VARs are, however, typically identified using\nfull orthogonalization (Sims, 1980), or no orthogonalization (Koop, Pesaran,\nand Potter, 1996; Pesaran and Shin, 1998), which, although useful, are special\nand extreme cases of a more general framework that we develop in this paper. In\nparticular, we allow network nodes to be connected in \"clusters\", such as asset\nclasses, industries, regions, etc., where shocks are orthogonal across clusters\n(Sims style orthogonalized identification) but correlated within clusters\n(Koop-Pesaran-Potter-Shin style generalized identification), so that the\nordering of network nodes is relevant across clusters but irrelevant within\nclusters. After developing the clustered connectedness framework, we apply it\nin a detailed empirical exploration of sixteen country equity markets spanning\nthree global regions.",
      "generated_abstract": "uce a novel measure of network connectedness that is based on\nclustered network connectivity. The framework is derived from a theoretical\nframework that we apply to global equity markets. The new measure, termed\nclustered network connectedness, is based on a clustered network connectivity\nmeasure that we derive from a theoretical framework. We apply clustered\nnetwork connectedness to the global equity markets and show that it captures\nimportant features of the network structure. Our results show that the\nclustered network connectedness captures important features of the network\nstructure, including the degree distribution, the power law, the clustering\ncoefficient, and the correlation structure. Our results also show that the\nclustered network connectedness captures important features of the network\nstructure, including the degree distribution, the power law, the clustering\ncoefficient, and the correlation structure. The results of our study are\nimportant for understanding the network structure of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1732283464566929,
          "p": 0.4,
          "f": 0.24175823754075598
        },
        "rouge-2": {
          "r": 0.05660377358490566,
          "p": 0.09782608695652174,
          "f": 0.07171314276662305
        },
        "rouge-l": {
          "r": 0.15748031496062992,
          "p": 0.36363636363636365,
          "f": 0.219780215562734
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.03026v2",
      "true_abstract": "When can interventions in markets be designed to increase surplus robustly --\ni.e., with high probability -- accounting for uncertainty due to imprecise\ninformation about economic primitives? In a setting with many strategic firms,\neach possessing some market power, we present conditions for such interventions\nto exist. The key condition, recoverable structure, requires large-scale\ncomplementarities among families of products. The analysis works by decomposing\nthe incidence of interventions in terms of principal components of a Slutsky\nmatrix. Under recoverable structure, a noisy signal of this matrix reveals\nenough about these principal components to design robust interventions. Our\nresults demonstrate the usefulness of spectral methods for analyzing\nimperfectly observed strategic interactions with many agents.",
      "generated_abstract": "This paper considers the design of a market intervention to maximize\nthe net present value of the social benefits of the market. The market\nintervention consists of a monopolist selling its output to a public\nutility maximizing firm, which sells its output to the public in a competitive\nmarket. We show that the optimal intervention depends on the market size and\nutility function, and we characterize the optimal intervention in terms of\nmarket size. We also characterize the price at which the public utility\nintervenes, which depends on the market size and the utility function.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12790697674418605,
          "p": 0.23404255319148937,
          "f": 0.1654135292645148
        },
        "rouge-2": {
          "r": 0.02727272727272727,
          "p": 0.03896103896103896,
          "f": 0.03208555665303628
        },
        "rouge-l": {
          "r": 0.10465116279069768,
          "p": 0.19148936170212766,
          "f": 0.13533834129459
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.05601v2",
      "true_abstract": "This paper proposes a Matrix Error Correction Model to identify cointegration\nrelations in matrix-valued time series. We hereby allow separate cointegrating\nrelations along the rows and columns of the matrix-valued time series and use\ninformation criteria to select the cointegration ranks. Through Monte Carlo\nsimulations and a macroeconomic application, we demonstrate that our approach\nprovides a reliable estimation of the number of cointegrating relationships.",
      "generated_abstract": "This paper studies the identification of cointegration relationships in\nmatrix-valued time series. We propose a novel methodology that combines\ncointegration testing with cross-sectional covariance estimation, in order to\nidentify the underlying time series cointegration relationships. The methodology\nis particularly effective when the number of time series is large and when the\ndata are high-dimensional, especially if the time series are non-stationary.\nOur proposed methodology is general and can be used for time series with\nany number of time series, including time series with multiple time series. We\nalso study the statistical properties of the proposed methodology and provide\nsimulation studies to assess its performance. Finally, we apply the methodology\nto a dataset of time series of the U.S. dollar-yen exchange rate.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.375,
          "p": 0.2535211267605634,
          "f": 0.302521003590142
        },
        "rouge-2": {
          "r": 0.15,
          "p": 0.08653846153846154,
          "f": 0.10975609292088062
        },
        "rouge-l": {
          "r": 0.3541666666666667,
          "p": 0.23943661971830985,
          "f": 0.2857142809010664
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.10431v1",
      "true_abstract": "Deep learning methods for point tracking are applicable in 2D\nechocardiography, but do not yet take advantage of domain specifics that enable\nextremely fast and efficient configurations. We developed MyoTracker, a\nlow-complexity architecture (0.3M parameters) for point tracking in\nechocardiography. It builds on the CoTracker2 architecture by simplifying its\ncomponents and extending the temporal context to provide point predictions for\nthe entire sequence in a single step. We applied MyoTracker to the right\nventricular (RV) myocardium in RV-focused recordings and compared the results\nwith those of CoTracker2 and EchoTracker, another specialized point tracking\narchitecture for echocardiography. MyoTracker achieved the lowest average point\ntrajectory error at 2.00 $\\pm$ 0.53 mm. Calculating RV Free Wall Strain (RV\nFWS) using MyoTracker's point predictions resulted in a -0.3$\\%$ bias with\n95$\\%$ limits of agreement from -6.1$\\%$ to 5.4$\\%$ compared to reference\nvalues from commercial software. This range falls within the interobserver\nvariability reported in previous studies. The limits of agreement were wider\nfor both CoTracker2 and EchoTracker, worse than the interobserver variability.\nAt inference, MyoTracker used 67$\\%$ less GPU memory than CoTracker2 and 84$\\%$\nless than EchoTracker on large sequences (100 frames). MyoTracker was 74 times\nfaster during inference than CoTracker2 and 11 times faster than EchoTracker\nwith our setup. Maintaining the entire sequence in the temporal context was the\ngreatest contributor to MyoTracker's accuracy. Slight additional gains can be\nmade by re-enabling iterative refinement, at the cost of longer processing\ntime.",
      "generated_abstract": "rdium is a complex anatomical region that undergoes rapid\nchanges during cardiac contraction, requiring precise tracking to ensure\naccurate cardiac physiology assessment. Traditional deep learning-based methods\nexhibit high computational complexity and limited interpretability. To address\nthese challenges, we propose a low-complexity point tracking method for the\nmyocardium in 2D echocardiography. Our approach leverages a sparse convolution\nlayer to reduce the number of feature channels while maintaining\ninterpretability, reducing the computational complexity by a factor of 100. We\nalso introduce a novel attention mechanism that effectively captures the\ncontour shape and structure of the myocardium. Our method achieves high\naccuracy in tracking the myocardium with a mean error of 0.066 mm. Our results\ndemonstrate that our method is compet",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15432098765432098,
          "p": 0.2840909090909091,
          "f": 0.1999999954380801
        },
        "rouge-2": {
          "r": 0.02252252252252252,
          "p": 0.043478260869565216,
          "f": 0.029673586008506527
        },
        "rouge-l": {
          "r": 0.13580246913580246,
          "p": 0.25,
          "f": 0.17599999543808012
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-lat/2503.08847v1",
      "true_abstract": "The concept of nucleon radii plays a central role in our understanding of the\ninternal structure of protons and neutrons, providing critical insights into\nthe non-perturbative regime of quantum chromodynamics (QCD). While the charge\nradius is often interpreted as the ``size\" of the nucleon, this interpretation\nis an oversimplification that overlooks the multifaceted nature of nucleon\nstructure. This paper provides a comprehensive overview of the different\nnucleon radii, including the charge and magnetic radii, the axial radius, and\nthe emerging concepts of mechanical and mass radii. We discuss the definitions\nas well as the experimental, theoretical and phenomenological determinations of\nthese radii, highlighting their distinct physical origins and implications. By\nsynthesizing recent experimental results and theoretical advancements, we\nemphasize that each radius reflects a specific aspect of the nucleon's internal\nstructure, such as its electric charge distribution, magnetic properties, weak\ninteractions, or internal mechanical stress. In particular, we address the\ncommon but misleading interpretation of the proton radius as a simple measure\nof its size, underscoring the nuanced and context-dependent nature of nucleon\nradii. Through this exploration, we aim to clarify the roles of these radii in\ncharacterizing nucleon structure and to identify open questions that remain to\nbe addressed. This work contributes to a deeper understanding of the nucleon\nand its significance in the broader context of particle and nuclear physics.",
      "generated_abstract": "of the nucleon is an important factor in the description of hadron\ndistributions and the properties of the nucleon in hadronic matter. In this\npaper, we present a comprehensive study of the size of the nucleon using the\nDyson-Schwinger equation (DSE) approach and the light-front formalism. We\nevaluate the hadronic and nuclear radii, the effective charge, the electromagnetic\ncoupling, and the chiral condensate using the mass of the nucleon as a\ndeterminant. Our results reveal that the nucleon size depends on the choice of\nthe boundary conditions, the form factor, and the wave function, but the size\nis always large. The electromagnetic coupling is larger than the\nquark-antiquark coupling, but the electromagnetic coupling is not always\nlarger than the chiral condensate. These results indicate that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1590909090909091,
          "p": 0.3088235294117647,
          "f": 0.2099999955120001
        },
        "rouge-2": {
          "r": 0.043478260869565216,
          "p": 0.08823529411764706,
          "f": 0.05825242276180636
        },
        "rouge-l": {
          "r": 0.15151515151515152,
          "p": 0.29411764705882354,
          "f": 0.1999999955120001
        }
      }
    },
    {
      "paper_id": "math.RT.math/RT/2503.06607v1",
      "true_abstract": "We prove that any complex local representation of the flat virtual braid\ngroup, $FVB_2$, into $GL_2(\\mathbb{C})$, for $n\\geq 2$, has one of the types\n$\\lambda_i: FVB_2 \\rightarrow GL_2(\\mathbb{C})$, $1\\leq i\\leq 12$. We find\nnecessary and sufficient conditions that guarantee the irreducibility of\nrepresentations of type $\\lambda_i$, $1\\leq i\\leq 5$, and we prove that\nrepresentations of type $\\lambda_i$, $6\\leq i\\leq 12$, are reducible. Regarding\nfaithfulness, we find necessary and sufficient conditions for representations\nof type $\\lambda_6$ or $\\lambda_7$ to be faithful. Moreover, we give sufficient\nconditions for representations of type $\\lambda_1$, $\\lambda_2$, or $\\lambda_4$\nto be unfaithful, and we show that representations of type $\\lambda_i$, $i=3,\n5, 8, 9, 10, 11, 12$ are unfaithful. We prove that any complex homogeneous\nlocal representations of the flat virtual braid group, $FVB_n$, into\n$GL_{n}(\\mathbb{C})$, for $n\\geq 2$, has one of the types $\\gamma_i: FVB_n\n\\rightarrow GL_n(\\mathbb{C})$, $i=1, 2$. We then prove that representations of\ntype $\\gamma_1: FVB_n \\rightarrow GL_n(\\mathbb{C})$ are reducible for $n\\geq\n6$, while representations of type $\\gamma_2: FVB_n \\rightarrow\nGL_n(\\mathbb{C})$ are reducible for $n\\geq 3$. Then, we show that\nrepresentations of type $\\gamma_1$ are unfaithful for $n\\geq 3$ and that\nrepresentations of type $\\gamma_2$ are unfaithful if $y=b$. Furthermore, we\nprove that any complex homogeneous local representation of the flat virtual\nbraid group, $FVB_n$, into $GL_{n+1}(\\mathbb{C})$, for all $n\\geq 4$, has one\nof the types $\\delta_i: FVB_n \\rightarrow GL_{n+1}(\\mathbb{C})$, $1\\leq i\\leq\n8$. We prove that these representations are reducible for $n\\geq 10$. Then, we\nshow that representations of types $\\delta_i$, $i\\neq 5, 6$, are unfaithful,\nwhile representations of types $\\delta_5$ or $\\delta_6$ are unfaithful if\n$x=y$.",
      "generated_abstract": "virtual Braid group is the semidirect product of the flat\ngenerating set with the universal cover of the flat Braid group. We give an\nexplicit description of its representation ring as an affine scheme over the\nalgebraic group of all flat representations of the flat Braid group. This\ndescription is obtained as the quotient of the affine scheme of all flat\nrepresentations of the flat Braid group by a certain finite group of automorphisms\nwhich is in fact a finite subgroup of the universal cover of the flat Braid\ngroup. This finite group is in fact a finite quotient of the universal cover of\nthe flat Braid group, and we show that it is in fact a finite quotient of the\nuniversal cover of the flat Braid group. The description of the representation\nring is a consequence of the description of the universal cover of the flat\nBraid group as a semidirect product of a flat group",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13861386138613863,
          "p": 0.2857142857142857,
          "f": 0.18666666226755566
        },
        "rouge-2": {
          "r": 0.0375,
          "p": 0.06976744186046512,
          "f": 0.04878048325732081
        },
        "rouge-l": {
          "r": 0.12871287128712872,
          "p": 0.2653061224489796,
          "f": 0.17333332893422235
        }
      }
    },
    {
      "paper_id": "math.ST.math/MP/2503.08808v1",
      "true_abstract": "We consider two random variables $X$ and $Y$ following correlated Gamma\ndistributions, characterized by identical scale and shape parameters and a\nlinear correlation coefficient $\\rho$. Our focus is on the parameter: \\[\n  D(X,Y) = \\frac{|X - Y|}{X + Y}, \\] which appears in applied contexts such as\ndynamic speckle imaging, where it is known as the \\textit{Fujii index}. In this\nwork, we derive a closed-form expression for the probability density function\nof $D(X,Y)$ as well as analytical formulas for its moments of order $k$. Our\nderivation starts by representing $X$ and $Y$ as two correlated exponential\nrandom variables, obtained from the squared magnitudes of circular complex\nGaussian variables. By considering the sum of $k$ independent exponential\nvariables, we then derive the joint density of $(X,Y)$ when $X$ and $Y$ are two\ncorrelated Gamma variables. Through appropriate varable transformations, we\nobtain the theoretical distribution of $D(X,Y)$ and evaluate its moments\nanalytically. These theoretical findings are validated through numerical\nsimulations, with particular attention to two specific cases: zero correlation\nand unit shape parameter.",
      "generated_abstract": "The present work is devoted to the study of the distribution of the\ncomplementary mean of the dissimilarity ratio of two correlated Gamma variables\nand its moment generating function. We provide explicit formulas for the\ndistribution of the complementary mean of the dissimilarity ratio and the\nmoments of its density function, which are in terms of the incomplete beta\nfunction. Our formulas are based on the properties of the incomplete beta\nfunction and the Gamma distribution, which are in turn obtained from the\nproperties of the Laplace transform of the Gamma distribution. We also derive\nthe distribution of the moments of the density function of the complementary\nmean of the dissimilarity ratio and the moment generating function of the\ndensity function of the complementary mean of the dissimilarity ratio.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.211864406779661,
          "p": 0.5102040816326531,
          "f": 0.29940119345835275
        },
        "rouge-2": {
          "r": 0.08024691358024691,
          "p": 0.17333333333333334,
          "f": 0.10970463702398137
        },
        "rouge-l": {
          "r": 0.2033898305084746,
          "p": 0.4897959183673469,
          "f": 0.2874251455541612
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2411.16373v1",
      "true_abstract": "Slow-fast dynamics are intrinsically related to complex phenomena, and are\nresponsible for many of the homeostatic dynamics that keep biological systems\nhealthfully functioning. We study a discrete-time membrane potential model that\ncan generate a diverse set of spiking behavior depending on the choice of\nslow-fast time scales, from fast spiking to bursting, or plateau action\npotentials -- also known as cardiac spikes, since they are characteristic in\nheart myocytes. The plateau of cardiac spikes may lose stability, generating\nearly or delayed afterdepolarizations (EAD and DAD, respectively), both of\nwhich are related to cardiac arrhythmia. We show the periodicity changes along\nthe transition from the healthy action potentials to these impaired spikes. We\nshow that while EADs are mainly periodic attractors, DAD usually comes with\nchaos. EADs are found inside shrimps -- isoperiodic structures of the parameter\nspace. However, in our system, the shrimps have an internal structure made of\nmultiple periodicities, revealing a complete devil's staircase. Understanding\nthe periodicity of plateau attractors in slow-fast systems could come in handy\nto unveil the features of heart myocytes behavior that are linked to cardiac\narrhythmias.",
      "generated_abstract": "ability of neurons is influenced by the spiking activity of\npotential neighbours. The mechanism underlying this phenomenon remains\nunknown. In this work, we investigate the excitability of the plateau spike\n(PS) burst in shrimp, which is a neuron model with a spiking activity\ncorresponding to a plateau-like response. We show that the excitability of\nshrimp PS neurons is determined by their intrinsic properties. We first show\nthat the spiking activity of PS neurons is periodic and can be divided into\nplateau spikes (PS) and bursts (PB) by their firing times. We then propose a\ntheoretical model to explain the periodicity of PS spikes, which is based on\nthe idea that the PS spike duration is related to the duration of a period of\ntheir neighbours. The model predicts that the exc",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1885245901639344,
          "p": 0.3150684931506849,
          "f": 0.23589743121314927
        },
        "rouge-2": {
          "r": 0.040229885057471264,
          "p": 0.061946902654867256,
          "f": 0.04878048303075233
        },
        "rouge-l": {
          "r": 0.16393442622950818,
          "p": 0.273972602739726,
          "f": 0.2051282004439186
        }
      }
    },
    {
      "paper_id": "math.OA.math/OA/2503.10505v1",
      "true_abstract": "We compute the $K_1$-group of ultraproducts of unital, simple $C^*$-algebras\nwith unique trace and strict comparison. As an application, we prove that the\nreduced free group $C^*$-algebras $C^*_r(F_m)$ and $C^*_r(F_n)$ are\nelementarily equivalent (i.e., have isomorphic ultrapowers) if and only if $m =\nn$. This settles in the negative the $C^*$-algebraic analogue of Tarski's 1945\nproblem for groups.",
      "generated_abstract": "e an answer to the Tarski problem, i.e., the problem of classifying\nthe algebras of all formulas of the form $\\varphi(x_1,\\dots,x_n) = 0$ with\n$\\varphi(x_1,\\dots,x_n)$ of the form $x_i \\equiv \\neg x_j$ for $i \\neq j$. The\nanswer is negative: There exist a unital $C^*$-algebra $A$ with $\\varphi(x_1,\\dots,x_n) = 0$\nfor all formulas of the form $\\varphi(x_1,\\dots,x_n) = 0$ with $\\varphi(x_1,\\dots,x_n)$\nof the form $x_i \\equiv \\neg x_j$ for $i \\neq j$ such that $A$ is not\n$C^*$-isomorphic to $A_n",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19607843137254902,
          "p": 0.23255813953488372,
          "f": 0.2127659524830241
        },
        "rouge-2": {
          "r": 0.01694915254237288,
          "p": 0.017543859649122806,
          "f": 0.0172413743118326
        },
        "rouge-l": {
          "r": 0.1568627450980392,
          "p": 0.18604651162790697,
          "f": 0.17021276099366245
        }
      }
    },
    {
      "paper_id": "cs.CE.econ/GN/2503.02692v1",
      "true_abstract": "To improve stock trend predictions and support personalized investment\ndecisions, this paper proposes FinArena, a novel Human-Agent collaboration\nframework. Inspired by the mixture of experts (MoE) approach, FinArena combines\nmultimodal financial data analysis with user interaction. The human module\nfeatures an interactive interface that captures individual risk preferences,\nallowing personalized investment strategies. The machine module utilizes a\nLarge Language Model-based (LLM-based) multi-agent system to integrate diverse\ndata sources, such as stock prices, news articles, and financial statements. To\naddress hallucinations in LLMs, FinArena employs the adaptive\nRetrieval-Augmented Generative (RAG) method for processing unstructured news\ndata. Finally, a universal expert agent makes investment decisions based on the\nfeatures extracted from multimodal data and investors' individual risk\npreferences. Extensive experiments show that FinArena surpasses both\ntraditional and state-of-the-art benchmarks in stock trend prediction and\nyields promising results in trading simulations across various risk profiles.\nThese findings highlight FinArena's potential to enhance investment outcomes by\naligning strategic insights with personalized risk considerations.",
      "generated_abstract": "Financial markets are highly dynamic and complex, posing significant\nrisks for investors. Traditional approaches, such as data-driven models and\nautomated trading systems, struggle to capture the nuances of market\ninteractions, leading to inaccurate forecasts. This paper presents FinArena, a\nhuman-agent collaboration framework designed to overcome these challenges.\nFinArena integrates multiple agents, including human experts and advanced\nAI models, to enhance trading decisions and mitigate risk. By integrating\nhuman insights and advanced AI techniques, FinArena addresses the limitations\nof traditional models and delivers more accurate predictions. By combining\nhuman expertise with advanced AI technologies, FinArena offers a scalable and\nreliable solution for financial markets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19834710743801653,
          "p": 0.3076923076923077,
          "f": 0.24120602538420755
        },
        "rouge-2": {
          "r": 0.025806451612903226,
          "p": 0.04081632653061224,
          "f": 0.03162054861347694
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.28205128205128205,
          "f": 0.22110552287164476
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.astro-ph/CO/2503.10361v1",
      "true_abstract": "We study the stochastic gravitational wave background sourced by a network of\ncosmic superstrings and demonstrate that incorporating higher-mass string\nspecies, beyond the fundamental string, is crucial for accurately modeling the\nresulting gravitational wave spectrum across frequencies ranging from nanohertz\nto kilohertz. Using the multi-tension velocity-dependent one-scale model to\nevolve the cosmic superstring network, we perform several fits to the NANOGrav\n15-year dataset and obtain expectation values for the fundamental string\ntension, string coupling and effective size of compact extra dimensions. We\nfind that the cosmic superstring best-fits are comparable in likelihood to\nSupermassive Black Hole models, thought by many to be the leading candidate\nexplanation of the signal. The implications of the best-fit spectra are\ndiscussed within the context of future gravitational wave experiments. We\nobtain expectation values for the fundamental string tension of\n$\\log_{10}(G\\mu_1)=-11.5^{+0.3}_{-0.3}$($-11.6^{+0.2}_{-0.3}$) for\ngravitational waves originating from large cuspy (kinky) cosmic superstring\nloops and $\\log_{10}(G\\mu_1)=-9.7^{+0.7}_{-0.7}$($-9.9^{+1.0}_{-0.5}$) for\nsmall cuspy (kinky) loops. We also place $2\\sigma$ upper bounds on the string\ncoupling, finding $g_s<0.65$ in all cases, and comment on the implication of\nour results for the effective size of the compact extra dimensions.",
      "generated_abstract": "t a stochastic gravitational wave (GW) background model based on\nthe cosmic superstring (CS) scenario. The CS model is an extension of the\nstandard model of cosmology and describes the formation of the universe through\nthe spontaneous creation of string-like particles. In our model, the GW signal\nis produced by the decay of these string-like particles. The CS background is\nconstructed using the GW data from the LIGO-Virgo-KAGRA observational\nprograms. We estimate the energy spectrum of the CS background and compare it\nwith the LIGO-Virgo-KAGRA observation results. Our results show that the CS\nbackground is consistent with the LIGO-Virgo-KAGRA observation results, with\na spectral index of $-0.73^{+0.18}_{-0.14}$, which is consistent with the\nexpectation from",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18110236220472442,
          "p": 0.32857142857142857,
          "f": 0.23350253348965452
        },
        "rouge-2": {
          "r": 0.03954802259887006,
          "p": 0.07,
          "f": 0.05054151163184758
        },
        "rouge-l": {
          "r": 0.15748031496062992,
          "p": 0.2857142857142857,
          "f": 0.2030456806977764
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/SE/2503.10407v1",
      "true_abstract": "The cloud computing model enables the on-demand provisioning of computing\nresources, reducing manual management, increasing efficiency, and improving\nenvironmental impact. Software architects now play a strategic role in\ndesigning and deploying elasticity policies for automated resource management.\nHowever, creating policies that meet performance and cost objectives is\ncomplex. Existing approaches, often relying on formal models like Queueing\nTheory, require advanced skills and lack specific methods for representing\nelasticity within architectural models. This paper introduces an architectural\nview type for modeling and simulating elasticity, supported by the Scaling\nPolicy Definition (SPD) modeling language, a visual notation, and precise\nsimulation semantics. The view type is integrated into the Palladio ecosystem,\nproviding both conceptual and tool-based support. We evaluate the approach\nthrough two single-case experiments and a user study. In the first experiment,\nsimulations of elasticity policies demonstrate sufficient accuracy when\ncompared to load tests, showing the utility of simulations for evaluating\nelasticity. The second experiment confirms feasibility for larger applications,\nthough with increased simulation times. The user study shows that participants\ncompleted 90% of tasks, rated the usability at 71%, and achieved an average\nscore of 76% in nearly half the allocated time. However, the empirical evidence\nsuggests that modeling with this architectural view requires more time than\nmodeling control flow, resource environments, or usage profiles, despite its\nbenefits for elasticity policy design and evaluation.",
      "generated_abstract": "The increasing demands on cloud and edge computing infrastructures are\nsignificantly impacting the efficiency of applications, making it more\nimportant than ever to improve resource management. This research proposes a\nmodular approach to modeling and simulating elasticity, which can be used to\nanalyze the effect of resource demand on application performance. The\napproach is based on the slingshot model, which allows for an accurate and\nrobust representation of the application behavior. The slingshot model is\nexplained in this paper, followed by a case study that demonstrates its\neffectiveness in simulating the impact of resource demand on application\nperformance. This study is an important step toward advancing the development of\nefficient and scalable elasticity-based applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19631901840490798,
          "p": 0.4155844155844156,
          "f": 0.26666666230868064
        },
        "rouge-2": {
          "r": 0.01834862385321101,
          "p": 0.037037037037037035,
          "f": 0.02453987286988676
        },
        "rouge-l": {
          "r": 0.1901840490797546,
          "p": 0.4025974025974026,
          "f": 0.2583333289753473
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SY/2503.03151v1",
      "true_abstract": "Subset selection is central to many wireless communication problems,\nincluding link scheduling, power allocation, and spectrum management. However,\nthese problems are often NP-complete, because of which heuristic algorithms\napplied to solve these problems struggle with scalability in large-scale\nsettings. To address this, we propose a determinantal point process-based\nlearning (DPPL) framework for efficiently solving general subset selection\nproblems in massive networks. The key idea is to model the optimal subset as a\nrealization of a determinantal point process (DPP), which balances the\ntrade-off between quality (signal strength) and similarity (mutual\ninterference) by enforcing negative correlation in the selection of {\\em\nsimilar} links (those that create significant mutual interference). However,\nconventional methods for constructing similarity matrices in DPP impose\ndecomposability and symmetry constraints that often do not hold in practice. To\novercome this, we introduce a new method based on the Gershgorin Circle Theorem\nfor constructing valid similarity matrices. The effectiveness of the proposed\napproach is demonstrated by applying it to two canonical wireless network\nsettings: an ad hoc network in 2D and a cellular network serving drones in 3D.\nSimulation results show that DPPL selects near-optimal subsets that maximize\nnetwork sum-rate while significantly reducing computational complexity compared\nto traditional optimization methods, demonstrating its scalability for\nlarge-scale networks.",
      "generated_abstract": "er the problem of selecting a subset of transmitters for a\nnetwork with $N$ nodes subject to a minimum transmission rate constraint.\nExisting learning methods either assume a priori knowledge of the network\nstructure or rely on approximations of the network structure, which may\nhinder their practical implementation. In this paper, we propose a\ndeterminantal point process (DPP)-based approach for subset selection. Our\nframework utilizes a novel DPP-based formulation to learn the network\nstructure. Additionally, we introduce a novel and practical algorithm for\nselecting the subset, which leverages the DPP structure to formulate an\noptimization problem. This approach enables a simple and computationally\nefficient implementation. We further extend our approach to multiple rate\nconstraints, enabling the simultaneous selection of transmitters with\ndifferent rates. We demonstrate the effectiveness of our method through\nsimulation studies and compare it against state-of-the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19047619047619047,
          "p": 0.3076923076923077,
          "f": 0.23529411292387553
        },
        "rouge-2": {
          "r": 0.055,
          "p": 0.08396946564885496,
          "f": 0.06646525201485966
        },
        "rouge-l": {
          "r": 0.17687074829931973,
          "p": 0.2857142857142857,
          "f": 0.21848739023479993
        }
      }
    },
    {
      "paper_id": "math.CO.cs/CR/2503.10320v1",
      "true_abstract": "Cellular Automata (CA) are commonly investigated as a particular type of\ndynamical systems, defined by shift-invariant local rules. In this paper, we\nconsider instead CA as algebraic systems, focusing on the combinatorial designs\ninduced by their short-term behavior. Specifically, we review the main results\npublished in the literature concerning the construction of mutually orthogonal\nLatin squares via bipermutive CA, considering both the linear and nonlinear\ncases. We then survey some significant applications of these results to\ncryptography, and conclude with a discussion of open problems to be addressed\nin future research on CA-based combinatorial designs.",
      "generated_abstract": "In this survey, we examine the interplay between combinatorial designs and\ncellular automata (CA). Combinatorial designs are an integral part of\ncomputer science, particularly in data structures and algorithms, and CA are\nwell-known to be ubiquitous in the physical and life sciences. This article\ncompares the two approaches, focusing on their similarities and differences,\nincluding their applications in computer science and the physical and life\nsciences. Additionally, we discuss how CA can be used to solve combinatorial\nproblems, including knapsack problems, traveling salesman problems, and\n$k$-satisfiability. Finally, we provide a survey of recent advancements in the\nfield of CA-based combinatorial design.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27631578947368424,
          "p": 0.3088235294117647,
          "f": 0.2916666616820988
        },
        "rouge-2": {
          "r": 0.06451612903225806,
          "p": 0.06315789473684211,
          "f": 0.0638297822346088
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.27941176470588236,
          "f": 0.2638888839043211
        }
      }
    },
    {
      "paper_id": "cs.CE.econ/TH/2503.00201v1",
      "true_abstract": "This paper demonstrates that Automated Market Maker (AMM) based markets, such\nas those using constant product formulas (e.g., Uniswap), are inherently\npath-dependent. We prove mathematically that the sequence of operations in AMMs\ndetermines the final state, challenging the notion that market prices solely\nreflect information. This property has profound implications for decentralized\nprediction markets that rely on AMMs for price discovery, as it demonstrates\nthey cannot function as pure \"truth machines.\" Using both mathematical proofs\nand empirical evidence from ETH/USDC pools, we show that AMM-based markets\nincorporate historical path information beyond the current market beliefs. Our\nfindings contribute to the understanding of market efficiency, mechanism\ndesign, and the interpretation of prices in decentralized finance systems.",
      "generated_abstract": "r presents a mathematical proof for the existence of path\ndependence in AMM-based markets. We show that if an AMM has a path-dependent\nprice, then a non-trivial fraction of agents may trade at prices that differ\nfrom the equilibrium price by an amount that is not constant in time. We\npresent a novel approach for identifying such path-dependent prices, which we\ncall the \"triviality of price differences.\" We show that this method can be\napplied to a wide range of AMMs, including the first-price auction and the\nfirst-price sealed-bid auction. In particular, we show that both auction\nformats exhibit path dependence in the sense that agents may trade at prices\nthat differ from the equilibrium price by an amount that is not constant in\ntime. This finding has important implications for truth discovery in auction\nmarkets. For example, if the price",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26881720430107525,
          "p": 0.30864197530864196,
          "f": 0.28735631686286167
        },
        "rouge-2": {
          "r": 0.042735042735042736,
          "p": 0.04310344827586207,
          "f": 0.042918449935715
        },
        "rouge-l": {
          "r": 0.24731182795698925,
          "p": 0.2839506172839506,
          "f": 0.2643678111157354
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/OT/2405.07102v3",
      "true_abstract": "Instrumental variables (IV) are a commonly used tool to estimate causal\neffects from non-randomized data. An archetype of an IV is a randomized trial\nwith non-compliance where the randomized treatment assignment serves as an IV\nfor the non-ignorable treatment received. Under a monotonicity assumption, a\nvalid IV non-parametrically identifies the average treatment effect among a\nnon-identified, latent complier subgroup, whose generalizability is often under\ndebate. In many studies, there could exist multiple versions of an IV, for\ninstance, different nudges to take the same treatment in different study sites\nin a multicentre clinical trial. These different versions of an IV may result\nin different compliance rates and offer a unique opportunity to study IV\nestimates' generalizability. In this article, we introduce a novel nested IV\nassumption and study identification of the average treatment effect among two\nlatent subgroups: always-compliers and switchers, who are defined based on the\njoint potential treatment received under two versions of a binary IV. We derive\nthe efficient influence function for the SWitcher Average Treatment Effect\n(SWATE) under a non-parametric model and propose efficient estimators. We then\npropose formal statistical tests of the generalizability of IV estimates under\nthe nested IV framework. We apply the proposed method to the Prostate, Lung,\nColorectal and Ovarian (PLCO) Cancer Screening Trial and study the causal\neffect of colorectal cancer screening and its generalizability.",
      "generated_abstract": "We present a novel nested instrumental variable design (NIVD) for estimating\nthe average treatment effect (ATE) using switcher average treatment effects (SATE).\nThis NIVD is based on a two-step estimation procedure that involves a\nsimultaneous estimation of the conditional SATE and a two-step estimation of\nthe conditional ATE. We show that the two-step estimation procedure is\nefficient, in the sense that it requires only a constant number of switches to\nestimate the conditional ATE. Furthermore, we provide a practical criterion\nthat ensures that the switcher ATE is consistent. We further demonstrate that\nthe proposed NIVD can be implemented in a straightforward way and can be used\nfor the estimation of SATE and ATE for a wide range of linear and nonlinear\nmodels. We illustrate the practical implementation of our NIVD by applying it\nto a tobacco smoking example.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15555555555555556,
          "p": 0.2727272727272727,
          "f": 0.1981132029214134
        },
        "rouge-2": {
          "r": 0.05314009661835749,
          "p": 0.09016393442622951,
          "f": 0.06686929624560042
        },
        "rouge-l": {
          "r": 0.14074074074074075,
          "p": 0.24675324675324675,
          "f": 0.17924527839311158
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10060v1",
      "true_abstract": "This paper investigates the resource allocation design for a pinching antenna\n(PA)-assisted multiuser multiple-input single-output (MISO) non-orthogonal\nmultiple access (NOMA) system featuring multiple dielectric waveguides. To\nenhance model accuracy, we propose a novel frequency-dependent power\nattenuation model for dielectric waveguides in PA-assisted systems. By jointly\noptimizing the precoder vector and the PA placement, we aim to maximize the\nsystem's sum-rate while accounting for the power attenuation across dielectric\nwaveguides. The design is formulated as a non-convex optimization problem. To\neffectively address the problem at hand, we introduce an alternating\noptimization-based algorithm to obtain a suboptimal solution in polynomial\ntime. Our results demonstrate that the proposed PA-assisted system not only\nsignificantly outperforms the conventional system but also surpasses a naive\nPA-assisted system that disregards power attenuation. The performance gain\ncompared to the naive PA-assisted system becomes more pronounced at high\ncarrier frequencies, emphasizing the importance of considering power\nattenuation in system design.",
      "generated_abstract": "We study the problem of optimizing the number of antenna elements in a\nchannel with multiple dielectric waveguides, which can be used to enhance the\nchannel capacity. The problem is formulated as a sum-rate maximization problem\nover the set of possible channel parameters, where the number of antenna\nelements is subject to a constraint on the sum-rate. We derive the optimal\nsolution in the special case of equal transmit and receive antenna arrays. We\nthen extend our analysis to the general case and show that the optimal\nsolution is a critical point of the sum-rate function. We further discuss the\nimplications of our results on the design of NOMA systems with multiple dielectric\nwaveguides.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22549019607843138,
          "p": 0.35384615384615387,
          "f": 0.2754490970418445
        },
        "rouge-2": {
          "r": 0.07042253521126761,
          "p": 0.0970873786407767,
          "f": 0.08163264818792199
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.27692307692307694,
          "f": 0.21556885752088648
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/CR/2503.09712v1",
      "true_abstract": "Time series classification (TSC) is a cornerstone of modern web applications,\npowering tasks such as financial data analysis, network traffic monitoring, and\nuser behavior analysis. In recent years, deep neural networks (DNNs) have\ngreatly enhanced the performance of TSC models in these critical domains.\nHowever, DNNs are vulnerable to backdoor attacks, where attackers can covertly\nimplant triggers into models to induce malicious outcomes. Existing backdoor\nattacks targeting DNN-based TSC models remain elementary. In particular, early\nmethods borrow trigger designs from computer vision, which are ineffective for\ntime series data. More recent approaches utilize generative models for trigger\ngeneration, but at the cost of significant computational complexity. In this\nwork, we analyze the limitations of existing attacks and introduce an enhanced\nmethod, FreqBack. Drawing inspiration from the fact that DNN models inherently\ncapture frequency domain features in time series data, we identify that\nimproper perturbations in the frequency domain are the root cause of\nineffective attacks. To address this, we propose to generate triggers both\neffectively and efficiently, guided by frequency analysis. FreqBack exhibits\nsubstantial performance across five models and eight datasets, achieving an\nimpressive attack success rate of over 90%, while maintaining less than a 3%\ndrop in model accuracy on clean data.",
      "generated_abstract": "ork, we investigate the effectiveness of backdoor attacks on\ntime series classification tasks in the frequency domain. We evaluate the\neffectiveness of a series of attacks on two popular time series classification\ndatasets. We show that the backdoor attacks are effective on these datasets\neven when the backdoor is embedded in the time series itself. In addition, we\nshow that the frequency domain attack is more effective than the time domain\nattack on the same datasets. We also investigate the effectiveness of the\nfrequency domain attack on the time series classification datasets in the\nspectral domain. We show that the frequency domain attack is more effective on\nthese datasets than the time domain attack. In addition, we show that the\nfrequency domain attack is more effective than the time domain attack on the\nsame datasets. Additionally, we investigate the effectiveness of the frequency\ndomain attack on two popular time series classification datasets. We show that\nthe frequency domain attack is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14093959731543623,
          "p": 0.5121951219512195,
          "f": 0.22105262819445987
        },
        "rouge-2": {
          "r": 0.03015075376884422,
          "p": 0.08571428571428572,
          "f": 0.044609661577369356
        },
        "rouge-l": {
          "r": 0.14093959731543623,
          "p": 0.5121951219512195,
          "f": 0.22105262819445987
        }
      }
    },
    {
      "paper_id": "math.NT.math/NT/2503.08800v1",
      "true_abstract": "We prove the Fontaine-Plamondon conjecture and show that there are precisely\n$4400$ and $26952$ positive integral $E_7$-friezes and $E_8$-friezes\nrespectively, completing the enumerative classification of all positive\nintegral friezes of Dynkin type. In general, we count positive integral friezes\nof rank $n$ by determining the positive integral points on a $n$-dimensional\nsingular affine variety. This gives new Diophantine proofs of the enumeration\ntheorems for friezes of the other Dynkin types, which were previously proved\nusing discrete geometry, algebraic combinatorics, and the theory of cluster\nalgebras.",
      "generated_abstract": "We study the Diophantine enumeration of Dynkin friezes. We show that the\ndifference between the number of vertices and the number of edges of a Dynkin\nfrieze is bounded by a function of the order of the Dynkin diagram. This bound\nimplies that the number of vertices of a Dynkin frieze is a polynomial of degree\nat most the order of the Dynkin diagram. We then study the Diophantine\nenumeration of Dynkin friezes. We show that a frieze is determined by its\nnumber of edges and its Dynkin diagram. We also study the Diophantine\nenumeration of Dynkin friezes. We show that a frieze is determined by its\nnumber of edges and its Dynkin diagram.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20634920634920634,
          "p": 0.37142857142857144,
          "f": 0.2653061178571429
        },
        "rouge-2": {
          "r": 0.05194805194805195,
          "p": 0.06557377049180328,
          "f": 0.05797100955996682
        },
        "rouge-l": {
          "r": 0.15873015873015872,
          "p": 0.2857142857142857,
          "f": 0.20408162806122457
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.19869v1",
      "true_abstract": "This study investigates event-related desynchronization (ERD) phenomena\nduring motor imagery and actual movement. Using sLORETA software, we analyzed\nthe cortical current source density distributions in Mu and Beta frequency\nbands for 33 subjects during rest, motor imagery, and actual movement\nconditions. The results were normalized for analysis. Using sLORETA's\nstatistical tools, paired t-tests were conducted to compare the normalized\ncurrent source density results between rest and motor imagery, rest and actual\nmovement, and motor imagery and actual movement conditions in both frequency\nbands. The findings revealed: In both Mu and Beta frequency bands, during motor\nimagery, significant ERD (P<0.01) was observed in the salience network,\nsupplementary motor area, primary motor area, premotor cortex, primary\nsomatosensory cortex, and parietofrontal mirror neuron system. During actual\nmovement, significant ERD (P<0.05) was observed in the primary somatosensory\ncortex, primary motor area, and parietofrontal mirror neuron system in both\nfrequency bands. Comparing motor imagery to actual movement, the current source\ndensity in the primary somatosensory cortex and parietofrontal mirror neuron\nsystem was higher during motor imagery, though this difference was not\nstatistically significant (P>0.05). This paper analyzes the factors\ncontributing to these statistical results and proposes preliminary solutions.",
      "generated_abstract": "ated Desynchronization (ERD) is a key indicator of cortical\nmotor inhibition and is known to be present in healthy humans during motor\nactivities. However, previous studies on ERD in motor imagery (MI) and\nmovement-based ERD (MERD) have mostly used continuous EEG (cEEG) data, which\nhave limitations in terms of temporal resolution and non-invasiveness. To\naddress these limitations, we conducted a study using cERD and localized EEG\ncortical sources (LES) to investigate the ERD in MI and MERD. The ERD in MI and\nMERD was evaluated using two independent EEG datasets. We analyzed 100 EEG\nepisodes of MI and 100 MERD episodes from healthy volunteers. We evaluated the\nERD in MI using two independent datasets",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1827956989247312,
          "p": 0.23943661971830985,
          "f": 0.20731706826070806
        },
        "rouge-2": {
          "r": 0.013888888888888888,
          "p": 0.0196078431372549,
          "f": 0.016260157747374042
        },
        "rouge-l": {
          "r": 0.1827956989247312,
          "p": 0.23943661971830985,
          "f": 0.20731706826070806
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/GN/2412.05430v1",
      "true_abstract": "Recent advances in self-supervised models for natural language, vision, and\nprotein sequences have inspired the development of large genomic DNA language\nmodels (DNALMs). These models aim to learn generalizable representations of\ndiverse DNA elements, potentially enabling various genomic prediction,\ninterpretation and design tasks. Despite their potential, existing benchmarks\ndo not adequately assess the capabilities of DNALMs on key downstream\napplications involving an important class of non-coding DNA elements critical\nfor regulating gene activity. In this study, we introduce DART-Eval, a suite of\nrepresentative benchmarks specifically focused on regulatory DNA to evaluate\nmodel performance across zero-shot, probed, and fine-tuned scenarios against\ncontemporary ab initio models as baselines. Our benchmarks target biologically\nmeaningful downstream tasks such as functional sequence feature discovery,\npredicting cell-type specific regulatory activity, and counterfactual\nprediction of the impacts of genetic variants. We find that current DNALMs\nexhibit inconsistent performance and do not offer compelling gains over\nalternative baseline models for most tasks, while requiring significantly more\ncomputational resources. We discuss potentially promising modeling, data\ncuration, and evaluation strategies for the next generation of DNALMs. Our code\nis available at https://github.com/kundajelab/DART-Eval.",
      "generated_abstract": "nce models have been widely adopted in regulatory drug design and\nevaluation. Despite the advances in DNA sequence modeling, there is a\nsignificant gap in the evaluation of these models. While existing benchmarks\nfocus on single-drug design, evaluating the generalization capability of\ndrug-like DNA models across multiple drug discovery tasks, they do not\naccount for the complex regulatory requirements of biological targets.\nAdditionally, existing benchmarks focus on the performance of single-drug\nmodels, neglecting the diverse functionalities of regulatory DNA models, and\nexcluding models with complex regulatory functions. This paper introduces the\nDART-Eval benchmark, a comprehensive evaluation benchmark for DNA sequence\nmodels on regulatory DNA tasks. DART-Eval includes 42 regulatory tasks\nrepresenting diverse regulatory functions, including regulatory function\nprediction, regulatory network construction, and regulatory",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19858156028368795,
          "p": 0.37333333333333335,
          "f": 0.25925925472608036
        },
        "rouge-2": {
          "r": 0.03910614525139665,
          "p": 0.0603448275862069,
          "f": 0.04745762234668247
        },
        "rouge-l": {
          "r": 0.18439716312056736,
          "p": 0.3466666666666667,
          "f": 0.24074073620756173
        }
      }
    },
    {
      "paper_id": "math.PR.stat/TH/2502.17412v1",
      "true_abstract": "Gaussian multiplicative chaos (GMC) is a canonical random fractal measure\nobtained by exponentiating log-correlated Gaussian processes, first constructed\nin the seminal work of Kahane (1985). Since then it has served as an important\nbuilding block in constructions of quantum field theories and Liouville quantum\ngravity. However, in many natural settings, non-Gaussian log-correlated\nprocesses arise. In this paper, we investigate the universality of GMC through\nan invariance principle. We consider the model of a random Fourier series, a\nprocess known to be log-correlated. While the Gaussian Fourier series has been\na classical object of study, recently, the non-Gaussian counterpart was\ninvestigated and the associated multiplicative chaos constructed by Junnila in\n2016. We show that the Gaussian and non-Gaussian variables can be coupled so\nthat the associated chaos measures are almost surely mutually absolutely\ncontinuous throughout the entire sub-critical regime. This solves the main open\nproblem from Kim and Kriechbaum (2024) who had earlier established such a\nresult for a part of the regime. The main ingredient is a new high dimensional\nCLT for a sum of independent (but not i.i.d.) random vectors belonging to rank\none subspaces with error bounds involving the isotropic properties of the\ncovariance matrix of the sum, which we expect will find other applications. The\nproof relies on a path-wise analysis of Skorokhod embeddings as well as a\nperturbative result about square roots of positive semi-definite matrices\nwhich, surprisingly, appears to be new.",
      "generated_abstract": "We study the Gaussian Multiplicative Chaos, a family of Gaussian random\nobjects that is defined through a family of positive matrices, and that is\ninvariant under the Gaussian chaos transformation. In this paper, we propose a\nlow-dimensional Gaussian chaos CLT with a rank-one correction term that is\nbased on the high-dimensional Gaussian CLT. The proposed CLT is a\nhigh-dimensional counterpart of the well-known Gaussian CLT. This result has\nseveral applications to Gaussian chaos theory, including the low-dimensional\nGaussian chaos expansion, the low-rank Gaussian chaos expansion, and the\ninvariance principle for Gaussian chaos.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17575757575757575,
          "p": 0.5471698113207547,
          "f": 0.2660550421913139
        },
        "rouge-2": {
          "r": 0.039301310043668124,
          "p": 0.11538461538461539,
          "f": 0.058631918033719435
        },
        "rouge-l": {
          "r": 0.16363636363636364,
          "p": 0.5094339622641509,
          "f": 0.2477064183381029
        }
      }
    },
    {
      "paper_id": "math-ph.math-ph/2503.09558v1",
      "true_abstract": "For a given graph $G$, Budzik, Gaiotto, Kulp, Wang, Williams, Wu, Yu, and the\nfirst author studied a ''topological'' differential form $\\alpha_G$, which\nexpresses violations of BRST-closedness of a quantum field theory along a\nsingle topological direction. In a seemingly unrelated context, Brown, Panzer,\nand the second author studied a ''Pfaffian'' differential form $\\phi_G$, which\nis used to construct cohomology classes of the odd commutative graph complex.\nWe give an explicit combinatorial proof that $\\alpha_G$ coincides with\n$\\phi_G$. We also discuss the equivalence of several properties of these forms,\nwhich had been established independently for both contexts in previous work.",
      "generated_abstract": "We prove that the topological form of a closed $2n$-dimensional closed\nstrictly pseudoconvex domain is the Pfaffian form. In particular, this\nproposition implies that a strictly pseudoconvex domain is of type $\\mathrm{E}_8$\nif and only if its topological form is the Pfaffian form.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12658227848101267,
          "p": 0.38461538461538464,
          "f": 0.1904761867501135
        },
        "rouge-2": {
          "r": 0.010526315789473684,
          "p": 0.02857142857142857,
          "f": 0.015384611449705148
        },
        "rouge-l": {
          "r": 0.11392405063291139,
          "p": 0.34615384615384615,
          "f": 0.1714285677024944
        }
      }
    },
    {
      "paper_id": "math.CO.math/IT/2503.08948v1",
      "true_abstract": "Let $C$ be a binary code of length $n$ with distances $0<d_1<\\cdots<d_s\\le\nn$. In this note we prove a general upper bound on the size of $C$ without any\nrestriction on the distances $d_i$. The bound is asymptotically optimal.",
      "generated_abstract": "We present a new upper bound on the size of a linear code over $\\mathbb{F}_q$\nwith $s$ distances. The bound is obtained by extending the upper bound on the\nsize of a linear code over $\\mathbb{F}_q$ with $s$ distances obtained by\nY. Xin in 2002. In this paper, we also show that the upper bound on the size of\na linear code over $\\mathbb{F}_q$ with $s$ distances given by Xin is sharp. We\nalso show that the upper bound on the size of a linear code over $\\mathbb{F}_q$\nwith $s$ distances obtained by D. Liu in 2021 is also sharp.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.46875,
          "p": 0.39473684210526316,
          "f": 0.4285714236081633
        },
        "rouge-2": {
          "r": 0.21621621621621623,
          "p": 0.15384615384615385,
          "f": 0.17977527604090407
        },
        "rouge-l": {
          "r": 0.46875,
          "p": 0.39473684210526316,
          "f": 0.4285714236081633
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.18868v1",
      "true_abstract": "This study examines the relationship between income inequality, gender, and\nschool completion rates in Malaysia using machine learning techniques. The\ndataset utilized is from the Malaysia's Public Sector Open Data Portal,\ncovering the period 2016-2022. The analysis employs various machine learning\ntechniques, including K-means clustering, ARIMA modeling, Random Forest\nregression, and Prophet for time series forecasting. These models are used to\nidentify patterns, trends, and anomalies in the data, and to predict future\nschool completion rates. Key findings reveal significant disparities in school\ncompletion rates across states, genders, and income levels. The analysis also\nidentifies clusters of states with similar completion rates, suggesting\npotential regional factors influencing educational outcomes. Furthermore, time\nseries forecasting models accurately predict future completion rates,\nhighlighting the importance of ongoing monitoring and intervention strategies.\nThe study concludes with recommendations for policymakers and educators to\naddress the observed disparities and improve school completion rates in\nMalaysia. These recommendations include targeted interventions for specific\nstates and demographic groups, investment in early childhood education, and\naddressing the impact of income inequality on educational opportunities. The\nfindings of this study contribute to the understanding of the factors\ninfluencing school completion in Malaysia and provide valuable insights for\npolicymakers and educators to develop effective strategies to improve\neducational outcomes.",
      "generated_abstract": "y aims to investigate the impact of income inequality and gender\non school completion using machine learning techniques. The research adopted\na panel dataset of 3,502 students from 2017 to 2018 and 2018 to 2019 in\nMalaysia, which consists of students from both government and private\nschools. The findings reveal that income inequality and gender are significant\npredictors of school completion. Furthermore, the study also shows that income\ninequality and gender are significant predictors of school completion. This\nfinding indicates that income inequality and gender are significant predictors\nof school completion, which is consistent with the literature. The study\nhighlights that income inequality and gender are significant predictors of\nschool completion, which is consistent with the literature. The findings\nprovide valuable insights into the potential impact of income inequality and\ngender on school completion, which may be utilized",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2661290322580645,
          "p": 0.5076923076923077,
          "f": 0.3492063446935976
        },
        "rouge-2": {
          "r": 0.07567567567567568,
          "p": 0.16279069767441862,
          "f": 0.10332102887760261
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.47692307692307695,
          "f": 0.32804232352957646
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2502.02619v1",
      "true_abstract": "This paper introduces a novel agent-based approach for enhancing existing\nportfolio strategies using Proximal Policy Optimization (PPO). Rather than\nfocusing solely on traditional portfolio construction, our approach aims to\nimprove an already high-performing strategy through dynamic rebalancing driven\nby PPO and Oracle agents. Our target is to enhance the traditional 60/40\nbenchmark (60% stocks, 40% bonds) by employing the Regret-based Sharpe reward\nfunction. To address the impact of transaction fee frictions and prevent signal\nloss, we develop a transaction cost scheduler. We introduce a future-looking\nreward function and employ synthetic data training through a circular block\nbootstrap method to facilitate the learning of generalizable allocation\nstrategies. We focus on two key evaluation measures: return and maximum\ndrawdown. Given the high stochasticity of financial markets, we train 20\nindependent agents each period and evaluate their average performance against\nthe benchmark. Our method not only enhances the performance of the existing\nportfolio strategy through strategic rebalancing but also demonstrates strong\nresults compared to other baselines.",
      "generated_abstract": "y investigates the application of reinforcement learning (RL)\nas a means to enhance portfolio performance in the context of financial\nportfolio management. In particular, we propose a Deep RL-based framework that\noptimizes portfolio allocation by learning to select a portfolio of assets\naccording to future expected returns. To address the challenges of large\nstate spaces and limited data, we propose a future-looking reward function\nwhich utilizes historical returns to prioritize portfolio performance. The\nproposed framework is implemented in the PyTorch framework, and its performance\nis evaluated through both theoretical analysis and empirical simulations. The\nresults demonstrate that the proposed framework outperforms existing approaches\nin terms of both average and minimum returns and exhibits superior portfolio\nflexibility compared to traditional methods. The proposed framework provides\ninsights into the potential of RL-based portfolio optimization, offering\npotential applications in financial management and decision",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2066115702479339,
          "p": 0.2840909090909091,
          "f": 0.2392344448854194
        },
        "rouge-2": {
          "r": 0.050314465408805034,
          "p": 0.06060606060606061,
          "f": 0.054982812912460155
        },
        "rouge-l": {
          "r": 0.19008264462809918,
          "p": 0.26136363636363635,
          "f": 0.22009568890455813
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2503.04246v1",
      "true_abstract": "Bayesian inference has many advantages for complex models. However, standard\nMonte Carlo methods for summarizing the posterior can be computationally\ndemanding, and it is attractive to consider optimization-based variational\napproximations. Our work considers Gaussian approximations with sparse\nprecision matrices which are tractable to optimize in high-dimensional\nproblems. Although the optimal Gaussian approximation is usually defined as the\none closest to the target posterior in Kullback-Leibler divergence, it is\nuseful to consider other divergences when the Gaussian assumption is crude, in\norder to capture important features of the posterior for a given application.\nOur work studies the weighted Fisher divergence, which focuses on gradient\ndifferences between the target posterior and its approximation, with the Fisher\nand score-based divergences being special cases. We make three main\ncontributions. First, we compare approximations for weighted Fisher divergences\nunder mean-field assumptions for both Gaussian and non-Gaussian targets with\nKullback-Leibler approximations. Second, we go beyond mean-field and consider\napproximations with sparse precision matrices reflecting posterior conditional\nindependence structure for hierarchical models. Using stochastic gradient\ndescent to enforce sparsity, we develop two approaches to minimize the weighted\nFisher divergence, based on the reparametrization trick and a batch\napproximation of the objective. Finally, we examine the performance of our\nmethods for examples involving logistic regression, generalized linear mixed\nmodels and stochastic volatility models.",
      "generated_abstract": "variational inference (GVI) is a popular method for optimizing\nhigh-dimensional distributions, with applications in Bayesian inference,\nstatistical learning, and machine learning. In GVI, a prior distribution\nparameterizes the unknown distribution of interest, and a variational\ndistribution is introduced to approximate the posterior distribution. The\nlog-posterior is then minimized to obtain a point estimate of the unknown\ndistribution. The choice of the prior and variational distribution is crucial\nfor optimizing the convergence rate of the optimizer. In this work, we\nintroduce a novel weighted Fisher divergence, called the generalized weighted\nFisher divergence (GWF), to evaluate the convergence of GVI. We establish the\nasymptotic properties of the GWF for two commonly used variational distributions,\nand we propose a new algorithm to optimize the GWF. The proposed algorithm\nreduces the computational complexity by a factor",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16793893129770993,
          "p": 0.28205128205128205,
          "f": 0.21052631111100947
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.05042016806722689,
          "f": 0.037854884900437445
        },
        "rouge-l": {
          "r": 0.1450381679389313,
          "p": 0.24358974358974358,
          "f": 0.18181817713971762
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/SY/2503.09865v1",
      "true_abstract": "Remote driving of vehicles is gaining in importance in the transportation\nsector, especially when Automated Driving Systems (ADSs) reach the limits of\ntheir system boundaries. This study investigates the challenges faced by human\nRemote Drivers (RDs) during remote driving, particularly focusing on the\nidentification and classification of human performance-related challenges\nthrough a comprehensive analysis of real-world remote driving data Las Vegas.\nFor this purpose, a total of 183 RD performance-related Safety Driver (SD)\ninterventions were analyzed and classified using an introduced severity\nclassification. As it is essential to prevent the need for SD interventions,\nthis study identified and analyzed harsh driving events to detect an increased\nlikelihood of interventions by the SD. In addition, the results of the\nsubjective RD questionnaire are used to evaluate whether the objective metrics\nfrom SD interventions and harsh driving events can also be confirmed by the RDs\nand whether additional challenges can be uncovered. The analysis reveals\nlearning curves, showing a significant decrease in SD interventions as RD\nexperience increases. Early phases of remote driving experience, especially\nbelow 200 km of experience, showed the highest frequency of safety-related\nevents, including braking too late for traffic signs and responding impatiently\nto other traffic participants. Over time, RDs follow defined rules for\nimproving their control, with experience leading to less harsh braking,\nacceleration, and steering maneuvers. The study contributes to understanding\nthe requirements of RDS, emphasizing the importance of targeted training to\naddress human performance limitations. It further highlights the need for\nsystem improvements to address challenges like latency and the limited haptic\nfeedback replaced by visual feedback, which affect the RDs' perception and\nvehicle control.",
      "generated_abstract": "s a complex task that requires precise and accurate decision-making\nin real-time conditions. Despite the progress in driver assistance systems,\ndriving is still a dangerous task and drivers are prone to errors. Human\nperformance can be classified into three categories: 1) low performance, 2)\nnormal performance, and 3) high performance. This study focuses on the high\nperformance category. The high performance category is characterized by\ndriving under certain conditions, such as night driving, driving in a\ndifferent mode, driving in a different vehicle, and driving under certain\nconditions. This study aims to identify the challenges that occur during\nhigh-performance driving and to classify the performance level of the driver.\nThe proposed approach is based on the analysis of the collected data and\nclassification of the driver performance using machine learning techniques.\nThe results show that the proposed method is able to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16184971098265896,
          "p": 0.32941176470588235,
          "f": 0.2170542591475874
        },
        "rouge-2": {
          "r": 0.03065134099616858,
          "p": 0.06299212598425197,
          "f": 0.041237108998432824
        },
        "rouge-l": {
          "r": 0.14450867052023122,
          "p": 0.29411764705882354,
          "f": 0.193798445194099
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.07461v1",
      "true_abstract": "We study the optimal management of a photovoltaic system's battery owned by a\nself-consumption group that aims to minimize energy consumption costs. We\nassume that the photovoltaic system is composed of a photovoltaic panel and a\nbattery, where the photovoltaic panel produces energy according to a certain\nstochastic process. The management of the battery is the responsibility of a\ngroup administrator, who makes the joint decision to either store part of the\nphotovoltaic energy production and sell the remaining energy at the electricity\nspot price, or discharge part of the energy stored in the battery and sell it\nin the electricity market. Inspired by European Union and Italian legislation,\nwhich promote incentives for energy transition and renewable energy production,\nwe assume that the group receives a monetary incentive for the virtual\nself-consumed energy, defined as the minimum between the power bought from the\ngrid to satisfy the group's power demand and the energy sold to the market. In\nthis case, the energy sold by the group is a mix of part of the photovoltaic\nproduction that is not stored and part of the energy discharged from the\nbattery. We model the problem as a stochastic optimal control problem, where\nthe optimal strategy is the joint charge-discharge decision that minimizes the\ngroup's energy consumption costs. We find the solution numerically by applying\na finite difference scheme to solve the Hamilton-Jacobi-Bellman equation\nassociated with the value function of the optimal control problem.",
      "generated_abstract": "r studies the optimal energy storage management of a self-consumption\ngroup. The storage capacity of the storage owner is assumed to be infinite,\nhence the storage owner has full control over the energy supply. In this\nsituation, the storage owner faces the challenges of maximizing the energy\nrevenue and minimizing the storage cost. To address the challenges, the\nstorage owner may consider either optimizing the storage capacity or optimizing\nthe storage cost. The storage owner's storage capacity optimization problem\nis formulated as a minimization problem, and the storage owner's storage cost\noptimization problem is formulated as a maximization problem. The storage\nowner's storage capacity optimization problem is modeled as a linear programming\nproblem, and the storage owner's storage cost optimization problem is modeled\nas a mixed integer linear programming problem. The storage owner's storage\ncapacity optimization problem is solved using a modified algorithm and the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.3333333333333333,
          "f": 0.22857142406530614
        },
        "rouge-2": {
          "r": 0.05,
          "p": 0.1111111111111111,
          "f": 0.06896551296076128
        },
        "rouge-l": {
          "r": 0.17391304347826086,
          "p": 0.3333333333333333,
          "f": 0.22857142406530614
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/EM/2501.07615v1",
      "true_abstract": "Climate change is increasing the frequency and severity of natural disasters\nworldwide. Media coverage of these events may be vital to generate empathy and\nmobilize global populations to address the common threat posed by climate\nchange. Using a dataset of 466 news sources from 123 countries, covering 135\nmillion news articles since 2016, we apply an event study framework to measure\ncross-border media activity following natural disasters. Our results shows that\nwhile media attention rises after disasters, it is heavily skewed towards\ncertain events, notably earthquakes, accidents, and wildfires. In contrast,\nclimatologically salient events such as floods, droughts, or extreme\ntemperatures receive less coverage. This cross-border disaster reporting is\nstrongly related to the number of deaths associated with the event, especially\nwhen the affected populations share strong social ties or genetic similarities\nwith those in the reporting country. Achieving more balanced media coverage\nacross different types of natural disasters may be essential to counteract\nskewed perceptions. Further, fostering closer social connections between\ncountries may enhance empathy and mobilize the resources necessary to confront\nthe global threat of climate change.",
      "generated_abstract": "novel dataset on cross-border media coverage of natural disasters to\ninvestigate the role of social ties, including family, friends, and work\nrelationships, in shaping coverage of disasters. Our findings show that social\nties are particularly important in shaping media coverage of natural disasters\nthat cause widespread deaths, such as earthquakes, hurricanes, and tornadoes,\nand that family ties are particularly important for coverage of natural\ndisasters that cause fewer deaths, such as floods and forest fires. We also\nfind that ties with a person of importance, such as a governor or president,\nincrease the likelihood of coverage, while ties with an unknown person\nincrease the likelihood of being ignored. Finally, we find that ties with\nhigh-income, educated, and male family members and friends increase the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18461538461538463,
          "p": 0.34285714285714286,
          "f": 0.23999999545000006
        },
        "rouge-2": {
          "r": 0.04678362573099415,
          "p": 0.08163265306122448,
          "f": 0.0594795492715693
        },
        "rouge-l": {
          "r": 0.18461538461538463,
          "p": 0.34285714285714286,
          "f": 0.23999999545000006
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2412.03883v1",
      "true_abstract": "To advance the understanding of cellular metabolisms and control\nbatch-to-batch variations in cell culture processes, a multi-scale mechanistic\nmodel with a bottom-up and top-down structure was developed to simulate the\ndynamics of cell culture process undergoing metabolic state transitions. This\nmodel integrates interactions at the molecular, cellular, and macro-kinetic\nlevels, accounting for inherent variations in metabolic state transitions of\nindividual cells. By incorporating both online (e.g., oxygen uptake, pH) and\noffline measurements (e.g., viable cell density, metabolite concentrations),\nthe proposed mechanistic model enables accurate long-term prediction of cell\nculture trajectories and provides reliable prediction intervals quantifying\nbatch-to-batch variations. This work can guide optimal design of experiments\nand robust process control to improve yield and production stability.\nAdditionally, the proposed multi-scale model has a modular design enables\nflexible in silico simulations and extrapolation across diverse conditions,\nproviding a robust prediction framework for scalable and flexible\nbiomanufacturing applications.",
      "generated_abstract": "ure is a critical step in the development of biotechnological\nproducts. To ensure the quality of the products, the quality of the culture\nmedium must be continuously monitored and adjusted. In this study, we introduce\na multi-scale kinetic modeling framework for cell culture process with metabolic\nstate transition, which consists of three levels: Cell, medium, and system\nlevels. The framework is developed by coupling two coupled kinetic models,\nKinetic Model 1 and Kinetic Model 2, into a unified framework. The model\nprovides a more comprehensive and detailed understanding of the metabolic\nprocess in cell culture medium and the effects of metabolic state transition.\nThe framework is validated by simulating 2000 time steps of cell culture\nprocesses with different metabolic states. The results show that the\nmulti-scale modeling framework can effectively capture the metabolic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20408163265306123,
          "p": 0.24390243902439024,
          "f": 0.22222221726172853
        },
        "rouge-2": {
          "r": 0.057971014492753624,
          "p": 0.06722689075630252,
          "f": 0.06225680436585
        },
        "rouge-l": {
          "r": 0.1836734693877551,
          "p": 0.21951219512195122,
          "f": 0.1999999950395063
        }
      }
    },
    {
      "paper_id": "physics.plasm-ph.physics/space-ph/2503.00620v1",
      "true_abstract": "This study explores the generation of Electrostatic (ES) Electron\nKelvin-Helmholtz instability (EKHI) in collisionless plasma with a\nstep-function electron velocity shear akin to that developed in the electron\ndiffusion region in magnetic reconnection. In incompressible plasma, ES EKHI\ndoesn't arise in any velocity shear profile due to the decoupling of the\nelectric potential from the electron momentum equation. Instead a fluid-like\nKelvin-Helmholtz instability (KHI) can arise. However, in compressible plasma,\nthe compressibility couples the electric potential with the electron dynamics,\nleading to the emergence of a new ES mode EKHI on Debye length $\\lambda_{De}$,\naccompanied by the co-generation of an electron acoustic-like wave. The minimum\nthreshold of ES EKHI is $\\Delta \\mathbf{U}> 2c_{se}$, i.e., the electron\nvelocity shear larger than twice the electron acoustic speed $c_{se}$. The\ncorresponding growth rate is $Im(\\omega) = ((\\Delta \\mathbf{U}/c_{se})^2 -\n4)^{1/2} \\omega_{pe}$, where $\\omega_{pe}$ is the electron plasma frequency.",
      "generated_abstract": "of compressible plasma phenomena has attracted a great deal of\ninterest in recent years due to the rapid advancement in the field of plasma\nphysics. The study of the generation of plasma instabilities has gained\nsignificant attention due to the relevance of such phenomena in many practical\napplications. In this paper, we investigate the generation of electrostatic\nKelvin-Helmholtz instabilities (KHI) in an electrostatic plasma using a\ncompressible numerical simulation. The study is conducted using the\nLattice-Boltzmann Method (LBM) with a finite element discretization for the\nelectrostatic field and the plasma dynamics. We focus on the generation of\nelectrostatic KHI using the electrostatic force-free (EF-1) model for the\nelectrostatic field. Our findings indicate that the generation of electrostatic\nKHI is a non",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1958762886597938,
          "p": 0.27941176470588236,
          "f": 0.23030302545748405
        },
        "rouge-2": {
          "r": 0.05263157894736842,
          "p": 0.06862745098039216,
          "f": 0.059574463172114484
        },
        "rouge-l": {
          "r": 0.18556701030927836,
          "p": 0.2647058823529412,
          "f": 0.2181818133362719
        }
      }
    },
    {
      "paper_id": "physics.comp-ph.physics/comp-ph/2503.10263v1",
      "true_abstract": "A new parallelized simulation code is presented, which uses a Monte Carlo\nmethod to determine particle spectra in the KATRIN source. Reaction chains are\ngenerated from the decay of tritium within the source. The code includes all\nrelevant processes: elastic scattering, ionization, excitation (electric,\nvibrational, rotational), recombination and various clustering processes. The\nmain emphasis of the code is the calculation of particle spectra and particle\ndensities and currents at specific points within the source. It features a new\ntechnique to determine these quantities. It also calculates target fields for\nthe interaction of particles with each other as it is needed for recombination\nprocesses. The code has been designed for the KATRIN experiment but is easily\nadapt-able for other tritium based experiments like Project 8. Geometry and\nbackground tritium gas flow can be given as user input. The code is\nparallelized using MPI and writes output using HDF5. Input to the simulation is\nread from a JSON description.",
      "generated_abstract": "N experiment will measure the tritium abundance in the atmosphere\nof the Kastler-Antonelli Ridge Layer (KARL), a region with a high concentration\nof tritium. We present a Monte Carlo model for the tritium production and\ndissolution processes in the KARL region. We use a general-purpose Monte Carlo\ncode to compute the tritium concentration in the atmosphere and the\ndissolution processes in the tritium-rich water. We use an energy-dependent\ndissolution rate for tritium in water to simulate the dissolution of tritium in\nthe KARL water. Our model accounts for the isotopic composition of the\ntritium, its concentration in the atmosphere, and the dissolution processes in\nthe KARL water. Our model is validated against the measurements by\nKATRIN. We also",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1650485436893204,
          "p": 0.3090909090909091,
          "f": 0.21518986887918612
        },
        "rouge-2": {
          "r": 0.041379310344827586,
          "p": 0.06741573033707865,
          "f": 0.05128204656841302
        },
        "rouge-l": {
          "r": 0.1650485436893204,
          "p": 0.3090909090909091,
          "f": 0.21518986887918612
        }
      }
    },
    {
      "paper_id": "cs.IR.cs/IR/2503.08452v1",
      "true_abstract": "We propose Knowledge-Aware Preprocessing (KAP), a two-stage preprocessing\nframework tailored for Traditional Chinese non-narrative documents, designed to\nenhance retrieval accuracy in Hybrid Retrieval systems. Hybrid Retrieval, which\nintegrates Sparse Retrieval (e.g., BM25) and Dense Retrieval (e.g., vector\nembeddings), has become a widely adopted approach for improving search\neffectiveness. However, its performance heavily depends on the quality of input\ntext, which is often degraded when dealing with non-narrative documents such as\nPDFs containing financial statements, contractual clauses, and tables. KAP\naddresses these challenges by integrating Multimodal Large Language Models\n(MLLMs) with LLM-driven post-OCR processing, refining extracted text to reduce\nOCR noise, restore table structures, and optimize text format. By ensuring\nbetter compatibility with Hybrid Retrieval, KAP improves the accuracy of both\nSparse and Dense Retrieval methods without modifying the retrieval architecture\nitself.",
      "generated_abstract": "years, Chinese non-narrative documents (CNNDs) have become an\nprimary research topic in machine learning (ML). However, existing methods\noften face limitations in both text enhancement and retrieval, which hinder\ntheir application in CNND. To address this, we propose KAP (Knowledge-Augmented\nMLLM-assisted OCR Text Enhancement for Hybrid Retrieval in Chinese Non-Narrative\nDocuments), a novel model that leverages the knowledge of a large knowledge base\n(KB) to enhance OCR text and enhance retrieval in Chinese non-narrative\ndocuments. Specifically, our KAP first enhances OCR text with knowledge from\nthe KB to enhance OCR quality. Then, to enhance retrieval, it generates\nsynthetic annotations to enhance the retrieval model's understanding of the\nCNND. Experimental",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24299065420560748,
          "p": 0.33766233766233766,
          "f": 0.2826086907850898
        },
        "rouge-2": {
          "r": 0.046875,
          "p": 0.06,
          "f": 0.052631574022776705
        },
        "rouge-l": {
          "r": 0.21495327102803738,
          "p": 0.2987012987012987,
          "f": 0.24999999513291596
        }
      }
    },
    {
      "paper_id": "math.SP.math/SP/2503.03248v1",
      "true_abstract": "We introduce and study a new theoretical concept of $\\textit{spectral pair}$\nfor a Schr\\\"{o}dinger operator $H$ in $L^2(\\mathbb{R}_{+})$ with a bounded\n$\\textit{complex-valued}$ potential. The spectral pair consists of a measure\nand a complex-valued function, both of which are defined on $\\mathbb{R}_{+}$.\nWe show that in many ways, the spectral pair generalises the classical spectral\nmeasure to the non-self-adjoint case. First, extending the classical\nBorg-Marchenko theorem, we prove a uniqueness result: the spectral pair\nuniquely determines the potential. Second, we derive asymptotic formulas for\nthe spectral pair in the spirit of the classical result of Marchenko. In the\ncase of real-valued potentials, we relate the spectral pair to the spectral\nmeasure of $H$. Lastly, we provide formulas for the spectral pair at a~simple\neigenvalue of $|H|$.",
      "generated_abstract": "We establish the Borg-Marchenko uniqueness theorem for complex potentials on\nthe unit ball of a Hilbert space. In particular, we prove the uniqueness of\ncomplex potentials that are homogeneous of degree one on the unit ball and that\nare positive on a certain closed ball.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19718309859154928,
          "p": 0.4827586206896552,
          "f": 0.27999999588200003
        },
        "rouge-2": {
          "r": 0.018518518518518517,
          "p": 0.05128205128205128,
          "f": 0.02721088045536638
        },
        "rouge-l": {
          "r": 0.16901408450704225,
          "p": 0.41379310344827586,
          "f": 0.23999999588200008
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.04851v2",
      "true_abstract": "In protein engineering, while computational models are increasingly used to\npredict mutation effects, their evaluations primarily rely on high-throughput\ndeep mutational scanning (DMS) experiments that use surrogate readouts, which\nmay not adequately capture the complex biochemical properties of interest. Many\nproteins and their functions cannot be assessed through high-throughput methods\ndue to technical limitations or the nature of the desired properties, and this\nis particularly true for the real industrial application scenario. Therefore,\nthe desired testing datasets, will be small-size (~10-100) experimental data\nfor each protein, and involve as many proteins as possible and as many\nproperties as possible, which is, however, lacking. Here, we present\nVenusMutHub, a comprehensive benchmark study using 905 small-scale experimental\ndatasets curated from published literature and public databases, spanning 527\nproteins across diverse functional properties including stability, activity,\nbinding affinity, and selectivity. These datasets feature direct biochemical\nmeasurements rather than surrogate readouts, providing a more rigorous\nassessment of model performance in predicting mutations that affect specific\nmolecular functions. We evaluate 23 computational models across various\nmethodological paradigms, such as sequence-based, structure-informed and\nevolutionary approaches. This benchmark provides practical guidance for\nselecting appropriate prediction methods in protein engineering applications\nwhere accurate prediction of specific functional properties is crucial.",
      "generated_abstract": "utation has been recognized as a powerful tool for exploring\nthe molecular basis of biological processes. However, the selection of the\nbest predictor for mutation effect remains a challenging task. In this work, we\nintroduce VenusMutHub, a publicly available repository that provides a\nsystematic evaluation of 100 predictors for protein mutation effect in\nsmall-scale experimental data. VenusMutHub has three main contributions. First,\nit provides a comprehensive evaluation of predictors using the largest and most\nrobust experimental data set. Second, it organizes predictor evaluation into\ntwo independent aspects: (1) prediction accuracy on the training data set, and\n(2) prediction accuracy on the testing data set. Third, VenusMutHub provides\ndetailed experimental design and statistical analysis for all predictors to\nfacilitate further experiments. By systematically evaluating and comparing\npredict",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17763157894736842,
          "p": 0.3103448275862069,
          "f": 0.22594141796397132
        },
        "rouge-2": {
          "r": 0.025510204081632654,
          "p": 0.042735042735042736,
          "f": 0.03194887710765719
        },
        "rouge-l": {
          "r": 0.16447368421052633,
          "p": 0.28735632183908044,
          "f": 0.20920501629033117
        }
      }
    },
    {
      "paper_id": "cs.SI.cs/SI/2503.09725v1",
      "true_abstract": "Avian Influenza Virus (AIV) poses significant threats to the poultry\nindustry, humans, domestic animals, and wildlife health worldwide. Monitoring\nthis infectious disease is important for rapid and effective response to\npotential outbreaks. Conventional avian influenza surveillance systems have\nexhibited limitations in providing timely alerts for potential outbreaks. This\nstudy aimed to examine the idea of using online activity on social media, and\nGoogle searches to improve the identification of AIV in the early stage of an\noutbreak in a region. To this end, to evaluate the feasibility of this\napproach, we collected historical data on online user activities from X\n(formerly known as Twitter) and Google Trends and assessed the statistical\ncorrelation of activities in a region with the AIV outbreak officially reported\ncase numbers. In order to mitigate the effect of the noisy content on the\noutbreak identification process, large language models were utilized to filter\nout the relevant online activity on X that could be indicative of an outbreak.\nAdditionally, we conducted trend analysis on the selected internet-based data\nsources in terms of their timeliness and statistical significance in\nidentifying AIV outbreaks. Moreover, we performed an ablation study using\nautoregressive forecasting models to identify the contribution of X and Google\nTrends in predicting AIV outbreaks. The experimental findings illustrate that\nonline activity on social media and search engine trends can detect avian\ninfluenza outbreaks, providing alerts earlier compared to official reports.\nThis study suggests that real-time analysis of social media outlets and Google\nsearch trends can be used in avian influenza outbreak early warning systems,\nsupporting epidemiologists and animal health professionals in informed\ndecision-making.",
      "generated_abstract": "ublic health concern in the United States and Canada is the\ndetection of outbreaks of Avian Influenza (AI). We leverage social media\ninformation, including the number of tweets about the AI outbreaks, to\nidentify the AI outbreaks' trajectories in the USA and Canada. We employ\nGoogle Trends to calculate the AI outbreaks' search interest and social media\nactivity. Our analysis reveals that the 2003-2009 AI outbreaks in the USA\noccurred in the spring and autumn, while the 2013-2016 AI outbreaks occurred\nin the spring and autumn. The 2019 AI outbreaks in the USA and Canada occurred\nin the spring and autumn. Our findings provide valuable insights for public\nhealth authorities to manage outbreaks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1464968152866242,
          "p": 0.38333333333333336,
          "f": 0.21198156281934213
        },
        "rouge-2": {
          "r": 0.02459016393442623,
          "p": 0.06896551724137931,
          "f": 0.03625377255994419
        },
        "rouge-l": {
          "r": 0.12738853503184713,
          "p": 0.3333333333333333,
          "f": 0.18433179323408871
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.14154v1",
      "true_abstract": "In allocating objects via lotteries, it is common to consider ordinal rules\nthat rely solely on how agents rank degenerate lotteries. While ordinality is\noften imposed due to cognitive or informational constraints, we provide another\njustification from an axiomatic perspective: for three-agent problems, the\ncombination of efficiency, strategy-proofness, non-bossiness, and a weak form\nof continuity collectively implies ordinality.",
      "generated_abstract": "I study the problem of allocating a random set of items to a random\ngroup of agents. When the agents are ordered and have different ordinal\ninterests, how should the agents be allocated? I show that the most natural\ndistribution is a random binomial allocation, but that this is not optimal in\nmany cases. I also show that the most natural distribution is a random\nproportional allocation if the agents have the same ordinal interests, but that\nthis is not optimal in some cases. Finally, I show that the optimal allocation\nis a random linear allocation, but that this is not optimal in some cases. I\nalso show that the optimal allocation is a random random allocation, but that\nthis is not optimal in some cases.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2037037037037037,
          "p": 0.22916666666666666,
          "f": 0.21568626952710504
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.2037037037037037,
          "p": 0.22916666666666666,
          "f": 0.21568626952710504
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.12731v1",
      "true_abstract": "We address counterfactual analysis in empirical models of games with\npartially identified parameters, and multiple equilibria and/or randomized\nstrategies, by constructing and analyzing the counterfactual predictive\ndistribution set (CPDS). This framework accommodates various outcomes of\ninterest, including behavioral and welfare outcomes. It allows a variety of\nchanges to the environment to generate the counterfactual, including\nmodifications of the utility functions, the distribution of utility\ndeterminants, the number of decision makers, and the solution concept. We use a\nBayesian approach to summarize statistical uncertainty. We establish conditions\nunder which the population CPDS is sharp from the point of view of\nidentification. We also establish conditions under which the posterior CPDS is\nconsistent if the posterior distribution for the underlying model parameter is\nconsistent. Consequently, our results can be employed to conduct counterfactual\nanalysis after a preliminary step of identifying and estimating the underlying\nmodel parameter based on the existing literature. Our consistency results\ninvolve the development of a new general theory for Bayesian consistency of\nposterior distributions for mappings of sets. Although we primarily focus on a\nmodel of a strategic game, our approach is applicable to other structural\nmodels with similar features.",
      "generated_abstract": "We introduce a new framework for analyzing counterfactual outcomes in\nempirical games. The framework captures the fundamental features of\ncounterfactual analysis, such as the counterfactual distribution, the counterfactual\ntreatment effect, and the counterfactual treatment assignment. We then use the\nframework to analyze empirical games that model the outcomes of treatments\narising from the market for goods. Our analysis shows that the counterfactual\ndistribution of the outcome of a treatment that the government decides to\nimplement depends on the distribution of the outcome of the treatment that the\ngovernment decides to implement.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19658119658119658,
          "p": 0.48936170212765956,
          "f": 0.2804878007889649
        },
        "rouge-2": {
          "r": 0.061452513966480445,
          "p": 0.1506849315068493,
          "f": 0.08730158318625618
        },
        "rouge-l": {
          "r": 0.1794871794871795,
          "p": 0.44680851063829785,
          "f": 0.2560975568865259
        }
      }
    },
    {
      "paper_id": "math.AP.physics/class-ph/2503.10300v1",
      "true_abstract": "We derive a new approach to analyze the coupling of linear Boussinesq and\nSaint-Venant shallow water wave equations in the case where the interface\nremains at a constant position in space. We propose a one-way coupling model as\na reference, which allows us to obtain an analytical solution, prove the\nwell-posedness of the original coupled model and compute what we call the\ncoupling error-a quantity that depends solely on the choice of transmission\nconditions at the interface. We prove that this coupling error is\nasymptotically small for a certain class of data and discuss its role as a\nproxy for the full error with respect to the 3D water wave problem.\nAdditionally, we highlight that this error can be easily computed in other\nscenarios. We show that the coupling error consists of reflected waves and\nargue that this explains some previously unexplained spurious oscillations\nreported in the literature. Finally, we prove the well-posedness of the\nhalf-line linear Boussinesq problem.",
      "generated_abstract": "We study linear Boussinesq-type models that couple with static interfaces.\nThe key feature is the presence of a coupling term between the velocity and\nthe pressure, which is not present in the standard linear Boussinesq model.\nWe provide a rigorous analysis of the linear stability of the coupled system.\nOur approach relies on a direct analysis of the linearized system and on a\nrigorous analysis of the eigenfunctions. We show that the linear problem is\nwell-posed, that there exists a unique positive solution and that the\neigenfunctions are characterized by an energy norm. We also provide a\ncharacterization of the positive solutions of the coupled system in terms of\nthose of the linear model. We establish the existence of a unique positive\nsolution of the coupled system under a local Lipschitz condition on the\ninterface. Finally, we present an algorithm to numerically solve the coupled\nsystem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24742268041237114,
          "p": 0.3157894736842105,
          "f": 0.27745664247251833
        },
        "rouge-2": {
          "r": 0.0625,
          "p": 0.07692307692307693,
          "f": 0.0689655122948874
        },
        "rouge-l": {
          "r": 0.21649484536082475,
          "p": 0.27631578947368424,
          "f": 0.24277456154766294
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/ST/2503.00603v1",
      "true_abstract": "Signature methods have been widely and effectively used as a tool for feature\nextraction in statistical learning methods, notably in mathematical finance.\nThey lack, however, interpretability: in the general case, it is unclear why\nsignatures actually work. The present article aims to address this issue\ndirectly, by introducing and developing the concept of signature perturbations.\nIn particular, we construct a regular perturbation of the signature of the term\nstructure of log prices for various commodities, in terms of the convenience\nyield. Our perturbation expansion and rigorous convergence estimates help\nexplain the success of signature-based classification of commodities markets\naccording to their term structure, with the volatility of the convenience yield\nas the major discriminant.",
      "generated_abstract": "This paper investigates the dynamics of the term structure of commodity\nfutures using the power-law scaling law and its associated signature. We develop\na novel methodology to extract the term structure of the futures market from\nhistorical data. Our findings demonstrate that the term structure is power-law\nin nature, and the power-law exponent is a key indicator of market volatility.\nThis research provides a valuable tool for analyzing the dynamics of the\ncommodity futures market and its implications for market structure and\nfinancial market regulation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16470588235294117,
          "p": 0.2857142857142857,
          "f": 0.2089552192414793
        },
        "rouge-2": {
          "r": 0.046296296296296294,
          "p": 0.07042253521126761,
          "f": 0.05586591700134244
        },
        "rouge-l": {
          "r": 0.15294117647058825,
          "p": 0.2653061224489796,
          "f": 0.19402984610715093
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2502.02397v1",
      "true_abstract": "Many data problems contain some reference or normal conditions, upon which to\ncompare newly collected data. This scenario occurs in data collected as part of\nclinical trials to detect adverse events, or for measuring climate change\nagainst historical norms. The data is typically multivariate, and often the\nnormal ranges are specified by a multivariate normal distribution. The work\npresented in this paper develops methods to compare the new sample against the\nreference distribution with high-dimensional visualisation. It uses a\nprojection pursuit guided tour to produce a sequence of low-dimensional\nprojections steered towards those where the new sample is most different from\nthe reference. A new projection pursuit index is defined for this purpose. The\ntour visualisation also includes drawing of the projected ellipse, which is\ncomputed analytically, corresponding to the reference distribution. The methods\nare implemented in the R package, tourr.",
      "generated_abstract": "on the normality of a multivariate sample is commonly based on the\nnormally distributed sample mean and variance. A common approach to assessing\nwhether the sample follows a multivariate normal distribution is to compare\nthe sample mean and variance to the sample mean and variance of a multivariate\nnormal distribution. However, the sample mean and variance of a multivariate\nnormal distribution are not available, and therefore, this comparison is not\nwell defined. We propose a new projection pursuit index to assess the\nnormality of a sample. The proposed index is based on the sample mean and\nvariance of the sample, and its interpretation is explained. We show that the\nproposed index is equivalent to comparing the sample mean and variance of the\nsample to the sample mean and variance of the multivariate normal distribution.\nWe apply the proposed index to a simulation study and to the 2019 U.S. Census\ndata to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22826086956521738,
          "p": 0.3559322033898305,
          "f": 0.2781456906030438
        },
        "rouge-2": {
          "r": 0.09090909090909091,
          "p": 0.125,
          "f": 0.1052631530193908
        },
        "rouge-l": {
          "r": 0.20652173913043478,
          "p": 0.3220338983050847,
          "f": 0.25165562437787825
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.00945v1",
      "true_abstract": "Deep learning-based computer-aided diagnosis (CAD) of medical images requires\nlarge datasets. However, the lack of large publicly available labeled datasets\nlimits the development of deep learning-based CAD systems. Generative\nAdversarial Networks (GANs), in particular, CycleGAN, can be used to generate\nnew cross-domain images without paired training data. However, most\nCycleGAN-based synthesis methods lack the potential to overcome alignment and\nasymmetry between the input and generated data. We propose a two-stage\ntechnique for the synthesis of abdominal MRI using cross-modality translation\nof abdominal CT. We show that the synthetic data can help improve the\nperformance of the liver segmentation network. We increase the number of\nabdominal MRI images through cross-modality image transformation of unpaired CT\nimages using a CycleGAN inspired deformation invariant network called EssNet.\nSubsequently, we combine the synthetic MRI images with the original MRI images\nand use them to improve the accuracy of the U-Net on a liver segmentation task.\nWe train the U-Net on real MRI images and then on real and synthetic MRI\nimages. Consequently, by comparing both scenarios, we achieve an improvement in\nthe performance of U-Net. In summary, the improvement achieved in the\nIntersection over Union (IoU) is 1.17%. The results show potential to address\nthe data scarcity challenge in medical imaging.",
      "generated_abstract": "mentation is a critical step in the diagnosis and treatment of\nliver diseases. Traditional methods rely on multi-modal data, but the\ndifferences in modalities and modalities make it difficult to achieve the\nexpected results. In this study, we propose a novel cross-modal medical image\nsynthesis (CMIS) framework for liver segmentation. The proposed framework\nintegrates the medical imaging data from chest X-ray (CXR) and computed tomography\n(CT) into the segmentation model, enabling the synthesis of cross-modal medical\nimage data. The CMIS framework combines the cross-modal data with the\nground-truth labels, enhancing the model's capability of generating accurate\nmedical images. The CMIS model was evaluated using two clinical datasets, and\nthe results show that the CMIS framework achieved a mean Dice score of 0",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24390243902439024,
          "p": 0.3614457831325301,
          "f": 0.2912621311108493
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.09401709401709402,
          "f": 0.0723684163177375
        },
        "rouge-l": {
          "r": 0.21138211382113822,
          "p": 0.3132530120481928,
          "f": 0.25242717965453865
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.14934v1",
      "true_abstract": "Molecular docking that predicts the bound structures of small molecules\n(ligands) to their protein targets, plays a vital role in drug discovery.\nHowever, existing docking methods often face limitations: they either overlook\ncrucial structural changes by assuming protein rigidity or suffer from low\ncomputational efficiency due to their reliance on generative models for\nstructure sampling. To address these challenges, we propose FABFlex, a fast and\naccurate regression-based multi-task learning model designed for realistic\nblind flexible docking scenarios, where proteins exhibit flexibility and\nbinding pocket sites are unknown (blind). Specifically, FABFlex's architecture\ncomprises three specialized modules working in concert: (1) A pocket prediction\nmodule that identifies potential binding sites, addressing the challenges\ninherent in blind docking scenarios. (2) A ligand docking module that predicts\nthe bound (holo) structures of ligands from their unbound (apo) states. (3) A\npocket docking module that forecasts the holo structures of protein pockets\nfrom their apo conformations. Notably, FABFlex incorporates an iterative update\nmechanism that serves as a conduit between the ligand and pocket docking\nmodules, enabling continuous structural refinements. This approach effectively\nintegrates the three subtasks of blind flexible docking-pocket identification,\nligand conformation prediction, and protein flexibility modeling-into a\nunified, coherent framework. Extensive experiments on public benchmark datasets\ndemonstrate that FABFlex not only achieves superior effectiveness in predicting\naccurate binding modes but also exhibits a significant speed advantage (208\n$\\times$) compared to existing state-of-the-art methods. Our code is released\nat https://github.com/tmlr-group/FABFlex.",
      "generated_abstract": "king is a powerful technique for automated protein-protein\ninteraction (PPI) prediction. However, it suffers from high computational\nexpense, requiring many molecular dynamics (MD) simulations to complete. In\nthis work, we propose an efficient, accurate, and scalable algorithm for\nautomated flexible docking using a blunderbuss method. Our approach leverages\nthe flexibility of the ligand to enable the prediction of multiple conformers,\nenabling flexible docking. Additionally, we introduce a novel approach to\ndocking using molecular dynamics trajectories to capture the dynamical\nproperties of the ligand. The flexibility of the ligand and the dynamical\nproperties of the ligand are key to our approach. Our method shows promising\nresults on a variety of datasets, with a mean F1-score of 0.82 for the\nC20_C21_C22",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13450292397660818,
          "p": 0.3026315789473684,
          "f": 0.18623481355341026
        },
        "rouge-2": {
          "r": 0.017937219730941704,
          "p": 0.0380952380952381,
          "f": 0.024390239549562053
        },
        "rouge-l": {
          "r": 0.13450292397660818,
          "p": 0.3026315789473684,
          "f": 0.18623481355341026
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/optics/2503.09339v1",
      "true_abstract": "We report a single-frequency, narrow-linewidth semiconductor pulsed laser\nbased on pump current modulation and optical injection locking technique. A\nmonolithic non-planar ring oscillator laser is employed as the seed source to\nguarantee the single-frequency narrow-linewidth performance. Simultaneously,\npulse operation is achieved by directly modulating the pump current of the\nsemiconductor laser. The single-frequency pulsed laser (SFPL) has achieved a\npulse repetition rate of 50 kHz-1 MHz, a pulse duration ranging from 120 ns to\na quasi-continuous state, and a peak power of 160 mW. Moreover, the SFPL has\nreached a pulsed laser linewidth as narrow as 905 Hz, optical spectrum\nsignal-to-noise ratio of better than 65 dB at a center wavelength of 1064.45\nnm. Such extremely narrow-linewidth, repetition-rate and pulse-width tunable\nSFPL has great potential for applications in coherent LIDAR, metrology, remote\nsensing, and nonlinear frequency conversion.",
      "generated_abstract": "We demonstrate the feasibility of sub-kHz pulsed semiconductor laser based on\nNano-Photonic Optical (NPRO) injection locking. A tunable sub-kHz NPRO\ninjection-locked semiconductor laser with 1-2 nm lasing threshold was\ndemonstrated, which is the first demonstration of a tunable sub-kHz NPRO\nlaser in the literature. The laser features a sub-kHz pulse width and\nsub-kHz pulse repetition rate, making it a practical candidate for a variety of\napplications, such as in nanophotonic integrated circuits, photonic integrated\ncircuits, and quantum information applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23232323232323232,
          "p": 0.40350877192982454,
          "f": 0.29487179023422094
        },
        "rouge-2": {
          "r": 0.03787878787878788,
          "p": 0.0684931506849315,
          "f": 0.048780483219036726
        },
        "rouge-l": {
          "r": 0.21212121212121213,
          "p": 0.3684210526315789,
          "f": 0.26923076459319534
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.20745v1",
      "true_abstract": "Exposure to high ambient temperatures is a significant driver of preventable\nmortality, with non-linear health effects and elevated risks in specific\nregions. To capture this complexity and account for spatial dependencies across\nsmall areas, we propose a Bayesian framework that integrates non-linear\nfunctions with the Besag, York, and Mollie (BYM2) model. Applying this\nframework to all-cause mortality data in Switzerland, we quantified spatial\ninequalities in heat-related mortality. We retrieved daily all-cause mortality\nat small areas (2,145 municipalities) for people older than 65 years from the\nSwiss Federal Office of Public Health and daily mean temperature at\n1km$\\times$1km grid from the Swiss Federal Office of Meteorology. By fully\npropagating uncertainties, we derived key epidemiological metrics, including\nheat-related excess mortality and minimum mortality temperature (MMT).\nHeat-related excess mortality rates were higher in northern Switzerland, while\nlower MMTs were observed in mountainous regions. Further, we explored the role\nof the proportion of individuals older than 85 years, green space, average\ntemperature, deprivation, urbanicity, and language regions in explaining these\ndiscrepancies. We found that spatial disparities in heat-related excess\nmortality were primarily driven by population age distribution, green space,\nand vulnerabilities associated with elevated temperature exposure.",
      "generated_abstract": "r presents a novel statistical methodology for modelling the\neffects of spatially varying non-linear temperature-related effects on the\nmortality of a cohort of individuals exposed to heat. We introduce a novel\napproach for modelling the effects of spatially varying non-linear temperature\neffects on mortality. We develop a model for the spatially varying effects of\nthe temperature-related non-linear effects on mortality. We also propose a\nmethod for modelling the non-linear effects of heat exposure using\nspatially varying non-linear effects. The method is illustrated using a\nspatially varying non-linear heat effect on mortality in the United States\nduring 1980-2019. The modeling approach is evaluated using a simulation study\nand a case study. The simulation study illustrates the model's ability to\ncapture the spatial distribution of the non-linear effects of heat expos",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14960629921259844,
          "p": 0.34545454545454546,
          "f": 0.20879120457372308
        },
        "rouge-2": {
          "r": 0.0223463687150838,
          "p": 0.041237113402061855,
          "f": 0.028985502687723888
        },
        "rouge-l": {
          "r": 0.14960629921259844,
          "p": 0.34545454545454546,
          "f": 0.20879120457372308
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2501.17463v1",
      "true_abstract": "We discuss generalized linear models for directional data where the\nconditional distribution of the response is a von Mises-Fisher distribution in\narbitrary dimension or a Bingham distribution on the unit circle. To do this\nproperly, we parametrize von Mises-Fisher distributions by Euclidean parameters\nand investigate computational aspects of this parametrization. Then we modify\nthis approach for local polynomial regression as a means of nonparametric\nsmoothing of distributional data. The methods are illustrated with simulated\ndata and a data set from planetary sciences involving covariate vectors on a\nsphere with axial response.",
      "generated_abstract": "This paper studies the problem of nonparametric smoothing of directional and\naxial data. The nonparametric smoothing methods are based on the concept of\ndirectional and axial principal components. In the paper, the main focus is\non the method of singular value decomposition with applications to the\nnonparametric smoothing of directional and axial data. The singular value\ndecomposition is applied to the problem of nonparametric smoothing of\ndirectional and axial data. The main advantage of the proposed method is the\npreservation of the directional structure in the nonparametric smoothing of\ndirectional and axial data. The paper includes a numerical study of the\nproposed method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23880597014925373,
          "p": 0.38095238095238093,
          "f": 0.2935779769144012
        },
        "rouge-2": {
          "r": 0.07865168539325842,
          "p": 0.1076923076923077,
          "f": 0.09090908603052818
        },
        "rouge-l": {
          "r": 0.22388059701492538,
          "p": 0.35714285714285715,
          "f": 0.2752293530611902
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.03515v1",
      "true_abstract": "We consider Inverse Optimal Stopping (IOS) problem where, based on stopped\nexpert trajectories, one aims to recover the optimal stopping region through\ncontinuation and stopping gain functions approximation. The uniqueness of the\nstopping region allows the use of IOS in real-world applications with safety\nconcerns. While current state-of-the-art inverse reinforcement learning methods\nrecover both a Q-function and the corresponding optimal policy, they fail to\naccount for specific challenges posed by optimal stopping problems. These\ninclude data sparsity near the stopping region, non-Markovian nature of the\ncontinuation gain, a proper treatment of boundary conditions, the need for a\nstable offline approach for risk-sensitive applications, and a lack of a\nquality evaluation metric. These challenges are addressed with the proposed\nDynamics-Aware Offline Inverse Q-Learning for Optimal Stopping (DO-IQS), which\nincorporates temporal information by approximating the cumulative continuation\ngain together with the world dynamics and the Q-function without querying to\nthe environment. Moreover, a confidence-based oversampling approach is proposed\nto treat the data sparsity problem. We demonstrate the performance of our\nmodels on real and artificial data including an optimal intervention for\ncritical events problem.",
      "generated_abstract": "This paper introduces a novel algorithm for the optimization of the\noptimal stopping problem with unknown gain functions. The proposed approach\ncombines the offline optimization of the inverse Q-learning algorithm with\ndynamic programming. The proposed algorithm is able to effectively capture\nthe dynamics of the unknown gain function and its impact on the optimal\nstopping problem. The proposed approach has been implemented on a simulated\ndata and the results show the effectiveness of the proposed algorithm in\naddressing the unknown gain function. The paper also introduces a novel\nalgorithm for the optimization of the optimal stopping problem with unknown\ngain functions. The proposed algorithm is based on the inverse Q-learning\nalgorithm, and the dynamic programming method has been used to capture the\ndynamics of the unknown gain function and its impact on the optimal stopping\nproblem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19491525423728814,
          "p": 0.4423076923076923,
          "f": 0.2705882310477509
        },
        "rouge-2": {
          "r": 0.04046242774566474,
          "p": 0.0875,
          "f": 0.055335964055055094
        },
        "rouge-l": {
          "r": 0.1864406779661017,
          "p": 0.4230769230769231,
          "f": 0.25882352516539797
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.18875v1",
      "true_abstract": "We study competitive equilibria in exchange economies when a continuum of\ngoods is conflated into a finite set of commodities. The design of conflation\nchoices affects the allocation of scarce resources among agents, by\nconstraining trading opportunities and shifting competitive pressures. We\ndemonstrate the consequences on relative prices, trading positions, and\nwelfare.",
      "generated_abstract": "We study the allocation of goods when consumers have heterogeneous preferences\nover good types. In contrast to the traditional setting where goods are\nassigned to consumers at random, we allow the consumer to choose the type of\ngood assigned to her. We model the preference distribution as a non-stationary\nBernoulli distribution, and derive a central limit theorem for the market\nallocations. We provide a simple heuristic for estimating the distribution of\npreferences, and derive an asymptotic approximation for the market allocations.\n  Our model provides a simple framework for studying the allocation of goods\nunder heterogeneous preferences, and we demonstrate its usefulness in\nsimulating large-scale market experiments.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2558139534883721,
          "p": 0.16666666666666666,
          "f": 0.2018348576079456
        },
        "rouge-2": {
          "r": 0.0784313725490196,
          "p": 0.042105263157894736,
          "f": 0.05479451600206456
        },
        "rouge-l": {
          "r": 0.18604651162790697,
          "p": 0.12121212121212122,
          "f": 0.1467889860483126
        }
      }
    },
    {
      "paper_id": "quant-ph.physics/chem-ph/2503.09670v1",
      "true_abstract": "Solving challenging problems in quantum chemistry is one of the leading\npromised applications of quantum computers. Within the quantum algorithms\nproposed for problems in excited state quantum chemistry, subspace-based\nquantum algorithms, including quantum subspace expansion (QSE), quantum\nequation of motion (qEOM) and quantum self-consistent equation-of-motion\n(q-sc-EOM), are promising for pre-fault-tolerant quantum devices. The working\nequation of QSE and qEOM requires solving a generalized eigenvalue equation\nwith associated matrix elements measured on a quantum computer. Our\nquantitative analysis of the QSE method shows that the errors in eigenvalues\nincrease drastically with an increase in the condition number of the overlap\nmatrix when a generalized eigenvalue equation is solved in the presence of\nstatistical sampling errors. This makes such methods unstable to errors that\nare unavoidable when using quantum computers. Further, at very high condition\nnumbers of overlap matrix, the QSE's working equation could not be solved\nwithout any additional steps in the presence of sampling errors as it becomes\nill-conditioned. It was possible to use the thresholding technique in this case\nto solve the equation, but the solutions achieved had missing excited states,\nwhich may be a problem for future chemical studies. We also show that\nexcited-state methods that have an eigenvalue equation as the working equation,\nsuch as q-sc-EOM, do not have such problems and could be suitable candidates\nfor excited-state quantum chemistry calculations using quantum computers.",
      "generated_abstract": "In this work we explore the possibility of generalizing the eigenvalue problem\nfor subspace-based excited state methods to include more than two excited\nstates. We focus on two particular examples of excited states: the ground\nstate and the first excited state. We prove that the solution of this generalized\neigenvalue problem is the solution of the eigenvalue problem for the\n$N$-body Hamiltonian, which can be solved exactly for $N\\geq 3$. We also\ndiscuss the computational complexity of solving the generalized eigenvalue\nproblem in subspace-based excited state methods for $N\\geq 3$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17557251908396945,
          "p": 0.45098039215686275,
          "f": 0.25274724871331966
        },
        "rouge-2": {
          "r": 0.03365384615384615,
          "p": 0.09333333333333334,
          "f": 0.04946996076864519
        },
        "rouge-l": {
          "r": 0.16030534351145037,
          "p": 0.4117647058823529,
          "f": 0.23076922673529765
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.16355v1",
      "true_abstract": "We study monotonicity testing of high-dimensional distributions on\n$\\{-1,1\\}^n$ in the model of subcube conditioning, suggested and studied by\nCanonne, Ron, and Servedio~\\cite{CRS15} and Bhattacharyya and\nChakraborty~\\cite{BC18}. Previous work shows that the \\emph{sample complexity}\nof monotonicity testing must be exponential in $n$ (Rubinfeld,\nVasilian~\\cite{RV20}, and Aliakbarpour, Gouleakis, Peebles, Rubinfeld,\nYodpinyanee~\\cite{AGPRY19}). We show that the subcube \\emph{query complexity}\nis $\\tilde{\\Theta}(n/\\varepsilon^2)$, by proving nearly matching upper and\nlower bounds. Our work is the first to use directed isoperimetric inequalities\n(developed for function monotonicity testing) for analyzing a distribution\ntesting algorithm. Along the way, we generalize an inequality of Khot, Minzer,\nand Safra~\\cite{KMS18} to real-valued functions on $\\{-1,1\\}^n$.\n  We also study uniformity testing of distributions that are promised to be\nmonotone, a problem introduced by Rubinfeld, Servedio~\\cite{RS09} , using\nsubcube conditioning. We show that the query complexity is\n$\\tilde{\\Theta}(\\sqrt{n}/\\varepsilon^2)$. Our work proves the lower bound,\nwhich matches (up to poly-logarithmic factors) the uniformity testing upper\nbound for general distributions (Canonne, Chen, Kamath, Levi,\nWaingarten~\\cite{CCKLW21}). Hence, we show that monotonicity does not help,\nbeyond logarithmic factors, in testing uniformity of distributions with subcube\nconditional queries.",
      "generated_abstract": "In this paper, we consider the problem of testing the hypothesis that a\ndistribution has a specific shape, and propose a novel test for this problem\nwith subcube conditioning. Subcube conditioning, introduced by\nDavenport\\cite{davenport_subcube_2009}, is a generalization of the classical\ntest for the hypothesis that the sample mean lies in a certain convex set. We\nprove that the test under subcube conditioning is monotone with respect to the\nsample size $n$ and the dimension $d$, and we show that it achieves a\nconstant-factor improvement over the classical test. We also show that, in the\nsetting where the distribution has a certain shape, the subcube conditioning\ntest is asymptotically more powerful than the classical test as $n\\to\\infty$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20175438596491227,
          "p": 0.35384615384615387,
          "f": 0.25698323559813996
        },
        "rouge-2": {
          "r": 0.05917159763313609,
          "p": 0.09900990099009901,
          "f": 0.07407406939122115
        },
        "rouge-l": {
          "r": 0.19298245614035087,
          "p": 0.3384615384615385,
          "f": 0.24581005124059802
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.09174v1",
      "true_abstract": "This paper examines the number of communication modes, that is, the degrees\nof freedom (DoF), in a wireless setup comprising a small continuous linear\nintelligent antenna array in the near field of a large one. The framework\nallows for any orientations between the arrays and any positions in a\ntwo-dimensional space assuming that the transmitting array is placed at the\norigin. Therefore, apart from the length of the two continuous arrays, four key\nparameters determine the DoF and are hence considered in the analysis: the\nCartesian coordinates of the center of the receiving array and two angles that\nmodel the rotation of each array around its center. The paper starts with the\ncalculation of the deterministic DoF for a generic geometric setting, which\nextends beyond the widely studied paraxial case. Subsequently, a stochastic\ngeometry framework is proposed to study the statistical DoF, as a first step\ntowards the investigation of the system-level performance in near field\nnetworks. Numerical results applied to millimeter wave networks reveal the\nlarge number of DoF provided by near-field communications and unveiled key\nsystem-level insights.",
      "generated_abstract": "r of free-space optical (FSO) channels is a critical factor in\nthe performance of optical wireless communication systems. The Doppler\nfrequency shift (DFS) effect has been extensively studied in the literature,\nwhere the Doppler shift causes intersymbol interference (ISI) in the channel\ngain. However, the DoF of continuous linear array (CLA) systems is not well\nunderstood. In this paper, we analyze the DoF of CLA systems, focusing on the\nnear-field regime. To address this issue, we first define a new DoF metric and\npropose a new approach to analyze the DoF of CLA systems. We then conduct a\nnumerical study to show that the proposed approach is valid and that the\nresults obtained using our approach can be used to analyze the DoF of CLA\nsystems in the near-field regime. Additionally, we compare the results of our",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19469026548672566,
          "p": 0.26506024096385544,
          "f": 0.22448979103550612
        },
        "rouge-2": {
          "r": 0.023529411764705882,
          "p": 0.034482758620689655,
          "f": 0.027972023150277126
        },
        "rouge-l": {
          "r": 0.1504424778761062,
          "p": 0.20481927710843373,
          "f": 0.17346938287224087
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.10065v1",
      "true_abstract": "This paper is the first to propose valid inference tools, based on\nself-normalization, in time series expected shortfall regressions. In doing so,\nwe propose a novel two-step estimator for expected shortfall regressions which\nis based on convex optimization in both steps (rendering computation easy) and\nit only requires minimization of quantile losses and squared error losses\n(methods for both of which are implemented in every standard statistical\ncomputing package). As a corollary, we also derive self-normalized inference\ntools in time series quantile regressions. Extant methods, based on a bootstrap\nor direct estimation of the long-run variance, are computationally more\ninvolved, require the choice of tuning parameters and have serious size\ndistortions when the regression errors are strongly serially dependent. In\ncontrast, our inference tools only require estimates of the quantile regression\nparameters that are computed on an expanding window and are correctly sized.\nSimulations show the advantageous finite-sample properties of our methods.\nFinally, two applications to stock return predictability and to Growth-at-Risk\ndemonstrate the practical usefulness of the developed inference tools.",
      "generated_abstract": "The shortfall of a time series is a measure of the residual between the\ntime series and a given trend. In the literature, self-normalized regression\n(SNR) is used for this purpose, but it is not well-suited for high-frequency\ntime series with long tails. We propose a self-normalized regression\nalgorithm for time series with high-frequency disturbances. Our approach\nincorporates the quantile regression model for time series and a truncated\nnormal distribution. We provide theoretical guarantees for the accuracy of our\nalgorithm and demonstrate its efficacy through simulations and real-world\napplications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16071428571428573,
          "p": 0.32727272727272727,
          "f": 0.21556885785793695
        },
        "rouge-2": {
          "r": 0.0375,
          "p": 0.0759493670886076,
          "f": 0.05020920059522807
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.2909090909090909,
          "f": 0.19161676204955366
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.08451v1",
      "true_abstract": "Early neural channel coding approaches leveraged dense neural networks with\none-hot encodings to design adaptive encoder-decoder pairs, improving block\nerror rate (BLER) and automating the design process. However, these methods\nstruggled with scalability as the size of message sets and block lengths\nincreased. TurboAE addressed this challenge by focusing on bit-sequence inputs\nrather than symbol-level representations, transforming the scalability issue\nassociated with large message sets into a sequence modeling problem. While\nrecurrent neural networks (RNNs) were a natural fit for sequence processing,\ntheir reliance on sequential computations made them computationally expensive\nand inefficient for long sequences. As a result, TurboAE adopted convolutional\nnetwork blocks, which were faster to train and more scalable, but lacked the\nsequential modeling advantages of RNNs. Recent advances in efficient RNN\narchitectures, such as minGRU and minLSTM, and structured state space models\n(SSMs) like S4 and S6, overcome these limitations by significantly reducing\nmemory and computational overhead. These models enable scalable sequence\nprocessing, making RNNs competitive for long-sequence tasks. In this work, we\nrevisit RNNs for Turbo autoencoders by integrating the lightweight minGRU model\nwith a Mamba block from SSMs into a parallel Turbo autoencoder framework. Our\nresults demonstrate that this hybrid design matches the performance of\nconvolutional network-based Turbo autoencoder approaches for short sequences\nwhile significantly improving scalability and training efficiency for long\nblock lengths. This highlights the potential of efficient RNNs in advancing\nneural channel coding for long-sequence scenarios.",
      "generated_abstract": "oencoders (TAE) have gained widespread attention due to their\nability to reduce training cost and improve model performance. However,\nexisting TAE frameworks often struggle with the encoding process, which\nrequires complex encoding strategies, such as masking and tokenization. In\naddition, existing TAE frameworks often suffer from low reconstruction\nperformance, which limits their practical applications. In this paper, we\npropose a novel encoder-decoder framework, MinGRU-Based Encoder for\nTurbo-Autoencoder (MiEGA), which utilizes a minimal GRU recurrent unit (MinGRU)\nto directly encode frame-level features into low-dimensional latent codes. By\nusing the MinGRU as the encoder, the MiEGA framework significantly reduces the\nencoding complexity, while maintaining the encoding performance. Furthermore,\nwe introduce a novel masking",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.2823529411764706,
          "f": 0.19917011991529085
        },
        "rouge-2": {
          "r": 0.013392857142857142,
          "p": 0.028846153846153848,
          "f": 0.01829267859607479
        },
        "rouge-l": {
          "r": 0.1346153846153846,
          "p": 0.24705882352941178,
          "f": 0.17427385435512488
        }
      }
    },
    {
      "paper_id": "math-ph.math/SP/2502.17290v1",
      "true_abstract": "The two-dimensional magnetic Laplacian is considered. We calculate the\nleading term of the splitting between the first two eigenvalues of the operator\nin the semiclassical limit under the assumption that the magnetic field does\nnot vanish and has two symmetric magnetic wells with respect to the coordinate\naxes. This is the first result of quantum tunneling between purely magnetic\nwells under generic assumptions. The proof, which strongly relies on microlocal\nanalysis, reveals a purely magnetic Agmon distance between the wells.\nSurprisingly, it is discovered that the exponential decay of the eigenfunctions\naway from the magnetic wells is not crucial to derive the tunneling formula.\nThe key is a microlocal exponential decay inside the characteristic manifold,\nwith respect to the variable quantizing the classical center guide motion.",
      "generated_abstract": "In this paper, we study the problem of tunneling between magnetic wells in\ntwo dimensions. We first give a rigorous definition of the problem and\ndiscuss the existence of the corresponding energy level. Next, we show that\nthe problem is equivalent to the study of tunneling between two magnetic\nwells in one dimension. We then introduce a method to obtain an asymptotic\nexpansion of the tunneling probability, and use it to prove the existence of\nthe corresponding energy level. We also show that the problem is not\nwell-defined in the general case of magnetic wells with nontrivial dependence on\nthe boundary conditions. Finally, we give a simple example of two magnetic\nwells in two dimensions, and show that the problem is ill-defined in this case.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.3064516129032258,
          "f": 0.2753623138920396
        },
        "rouge-2": {
          "r": 0.08035714285714286,
          "p": 0.09375,
          "f": 0.08653845656804764
        },
        "rouge-l": {
          "r": 0.21052631578947367,
          "p": 0.25806451612903225,
          "f": 0.23188405302247436
        }
      }
    },
    {
      "paper_id": "eess.SP.cs/SD/2503.09349v1",
      "true_abstract": "Correlation-based auditory attention decoding (AAD) algorithms exploit neural\ntracking mechanisms to determine listener attention among competing speech\nsources via, e.g., electroencephalography signals. The correlation coefficients\nbetween the decoded neural responses and encoded speech stimuli of the\ndifferent speakers then serve as AAD decision variables. A critical trade-off\nexists between the temporal resolution (the decision window length used to\ncompute these correlations) and the AAD accuracy. This trade-off is typically\ncharacterized by evaluating AAD accuracy across multiple window lengths,\nleading to the performance curve. We propose a novel method to model this\ntrade-off curve using labeled correlations from only a single decision window\nlength. Our approach models the (un)attended correlations with a normal\ndistribution after applying the Fisher transformation, enabling accurate AAD\naccuracy prediction across different window lengths. We validate the method on\ntwo distinct AAD implementations: a linear decoder and the non-linear VLAAI\ndeep neural network, evaluated on separate datasets. Results show consistently\nlow modeling errors of approximately 2 percent points, with 94% of true\naccuracies falling within estimated 95%-confidence intervals. The proposed\nmethod enables efficient performance curve modeling without extensive\nmulti-window length evaluation, facilitating practical applications in, e.g.,\nperformance tracking in neuro-steered hearing devices to continuously adapt the\nsystem parameters over time.",
      "generated_abstract": "coding of auditory attention to speech has attracted significant\nresearch interest in the last few years. However, its performance remains\nuncertain, especially in non-stationary conditions. This study proposes a\nframework for performance modeling and optimization of neural decoding of\nauditory attention to speech. It employs correlation-based neural decoding,\nwhich can detect both short-term and long-term effects of speech on auditory\nattention. We first describe the neural decoding framework, including a\npre-training stage and a training stage. The former is designed to extract\ninformation from a large number of auditory attention to speech decodings\ngenerated by the latter. This allows us to estimate the performance of the\ndecoder on the test data, which can then be used for modeling the performance\nof the decoder. We conduct performance modeling experiments for different\nspeech-to-speech and speech-to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1610738255033557,
          "p": 0.2926829268292683,
          "f": 0.20779220321283343
        },
        "rouge-2": {
          "r": 0.015228426395939087,
          "p": 0.02586206896551724,
          "f": 0.01916932440833438
        },
        "rouge-l": {
          "r": 0.14093959731543623,
          "p": 0.25609756097560976,
          "f": 0.18181817723880747
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.01376v1",
      "true_abstract": "Structure-Based Drug Design (SBDD) has revolutionized drug discovery by\nenabling the rational design of molecules for specific protein targets. Despite\nsignificant advancements in improving docking scores, advanced 3D-SBDD\ngenerative models still face challenges in producing drug-like candidates that\nmeet medicinal chemistry standards and pharmacokinetic requirements. These\nlimitations arise from their inherent focus on molecular interactions, often\nneglecting critical aspects of drug-likeness. To address these shortcomings, we\nintroduce the Collaborative Intelligence Drug Design (CIDD) framework, which\ncombines the structural precision of 3D-SBDD models with the chemical reasoning\ncapabilities of large language models (LLMs). CIDD begins by generating\nsupporting molecules with 3D-SBDD models and then refines these molecules\nthrough LLM-supported modules to enhance drug-likeness and structural\nreasonability. When evaluated on the CrossDocked2020 dataset, CIDD achieved a\nremarkable success ratio of 37.94%, significantly outperforming the previous\nstate-of-the-art benchmark of 15.72%. Although improving molecular interactions\nand drug-likeness is often seen as a trade-off, CIDD uniquely achieves a\nbalanced improvement in both by leveraging the complementary strengths of\ndifferent models, offering a robust and innovative pathway for designing\ntherapeutically promising drug candidates.",
      "generated_abstract": "-based drug design (SBDD) has emerged as a powerful and efficient\nmethod for identifying small molecules that bind to biologically relevant\ntargets. However, the current landscape of SBDD tools is dominated by\nclassical, rule-based approaches that often struggle to capture the complex\ninteractions between target and ligand. To address this challenge, we introduce\na novel computational framework based on Large Language Models (LLMs) that\nintegrates multi-step, context-aware reasoning into SBDD pipelines. Our model\nenables users to define complex drug-like molecules using simple textual\ndescriptions and provides a systematic, step-by-step methodology for\noptimizing molecular design. The framework is demonstrated through a series of\ncase studies that demonstrate the effectiveness of LLM-based SBDD approaches\nin achieving better hit-to-lead (H2",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21804511278195488,
          "p": 0.30851063829787234,
          "f": 0.2555066030771023
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.034482758620689655,
          "f": 0.027397255485082463
        },
        "rouge-l": {
          "r": 0.17293233082706766,
          "p": 0.24468085106382978,
          "f": 0.20264316695375428
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2501.13143v1",
      "true_abstract": "We introduce a model-free preference under ambiguity, as a primitive trait of\nbehavior, which we apply once as well as repeatedly. Its single and double\napplication yield simple, easily interpretable definitions of ambiguity\naversion and ambiguity prudence. We derive their implications within canonical\nmodels for decision under risk and ambiguity. We establish in particular that\nour new definition of ambiguity prudence is equivalent to a positive third\nderivative of: (i) the capacity in the Choquet expected utility model, (ii) the\ndual conjugate of the divergence function under variational divergence\npreferences and (iii) the ambiguity attitude function in the smooth ambiguity\nmodel. We show that our definition of ambiguity prudent behavior may be\nnaturally linked to an optimal insurance problem under ambiguity.",
      "generated_abstract": "This study examines the psychological and behavioral effects of\nhigher-order ambiguity attitudes, such as those found in the literature on\nambiguity aversion. We tested whether these attitudes can be explained using\nexisting theories of ambiguity aversion and ambiguity avoidance. Results show\nthat ambiguity avoidance is associated with greater psychological and\nbehavioral resistance to ambiguity, but ambiguity aversion is associated with\ngreater psychological and behavioral preference for ambiguity. These findings\nreinforce the need for a more nuanced understanding of ambiguity attitudes.\n  Our findings suggest that ambiguity attitudes may be more complex than\nconventional theories have assumed, with higher-order attitudes possibly\nproviding a more nuanced understanding of ambiguity aversion.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21686746987951808,
          "p": 0.2857142857142857,
          "f": 0.24657533755957972
        },
        "rouge-2": {
          "r": 0.061946902654867256,
          "p": 0.08139534883720931,
          "f": 0.07035175388601331
        },
        "rouge-l": {
          "r": 0.20481927710843373,
          "p": 0.2698412698412698,
          "f": 0.23287670742259345
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.01296v1",
      "true_abstract": "The separating Noether number $\\beta_{\\mathrm{sep}}(G)$ of a finite group $G$\nis the minimal positive integer $d$ such that for every $G$-module $V$ there is\na separating set of degree $\\leq d$. In this manuscript, we investigate the\nseparating Noether number $\\beta_{\\mathrm{sep}}(G)$. Among others, we obtain\nthe exact value of $\\beta_{\\mathrm{sep}}(G)$ for finite abelian groups $G$,\nwhen either $G$ is a $p$-group or $\\mathsf r(G)\\in \\{3,5\\}$.",
      "generated_abstract": "We prove that the separating Noether number of a finite abelian group $A$\nis equal to the sum of the numbers of primes dividing the orders of the\nelements of the group. Moreover, we show that this number coincides with the\nnumber of irreducible factors of the characteristic polynomial of any element\nof the group.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2765957446808511,
          "p": 0.38235294117647056,
          "f": 0.320987649449779
        },
        "rouge-2": {
          "r": 0.1016949152542373,
          "p": 0.125,
          "f": 0.11214952776312365
        },
        "rouge-l": {
          "r": 0.23404255319148937,
          "p": 0.3235294117647059,
          "f": 0.2716049334003964
        }
      }
    },
    {
      "paper_id": "physics.chem-ph.physics/chem-ph/2503.10038v1",
      "true_abstract": "We synthesize MoS$_{2}$ atomic layer flakes at different growth conditions to\ntailor S-terminated and Mo-terminated edge defect states that are investigated\nfor their ferromagnetic response. We leverage quantum weak measurement\nprinciples to construct a spin Hall effect of light-based magneto-optic Kerr\neffect (SHEL-MOKE) setup to sense the ultra-small magnetic response from the\nsynthesized atomic layers. Our findings demonstrate that Mo-terminated edge\nstates are the primary source of ferromagnetic response from MoS$_{2}$ flakes,\nwhich is consistent with X-ray photoelectron, Raman and photoluminescence\nspectroscopic results. In the process, we demonstrate SHEL-MOKE to be a robust\ntechnique to investigate ultra weak properties in novel atomic-scale materials.",
      "generated_abstract": "transition metal dichalcogenides (TMDs) are an emerging family of\nmaterials with interesting electronic properties. While the bulk materials\nexhibit metallic behavior, the surface states of the monolayer TMDs, including\nMoS$_{2}$, can exhibit ferromagnetic (FM) or antiferromagnetic (AFM)\nproperties. The surface state of MoS$_{2}$ can be distinguished by the\nintrinsic orbital character, which can be classified as $p$, $d$, or $t_{2g}$\norbitals. The surface states of MoS$_{2}$ exhibit a rich and complex character\ndue to the multiband character and the localized $p$ orbitals, which are\nresponsible for the FM behavior. However, the understanding of the FM\ncharacteristics of the surface states of MoS$_{2}$ is still limited.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20512820512820512,
          "p": 0.2318840579710145,
          "f": 0.2176870698486743
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.19230769230769232,
          "p": 0.21739130434782608,
          "f": 0.20408162767180354
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2502.14228v1",
      "true_abstract": "The umbilical cord plays a critical role in delivering nutrients and oxygen\nfrom the placenta to the fetus through the umbilical vein, while the two\numbilical arteries carry deoxygenated blood with waste products back to the\nplacenta. Although solute exchange in the placenta has been extensively\nstudied, exchange within the cord tissue has not been investigated. Here, we\nexplore the hypothesis that the coiled structure of the umbilical cord could\nstrengthen diffusive coupling between the arteries and the vein, resulting in a\nfunctional shunt. We calculate the diffusion of solutes, such as oxygen, and\nheat in the umbilical cord to quantify how this shunt is affected by vascular\nconfiguration within the cord. We demonstrate that the shunt is enhanced by\ncoiling and vessel proximity. Furthermore, our model predicts that typical\nvascular configurations of the human cord tend to minimise shunting, which\ncould otherwise disrupt thermal regulation of the fetus. We also show that the\nexchange, amplified by coiling, can provide additional oxygen supply to the\ncord tissue surrounding the umbilical vessels.",
      "generated_abstract": "ical cord plays a vital role in neonatal circulation and\nexchanges nutrients, oxygen and carbon dioxide between the fetus and\nmaternal circulation. Despite its importance, the role of the umbilical\ncord-based exchange shunt in neonatal circulation is poorly understood. We\nshowed previously that the umbilical cord-based exchange shunt is functionally\npresent, and coiling is a crucial step in its assembly. Here, we tested the\nassumption that coiling is the sole step in the assembly of the exchange\nshunt, and we measured the effect of coiling on heat and solute transfer. We\nshowed that coiling is required for the assembly of the exchange shunt, and we\nshowed that coiling in the umbilical cord has a limited effect on heat and\nsolute transfer. We also showed that the effect of coiling on heat and sol",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23300970873786409,
          "p": 0.41379310344827586,
          "f": 0.2981366413533429
        },
        "rouge-2": {
          "r": 0.08666666666666667,
          "p": 0.13541666666666666,
          "f": 0.10569105215149734
        },
        "rouge-l": {
          "r": 0.22330097087378642,
          "p": 0.39655172413793105,
          "f": 0.2857142811048957
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2405.20522v1",
      "true_abstract": "This project is a collaboration between industry and academia to delve into\nFinance Social Networks, specifically the Board of Directors of public\ncompanies. Knowing the connections between Directors and Executives in\ndifferent companies can generate powerful stories and meaningful insights on\ninvestments. A proof of concept in the form of a Data Visualization tool\nreveals its strength in investigating corporate governance and sustainability,\nas well as in the partnership between industry and academic institutions.",
      "generated_abstract": "of socially responsible investing (SRI) has made it increasingly\nimportant to understand how companies are governed and how their boards\nmaintain control over executives. This study focuses on the board of directors\n(BoD) as a key element of corporate governance, and aims to provide a\ncomprehensive visualization of the connections between executives and BoD\nmembers. We conduct an analysis of 110,000 corporate directors across 5,300\ncompanies, and identify patterns of executive and BoD connection. Our results\nshow that directors tend to have strong connections with other executives and\nBoD members, and that the strength of these connections varies by company size.\nWe also find that the BoD is more likely to have connections with other BoD\nmembers, and that the level of CEO independence has a significant impact on\nBo",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25925925925925924,
          "p": 0.16666666666666666,
          "f": 0.20289854596093268
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.01694915254237288,
          "f": 0.021276591070621218
        },
        "rouge-l": {
          "r": 0.2222222222222222,
          "p": 0.14285714285714285,
          "f": 0.1739130387145559
        }
      }
    },
    {
      "paper_id": "cs.CL.econ/GN/2412.20438v1",
      "true_abstract": "The financial sector, a pivotal force in economic development, increasingly\nuses the intelligent technologies such as natural language processing to\nenhance data processing and insight extraction. This research paper through a\nreview process of the time span of 2018-2023 explores the use of text mining as\nnatural language processing techniques in various components of the financial\nsystem including asset pricing, corporate finance, derivatives, risk\nmanagement, and public finance and highlights the need to address the specific\nproblems in the discussion section. We notice that most of the research\nmaterials combined probabilistic with vector-space models, and text-data with\nnumerical ones. The most used technique regarding information processing is the\ninformation classification technique and the most used algorithms include the\nlong-short term memory and bidirectional encoder models. The research noticed\nthat new specific algorithms are developed and the focus of the financial\nsystem is mainly on asset pricing component. The research also proposes a path\nfrom engineering perspective for researchers who need to analyze financial\ntext. The challenges regarding text mining perspective such as data quality,\ncontext-adaption and model interpretability need to be solved so to integrate\nadvanced natural language processing models and techniques in enhancing\nfinancial analysis and prediction. Keywords: Financial System (FS), Natural\nLanguage Processing (NLP), Software and Text Engineering, Probabilistic,\nVector-Space, Models, Techniques, TextData, Financial Analysis.",
      "generated_abstract": "cial sector is a crucial sector for economic development,\nsignificant in terms of both economic growth and financial stability.\nHowever, the financial sector is susceptible to cyberattacks, fraud, and other\nadversarial activities, posing a significant threat to its integrity and\neffectiveness. The financial sector also faces challenges in integrating\nvarious technologies, including natural language processing (NLP), machine\nlearning (ML), and artificial intelligence (AI), into its operations. These\nchallenges can be overcome by integrating these technologies through\ncombining them into a unified framework that provides a holistic view of the\nfinancial sector. This paper explores the integration of NLP techniques into the\nfinancial sector, focusing on the integration of text mining and natural\nlanguage processing techniques. This paper explores the potential of text\nmining and natural language processing techniques in enhancing the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2426470588235294,
          "p": 0.4125,
          "f": 0.3055555508916324
        },
        "rouge-2": {
          "r": 0.06565656565656566,
          "p": 0.11711711711711711,
          "f": 0.08414239021836832
        },
        "rouge-l": {
          "r": 0.22794117647058823,
          "p": 0.3875,
          "f": 0.2870370323731139
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.02349v1",
      "true_abstract": "Sequential change-point detection for time series is widely used in data\nmonitoring in practice. In this work, we focus on sequential change-point\ndetection on high-order compositional time series models. Under the regularity\nconditions, we prove that a process following the generalized Beta AR(p) model\nwith exogenous variables is stationary and ergodic. We develop a nonparametric\nsequential change-point detection method for the generalized Beta AR(p) model,\nwhich does not rely on any strong assumptions about the sources of the change\npoints. We show that the power of the test converges to one given that the\namount of initial observations is large enough. We apply the nonparametric\nmethod to a rate of automobile crashes with alcohol involved, which is recorded\nmonthly from January 2010 to December 2020; the exogenous variable is the price\nlevel of alcoholic beverages, which has a change point around August 2019. We\nfit a generalized Beta AR(p) model to the crash rate sequence, and we use the\nnonparametric sequential change-point detection method to successfully detect\nthe change point.",
      "generated_abstract": "aper, we consider a novel sequential change-point detection\nmethodology based on the nonparametric theory of high-order composite\ntime-series models with exogenous variables. The proposed methodology,\nsequential change-point detection on high-order composite time-series models\nwith exogenous variables, is based on the theory of high-order composite time\nseries models with exogenous variables developed by the author. The model\nframework is based on the framework of nonparametric change-point detection on\nhigh-order composite time series models with exogenous variables. The key\ndifference between this paper and previous studies is that the model framework\nin this paper is more flexible than the model framework in previous studies.\nIn the model framework in previous studies, the exogenous variables are\nassumed to be constant over time, which is not the case in this paper. The\nmodel framework in this paper is based on",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26,
          "p": 0.4406779661016949,
          "f": 0.3270440204896959
        },
        "rouge-2": {
          "r": 0.08053691275167785,
          "p": 0.1411764705882353,
          "f": 0.10256409793812571
        },
        "rouge-l": {
          "r": 0.26,
          "p": 0.4406779661016949,
          "f": 0.3270440204896959
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/QM/2503.02685v1",
      "true_abstract": "Precise parcellation of functional networks (FNs) of early developing human\nbrain is the fundamental basis for identifying biomarker of developmental\ndisorders and understanding functional development. Resting-state fMRI\n(rs-fMRI) enables in vivo exploration of functional changes, but adult FN\nparcellations cannot be directly applied to the neonates due to incomplete\nnetwork maturation. No standardized neonatal functional atlas is currently\navailable. To solve this fundamental issue, we propose TReND, a novel and fully\nautomated self-supervised transformer-autoencoder framework that integrates\nregularized nonnegative matrix factorization (RNMF) to unveil the FNs in\nneonates. TReND effectively disentangles spatiotemporal features in voxel-wise\nrs-fMRI data. The framework integrates confidence-adaptive masks into\ntransformer self-attention layers to mitigate noise influence. A self\nsupervised decoder acts as a regulator to refine the encoder's latent\nembeddings, which serve as reliable temporal features. For spatial coherence,\nwe incorporate brain surface-based geodesic distances as spatial encodings\nalong with functional connectivity from temporal features. The TReND clustering\napproach processes these features under sparsity and smoothness constraints,\nproducing robust and biologically plausible parcellations. We extensively\nvalidated our TReND framework on three different rs-fMRI datasets: simulated,\ndHCP and HCP-YA against comparable traditional feature extraction and\nclustering techniques. Our results demonstrated the superiority of the TReND\nframework in the delineation of neonate FNs with significantly better spatial\ncontiguity and functional homogeneity. Collectively, we established TReND, a\nnovel and robust framework, for neonatal FN delineation. TReND-derived neonatal\nFNs could serve as a neonatal functional atlas for perinatal populations in\nhealth and disease.",
      "generated_abstract": "brain development is characterized by rapid, dynamic, and\nrepeating patterns of neural activity. These patterns emerge from complex\ninteractions between the developing brain and the environment, and are\ncritical for establishing the foundation for later neurodevelopment.\nIdentifying these patterns is essential for understanding the neural basis of\nneonatal brain development and for guiding interventions aimed at supporting\nneuroplasticity. While high-dimensional functional neuroimaging data have\nprovided valuable insights into neonatal brain development, interpreting the\nfindings from these data has been challenging. We present TReND, a\nTransformer-based approach for deriving high-dimensional features from\nneonatal functional neuroimaging data. These features are used to train a\nregularized NMF-based classifier for neonatal functional network delineation.\nThe proposed method is implemented in a Python package",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16265060240963855,
          "p": 0.3375,
          "f": 0.21951219073302933
        },
        "rouge-2": {
          "r": 0.01293103448275862,
          "p": 0.02654867256637168,
          "f": 0.017391299942702228
        },
        "rouge-l": {
          "r": 0.1566265060240964,
          "p": 0.325,
          "f": 0.21138210943221636
        }
      }
    },
    {
      "paper_id": "math.ST.econ/EM/2502.20917v1",
      "true_abstract": "We examine the location characteristics of a conditional selective confidence\ninterval based on the polyhedral method. This interval is constructed from the\ndistribution of a test statistic conditional upon the event of statistical\nsignificance. In the case of a one-sided test, the behavior of the interval\nvaries depending on whether the parameter is highly significant or only\nmarginally significant. When the parameter is highly significant, the interval\nis similar to the usual confidence interval derived without considering\nselection. However, when the parameter is only marginally significant, the\ninterval falls into an extreme range and deviates greatly from the estimated\nvalue of the parameter. In contrast, an interval conditional on two-sided\nsignificance does not yield extreme results, although it may exclude the\nestimated parameter value.",
      "generated_abstract": "The location of confidence intervals for conditional means is of significant\ninterest. This study addresses this problem through the application of polyhedral\nmethods. Specifically, the problem is formulated as a convex optimization\nproblem. We then establish the conditions for the convexity of the objective\nfunction, and prove that the resulting polyhedral method is a convex\ncombination of the convex polyhedral method and a generalized Gaussian\nregression. The proposed method is compared with a Gaussian regression method\nand the conventional confidence interval method. The results of simulation\nstudies demonstrate that the proposed method has a lower variance than the\nconventional confidence interval method and a lower bias than the Gaussian\nregression method. In addition, the proposed method has a lower variance than\nthe conventional confidence interval method and a lower bias than the Gaussian\nregression method under the same confidence level. This study provides a new\napproach for the location of conditional confidence intervals.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2112676056338028,
          "p": 0.23076923076923078,
          "f": 0.22058823030384958
        },
        "rouge-2": {
          "r": 0.03773584905660377,
          "p": 0.03669724770642202,
          "f": 0.03720929732655557
        },
        "rouge-l": {
          "r": 0.2112676056338028,
          "p": 0.23076923076923078,
          "f": 0.22058823030384958
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/PM/2412.11019v1",
      "true_abstract": "The domain of hedge fund investments is undergoing significant\ntransformation, influenced by the rapid expansion of data availability and the\nadvancement of analytical technologies. This study explores the enhancement of\nhedge fund investment performance through the integration of machine learning\ntechniques, the application of PolyModel feature selection, and the analysis of\nfund size. We address three critical questions: (1) the effect of machine\nlearning on trading performance, (2) the role of PolyModel feature selection in\nfund selection and performance, and (3) the comparative reliability of larger\nversus smaller funds.\n  Our findings offer compelling insights. We observe that while machine\nlearning techniques enhance cumulative returns, they also increase annual\nvolatility, indicating variability in performance. PolyModel feature selection\nproves to be a robust strategy, with approaches that utilize a comprehensive\nset of features for fund selection outperforming more selective methodologies.\nNotably, Long-Term Stability (LTS) effectively manages portfolio volatility\nwhile delivering favorable returns. Contrary to popular belief, our results\nsuggest that larger funds do not consistently yield better investment outcomes,\nchallenging the assumption of their inherent reliability.\n  This research highlights the transformative impact of data-driven approaches\nin the hedge fund investment arena and provides valuable implications for\ninvestors and asset managers. By leveraging machine learning and PolyModel\nfeature selection, investors can enhance portfolio optimization and reassess\nthe dependability of larger funds, leading to more informed investment\nstrategies.",
      "generated_abstract": "This paper introduces a novel approach for hedge fund portfolio construction\nusing machine learning. The proposed method, PolyModel, is a combination of\nPolyFactor and RiskMetric. By incorporating these factors, PolyModel\naccelerates the hedge fund portfolio construction process by reducing the\nnumber of risk factors to be analyzed from 45 to 13. PolyModel is also\nefficient in handling data, with a computation time of 0.45 seconds per\ndata point on a CPU. We apply PolyModel to a dataset of 2026 hedge funds,\nobtaining a portfolio return of 3.35% with an average volatility of 4.34%.\nFurthermore, we show that PolyModel outperforms existing hedge fund portfolio\nconstruction methods, including the state-of-the-art PolyFactor method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1875,
          "p": 0.32926829268292684,
          "f": 0.23893804847364714
        },
        "rouge-2": {
          "r": 0.019417475728155338,
          "p": 0.03773584905660377,
          "f": 0.025641021154668764
        },
        "rouge-l": {
          "r": 0.1875,
          "p": 0.32926829268292684,
          "f": 0.23893804847364714
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.07327v1",
      "true_abstract": "Multilinear Principal Component Analysis (MPCA) is an important tool for\nanalyzing tensor data. It performs dimension reduction similar to PCA for\nmultivariate data. However, standard MPCA is sensitive to outliers. It is\nhighly influenced by observations deviating from the bulk of the data, called\ncasewise outliers, as well as by individual outlying cells in the tensors,\nso-called cellwise outliers. This latter type of outlier is highly likely to\noccur in tensor data, as tensors typically consist of many cells. This paper\nintroduces a novel robust MPCA method that can handle both types of outliers\nsimultaneously, and can cope with missing values as well. This method uses a\nsingle loss function to reduce the influence of both casewise and cellwise\noutliers. The solution that minimizes this loss function is computed using an\niteratively reweighted least squares algorithm with a robust initialization.\nGraphical diagnostic tools are also proposed to identify the different types of\noutliers that have been found by the new robust MPCA method. The performance of\nthe method and associated graphical displays is assessed through simulations\nand illustrated on two real datasets.",
      "generated_abstract": "Multilinear principal component analysis is a powerful tool for analyzing\nmultivariate data with linear structure. However, its use in high-dimensional\nsettings is challenging due to the presence of multilinear dependence. To\naddress this challenge, we propose an algorithm that incorporates a robust\nmultilinear PCA (RMCPCA) framework. The key idea is to use a sparse multilinear\nregression model to address multilinear dependence and then utilize the\nrobust multilinear PCA algorithm to obtain robust estimates of the\nmultilinear principal components. Theoretically, we establish the consistency\nand asymptotic normality of our proposed algorithm under mild assumptions.\nNumerical simulations are conducted to demonstrate the performance of our\nproposed methodology.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.3611111111111111,
          "f": 0.2751322704157219
        },
        "rouge-2": {
          "r": 0.03488372093023256,
          "p": 0.06060606060606061,
          "f": 0.04428043816723677
        },
        "rouge-l": {
          "r": 0.19658119658119658,
          "p": 0.3194444444444444,
          "f": 0.24338623866969017
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/SC/2307.10289v1",
      "true_abstract": "This is the second part of the previous review. In the previous review we\nsuspected that Orai3 channels were involved in lung cancer and more precisely\nin several cancers. Here we confirm that calcium dysregulation is important for\ncancer development. in this paper we show that Orai3 is an upstream activator\nof AKT and we prove that AKT is involved in chemoresistance in NSCLC.",
      "generated_abstract": "d: Chemoresistance is a major challenge in cancer treatment. The\nChemokine Receptor Orai3 (Orai3) plays a crucial role in mediating Ca2+-dependent\nion currents and has been reported to play a crucial role in the response to\nchemotherapeutic drugs. Orai3 is expressed in multiple types of cancer cells\nand plays an important role in the response to various types of chemotherapeutic\ndrugs. However, the role of Orai3 in chemoresistance to cisplatin in NSCLC is\nstill not fully understood. Objective: This study aimed to investigate the\nrole of Orai3 in chemoresistance to cisplatin in NSCLC. Methods: The\nexpression of Orai3 in NSCLC cells was examined using Western blotting. The\nin vitro cisplatin resistance of NSCLC cells was investigated using",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2926829268292683,
          "p": 0.1935483870967742,
          "f": 0.23300970394570655
        },
        "rouge-2": {
          "r": 0.05084745762711865,
          "p": 0.03225806451612903,
          "f": 0.039473679460700024
        },
        "rouge-l": {
          "r": 0.24390243902439024,
          "p": 0.16129032258064516,
          "f": 0.19417475248939592
        }
      }
    },
    {
      "paper_id": "physics.ed-ph.physics/ed-ph/2502.15781v1",
      "true_abstract": "Digital innovation in education has revolutionized teaching and learning\nprocesses, demanding a rethink of pedagogical competence among educators. This\nstudy evaluates the preparation of instructors to use digital technologies into\ntheir educational practices. The study used a mixed-methods approach,\nintegrating both qualitative interviews and quantitative surveys to evaluate\nteachers' institutional support systems, beliefs, and technical proficiency.\nThe results show that even while a large number of educators acknowledge the\nbenefits of digital tools, problems including poor professional development and\nchange aversion still exist. In order to improve digital pedagogical\npreparation, the study emphasizes the necessity of focused training initiatives\nand encouraging institutional regulations. There is discussion on the\nimplications for educational institutions and policymakers.",
      "generated_abstract": "This study investigates pedagogical readiness in undergraduate science\nstudents for the implementation of digital tools in the classroom. We conducted\na mixed-methods study using an online survey and a focus group with students\nwho had recently implemented a digital tool in their coursework. Our findings\nsuggest that students were prepared for the implementation of digital tools,\nbut the majority of them lacked the skills and confidence to implement the\ntools effectively. Furthermore, students reported that the implementation\nprocess was stressful and challenging, with many citing the lack of support\nfrom faculty members as a barrier to success. The study offers insights into\nhow educators can support students in developing the skills and confidence to\nengage with digital tools in the classroom, and provides strategies for\nfaculty members to enhance student learning in digital science courses.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22727272727272727,
          "p": 0.24096385542168675,
          "f": 0.23391812365924566
        },
        "rouge-2": {
          "r": 0.04424778761061947,
          "p": 0.04132231404958678,
          "f": 0.04273503774088743
        },
        "rouge-l": {
          "r": 0.20454545454545456,
          "p": 0.21686746987951808,
          "f": 0.21052631079374862
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.astro-ph/CO/2503.09769v1",
      "true_abstract": "The Standard Model of cosmology, $\\Lambda$CDM, while enormously successful,\nis currently unable to account for several cosmological anomalies the most\nprominent of which are in the measurements of the Hubble parameter and $S_8$.\nAdditionally, the inclusion of the cosmological constant is theoretically\nunappealing. This has lead to extensions of the model such as the use of fluid\nequations for interacting dark matter and dark energy which, however, are ad\nhoc since they do not appear to arise from a Lagrangian. Recently, we have\nproposed $\\mathcal{Q}_{\\rm CDM}$ as an alternative to $\\Lambda$CDM which is a\ndynamical model of a quintessence field interacting with dark matter within a\nfield theoretic approach. In this approach, we analyze the effect of the dark\nmatter mass, the dark matter-dark energy interaction strength and the dark\nmatter self-interaction on the cosmological parameters. Further, within\n$\\mathcal{Q}_{\\rm CDM}$ we investigate the possible alleviation of the Hubble\ntension and the $S_8$ anomaly and the nature of dark energy.",
      "generated_abstract": "hcal{Q}_{\\rm_CDM}$ model is a modified version of the $\\Lambda$CDM\nmodel, where the ratio of the dark energy density and the matter density is\nexpressed as a function of the Hubble time $H(t)$ rather than as a constant.\nThis model avoids the cosmological constant and has the advantage of\naccommodating the accelerated expansion of the Universe. However, it faces\ncosmological tensions with other standard models. In this paper, we propose a\nnew cosmological model in which the dark energy density and the matter density\nare expressed as functions of $H(t)$ and $\\tau(t)$, where $\\tau(t)$ is the\ncosmological age of the Universe. We show that this model, which we call\n$\\mathcal{Q}_{\\rm_CDM}$ is consistent with cosmological observations and\nprovides a natural solution to the cosmological tensions",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.3582089552238806,
          "f": 0.2944785227656292
        },
        "rouge-2": {
          "r": 0.06293706293706294,
          "p": 0.08571428571428572,
          "f": 0.0725806402786814
        },
        "rouge-l": {
          "r": 0.20833333333333334,
          "p": 0.29850746268656714,
          "f": 0.24539876816440218
        }
      }
    },
    {
      "paper_id": "nlin.AO.nlin/AO/2503.07585v1",
      "true_abstract": "In this paper we explore the effects of instantaneous stochastic resetting on\na planar slow-fast dynamical system of the form $\\dot{x}=f(x)-y$ and\n$\\dot{y}=\\epsilon (x-y)$ with $0<\\epsilon \\ll 1$. We assume that only the fast\nvariable $x(t)$ resets to its initial state $x_0$ at a random sequence of times\ngenerated from a Poisson process of rate $r$. Fixing the slow variable, we\ndetermine the parameterized probability density $p(x,t|y)$, which is the\nsolution to a modified Liouville equation. We then show how for $r\\gg \\epsilon$\nthe slow dynamics can be approximated by the averaged equation\n$dy/d\\tau=\\E[x|y]-y$ where $\\tau=\\epsilon t$, $\\E[x|y]=\\int x p^*(x|y)dx$ and\n$p^*(x|y)=\\lim_{t\\rightarrow \\infty}p(x,t|y)$. We illustrate the theory for\n$f(x)$ given by the cubic function of the FitzHugh-Nagumo equation. We find\nthat the slow variable typically converges to an $r$-dependent fixed point\n$y^*$ that is a solution of the equation $y^*=\\E[x|y^*]$. Finally, we\nnumerically explore deviations from averaging theory when $r=O(\\epsilon)$.",
      "generated_abstract": "er a system of coupled stochastic differential equations (SDEs)\nwith a memory kernel of the form\n  $\\sigma(t,x,u) = \\sum_{k=1}^\\infty \\sqrt{\\lambda_k} \\phi_k(t,x) u_k(t,u)$,\nwhere $\\phi_k$ are white noises of the same dimension as $u$, $\\lambda_k$ are\na sequence of positive numbers, and $\\phi_k$ and $u_k$ are independently\ndistributed. We show that the system has a weakly stochastic initial\ncondition, and that it is Markovian if the sequence $\\{\\lambda_k\\}$ is\nincreasing. We also show that the system is equivalent to a slow-fast system\nwith a deterministic initial condition, and we prove that the slow component\nis Markovian if the sequence $\\{\\lambda_k\\}$ is decre",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1792452830188679,
          "p": 0.31666666666666665,
          "f": 0.22891565803454791
        },
        "rouge-2": {
          "r": 0.04895104895104895,
          "p": 0.08139534883720931,
          "f": 0.061135366488816366
        },
        "rouge-l": {
          "r": 0.16981132075471697,
          "p": 0.3,
          "f": 0.21686746526346357
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/GL/2211.09554v1",
      "true_abstract": "It is essential to discuss the role, difficulties, and opportunities\nconcerning people of different gender in the field of software engineering\nresearch, education, and industry. Although some literature reviews address\nsoftware engineering and gender, it is still unclear how research and practices\nin Asia exist for handling gender aspects in software development and\nengineering. We conducted a systematic literature review to grasp the\ncomprehensive view of gender research and practices in Asia. We analyzed the 32\nidentified papers concerning countries and publication years among 463\npublications. Researchers and practitioners from various organizations actively\nwork on gender research and practices in some countries, including China,\nIndia, and Turkey. We identified topics and classified them into seven\ncategories varying from personal mental health and team building to\norganization. Future research directions include investigating the synergy\nbetween (regional) gender aspects and cultural concerns and considering\npossible contributions and dependency among different topics to have a solid\nfoundation for accelerating further research and getting actionable practices.",
      "generated_abstract": "This study systematically reviews the literature on gender and software\nengineering (SE) in Asia, examining the role of women in SE, gender\ninequalities, and the role of leadership. The study focuses on three Asian\ncountries: Singapore, Taiwan, and India. It identifies the most frequently\nused terms in the literature and highlights the key gaps and research\nopportunities. The study contributes to the literature by identifying key gaps\nin the field and offering recommendations for future research. The findings\ncontribute to the literature by providing a comprehensive review of the\nliterature and highlighting key gaps. The findings also offer recommendations\nfor future research.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18269230769230768,
          "p": 0.3275862068965517,
          "f": 0.23456789663770772
        },
        "rouge-2": {
          "r": 0.02666666666666667,
          "p": 0.047619047619047616,
          "f": 0.03418802958579944
        },
        "rouge-l": {
          "r": 0.16346153846153846,
          "p": 0.29310344827586204,
          "f": 0.20987653861301642
        }
      }
    },
    {
      "paper_id": "physics.hist-ph.physics/hist-ph/2502.18231v1",
      "true_abstract": "1. Strong and weak notions of erasure are distinguished according to whether\nthe single erasure procedure does or does not leave the environment in the same\nstate independently of the pre-erasure state.\n  2. Purely thermodynamic considerations show that strong erasure cannot be\ndissipationless.\n  3. The main source of entropy creation in erasure processes at molecular\nscales is the entropy that must be created to suppress thermal fluctuations\n(\"noise\").\n  4. A phase space analysis recovers no minimum entropy cost for weak erasure\nand a positive minimum entropy cost for strong erasure.\n  5. An information entropy term has been attributed mistakenly to pre-erasure\nstates in the Gibbs formalism through the neglect of an additive constant in\nthe \"-k sum p log p\" Gibbs entropy formula.",
      "generated_abstract": "The concept of erasure is commonly used in the study of thermodynamics, but\nthe physical meaning of erasure is often unclear. This paper reconsiders\nergodic theory from a purely statistical point of view, demonstrating that the\nstandard ergodic theorem, which states that the entropy of a stationary Markov\nchain is equal to the mean number of eras, can be rephrased as the ergodic\ntheorem for the simpler case of the simple uninformed thermodynamics. This\nresult provides a unified mathematical framework for understanding the\nrelationship between erasure and ergodicity, and it offers a clear and\nmathematically rigorous explanation of the surprising fact that erasure can be\ndescribed by the simple uninformed thermodynamics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16279069767441862,
          "p": 0.1917808219178082,
          "f": 0.17610062396424206
        },
        "rouge-2": {
          "r": 0.04310344827586207,
          "p": 0.049019607843137254,
          "f": 0.04587155465364923
        },
        "rouge-l": {
          "r": 0.1511627906976744,
          "p": 0.1780821917808219,
          "f": 0.1635220076120408
        }
      }
    },
    {
      "paper_id": "physics.ed-ph.physics/ed-ph/2503.00336v1",
      "true_abstract": "Continuing professional development for teachers in the physical sciences is\ncrucial to maintaining high-quality instruction, especially when addressing\nmodern physics. Nevertheless, the teaching of these topics often relies on\ntheoretical models that may seem abstract and removed from practical\napplications. In this context, research in astrophysics provides many valuable\ninsights into the nature of light and its fundamental properties, such as\ncontinuous and discrete spectra, blackbody radiation, and atomic orbitals. This\npaper, aimed at both high school and university-level physics teachers,\nexamines the peculiarities of the emission and absorption spectra of various\ntypes of astronomical objects and demonstrates how spectroscopy is applied in\nastrophysics research. From this perspective, the study conceptually\nillustrates how astrophysicists, by measuring light spectra, determine the\ncomposition, physical properties, origin, and evolution of celestial bodies\nand, by extension, of the universe as a whole. By understanding not only the\ntheory but also the direct applications of astronomical spectroscopy, teachers\nwill be better prepared to guide their students, thereby showcasing the true\nvalue of modern physics in the real world.",
      "generated_abstract": "This article reviews the historical development of the modern spectroscopic\nobservation of celestial bodies, from the pioneering works of Galileo Galilei\nand William Herschel, to the present-day use of the techniques of\nhigh-resolution spectroscopy in astronomy. We emphasize the fundamental role of\noptical telescopes in enabling the progressive refinement of this technique,\nfrom the early 19th century to the present, and discuss the application of\nspectroscopy in the study of stellar and planetary atmospheres, as well as in\nthe search for exoplanets, black holes, and dark matter.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12698412698412698,
          "p": 0.27586206896551724,
          "f": 0.1739130391611532
        },
        "rouge-2": {
          "r": 0.023952095808383235,
          "p": 0.04938271604938271,
          "f": 0.032258060117391345
        },
        "rouge-l": {
          "r": 0.10317460317460317,
          "p": 0.22413793103448276,
          "f": 0.14130434350897933
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.07126v1",
      "true_abstract": "We propose to relax traditional axioms in decision theory by incorporating a\nmeasurement, or degree, of satisfaction. For example, if the independence axiom\nof expected utility theory is violated, we can measure the size of the\nviolation. This measure allows us to derive an approximation guarantee for a\nutility representation that aligns with the unmodified version of the axiom.\nAlmost satisfying the axiom implies, then, a utility that is near a utility\nrepresentation. We develop specific examples drawn from expected utility theory\nunder risk and uncertainty.",
      "generated_abstract": "I show that a broad class of decision problems involving a decision\nleader and multiple followers, with the decision leader making a choice between\ntwo possible actions, is not a simple average-case decision problem. In fact,\neven if the decision leader knows the followers' decision probabilities, she\nmay not be able to find a good average-case decision strategy. I show that a\nvariation of this phenomenon is that a decision problem is not a simple\naverage-case decision problem if and only if it is not a simple average-case\ndecision problem with a single decision leader.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.20833333333333334,
          "f": 0.18518518024691372
        },
        "rouge-2": {
          "r": 0.02564102564102564,
          "p": 0.028169014084507043,
          "f": 0.026845632594929086
        },
        "rouge-l": {
          "r": 0.15,
          "p": 0.1875,
          "f": 0.16666666172839517
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.07843v1",
      "true_abstract": "We show that compact locally symmetric Lorentz manifolds are geodesically\ncomplete.",
      "generated_abstract": "In this paper, we establish the compactness of the compact Locally symmetric\nSpacetime (LSS) manifold, which is a compact Lorentz manifold with a\nnon-trivial, compactly supported metric tensor. By considering a local\nLagrangian structure on the LSS manifold, we prove that the metric tensor\ntends to zero as the distance between the points tends to infinity. Moreover,\nwe give a simple proof that the metric tensor is compactly supported on the\nLSS manifold. Finally, we prove the completeness of the compact LSS manifold.\nThis completeness result is not a consequence of the completeness of the\nconformally flat spacetime. We also give a complete proof of the completeness\nof the compact Locally symmetric Lorentz manifold.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.5454545454545454,
          "p": 0.10344827586206896,
          "f": 0.17391304079815167
        },
        "rouge-2": {
          "r": 0.1,
          "p": 0.011363636363636364,
          "f": 0.020408161432736528
        },
        "rouge-l": {
          "r": 0.45454545454545453,
          "p": 0.08620689655172414,
          "f": 0.1449275335517749
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.q-bio/MN/2501.00983v1",
      "true_abstract": "We study Hopfield networks with non-reciprocal coupling inducing switches\nbetween memory patterns. Dynamical phase transitions occur between phases of no\nmemory retrieval, retrieval of multiple point-attractors, and limit-cycle\nattractors. The limit cycle phase is bounded by two critical regions: a Hopf\nbifurcation line and a fold bifurcation line, each with unique dynamical\ncritical exponents and sensitivity to perturbations. A Master Equation approach\nnumerically verifies the critical behavior predicted analytically. We discuss\nhow these networks could model biological processes near a critical threshold\nof cyclic instability evolving through multi-step transitions.",
      "generated_abstract": "networks are one of the most widely used models of neuronal\nmemory and have been used to study many biological processes. However, their\nability to capture non-reciprocal dynamics has not been well studied. In this\nwork, we use a non-reciprocal version of the Hopfield network to study the\ndynamics of non-reciprocal memory retrieval in the presence of aperiodic\nnoise. We show that the non-reciprocal network exhibits critical behavior that\nis similar to that of the aperiodic version. In particular, we show that the\ncritical slowing down of the network can be understood in terms of a phase\ntransition between fast and slow dynamics, similar to what has been found in a\nreciprocal version of the network. We also show that the memory can be retrieved\ncyclically, unlike the non-reciprocal aperiodic version, where it is\nnon-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2638888888888889,
          "p": 0.25333333333333335,
          "f": 0.25850339636262676
        },
        "rouge-2": {
          "r": 0.03409090909090909,
          "p": 0.025,
          "f": 0.028846148964497866
        },
        "rouge-l": {
          "r": 0.2361111111111111,
          "p": 0.22666666666666666,
          "f": 0.2312925120088853
        }
      }
    },
    {
      "paper_id": "cs.LG.econ/TH/2502.06387v1",
      "true_abstract": "Human-annotated preference data play an important role in aligning large\nlanguage models (LLMs). In this paper, we investigate the questions of\nassessing the performance of human annotators and incentivizing them to provide\nhigh-quality annotations. The quality assessment of language/text annotation\nfaces two challenges: (i) the intrinsic heterogeneity among annotators, which\nprevents the classic methods that assume the underlying existence of a true\nlabel; and (ii) the unclear relationship between the annotation quality and the\nperformance of downstream tasks, which excludes the possibility of inferring\nthe annotators' behavior based on the model performance trained from the\nannotation data. Then we formulate a principal-agent model to characterize the\nbehaviors of and the interactions between the company and the human annotators.\nThe model rationalizes a practical mechanism of a bonus scheme to incentivize\nannotators which benefits both parties and it underscores the importance of the\njoint presence of an assessment system and a proper contract scheme. From a\ntechnical perspective, our analysis extends the existing literature on the\nprincipal-agent model by considering a continuous action space for the agent.\nWe show the gap between the first-best and the second-best solutions (under the\ncontinuous action space) is of $\\Theta(1/\\sqrt{n \\log n})$ for the binary\ncontracts and $\\Theta(1/n)$ for the linear contracts, where $n$ is the number\nof samples used for performance assessment; this contrasts with the known\nresult of $\\exp(-\\Theta(n))$ for the binary contracts when the action space is\ndiscrete. Throughout the paper, we use real preference annotation data to\naccompany our discussions.",
      "generated_abstract": "guage Models (LLMs) have demonstrated exceptional performance in\ntask-specific tasks, but their ability to generalize to new tasks remains\nuncertain. This uncertainty stems from their inherent incompleteness,\nlimiting their ability to generalize beyond the training data. To overcome this,\nLLMs can be assisted by human annotators, who provide detailed feedback on\nincompleteness and accuracy. We introduce a novel framework for assessing and\nincentivizing human preference annotation, which we apply to the LLM task of\nanswering questions about the meaning of sentences. We find that annotators\nrespond most positively to examples that are more challenging to interpret,\nwhich aligns with prior research suggesting that LLMs rely more heavily on\nabstract reasoning. Additionally, we find that incentives can incentivize\nhuman preference annotation while maintaining their independence. Our framework\ndemonstrates the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19863013698630136,
          "p": 0.29591836734693877,
          "f": 0.23770491322628334
        },
        "rouge-2": {
          "r": 0.00881057268722467,
          "p": 0.01639344262295082,
          "f": 0.011461313504160226
        },
        "rouge-l": {
          "r": 0.17123287671232876,
          "p": 0.25510204081632654,
          "f": 0.20491802798038175
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/AP/2502.16988v1",
      "true_abstract": "A dynamic treatment regime is a sequence of treatment decision rules tailored\nto an individual's evolving status over time. In precision medicine, much focus\nhas been placed on finding an optimal dynamic treatment regime which, if\nfollowed by everyone in the population, would yield the best outcome on\naverage; and extensive investigation has been conducted from both\nmethodological and applications standpoints. The aim of this tutorial is to\nprovide readers who are interested in optimal dynamic treatment regimes with a\nsystematic, detailed but accessible introduction, including the formal\ndefinition and formulation of this topic within the framework of causal\ninference, identification assumptions required to link the causal quantity of\ninterest to the observed data, existing statistical models and estimation\nmethods to learn the optimal regime from data, and application of these methods\nto both simulated and real data.",
      "generated_abstract": "r offers a tutorial on the optimal dynamic treatment regime (ODT\nTechnique), which is a general method for treating time-varying patient\nsubgroups. The ODT Technique is based on the principle of minimizing an\nobjective function subject to a set of constraints. The objective function is\nthe expected loss in outcomes due to a change in treatment and is used to\noptimize the treatment assignment. The constraints are based on the\nassumptions of a homogeneous population, and on the patient subgroups being\nassigned to treatment. The ODT Technique has been applied to various patient\nsubgroups, including those with different stages of disease, those with different\ntreatment responses, and those with different treatment responses and\ncomorbidities. The ODT Technique can be used to treat time-varying patient\nsubgroups in a number of clinical settings, including clinical trials",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23157894736842105,
          "p": 0.3055555555555556,
          "f": 0.26347304898705587
        },
        "rouge-2": {
          "r": 0.046153846153846156,
          "p": 0.05263157894736842,
          "f": 0.04918032289035257
        },
        "rouge-l": {
          "r": 0.22105263157894736,
          "p": 0.2916666666666667,
          "f": 0.2514970010828643
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.10304v2",
      "true_abstract": "A popular approach to perform inference on a target parameter in the presence\nof nuisance parameters is to construct estimating equations that are orthogonal\nto the nuisance parameters, in the sense that their expected first derivative\nis zero. Such first-order orthogonalization may, however, not suffice when the\nnuisance parameters are very imprecisely estimated. Leading examples where this\nis the case are models for panel and network data that feature fixed effects.\nIn this paper, we show how, in the conditional-likelihood setting, estimating\nequations can be constructed that are orthogonal to any chosen order. Combining\nthese equations with sample splitting yields higher-order bias-corrected\nestimators of target parameters. In an empirical application we apply our\nmethod to a fixed-effect model of team production and obtain estimates of\ncomplementarity in production and impacts of counterfactual re-allocations.",
      "generated_abstract": "We propose a Neyman-Orthogonalization (NO) approach to the incidental\nparameters problem in panel data settings. The proposed method provides a\nframework to estimate and test for the presence of incidental parameters in\nnon-experimental settings. The framework is applicable to any type of\nincidental parameter (with or without common identification) and any type of\nexposure (with or without shared common identification). The proposed method\nis flexible and can be used to estimate and test for incidental parameters in\nany type of non-experimental settings. The proposed method can be used to\ntest for the presence of incidental parameters in non-experimental settings in\nthe presence of shared identification. We illustrate the proposed method using\na tobacco industry dataset.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1935483870967742,
          "p": 0.3673469387755102,
          "f": 0.2535211222406269
        },
        "rouge-2": {
          "r": 0.056910569105691054,
          "p": 0.09090909090909091,
          "f": 0.06999999526450032
        },
        "rouge-l": {
          "r": 0.1935483870967742,
          "p": 0.3673469387755102,
          "f": 0.2535211222406269
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.02178v1",
      "true_abstract": "This paper proposes an online inference method of the stochastic gradient\ndescent (SGD) with a constant learning rate for quantile loss functions with\ntheoretical guarantees. Since the quantile loss function is neither smooth nor\nstrongly convex, we view such SGD iterates as an irreducible and positive\nrecurrent Markov chain. By leveraging this interpretation, we show the\nexistence of a unique asymptotic stationary distribution, regardless of the\narbitrarily fixed initialization. To characterize the exact form of this\nlimiting distribution, we derive bounds for its moment generating function and\ntail probabilities, controlling over the first and second moments of SGD\niterates. By these techniques, we prove that the stationary distribution\nconverges to a Gaussian distribution as the constant learning rate\n$\\eta\\rightarrow0$. Our findings provide the first central limit theorem\n(CLT)-type theoretical guarantees for the last iterate of constant\nlearning-rate SGD in non-smooth and non-strongly convex settings. We further\npropose a recursive algorithm to construct confidence intervals of SGD iterates\nin an online manner. Numerical studies demonstrate strong finite-sample\nperformance of our proposed quantile estimator and inference method. The\ntheoretical tools in this study are of independent interest to investigate\ngeneral transition kernels in Markov chains.",
      "generated_abstract": "for quantile regression has received a lot of attention due to its\napplications in various fields, such as machine learning, finance, and\nstatistics. In the past few years, stochastic gradient descent (SGD) has\nrecently been proposed as a computationally efficient approach for online\ninference. However, most existing methods for online quantile inference rely on\nSGD to estimate the gradient of the loss function, which requires the\nestimation of the gradient in each iteration. We show that this approach\nincreases the computational complexity, and consequently, inference for\nquantile regression is not feasible in practice. To address this issue, we\npropose an online algorithm for quantile regression that only requires\nestimation of the gradient for a single iteration. Our algorithm requires a\nconstant learning rate for all iterations, which makes it practical and\ncomputationally efficient. We also show that our algorithm is equivalent to a\nclass",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2734375,
          "p": 0.39325842696629215,
          "f": 0.32258064032279304
        },
        "rouge-2": {
          "r": 0.06629834254143646,
          "p": 0.0916030534351145,
          "f": 0.07692307205148782
        },
        "rouge-l": {
          "r": 0.2734375,
          "p": 0.39325842696629215,
          "f": 0.32258064032279304
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/CP/2501.15106v1",
      "true_abstract": "We study operator learning in the context of linear propagator models for\noptimal order execution problems with transient price impact \\`a la Bouchaud et\nal. (2004) and Gatheral (2010). Transient price impact persists and decays over\ntime according to some propagator kernel. Specifically, we propose to use\nIn-Context Operator Networks (ICON), a novel transformer-based neural network\narchitecture introduced by Yang et al. (2023), which facilitates data-driven\nlearning of operators by merging offline pre-training with an online few-shot\nprompting inference. First, we train ICON to learn the operator from various\npropagator models that maps the trading rate to the induced transient price\nimpact. The inference step is then based on in-context prediction, where ICON\nis presented only with a few examples. We illustrate that ICON is capable of\naccurately inferring the underlying price impact model from the data prompts,\neven with propagator kernels not seen in the training data. In a second step,\nwe employ the pre-trained ICON model provided with context as a surrogate\noperator in solving an optimal order execution problem via a neural network\ncontrol policy, and demonstrate that the exact optimal execution strategies\nfrom Abi Jaber and Neuman (2022) for the models generating the context are\ncorrectly retrieved. Our introduced methodology is very general, offering a new\napproach to solving optimal stochastic control problems with unknown state\ndynamics, inferred data-efficiently from a limited number of examples by\nleveraging the few-shot and transfer learning capabilities of transformer\nnetworks.",
      "generated_abstract": "aper, we propose a novel method for operator learning in linear\npropagator models. In linear propagator models, the operator is defined as a\nlinear combination of a set of operators, each of which is applied to a vector\nof data. These operators are learned from data. We propose an in-context\noperator learning method that is applicable to any linear propagator model. The\nkey idea is that we learn the operator in the context of data by using\nreinforcement learning to select the operator to be learned from the data. We\nintroduce a novel approach for selecting the operator to be learned from data\nthat is based on the in-context learning of the operator. We also propose a\nframework for learning the operator in the context of data using a reinforcement\nlearning algorithm, and demonstrate its effectiveness through experiments.\nFirst, we apply the in-context operator learning method to the linear\npropagator model with a linear",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2550335570469799,
          "p": 0.5588235294117647,
          "f": 0.3502304104432033
        },
        "rouge-2": {
          "r": 0.09292035398230089,
          "p": 0.17355371900826447,
          "f": 0.12103745943476003
        },
        "rouge-l": {
          "r": 0.2483221476510067,
          "p": 0.5441176470588235,
          "f": 0.3410138205814522
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2408.15310v1",
      "true_abstract": "Recent studies suggest that drug-drug interaction (DDI) prediction via\ncomputational approaches has significant importance for understanding the\nfunctions and co-prescriptions of multiple drugs. However, the existing silico\nDDI prediction methods either ignore the potential interactions among drug-drug\npairs (DDPs), or fail to explicitly model and fuse the multi-scale drug feature\nrepresentations for better prediction. In this study, we propose RGDA-DDI, a\nresidual graph attention network (residual-GAT) and dual-attention based\nframework for drug-drug interaction prediction. A residual-GAT module is\nintroduced to simultaneously learn multi-scale feature representations from\ndrugs and DDPs. In addition, a dual-attention based feature fusion block is\nconstructed to learn local joint interaction representations. A series of\nevaluation metrics demonstrate that the RGDA-DDI significantly improved DDI\nprediction performance on two public benchmark datasets, which provides a new\ninsight into drug development.",
      "generated_abstract": "vances in drug-drug interaction (DDI) prediction have demonstrated\ncrucial applications in drug discovery and personalized medicine. However,\ntraditional DDI prediction methods often struggle with data scarcity,\nunstructured drug-drug interaction data, and limited modeling capabilities.\nExisting approaches often rely on heuristic methods, such as multi-step\nregression, to address these issues. While such methods can produce\nsufficiently accurate predictions, they lack the flexibility to handle\ndrug-drug interaction data in a more structured manner. In this paper, we\npropose a novel residual graph attention network (RGDA-DDI) framework that\nintegrates residual blocks with graph attention networks to enhance\ndata-efficiency. Specifically, we introduce the residual graph attention\nnetwork (RGAN) module, which is composed of residual graph convolution layers\nand residual blocks",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.28421052631578947,
          "p": 0.313953488372093,
          "f": 0.2983425364488264
        },
        "rouge-2": {
          "r": 0.07874015748031496,
          "p": 0.09345794392523364,
          "f": 0.08547008050661144
        },
        "rouge-l": {
          "r": 0.28421052631578947,
          "p": 0.313953488372093,
          "f": 0.2983425364488264
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2502.21311v1",
      "true_abstract": "Comb Sign is an important imaging biomarker to detect multiple\ngastrointestinal diseases. It shows up as increased blood flow along the\nintestinal wall indicating potential abnormality, which helps doctors diagnose\ninflammatory conditions. Despite its clinical significance, current detection\nmethods are manual, time-intensive, and prone to subjective interpretation due\nto the need for multi-planar image-orientation. To the best of our knowledge,\nwe are the first to propose a fully automated technique for the detection of\nComb Sign from CTE scans. Our novel approach is based on developing a\nprobabilistic map that shows areas of pathological hypervascularity by\nidentifying fine vascular bifurcations and wall enhancement via processing\nthrough stepwise algorithmic modules. These modules include utilising deep\nlearning segmentation model, a Gaussian Mixture Model (GMM), vessel extraction\nusing vesselness filter, iterative probabilistic enhancement of vesselness via\nneighborhood maximization and a distance-based weighting scheme over the\nvessels. Experimental results demonstrate that our pipeline effectively\nidentifies Comb Sign, offering an objective, accurate, and reliable tool to\nenhance diagnostic accuracy in Crohn's disease and related hypervascular\nconditions where Comb Sign is considered as one of the important biomarkers.",
      "generated_abstract": "(CS) is a key imaging feature in 3D CT Echocardiography (3DE)\ncomparisons. The prevalence of CS in 3DE is variable, with some studies\nreporting CS frequencies of up to 20% and others reporting CS frequencies of\nless than 1%. However, most studies rely on manual assessment, which is\ntime-consuming, error-prone, and prone to subjective interpretation. To address\nthis, we propose an automated comb sign detector (AutoComb), a novel model that\nautomatically identifies CS using a multimodal deep learning framework. AutoComb\nrelies on two key components: a 3D CNN that extracts features from the 3D\nCTE scan, and a multimodal attention mechanism that leverages both CT and\nultrasound features to predict CS. Our findings demonstrate that Auto",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21014492753623187,
          "p": 0.3411764705882353,
          "f": 0.26008968138108557
        },
        "rouge-2": {
          "r": 0.03932584269662921,
          "p": 0.06306306306306306,
          "f": 0.048442901843129736
        },
        "rouge-l": {
          "r": 0.18840579710144928,
          "p": 0.3058823529411765,
          "f": 0.23318385178467305
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.08747v1",
      "true_abstract": "Cognitive delegation to artificial intelligence (AI) systems is transforming\nscientific research by enabling the automation of analytical processes and the\ndiscovery of new patterns in large datasets. This study examines the ability of\nAI to complement and expand knowledge in the analysis of breast cancer using\ndynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Building on a\nprevious study, we assess the extent to which AI can generate novel approaches\nand successfully solve them. For this purpose, AI models, specifically\nChatGPT-4o, were used for data preprocessing, hypothesis generation, and the\napplication of clustering techniques, predictive modeling, and correlation\nnetwork analysis. The results obtained were compared with manually computed\noutcomes, revealing limitations in process transparency and the accuracy of\ncertain calculations. However, as AI reduces errors and improves reasoning\ncapabilities, an important question arises regarding the future of scientific\nresearch: could automation replace the human role in science? This study seeks\nto open the debate on the methodological and ethical implications of a science\ndominated by artificial intelligence.",
      "generated_abstract": "This work is focused on the detection of cancer through resonance\n(magnetic) imaging. We have used a technique known as generative modeling,\nwhich is a form of machine learning in which a large number of samples is used\nto generate new samples. This technique is used in several fields, including\nimage processing, and we have used it to generate samples of magnetic resonance\nimaging data. This technique is called generative adversarial networks, which\nconsists of two networks: a generator that generates new samples and a discrimination\nnetwork that is trained to distinguish the new samples from the original data.\nThe results show that the method is very effective at detecting cancer, and\nthat it can be used to detect cancer in clinical settings.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21487603305785125,
          "p": 0.3611111111111111,
          "f": 0.2694300471357621
        },
        "rouge-2": {
          "r": 0.025,
          "p": 0.03508771929824561,
          "f": 0.029197075432895476
        },
        "rouge-l": {
          "r": 0.19834710743801653,
          "p": 0.3333333333333333,
          "f": 0.24870465853472587
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/SR/2503.09744v1",
      "true_abstract": "The Sun's open-closed flux boundary (OCB) separates closed and open magnetic\nfield lines, and is the site for interchange magnetic reconnection processes\nthought to be linked to the origin of the slow solar wind (SSW). We analyse the\nglobal magnetic field structure and OCB from December 2010 to December 2019\nusing three coronal magnetic field models: a potential field source surface\n(PFSS) model, a static equilibrium magnetofrictional model, and a\ntime-dependent magnetofrictional model. We analyse the model and cycle\ndependence of the OCB length on the photosphere, as well as the magnetic flux\nin the vicinity of the OCB. Near solar maximum, the coronal magnetic field for\neach model consists predominantly of long, narrow coronal holes, and nearly all\nthe open flux lies within one supergranule-diameter (25 Mm) of the OCB. By\ncomparing to interplanetary scintillation measurements of SSW speeds, we argue\nthat the fraction of open flux within this 25 Mm band is a good predictor of\nthe amount of SSW in the heliosphere. Importantly, despite its simplicity, we\nshow that the PFSS model estimates this fraction as well as the time-dependent\nmodel. We discuss the implications of our results for understanding SSW origins\nand interchange reconnection at the OCB.",
      "generated_abstract": "closed flux boundary (OCFB) has been used to define the boundary of\nthe solar corona and is defined by the coronal open magnetic flux $\\mathcal{F}_o$\nand the closed magnetic flux $\\mathcal{F}_c$. In the past, it was argued that\nthe boundary is determined by the coronal open magnetic flux $\\mathcal{F}_o$\nbecause the coronal closed magnetic flux $\\mathcal{F}_c$ is proportional to\nthe coronal open magnetic flux $\\mathcal{F}_o$. However, we show that the\nOCFB is not determined by $\\mathcal{F}_o$ alone, and the boundary can be\nshifted to the right by a certain amount, depending on the coronal magnetic\nfield strength $B_c$. Moreover, the boundary can be shifted to the left by a\ncertain amount, depending on the coronal magnetic field strength $B_c$. We\nfind that the boundary",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17857142857142858,
          "p": 0.37735849056603776,
          "f": 0.2424242380635446
        },
        "rouge-2": {
          "r": 0.07262569832402235,
          "p": 0.16049382716049382,
          "f": 0.09999999571035521
        },
        "rouge-l": {
          "r": 0.16071428571428573,
          "p": 0.33962264150943394,
          "f": 0.21818181382112034
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SY/2503.08920v1",
      "true_abstract": "A distributed integrated sensing and communication (D-ISAC) system offers\nsignificant cooperative gains for both sensing and communication performance.\nThese gains, however, can only be fully realized when the distributed nodes are\nperfectly synchronized, which is a challenge that remains largely unaddressed\nin current ISAC research. In this paper, we propose an over-the-air\ntime-frequency synchronization framework for the D-ISAC system, leveraging the\nreciprocity of bistatic sensing channels. This approach overcomes the\nimpractical dependency of traditional methods on a direct line-of-sight (LoS)\nlink, enabling the estimation of time offset (TO) and carrier frequency offset\n(CFO) between two ISAC nodes even in non-LoS (NLOS) scenarios. To achieve this,\nwe introduce a bistatic signal matching (BSM) technique with delay-Doppler\ndecoupling, which exploits offset reciprocity (OR) in bistatic observations.\nThis method compresses multiple sensing links into a single offset for\nestimation. We further present off-grid super-resolution estimators for TO and\nCFO, including the maximum likelihood estimator (MLE) and the matrix pencil\n(MP) method, combined with BSM processing. These estimators provide accurate\noffset estimation compared to spectral cross-correlation techniques. Also, we\nextend the pairwise synchronization leveraging OR between two nodes to the\nsynchronization of $N$ multiple distributed nodes, referred to as centralized\npairwise synchronization. We analyze the Cramer-Rao bounds (CRBs) for TO and\nCFO estimates and evaluate the impact of D-ISAC synchronization on the\nbottom-line target localization performance. Simulation results validate the\neffectiveness of the proposed algorithm, confirm the theoretical analysis, and\ndemonstrate that the proposed synchronization approach can recover up to 96% of\nthe bottom-line target localization performance of the fully-synchronous\nD-ISAC.",
      "generated_abstract": "ed ISAC systems can provide significantly higher levels of\ncommunication capacity compared to conventional centralized systems, but they\nface challenges related to time-frequency (TF) synchronization and data\nintegrity. This paper explores the challenges associated with TF synchronization\nin distributed systems and proposes a novel TF synchronization scheme based on\na hybrid approach that combines a block synchronization method with a\nnon-blocking synchronization method. The proposed scheme is shown to improve\nthe performance of distributed ISAC systems compared to existing block\nsynchronization methods. It is also shown that the proposed scheme is\nefficient and easy to implement. Furthermore, the proposed scheme is shown to\nbe robust against various sources of error, such as time-variant channel\nfading, non-linearity, and quantization noise, which can all adversely affect\nTF synchronization. Finally, the performance of the proposed scheme is\nevaluated using",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1696969696969697,
          "p": 0.3333333333333333,
          "f": 0.22489959392267875
        },
        "rouge-2": {
          "r": 0.024691358024691357,
          "p": 0.05128205128205128,
          "f": 0.033333328945833914
        },
        "rouge-l": {
          "r": 0.15757575757575756,
          "p": 0.30952380952380953,
          "f": 0.20883533689456632
        }
      }
    },
    {
      "paper_id": "cs.LO.cs/LO/2503.08530v1",
      "true_abstract": "We present a choreographic framework for modelling and\n  analysing concurrent probabilistic systems based on the PRISM\n  model-checker. This is achieved through the development of a\n  choreography language, which is a specification language that allows\n  to describe the desired interactions within a concurrent system from\n  a global viewpoint. Using choreographies gives a clear and complete\n  view of system interactions, making it easier to understand the\n  process flow and identify potential errors, which helps ensure\n  correct execution and improves system reliability. We equip our\n  language with a probabilistic semantics and then define a formal\n  encoding into the PRISM language and discuss its\n  correctness. Properties of programs written in our choreographic\n  language can be model-checked by the PRISM model-checker via their\n  translation into the PRISM language. Finally, we implement a\n  compiler for our language and demonstrate its practical\n  applicability via examples drawn from the use cases featured in the\n  PRISM website.",
      "generated_abstract": "t a probabilistic choreography language for expressing and\nchoreographing probabilistic programs. The language allows for\nprobabilistic choreography expressions, which are built by composing\nprobabilistic choreography expressions with logic operators. We further\nprovide a set of operators that extend the set of logical operators with\nprobability-related operators such as conditional probability, expectation,\nconditional expectation, and randomness. These operators are essential for\nrealizing probabilistic choreography expressions. We also provide a set of\noperator applications that extend the set of operator applications with\nprobability-related operators such as conditional probability, expectation,\nconditional expectation, and randomness. We evaluate our approach by showing\nthat our language allows for the expression of probabilistic choreography\nexpressions and applying probabilistic choreography expressions to\nprobabilistic programs. We also show that the language can be extended to\nexpress and choreograph",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1958762886597938,
          "p": 0.3333333333333333,
          "f": 0.24675324209057178
        },
        "rouge-2": {
          "r": 0.028985507246376812,
          "p": 0.042105263157894736,
          "f": 0.034334759118790865
        },
        "rouge-l": {
          "r": 0.17525773195876287,
          "p": 0.2982456140350877,
          "f": 0.22077921611654588
        }
      }
    },
    {
      "paper_id": "math.RA.math/KT/2502.16257v1",
      "true_abstract": "The aim of this paper is twofold. In the first part, we define the cohomology\nof a Nijenhuis Lie algebra with coefficients in a suitable representation. Our\ncohomology of a Nijenhuis Lie algebra governs the simultaneous deformations of\nthe underlying Lie algebra and the Nijenhuis operator. Subsequently, we define\nhomotopy Nijenhuis operators on $2$-term $L_\\infty$-algebras and show that in\nsome cases they are related to third cocycles of Nijenhuis Lie algebras. In\nanother part of this paper, we extend our study to (generic) Nijenhuis Lie\nbialgebras where the Nijenhuis operators on the underlying Lie algebras and Lie\ncoalgebras need not be the same. In due course, we introduce matched pairs and\nManin triples of Nijenhuis Lie algebras and show that they are equivalent to\nNijenhuis Lie bialgebras. Finally, we consider the admissible classical\nYang-Baxter equation whose antisymmetric solutions yield Nijenhuis Lie\nbialgebras.",
      "generated_abstract": "We investigate the structure of the cohomology of Nijenhuis Lie algebras and\nNijenhuis Lie bialgebras, focusing on a particular class of such algebras. In\nparticular, we investigate the Nijenhuis cohomology of Nijenhuis Lie\nbialgebras. We show that in general, the cohomology of Nijenhuis Lie\nbialgebras is not finitely generated, and we provide examples of Nijenhuis\nLie bialgebras for which the cohomology is finitely generated. We also investigate\nthe cohomology of Nijenhuis Lie algebras with trivial coefficients, and we\nprovide examples of Nijenhuis Lie algebras with trivial coefficients for which\nthe cohomology is finitely generated.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24050632911392406,
          "p": 0.48717948717948717,
          "f": 0.3220338938796323
        },
        "rouge-2": {
          "r": 0.10526315789473684,
          "p": 0.20689655172413793,
          "f": 0.13953487925094663
        },
        "rouge-l": {
          "r": 0.21518987341772153,
          "p": 0.4358974358974359,
          "f": 0.2881355887948866
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/optics/2503.10553v1",
      "true_abstract": "Between the absorption and the emission spectral lineshapes of dense atomic\nand molecular media, such as dye solutions and alkali-noble buffer gas mixtures\nat high pressure, in many cases there exists a universal scaling, the\nKennard-Stepanov relation, which is a manifestation of detailed balance. This\nrelation plays a crucial role in recent Bose-Einstein condensation experiments\nof visible-spectral-photons in e.g. dye-solution-filled optical microcavities.\nIt has recently been proposed to use high-pressure xenon-noble gas mixtures as\na thermalization medium for vacuum-ultraviolet regime photons, so as to extend\nthe achievable wavelength range of such Bose-Einstein-condensed optical sources\nfrom the visible to the vacuum-ultraviolet regime. In this work, we report\ntwo-photon excitation spectroscopy measurements of ground state ($5p^6$) xenon\natoms subject to up to 80bar of helium or krypton buffer gas pressure,\nrespectively, in the 220nm - 260nm wavelength range. The study of such\ntwo-photon spectra is of interest e.g. for the exploration of possible pumping\nschemes of a future vacuum-ultraviolet photon Bose-Einstein condensate. We have\nalso recorded absorption and emission spectra of the $5p^6 \\leftrightarrow\n5p^56s$ single-photon transition near 147nm wavelength of xenon atoms subject\nto 80bar of krypton buffer gas pressure. We find that the ratio of absorption\nand emission follows a Kennard-Stepanov scaling, which suggests that such gas\nmixtures are promising candidates as a thermalization medium for a\nBose-Einstein condensate of vacuum-ultraviolet photons.",
      "generated_abstract": "rd-Stepanov relation is a relation between the intensity of the\ntwo-photon absorption and the intensity of the two-photon emission in\ntwo-component optical spectra. In this paper, we present the measurement of the\ntwo-photon absorption and emission intensity in a high-pressure two-component\nmixture of xenon and noble gases. We show that the relation holds for both\ncomponents, which is a test of the validity of the relation. The spectra are\nobtained by two-photon absorption spectroscopy using a two-photon absorption\ndetector. The spectra are normalized to the intensity of the two-photon\nemission. We present the intensity ratio of the two-photon absorption and\nemission, which is in good agreement with the Kennard-Stepanov relation. The\nratio",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20300751879699247,
          "p": 0.5192307692307693,
          "f": 0.29189188785040177
        },
        "rouge-2": {
          "r": 0.05970149253731343,
          "p": 0.14634146341463414,
          "f": 0.0848056495943265
        },
        "rouge-l": {
          "r": 0.19548872180451127,
          "p": 0.5,
          "f": 0.28108107703959095
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.gr-qc/2503.10423v1",
      "true_abstract": "Sterile neutrinos can influence the evolution of the universe, and thus\ncosmological observations can be used to search for sterile neutrinos. In this\nstudy, we utilized the latest baryon acoustic oscillations data from DESI,\ncombined with the cosmic microwave background data from Planck and the\nfive-year supernova data from DES, to constrain the interacting dark energy\n(IDE) models involving both cases of massless and massive sterile neutrinos. We\nconsider four typical forms of the interaction term $Q=\\beta H \\rho_{\\rm de}$,\n$Q=\\beta H \\rho_{\\rm c}$, $Q=\\beta H_{0} \\rho_{\\rm de}$, and $Q=\\beta H_{0}\n\\rho_{\\rm c}$, respectively. Our analysis indicates that the current data\nprovide only a hint of the existence of massless sterile neutrinos (as dark\nradiation) at about the $1\\sigma$ level. In contrast, no evidence supports the\nexistence of massive sterile neutrinos. Furthermore, in IDE models, the\ninclusion of (massless/massive) sterile neutrinos has a negligible impact on\nthe constraint of the coupling parameter $\\beta$. The IDE model of $Q=\\beta H\n\\rho_{\\rm c}$ with sterile neutrinos does not favor an interaction. However,\nthe other three IDE models with sterile neutrinos support an interaction in\nwhich dark energy decays into dark matter.",
      "generated_abstract": "t a search for sterile neutrinos using the baryon acoustic\noscillations (BAO) and the supernova data from DES. We use the Boltzmann code\nCosmoMC to sample the parameters of the interacting dark energy (IDE) models\nconsidered in this study. We find that the BAO and supernova data jointly\nconstrain the IDE parameters of the form $w_1+w_2=0.8\\pm0.4$, where the\nparameters $w_1$ and $w_2$ are determined by the two data sets. We further\ninvestigate the degeneracy between $w_1$ and $w_2$ by fixing $w_1=0.9$ and\n$w_2=0.5$ and re-running the joint fit. We find that this degeneracy is not\nstrong enough to be resolved",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24778761061946902,
          "p": 0.39436619718309857,
          "f": 0.3043478213474717
        },
        "rouge-2": {
          "r": 0.09876543209876543,
          "p": 0.16842105263157894,
          "f": 0.12451361401686645
        },
        "rouge-l": {
          "r": 0.23008849557522124,
          "p": 0.36619718309859156,
          "f": 0.2826086909126891
        }
      }
    },
    {
      "paper_id": "physics.ins-det.hep-ex/2503.09303v1",
      "true_abstract": "This contribution introduces a novel test system developed to evaluate the\nsignal transmission quality in high-speed data links for the 2026 Inner Tracker\n(ITk) upgrade of the ATLAS experiment. Using an FPGA-based data acquisition\n(DAQ) framework, the setup can run simultaneous Bit Error Rate (BER) tests for\nup to 64 channels and generate virtual eye diagrams, for qualifying the\n$\\sim$26k electrical links at the ATLAS ITk data rate of 1.28Gb/s. The paper\nincludes results from system calibration, yielding its contribution to the\nmeasured losses, and preliminary results from tests of prototype and\npre-production assemblies of on-detector links of the three ATLAS ITk Pixel\nsubsystems.",
      "generated_abstract": "pgrade Silicon Pixel Detector (Sudoku) of the ATLAS Inner Tracker\n(ITk) is under development. To evaluate the performance of the Sudoku,\nexperimental data from the CMSSW12.11.1.pre6 (CMSSW12.11.1) running at the LHC,\nwhich were taken in 2019, have been used. The data were recorded with the\nITk/Sudoku in the CMS Run-2 (CMSSW12.10.2) conditions, which were significantly\ndifferent from the conditions of the LHC Run-3 (CMSSW12.11.1) data. The\nexperimental data have been processed using a customized software (Sudoku\nsimulator) to produce a test system for the data links of the Sudoku. The test\nsystem has been used",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2631578947368421,
          "p": 0.29850746268656714,
          "f": 0.27972027474008515
        },
        "rouge-2": {
          "r": 0.1,
          "p": 0.10869565217391304,
          "f": 0.10416666167534745
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.2835820895522388,
          "f": 0.2657342607540712
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.10004v1",
      "true_abstract": "In this paper, we present a hierarchical framework that integrates\nupper-level routing with low-level optimal trajectory planning for connected\nand automated vehicles (CAVs) traveling in an urban network. The upper-level\ncontroller efficiently distributes traffic flows by utilizing a dynamic\nre-routing algorithm that leverages real-time density information and the\nfundamental diagrams of each network edge. This re-routing approach predicts\nwhen each edge will reach critical density and proactively adjusts the routing\nalgorithm's weights to prevent congestion before it occurs. The low-level\ncontroller coordinates CAVs as they cross signal-free intersections, generating\noptimal, fuel-efficient trajectories while ensuring safe passage by satisfying\nall relevant constraints. We formulate the problem as an optimal control\nproblem and derive an analytical solution. Using the SUMO micro-simulation\nplatform, we conduct simulation experiments on a realistic network. The results\nshow that our hierarchical framework significantly enhances network performance\ncompared to a baseline static routing approach. By dynamically re-routing\nvehicles, our approach successfully reduces total travel time and mitigates\ncongestion before it develops.",
      "generated_abstract": "ve re-routing has been proposed to facilitate traffic flow in\nnetworks with limited capacity. However, when vehicles encounter a\ncongested intersection, conventional re-routing methods may result in\ndivergent routes that lead to gridlock. In this paper, we propose a\ncooperative re-routing method for connected and automated vehicles (CAVs) in\nurban networks. Our approach combines cooperative re-routing with\nintersection coordination to improve traffic efficiency. In the cooperative\nre-routing phase, we use an optimal control approach to route CAVs around\ncongested intersections based on the distance to the next intersection. In the\nintersection coordination phase, we propose a novel cooperative control scheme\nbased on the Lagrange multiplier method, which ensures the coordination\nbetween the CAVs and the intersection controllers. Simulation results demonstrate\nthat our approach significantly impro",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24166666666666667,
          "p": 0.3670886075949367,
          "f": 0.291457281644403
        },
        "rouge-2": {
          "r": 0.08227848101265822,
          "p": 0.11403508771929824,
          "f": 0.095588230424957
        },
        "rouge-l": {
          "r": 0.225,
          "p": 0.34177215189873417,
          "f": 0.27135677913184014
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2502.12479v2",
      "true_abstract": "The motif-scaffolding problem is a central task in computational protein\ndesign: Given the coordinates of atoms in a geometry chosen to confer a desired\nbiochemical function (a motif), the task is to identify diverse protein\nstructures (scaffolds) that include the motif and maintain its geometry.\nSignificant recent progress on motif-scaffolding has been made due to\ncomputational evaluation with reliable protein structure prediction and\nfixed-backbone sequence design methods. However, significant variability in\nevaluation strategies across publications has hindered comparability of\nresults, challenged reproducibility, and impeded robust progress. In response\nwe introduce MotifBench, comprising (1) a precisely specified pipeline and\nevaluation metrics, (2) a collection of 30 benchmark problems, and (3) an\nimplementation of this benchmark and leaderboard at\ngithub.com/blt2114/MotifBench. The MotifBench test cases are more difficult\ncompared to earlier benchmarks, and include protein design problems for which\nsolutions are known but on which, to the best of our knowledge,\nstate-of-the-art methods fail to identify any solution.",
      "generated_abstract": "esign is a core challenge in biology, enabling the creation of\nproteins with desired properties. Recent advances in deep learning have enabled\nthe development of powerful architectures for protein design, such as the\nLUCB model. Despite these advancements, existing benchmarks for protein\ndesign are limited by their design space, which often lacks diversity. To\naddress this, we introduce MotifBench, a comprehensive benchmark designed to\nevaluate deep learning-based protein design systems. MotifBench is composed of\nthree components: (1) a dataset consisting of 234,201 motifs and 124,215\ndesigns, which covers a range of biological functions; (2) a dataset of\nmotif-scaffolding pairs, which provides a training-test split to assess the\nrobustness of the models in scaffolding motifs;",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19642857142857142,
          "p": 0.2558139534883721,
          "f": 0.22222221730843805
        },
        "rouge-2": {
          "r": 0.03896103896103896,
          "p": 0.05454545454545454,
          "f": 0.04545454059343486
        },
        "rouge-l": {
          "r": 0.1875,
          "p": 0.2441860465116279,
          "f": 0.21212120720742791
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/CV/2503.10633v1",
      "true_abstract": "As there are now millions of publicly available neural networks, searching\nand analyzing large model repositories becomes increasingly important.\nNavigating so many models requires an atlas, but as most models are poorly\ndocumented charting such an atlas is challenging. To explore the hidden\npotential of model repositories, we chart a preliminary atlas representing the\ndocumented fraction of Hugging Face. It provides stunning visualizations of the\nmodel landscape and evolution. We demonstrate several applications of this\natlas including predicting model attributes (e.g., accuracy), and analyzing\ntrends in computer vision models. However, as the current atlas remains\nincomplete, we propose a method for charting undocumented regions.\nSpecifically, we identify high-confidence structural priors based on dominant\nreal-world model training practices. Leveraging these priors, our approach\nenables accurate mapping of previously undocumented areas of the atlas. We\npublicly release our datasets, code, and interactive atlas.",
      "generated_abstract": "ace's Model Atlas provides a comprehensive view of the\nmodel architecture, hyperparameters, and source code for all of the models\nin the organization's model repository. It is an invaluable tool for\nunderstanding the internal architecture of models, finding and fixing bugs,\nand evaluating model performance. However, the Atlas lacks a unified view of\nmodel architecture and hyperparameter configurations. To address this gap, we\ndevelop a novel approach for visualizing model architecture and hyperparameter\nconfigurations. We introduce a novel visualization framework for model\narchitecture, named \"Model Architectures in Action\", which allows users to\ninteractively explore the architecture of a model, navigate across the\narchitecture, and find and fix bugs. We further propose a new framework,\ncalled \"Model Hyperparameters in Action\", which enables users to explore\nhyperparameter configurations and identify potential bugs. Our framework\nimproves Model Atlas's utility",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2169811320754717,
          "p": 0.2804878048780488,
          "f": 0.24468084614531474
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.040983606557377046,
          "f": 0.038167933954898384
        },
        "rouge-l": {
          "r": 0.20754716981132076,
          "p": 0.2682926829268293,
          "f": 0.23404254827297433
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/SY/2503.09904v1",
      "true_abstract": "In studies on complex network systems using graph theory, eigen-analysis is\ntypically performed on an undirected graph model of the network. However, when\nanalyzing cascading failures in a power system, the interactions among failures\nsuggest the need for a directed graph beyond the topology of the power system\nto model directions of failure propagation. To accurately quantify failure\ninteractions for effective mitigation strategies, this paper proposes a\nstochastic interaction graph model and associated eigen-analysis. Different\ntypes of modes on failure propagations are defined and characterized by the\neigenvalues of a stochastic interaction matrix, whose absolute values are\nunity, zero, or in between. Finding and interpreting these modes helps identify\nthe probable patterns of failure propagation, either local or widespread, and\nthe participating components based on eigenvectors. Then, by lowering the\nfailure probabilities of critical components highly participating in a mode of\nwidespread failures, cascading can be mitigated. The validity of the proposed\nstochastic interaction graph model, eigen-analysis and the resulting mitigation\nstrategies is demonstrated using simulated cascading failure data on an NPCC\n140-bus system.",
      "generated_abstract": "sis of cascading failures is crucial for understanding the\neffect of unpredictable events on system dynamics. This paper introduces a\nstochastic interaction graph (SIG) framework for analyzing the interdependencies\namong failure events. The SIG model is a stochastic graph model that captures\nthe dependencies among failures and is used to represent the probabilistic\nrelationships between failures. The failure interactions are modeled as\neigenfunctions of a matrix that represents the failure interactions among\ndifferent failures. The eigenfunctions are obtained through eigen-analysis of\nthe interaction graph, which can be computed efficiently using the\nHamiltonian Monte Carlo (HMC) algorithm. The analysis of the SIG framework\nallows for the identification of critical failure interactions and the\ndevelopment of mitigation strategies for cascading failures. The results show\nthat the presence of critical interactions can significantly impact the\nresilience of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2803738317757009,
          "p": 0.38961038961038963,
          "f": 0.32608695165465507
        },
        "rouge-2": {
          "r": 0.09259259259259259,
          "p": 0.12,
          "f": 0.10452961180784055
        },
        "rouge-l": {
          "r": 0.27102803738317754,
          "p": 0.37662337662337664,
          "f": 0.31521738643726377
        }
      }
    },
    {
      "paper_id": "quant-ph.physics/atom-ph/2503.09946v1",
      "true_abstract": "The radiative properties of atoms are inherently linked to their surrounding\nenvironment. Placing an electromagnetic resonator around atoms can enhance\nspontaneous emission, as shown by Purcell in the 1940s. This approach is now\nroutinely used in quantum computing and communication to channel photons\nemitted by atoms into well-defined modes and control atom-photon interactions.\nFor solid-state artificial atoms, such as color-centers, the host lattice\nintroduces an acoustic environment, allowing excited atoms to relax by emitting\nphonons. Here we observe the acoustic Purcell effect by constructing a\nspecially engineered, microwave-frequency nanomechanical resonator around a\ncolor-center spin qubit in diamond. Using a co-localized optical mode of the\nstructure that strongly couples to the color-center's excited state, we perform\nsingle-photon-level laser spectroscopy at milliKelvin temperatures and observe\nten-fold faster spin relaxation when the spin qubit is tuned into resonance\nwith a 12 GHz acoustic mode. Additionally, we use the color-center as an\natomic-scale probe to measure the broadband phonon spectrum of the\nnanostructure up to a frequency of 28 GHz. Our work establishes a new regime of\ncontrol for quantum defects in solids and paves the way for interconnects\nbetween atomic-scale quantum memories and qubits encoded in acoustic and\nsuperconducting devices.",
      "generated_abstract": "In this paper, we investigate the acoustic Purcell effect by exploiting a\ncolor-center (CC) and a nanomechanical (NM) resonator. The CC is a bound state\nformed by an electron and a positron, which is generated by a laser pulse. We\nshow that the laser pulse can be used to control the CC population, enabling us\nto tune the CC energy into the resonance region of the NM resonator. We then\ninvestigate the effects of the laser pulse intensity and frequency on the CC\npopulation. Additionally, we consider the effect of the NM resonator on the\nCC, finding that the NM resonator can enhance the CC energy and thus increase\nthe intensity of the laser pulse. These results suggest that the NM resonator\ncan be used as a tunable and highly sensitive acoustic detector.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20454545454545456,
          "p": 0.38571428571428573,
          "f": 0.2673267281442997
        },
        "rouge-2": {
          "r": 0.04145077720207254,
          "p": 0.07547169811320754,
          "f": 0.05351170110893654
        },
        "rouge-l": {
          "r": 0.20454545454545456,
          "p": 0.38571428571428573,
          "f": 0.2673267281442997
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.18710v1",
      "true_abstract": "Understanding convergent learning -- the extent to which artificial and\nbiological neural networks develop similar representations -- is crucial for\nneuroscience and AI, as it reveals shared learning principles and guides\nbrain-like model design. While several studies have noted convergence in early\nand late layers of vision networks, key gaps remain. First, much existing work\nrelies on a limited set of metrics, overlooking transformation invariances\nrequired for proper alignment. We compare three metrics that ignore specific\nirrelevant transformations: linear regression (ignoring affine\ntransformations), Procrustes (ignoring rotations and reflections), and\npermutation/soft-matching (ignoring unit order). Notably, orthogonal\ntransformations align representations nearly as effectively as more flexible\nlinear ones, and although permutation scores are lower, they significantly\nexceed chance, indicating a robust representational basis. A second critical\ngap lies in understanding when alignment emerges during training. Contrary to\nexpectations that convergence builds gradually with task-specific learning, our\nfindings reveal that nearly all convergence occurs within the first epoch --\nlong before networks achieve optimal performance. This suggests that shared\ninput statistics, architectural biases, or early training dynamics drive\nconvergence rather than the final task solution. Finally, prior studies have\nnot systematically examined how changes in input statistics affect alignment.\nOur work shows that out-of-distribution (OOD) inputs consistently amplify\ndifferences in later layers, while early layers remain aligned for both\nin-distribution and OOD inputs, suggesting that this alignment is driven by\ngeneralizable features stable across distribution shifts. These findings fill\ncritical gaps in our understanding of representational convergence, with\nimplications for neuroscience and AI.",
      "generated_abstract": "t learning, which occurs in a wide range of biological and\nbiomimetic systems, is a form of adaptive learning where a learning agent\nadaptively changes its behavior to optimize performance under novel\nconditions. While the evolution of convergent learning is well-studied in\nthe context of individual agents, the mechanisms driving this process in\nmulti-agent systems remain largely unexplored. Here, we study how\nrepresentational alignment across agents evolves across layers and training\nconditions in a simple biomimetic learning system inspired by spiking neural\nnetworks. We find that, while the alignment of representations across layers\nimproves over training, the alignment of representations across layers decreases\nwith training. In addition, we show that alignment across layers can be\nreversed by a distribution shift. These results highlight the importance of\nalignment across layers in driving convergent learning and suggest that\nrepresentational",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18085106382978725,
          "p": 0.38636363636363635,
          "f": 0.2463768072505777
        },
        "rouge-2": {
          "r": 0.008130081300813009,
          "p": 0.016129032258064516,
          "f": 0.01081080635442112
        },
        "rouge-l": {
          "r": 0.1702127659574468,
          "p": 0.36363636363636365,
          "f": 0.23188405362738926
        }
      }
    },
    {
      "paper_id": "cs.DB.stat/OT/2403.08127v2",
      "true_abstract": "Globally, there is an increased need for guidelines to produce high-quality\ndata outputs for analysis. No framework currently exists that provides\nguidelines for a comprehensive approach to producing analysis ready data (ARD).\nThrough critically reviewing and summarising current literature, this paper\nproposes such guidelines for the creation of ARD. The guidelines proposed in\nthis paper inform ten steps in the generation of ARD: ethics, project\ndocumentation, data governance, data management, data storage, data discovery\nand collection, data cleaning, quality assurance, metadata, and data\ndictionary. These steps are illustrated through a substantive case study that\naimed to create ARD for a digital spatial platform: the Australian Child and\nYouth Wellbeing Atlas (ACYWA).",
      "generated_abstract": "cle provides guidance for the creation of analysis-ready data,\nwhich are necessary for researchers to use a data science approach.\nAnalysis-ready data are data that are suitable for analysis using data\nscience tools, such as statistical tools, machine learning models, and\nanalytical tools. This guidance focuses on the creation of data that can be\nused for data science tasks. The article provides a step-by-step process for\nthe creation of analysis-ready data. This process focuses on the creation of\ndata that is clean, consistent, and understandable. The process also focuses\non the creation of data that is free of bias and is representative of the\npopulation that is being studied. The article provides guidance for the\ncreation of analysis-ready data, including data that is collected through\nsurveys, interviews, observations, and experimental studies. The article\nprovides guidance for the creation of analysis-ready data",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2125,
          "p": 0.265625,
          "f": 0.2361111061728396
        },
        "rouge-2": {
          "r": 0.037383177570093455,
          "p": 0.041237113402061855,
          "f": 0.03921568128652505
        },
        "rouge-l": {
          "r": 0.1625,
          "p": 0.203125,
          "f": 0.18055555061728412
        }
      }
    },
    {
      "paper_id": "gr-qc.math/MP/2503.09222v1",
      "true_abstract": "In this paper, we study the energy conditions of charged traversable\nwormholes in the framework of $f(R, \\mathscr{L}_m)$\n  modified gravity. In the first case, we derive the shape functions (SFs) for\ntwo different choices of the charge function $\\mathcal{E}^2$ by considering the\nExponential Spheroid (ES) model and analyze the null energy condition (NEC). In\nthe second case, we consider a particular shape function and study its\nimplications for the energy conditions. In both cases, we obtain expressions\nfor energy density and pressure in radial and tangential directions. Our\nfindings show that the radial NEC remains satisfied across a wide range of\ncharge parameters $\\mathcal{E}$ consistent with established physical laws.\nHowever, the tangential NEC is only sustained in the range $0.1 \\leq\n\\mathcal{E} \\leq 0.6$; for higher charge values, violations occur, indicating\nthe formation of a throat-like structure necessary for wormhole stability.\nAdditionally, we compare the pressure-density profiles of these charged\nwormholes with those of compact objects such as neutron stars, revealing\ndistinct variations in matter distribution. This analysis highlights the\ncrucial role of charge and modified gravity in determining the stability and\nphysical characteristics of wormhole structures.",
      "generated_abstract": "aper, we examine the stability of charged wormholes in the\n$f(R, \\mathscr{L}_m)$ gravity, which is a generalization of the Einstein\ngravity. We find that the stability of charged wormholes is determined by\nthe energy conditions and the null energy condition (NEC), which are\ncharacterized by the null energy condition parameter $\\epsilon$, the\nquasi-local mass parameter $M_{\\text{qlm}}$, and the mass parameter $M_0$. In\nthe absence of the compact objects, charged wormholes with $\\epsilon=1$ and\n$M_{\\text{qlm}}<1$ are stable, while charged wormholes with $\\epsilon=0$ and\n$M_{\\text{qlm}}>1$ are unstable. However, when the compact objects are\npresent, the stability of charged wormholes with $\\epsilon=0$ and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.4528301886792453,
          "f": 0.2774566431487855
        },
        "rouge-2": {
          "r": 0.08839779005524862,
          "p": 0.2077922077922078,
          "f": 0.1240310035643893
        },
        "rouge-l": {
          "r": 0.18333333333333332,
          "p": 0.41509433962264153,
          "f": 0.2543352558655485
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2502.16378v1",
      "true_abstract": "Machine learning (ML) has been playing important roles in drug discovery in\nthe past years by providing (pre-)screening tools for prioritising chemical\ncompounds to pass through wet lab experiments. One of the main ML tasks in drug\ndiscovery is to build quantitative structure-activity relationship (QSAR)\nmodels, associating the molecular structure of chemical compounds with an\nactivity or property. These properties -- including absorption, distribution,\nmetabolism, excretion and toxicity (ADMET) -- are essential to model compound\nbehaviour, activity and interactions in the organism. Although several methods\nexist, the majority of them do not provide an appropriate model's\npersonalisation, yielding to bias and lack of generalisation to new data since\nthe chemical space usually shifts from application to application. This fact\nleads to low predictive performance when completely new data is being tested by\nthe model. The area of Automated Machine Learning (AutoML) emerged aiming to\nsolve this issue, outputting tailored ML algorithms to the data at hand.\nAlthough an important task, AutoML has not been practically used to assist\ncheminformatics and computational chemistry researchers often, with just a few\nworks related to the field. To address these challenges, this work introduces\nAuto-ADMET, an interpretable evolutionary-based AutoML method for chemical\nADMET property prediction. Auto-ADMET employs a Grammar-based Genetic\nProgramming (GGP) method with a Bayesian Network Model to achieve comparable or\nbetter predictive performance against three alternative methods -- standard GGP\nmethod, pkCSM and XGBOOST model -- on 12 benchmark chemical ADMET property\nprediction datasets. The use of a Bayesian Network model on Auto-ADMET's\nevolutionary process assisted in both shaping the search procedure and\ninterpreting the causes of its AutoML performance.",
      "generated_abstract": "of the adverse drug reaction (ADR) properties of new chemicals is\nimportant for both regulatory and clinical drug development. The aim of this\nwork is to develop an effective and interpretable method for predicting the\nadverse drug reaction (ADR) properties of chemicals. The study leverages the\nAutoML framework to develop a new method, Auto-ADMET, which is designed to\nautomatically optimize the chemical ADMET properties. The study conducted a\ncomprehensive evaluation of Auto-ADMET by evaluating its performance on a\nvariety of benchmark datasets. The evaluation results demonstrate that Auto-ADMET\noutperforms the state-of-the-art methods, with an average of 2.65% and 3.17%\nabsolute improvement in the Chemical-ADMET-1 and Chemical-ADMET-2 datasets,\nrespectively. Additionally",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1787709497206704,
          "p": 0.4383561643835616,
          "f": 0.2539682498529227
        },
        "rouge-2": {
          "r": 0.03529411764705882,
          "p": 0.08823529411764706,
          "f": 0.05042016398559457
        },
        "rouge-l": {
          "r": 0.16759776536312848,
          "p": 0.410958904109589,
          "f": 0.2380952339799068
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ph/2503.10397v1",
      "true_abstract": "We give a pedagogical introduction to hadron spectroscopy and structure\nstudies using functional methods. We explain the basic features of\nDyson-Schwinger, Bethe-Salpeter and Faddeev equations, which are employed to\ncalculate the spectra of mesons, baryons and four-quark states. We discuss\ndynamical mass generation as a consequence of the spontaneous breaking of\nchiral symmetry, which is intertwined with the emergence of the light pions as\nGoldstone bosons of QCD. We highlight the importance of diquark correlations in\nthe baryon sector, while for four-quark states such as the light scalar mesons\nand heavy exotics the dominant two-body clusters are typically mesons. We\nconclude with a brief discussion of hadron matrix elements like electromagnetic\nform factors and how vector-meson dominance is an automatic outcome of\nfunctional equations.",
      "generated_abstract": "In the last decade, functional methods have been widely used in the study\nof hadron physics, especially in the context of the lattice QCD. The main\nadvantage of functional methods is the ability to obtain the effective\ninteraction from the bare one, which is free of the infrared problem. In this\npaper, we provide a review of the functional methods used in hadron physics,\nwith a focus on the hadronic tensor and the hadron form factor. We also\ndiscuss some of the recent developments in functional methods in hadron\nphysics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18823529411764706,
          "p": 0.2857142857142857,
          "f": 0.22695034982143766
        },
        "rouge-2": {
          "r": 0.058333333333333334,
          "p": 0.08860759493670886,
          "f": 0.0703517540062123
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.26785714285714285,
          "f": 0.2127659526583171
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.08262v1",
      "true_abstract": "The search for the optimal pair of active and protection paths in a network\nwith Shared Risk Link Groups (SRLG) is a challenging but high-value problem in\nthe industry that is inevitable in ensuring reliable connections on the modern\nInternet. We propose a new approach to solving this problem, with a novel use\nof statistical analysis of the distribution of paths with respect to their\ncost, which is an integral part of our innovation. The key idea in our\nalgorithm is to employ iterative updates of cost bounds, allowing efficient\npruning of suboptimal paths. This idea drives an efficacious exploration of the\nsearch space. We benchmark our algorithms against the state-of-the-art\nalgorithms that exploit the alternative strategy of conflicting links\nexclusion, showing that our approach has the advantage of finding more feasible\nconnections within a set time limit.",
      "generated_abstract": "er the routing problem with SRLG-disjoint protection, where\nsolution quality is measured by the sum of the sum of the travel times and\nminimum cost. The routing problem is to connect the vertices with the shortest\npath to the destination, while avoiding conflicts. In this paper, we introduce\na pruning method for the iterative solving of the routing problem with SRLG-\ndisjoint protection, which can be applied to various pruning heuristics.\nSpecifically, we propose two pruning methods: the first one is based on the\npruning of the shortest paths, and the second one is based on the pruning of\nthe shortest paths and the cost of the conflicts. We show that the pruning\nmethods yield optimal cost-driven prunings, and we show that the optimal\ncost-driven pruning can be computed in polynomial time. We also show that the\ncost-dri",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25806451612903225,
          "p": 0.32432432432432434,
          "f": 0.2874251447653197
        },
        "rouge-2": {
          "r": 0.04411764705882353,
          "p": 0.05454545454545454,
          "f": 0.048780482860731555
        },
        "rouge-l": {
          "r": 0.21505376344086022,
          "p": 0.2702702702702703,
          "f": 0.2395209531485533
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2503.07357v1",
      "true_abstract": "In this work, we investigate the generalization of a multi-channel\nlearning-based replay speech detector, which employs adaptive beamforming and\ndetection, across different microphone arrays. In general, deep neural\nnetwork-based microphone array processing techniques generalize poorly to\nunseen array types, i.e., showing a significant training-test mismatch of\nperformance. We employ the ReMASC dataset to analyze performance degradation\ndue to inter- and intra-device mismatches, assessing both single- and\nmulti-channel configurations. Furthermore, we explore fine-tuning to mitigate\nthe performance loss when transitioning to unseen microphone arrays. Our\nfindings reveal that array mismatches significantly decrease detection\naccuracy, with intra-device generalization being more robust than inter-device.\nHowever, fine-tuning with as little as ten minutes of target data can\neffectively recover performance, providing insights for practical deployment of\nreplay detection systems in heterogeneous automatic speaker verification\nenvironments.",
      "generated_abstract": "Recent advances in speech replay have demonstrated remarkable performance\nin speech recognition. However, the learning-based approaches often struggle\nwith the mismatch between the replay speech and the acoustic environment,\ncausing significant performance degradation. This paper proposes a\nnovel framework, which combines the transfer learning strategy and the\nspeech-specific replay. The proposed method is evaluated on the publicly\navailable LibriSpeech dataset, and the results demonstrate that the proposed\nframework can effectively handle the mismatch between the replay speech and the\nacoustic environment. Furthermore, the proposed method can effectively adapt to\nthe acoustic environment, and the results show that the performance of the\nproposed method significantly outperforms the existing methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20388349514563106,
          "p": 0.328125,
          "f": 0.2514970012607121
        },
        "rouge-2": {
          "r": 0.03076923076923077,
          "p": 0.046511627906976744,
          "f": 0.03703703224451365
        },
        "rouge-l": {
          "r": 0.1941747572815534,
          "p": 0.3125,
          "f": 0.23952095335652054
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.02342v1",
      "true_abstract": "This paper proposes an original methodology based on Named Entity Recognition\n(NER) to identify companies involved in downstream space activities, i.e.,\ncompanies that provide services or products exploiting data and technology from\nspace. Using a rule-based approach, the method leverages a corpus of texts from\ndigitized French press articles to extract company names related to the\ndownstream space segment. This approach allowed the detection of 88 new\ndownstream space companies, enriching the existing database of the sector by\n33\\%. The paper details the identification process and provides guidelines for\nfuture replications, applying the method to other geographic areas, or adapting\nit to other industries where new entrants are challenging to identify using\ntraditional activity classifications.",
      "generated_abstract": "This paper introduces a novel methodology for identifying companies in\nthe downstream sector, which is of particular interest to companies operating\nin the petrochemical and energy sectors. The methodology is based on a\nminimum-distance algorithm, which is able to distinguish between companies\nwithin the same industry, while also distinguishing between companies with\nsimilar characteristics. The algorithm is based on the construction of a\ngraph, which is used to calculate a metric that measures the distance between\ntwo companies. This metric is then used to rank companies according to their\npotential for future success. The methodology is illustrated through a\ncomparative analysis of the companies in the petrochemical and energy sectors.\nThe results show that the methodology is able to identify potential acquisition\ntargets and provides a more objective evaluation of potential acquisition\nproposals.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2159090909090909,
          "p": 0.2638888888888889,
          "f": 0.23749999505000013
        },
        "rouge-2": {
          "r": 0.06306306306306306,
          "p": 0.0625,
          "f": 0.0627802640583969
        },
        "rouge-l": {
          "r": 0.19318181818181818,
          "p": 0.2361111111111111,
          "f": 0.2124999950500001
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2503.02697v1",
      "true_abstract": "This paper investigates an infinite horizon, discounted,\nconsumption-portfolio problem in a market with one bond, one liquid risky\nasset, and one illiquid risky asset with proportional transaction costs. We\nconsider an agent with liquidity preference, modeled by a Cobb-Douglas utility\nfunction that includes the liquid wealth. We analyze the properties of the\nvalue function and divide the solvency region into three regions: the buying\nregion, the no-trading region, and the selling region, and prove that all three\nregions are non-empty. We mathematically characterize and numerically solve the\noptimal policy and prove its optimality. Our numerical analysis sheds light on\nthe impact of various parameters on the optimal policy, and some intuition and\neconomic insights behind it are also analyzed. We find that liquidity\npreference encourages agents to retain more liquid wealth and inhibits\nconsumption, and may even result in a negative allocation to the illiquid\nasset. The liquid risky asset not only affects the location of the three\nregions but also has an impact on consumption. However, whether this impact on\nconsumption is promoted or inhibited depends on the degree of risk aversion of\nagents.",
      "generated_abstract": "a consumer who prefers liquid assets to other assets. We model\nconsumer preferences using a discrete choice model with two liquid assets and\nthree consumption-portfolio choices. We characterize the liquid asset preferences\nand find conditions under which the consumer's preferences are well-defined. We\nthen use this result to derive conditions for the existence of the\nliquid-asset preferences and characterize the liquid asset preferences in terms\nof the portfolio choice. The liquid asset preferences are shown to be\nmonotone and convex in the consumption-portfolio choice. Furthermore, we\nderive conditions under which the liquid asset preferences are also convex in\nthe liquid asset choices. We also derive conditions under which the liquid asset\npreferences are also convex in the liquid asset choices and show that in the\npresence of liquid asset preferences, the consumption-portfolio choice is\nconcave. We illustrate the existence of liquid",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1810344827586207,
          "p": 0.3684210526315789,
          "f": 0.2427745620555315
        },
        "rouge-2": {
          "r": 0.023391812865497075,
          "p": 0.042105263157894736,
          "f": 0.030075183378088784
        },
        "rouge-l": {
          "r": 0.15517241379310345,
          "p": 0.3157894736842105,
          "f": 0.20809248113067605
        }
      }
    },
    {
      "paper_id": "cs.AI.q-fin/CP/2501.05278v1",
      "true_abstract": "Counterfactual estimators are critical for learning and refining policies\nusing logged data, a process known as Off-Policy Evaluation (OPE). OPE allows\nresearchers to assess new policies without costly experiments, speeding up the\nevaluation process. Online experimental methods, such as A/B tests, are\neffective but often slow, thus delaying the policy selection and optimization\nprocess.\n  In this work, we explore the application of OPE methods in the context of\nresource allocation in dynamic auction environments. Given the competitive\nnature of environments where rapid decision-making is crucial for gaining a\ncompetitive edge, the ability to quickly and accurately assess algorithmic\nperformance is essential. By utilizing counterfactual estimators as a\npreliminary step before conducting A/B tests, we aim to streamline the\nevaluation process, reduce the time and resources required for experimentation,\nand enhance confidence in the chosen policies. Our investigation focuses on the\nfeasibility and effectiveness of using these estimators to predict the outcomes\nof potential resource allocation strategies, evaluate their performance, and\nfacilitate more informed decision-making in policy selection. Motivated by the\noutcomes of our initial study, we envision an advanced analytics system\ndesigned to seamlessly and dynamically assess new resource allocation\nstrategies and policies.",
      "generated_abstract": "uction markets are increasingly common in online platforms, and\nefficiently designing and implementing such markets requires the ability to\nunderstand the underlying dynamics. In this paper, we focus on auctions where\nbidders submit their bids dynamically, with the goal of maximizing their\nutility. We study off-policy evaluation and counterfactual methods for\noptimizing dynamic auction design. We first investigate off-policy evaluation\nin the classical setting where a bidder submits its current bid and the market\nresponds with the next bid. We show that off-policy evaluation is an\nunreliable strategy, as the bidders' expected utility may change significantly\nafter receiving the next bid. To address this issue, we propose a new approach\nthat uses the next-round bid as a baseline, and iteratively improves the\nbidding strategy based on a counterfactual method. We show that our method",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2047244094488189,
          "p": 0.2826086956521739,
          "f": 0.23744291750213725
        },
        "rouge-2": {
          "r": 0.03278688524590164,
          "p": 0.047619047619047616,
          "f": 0.03883494662644984
        },
        "rouge-l": {
          "r": 0.1889763779527559,
          "p": 0.2608695652173913,
          "f": 0.21917807731948885
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/GN/2412.11084v1",
      "true_abstract": "DNA barcodes are crucial in biodiversity analysis for building automatic\nidentification systems that recognize known species and discover unseen\nspecies. Unlike human genome modeling, barcode-based invertebrate\nidentification poses challenges in the vast diversity of species and taxonomic\ncomplexity. Among Transformer-based foundation models, BarcodeBERT excelled in\nspecies-level identification of invertebrates, highlighting the effectiveness\nof self-supervised pretraining on barcode-specific datasets. Recently,\nstructured state space models (SSMs) have emerged, with a time complexity that\nscales sub-quadratically with the context length. SSMs provide an efficient\nparameterization of sequence modeling relative to attention-based\narchitectures. Given the success of Mamba and Mamba-2 in natural language, we\ndesigned BarcodeMamba, a performant and efficient foundation model for DNA\nbarcodes in biodiversity analysis. We conducted a comprehensive ablation study\non the impacts of self-supervised training and tokenization methods, and\ncompared both versions of Mamba layers in terms of expressiveness and their\ncapacity to identify \"unseen\" species held back from training. Our study shows\nthat BarcodeMamba has better performance than BarcodeBERT even when using only\n8.3% as many parameters, and improves accuracy to 99.2% on species-level\naccuracy in linear probing without fine-tuning for \"seen\" species. In our\nscaling study, BarcodeMamba with 63.6% of BarcodeBERT's parameters achieved\n70.2% genus-level accuracy in 1-nearest neighbor (1-NN) probing for unseen\nspecies. The code repository to reproduce our experiments is available at\nhttps://github.com/bioscan-ml/BarcodeMamba.",
      "generated_abstract": "ity data are often represented as time-series of counts, but\ntraits like phenotypes are often continuous. This paper introduces BarcodeMamba,\na new framework for modeling continuous phenotypes and counting observations\nover time, which can be used for biodiversity analysis. BarcodeMamba models\nphenotypes as binary codes, called barcodes, which encode continuous\nphenotypes. The model is parameterized by a linear mapping of continuous\nphenotypes into binary codes, which can be used to represent continuous\nphenotypes as binary sequences. This paper demonstrates how BarcodeMamba can\nbe used to represent biodiversity data with continuous phenotypes as binary\nsequences. It also introduces a novel method for inferring the underlying\nbarcode sequences from time-series data. The model and code for BarcodeMamba\nare available at\n  https://github.com/john-baker",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.34782608695652173,
          "f": 0.21333332908088895
        },
        "rouge-2": {
          "r": 0.018691588785046728,
          "p": 0.04040404040404041,
          "f": 0.025559101106269043
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.34782608695652173,
          "f": 0.21333332908088895
        }
      }
    },
    {
      "paper_id": "cs.LG.physics/ao-ph/2503.03038v1",
      "true_abstract": "Machine learning models have shown great success in predicting weather up to\ntwo weeks ahead, outperforming process-based benchmarks. However, existing\napproaches mostly focus on the prediction task, and do not incorporate the\nnecessary data assimilation. Moreover, these models suffer from error\naccumulation in long roll-outs, limiting their applicability to seasonal\npredictions or climate projections. Here, we introduce Generative Assimilation\nand Prediction (GAP), a unified deep generative framework for assimilation and\nprediction of both weather and climate. By learning to quantify the\nprobabilistic distribution of atmospheric states under observational,\npredictive, and external forcing constraints, GAP excels in a broad range of\nweather-climate related tasks, including data assimilation, seamless\nprediction, and climate simulation. In particular, GAP is competitive with\nstate-of-the-art ensemble assimilation, probabilistic weather forecast and\nseasonal prediction, yields stable millennial simulations, and reproduces\nclimate variability from daily to decadal time scales.",
      "generated_abstract": "t a generative approach to forecasting and assimilation of weather\nand climate data. The framework employs a multimodal generative model that\nintegrates multi-scale atmospheric and oceanic data into a single, high-resolution\nimage. This model is capable of generating weather and climate variables from\nobservations at varying temporal and spatial resolutions, and it can also\ngenerate synthetic observations with prescribed properties. This approach\nenables the automatic generation of climatological maps, as well as the\nprediction of extremes, such as heat waves and floods. The framework can be\napplied to a wide range of meteorological and climatic variables, including\ntemperature, precipitation, wind speed, and surface humidity. It can also\ngenerate synthetic climate data for use in climate change research and\napplied climate modelling. This approach offers a novel and scalable solution\nto the challenges",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18691588785046728,
          "p": 0.23255813953488372,
          "f": 0.20725388106955905
        },
        "rouge-2": {
          "r": 0.036231884057971016,
          "p": 0.04065040650406504,
          "f": 0.038314171261726114
        },
        "rouge-l": {
          "r": 0.17757009345794392,
          "p": 0.22093023255813954,
          "f": 0.1968911867690409
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PR/2411.05425v1",
      "true_abstract": "This article presents a generic hybrid numerical method to price a wide range\nof options on one or several assets, as well as assets with stochastic drift or\nvolatility. In particular for equity and interest rate hybrid with local\nvolatility.",
      "generated_abstract": "r introduces a new class of local volatility models which is\nfeasible and computationally efficient to implement. These models are built\nusing a generalised version of the local volatility, which allows for more\nflexible and generalisable models. The proposed models are flexible in that\nthey allow for the inclusion of various terms in the volatility, such as\nscalar volatility, trend volatility, trend reversal volatility, and\ninterpolation functions. Furthermore, the models are general in that they can\nbe implemented for any number of assets. In addition, the models are built in\nsuch a way that they can be implemented using the Monte Carlo simulation\ntechnique, which is computationally efficient. The proposed models are\nconstructed using the Black-Scholes model, which is well-known and widely used\nin the financial markets. The models are built using the Black-Scholes",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.29411764705882354,
          "p": 0.14492753623188406,
          "f": 0.1941747528588935
        },
        "rouge-2": {
          "r": 0.02564102564102564,
          "p": 0.009259259259259259,
          "f": 0.013605438278496187
        },
        "rouge-l": {
          "r": 0.2647058823529412,
          "p": 0.13043478260869565,
          "f": 0.17475727713073816
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ph/2503.10343v1",
      "true_abstract": "We present a comprehensive reappraisal of the in-medium properties of the rho\nmeson using the inverse QCD sum rules (QCDSR) formalism, offering a novel,\nmodel-independent approach to studying hadronic modifications in nuclear\nmatter. Unlike conventional QCDSR, which rely on a predefined pole+continuum\nstructure, the inverse method reconstructs the spectral function directly from\nthe operator product expansion (OPE), eliminating assumptions about the\nspectral ansatz. To the best of our knowledge, this is the first application of\nthe inverse QCDSR method to the rho meson in nuclear matter. Our analysis\nreveals a significant reduction in the rho meson mass, consistent with previous\ntheoretical predictions, and highlights the crucial role of medium-induced\nmodifications, including condensate suppression and factorization-breaking\neffects. Furthermore, we assess the sensitivity of our results to the\nfactorization assumption and higher-dimensional condensates, demonstrating the\nnecessity of refining nonperturbative contributions for an accurate description\nof in-medium hadron properties. Our findings establish the inverse QCDSR method\nas a robust alternative to conventional spectral analysis techniques, providing\na systematically controlled framework for exploring strongly interacting matter\nunder extreme conditions. These results offer important theoretical benchmarks\nfor lattice QCD simulations and heavy-ion collision experiments, shedding light\non the restoration of chiral symmetry and the evolution of hadronic matter in\ndense environments.",
      "generated_abstract": "In this work, we reappraise the electromagnetic charge of the rho meson\nin nuclear matter using the inverse QCD sum rules method. We show that the\nelectromagnetic charge of the rho meson in nuclear matter is determined by its\ndynamical contribution to the electromagnetic current, rather than by the\nelectromagnetic current of the nucleon. Our results are in good agreement with\nexperimental data and the predictions from QCD sum rules.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17037037037037037,
          "p": 0.5,
          "f": 0.25414364261774675
        },
        "rouge-2": {
          "r": 0.06878306878306878,
          "p": 0.23636363636363636,
          "f": 0.10655737355717558
        },
        "rouge-l": {
          "r": 0.16296296296296298,
          "p": 0.4782608695652174,
          "f": 0.2430939188608407
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.16352v1",
      "true_abstract": "We present a design-based model of a randomized experiment in which the\nobserved outcomes are informative about the joint distribution of potential\noutcomes within the experimental sample. We derive a likelihood function that\nmaintains curvature with respect to the joint distribution of potential\noutcomes, even when holding the marginal distributions of potential outcomes\nconstant -- curvature that is not maintained in a sampling-based likelihood\nthat imposes a large sample assumption. Our proposed decision rule guesses the\njoint distribution of potential outcomes in the sample as the distribution that\nmaximizes the likelihood. We show that this decision rule is Bayes optimal\nunder a uniform prior. Our optimal decision rule differs from and significantly\noutperforms a ``monotonicity'' decision rule that assumes no defiers or no\ncompliers. In sample sizes ranging from 2 to 40, we show that the Bayes\nexpected utility of the optimal rule increases relative to the monotonicity\nrule as the sample size increases. In two experiments in health care, we show\nthat the joint distribution of potential outcomes that maximizes the likelihood\nneed not include compliers even when the average outcome in the intervention\ngroup exceeds the average outcome in the control group, and that the maximizer\nof the likelihood may include both compliers and defiers, even when the average\nintervention effect is large and statistically significant.",
      "generated_abstract": "We present a likelihood-based approach for estimating the joint\ndistribution of potential outcomes of health care interventions. Our method\nintegrates a design-based likelihood with a modified Wald estimator of the\nconditional expectation of the potential outcome. We demonstrate the method's\nusefulness by applying it to a real-world study on the effectiveness of\nintensive behavioral interventions for smoking cessation. We find that the\nconditional expectation of the potential outcome, estimated using the modified\nWald estimator, is positively correlated with the outcome variable. This\nresult suggests that the interventions are effective in reducing smoking\nbehavior. We also find that the mean of the potential outcome is positively\ncorrelated with the prevalence of smoking, suggesting that the interventions\nhave some impact on smoking behavior.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1941747572815534,
          "p": 0.2898550724637681,
          "f": 0.2325581347302597
        },
        "rouge-2": {
          "r": 0.056818181818181816,
          "p": 0.1,
          "f": 0.07246376349506435
        },
        "rouge-l": {
          "r": 0.18446601941747573,
          "p": 0.2753623188405797,
          "f": 0.2209302277535155
        }
      }
    },
    {
      "paper_id": "q-bio.SC.q-bio/SC/2402.10638v2",
      "true_abstract": "During cell division, the mitotic spindle moves dynamically through the cell\nto position the chromosomes and determine the ultimate spatial position of the\ntwo daughter cells. These movements have been attributed to the action of\ncortical force generators which pull on the astral microtubules to position the\nspindle, as well as pushing events by these same microtubules against the cell\ncortex and plasma membrane. Attachment and detachment of cortical force\ngenerators working antagonistically against centring forces of microtubules\nhave been modelled previously (Grill et al. 2005, Phys. Rev. Lett. 94:108104)\nvia stochastic simulations and mean-field Fokker-Planck equations (describing\nrandom motion of force generators) to predict oscillations of a spindle pole in\none spatial dimension. Using systematic asymptotic methods, we reduce the\nFokker-Planck system to a set of ordinary differential equations (ODEs),\nconsistent with a set proposed by Grill et al., which can provide accurate\npredictions of the conditions for the Fokker-Planck system to exhibit\noscillations. In the limit of small restoring forces, we derive an algebraic\nprediction of the amplitude of spindle-pole oscillations and demonstrate the\nrelaxation structure of nonlinear oscillations. We also show how noise-induced\noscillations can arise in stochastic simulations for conditions in which the\nmean-field Fokker-Planck system predicts stability, but for which the period\ncan be estimated directly by the ODE model and the amplitude by a related\nstochastic differential equation that incorporates random binding kinetics.",
      "generated_abstract": "pindles are important structures that ensure chromosome segregation\nand the proper division of the cell. In mitosis, chromosomes align and form a\nspindle that spans the length of the cell. A spindle consists of two separate\nstructures: the microtubules that transport the chromosomes and the spindle\npoles, which are responsible for their orientation. During the process of\nmitotic chromosome alignment, the microtubules become unstable and begin to\noscillate. This oscillation, which is often referred to as \"spindle\noscillations\", is believed to be responsible for chromosome alignment.\n  In this work, we study the dynamics of a model of mitotic spindle oscillations\nin the presence of noise. We derive the equations governing the time evolution\nof the oscillations and show that the oscillations undergo periodic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18840579710144928,
          "p": 0.3466666666666667,
          "f": 0.24413145083647436
        },
        "rouge-2": {
          "r": 0.04285714285714286,
          "p": 0.07894736842105263,
          "f": 0.05555555099451341
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.30666666666666664,
          "f": 0.2159624367519673
        }
      }
    },
    {
      "paper_id": "q-fin.PM.econ/EM/2502.13461v1",
      "true_abstract": "Style investing creates asset classes (or the so-called \"styles\") with low\ncorrelations, aligning well with the principle of \"Holy Grail of investing\" in\nterms of portfolio selection. The returns of styles naturally form a\ntensor-valued time series, which requires new tools for studying the dynamics\nof the conditional correlation matrix to facilitate the aforementioned\nprinciple. Towards this goal, we introduce a new tensor dynamic conditional\ncorrelation (TDCC) model, which is based on two novel treatments:\ntrace-normalization and dimension-normalization. These two normalizations adapt\nto the tensor nature of the data, and they are necessary except when the tensor\ndata reduce to vector data. Moreover, we provide an easy-to-implement\nestimation procedure for the TDCC model, and examine its finite sample\nperformance by simulations. Finally, we assess the usefulness of the TDCC model\nin international portfolio selection across ten global markets and in large\nportfolio selection for 1800 stocks from the Chinese stock market.",
      "generated_abstract": "In this paper, we propose a novel dynamic conditional correlation model,\ntitled the Tensor Dynamic Conditional Correlation Model (TDCCM). TDCCM\nintegrates the state-space model and the tensor product autoregressive model,\nenabling the model to capture the interdependencies between assets in an\narbitrary number of dimensions. Additionally, we propose a novel estimation\nmethod, which allows for a fast, efficient, and accurate estimation of the\nTDCCM parameters. The TDCCM model is applied to the U.S. equity market, and the\nmodel's prediction accuracy is compared to that of other state-of-the-art\nmodels. The results show that the TDCCM model significantly outperforms other\nmodels, especially in forecasting the correlation matrix and the conditional\ncorrelation matrix.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20754716981132076,
          "p": 0.3055555555555556,
          "f": 0.24719100641838163
        },
        "rouge-2": {
          "r": 0.04895104895104895,
          "p": 0.06930693069306931,
          "f": 0.05737704432847394
        },
        "rouge-l": {
          "r": 0.19811320754716982,
          "p": 0.2916666666666667,
          "f": 0.23595505136220185
        }
      }
    },
    {
      "paper_id": "physics.plasm-ph.physics/plasm-ph/2503.06176v1",
      "true_abstract": "In strictly axisymmetric configurations of tokamaks, field-line tracing\nreduces from a three-dimensional ODE system to a two-dimensional one, where\nPoincar\\'e-Bendixson theorem applies and guarantees the nonexistence of chaos.\nThe formulae of functional perturbation theory (FPT) mostly simplify to compact\nclosed-form expressions to allow the computation to finish instantly, which\ncould improve and accelerate the existing plasma control systems by detangling\nthe plasma dynamics from the magnetic topology change. FPT can conveniently\ncalculate how the key geometric objects of magnetic topology:\n  1. the divertor X-point(s) and the magnetic axis,\n  2. the last closed flux surface (LCFS)\n  3. flux surfaces\n  change under perturbation. For example, when the divertor X-point shifts\noutwards, the LCFS there must expand accordingly, but not necessarily for other\nplaces of the LCFS, which could also contract, depending on the perturbation.\nFPT can not only facilitate adaptive control of plasma, but also enable\nutilizing as much as possible space in the vacuum vessel by weakening the\nplasma-wall interaction (PWI) via tuning the eigenvalues of $\\mathcal{DP}^m$ of\nthe divertor X-point(s), such that the field line connection lengths in the\nscrape-off layer (SOL) are long enough to achieve detachment. Increasing flux\nexpansion $f_x$ is another option for detachment and can also be facilitated by\nFPT.\n  Apart from the edge, FPT can also benefit the understanding of the plasma\ncore. Since the magnetic axis O-point would also shift under perturbation and\nthe shift is known by FPT, the O-point can be controlled without full knowledge\nof the plasma response, which shall not significantly change the tendency.",
      "generated_abstract": "l perturbation theory (FPT) is a perturbation theory based on the\nfunctional form of the perturbed Hamiltonian. In FPT, the perturbed Hamiltonian\nis expressed as a functional of the perturbed configuration and the perturbed\nHamiltonian itself. This formalism has been widely used in plasma physics for\ncalculating equilibrium and non-equilibrium properties of plasmas. The present\npaper is devoted to the simplified form of FPT. The paper describes a\nconvenient mathematical form of the perturbed Hamiltonian. This form allows\nus to write down the perturbation series of the perturbed Hamiltonian. This\nform allows us to obtain the perturbation series of the perturbed Hamiltonian\nin a simplified way. This simplification allows us to get the series of the\nperturbed Hamiltonian in a simplified way. We also present a simplified form of\nthe series of the perturbation series of the perturbed Hamiltonian. This\nsimplified",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12269938650306748,
          "p": 0.3508771929824561,
          "f": 0.1818181779789257
        },
        "rouge-2": {
          "r": 0.029661016949152543,
          "p": 0.07777777777777778,
          "f": 0.04294478127893447
        },
        "rouge-l": {
          "r": 0.11042944785276074,
          "p": 0.3157894736842105,
          "f": 0.1636363597971075
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.16694v1",
      "true_abstract": "When is it beneficial to constrain creativity? Creativity thrives with\nfreedom, but when people collaborate to create artifacts, there is tension\nbetween giving individuals freedom to revise, and protecting prior\nachievements. To test how imposing constraints may affect collective\ncreativity, we performed cultural evolution experiments where participants\ncollaborated to create melodies and images in chains. With melodies, we found\nthat limiting step size (number of musical notes that can be changed) improved\npleasantness ratings for created tunes. Similar results were observed in\ncohorts of musicians, and with different selection regimes. In contrast,\nlimiting step size in creating images consistently reduced pleasantness. These\nconflicting findings suggest that in domains such as music, where artifacts can\nbe easily damaged, and where evolutionary outcomes are hard to foresee,\ncollective creativity may benefit from imposing small step sizes. We discuss\nparallels with search algorithms and the evolution of conservative birdsong\ncultures.",
      "generated_abstract": "raints on editing affects cultural evolution has been a long-standing\nquestion in evolutionary theory. In this paper, we present a computational\nframework that combines computational modeling and evolutionary theory to\nanswer this question. Our framework incorporates the constraint of editability\ninto the evolutionary dynamics of cultural traits, which we model as a\nrepresentative of a gene pool of cultural traits. We find that editability\nprovides an important source of selection on cultural traits. Editability\nprovides a means for cultural traits to evolve faster than non-editable\ntraits. This is because editable cultural traits have a higher fitness than\nnon-editable cultural traits. We also find that editability influences the\nevolutionary dynamics of cultural traits. We find that editability can\nenhance cultural traits that have been evolved through the evolution of\nediting. We also find that edit",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1504424778761062,
          "p": 0.23943661971830985,
          "f": 0.18478260395616744
        },
        "rouge-2": {
          "r": 0.02112676056338028,
          "p": 0.02727272727272727,
          "f": 0.023809518890149668
        },
        "rouge-l": {
          "r": 0.1415929203539823,
          "p": 0.22535211267605634,
          "f": 0.17391303873877612
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2503.00455v1",
      "true_abstract": "Existing Existing automatic audio generation methods struggle to generate\npodcast-like audio programs effectively. The key challenges lie in in-depth\ncontent generation, appropriate and expressive voice production. This paper\nproposed PodAgent, a comprehensive framework for creating audio programs.\nPodAgent 1) generates informative topic-discussion content by designing a\nHost-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for\nsuitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis\nmethod to generate expressive conversational speech. Given the absence of\nstandardized evaluation criteria for podcast-like audio generation, we\ndeveloped comprehensive assessment guidelines to effectively evaluate the\nmodel's performance. Experimental results demonstrate PodAgent's effectiveness,\nsignificantly surpassing direct GPT-4 generation in topic-discussion dialogue\ncontent, achieving an 87.4% voice-matching accuracy, and producing more\nexpressive speech through LLM-guided synthesis. Demo page:\nhttps://podcast-agent.github.io/demo/. Source code:\nhttps://github.com/yujxx/PodAgent.",
      "generated_abstract": "g is the fastest growing audio content format, with a growing\nradar of podcasts being produced by both professional and amateur podcasters.\nHowever, the sheer number of podcasts makes it difficult for consumers to\nfind relevant content. This paper presents PodAgent, a comprehensive framework\nfor podcast generation. PodAgent leverages a novel semantic-aware generative\nmodel to generate podcasts with rich audio, text, and metadata content. This\nmodel leverages the audio and text generated by the Generative Podcasting\nSystem (GPS) to produce diverse content. The framework also includes\nstructured metadata generation, which includes the creation of episode\nstructures, episode tags, and episode descriptions. The framework is evaluated\non three podcast generation benchmarks: podcast dataset generation,\nmulti-task training, and the generation of podcasts with audio and text\nstructures. Results demonstrate that Pod",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19801980198019803,
          "p": 0.24390243902439024,
          "f": 0.218579230026576
        },
        "rouge-2": {
          "r": 0.047619047619047616,
          "p": 0.05,
          "f": 0.048780482807852986
        },
        "rouge-l": {
          "r": 0.18811881188118812,
          "p": 0.23170731707317074,
          "f": 0.20765026827794214
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10316v1",
      "true_abstract": "In this paper, we consider a tunable liquid convex lens-assisted imaging\nreceiver for indoor multiple-input multiple-output (MIMO) visible light\ncommunication (VLC) systems. In contrast to existing MIMO VLC receivers that\nrely on fixed optical lenses, the proposed receiver leverages the additional\ndegrees of freedom offered by liquid lenses via adjusting both focal length and\norientation angles of the lens. This capability facilitates the mitigation of\nspatial correlation between the channel gains, thereby enhancing the overall\nsignal quality and leading to improved bit-error rate (BER) performance. We\npresent an accurate channel model for the liquid lens-assisted VLC system by\nusing three-dimensional geometry and geometric optics. To achieve optimal\nperformance under practical conditions such as random receiver orientation and\nuser mobility, optimization of both focal length and orientation angles of the\nlens are required. To this end, driven by the fact that channel models are\nmathematically complex, we present two optimization schemes including a\nblockwise machine learning (ML) architecture that includes convolution layers\nto extract spatial features from the received signal, long-short term memory\nlayers to predict the user position and orientation, and fully connected layers\nto estimate the optimal lens parameters. Numerical results are presented to\ncompare the performance of each scheme with conventional receivers. Results\nshow that a significant BER improvement is achieved when liquid lenses and\npresented ML-based optimization approaches are used. Specifically, the BER can\nbe improved from $6\\times 10^{-2}$ to $1.4\\times 10^{-3}$ at an average\nsignal-to-noise ratio of $30$ dB.",
      "generated_abstract": "aper, we propose a novel liquid lens-based imaging receiver\nfor multi-input multi-output (MIMO) virtual lightwave circuits (VLCs) using\nliquid crystal (LC) based liquid lens (LL). The LL is a transparent\nmaterial with anisotropic optical properties and can be used to create\noptical lenses with arbitrary shapes, even in the liquid state. By applying\nLL to the liquid crystal display (LCD), the liquid lens can create an optical\nlens with a single focal point. By tuning the liquid crystal thickness and\nrotating the liquid crystal, we can control the focal length and shape of the\nLC-based liquid lens. The LC-based liquid lens is used to create an\noptical-to-electrical (O2E) converter for liquid crystal-based liquid lens\n(LC-LL)",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1411042944785276,
          "p": 0.34328358208955223,
          "f": 0.19999999587107758
        },
        "rouge-2": {
          "r": 0.03017241379310345,
          "p": 0.07,
          "f": 0.04216867048918608
        },
        "rouge-l": {
          "r": 0.1411042944785276,
          "p": 0.34328358208955223,
          "f": 0.19999999587107758
        }
      }
    },
    {
      "paper_id": "math.ST.stat/CO/2502.03849v1",
      "true_abstract": "This paper presents a new algorithm (and an additional trick) that allows to\ncompute fastly an entire curve of post hoc bounds for the False Discovery\nProportion when the underlying bound $V^*_{\\mathfrak{R}}$ construction is based\non a reference family $\\mathfrak{R}$ with a forest structure {\\`a} la Durand et\nal. (2020). By an entire curve, we mean the values\n$V^*_{\\mathfrak{R}}(S_1),\\dotsc,V^*_{\\mathfrak{R}}(S_m)$ computed on a path of\nincreasing selection sets $S_1\\subsetneq\\dotsb\\subsetneq S_m$, $|S_t|=t$. The\nnew algorithm leverages the fact that going from $S_t$ to $S_{t+1}$ is done by\nadding only one hypothesis.",
      "generated_abstract": "We propose a new method to compute a confidence curve of the False Discovery\nProportion (FDP) using a reference family with a forest structure. The\ncurve is computed by computing the confidence intervals for the number of\nfalse positives in each bin. The method is based on a fast algorithm that\nincorporates the reference family into the computation of the confidence\nintervals. The computational efficiency of the algorithm is compared to the\nstandard method using a reference family with a forest structure and a\nconventional method. We validate our method on simulated data and real data\nfrom the U.S. Immigration and Customs Enforcement dataset. The results show\nthat the proposed method is more efficient than the conventional method in\nterms of computational speed and the number of confidence intervals. The\nproposed method is implemented in the R package\nfdpcurve.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3424657534246575,
          "p": 0.3472222222222222,
          "f": 0.3448275812071344
        },
        "rouge-2": {
          "r": 0.17647058823529413,
          "p": 0.12605042016806722,
          "f": 0.1470588186683008
        },
        "rouge-l": {
          "r": 0.3287671232876712,
          "p": 0.3333333333333333,
          "f": 0.3310344777588586
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.09848v1",
      "true_abstract": "In this work, we present a second-order numerical scheme to address the\nsolution of optimal control problems constrained by the evolution of nonlinear\nFokker-Planck equations arising from socio-economic dynamics. In order to\ndesign an appropriate numerical scheme for control realization, a coupled\nforward-backward system is derived based on the associated optimality\nconditions. The forward equation, corresponding to the Fokker-Planck dynamics,\nis discretized using a structure preserving scheme able to capture steady\nstates. On the other hand, the backward equation, modeled as a\nHamilton-Jacobi-Bellman problem, is solved via a semi-Lagrangian scheme that\nsupports large time steps while preserving stability. Coupling between the\nforward and backward problems is achieved through a gradient descent\noptimization strategy, ensuring convergence to the optimal control. Numerical\nexperiments demonstrate second-order accuracy, computational efficiency, and\neffectiveness in controlling different examples across various scenarios in\nsocial dynamics. This approach provides a reliable computational tool for the\nstudy of opinion manipulation and consensus formation in socially structured\nsystems.",
      "generated_abstract": "In this paper, we propose a second order numerical scheme for solving\noptimization problems of the form\n$ \\underset{t \\in [0,T",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10619469026548672,
          "p": 0.5714285714285714,
          "f": 0.1791044749688127
        },
        "rouge-2": {
          "r": 0.01948051948051948,
          "p": 0.15,
          "f": 0.03448275658607489
        },
        "rouge-l": {
          "r": 0.10619469026548672,
          "p": 0.5714285714285714,
          "f": 0.1791044749688127
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.12309v1",
      "true_abstract": "Square matrices often arise in microeconomics, particularly in network models\naddressing applications from opinion dynamics to platform regulation. Spectral\ntheory provides powerful tools for analyzing their properties. We present an\naccessible overview of several fundamental applications of spectral methods in\nmicroeconomics, focusing especially on the Perron-Frobenius Theorem's role and\nits connection to centrality measures. Applications include social learning,\nnetwork games, public goods provision, and market intervention under\nuncertainty. The exposition assumes minimal social science background, using\nspectral theory as a unifying mathematical thread to introduce interested\nreaders to some exciting current topics in microeconomic theory.",
      "generated_abstract": "r introduces a new perspective on eigenvalues in microeconomics.\nEven when the graph of a linear stochastic differential equation (LSE) has\nno eigenvalues, it may be possible to construct a stochastic matrix that has\neigenvalues, even when the LSE is singular. This leads to an alternative\ndefinition of eigenvalues in microeconomics. We use this definition to study\neigenvalues in the presence of non-stationary shocks to the demand function.\nWe find that the eigenvalue decomposition of the matrix of mean-field expectations\nof the LSE is related to the eigenvalue decomposition of the matrix of mean-field\nexpectations of the corresponding LSE with the non-stationary shocks. The\neigenvalue decomposition of the matrix of mean-field expectations of the LSE\nwith the non-stationary shocks is obtained by the eigenvalue decomposition of\nthe matrix of mean-field expectations",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1125,
          "p": 0.15254237288135594,
          "f": 0.12949639799182255
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1125,
          "p": 0.15254237288135594,
          "f": 0.12949639799182255
        }
      }
    },
    {
      "paper_id": "math.GM.math/GM/2503.07406v1",
      "true_abstract": "This paper presents a distinctive prime detection approach. This method use\nGM-(n+1) sequences to effectively eliminate complex numbers. The sequences,\nwhich consist of odd a number of (n+1), exclude all components except for the\ninitial prime integer. Only the first prime number is presented. This research\nproposes an approach using this model to identify exceptional candidates and\nexamine their distribution. This study examines the interconnections among the\nlaws of division, basic gaps, and their applications in analytical procedures.\nComputer studies may provide a novel perspective on the theory of prime\nnumbers, demonstrating the effectiveness of this approach in refining the\nsearch space for primes.",
      "generated_abstract": "aper we study the relationship between the prime numbers and Gauss-Miller\nsequences. The Gauss-Miller sequence is a sequence that represents the\ncombinatorial structure of the prime numbers. We provide a new approach to\nidentify the prime numbers using Gauss-Miller sequences. Our approach is based\non the fact that the Gauss-Miller sequence is a subset of the prime numbers. We\nprovide a new algorithm for identifying the prime numbers using Gauss-Miller\nsequences. Our approach is based on the fact that the Gauss-Miller sequence is\na subset of the prime numbers. We also provide a new approach to identify the\ncomposite numbers using Gauss-Miller sequences. Our approach is based on the\nfact that the Gauss-Miller sequence is a subset of the prime numbers. We provide\na new algorithm for identifying the composite numbers using Gauss-Miller sequences.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22077922077922077,
          "p": 0.4594594594594595,
          "f": 0.29824560965066177
        },
        "rouge-2": {
          "r": 0.02912621359223301,
          "p": 0.05555555555555555,
          "f": 0.03821655599659269
        },
        "rouge-l": {
          "r": 0.2077922077922078,
          "p": 0.43243243243243246,
          "f": 0.28070175000153896
        }
      }
    },
    {
      "paper_id": "physics.class-ph.physics/class-ph/2503.04206v1",
      "true_abstract": "The refraction of light by dispersion-free dielectric media can be modeled\nusing well-localized macroscopic wave packets, enabling a description in terms\nof pseudo-particles. This approach is often used in thought experiments to\nillustrate aspects of the Abraham-Minkowski debate. This work uses the particle\npicture to show at an elementary level how different types of momenta come into\nplay, and how light refraction can be explained at the level of particles. A\nspecial exactly solvable microscopic model is used to illustrate the interplay\nand tension between microscopic physics and the conventional effective-medium\nMaxwell equations.",
      "generated_abstract": "n is the phenomenon of light bending around obstacles. A common\nobservation is that, when the medium is a vacuum, light refracts at an angle to\nthe direction of propagation. The law of reflection states that if a\ntranslucent surface faces a medium with a refractive index n, then light\nrefracted from that surface must have a refractive index n. A more general\nform of the law of reflection is called the law of refraction, and states that\nlight refracted from a surface with a refractive index n must have a\nrefractive index n. This law is derived by observing that the angle of incidence\nis related to the angle of refraction by the law of sines. We present a\nmodern derivation of the law of refraction based on the concept of\nconservation laws. We derive a closed formula for the angle of refraction",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20588235294117646,
          "p": 0.2028985507246377,
          "f": 0.20437955704406216
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.02654867256637168,
          "f": 0.02955664531049125
        },
        "rouge-l": {
          "r": 0.19117647058823528,
          "p": 0.18840579710144928,
          "f": 0.18978101689807675
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/CV/2503.10629v1",
      "true_abstract": "Adversarial attacks pose significant challenges for vision models in critical\nfields like healthcare, where reliability is essential. Although adversarial\ntraining has been well studied in natural images, its application to biomedical\nand microscopy data remains limited. Existing self-supervised adversarial\ntraining methods overlook the hierarchical structure of histopathology images,\nwhere patient-slide-patch relationships provide valuable discriminative\nsignals. To address this, we propose Hierarchical Self-Supervised Adversarial\nTraining (HSAT), which exploits these properties to craft adversarial examples\nusing multi-level contrastive learning and integrate it into adversarial\ntraining for enhanced robustness. We evaluate HSAT on multiclass histopathology\ndataset OpenSRH and the results show that HSAT outperforms existing methods\nfrom both biomedical and natural image domains. HSAT enhances robustness,\nachieving an average gain of 54.31% in the white-box setting and reducing\nperformance drops to 3-4% in the black-box setting, compared to 25-30% for the\nbaseline. These results set a new benchmark for adversarial training in this\ndomain, paving the way for more robust models. Our Code for training and\nevaluation is available at https://github.com/HashmatShadab/HSAT.",
      "generated_abstract": "ological images are crucial for diagnosing cancer. However,\nlarge-scale, multi-class classification models struggle to capture the\ncomplexity of cancer subtypes, which hinders the development of accurate\nclassifiers. To address this issue, we propose a novel framework that\nintroduces a hierarchical self-supervised adversarial training strategy. The\nframework consists of a single-level self-supervised adversarial training\nstrategy, which learns to distinguish benign and malignant tissue. The\nframework leverages an adversarial loss function to optimize the model, which\nis adapted to the specific task and the characteristics of the data. To\noptimize the model, the framework employs a hierarchical self-supervised\nadversarial training strategy. In the first level, the framework employs a\nself-supervised learning algorithm to learn the general structure of the data.\nIn the second level",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18110236220472442,
          "p": 0.3150684931506849,
          "f": 0.22999999536450008
        },
        "rouge-2": {
          "r": 0.036585365853658534,
          "p": 0.05714285714285714,
          "f": 0.044609660668039924
        },
        "rouge-l": {
          "r": 0.16535433070866143,
          "p": 0.2876712328767123,
          "f": 0.20999999536450012
        }
      }
    },
    {
      "paper_id": "gr-qc.gr-qc/2503.10323v1",
      "true_abstract": "Einstein equations can be written in the so-called Fully Constrained\nFormulation (FCF). This formulation has two different sectors: the elliptic\nsector, formed by the Hamiltonian and Momentum constraints together with the\nequations derived from the gauge choice; and the hyperbolic sector, formed by\nthe evolution of the rest of the spacetime metric variables, which encodes the\ngravitational radiation. In this work, we present a modification of both\nsectors that keeps local uniqueness properties of the elliptic system of\nequations and includes a hierarchical post-Newtonian structure of all the\nelliptic and hyperbolic equations. This reformulation can have potential\napplications in cosmology and relativistic astrophysics. Moreover, we show how\ninitial stationary data can be computed numerically using this formulation\nwithout assuming a conformally flat spatial metric, with the illustrative\nexample of a rotating neutron star.",
      "generated_abstract": "In this paper, we reformulate Einstein equations in the Fully\nconstrained Formulation (FCF) by introducing a new scalar function $f$, which\nis a generalization of the gravitational potential. We show that the\nconstraints are preserved under this new scalar function. We derive the\nequations of motion for the new scalar function in the FCF, and show that the\nnew scalar function obeys the constraint equations in the FCF, as well as the\nNewton equations. We also show that the new scalar function obeys the post-Newtonian\nexpansion. We also show that the new scalar function obeys the initial data\ncondition in the FCF. Furthermore, we show that the new scalar function is\nunique in the FCF.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25555555555555554,
          "p": 0.4423076923076923,
          "f": 0.3239436573298949
        },
        "rouge-2": {
          "r": 0.06504065040650407,
          "p": 0.1038961038961039,
          "f": 0.07999999526450029
        },
        "rouge-l": {
          "r": 0.23333333333333334,
          "p": 0.40384615384615385,
          "f": 0.2957746432453879
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/NE/2503.05573v1",
      "true_abstract": "Model-based Reinforcement Learning (MBRL) has emerged as a promising paradigm\nfor autonomous driving, where data efficiency and robustness are critical. Yet,\nexisting solutions often rely on carefully crafted, task specific extrinsic\nrewards, limiting generalization to new tasks or environments. In this paper,\nwe propose InDRiVE (Intrinsic Disagreement based Reinforcement for Vehicle\nExploration), a method that leverages purely intrinsic, disagreement based\nrewards within a Dreamer based MBRL framework. By training an ensemble of world\nmodels, the agent actively explores high uncertainty regions of environments\nwithout any task specific feedback. This approach yields a task agnostic latent\nrepresentation, allowing for rapid zero shot or few shot fine tuning on\ndownstream driving tasks such as lane following and collision avoidance.\nExperimental results in both seen and unseen environments demonstrate that\nInDRiVE achieves higher success rates and fewer infractions compared to\nDreamerV2 and DreamerV3 baselines despite using significantly fewer training\nsteps. Our findings highlight the effectiveness of purely intrinsic exploration\nfor learning robust vehicle control behaviors, paving the way for more scalable\nand adaptable autonomous driving systems.",
      "generated_abstract": "unknown environments is a challenging task that requires\ncomprehensive knowledge of the world. Exploration with a goal-oriented\nreinforcement learning (RL) agent is a popular method, yet it often suffers\nfrom high exploration costs and suboptimal exploration. To address these\nissues, we propose Intrinsic Disagreement-based Reinforcement for Vehicle\nExploration (InDRiVE). InDRiVE learns a generalized world model (GWM) and\ncombines it with an intrinsic-based RL agent. The GWM encodes the world state,\nallowing the agent to explore without knowing the exact goal. This\nintrinsic-based exploration is guided by intrinsic reward functions, which\nencourage the agent to explore the GWM. InDRiVE achieves superior exploration\nperformance and is less costly than state-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19117647058823528,
          "p": 0.3291139240506329,
          "f": 0.24186046046771234
        },
        "rouge-2": {
          "r": 0.029239766081871343,
          "p": 0.049019607843137254,
          "f": 0.0366300319494431
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.3037974683544304,
          "f": 0.2232558093049217
        }
      }
    },
    {
      "paper_id": "math.ST.stat/ML/2503.03356v1",
      "true_abstract": "We use tools from random matrix theory to study the multi-spiked tensor\nmodel, i.e., a rank-$r$ deformation of a symmetric random Gaussian tensor. In\nparticular, thanks to the nature of local optimization methods used to find the\nmaximum likelihood estimator of this model, we propose to study the phase\ntransition phenomenon for finding critical points of the corresponding\noptimization problem, i.e., those points defined by the Karush-Kuhn-Tucker\n(KKT) conditions. Moreover, we characterize the limiting alignments between the\nestimated signals corresponding to a critical point of the likelihood and the\nground truth signals. With the help of these results, we propose a new\nestimator of the rank-$r$ tensor weights by solving a system of polynomial\nequations, which is asymptotically unbiased contrary the maximum likelihood\nestimator.",
      "generated_abstract": "the statistical properties of random tensors with multiple\ncorrelated spikes. We consider the spike-and-slab model where a spike of\nprobability $p$ is placed at the origin, followed by a spike of\nprobability $q$ at the origin. The spike-and-slab model arises in various\nfields, including spin glasses and the Gaussian free field, and has been\ninvestigated in numerous papers. We study the correlation structure of the\ncorrelated spikes, which depends on the tensor rank. We show that the\ncorrelation between the spikes is weakly dependent on the tensor rank.\nMoreover, we show that the correlation between the spikes is negligible if the\ntensor rank is large. This result implies that the correlation structure of\nthe correlated spikes is independent of the tensor rank. In addition, we show\nthat the correlation between the spikes is weakly",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20512820512820512,
          "p": 0.2318840579710145,
          "f": 0.2176870698486743
        },
        "rouge-2": {
          "r": 0.04310344827586207,
          "p": 0.05263157894736842,
          "f": 0.04739335997843767
        },
        "rouge-l": {
          "r": 0.20512820512820512,
          "p": 0.2318840579710145,
          "f": 0.2176870698486743
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/SE/2503.10099v1",
      "true_abstract": "While the trend of decentralized governance is obvious (cryptocurrencies and\nblockchains are widely adopted by multiple sovereign countries), initiating\ngovernance proposals within Decentralized Autonomous Organizations (DAOs) is\nstill challenging, i.e., it requires providing a low-level transaction payload,\ntherefore posing significant barriers to broad community participation. To\naddress these challenges, we propose a multi-agent system powered by Large\nLanguage Models with a novel Label-Centric Retrieval algorithm to automate the\ntranslation from natural language inputs into executable proposal transactions.\nThe system incorporates DAOLang, a Domain-Specific Language to simplify the\nspecification of various governance proposals. The key optimization achieved by\nDAOLang is a semantic-aware abstraction of user input that reliably secures\nproposal generation with a low level of token demand. A preliminary evaluation\non real-world applications reflects the potential of DAOLang in terms of\ngenerating complicated types of proposals with existing foundation models, e.g.\nGPT-4o.",
      "generated_abstract": "The evolution of blockchain technology has led to a need for a new class of\ntransactions that can be executed on behalf of multiple parties. These\n\"proposal transactions\" enable the creation and distribution of new tokens,\namong other functions. However, existing proposals do not provide a way to\nvalidate these transactions. We propose an alternative approach, using abstract\nDAO semantics to specify the requirements of a proposal. We demonstrate that\nthis approach yields an efficient and composable system for creating, validating,\nand executing proposals. Our system is implemented in the Ethereum\nhardfork-compatible EVM, and is available at\nhttps://github.com/agentsdao/agentdao-contracts.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1592920353982301,
          "p": 0.23684210526315788,
          "f": 0.19047618566781457
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1415929203539823,
          "p": 0.21052631578947367,
          "f": 0.1693121645037934
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.10578v1",
      "true_abstract": "In this paper we investigate the generalization error of gradient descent\n(GD) applied to an $\\ell_2$-regularized OLS objective function in the linear\nmodel. Based on our analysis we develop new methodology for computationally\ntractable and statistically efficient linear prediction in a high-dimensional\nand massive data scenario (large-$n$, large-$p$). Our results are based on the\nsurprising observation that the generalization error of optimally tuned\nregularized gradient descent approaches that of an optimal benchmark procedure\n$monotonically$ in the iteration number $m$. On the other hand standard GD for\nOLS (without explicit regularization) can achieve the benchmark only in\ndegenerate cases. This shows that (optimal) explicit regularization can be\nnearly statistically efficient (for large $m$) whereas implicit regularization\nby (optimal) early stopping can not.\n  To complete our methodology, we provide a fully data driven and\ncomputationally tractable choice of $\\ell_2$ regularization parameter $\\lambda$\nthat is computationally cheaper than cross-validation. On this way, we follow\nand extend ideas of Dicker (2014) to the non-gaussian case, which requires new\nresults on high-dimensional sample covariance matrices that might be of\nindependent interest.",
      "generated_abstract": "In this work, we study the performance of high-dimensional gradient\ndescent (HDGD) on large-scale optimization problems. Unlike the commonly used\nexplicit regularization methods such as L-BFGS and ADAM, we focus on implicit\nregularization methods, including L-BFGS-B, ADAM-B, and HMC-B, which are\noften more efficient than explicit methods. We show that the performance of\nimplicit methods is comparable to the performance of explicit methods in the\nsame settings. Moreover, we propose a simple algorithm to construct implicit\nmethods, and we show that implicit methods can achieve better performance than\nexplicit methods in the same settings. Finally, we conduct extensive numerical\nexperiments to validate our findings.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21008403361344538,
          "p": 0.3787878787878788,
          "f": 0.27027026568064283
        },
        "rouge-2": {
          "r": 0.047619047619047616,
          "p": 0.08888888888888889,
          "f": 0.06201549933297311
        },
        "rouge-l": {
          "r": 0.18487394957983194,
          "p": 0.3333333333333333,
          "f": 0.23783783324821048
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2502.11954v1",
      "true_abstract": "This paper presents a novel approach to stochastic volatility (SV) modeling\nby utilizing nonparametric techniques that enhance our ability to capture the\nvolatility of financial time series data, with a particular emphasis on the\nnon-Gaussian behavior of asset return distributions. Although traditional\nparametric SV models can be useful, they often suffer from restrictive\nassumptions regarding errors, which may inadequately represent extreme values\nand tail behavior in financial returns. To address these limitations, we\npropose two semiparametric SV models that use data to better approximate error\ndistributions. To facilitate the computation of model parameters, we developed\na Markov Chain Monte Carlo (MCMC) method for estimating model parameters and\nvolatility dynamics. Simulations and empirical tests on S&P 500 data indicate\nthat nonparametric models can minimize bias and variance in volatility\nestimation, providing a more accurate reflection of market expectations about\nvolatility. This methodology serves as a promising alternative to conventional\nparametric models, improving precision in financial risk assessment and\ndeepening our understanding of the volatility dynamics of financial returns.",
      "generated_abstract": "This paper proposes a semiparametric stochastic volatility model for\noil prices. The model incorporates both stochastic volatility and\nsemiparametric effects, and is characterized by a parameter vector\n$\\boldsymbol{\\beta}$ that is estimated through a maximum likelihood\nestimator. The model is extended to incorporate an error correction term\nto address the misspecification of the stochastic volatility function. The\nestimated model is tested using the Tobit model and the model with the\nsemiparametric error correction term. The results of the Tobit model are\ncompared with the results of the ARCH model. The results of the model with\nthe semiparametric error correction term are compared with the results of the\nARCH model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.3541666666666667,
          "f": 0.2035928102750189
        },
        "rouge-2": {
          "r": 0.018867924528301886,
          "p": 0.03896103896103896,
          "f": 0.025423724417194093
        },
        "rouge-l": {
          "r": 0.12605042016806722,
          "p": 0.3125,
          "f": 0.17964071446663568
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2502.15867v1",
      "true_abstract": "Artificial intelligence (AI) is transforming scientific research, including\nproteomics. Advances in mass spectrometry (MS)-based proteomics data quality,\ndiversity, and scale, combined with groundbreaking AI techniques, are unlocking\nnew challenges and opportunities in biological discovery. Here, we highlight\nkey areas where AI is driving innovation, from data analysis to new biological\ninsights. These include developing an AI-friendly ecosystem for proteomics data\ngeneration, sharing, and analysis; improving peptide and protein identification\nand quantification; characterizing protein-protein interactions and protein\ncomplexes; advancing spatial and perturbation proteomics; integrating\nmulti-omics data; and ultimately enabling AI-empowered virtual cells.",
      "generated_abstract": "cement of biology through advances in proteomics and artificial\nintelligence (AI) has the potential to transform biology and biomedical\nresearch. However, the rapid pace of advances in these technologies has\nhighlighted significant challenges in integrating them into effective\npractices. To address these challenges, we propose a framework for strategic\npriorities for transformative progress in advancing biology with proteomics and\nAI. The framework identifies five key areas for advancing biology with\nproteomics and AI: (1) establishing a multidisciplinary research program,\n(2) developing research infrastructures, (3) improving data quality and\naccessibility, (4) establishing data sharing mechanisms, and (5) advancing\ndata-driven research. By aligning these priorities with the strategic plans of\nvarious stakeholders,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24324324324324326,
          "p": 0.24324324324324326,
          "f": 0.24324323824324334
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.019417475728155338,
          "f": 0.0209424034078025
        },
        "rouge-l": {
          "r": 0.22972972972972974,
          "p": 0.22972972972972974,
          "f": 0.22972972472972986
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2502.00024v1",
      "true_abstract": "This project focuses on analyzing retail market trends using historical sales\ndata, search trends, and customer reviews. By identifying the patterns and\ntrending products, the analysis provides actionable insights for retailers to\noptimize inventory management and marketing strategies, ultimately enhancing\ncustomer satisfaction and maximizing revenue.",
      "generated_abstract": "aper, we have conducted a research on retail market. We have\nexplored the characteristics of retail market, and analyzed the retail\nmarket. The retail market is a complex economic system, which has a variety of\ncharacteristics and is not easy to be described. The retail market has\nmultiple layers, and each layer has different characteristics and impacts. The\nretail market has a variety of economic layers, and each layer has its own\ncharacteristics and impacts. We have conducted a research on the retail market\nto understand its characteristics, and analyzed the retail market. We have\nfound that the retail market has multiple layers, and each layer has different\ncharacteristics and impacts. The retail market has a variety of economic\nlayers, and each layer has its own characteristics and impacts. We have\nconducted a research on the retail market to understand its characteristics,\nand analyzed the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15,
          "p": 0.14285714285714285,
          "f": 0.14634145841760873
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.014925373134328358,
          "f": 0.018018013232693423
        },
        "rouge-l": {
          "r": 0.15,
          "p": 0.14285714285714285,
          "f": 0.14634145841760873
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.02713v1",
      "true_abstract": "This paper presents new empirical evidence from four emerging economies on\nthe relationship between educational assortative mating and household income\ninequality. Using a methodological approach that allows for studying marital\nsorting patterns without imposing restrictive assumptions about search\nfrictions, the study finds that people in Brazil, Indonesia, Mexico, and South\nAfrica tend to sort into internally homogeneous marriages based on education\nlevel. While educational sorting has a noticeable impact on household income\ninequality in any given year, changes in the degree of sorting over time barely\nhave any impact on inequality. Further analysis reveals that this\ncounterintuitive result is due to different dynamics within educational groups.\nThe inequality-decreasing impact from reduced sorting among the highly educated\nis almost entirely offset by the inequality-increasing impact from increased\nsorting among the least educated. While it is certainly reassuring that\nconcerns about educational assortative mating having a potentially large effect\non income disparities between households appear to be unwarranted, these\nfindings suggest another concerning narrative. Marginalization processes are\noccurring at low levels of the educational distribution. The least educated are\nbeing left behind, facing limited labor market opportunities and diminished\nchances of achieving upward socioeconomic mobility through marriage to more\neducated partners.",
      "generated_abstract": "r examines the determinants of educational assortative mating\nin Brazil, Indonesia, Mexico, and South Africa. Using data from the Brazilian\nNational Survey on Educational Assortative Mating (ENAMA) and the International\nEconomic Association Household Survey (IHASS), we investigate the impact of\neducational attainment and educational intergenerational mobility on\nassortative mating. Our findings suggest that a higher educational attainment\nleads to a higher likelihood of having a spouse with the same level of\neducation. We further find that a higher educational intergenerational\nmobility level reduces the likelihood of educational assortative mating. Our\nfindings also show that the gender gap in educational attainment is a key\ndeterminant of educational assortative mating. The gender gap in educational\nattainment also increases the likelihood of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19014084507042253,
          "p": 0.4153846153846154,
          "f": 0.26086956090923946
        },
        "rouge-2": {
          "r": 0.05291005291005291,
          "p": 0.10752688172043011,
          "f": 0.07092198139505082
        },
        "rouge-l": {
          "r": 0.16901408450704225,
          "p": 0.36923076923076925,
          "f": 0.2318840536628627
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.cond-mat/mes-hall/2503.09689v1",
      "true_abstract": "Using a self-consistent Hartree-Fock theory, we show that the recently\nobserved ferromagnetism in twisted bilayer WSe$_2$ [Nat. Commun. 16, 1959\n(2025)] can be understood as a Stoner-like instability of\ninteraction-renormalized moir\\'e bands. We quantitatively reproduce the\nobserved Lifshitz transition as function of hole filling and applied electric\nfield that marks the boundary between layer-hybridized and layer-polarized\nregimes. The former supports a ferromagnetic valley-polarized ground state\nbelow half-filling, developing a topological charge gap at half-filling for\nsmall twists. At larger twist angles there is a transition to a gapped\ntriangular N\\'eel antiferromagnet. The layer-polarized regime supports a stripe\nantiferromagnet below half-filling and a wing-shaped multiferroic ground state\nabove half-filling. We map the evolution of these states as a function of\nfilling factor, electric field, twist angle, and interaction strength. Beyond\nproviding an understanding of recent experiments, our methodology is applicable\nto a broad class of moir\\'e systems.",
      "generated_abstract": "ed bilayer WSe$_2$ is a promising material for future spintronics\nand quantum technologies. In the twisted bilayer WSe$_2$, the spin orbit\ninteraction and the antisymmetric band structure lead to the formation of\nspin-polarized topological states. In this work, we explore the magnetic\nproperties in the twisted bilayer WSe$_2$ with the spin-orbit interaction and\nantisymmetric band structure. The band structure is calculated by the\nGaussian-type method, and the Berry curvature is calculated by the\nBloch-Wannier integration method. The calculated band structures show that the\ntopological band structure is preserved, and the antisymmetric band structure\nis not destroyed. The calculated band dispersion shows that the magnetic\nfield is able to suppress the band gap and induce magnetic orders. The\nmagnetic order parameters are calculated",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19230769230769232,
          "p": 0.30303030303030304,
          "f": 0.2352941128968859
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.050505050505050504,
          "f": 0.04184099933124476
        },
        "rouge-l": {
          "r": 0.19230769230769232,
          "p": 0.30303030303030304,
          "f": 0.2352941128968859
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.hep-th/2503.10423v1",
      "true_abstract": "Sterile neutrinos can influence the evolution of the universe, and thus\ncosmological observations can be used to search for sterile neutrinos. In this\nstudy, we utilized the latest baryon acoustic oscillations data from DESI,\ncombined with the cosmic microwave background data from Planck and the\nfive-year supernova data from DES, to constrain the interacting dark energy\n(IDE) models involving both cases of massless and massive sterile neutrinos. We\nconsider four typical forms of the interaction term $Q=\\beta H \\rho_{\\rm de}$,\n$Q=\\beta H \\rho_{\\rm c}$, $Q=\\beta H_{0} \\rho_{\\rm de}$, and $Q=\\beta H_{0}\n\\rho_{\\rm c}$, respectively. Our analysis indicates that the current data\nprovide only a hint of the existence of massless sterile neutrinos (as dark\nradiation) at about the $1\\sigma$ level. In contrast, no evidence supports the\nexistence of massive sterile neutrinos. Furthermore, in IDE models, the\ninclusion of (massless/massive) sterile neutrinos has a negligible impact on\nthe constraint of the coupling parameter $\\beta$. The IDE model of $Q=\\beta H\n\\rho_{\\rm c}$ with sterile neutrinos does not favor an interaction. However,\nthe other three IDE models with sterile neutrinos support an interaction in\nwhich dark energy decays into dark matter.",
      "generated_abstract": "t a systematic search for sterile neutrinos using the DESI\nobservational data. The analysis is based on the combination of two independent\ndata sets, the DESI baryon acoustic oscillations (BAO) and the DESI supernovae\n(SN) data. We use the two independent data sets to constrain the parameter\nspace of the interacting dark energy (IDE) models. The results indicate that\nthere is a low significance of the sterile neutrino parameter space. The\nresults of the analysis using the combined data set are consistent with the\nDES data alone, indicating that the dark energy is dominated by a cosmological\nconstant. The results of the analysis using the combined data set are\nconsistent with the DES data alone, indicating that the dark energy is dominated\nby a cosmological constant. The results of the analysis using the combined data\nset are consistent with the DES data alone, indicating that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24778761061946902,
          "p": 0.4666666666666667,
          "f": 0.32369941743459524
        },
        "rouge-2": {
          "r": 0.10493827160493827,
          "p": 0.19318181818181818,
          "f": 0.13599999543808017
        },
        "rouge-l": {
          "r": 0.23893805309734514,
          "p": 0.45,
          "f": 0.3121387237929768
        }
      }
    },
    {
      "paper_id": "math.SG.math/GT/2503.10283v1",
      "true_abstract": "Given a closed connected symplectic manifold $(M,\\omega)$, we construct an\nalternating $\\mathbb{R}$-bilinear form\n$\\mathfrak{b}=\\mathfrak{b}_{\\mu_{\\mathrm{Sh}}}$ on the real first cohomology of\n$M$ from Shelukhin's quasimorphism $\\mu_{\\mathrm{Sh}}$. Here\n$\\mu_{\\mathrm{Sh}}$ is defined on the universal cover of the group of\nHamiltonian diffeomorphisms on $(M,\\omega)$. This bilinear form is invariant\nunder the symplectic mapping class group action, and $\\mathfrak{b}$ yields a\nconstraint on the fluxes of commuting two elements in the group of\nsymplectomorphisms on $(M,\\omega)$. These results might be seen as an analog of\nRousseau's result for an open connected symplectic manifold, where he recovered\nthe symplectic pairing from the Calabi homomorphism. Furthermore,\n$\\mathfrak{b}$ controls the extendability of Shelukhin's quasimorphisms, as\nwell as the triviality of a characteristic class of Reznikov. To construct\n$\\mathfrak{b}$, we build general machinery for a group $G$ of producing a\nreal-valued $\\mathbb{Z}$-bilinear form $\\mathfrak{b}_{\\mu}$ from a\n$G$-invariant quasimorphism $\\mu$ on the commutator subgroup of $G$.",
      "generated_abstract": "uct a bilinear form and a homomorphism from the group of\nhomotopy classes of paths of length two in a Riemann surface to the group of\npolynomials in three variables. The bilinear form is the product of two\nhomomorphisms from the group of homotopy classes of paths of length two in a\nRiemann surface to the group of linear forms. The homomorphism from the group\nof homotopy classes of paths of length two in a Riemann surface to the group\nof polynomials in three variables is constructed from the quasimorphism\nShelukhin's. We also prove the Shelukhin's conjecture: the homomorphism from\nthe group of homotopy classes of paths of length two in a Riemann surface to\nthe group of linear forms is induced by the Shelukhin's quasimorphism. We\nconstruct a homomorphism from the group of homotopy classes of paths of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16483516483516483,
          "p": 0.39473684210526316,
          "f": 0.23255813537888353
        },
        "rouge-2": {
          "r": 0.043478260869565216,
          "p": 0.09836065573770492,
          "f": 0.06030150328628096
        },
        "rouge-l": {
          "r": 0.16483516483516483,
          "p": 0.39473684210526316,
          "f": 0.23255813537888353
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.03356v1",
      "true_abstract": "We use tools from random matrix theory to study the multi-spiked tensor\nmodel, i.e., a rank-$r$ deformation of a symmetric random Gaussian tensor. In\nparticular, thanks to the nature of local optimization methods used to find the\nmaximum likelihood estimator of this model, we propose to study the phase\ntransition phenomenon for finding critical points of the corresponding\noptimization problem, i.e., those points defined by the Karush-Kuhn-Tucker\n(KKT) conditions. Moreover, we characterize the limiting alignments between the\nestimated signals corresponding to a critical point of the likelihood and the\nground truth signals. With the help of these results, we propose a new\nestimator of the rank-$r$ tensor weights by solving a system of polynomial\nequations, which is asymptotically unbiased contrary the maximum likelihood\nestimator.",
      "generated_abstract": "er the problem of estimating a tensor from a collection of\nrandom tensors each of which has multiple correlated spikes. This problem\nappears in a variety of applications, such as the reconstruction of tensors\nfrom noisy measurements, or the estimation of tensors from measurements\ncorrelated with the tensor. We derive a new result on the statistical limit of\nthis problem. Our result is a generalization of the results of\n\\cite{arjevani2022high} and \\cite{golowich2023estimating}, which focused on the\ncase of a single correlated spike. We show that the statistical limit is\ndeterministic, in the sense that it is the same for all tensors in the\ncollection. This is an improvement over the deterministic limit of\n\\cite{arjevani2022high} and \\cite{golowich2023est",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.20634920634920634,
          "f": 0.18439715817715419
        },
        "rouge-2": {
          "r": 0.034482758620689655,
          "p": 0.03773584905660377,
          "f": 0.036036031046182006
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.19047619047619047,
          "f": 0.17021276101403365
        }
      }
    },
    {
      "paper_id": "cs.AR.cs/AR/2503.05290v1",
      "true_abstract": "Transformers are central to advances in artificial intelligence (AI),\nexcelling in fields ranging from computer vision to natural language\nprocessing. Despite their success, their large parameter count and\ncomputational demands challenge efficient acceleration. To address these\nlimitations, this paper proposes MatrixFlow, a novel co-designed\nsystem-accelerator architecture based on a loosely coupled systolic array\nincluding a new software mapping approach for efficient transformer code\nexecution. MatrixFlow is co-optimized via a novel dataflow-based matrix\nmultiplication technique that reduces memory overhead. These innovations\nsignificantly improve data throughput, which is critical for handling the\nextensive computations required by transformers. We validate our approach\nthrough full system simulation using gem5 across various BERT and ViT\nTransformer models featuring different data types, demonstrating significant\napplication-wide speed-ups. Our method achieves up to a 22x improvement\ncompared to a many-core CPU system, and outperforms the closest\nstate-of-the-art loosely-coupled and tightly-coupled accelerators by over 5x\nand 8x, respectively.",
      "generated_abstract": "Accelerating transformer models with system-on-chip (SoC) accelerators is\nan emerging trend in AI. However, the complex, interconnected architecture of\ntransformer models makes it challenging to design efficient SoC accelerators for\nthem. In this paper, we present a novel methodology that integrates\nsystem-level simulations with accelerator design, enabling efficient\nco-design of accelerators for transformer applications. We demonstrate that\nour approach enables system-accelerator co-design with high performance for\ntransformer models, enabling a 1.4X performance boost on top of the\nexisting SoC performance of the same model. Our approach can be extended to\nother transformer-based models, offering a novel framework for accelerating\nAI applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15748031496062992,
          "p": 0.28169014084507044,
          "f": 0.2020201974201613
        },
        "rouge-2": {
          "r": 0.0136986301369863,
          "p": 0.020833333333333332,
          "f": 0.016528920833277803
        },
        "rouge-l": {
          "r": 0.14960629921259844,
          "p": 0.2676056338028169,
          "f": 0.19191918731915122
        }
      }
    },
    {
      "paper_id": "math.MG.math/GN/2502.11615v1",
      "true_abstract": "The Gromov-Hausdorff distance is a dissimilarity metric capturing how far two\nspaces are from being isometric. The Gromov-Prokhorov distance is a similar\nnotion for metric measure spaces. In this paper, we study the topological\ndimension of the Gromov-Hausdorff and Gromov-Prokhorov spaces. We show that the\ndimension of the space of isometry classes of metric spaces with at most $n$\npoints endowed with the Gromov-Hausdorff distance is $\\frac{n(n-1)}{2}$, and\nthat of mm-isomorphism classes of metric measure spaces whose support consists\nof $n$ points is $\\frac{(n+2)(n-1)}{2}$. Hence, the spaces of all isometry\nclasses of finite metric spaces and of all mm-isomorphism classes of finite\nmetric measure spaces are strongly countable dimensional. If, instead, the\ncardinalities are not limited, the spaces are strongly infinite-dimensional.",
      "generated_abstract": "We study the topological dimension of the Gromov-Hausdorff space and the\nGromov-Prokhorov space, and characterize the topological dimension of their\nlimit spaces. We also show that the topological dimension of the Gromov-Hausdorff\nspace is always equal to or less than that of the Gromov-Prokhorov space, and\nshow that the topological dimension of the Gromov-Prokhorov space is always\nequal to or less than that of the Gromov-Hausdorff space. We also show that\nthe topological dimension of the Gromov-Hausdorff space is equal to or less\nthan that of the Gromov-Prokhorov space when the metric space is compact.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2459016393442623,
          "p": 0.5357142857142857,
          "f": 0.3370786473728066
        },
        "rouge-2": {
          "r": 0.10638297872340426,
          "p": 0.24390243902439024,
          "f": 0.148148143918793
        },
        "rouge-l": {
          "r": 0.2459016393442623,
          "p": 0.5357142857142857,
          "f": 0.3370786473728066
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.04883v1",
      "true_abstract": "Automatic Speech Recognition (ASR) performance for low-resource languages is\nstill far behind that of higher-resource languages such as English, due to a\nlack of sufficient labeled data. State-of-the-art methods deploy\nself-supervised transfer learning where a model pre-trained on large amounts of\ndata is fine-tuned using little labeled data in a target low-resource language.\nIn this paper, we present and examine a method for fine-tuning an SSL-based\nmodel in order to improve the performance for Frisian and its regional dialects\n(Clay Frisian, Wood Frisian, and South Frisian). We show that Frisian ASR\nperformance can be improved by using multilingual (Frisian, Dutch, English and\nGerman) fine-tuning data and an auxiliary language identification task. In\naddition, our findings show that performance on dialectal speech suffers\nsubstantially, and, importantly, that this effect is moderated by the\nelicitation approach used to collect the dialectal data. Our findings also\nparticularly suggest that relying solely on standard language data for ASR\nevaluation may underestimate real-world performance, particularly in languages\nwith substantial dialectal variation.",
      "generated_abstract": "r presents a comprehensive study of two distinct ASR models\nfor dialectal Frisian, a Dutch-speaking language spoken in the Netherlands,\nBelgium, and the Netherlands Antilles. The first model is a standard ASR\nmodel, trained on a high-resource dataset with English as a primary language.\nThe second model is a dialectal ASR model trained on a multilingual dataset\nwith English as a primary language and Dutch as a secondary language. The\nstudy evaluates both models in a multilingual setting by comparing their\nperformance on English, Dutch, and Frisian languages. The results show that the\nstandard model outperforms the dialectal model, with a 5.5% improvement in\nFrisian recognition accuracy. Additionally, the dialectal model's performance\nwas further improved by fine-tuning on the multilingual dataset, with a\n5.3% improvement in Frisian",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25225225225225223,
          "p": 0.4057971014492754,
          "f": 0.3111111063833334
        },
        "rouge-2": {
          "r": 0.030864197530864196,
          "p": 0.045454545454545456,
          "f": 0.03676470106509579
        },
        "rouge-l": {
          "r": 0.24324324324324326,
          "p": 0.391304347826087,
          "f": 0.29999999527222226
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2412.13311v1",
      "true_abstract": "This paper investigates cash productivity as a signal for future stock\nperformance, building on the cash-return framework of Faulkender and Wang\n(2006). Using financial and market data from WRDS, we calculate cash returns as\na proxy for operational efficiency and evaluate a long-only strategy applied to\nNasdaq-listed non-financial firms. Results show limited predictive power across\nthe broader Nasdaq universe but strong performance in a handpicked portfolio,\nwhich achieves significant positive alpha after controlling for the Fama-French\nthree factors. These findings underscore the importance of refined universe\nselection. While promising, the strategy requires further validation, including\nthe incorporation of transaction costs and performance testing across economic\ncycles. Our results suggest that cash productivity, when combined with other\ncomplementary signals and careful universe selection, can be a valuable tool\nfor generating excess returns.",
      "generated_abstract": "y examines the relationship between short-term assets and\nstock performance by using a multivariate gravity model. Using the\nhistorical data of 152 companies listed on the Shenzhen Stock Exchange from\n2011 to 2020, the model is constructed using a set of 11 short-term assets\n(real estate, hotels, tourism, education, medical, retail, aviation,\nconstruction, cement, petroleum, and chemicals) and 52 stocks, and the model is\ntested for the presence of significant relationships between short-term assets\nand stock performance. The results show that the correlation between short-term\nassets and stock performance is positive and statistically significant. The\nresults also show that short-term assets play a significant role in predicting\nstock performance. This study provides new insights into the relationship\nbetween short-term assets and stock performance and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18269230769230768,
          "p": 0.2638888888888889,
          "f": 0.21590908607438028
        },
        "rouge-2": {
          "r": 0.015503875968992248,
          "p": 0.02040816326530612,
          "f": 0.017621140467699168
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.2222222222222222,
          "f": 0.18181817698347122
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.20838v1",
      "true_abstract": "Marine ecosystem monitoring via Passive Acoustic Monitoring (PAM) generates\nvast data, but deep learning often requires precise annotations and short\nsegments. We introduce DSMIL-LocNet, a Multiple Instance Learning framework for\nwhale call detection and localization using only bag-level labels. Our\ndual-stream model processes 2-30 minute audio segments, leveraging spectral and\ntemporal features with attention-based instance selection. Tests on Antarctic\nwhale data show longer contexts improve classification (F1: 0.8-0.9) while\nmedium instances ensure localization precision (0.65-0.70). This suggests MIL\ncan enhance scalable marine monitoring. Code:\nhttps://github.com/Ragib-Amin-Nihal/DSMIL-Loc",
      "generated_abstract": "tion passive acoustic monitoring (L-DPAM) has become a popular\nmethod for monitoring underwater sounds. However, the acoustic data often\ncontain low-frequency noise (LFN) and the inherent noise level is high.\nThis poses a significant challenge for acoustic event detection and\nlocalization. Traditional methods, such as spectral analysis, often rely on\nground truth, which is often difficult to obtain due to the complexity of\nLFN and the high noise level. In this paper, we propose a weakly supervised\nmultiple instance learning (WS-MIL) framework for whale call detection and\nlocalization in L-DPAM data. The WS-MIL framework consists of two components:\nthe feature extraction component and the localization component. The\nfeature extraction component extracts the acoustic features from the L-DPAM\ndata, and the localization",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21176470588235294,
          "p": 0.22784810126582278,
          "f": 0.21951219012864376
        },
        "rouge-2": {
          "r": 0.06741573033707865,
          "p": 0.05454545454545454,
          "f": 0.06030150259336926
        },
        "rouge-l": {
          "r": 0.21176470588235294,
          "p": 0.22784810126582278,
          "f": 0.21951219012864376
        }
      }
    },
    {
      "paper_id": "cs.CL.cs/CY/2503.08588v1",
      "true_abstract": "Previous studies have established that language models manifest stereotyped\nbiases. Existing debiasing strategies, such as retraining a model with\ncounterfactual data, representation projection, and prompting often fail to\nefficiently eliminate bias or directly alter the models' biased internal\nrepresentations. To address these issues, we propose BiasEdit, an efficient\nmodel editing method to remove stereotypical bias from language models through\nlightweight networks that act as editors to generate parameter updates.\nBiasEdit employs a debiasing loss guiding editor networks to conduct local\nedits on partial parameters of a language model for debiasing while preserving\nthe language modeling abilities during editing through a retention loss.\nExperiments on StereoSet and Crows-Pairs demonstrate the effectiveness,\nefficiency, and robustness of BiasEdit in eliminating bias compared to\ntangental debiasing baselines and little to no impact on the language models'\ngeneral capabilities. In addition, we conduct bias tracing to probe bias in\nvarious modules and explore bias editing impacts on different components of\nlanguage models.",
      "generated_abstract": "models (LMs) are widely used in many fields, including\ndebiasing and equity-focused applications. While debiasing LMs via fine-tuning\ncan effectively remove the stereotyped bias, it often results in performance\ndegradation, particularly in low-bias domains. To address this, we propose\nBiasEdit, a debiasing framework that enables fine-tuning and fine-tuning with\nfine-tuning on a bias-corrected dataset. Specifically, BiasEdit first\nfine-tunes the debiasing model on a bias-corrected dataset, and then fine-tunes\nthe debiasing model on a bias-corrected dataset. This approach not only\nincreases the model's performance in low-bias domains, but also improves the\nmodel's performance in high-bias domains. Our experiments on the SST-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17757009345794392,
          "p": 0.2878787878787879,
          "f": 0.21965317447158286
        },
        "rouge-2": {
          "r": 0.032679738562091505,
          "p": 0.05813953488372093,
          "f": 0.041840999577038726
        },
        "rouge-l": {
          "r": 0.1588785046728972,
          "p": 0.25757575757575757,
          "f": 0.19653178718834585
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/CV/2503.10624v1",
      "true_abstract": "Fitting a body to a 3D clothed human point cloud is a common yet challenging\ntask. Traditional optimization-based approaches use multi-stage pipelines that\nare sensitive to pose initialization, while recent learning-based methods often\nstruggle with generalization across diverse poses and garment types. We propose\nEquivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline\nthat estimates cloth-to-body surface mapping through locally approximate SE(3)\nequivariance, encoding tightness as displacement vectors from the cloth surface\nto the underlying body. Following this mapping, pose-invariant body features\nregress sparse body markers, simplifying clothed human fitting into an\ninner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show\nthat ETCH significantly outperforms state-of-the-art methods -- both\ntightness-agnostic and tightness-aware -- in body fitting accuracy on loose\nclothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant\ntightness design can even reduce directional errors by (67.2% ~ 89.8%) in\none-shot (or out-of-distribution) settings. Qualitative results demonstrate\nstrong generalization of ETCH, regardless of challenging poses, unseen shapes,\nloose clothing, and non-rigid dynamics. We will release the code and models\nsoon for research purposes at https://boqian-li.github.io/ETCH/.",
      "generated_abstract": "ing, the task of fitting a clothed human body to a given image,\nhas long been studied and has been shown to be challenging due to the\nintrinsic body-specific properties of human bodies. Recent advances in\nautonomous robotics have further highlighted the importance of body fitting,\nas it is essential for autonomous manipulation tasks such as grasping,\nclimbing, and grasping. However, existing body fitting methods often rely on\nad hoc assumptions, such as a single shape parameterization for the human body,\nwhich is not guaranteed to be equivariant to body rotations. To address this\nlimitation, we propose ETCH, a novel method that generalizes body fitting to\nclothed humans via equivariant tightness. ETCH uses a shape parameterization\nthat is equivariant to body rotations, which is a more natural assumption\nthan assuming a single shape parameterization",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18120805369127516,
          "p": 0.313953488372093,
          "f": 0.22978722940190144
        },
        "rouge-2": {
          "r": 0.053763440860215055,
          "p": 0.08130081300813008,
          "f": 0.06472491430169389
        },
        "rouge-l": {
          "r": 0.174496644295302,
          "p": 0.3023255813953488,
          "f": 0.22127659110402906
        }
      }
    },
    {
      "paper_id": "cs.CL.q-fin/TR/2412.10823v1",
      "true_abstract": "Financial sentiment analysis is crucial for understanding the influence of\nnews on stock prices. Recently, large language models (LLMs) have been widely\nadopted for this purpose due to their advanced text analysis capabilities.\nHowever, these models often only consider the news content itself, ignoring its\ndissemination, which hampers accurate prediction of short-term stock movements.\nAdditionally, current methods often lack sufficient contextual data and\nexplicit instructions in their prompts, limiting LLMs' ability to interpret\nnews. In this paper, we propose a data-driven approach that enhances\nLLM-powered sentiment-based stock movement predictions by incorporating news\ndissemination breadth, contextual data, and explicit instructions. We cluster\nrecent company-related news to assess its reach and influence, enriching\nprompts with more specific data and precise instructions. This data is used to\nconstruct an instruction tuning dataset to fine-tune an LLM for predicting\nshort-term stock price movements. Our experimental results show that our\napproach improves prediction accuracy by 8\\% compared to existing methods.",
      "generated_abstract": "g the sentiment and content of tweets is a crucial yet under-explored\ntask in stock market research. Traditional models struggle to capture the\ncomplex relationship between sentiment and stock movement, often relying on\nsimple machine learning approaches to generate sentiment labels. In this paper,\nwe propose FinGPT, a finance-specific LLM-based model that integrates\ndissemination-aware sentiment classification and context-enriched sentiment\nprediction to enhance stock movement prediction. Our framework utilizes the\ndissemination capability of LLMs to accurately predict the underlying sentiment\nof tweets. This enables the model to understand the context of tweets, which\nis crucial for predicting the sentiment of subsequent tweets. To achieve this,\nwe integrate two essential modules: (1) a sentiment-enhanced sentence\nembedding model that captures the contextual relationships between tweets,\ntweets",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25217391304347825,
          "p": 0.34523809523809523,
          "f": 0.2914572815534962
        },
        "rouge-2": {
          "r": 0.05333333333333334,
          "p": 0.06837606837606838,
          "f": 0.05992508870933846
        },
        "rouge-l": {
          "r": 0.23478260869565218,
          "p": 0.32142857142857145,
          "f": 0.27135677904093336
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.05514v1",
      "true_abstract": "Securing Internet of Things (IoT) devices presents increasing challenges due\nto their limited computational and energy resources. Radio Frequency\nFingerprint Identification (RFFI) emerges as a promising authentication\ntechnique to identify wireless devices through hardware impairments. RFFI\nperformance under low signal-to-noise ratio (SNR) scenarios is significantly\ndegraded because the minute hardware features can be easily swamped in noise.\nIn this paper, we leveraged the diffusion model to effectively restore the RFF\nunder low SNR scenarios. Specifically, we trained a powerful noise predictor\nand tailored a noise removal algorithm to effectively reduce the noise level in\nthe received signal and restore the device fingerprints. We used Wi-Fi as a\ncase study and created a testbed involving 6 commercial off-the-shelf Wi-Fi\ndongles and a USRP N210 software-defined radio (SDR) platform. We conducted\nexperimental evaluations on various SNR scenarios. The experimental results\nshow that the proposed algorithm can improve the classification accuracy by up\nto 34.9%.",
      "generated_abstract": "aper, we propose a novel denoise diffusion model (DDM) based\napplication to radio frequency (RF) fingerprint identification (FID). The\nproposed model leverages DDM to capture the noise patterns in the FID,\nthereby enhancing the robustness of the model in identifying the FID. The\nproposed model consists of two components: (1) the noise-robust DDM and (2) a\nclassifier. The noise-robust DDM is a convolutional autoencoder with an\nadditional residual block. The residual block enhances the model's ability to\ncapture the noise patterns in the FID. The classifier is a simple MLP model\nthat predicts the FID. Experimental results on a realistic RF FID dataset\nshow that the proposed model outperforms existing state-of-the-art methods in\nterms of both",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1592920353982301,
          "p": 0.2571428571428571,
          "f": 0.19672130675147073
        },
        "rouge-2": {
          "r": 0.04081632653061224,
          "p": 0.06,
          "f": 0.04858299113245635
        },
        "rouge-l": {
          "r": 0.1592920353982301,
          "p": 0.2571428571428571,
          "f": 0.19672130675147073
        }
      }
    },
    {
      "paper_id": "math.GN.math/GN/2502.20164v1",
      "true_abstract": "This paper concerns various models of ``at-most-$n$-valued maps''. That is,\nmultivalued maps $f:X\\multimap Y$ for which $f(x)$ has cardinality at most $n$\nfor each $x$. We consider 4 classes of such maps which have appeared in the\nliterature: $\\mathcal U$, the set of exactly $n$-valued maps, or unions of\nsuch; $\\mathcal F$, the set of $n$-fold maps defined by Crabb; $\\mathcal S$,\nthe set of symmetric product maps; and $\\mathcal W$, the set of weighted maps\nwith weights in $\\mathbb N$. Our main result is roughly that these classes\nsatisfy the following containments: \\[ \\mathcal U \\subsetneq \\mathcal F\n\\subsetneq \\mathcal S = \\mathcal W \\]\n  Furthermore we define the general class $\\mathcal C$ of all\nat-most-$n$-valued maps, and show that there are maps in $\\mathcal C$ which are\noutside of any of the other classes above. We also describe a\nconfiguration-space point of view for the class $\\mathcal C$, defining a\nconfiguration space $C_n(Y)$ such that any at-most-$n$-valued map $f:X\\multimap\nY$ corresponds naturally to a single-valued map $f:X\\to C_n(Y)$. We give a full\ncalculation of the fundamental group and homology groups of $C_n(S^1)$.",
      "generated_abstract": "r introduces a new notion of n-valued maps, which is a generalization\nof the classical notion of valued maps. A n-valued map from a set to a set is a\nfunction that maps each element of the domain to an element in the codomain, and\nfor each pair of elements, the value of the map is non-negative and non-increasing\nin the first argument. We also introduce a notion of n-valued functions, which\nis a generalization of the classical notion of valued functions. A n-valued\nfunction from a set to a set is a function that maps each element of the domain\nto an element in the codomain, and for each pair of elements, the value of the\nfunction is non-negative and non-increasing in the first argument. In this paper,\nwe consider the existence of a n-valued map between two n-valued",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15966386554621848,
          "p": 0.38,
          "f": 0.22485206683939643
        },
        "rouge-2": {
          "r": 0.029069767441860465,
          "p": 0.06097560975609756,
          "f": 0.03937007436790922
        },
        "rouge-l": {
          "r": 0.15126050420168066,
          "p": 0.36,
          "f": 0.21301774731276918
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/RO/2503.10341v1",
      "true_abstract": "The field of high-speed autonomous racing has seen significant advances in\nrecent years, with the rise of competitions such as RoboRace and the Indy\nAutonomous Challenge providing a platform for researchers to develop software\nstacks for autonomous race vehicles capable of reaching speeds in excess of 170\nmph. Ensuring the safety of these vehicles requires the software to\ncontinuously monitor for different faults and erroneous operating conditions\nduring high-speed operation, with the goal of mitigating any unreasonable risks\nposed by malfunctions in sub-systems and components. This paper presents a\ncomprehensive overview of the HALO safety architecture, which has been\nimplemented on a full-scale autonomous racing vehicle as part of the Indy\nAutonomous Challenge. The paper begins with a failure mode and criticality\nanalysis of the perception, planning, control, and communication modules of the\nsoftware stack. Specifically, we examine three different types of faults - node\nhealth, data health, and behavioral-safety faults. To mitigate these faults,\nthe paper then outlines HALO safety archetypes and runtime monitoring methods.\nFinally, the paper demonstrates the effectiveness of the HALO safety\narchitecture for each of the faults, through real-world data gathered from\nautonomous racing vehicle trials during multi-agent scenarios.",
      "generated_abstract": "s racing is a promising approach for high-speed, high-precision\nautonomous driving. However, racing in such an environment requires careful\ndesign, with the safety architecture playing a critical role in ensuring\nreliable systems. This paper introduces HALO, a safety architecture for high-speed\nautonomous racing that addresses key challenges including high-speed changes in\nstate, dynamic obstacles, and multi-vehicle interactions. HALO leverages the\nFault-Tolerant Constraint Satisfaction Problem (FT-CSP) framework to\nautomatically synthesize robust safety specifications for autonomous vehicles,\nenabling them to safely navigate complex environments. By leveraging FT-CSP,\nHALO reduces the complexity of safety verification and enables more efficient\ndevelopment of autonomous racing systems. The HALO architecture is evaluated\nthrough two case studies: a racing",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.24096385542168675,
          "f": 0.1970443301414741
        },
        "rouge-2": {
          "r": 0.045454545454545456,
          "p": 0.07339449541284404,
          "f": 0.05614034615352456
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.24096385542168675,
          "f": 0.1970443301414741
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2501.11189v1",
      "true_abstract": "This paper is a sequel of the 2019 paper [5]. It demonstrates the following:\na) the Poisson multi-Bernoulli mixture (PMBM) approach to detected vs.\nundetected (U/D) targets cannot be rigorously formulated using either the\ntwo-step or single-step multitarget recursive Bayes filter (MRBF); b) it can,\nhowever, be partially salvaged using a novel single-step MRBF; c) probability\nhypothesis density (PHD) filters can be derived for both the original \"S-U/D\"\napproach in [5] and the novel \"D-U/D\" approach; d) important U/D formulas in\n[5] can be verified using purely algebraic methods rather than the intricate\nstatistical analysis employed in that paper; and e) the claim, that PMBM\nfilters can propagate detected and undetected targets separately in parallel,\nis doubtful.",
      "generated_abstract": "er the case of target detection in a finite population where the\ntargets are detected by a continuous random variable $T$ and undetected by $U$.\nWe also assume that $T$ is stochastic and follows a continuous time Markov\nprocess with state space $\\mathbb{T}$ and transition kernel $P$. We study the\ntime evolution of the detection rate of the targets under the Markov process\n$T$ and under the alternative hypothesis $U$. We derive explicit expressions\nfor the detection rate in both cases. We also study the dynamics of the\ndetection rate when $T$ is no longer stochastic. Our results show that the\ndetection rate exhibits a rich dynamical behavior, depending on the parameter\nvalues. In particular, we show that the detection rate can exhibit\nasymptotic behavior depending on the parameter values. We also consider the\ndetection rate under the alternative hypothesis $U$ and show",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16470588235294117,
          "p": 0.1891891891891892,
          "f": 0.17610062395474876
        },
        "rouge-2": {
          "r": 0.017699115044247787,
          "p": 0.01818181818181818,
          "f": 0.017937214731848002
        },
        "rouge-l": {
          "r": 0.12941176470588237,
          "p": 0.14864864864864866,
          "f": 0.13836477489814505
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.07088v1",
      "true_abstract": "We construct a family of estimators for a regression function based on a\nsample following a qdistribution. Our approach is nonparametric, using kernel\nmethods built from operations that leverage the properties of q-calculus.\nFurthermore, under appropriate assumptions, we establish the weak convergence\nand strong consistency of this family of estimators.",
      "generated_abstract": "This paper investigates the asymptotic normality and strong consistency of\nkernel regression estimation in q-calculus. We establish the asymptotic normality\nof the estimator and the consistency of the estimator under a sufficient\nassumption. Moreover, we establish the asymptotic normality of the estimator in\nthe case of q-calculus under the condition that the number of observations is\nlarge enough. This condition is similar to the condition that the number of\nobservations in the classical regression estimation is large enough.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.36585365853658536,
          "p": 0.42857142857142855,
          "f": 0.39473683713642665
        },
        "rouge-2": {
          "r": 0.1276595744680851,
          "p": 0.10714285714285714,
          "f": 0.11650484940710738
        },
        "rouge-l": {
          "r": 0.3170731707317073,
          "p": 0.37142857142857144,
          "f": 0.34210525818905824
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.10101v1",
      "true_abstract": "Financial bubbles and crashes have repeatedly caused economic turmoil notably\nbut not only during the 2008 financial crisis. However, both in the popular\npress as well as scientific publications, the meaning of bubble is sometimes\nunspecified. Due to the multitude of bubble definitions, we conduct a\nsystematic review with the following questions: What definitions of asset price\nbubbles exist in the literature? Which definitions are used in which\ndisciplines and how frequently? We develop a system of definition categories\nand categorize a total of 122 papers from eleven research areas. Our results\nshow that although one definition is indeed prevalent in the literature, the\noverall definition landscape is not uniform. Next to the mostly used definition\nas deviation from a present value of expected future cash flows, we identify\nseveral other definitions, which rely on price properties or other\nspecifications of a fundamental value. This research contributes by shedding\nlight on the possible variations in which bubbles are defined and\noperationalized.",
      "generated_abstract": "r reviews existing definitions of financial bubbles, focusing on\nhow these definitions are used to characterize asset price bubbles. The\nrelevant literature on definitions of financial bubbles is reviewed. The\nrelevant literature on asset price bubbles is reviewed. The review focuses on\nthree main definitions: (1) asset price bubbles are defined as price\nincreases that are larger than the average of past increases, (2) asset price\nbubbles are defined as price increases that are larger than the average of\npast increases plus a threshold, and (3) asset price bubbles are defined as\nprice increases that are larger than the average of past increases plus a\nrandom variable. The review concludes that the most commonly used definition\nof asset price bubbles is the second definition. The review also concludes that\nthe three definitions of financial bubbles are mutually exclusive and that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.36363636363636365,
          "f": 0.24242423797979806
        },
        "rouge-2": {
          "r": 0.05806451612903226,
          "p": 0.10843373493975904,
          "f": 0.07563024755843542
        },
        "rouge-l": {
          "r": 0.16363636363636364,
          "p": 0.32727272727272727,
          "f": 0.21818181373737383
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.08001v1",
      "true_abstract": "Mobile edge computing (MEC) enables the provision of high-reliability and\nlow-latency applications by offering computation and storage resources in close\nproximity to end-users. Different from traditional computation task offloading\nin MEC systems, the large data volume and complex task computation of\nartificial intelligence involved intelligent computation task offloading have\nincreased greatly. To address this challenge, we propose a MEC system for\nmultiple base stations and multiple terminals, which exploits semantic\ntransmission and early exit of inference. Based on this, we investigate a joint\nsemantic transmission and resource allocation problem for maximizing system\nreward combined with analysis of semantic transmission and intelligent\ncomputation process. To solve the formulated problem, we decompose it into\ncommunication resource allocation subproblem, semantic transmission subproblem,\nand computation capacity allocation subproblem. Then, we use 3D matching and\nconvex optimization method to solve subproblems based on the block coordinate\ndescent (BCD) framework. The optimized feasible solutions are derived from an\nefficient BCD based joint semantic transmission and resource allocation\nalgorithm in MEC systems. Our simulation demonstrates that: 1) The proposed\nalgorithm significantly improves the delay performance for MEC systems compared\nwith benchmarks; 2) The design of transmission mode and early exit of inference\ngreatly increases system reward during offloading; and 3) Our proposed system\nachieves efficient utilization of resources from the perspective of system\nreward in the intelligent scenario.",
      "generated_abstract": "aper, we propose a joint semantic transmission and resource\nallocation (JSTRA) framework for intelligent computation task offloading\n(ICTO) in multi-edge computing (MEC) systems. To address the challenges of\nlow-rate transmissions and interference among ICTO tasks, the JSTRA framework\nintroduces a semantic coding mechanism to improve the transmission efficiency\nof ICTO tasks. Moreover, the JSTRA framework employs a resource allocation\nmechanism to allocate ICTO tasks to edge servers in a cooperative manner,\noptimizing the ICTO service quality. The proposed JSTRA framework is\ncomputationally efficient and can effectively mitigate the impacts of ICTO\ntasks on the system resources. Simulation results demonstrate that the\nproposed JSTRA framework can significantly enhance the ICTO service quality and\nreduce the ICTO task transmission errors.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24242424242424243,
          "p": 0.4444444444444444,
          "f": 0.3137254856286044
        },
        "rouge-2": {
          "r": 0.07575757575757576,
          "p": 0.14563106796116504,
          "f": 0.09966776958444187
        },
        "rouge-l": {
          "r": 0.21212121212121213,
          "p": 0.3888888888888889,
          "f": 0.27450979935409464
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.cs/CE/2503.07684v1",
      "true_abstract": "Lithium-sulfur (Li-S) batteries offer a promising alternative to current\nlithium-ion (Li-ion) batteries, with a high theoretical energy density,\nimproved safety and high abundance, low cost of materials. For Li-S to reach\ncommercial application, it is essential to understand how the behaviour scales\nbetween cell formats; new material development is predominately completed at\ncoin-cell level, whilst pouch-cells will be used for commercial applications.\nDifferences such as reduced electrolyte-to-sulfur (E/S) ratios and increased\ngeometric size at larger cell formats contribute to the behavioural\ndifferences, in terms of achievable capacity, cyclability and potential\ndegradation mechanisms.\n  This work focuses on the steps required to capture and test coin-cell\nbehaviour, building upon the existing models within the literature, which\npredominately focus on pouch-cells. The areas investigated throughout this\nstudy, to improve the capability of the model in terms of scaling ability and\ncausality of predictions, include the cathode surface area, precipitation\ndynamics and C-rate dependence.",
      "generated_abstract": "ulfur batteries are emerging as a promising next-generation\ntechnology for energy storage, offering high energy densities and long cycle\nlife. However, their performance is constrained by low cell voltage and\nthermal runaway. A crucial challenge is to understand the origin of these\nlimitations and to devise practical design rules to improve the cell performance.\n  Here, we present a systematic investigation of the voltage-sulfur-oxide\nrelationship and discuss how it can be leveraged to design a practical Li-S\ncell. We start by reviewing the theoretical models that have been used to\ndescribe the voltage-sulfur-oxide relationship, and we discuss how these models\ncan be misleading when applied to Li-S cells. Based on these insights, we\npropose a new model that accounts for the sulfur-oxide effect in a more\nrealistic way",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23076923076923078,
          "p": 0.3103448275862069,
          "f": 0.2647058774610727
        },
        "rouge-2": {
          "r": 0.034013605442176874,
          "p": 0.04065040650406504,
          "f": 0.03703703207654388
        },
        "rouge-l": {
          "r": 0.19658119658119658,
          "p": 0.26436781609195403,
          "f": 0.22549019118656297
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2410.22030v1",
      "true_abstract": "Metabolite biosynthesis is regulated via metabolic pathways, which can be\nactivated and deactivated within organisms. Understanding and identifying an\norganism's metabolic pathway network is a crucial aspect for various research\nfields, including crop and life stock breeding, pharmacology, and medicine. The\nproblem of identifying whether a pathway is part of a studied metabolic system\nis commonly framed as a hyperlink prediction problem. The most important\nchallenge in prediction of metabolic pathways is the sparsity of the labeled\ndata. This challenge can partially be mitigated using metabolite correlation\nnetworks which are affected by all active pathways including those that were\nnot confirmed yet in laboratory experiments. Unfortunately, extracting\nproperties that can confirm or refute existence of a metabolic pathway in a\nparticular organism is not a trivial task. In this research, we introduce the\nNetwork Auralization Hyperlink Prediction (NetAurHPD) which is a framework that\nrelies on (1) graph auralization to extract and aggregate representations of\nnodes in metabolite correlation networks and (2) data augmentation method that\ngenerates metabolite correlation networks given a subset of chemical reactions\ndefined as hyperlinks. Experiments with metabolites correlation-based networks\nof tomato pericarp demonstrate promising results for NetAurHPD, compared to\nalternative methods. Furthermore, the application of data augmentation improved\nNetAurHPD's learning capabilities and overall performance. Additionally,\nNetAurHPD outperformed state-of-the-art method in experiments under challenging\nconditions, and has the potential to be a valuable tool for exploring organisms\nwith limited existing knowledge.",
      "generated_abstract": "profiling is essential for the development of personalized medicine\nand precision diagnostics, yet it poses a significant challenge due to the\nhigh dimensionality of metabolic data. Traditional methods, such as\nmetabolite-based feature selection and feature engineering, are time-consuming\nand often inefficient, limiting their applicability in real-world scenarios. In\nthis paper, we propose a novel framework, NetAurHPD, that integrates\nnetwork-auralization hyperlink prediction (NAHP) and the hierarchical\nprobabilistic directed acyclic graph (HPDAG) model to identify metabolic pathways\nfrom metabolomics data. NAHP is a state-of-the-art method for predicting\nnetwork-level linkages between metabolites, enabling the identification of\nmetabolic pathways. In our model, the network-level linkages are predicted via",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17532467532467533,
          "p": 0.3333333333333333,
          "f": 0.22978722952503405
        },
        "rouge-2": {
          "r": 0.02666666666666667,
          "p": 0.06,
          "f": 0.036923072662722385
        },
        "rouge-l": {
          "r": 0.16883116883116883,
          "p": 0.32098765432098764,
          "f": 0.2212765912271617
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/MF/2502.00740v1",
      "true_abstract": "This paper examines a semi-analytical approach for pricing American options\nin time-inhomogeneous models characterized by negative interest rates (for\nequity, FX) or negative convenience yields (for commodities, cryptocurrencies).\nUnder such conditions, exercise boundaries may exhibit a \"floating\" structure -\ndynamically appearing and disappearing. For example, a second exercise boundary\ncould emerge within the computational domain and subsequently both could\ncollapse, demanding specialized pricing methodologies.",
      "generated_abstract": "er a stochastic volatility model with time-inhomogeneous volatility\nand with a time-dependent market impact. The model is specified by an\narbitrary time-dependent function $V$ on the price process of a European option\nwith time-inhomogeneous volatility, $S(t,s) = V(t,s)$, and the associated\nstochastic volatility, $\\sigma(t,s) = \\frac{1}{2} V(t,s) $. We consider the\ncase where the underlying price process is the solution of a stochastic\ndifferential equation with time-dependent volatility and market impact. We\nanalyze the impact of a floating exercise boundary on the value of the American\noption. We show that the value of the option can be made arbitrarily close to\ntheir exercise value, even if the underlying price is not. We show that the\noption's",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.12698412698412698,
          "f": 0.13445377652990625
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.010101010101010102,
          "f": 0.012345674259261087
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.1111111111111111,
          "f": 0.11764705384083066
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2501.05232v1",
      "true_abstract": "Tether Limited has the sole authority to create (mint) and destroy (burn)\nTether stablecoins (USDT). This paper investigates Bitcoin's response to USDT\nsupply change events between 2014 and 2021 and identifies an interesting\nasymmetry between Bitcoin's responses to USDT minting and burning events.\nBitcoin responds positively to USDT minting events over 5- to 30-minute event\nwindows, but this response begins declining after 60 minutes. State-dependence\nis also demonstrated, with Bitcoin prices exhibiting a greater increase when\nthe corresponding USDT minting event coincides with positive investor sentiment\nand is announced to the public by data service provider, Whale Alert, on\nTwitter.",
      "generated_abstract": "This study investigates the Bitcoin (BTC) price response to Tether (TTD)\nminting and burning events, focusing on the intraday period. Using Twitter\nsentiment analysis, we analyze the reaction of BTC investors, analyzing\nwhether Tether's market cap impact is positive or negative for BTC prices. We\nalso investigate the sentiment of whales, who have a disproportionate influence\non BTC markets, as they have the most influence on Tether's market cap,\ninfluencing BTC prices. Our results show that investors are more optimistic\nabout Tether's impact on BTC prices after Tether's minting and burning events\nthan after the withdrawal of Tether's reserves.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2702702702702703,
          "p": 0.2857142857142857,
          "f": 0.27777777278163585
        },
        "rouge-2": {
          "r": 0.042105263157894736,
          "p": 0.04395604395604396,
          "f": 0.04301074769048503
        },
        "rouge-l": {
          "r": 0.21621621621621623,
          "p": 0.22857142857142856,
          "f": 0.22222221722608038
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.00648v1",
      "true_abstract": "T-cells play a key role in adaptive immunity by mounting specific responses\nagainst diverse pathogens. An effective binding between T-cell receptors (TCRs)\nand pathogen-derived peptides presented on Major Histocompatibility Complexes\n(MHCs) mediate an immune response. However, predicting these interactions\nremains challenging due to limited functional data on T-cell reactivities.\nHere, we introduce a computational approach to predict TCR interactions with\npeptides presented on MHC class I alleles, and to design novel immunogenic\npeptides for specified TCR-MHC complexes. Our method leverages HERMES, a\nstructure-based, physics-guided machine learning model trained on the protein\nuniverse to predict amino acid preferences based on local structural\nenvironments. Despite no direct training on TCR-pMHC data, the implicit\nphysical reasoning in HERMES enables us to make accurate predictions of both\nTCR-pMHC binding affinities and T-cell activities across diverse viral epitopes\nand cancer neoantigens, achieving up to 72% correlation with experimental data.\nLeveraging our TCR recognition model, we develop a computational protocol for\nde novo design of immunogenic peptides. Through experimental validation in\nthree TCR-MHC systems targeting viral and cancer peptides, we demonstrate that\nour designs--with up to five substitutions from the native sequence--activate\nT-cells at success rates of up to 50%. Lastly, we use our generative framework\nto quantify the diversity of the peptide recognition landscape for various\nTCR-MHC complexes, offering key insights into T-cell specificity in both humans\nand mice. Our approach provides a platform for immunogenic peptide and\nneoantigen design, opening new computational paths for T-cell vaccine\ndevelopment against viruses and cancer.",
      "generated_abstract": "l receptor (TCR) is the cell surface receptor that recognizes\nand binds to foreign antigen and initiates the activation of the T-cell. The\nTCR consists of two heavy chains and two light chains, each of which binds to\na different antigen. The heavy chains are encoded by the variable region (V)\nloci, and the light chains are encoded by the constant region (C) loci. The\nvariable region (V) loci are highly conserved in all TCRs, while the constant\nregion (C) loci are significantly more variable. A TCR can bind to a\nspecific antigen in a highly specific manner, and this specificity is encoded\nby the V-C landscape, which represents the sequence and spatial arrangement of\nthe V-C loci in the TCR. The V-C landscape can be used to predict the\nspecificity of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08383233532934131,
          "p": 0.2153846153846154,
          "f": 0.12068965113889728
        },
        "rouge-2": {
          "r": 0.008438818565400843,
          "p": 0.017699115044247787,
          "f": 0.011428567056164936
        },
        "rouge-l": {
          "r": 0.07784431137724551,
          "p": 0.2,
          "f": 0.11206896148372489
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.09411v1",
      "true_abstract": "The learning rate in stochastic gradient methods is a critical hyperparameter\nthat is notoriously costly to tune via standard grid search, especially for\ntraining modern large-scale models with billions of parameters. We identify a\ntheoretical advantage of learning rate annealing schemes that decay the\nlearning rate to zero at a polynomial rate, such as the widely-used cosine\nschedule, by demonstrating their increased robustness to initial parameter\nmisspecification due to a coarse grid search. We present an analysis in a\nstochastic convex optimization setup demonstrating that the convergence rate of\nstochastic gradient descent with annealed schedules depends sublinearly on the\nmultiplicative misspecification factor $\\rho$ (i.e., the grid resolution),\nachieving a rate of $O(\\rho^{1/(2p+1)}/\\sqrt{T})$ where $p$ is the degree of\npolynomial decay and $T$ is the number of steps, in contrast to the\n$O(\\rho/\\sqrt{T})$ rate that arises with fixed stepsizes and exhibits a linear\ndependence on $\\rho$. Experiments confirm the increased robustness compared to\ntuning with a fixed stepsize, that has significant implications for the\ncomputational overhead of hyperparameter search in practical training\nscenarios.",
      "generated_abstract": "igate the effect of annealing learning rates in stochastic optimization\n(gradient descent) for tuning-robustness, where the optimization algorithm is\ntrained on a set of data points (the tuning points) to find a global minimum of\na function. We consider two scenarios: (i) the tuning points are randomly\nselected, (ii) the tuning points are chosen using a randomized gradient. We\nevaluate the effects of annealing on the convergence rate of the optimization\nalgorithm and the tuning-robustness of the obtained local minima. We show that\nthe annealing is beneficial in both scenarios, and the annealing rate can be\noptimized to achieve the best performance. Furthermore, we study the effects of\nannealing on the convergence rate and the tuning-robustness of the obtained\nlocal minima, and show that the annealing rate can be optimized to achieve the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18691588785046728,
          "p": 0.29411764705882354,
          "f": 0.2285714238197552
        },
        "rouge-2": {
          "r": 0.03592814371257485,
          "p": 0.06,
          "f": 0.0449438155395648
        },
        "rouge-l": {
          "r": 0.17757009345794392,
          "p": 0.27941176470588236,
          "f": 0.21714285239118375
        }
      }
    },
    {
      "paper_id": "math.OC.econ/GN/2412.05234v1",
      "true_abstract": "Risk measures, which typically evaluate the impact of extreme losses, are\nhighly sensitive to misspecification in the tails. This paper studies a robust\noptimization approach to combat tail uncertainty by proposing a unifying\nframework to construct uncertainty sets for a broad class of risk measures,\ngiven a specified nominal model. Our framework is based on a parametrization of\nrobust risk measures using two (or multiple) $\\phi$-divergence functions, which\nenables us to provide uncertainty sets that are tailored to both the\nsensitivity of each risk measure to tail losses and the tail behavior of the\nnominal distribution. In addition, our formulation allows for a tractable\ncomputation of robust risk measures, and elicitation of $\\phi$-divergences that\ndescribe a decision maker's risk and ambiguity preferences.",
      "generated_abstract": "r introduces a novel approach to constructing uncertainty sets for\nrobust risk measures. It combats tail uncertainty by employing composition of\n$\u03c6$-divergences, which has been shown to be a suitable measure for risk\nmeasures in the context of the uniform distribution. The methodology is\napplied to the $L_1$ and $L_2$ risk measures, and it is shown that the\ncomposition of $\u03c6$-divergences provides a reliable method for constructing\nuncertainty sets in these cases. Additionally, it is demonstrated that the\napproach is applicable to other risk measures that follow a similar\nstructure. The proposed methodology is tested through examples, including\nrisk measures based on the Sharpe ratio, the conditional variance, and the\nlogarithmic risk. It is shown that the proposed approach produces robust\nuncertainty sets that are tailored to the specific risk measure being\nconsider",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3125,
          "p": 0.33783783783783783,
          "f": 0.3246753196829145
        },
        "rouge-2": {
          "r": 0.15517241379310345,
          "p": 0.15254237288135594,
          "f": 0.1538461488465193
        },
        "rouge-l": {
          "r": 0.2875,
          "p": 0.3108108108108108,
          "f": 0.29870129370888854
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.08902v1",
      "true_abstract": "Mutual Information (MI) is a crucial measure for capturing dependencies\nbetween variables, but exact computation is challenging in high dimensions with\nintractable likelihoods, impacting accuracy and robustness. One idea is to use\nan auxiliary neural network to train an MI estimator; however, methods based on\nthe empirical distribution function (EDF) can introduce sharp fluctuations in\nthe MI loss due to poor out-of-sample performance, destabilizing convergence.\nWe present a Bayesian nonparametric (BNP) solution for training an MI estimator\nby constructing the MI loss with a finite representation of the Dirichlet\nprocess posterior to incorporate regularization in the training process. With\nthis regularization, the MI loss integrates both prior knowledge and empirical\ndata to reduce the loss sensitivity to fluctuations and outliers in the sample\ndata, especially in small sample settings like mini-batches. This approach\naddresses the challenge of balancing accuracy and low variance by effectively\nreducing variance, leading to stabilized and robust MI loss gradients during\ntraining and enhancing the convergence of the MI approximation while offering\nstronger theoretical guarantees for convergence. We explore the application of\nour estimator in maximizing MI between the data space and the latent space of a\nvariational autoencoder. Experimental results demonstrate significant\nimprovements in convergence over EDF-based methods, with applications across\nsynthetic and real datasets, notably in 3D CT image generation, yielding\nenhanced structure discovery and reduced overfitting in data synthesis. While\nthis paper focuses on generative models in application, the proposed estimator\nis not restricted to this setting and can be applied more broadly in various\nBNP learning procedures.",
      "generated_abstract": "This paper proposes a novel deep Bayesian nonparametric (DBNP) framework\nfor robust mutual information estimation. The proposed method is based on\ngenerative models that explicitly model the distribution of data while\ncapturing the dependencies between variables. The model is trained using\nvariational inference, which enables the estimation of mutual information under\nunknown distributions. The proposed method is shown to outperform\nnon-parametric estimation methods, including the kernel-based method and the\nvariational Bayesian method, on synthetic and real-world data. The proposed\nmethod is also shown to be robust to outliers, with the robustness measure\nproviding a measure of the degree of outlier robustness. The robustness of the\nproposed method is further examined by applying it to two different data\nsources, including a gene expression dataset and a textual dataset.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18674698795180722,
          "p": 0.40789473684210525,
          "f": 0.2561983427989892
        },
        "rouge-2": {
          "r": 0.0371900826446281,
          "p": 0.0782608695652174,
          "f": 0.05042016369999019
        },
        "rouge-l": {
          "r": 0.18072289156626506,
          "p": 0.39473684210526316,
          "f": 0.24793387998907185
        }
      }
    },
    {
      "paper_id": "cs.DC.cs/DC/2503.09318v1",
      "true_abstract": "Modern data analytics requires a huge amount of computing power and processes\na massive amount of data. At the same time, the underlying computing platform\nis becoming much more heterogeneous on both hardware and software. Even though\nspecialized hardware, e.g., FPGA- or GPU- or TPU-based systems, often achieves\nbetter performance than a CPU-only system due to the slowing of Moore's law,\nsuch systems are limited in what they can do. For example, GPU-only approaches\nsuffer from severe IO limitations. To truly exploit the potential of hardware\nheterogeneity, we present FpgaHub, an FPGA-centric hyper-heterogeneous\ncomputing platform for big data analytics. The key idea of FpgaHub is to use\nreconfigurable computing to implement a versatile hub complementing other\nprocessors (CPUs, GPUs, DPUs, programmable switches, computational storage,\netc.). Using an FPGA as the basis, we can take advantage of its highly\nreconfigurable nature and rich IO interfaces such as PCIe, networking, and\non-board memory, to place it at the center of the architecture and use it as a\ndata and control plane for data movement, scheduling, pre-processing, etc.\nFpgaHub enables architectural flexibility to allow exploring the rich design\nspace of heterogeneous computing platforms.",
      "generated_abstract": "r presents FpgaHub, a Fpga-centric Hyper-heterogeneous Computing\nplatform (HCP) for Big Data Analytics. FpgaHub is built on a modular\nFpga-based HCP platform designed to enable data scientists and developers to\nimplement customized Big Data Analytics applications. FpgaHub offers\nFpga-based accelerators such as Intel Stratix 10 GX and Xcelsius FPGAs,\nIntel Xeon Scalable Platform processors, and Intel Nervana Neural Networks\nfor data science. FpgaHub also offers Heterogeneous Computing Platforms such as\nCUDA-enabled NVIDIA GPUs, NVIDIA PTX Accelerators, and Intel Optane Memory\nfor high-performance computing (HPC) applications. FpgaHub is an open-source\nproject and is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11678832116788321,
          "p": 0.25,
          "f": 0.159203975759016
        },
        "rouge-2": {
          "r": 0.021164021164021163,
          "p": 0.047058823529411764,
          "f": 0.02919707601230816
        },
        "rouge-l": {
          "r": 0.11678832116788321,
          "p": 0.25,
          "f": 0.159203975759016
        }
      }
    },
    {
      "paper_id": "math.OC.math/OC/2503.10164v1",
      "true_abstract": "This paper addresses the safety challenges in impulsive systems, where abrupt\nstate jumps introduce significant complexities into system dynamics. A unified\nframework is proposed by integrating Quadratic Programming (QP), Control\nBarrier Functions (CBFs), and adaptive gain mechanisms to ensure system safety\nduring impulsive events. The CBFs are constructed to enforce safety constraints\nby capturing the system's continuous dynamics and the effects of impulsive\nstate transitions. An adaptive gain mechanism dynamically adjusts control\ninputs based on the magnitudes of the impulses and the system's proximity to\nsafety boundaries, maintaining safety during instantaneous state jumps. A\ntailored QP formulation incorporates CBFs constraints and adaptive gain\nadjustments, optimizing control inputs while ensuring compliance with\nsafety-critical requirements. Theoretical analysis establishes the boundedness,\ncontinuity, and feasibility of the adaptive gain and the overall framework. The\neffectiveness of the method is demonstrated through simulations on a robotic\nmanipulator, showcasing its practical applicability to impulsive systems with\nstate jumps.",
      "generated_abstract": "In this paper, we study the safety control of impulsive systems using control\nbarrier functions (CBFs) and adaptive gains. The safety control objective is to\nminimize the total energy consumption of the system while ensuring that the\ncontrolled state remains within a given safety region. The control gains are\nused to modify the system dynamics and control actions in order to achieve the\ndesired safety region. We show that a control barrier function with adaptive\ngains is a local minimizer of the cost functional, and that the resulting\ncontrol policy is optimal in the class of optimal control policies. We also\nprovide an analysis of the convergence of the solution to the optimal control\npolicy. Finally, we demonstrate the effectiveness of the proposed method by\nsimulation of a two-dof system subject to a safety constraint.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25252525252525254,
          "p": 0.3333333333333333,
          "f": 0.2873563169342054
        },
        "rouge-2": {
          "r": 0.06521739130434782,
          "p": 0.07377049180327869,
          "f": 0.06923076424970451
        },
        "rouge-l": {
          "r": 0.25252525252525254,
          "p": 0.3333333333333333,
          "f": 0.2873563169342054
        }
      }
    },
    {
      "paper_id": "math.AG.math/DG/2503.09195v1",
      "true_abstract": "Refined algebraic domains are regions in the plane surrounded by finitely\nmany non-singular real algebraic curves which may intersect with normal\ncrossing. We are interested in shapes of such regions with surrounding real\nalgebraic curves. Poincar'e-Reeb Graphs of them are graphs the regions\nnaturally collapse to respecting the projection to a straight line. Such graphs\nwere first formulated by Sorea, for example, around 2020, and regions\nsurrounded by mutually disjoint non-singular real algebraic curves were mainly\nconsidered. The author has generalized the studies to several general\nsituations.\n  We find classes of such objects defined inductively by adding curves. We\nrespect characteristic finite sets in the curves. We consider regions\nsurrounded by the curves and of a new type. We investigate geometric properties\nand combinatorial ones of them and discuss important examples. We also\npreviously studied explicit classes defined inductively in this way and review\nthem.",
      "generated_abstract": "e an algebraic domain with a finite set of closed points $P$. We\ndevelop a refined algebraic theory for algebraic domains with finite sets of\nclosed points, which allows to characterize the algebraic domains with finite\nsets of closed points $P$ which are refined algebraic domains with finite sets\nof closed points $P$. We also define a notion of refined algebraic domains with\nfinite sets of closed points $P$ and refined algebraic domains with finite\nsets of closed points $P$ which are refined algebraic domains with finite sets\nof closed points $P$. We show that refined algebraic domains with finite sets\nof closed points $P$ are refined algebraic domains with finite sets of closed\npoints $P$. We also show that refined algebraic domains with finite sets of\nclosed points $P$ are refined algebraic domains with finite sets of closed\npoints $P$ which are refined algebraic domains with finite sets of closed points",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16853932584269662,
          "p": 0.46875,
          "f": 0.24793388040707603
        },
        "rouge-2": {
          "r": 0.023076923076923078,
          "p": 0.06382978723404255,
          "f": 0.033898301184206774
        },
        "rouge-l": {
          "r": 0.16853932584269662,
          "p": 0.46875,
          "f": 0.24793388040707603
        }
      }
    },
    {
      "paper_id": "math.NA.cs/NA/2503.10402v1",
      "true_abstract": "We explore a family of numerical methods, based on the Steffensen divided\ndifference iterative algorithm, that do not evaluate the derivative of the\nobjective functions. The family of methods achieves second-order convergence\nwith two function evaluations per iteration with marginal additional\ncomputational cost. An important side benefit of the method is the improvement\nin stability for different initial conditions compared to the vanilla\nSteffensen method. We present numerical results for scalar functions, fields,\nand scalar fields. This family of methods outperforms the Steffensen method\nwith respect to standard quantitative metrics in most cases.",
      "generated_abstract": "The Steffensen algorithm has been widely used in the field of root\nfinding for its ability to find both local and global extrema of a function.\nHowever, as the number of search points increases, the algorithm becomes\ncomputationally expensive and prone to numerical errors. In this paper, we\npropose a new derivative-free algorithm that significantly reduces the number\nof search points needed to reach the root, and demonstrates the robustness of\nthe algorithm by comparing it with the Steffensen algorithm and the\nFletcher-Reeves algorithm. Additionally, we propose an algorithm to determine\nthe Steffensen constant, which is an essential parameter in the Steffensen\nalgorithm, and show that it is an essential parameter in the new algorithm.\nFinally, we apply the proposed algorithms to the root finding problem of the\nsecond-order differential equation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23529411764705882,
          "p": 0.21333333333333335,
          "f": 0.22377621878820494
        },
        "rouge-2": {
          "r": 0.03488372093023256,
          "p": 0.02654867256637168,
          "f": 0.03015074886088815
        },
        "rouge-l": {
          "r": 0.22058823529411764,
          "p": 0.2,
          "f": 0.20979020480219096
        }
      }
    },
    {
      "paper_id": "math.OA.math/FA/2503.09548v1",
      "true_abstract": "Let $\\Gamma$ be a countable discrete group. We say that $\\Gamma$ has\n$C^*$-invariant subalgebra rigidity (ISR) property if every $\\Gamma$-invariant\n$C^*$-subalgebra $\\mathcal{A}\\le C_r^*(\\Gamma)$ is of the form $C_r^*(N)$ for\nsome normal subgroup $N\\triangleleft\\Gamma$. We show that all torsion-free,\nnon-amenable (cylindrically) hyperbolic groups with property-AP and a finite\ndirect product of such groups have this property. We also prove that an\ninfinite group $\\Gamma$ has the C$^*$-ISR property only if $\\Gamma$ is simple\namenable or $C^*$-simple.",
      "generated_abstract": "Let $G$ be a finite group and let $A$ be a unital $C^*$-algebra. We\nconsider the following question: given a subgroup $H\\subset G$ and a left\n$A$-module $M$, is there a $H$-invariant subalgebra $M_H$ of $A$ such that\n$M_H\\otimes_A M\\cong M_H\\otimes_A M_H$ as left $A_H$-modules? In this paper,\nwe answer this question in the negative. In particular, we show that the\nfollowing three cases are excluded: $G=D_n$ is the dihedral group of order $n$,\n$G$ is the symmetric group $S_n$, or $G$ is the alternating group $A_n$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.29310344827586204,
          "p": 0.2833333333333333,
          "f": 0.28813558822177543
        },
        "rouge-2": {
          "r": 0.0547945205479452,
          "p": 0.04938271604938271,
          "f": 0.05194804696154543
        },
        "rouge-l": {
          "r": 0.25862068965517243,
          "p": 0.25,
          "f": 0.2542372831370297
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2501.15828v4",
      "true_abstract": "Recovery rate prediction plays a pivotal role in bond investment strategies,\nenhancing risk assessment, optimizing portfolio allocation, improving pricing\naccuracy, and supporting effective credit risk management. However, forecasting\nfaces challenges like high-dimensional features, small sample sizes, and\noverfitting. We propose a hybrid Quantum Machine Learning model incorporating\nParameterized Quantum Circuits (PQC) within a neural network framework. PQCs\ninherently preserve unitarity, avoiding computationally costly orthogonality\nconstraints, while amplitude encoding enables exponential data compression,\nreducing qubit requirements logarithmically. Applied to a global dataset of\n1,725 observations (1996-2023), our method achieved superior accuracy (RMSE\n0.228) compared to classical neural networks (0.246) and quantum models with\nangle encoding (0.242), with efficient computation times. This work highlights\nthe potential of hybrid quantum-classical architectures in advancing recovery\nrate forecasting.",
      "generated_abstract": "ery rate is the key metric used to assess the quality of a quantum\nquantum neural network (QNN) and is essential for its usage in quantum finance.\nTheoretically, the recovery rate is defined as the ratio of the amount of data\nthat is recovered from a quantum state to the data that is necessary to recreate\nthe state. However, in practice, the recovery rate is often measured in terms of\nthe number of measurements. This metric is more widely used due to its\neasiness of implementation, but it does not account for the error in\nquantum-measuring instruments. In this paper, we present a novel approach to\nmeasure the recovery rate by combining amplitude encoding with a\nquantum-measuring instrument. We show that the recovery rate is a function of\nthe number of quantum-measuring measurements and the amplitude encoding. Our\napproach yields an accurate estimation of the recovery rate by combining\namplit",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1651376146788991,
          "p": 0.225,
          "f": 0.1904761855939085
        },
        "rouge-2": {
          "r": 0.024,
          "p": 0.023622047244094488,
          "f": 0.0238095188098398
        },
        "rouge-l": {
          "r": 0.1651376146788991,
          "p": 0.225,
          "f": 0.1904761855939085
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.00732v1",
      "true_abstract": "In conventional approaches for multiobject tracking (MOT), raw sensor data\nundergoes several preprocessing stages to reduce data rate and computational\ncomplexity. This typically includes coherent processing that aims at maximizing\nthe signal-to-noise ratio (SNR), followed by a detector that extracts \"point\"\nmeasurements, e.g., the range and bearing of objects, which serve as inputs for\nsequential Bayesian MOT. While using point measurements significantly\nsimplifies the statistical model, the reduced data rate can lead to a loss of\ncritical, object-related information and, thus, potentially to reduced tracking\nperformance. In this paper, we propose a direct tracking approach that avoids a\ndetector and most preprocessing stages. For direct tracking, we introduce a\nmeasurement model for the data-generating process of the sensor data, along\nwith state-transition and birth models for the dynamics and the appearance and\ndisappearance of objects. Based on the new statistical model, we develop a\nfactor graph and particle-based belief propagation (BP) method for efficient\nsequential Bayesian estimation. Contrary to the track-before-detect (TBD)\nparadigm which also avoids a detector, direct tracking integrates coherent\nprocessing within the Bayesian MOT framework. Numerical experiments based on a\npassive acoustic dataset demonstrate that the proposed direct approach\noutperforms state-of-the-art conventional methods that rely on multiple\npreprocessing stages.",
      "generated_abstract": "of multiple objects in a dynamic environment is a critical task\nin many applications, such as surveillance, robot navigation, and autonomous\nvehicles. While tracking multiple objects can be achieved by tracking one object\nat a time, this approach has two major drawbacks: 1) it requires multiple\nsensors to track each object and 2) it only enables tracking of one object at a\ntime. In this paper, we propose a novel method to track multiple objects by\ntracking a reference object. The proposed method involves the tracking of a\nreference object, which is followed by tracking of multiple objects. The\nproposed method is applicable in both static and dynamic environments. To\naccommodate dynamic environments, the tracking of the reference object is\nperformed using a tracking algorithm that follows a reference object. The\ntracking of the reference object is performed using a tracking algorithm that\nfollows a reference object. The tracking of multiple",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18796992481203006,
          "p": 0.3424657534246575,
          "f": 0.24271844202610998
        },
        "rouge-2": {
          "r": 0.036458333333333336,
          "p": 0.06363636363636363,
          "f": 0.046357611262664376
        },
        "rouge-l": {
          "r": 0.17293233082706766,
          "p": 0.3150684931506849,
          "f": 0.22330096629795465
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.17600v1",
      "true_abstract": "Providing wellbeing for all while safeguarding planetary boundaries may\nrequire governments to pursue post-growth policies. Previous empirical studies\nof sustainable wellbeing initiatives investigating enablers of and barriers to\npost-growth policymaking are either based on a small number of empirical cases\nor lack an explicit analytical framework. To better understand how post-growth\npolicymaking could be fostered, we investigate 29 initiatives across governance\nscales in Europe, New Zealand, and Canada. We apply a framework that\ndistinguishes polity, politics, and policy to analyze the data. We find that\nthe main enablers and barriers relate to the economic growth paradigm, the\norganization of government, attitudes towards policymaking, political\nstrategies, and policy tools and outcomes. Engaging in positive framings of\npost-growth visions to change narratives and building broad-based alliances\ncould act as drivers. However, initiatives face a tension between the need to\nconnect to broad audiences and a risk of co-optation by depolitization.",
      "generated_abstract": "This paper explores the role of barriers and enablers in sustainable wellbeing\ninitiatives, and offers insights for policymakers to design and implement\nsuccessful policies for sustainable wellbeing. A set of barriers and enablers\nwas identified based on a literature review of the literature and policy\nexperiences from around the world. These include the influence of external\nfactors such as economic growth, the role of government in promoting wellbeing,\nand the role of technology and digitalization in supporting sustainable wellbeing\ninitiatives. The paper concludes by suggesting key considerations for policymakers\nin designing and implementing sustainable wellbeing policies.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17757009345794392,
          "p": 0.31666666666666665,
          "f": 0.22754490557567508
        },
        "rouge-2": {
          "r": 0.034722222222222224,
          "p": 0.05952380952380952,
          "f": 0.0438596444690679
        },
        "rouge-l": {
          "r": 0.1588785046728972,
          "p": 0.2833333333333333,
          "f": 0.20359280976729183
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.18714v1",
      "true_abstract": "The relationship of policy choice by majority voting and by maximization of\nutilitarian welfare has long been discussed. I consider choice between a status\nquo and a proposed policy when persons have interpersonally comparable cardinal\nutilities taking values in a bounded interval, voting is compulsory, and each\nperson votes for a policy that maximizes utility. I show that knowledge of the\nattained status quo welfare and the voting outcome yields an informative bound\non welfare with the proposed policy. The bound contains the value of status quo\nwelfare, so the better utilitarian policy is not known. The minimax-regret\ndecision and certain Bayes decisions choose the proposed policy if its vote\nshare exceeds the known value of status quo welfare. This procedure differs\nfrom majority rule, which chooses the proposed policy if its vote share exceeds\n1/2.",
      "generated_abstract": "We analyze the simple nonparametric estimation of the ordinal utility of\nthe status quo and a proposed policy in a two-alternative, two-outcome\nvoting-based setting. The proposed policy involves the adoption of a new\nenvironmental policy and is evaluated using a series of ordinal utility\nmeasures. We show that the ordinal utility of the status quo is equal to the\nexpected utility of the proposed policy. We derive the asymptotic distribution\nof the ordinal utility of the status quo and the proposed policy using a\nnonparametric approach, and we compare the ordinal utility of the status quo\nand the proposed policy using the empirical distribution of the ordinal utility\nmeasures. We find that the ordinal utility of the status quo is less than the\nproposed policy.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1728395061728395,
          "p": 0.3111111111111111,
          "f": 0.22222221763038558
        },
        "rouge-2": {
          "r": 0.08403361344537816,
          "p": 0.13513513513513514,
          "f": 0.10362693827700095
        },
        "rouge-l": {
          "r": 0.1728395061728395,
          "p": 0.3111111111111111,
          "f": 0.22222221763038558
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.08337v1",
      "true_abstract": "This paper provides a discretization-free solution to the synthesis of\napprox-imation-free closed-form controllers for unknown nonlinear systems to\nenforce complex properties expressed by $\\omega$-regular languages, as\nrecognized by Non-deterministic B\\\"uchi Automata (NBA). In order to solve this\nproblem, we first decompose NBA into a sequence of reach-avoid problems, which\nare solved using the Spatiotemporal Tubes (STT) approach. Controllers for each\nreach-avoid task are then integrated into a hybrid policy that ensures the\nfulfillment of the desired $\\omega$-regular properties. We validate our method\nthrough omnidirectional robot navigation and manipulator control case studies.",
      "generated_abstract": "This paper presents a novel framework for the design and synthesis of\nspatiotemporal tube controllers for unknown systems. The design process\ninvolves the identification of a tubular structure, the specification of\nspecifications for each control input and the synthesis of the tube controllers\nusing an optimisation method. The proposed methodology enables the design of\nspatiotemporal tube controllers that can be applied to a variety of unknown\nsystems. The efficacy of the proposed methodology is demonstrated through a\ncomparison of the synthesised controllers with the standard methods. Numerical\nexperiments are conducted using a modified version of the nonlinear\nquadratic-quadratic problem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26666666666666666,
          "p": 0.3389830508474576,
          "f": 0.29850745775785265
        },
        "rouge-2": {
          "r": 0.07865168539325842,
          "p": 0.08139534883720931,
          "f": 0.0799999950014697
        },
        "rouge-l": {
          "r": 0.24,
          "p": 0.3050847457627119,
          "f": 0.26865671148919584
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2502.13325v1",
      "true_abstract": "In this paper, we consider catastrophe stop-loss reinsurance valuation for a\nreinsurance company with dynamic contagion claims. To deal with conventional\nand emerging catastrophic events, we propose the use of a compound dynamic\ncontagion process for the catastrophic component of the liability. Under the\npremise that there is an absence of arbitrage opportunity in the market, we\nobtain arbitrage-free premiums for these contacts. To this end, the Esscher\ntransform is adopted to specify an equivalent martingale probability measure.\nWe show that reinsurers have various ways of levying the security loading on\nthe net premiums to quantify the catastrophic liability in light of the growing\nchallenges posed by emerging risks arising from climate change, cyberattacks,\nand pandemics. We numerically compare arbitrage-free catastrophe stop-loss\nreinsurance premiums via the Monte Carlo simulation method. Sensitivity\nanalyzes are performed by changing the Esscher parameters and the retention\nlevel.",
      "generated_abstract": "We develop a catastrophe reinsurance model for compound dynamic contagion\nclaims. The model incorporates contagion shocks and heterogeneity in the\nunderlying claims. We formulate a martingale problem and derive the value of\nthe reinsurance contract. We develop a numerical method to compute the\nvalue-at-risk of the reinsurance contract. We show that the value of the\nreinsurance contract decreases with increasing contagion and heterogeneity. We\nalso show that the value-at-risk of the reinsurance contract decreases with\nincreasing contagion and heterogeneity. We validate our model using a\nsimulation study and a case study.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20833333333333334,
          "p": 0.46511627906976744,
          "f": 0.2877697798995912
        },
        "rouge-2": {
          "r": 0.051470588235294115,
          "p": 0.109375,
          "f": 0.06999999564800027
        },
        "rouge-l": {
          "r": 0.20833333333333334,
          "p": 0.46511627906976744,
          "f": 0.2877697798995912
        }
      }
    },
    {
      "paper_id": "math.AC.math/HO/2502.13273v1",
      "true_abstract": "We give a new proof of the fundamental theorem of algebra. It is entirely\nelementary, focused on using long division to its fullest extent. Further, the\nmethod quickly recovers a more general version of the theorem recently obtained\nby Joseph Shipman.",
      "generated_abstract": "We give an elementary proof of the fundamental theorem of algebra, which\nis a consequence of the following result.\n  Let $A$ be a commutative ring with unit, and let $R$ be a unital subring of\n$A$. If $f\\in A$ is a non-zero element and $g\\in R$ is such that $f\\cdot g=0$,\nthen $g\\in R$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2571428571428571,
          "p": 0.225,
          "f": 0.23999999502222233
        },
        "rouge-2": {
          "r": 0.15384615384615385,
          "p": 0.12244897959183673,
          "f": 0.13636363142820265
        },
        "rouge-l": {
          "r": 0.2571428571428571,
          "p": 0.225,
          "f": 0.23999999502222233
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.08755v1",
      "true_abstract": "Combining the technique of employing coset codes for communicating over a\nquantum broadcast channel and the recent discovery of \\textit{tilting,\nsmoothing and augmentation} by Sen to perform simultaneous decoding over\nnetwork quantum channels, we derive new inner bounds to the capacity region of\na $3-$user classical quantum broadcast channel that subsumes all known.",
      "generated_abstract": "e a novel technique to decode classical coset codes over the\n$3$-user quantum broadcast channel (QBC). This is achieved by jointly decoding\nthe received signals from all users using a $3\\times 3$ binary shift-and-save\ncodebook and a classical channel code. By combining the achievable rate regions\nof the classical coset codes with the achievable rate regions of the binary\nshift-and-save codebook, we obtain a new achievable region. We also\nderive a closed-form expression for the probability of error for the new\nachievable region, which is useful for the design of error-resilient codes.\nFinally, we prove that the rate region of the proposed scheme is contained in\nthe rate region of the optimal joint decoding scheme. We validate our\nproposed scheme by simulating the decoding of 256-bit received sequences from\n$16$ users",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.5365853658536586,
          "p": 0.29333333333333333,
          "f": 0.3793103402571344
        },
        "rouge-2": {
          "r": 0.08,
          "p": 0.035398230088495575,
          "f": 0.04907975034815047
        },
        "rouge-l": {
          "r": 0.4146341463414634,
          "p": 0.22666666666666666,
          "f": 0.2931034437054103
        }
      }
    },
    {
      "paper_id": "cs.DC.cs/PF/2503.03274v1",
      "true_abstract": "Ensuring Service Level Objectives (SLOs) in large-scale architectures, such\nas Distributed Computing Continuum Systems (DCCS), is challenging due to their\nheterogeneous nature and varying service requirements across different devices\nand applications. Additionally, unpredictable workloads and resource\nlimitations lead to fluctuating performance and violated SLOs. To improve SLO\ncompliance in DCCS, one possibility is to apply machine learning; however, the\ndesign choices are often left to the developer. To that extent, we provide a\nbenchmark of Active Inference -- an emerging method from neuroscience --\nagainst three established reinforcement learning algorithms (Deep Q-Network,\nAdvantage Actor-Critic, and Proximal Policy Optimization). We consider a\nrealistic DCCS use case: an edge device running a video conferencing\napplication alongside a WebSocket server streaming videos. Using one of the\nrespective algorithms, we continuously monitor key performance metrics, such as\nlatency and bandwidth usage, to dynamically adjust parameters -- including the\nnumber of streams, frame rate, and resolution -- to optimize service quality\nand user experience. To test algorithms' adaptability to constant system\nchanges, we simulate dynamically changing SLOs and both instant and gradual\ndata-shift scenarios, such as network bandwidth limitations and fluctuating\ndevice thermal states. Although the evaluated algorithms all showed advantages\nand limitations, our findings demonstrate that Active Inference is a promising\napproach for ensuring SLO compliance in DCCS, offering lower memory usage,\nstable CPU utilization, and fast convergence.",
      "generated_abstract": "r presents a framework to benchmark the dynamic SLO compliance of\nnumerous continuous computing platforms, including high-performance computing,\ndistributed computing, and data analytics systems. The framework is based on\nthe use of a continuous system, which can be viewed as a dynamic SLO\ncompliance benchmark. This framework allows for the evaluation of the\nperformance of a system, including the response time, in a dynamic environment\nwhile maintaining a service level agreement. The framework also enables the\nevaluation of dynamic SLO compliance across various systems, including\ndistributed computing, high-performance computing, and data analytics. The\nframework is based on a continuous system, which can be viewed as a dynamic SLO\ncompliance benchmark. This framework allows for the evaluation of the performance\nof a system, including the response time, in a dynamic environment while\nmaintaining a service level agreement. The framework also enables the\nevaluation of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10429447852760736,
          "p": 0.32075471698113206,
          "f": 0.15740740370413245
        },
        "rouge-2": {
          "r": 0.013824884792626729,
          "p": 0.0379746835443038,
          "f": 0.020270266357058826
        },
        "rouge-l": {
          "r": 0.09815950920245399,
          "p": 0.3018867924528302,
          "f": 0.1481481444448732
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2503.08272v1",
      "true_abstract": "Monotone mean-variance (MMV) utility is the minimal modification of the\nclassical Markowitz utility that respects rational ordering of investment\nopportunities. This paper provides, for the first time, a complete\ncharacterization of optimal dynamic portfolio choice for the MMV utility in\nasset price models with independent returns. The task is performed under\nminimal assumptions, weaker than the existence of an equivalent martingale\nmeasure and with no restrictions on the moments of asset returns. We interpret\nthe maximal MMV utility in terms of the monotone Sharpe ratio (MSR) and show\nthat the global squared MSR arises as the nominal yield from continuously\ncompounding at the rate equal to the maximal local squared MSR. The paper gives\nsimple necessary and sufficient conditions for mean-variance (MV) efficient\nportfolios to be MMV efficient. Several illustrative examples contrasting the\nMV and MMV criteria are provided.",
      "generated_abstract": "portfolio optimization with monotone mean--variance preferences,\nwhere the utility function is characterized by a monotone convex function\n$u: \\mathbb{R}_+ \\to \\mathbb{R}_+$. We present a portfolio optimization\nframework that models the problem as a reinforcement learning (RL)\noptimization problem and characterizes the optimal portfolio as a\ndistribution-optimal portfolio. The optimal distribution-optimal portfolio\nsatisfies the monotone convexity constraint and is characterized by the\ndistribution-optimal utility function. We show that, in this setting, the\noptimal distribution-optimal portfolio can be computed via a Monte Carlo\nsimulation. We also show that, under some conditions, the optimal\ndistribution-optimal portfolio can be computed by a simple linear\nregression. We also present an optimal strategy for the portfolio optimization\nproblem that utilizes the optimal distribution-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21875,
          "p": 0.3442622950819672,
          "f": 0.26751591881536785
        },
        "rouge-2": {
          "r": 0.015151515151515152,
          "p": 0.021052631578947368,
          "f": 0.017621140507288503
        },
        "rouge-l": {
          "r": 0.17708333333333334,
          "p": 0.2786885245901639,
          "f": 0.216560504802629
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/SI/2503.07961v1",
      "true_abstract": "Although hypergraph neural networks (HGNNs) have emerged as a powerful\nframework for analyzing complex datasets, their practical performance often\nremains limited. On one hand, existing networks typically employ a single type\nof attention mechanism, focusing on either structural or feature similarities\nduring message passing. On the other hand, assuming that all nodes in current\nhypergraph models have the same level of overlap may lead to suboptimal\ngeneralization. To overcome these limitations, we propose a novel framework,\noverlap-aware meta-learning attention for hypergraph neural networks\n(OMA-HGNN). First, we introduce a hypergraph attention mechanism that\nintegrates both structural and feature similarities. Specifically, we linearly\ncombine their respective losses with weighted factors for the HGNN model.\nSecond, we partition nodes into different tasks based on their diverse overlap\nlevels and develop a multi-task Meta-Weight-Net (MWN) to determine the\ncorresponding weighted factors. Third, we jointly train the internal MWN model\nwith the losses from the external HGNN model and train the external model with\nthe weighted factors from the internal model. To evaluate the effectiveness of\nOMA-HGNN, we conducted experiments on six real-world datasets and benchmarked\nits perfor-mance against nine state-of-the-art methods for node classification.\nThe results demonstrate that OMA-HGNN excels in learning superior node\nrepresentations and outperforms these baselines.",
      "generated_abstract": "h neural networks (HNNs) have emerged as a powerful tool for node\nclassification, enabling deep learning-based reasoning across graph structures\nand complex node features. However, traditional HNNs often struggle to\ngeneralize to unseen tasks or complex node features, resulting in sub-optimal\nperformance. To address this, we propose a novel hypergraph attention\nmechanism that integrates attention mechanisms into HNNs to capture node\ninteractions and enhance generalization. Specifically, we introduce the\noverlap-aware hypergraph attention (OAHA), which introduces a pairwise\noverlap-aware attention mechanism that incorporates both global and local\ninteractions between nodes. By leveraging this attention mechanism, OAHA\nenhances HNNs' ability to generalize to unseen tasks and complex node features.\nExtensive experiments demonstrate that OAHA significantly improves the\nperformance of HNN",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2835820895522388,
          "p": 0.4523809523809524,
          "f": 0.3486238484740342
        },
        "rouge-2": {
          "r": 0.08854166666666667,
          "p": 0.16037735849056603,
          "f": 0.11409395514796651
        },
        "rouge-l": {
          "r": 0.2537313432835821,
          "p": 0.40476190476190477,
          "f": 0.31192660076761225
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/OT/2405.01342v1",
      "true_abstract": "Economic policy sciences are constantly investigating the quality of\nwell-being of broad sections of the population in order to describe the current\ninterdependence between unequal living conditions, low levels of education and\na lack of integration into society. Such studies are often carried out in the\nform of surveys, e.g. as part of the EU-SILC program. If the survey is designed\nat national or international level, the results of the study are often used as\na reference by a broad range of public institutions. However, the sampling\nstrategy per se may not capture enough information to provide an accurate\nrepresentation of all population strata. Problems might arise from rare, or\nhard-to-sample, populations and the conclusion of the study may be compromised\nor unrealistic. We propose here a two-phase methodology to identify rare,\npoorly sampled populations and then resample the hard-to-sample strata. We\nfocused our attention on the 2019 EU-SILC section concerning the Italian region\nof Liguria. Methods based on dispersion indices or deep learning were used to\ndetect rare populations. A multi-frame survey was proposed as the sampling\ndesign. The results showed that factors such as citizenship, material\ndeprivation and large families are still fundamental characteristics that are\ndifficult to capture.",
      "generated_abstract": "tive of this study is to develop a methodological framework for\nresearchers interested in rare population detection and sampling in Liguria,\nwhich is characterized by a complex and heterogeneous population structure. In\nthis context, the development of a methodological framework is essential, since\nit is difficult to design and implement sampling strategies in real-life\napplications. The framework proposed in this paper aims to provide a\ngeneral-purpose tool that can be used for a variety of research questions in\nrare population detection and sampling. The proposed framework is based on\nprobability theory, with a focus on the Bayesian analysis of the observed data\nin a Bayesian framework. The framework provides a rigorous statistical\nframework for rare population detection and sampling. The proposed framework\noffers a comprehensive and flexible approach for researchers interested in\nrare population detection and sampling, with the ability to accommodate\ndifferent types of data and research questions. This method",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15942028985507245,
          "p": 0.2857142857142857,
          "f": 0.20465115819318558
        },
        "rouge-2": {
          "r": 0.030927835051546393,
          "p": 0.048,
          "f": 0.037617550092865264
        },
        "rouge-l": {
          "r": 0.15217391304347827,
          "p": 0.2727272727272727,
          "f": 0.19534883261179026
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.02073v1",
      "true_abstract": "Analyzing time-series cross-sectional (also known as longitudinal or panel)\ndata is an important process across a number of fields, including the social\nsciences, economics, finance, and medicine. PanelMatch is an R package that\nimplements a set of tools enabling researchers to apply matching methods for\ncausal inference with time-series cross-sectional data. Relative to other\ncommonly used methods for longitudinal analyses, like regression with fixed\neffects, the matching-based approach implemented in PanelMatch makes fewer\nparametric assumptions and offers more diagnostics. In this paper, we discuss\nthe PanelMatch package, showing users a recommended pipeline for doing causal\ninference analysis with it and highlighting useful diagnostic and visualization\ntools.",
      "generated_abstract": "We introduce PanelMatch, a Python package for the analysis of causal\ninvestigations with panel data. PanelMatch offers a number of advanced\nmethods for matching causal effects of interest to the data, including\ndifferential and random effects matching, robust estimation of causal effects,\nand the analysis of the causal effects of multiple time-varying covariates.\nAdditionally, it features a novel method for the analysis of causal effects of\nrandom effects, which leverages the latent variable model.\n  PanelMatch is an open-source project that is released under the MIT license.\nIt is available at\n  https://github.com/causal-inference/PanelMatch.git.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2716049382716049,
          "p": 0.36065573770491804,
          "f": 0.3098591500287642
        },
        "rouge-2": {
          "r": 0.04950495049504951,
          "p": 0.06172839506172839,
          "f": 0.05494505000543457
        },
        "rouge-l": {
          "r": 0.19753086419753085,
          "p": 0.26229508196721313,
          "f": 0.22535210777524312
        }
      }
    },
    {
      "paper_id": "astro-ph.IM.eess/IV/2503.00156v1",
      "true_abstract": "Neural posterior estimation (NPE), a type of amortized variational inference,\nis a computationally efficient means of constructing probabilistic catalogs of\nlight sources from astronomical images. To date, NPE has not been used to\nperform inference in models with spatially varying covariates. However,\nground-based astronomical images have spatially varying sky backgrounds and\npoint spread functions (PSFs), and accounting for this variation is essential\nfor constructing accurate catalogs of imaged light sources. In this work, we\nintroduce a method of performing NPE with spatially varying backgrounds and\nPSFs. In this method, we generate synthetic catalogs and semi-synthetic images\nfor these catalogs using randomly sampled PSF and background estimates from\nexisting surveys. Using this data, we train a neural network, which takes an\nastronomical image and representations of its background and PSF as input, to\noutput a probabilistic catalog. Our experiments with Sloan Digital Sky Survey\ndata demonstrate the effectiveness of NPE in the presence of spatially varying\nbackgrounds and PSFs for light source detection, star/galaxy separation, and\nflux measurement.",
      "generated_abstract": "t a new framework for neural image post-processing, incorporating\nmodel-based estimation of the background and point spread function (PSF) for\ncataloging astronomical images. The framework is designed for high-contrast\nimages, with high-resolution PSFs, and is scalable to images with multiple\ncomponents. The framework uses a neural network to estimate the PSF for\neach component, as well as the background for each component. The network\nestimates the background for each component using a convolutional neural\nnetwork, which is a specialized architecture designed for image classification\nand segmentation. The network estimates the background for each component\nusing a spatially varying background kernel, which is a specialized architecture\ndesigned for image classification and segmentation. The framework is\nparameterized by the PSFs for each component, the background kernel for each\ncomponent, and the PSF of the background for each component. The framework\nuses",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2336448598130841,
          "p": 0.423728813559322,
          "f": 0.30120481469516625
        },
        "rouge-2": {
          "r": 0.06493506493506493,
          "p": 0.10638297872340426,
          "f": 0.08064515658298674
        },
        "rouge-l": {
          "r": 0.21495327102803738,
          "p": 0.3898305084745763,
          "f": 0.2771084291529976
        }
      }
    },
    {
      "paper_id": "cs.AI.q-fin/ST/2411.04788v1",
      "true_abstract": "In recent years, the application of generative artificial intelligence\n(GenAI) in financial analysis and investment decision-making has gained\nsignificant attention. However, most existing approaches rely on single-agent\nsystems, which fail to fully utilize the collaborative potential of multiple AI\nagents. In this paper, we propose a novel multi-agent collaboration system\ndesigned to enhance decision-making in financial investment research. The\nsystem incorporates agent groups with both configurable group sizes and\ncollaboration structures to leverage the strengths of each agent group type. By\nutilizing a sub-optimal combination strategy, the system dynamically adapts to\nvarying market conditions and investment scenarios, optimizing performance\nacross different tasks. We focus on three sub-tasks: fundamentals, market\nsentiment, and risk analysis, by analyzing the 2023 SEC 10-K forms of 30\ncompanies listed on the Dow Jones Index. Our findings reveal significant\nperformance variations based on the configurations of AI agents for different\ntasks. The results demonstrate that our multi-agent collaboration system\noutperforms traditional single-agent models, offering improved accuracy,\nefficiency, and adaptability in complex financial environments. This study\nhighlights the potential of multi-agent systems in transforming financial\nanalysis and investment decision-making by integrating diverse analytical\nperspectives.",
      "generated_abstract": "y proposes an agent-based model (ABM) for financial research\ninvolving AI agents that collect and analyze data from various sources, such as\nhistorical data, market news, and financial publications. The ABM model\nsimulates the interactions of multiple AI agents with a single human investor.\nThe ABM is implemented in Python and uses the OpenAI GPT-3.5 model for\ngenerative text generation. This study explores how the AI agents'\ncollaboration patterns affect the investment performance of the human investor.\nThe results show that the human investor's return is positively correlated\nwith the AI agents' collaboration patterns, especially in cases where the AI\nagents work together to analyze historical data. This study provides\ninsights into the potential of AI agents for financial research and the\nbenefits of collaborative AI-based strategies. The study highlight",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19230769230769232,
          "p": 0.3048780487804878,
          "f": 0.23584905186009264
        },
        "rouge-2": {
          "r": 0.05172413793103448,
          "p": 0.07758620689655173,
          "f": 0.06206896071724175
        },
        "rouge-l": {
          "r": 0.18461538461538463,
          "p": 0.2926829268292683,
          "f": 0.22641508959594173
        }
      }
    },
    {
      "paper_id": "cs.AR.cs/OS/2502.02349v1",
      "true_abstract": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times.",
      "generated_abstract": "In this paper, we present a novel cache placement policy that adaptively\nintroduces cache redundancies to improve performance. The proposed policy\nintroduces cache redundancies in random locations and dynamically adjusts the\nnumber of cache redundancies to achieve the desired performance. We\nintroduce the cache redundancy modeling using a genetic algorithm, which\noptimizes the cache redundancy using a fitness function based on the\nperformance metric. We further introduce a cache redundancy optimization\nstrategy using a local search algorithm. Our approach is based on\nexploration-exploitation strategy, which is the most effective approach in\noptimizing the cache placement. We evaluate our approach on a set of\nrepresentative benchmarks, and the results show that the proposed approach\nachieves the best performance in terms of cache hit ratio and hit ratio variation\nfor different cache sizes.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2087912087912088,
          "p": 0.25675675675675674,
          "f": 0.23030302535610664
        },
        "rouge-2": {
          "r": 0.015748031496062992,
          "p": 0.017241379310344827,
          "f": 0.01646090036004148
        },
        "rouge-l": {
          "r": 0.18681318681318682,
          "p": 0.22972972972972974,
          "f": 0.20606060111368243
        }
      }
    },
    {
      "paper_id": "math.ST.math/ST/2503.07022v1",
      "true_abstract": "For some discretely observed path of oscillating Brownian motion with level\nof self-organized criticality $\\rho_0$, we prove in the infill asymptotics that\nthe MLE is $n$-consistent, where $n$ denotes the sample size, and derive its\nlimit distribution with respect to stable convergence. As the transition\ndensity of this homogeneous Markov process is not even continuous in $\\rho_0$,\nthe analysis is highly non-standard. Therefore, interesting and somewhat\nunexpected phenomena occur: The likelihood function splits into several\ncomponents, each of them contributing very differently depending on how close\nthe argument $\\rho$ is to $\\rho_0$. Correspondingly, the MLE is successively\nexcluded to lay outside a compact set, a $1/\\sqrt{n}$-neighborhood and finally\na $1/n$-neigborhood of $\\rho_0$ asymptotically. The crucial argument to derive\nthe stable convergence is to exploit the semimartingale structure of the\nsequential suitably rescaled local log-likelihood function (as a process in\ntime). Both sequentially and as a process in $\\rho$, it exhibits a bivariate\nPoissonian behavior in the stable limit with its intensity being a multiple of\nthe local time at $\\rho_0$.",
      "generated_abstract": "We consider the multivariate Brownian motion in $n$ dimensions with a\nrandom intensity $\\lambda$, which evolves according to the diffusion\nequation: $\\frac{\\partial}{\\partial t} X(t,x)=\\lambda(X(t,x))^2-X(t,x)$. We\nprove that the limiting distribution of the logarithmic tail of the\nself-organized criticality (SOC) process is a Poisson point process on the\nunit cube. We also show that the convergence of the multivariate MLE in\n$\\ell_2$ is $n$-consistent. Finally, we prove that the limiting distribution of\nthe self-organized criticality (SOC) process is a Poisson point process on the\nunit cube.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18584070796460178,
          "p": 0.4375,
          "f": 0.2608695610323676
        },
        "rouge-2": {
          "r": 0.0375,
          "p": 0.09836065573770492,
          "f": 0.054298638537294774
        },
        "rouge-l": {
          "r": 0.1592920353982301,
          "p": 0.375,
          "f": 0.223602480287026
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/FL/2502.19603v1",
      "true_abstract": "This work studies the planning problem for robotic systems under both\nquantifiable and unquantifiable uncertainty. The objective is to enable the\nrobotic systems to optimally fulfill high-level tasks specified by Linear\nTemporal Logic (LTL) formulas. To capture both types of uncertainty in a\nunified modelling framework, we utilise Markov Decision Processes with\nSet-valued Transitions (MDPSTs). We introduce a novel solution technique for\nthe optimal robust strategy synthesis of MDPSTs with LTL specifications. To\nimprove efficiency, our work leverages limit-deterministic B\\\"uchi automata\n(LDBAs) as the automaton representation for LTL to take advantage of their\nefficient constructions. To tackle the inherent nondeterminism in MDPSTs, which\npresents a significant challenge for reducing the LTL planning problem to a\nreachability problem, we introduce the concept of a Winning Region (WR) for\nMDPSTs. Additionally, we propose an algorithm for computing the WR over the\nproduct of the MDPST and the LDBA. Finally, a robust value iteration algorithm\nis invoked to solve the reachability problem. We validate the effectiveness of\nour approach through a case study involving a mobile robot operating in the\nhexagonal world, demonstrating promising efficiency gains.",
      "generated_abstract": "We introduce a novel technique for planning under Linear Temporal Logic (LTL)\nspecifications that accounts for both quantifiable and unquantifiable\nuncertainty. Specifically, we use a quantifier elimination procedure to\ndetermine a set of possible executions of the specification, which is then\nused as the initial state of a planning algorithm. We then employ the\npreviously proposed LTL-Planning technique to determine a solution using\nprobabilistic planning, and we augment the solution with the initial\nexecution. This solution is then used as the initial state of a second planning\nalgorithm. The second planning algorithm uses a variant of the SMALL-PLAN\nalgorithm to search for the best possible solution. We evaluate our approach on\na series of benchmarks and demonstrate that our approach is effective in\ndetermining feasible solutions under both quantifiable and unquantifiable\nuncertainty.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2682926829268293,
          "p": 0.4520547945205479,
          "f": 0.3367346892029363
        },
        "rouge-2": {
          "r": 0.09444444444444444,
          "p": 0.15315315315315314,
          "f": 0.1168384832536226
        },
        "rouge-l": {
          "r": 0.2601626016260163,
          "p": 0.4383561643835616,
          "f": 0.32653060757028324
        }
      }
    },
    {
      "paper_id": "cs.CC.cs/CC/2503.05548v1",
      "true_abstract": "We study integer linear programs (ILP) of the form $\\min\\{c^\\top x\\ \\vert\\\nAx=b,l\\le x\\le u,x\\in\\mathbb Z^n\\}$ and analyze their parameterized complexity\nwith respect to their distance to the generalized matching problem--following\nthe well-established approach of capturing the hardness of a problem by the\ndistance to triviality. The generalized matching problem is an ILP where each\ncolumn of the constraint matrix has $1$-norm of at most $2$. It captures\nseveral well-known polynomial time solvable problems such as matching and flow\nproblems. We parameterize by the size of variable and constraint backdoors,\nwhich measure the least number of columns or rows that must be deleted to\nobtain a generalized matching ILP.\n  We present the following results: (i) a fixed-parameter tractable (FPT)\nalgorithm for ILPs parameterized by the size $p$ of a minimum variable backdoor\nto generalized matching; (ii) a randomized slice-wise polynomial (XP) time\nalgorithm for ILPs parameterized by the size $h$ of a minimum constraint\nbackdoor to generalized matching as long as $c$ and $A$ are encoded in unary;\n(iii) we complement (ii) by proving that solving an ILP is W[1]-hard when\nparameterized by $h$ even when $c,A,l,u$ have coefficients of constant size. To\nobtain (i), we prove a variant of lattice-convexity of the degree sequences of\nweighted $b$-matchings, which we study in the light of SBO jump M-convex\nfunctions. This allows us to model the matching part as a polyhedral constraint\non the integer backdoor variables. The resulting ILP is solved in FPT time\nusing an integer programming algorithm. For (ii), the randomized XP time\nalgorithm is obtained by pseudo-polynomially reducing the problem to the exact\nmatching problem. To prevent an exponential blowup in terms of the encoding\nlength of $b$, we bound the Graver complexity of the constraint matrix and\nemploy a Graver augmentation local search framework.",
      "generated_abstract": "aper, we study matching integer programs (MIPs) with additional rows\nand columns. In particular, we consider MIPs with multiple rows and columns,\nwhere the rows and columns are specified by positive integers.\n  We propose several parameterized algorithms for solving MIPs with multiple\nrows and columns. Our first algorithm uses a reduction from the minimum\nweighted bipartite matching problem to solve MIPs with multiple rows and\ncolumns. The reduction is based on the fact that a minimum weighted bipartite\nmatching of a bipartite graph can be converted into a matching of a bipartite\ngraph with multiple rows and columns. We use the reduction to obtain two\nparameterized algorithms: one for solving MIPs with one row and one with one\ncolumn, and one for solving MIPs with two rows and columns.\n  We also propose a parameterized algorithm for solving MIPs with multiple\ncolumns. Our",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1901840490797546,
          "p": 0.47692307692307695,
          "f": 0.2719298204851493
        },
        "rouge-2": {
          "r": 0.0299625468164794,
          "p": 0.08,
          "f": 0.043596726280542945
        },
        "rouge-l": {
          "r": 0.18404907975460122,
          "p": 0.46153846153846156,
          "f": 0.2631578906605879
        }
      }
    },
    {
      "paper_id": "cs.CC.cs/CC/2503.01180v1",
      "true_abstract": "It is well known that the graph isomorphism problem is polynomial-time\nreducible to the graph automorphism problem (in fact these two problems are\npolynomial-time equivalent). We show that, analogously, the group isomorphism\nproblem is polynomial-time reducible to the group automorphism problem.\nReductions to other relevant problems like automorphism counting are also\ngiven.",
      "generated_abstract": "In the context of finite groups, we define a new notion of isomorphism\nin the setting of finite groups, and show that this is equivalent to the\nproblem of deciding whether the group isomorphic to a subgroup of a given group\nis a subgroup of that given group. We then show that this problem is\nreducible to the problem of deciding whether the given group is a subgroup of\na given group. In particular, we show that the problem of deciding whether a\ngroup is a subgroup of a given group is reducible to the group isomorphism\nproblem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.34375,
          "p": 0.34375,
          "f": 0.34374999500000003
        },
        "rouge-2": {
          "r": 0.16666666666666666,
          "p": 0.11864406779661017,
          "f": 0.13861385652779157
        },
        "rouge-l": {
          "r": 0.34375,
          "p": 0.34375,
          "f": 0.34374999500000003
        }
      }
    },
    {
      "paper_id": "math.SP.math/SP/2503.06634v1",
      "true_abstract": "In our recent papers, we studied semiclassical spectral problems for the\nBochner-Schr\\\"odinger operator on a manifold of bounded geometry. We survey\nsome results of these papers in the setting of the magnetic Schr\\\"odinger\noperator in the Euclidean space and describe some ideas of the proofs.",
      "generated_abstract": "In this paper, we investigate the semiclassical spectral properties of\nmagnetic Schr\\\"odinger operators in a magnetic field, which are defined on the\nquantum space of magnetic operators. We prove the existence of a spectral\ntransfer map from the quantum space of magnetic operators to the quantum space\nof magnetic operators coupled to the magnetic field. We also prove the\nsemiclassical spectral rigidity of the magnetic Schr\\\"odinger operator and the\nspectral rigidity of the magnetic Schr\\\"odinger operator coupled to the\nmagnetic field.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.42857142857142855,
          "p": 0.42857142857142855,
          "f": 0.4285714235714286
        },
        "rouge-2": {
          "r": 0.11904761904761904,
          "p": 0.09433962264150944,
          "f": 0.10526315296177309
        },
        "rouge-l": {
          "r": 0.4,
          "p": 0.4,
          "f": 0.3999999950000001
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.03781v1",
      "true_abstract": "This technical report presents a novel DMD-based characterization method for\nvision sensors, particularly neuromorphic sensors such as event-based vision\nsensors (EVS) and Tianmouc, a complementary vision sensor. Traditional image\nsensor characterization standards, such as EMVA1288, are unsuitable for BVS due\nto their dynamic response characteristics. To address this, we propose a\nhigh-speed, high-precision testing system using a Digital Micromirror Device\n(DMD) to modulate spatial and temporal light intensity. This approach enables\nquantitative analysis of key parameters such as event latency, signal-to-noise\nratio (SNR), and dynamic range (DR) under controlled conditions. Our method\nprovides a standardized and reproducible testing framework, overcoming the\nlimitations of existing evaluation techniques for neuromorphic sensors.\nFurthermore, we discuss the potential of this method for large-scale BVS\ndataset generation and conversion, paving the way for more consistent\nbenchmarking of bio-inspired vision technologies.",
      "generated_abstract": "This paper introduces a novel method for characterizing vision sensors based on\nthe Discrete Wavelet Transform (DWT) and a DMD-based algorithm. The proposed\nmethod is based on the assumption that the system response to a\nsingle-frequency image can be well approximated by a wavelet transform of a\nfunction. This assumption is validated through numerical simulations, where\nthe proposed method is found to accurately estimate the system response. The\nmethod is then applied to two vision sensors: a 3D depth sensor and a 2D\ndepth sensor, and the results are compared with those obtained using the\ntraditional approach of fitting a second-order polynomial to the system\nresponse. The results show that the proposed method provides a more accurate\nestimation of the system response than the traditional approach.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21428571428571427,
          "p": 0.28378378378378377,
          "f": 0.24418604160897792
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.04672897196261682,
          "f": 0.04219408787409483
        },
        "rouge-l": {
          "r": 0.19387755102040816,
          "p": 0.25675675675675674,
          "f": 0.22093022765548956
        }
      }
    },
    {
      "paper_id": "physics.gen-ph.physics/gen-ph/2502.20425v1",
      "true_abstract": "The measurements of cluster abundances, gravitational lensings, redshift\nspace distortions and peculiar velocities at lower redshifts point out to much\nsmaller sigma_8 than its value deduced from the measurements of the CMB\nfluctuations assuming the standard LCDM cosmology. We examine and compare the\nsigma_8 redshift dependence calculated within the gauge invariant formalism in\nthe LCDM and the Einstein-Cartan cosmology. It appears that the Einstein-Cartan\ncosmology provides systematically larger sigma_8(z) for higher redshifts\ncompared to the LCDM. Because the CMB fluctuations comprise a cosmological data\nfrom the recombination era to the present, the S8 problem of the LCDM cosmology\nis not a surprise from the standpoint of the Einstein-Cartan cosmology.",
      "generated_abstract": "aper we revisit the Einstein-Cartan cosmology, which has been\nrecently proposed as a candidate for the Universe in the context of\ncosmological quantum field theory, and discuss its implications for the S8\nproblem. We show that the Einstein-Cartan cosmology is incompatible with the\nS8 problem due to the non-renormalizability of the Einstein-Cartan action,\nwhich is a consequence of the non-linearity of the Einstein-Cartan equations. We\nalso present a novel approach to the S8 problem, where we use the Einstein-Cartan\ncosmology as a starting point, and then introduce an auxiliary scalar field to\ngenerate the required curvature perturbations. We show that the auxiliary\nscalar field, which is introduced to generate the curvature perturbations, can\nbe interpreted as the inflaton field, and that the S8 problem can",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2318840579710145,
          "p": 0.23529411764705882,
          "f": 0.23357663733603296
        },
        "rouge-2": {
          "r": 0.0967741935483871,
          "p": 0.0891089108910891,
          "f": 0.09278350016314195
        },
        "rouge-l": {
          "r": 0.21739130434782608,
          "p": 0.22058823529411764,
          "f": 0.21897809719004754
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.00863v1",
      "true_abstract": "This paper studies the relationship between soft and hard paternalism by\nexamining two kinds of restriction: a waiting period and a hard limit (cap) on\nrisk-seeking behavior. Mandatory waiting periods have been instituted for\nmedical procedures, gun purchases and other high-stakes decisions. Are these\npolicies substitutes for hard restrictions, and are delayed decisions more\nrespected? In an experiment, decision-makers are informed about an impending\nhigh-stakes decision. Treatments define when the decision is made: on the spot\nor after one day, and whether the initial decision can be revised. In a general\npopulation survey experiment, another class of subjects (Choice Architects) is\ngranted the opportunity to make rules for decision-makers. Given a decision's\ntemporal structure, Choice Architects can decide on a cap to the\ndecision-maker's risk taking. In another treatment, Choice Architects can\nimplement a mandatory waiting period in addition to the cap. This allows us to\nstudy the substitutional relationship between waiting periods and paternalistic\naction and the effect of deliberation on the autonomy afforded to the\ndecision-maker. Our highly powered experiment reveals that exogenous\ndeliberation has no effect on the cap. Moreover, endogenously prescribed\nwaiting periods represent add-on restrictions that do not substitute for the\ncap. Choice Architects believe that, with time, the average decision-maker will\ntake less risk and -- because of the distribution of Choice Architects' bliss\npoints -- come closer to Choice Architects' subjective ideal choice. These\nfindings highlight the complementarity of policy tools in targeting various\nparts of a distribution of decision-makers.",
      "generated_abstract": "We conduct an experiment to explore whether deliberation can reduce the\npaternalism bias, whereby the decision-maker adopts a more restrictive rule\nthan what was agreed upon by the deliberators. The experiment employs a\nsimulated court system in which the decision-maker has to approve or reject\nrules of the form \"A person who is not an adult can do X.\" We find that\ndeliberation can reduce paternalism. We also find that deliberation can reduce\npaternalism when the decision-maker is an expert on the subject matter, but not\nwhen the decision-maker is an expert on the law.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14838709677419354,
          "p": 0.38333333333333336,
          "f": 0.21395348434829645
        },
        "rouge-2": {
          "r": 0.017316017316017316,
          "p": 0.05063291139240506,
          "f": 0.02580644781498495
        },
        "rouge-l": {
          "r": 0.12258064516129032,
          "p": 0.31666666666666665,
          "f": 0.17674418202271505
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/NE/2503.06484v1",
      "true_abstract": "Accurate sign language understanding serves as a crucial communication\nchannel for individuals with disabilities. Current sign language translation\nalgorithms predominantly rely on RGB frames, which may be limited by fixed\nframe rates, variable lighting conditions, and motion blur caused by rapid hand\nmovements. Inspired by the recent successful application of event cameras in\nother fields, we propose to leverage event streams to assist RGB cameras in\ncapturing gesture data, addressing the various challenges mentioned above.\nSpecifically, we first collect a large-scale RGB-Event sign language\ntranslation dataset using the DVS346 camera, termed VECSL, which contains\n15,676 RGB-Event samples, 15,191 glosses, and covers 2,568 Chinese characters.\nThese samples were gathered across a diverse range of indoor and outdoor\nenvironments, capturing multiple viewing angles, varying light intensities, and\ndifferent camera motions. Due to the absence of benchmark algorithms for\ncomparison in this new task, we retrained and evaluated multiple\nstate-of-the-art SLT algorithms, and believe that this benchmark can\neffectively support subsequent related research. Additionally, we propose a\nnovel RGB-Event sign language translation framework (i.e., M$^2$-SLT) that\nincorporates fine-grained micro-sign and coarse-grained macro-sign retrieval,\nachieving state-of-the-art results on the proposed dataset. Both the source\ncode and dataset will be released on https://github.com/Event-AHU/OpenESL.",
      "generated_abstract": "uage is an important language for people with hearing impairments.\nSign language translation is one of the most challenging tasks in language\ntranslation, due to the differences in the structure and content of the two\nlanguages. The current state-of-the-art methods are often based on deep\nlearning models, which have limitations in capturing the complex semantics of\nsign language. This paper proposes a new benchmark dataset and an\nextensible framework for sign language translation, designed to facilitate\nresearch in this area. The dataset consists of 152 videos of 150,000 frames\ncaptured from 281 different signs. We further introduce a new method based on\nthe Mask-RCNN architecture, which allows the system to predict the next frame\nbased on the current frame and the previous frame. This method is based on the\nintersection of the frames and events of the current and previous frames.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1564625850340136,
          "p": 0.27380952380952384,
          "f": 0.1991341945061001
        },
        "rouge-2": {
          "r": 0.03125,
          "p": 0.046875,
          "f": 0.037499995200000615
        },
        "rouge-l": {
          "r": 0.1360544217687075,
          "p": 0.23809523809523808,
          "f": 0.17316016853207414
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.11448v1",
      "true_abstract": "Gaussian processes (GPs) are flexible, probabilistic, non-parametric models\nwidely employed in various fields such as spatial statistics, time series\nanalysis, and machine learning. A drawback of Gaussian processes is their\ncomputational cost having $\\mathcal{O}(N^3)$ time and $\\mathcal{O}(N^2)$ memory\ncomplexity which makes them prohibitive for large datasets. Numerous\napproximation techniques have been proposed to address this limitation. In this\nwork, we systematically compare the accuracy of different Gaussian process\napproximations concerning marginal likelihood evaluation, parameter estimation,\nand prediction taking into account the time required to achieve a certain\naccuracy. We analyze this trade-off between accuracy and runtime on multiple\nsimulated and large-scale real-world datasets and find that Vecchia\napproximations consistently emerge as the most accurate in almost all\nexperiments. However, for certain real-world data sets, low-rank inducing\npoint-based methods, i.e., full-scale and modified predictive process\napproximations, can provide more accurate predictive distributions for\nextrapolation.",
      "generated_abstract": "t a rigorous analysis of the accuracy-runtime trade-off (ART)\nfor the Gaussian process (GP) and its extensions for spatial data. By\nintegrating a novel error analysis into the popular MMD test, we show that the\noverall error of the GP approximation of a data point is not a monotonic\nfunction of the dimension of the input space and the sampling rate of the\nestimator. This is the first rigorous analysis of the ART for GPs in the\ncontext of spatial data. We propose a new estimator, the spatial GP\napproximation (SPGPA) and a novel test, the spatial MMD test (SMT), which\nsignificantly improve the accuracy of the GP approximation for spatial data. We\nalso propose a novel kernel, the Gaussian process Gaussian kernel (GP-GK),\nwhich combines the accuracy of a GP with the efficiency of a Gaussian kernel. We\nshow that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16521739130434782,
          "p": 0.2835820895522388,
          "f": 0.20879120413899296
        },
        "rouge-2": {
          "r": 0.02097902097902098,
          "p": 0.02654867256637168,
          "f": 0.023437495068665586
        },
        "rouge-l": {
          "r": 0.1565217391304348,
          "p": 0.26865671641791045,
          "f": 0.19780219314998204
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.21306v1",
      "true_abstract": "Solar prosumers, residential electricity consumers equipped with photovoltaic\n(PV) systems and battery storage, are transforming electricity markets. Their\ninteractions with the transmission grid under varying tariff designs are not\nyet fully understood. We explore the influence of different pricing regimes on\nprosumer investment and dispatch decisions and their subsequent impact on the\ntransmission grid. Using an integrated modeling approach that combines two\nopen-source dispatch, investment and grid models, we simulate prosumage\nbehavior in Germany's electricity market under real-time pricing or\ntime-invariant pricing, as well as under zonal or nodal pricing. Our findings\nshow that zonal pricing favors prosumer investments, while time-invariant\npricing rather hinders it. In comparison, regional solar availability emerges\nas a larger driver for rooftop PV investments. The impact of prosumer\nstrategies on grid congestion remains limited within the scope of our\nmodel-setup, in which home batteries cannot be used for energy arbitrage.",
      "generated_abstract": "r explores the impact of solar prosumage (Solar PV + Energy Storage)\non the transmission grid. We use a stochastic electricity market model to\nsimulate the evolution of the transmission grid when an SSP is connected to it.\nWe first assess the impact of SSP's connection on the transmission grid's\ninvestment decisions and then estimate the transmission cost of the SSP's\nconnection using a multi-period game-theoretic model. Our results show that the\nprosumage SSP's connection leads to a significant increase in transmission\ninvestment, particularly in the early stages of grid integration. We also\nanalyze the impact of transmission costs and the impact of the prosumer's\nproduction on the transmission costs, demonstrating that the transmission\ncosts increase as the prosumer's production increases. Our findings provide\ninsights into the impact of prosumage S",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22935779816513763,
          "p": 0.3472222222222222,
          "f": 0.27624308913158946
        },
        "rouge-2": {
          "r": 0.05673758865248227,
          "p": 0.07547169811320754,
          "f": 0.06477732303561806
        },
        "rouge-l": {
          "r": 0.21100917431192662,
          "p": 0.3194444444444444,
          "f": 0.2541436416177773
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2412.03606v1",
      "true_abstract": "This paper aims to study the prediction of the bank stability index based on\nthe Time Series Transformer model. The bank stability index is an important\nindicator to measure the health status and risk resistance of financial\ninstitutions. Traditional prediction methods are difficult to adapt to complex\nmarket changes because they rely on single-dimensional macroeconomic data. This\npaper proposes a prediction framework based on the Time Series Transformer,\nwhich uses the self-attention mechanism of the model to capture the complex\ntemporal dependencies and nonlinear relationships in financial data. Through\nexperiments, we compare the model with LSTM, GRU, CNN, TCN and RNN-Transformer\nmodels. The experimental results show that the Time Series Transformer model\noutperforms other models in both mean square error (MSE) and mean absolute\nerror (MAE) evaluation indicators, showing strong prediction ability. This\nshows that the Time Series Transformer model can better handle multidimensional\ntime series data in bank stability prediction, providing new technical\napproaches and solutions for financial risk management.",
      "generated_abstract": "This paper presents a novel time series model that can predict the future\nrisk levels of banks using their past records. We apply the Transformer model,\nwhich is a state-of-the-art machine learning model, to analyze the historical\ntime series of banks in the United States. We also use the Transformer model to\nperform risk assessment and forecasting of banks using their historical time\nseries data. The results show that the Transformer model can accurately\npredict the future risk levels of banks based on their past records. Additionally,\nthe Transformer model can also provide an early warning signal for bank\nfailures, providing valuable insights for bank managers and regulators. This\npaper provides a comprehensive analysis of the Transformer model's capabilities\nin the financial industry and its potential applications for risk management and\nstrategic decision-making.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.28431372549019607,
          "p": 0.3815789473684211,
          "f": 0.325842691735892
        },
        "rouge-2": {
          "r": 0.0851063829787234,
          "p": 0.11214953271028037,
          "f": 0.09677418864236496
        },
        "rouge-l": {
          "r": 0.27450980392156865,
          "p": 0.3684210526315789,
          "f": 0.31460673667971223
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06572v1",
      "true_abstract": "Recent research has paid little attention to complex driving behaviors,\nnamely merging car-following and lane-changing behavior, and how lane-changing\naffects algorithms designed to model and control a car-following vehicle.\nDuring the merging behavior, the Follower Vehicle (FV) might significantly\ndiverge from typical car-following models. Thus, this paper aims to control the\nFV witnessing lane-changing behavior based on anticipation, perception,\npreparation, and relaxation states defined by a novel measurable human\nperception index. Data from human drivers are utilized to create a\nperception-based fuzzy controller for the behavior vehicle's route guidance,\ntaking into account the opacity of human driving judgments. We illustrate the\nefficacy of the established technique using simulated trials and data from\nactual drivers, focusing on the benefits of the increased comfort, safety, and\nuniformity of traffic flow and the decreased of wait time and motion sickness\nthis brings about.",
      "generated_abstract": "ence of autonomous vehicles (AVs) is transforming the road\ntransportation system, significantly affecting the traffic flow. A key challenge\nin AV-assisted traffic is to maintain the safety of all road users, including\nvehicles and pedestrians. To this end, intelligent control is necessary to\neffectively manage the interactions between AVs and human-driven vehicles. In\nthis paper, we present a novel approach for merging car-following and lane-\nchanging behavior, which integrates the concepts of intelligent control and\nvehicle dynamics. We propose a novel optimal control framework that utilizes\nthe dynamics of the AVs to adjust the lane trajectory of the human-driven\nvehicle. The proposed control strategy is designed to optimize the lane\ntrajectory while ensuring safety and traffic flow. Our experimental results\ndemonstrate the effectiveness of the proposed control",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18,
          "p": 0.22784810126582278,
          "f": 0.2011173135045724
        },
        "rouge-2": {
          "r": 0.043478260869565216,
          "p": 0.05084745762711865,
          "f": 0.046874995030518105
        },
        "rouge-l": {
          "r": 0.18,
          "p": 0.22784810126582278,
          "f": 0.2011173135045724
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2502.07806v1",
      "true_abstract": "The integration of Quantum Deep Learning (QDL) techniques into the landscape\nof financial risk analysis presents a promising avenue for innovation. This\nstudy introduces a framework for credit risk assessment in the banking sector,\ncombining quantum deep learning techniques with adaptive modeling for Row-Type\nDependent Predictive Analysis (RTDPA). By leveraging RTDPA, the proposed\napproach tailors predictive models to different loan categories, aiming to\nenhance the accuracy and efficiency of credit risk evaluation. While this work\nexplores the potential of integrating quantum methods with classical deep\nlearning for risk assessment, it focuses on the feasibility and performance of\nthis hybrid framework rather than claiming transformative industry-wide\nimpacts. The findings offer insights into how quantum techniques can complement\ntraditional financial analysis, paving the way for further advancements in\npredictive modeling for credit risk.",
      "generated_abstract": "sk assessment is a fundamental task in financial risk management,\nand it is essential to predict the probability of default for specific\nindividuals or companies. In this paper, we propose a novel quantum-assisted\ndeep learning framework for credit risk assessment, integrating hybrid quantum\nand classical neural network (QCNN) models to address the challenges of\ncomputational complexity and high-dimensionality. The hybrid QCNN model\nintegrates the benefits of both quantum-assisted and classical neural network\n(CNN) models, and it significantly improves the accuracy of predicting credit\nrisk. Additionally, the hybrid QCNN model uses a novel row-type dependent\npredictive analysis technique, which is capable of adapting to the characteristics\nof individual data. The proposed model is validated on a dataset of 3,106\nindividuals, which includes 1,559 bad debts",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.30434782608695654,
          "p": 0.3333333333333333,
          "f": 0.3181818131921489
        },
        "rouge-2": {
          "r": 0.064,
          "p": 0.06956521739130435,
          "f": 0.06666666167534761
        },
        "rouge-l": {
          "r": 0.2717391304347826,
          "p": 0.2976190476190476,
          "f": 0.2840909041012397
        }
      }
    },
    {
      "paper_id": "cs.MA.cs/GT/2503.10186v1",
      "true_abstract": "Beyond specific settings, many multi-agent learning algorithms fail to\nconverge to an equilibrium solution, and instead display complex,\nnon-stationary behaviours such as recurrent or chaotic orbits. In fact, recent\nliterature suggests that such complex behaviours are likely to occur when the\nnumber of agents increases. In this paper, we study Q-learning dynamics in\nnetwork polymatrix games where the network structure is drawn from classical\nrandom graph models. In particular, we focus on the Erdos-Renyi model, a\nwell-studied model for social networks, and the Stochastic Block model, which\ngeneralizes the above by accounting for community structures within the\nnetwork. In each setting, we establish sufficient conditions under which the\nagents' joint strategies converge to a unique equilibrium. We investigate how\nthis condition depends on the exploration rates, payoff matrices and,\ncrucially, the sparsity of the network. Finally, we validate our theoretical\nfindings through numerical simulations and demonstrate that convergence can be\nreliably achieved in many-agent systems, provided network sparsity is\ncontrolled.",
      "generated_abstract": "the convergence properties of multi-agent Q-learning dynamics in\nrandom networks. In particular, we investigate the case where the number of\nagents is large relative to the network size, and where agents' actions are\nrandomized according to a Gaussian process. We demonstrate that the\nconvergence of the Q-learning dynamics is driven by two factors: i) the\nexploration of the Q-function, and ii) the sparsity of the network. In\nparticular, we show that the convergence rate of the Q-learning dynamics in\nrandom networks depends on both factors. In the case of large network size, we\nfind that the convergence rate is dominated by i) for small exploration\nproportion, and ii) for large exploration proportion. In the case of large\nnetwork size, we find that the convergence rate is dominated by i) for small\nsparsity ratio, and ii) for large spars",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.256198347107438,
          "p": 0.5254237288135594,
          "f": 0.3444444400376544
        },
        "rouge-2": {
          "r": 0.10967741935483871,
          "p": 0.17894736842105263,
          "f": 0.13599999528800016
        },
        "rouge-l": {
          "r": 0.23140495867768596,
          "p": 0.4745762711864407,
          "f": 0.31111110670432107
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.physics/optics/2503.10359v1",
      "true_abstract": "In this study, we systematically explore the non-Hermitian skin effect (NHSE)\nand its associated complex-frequency detection in the context of a\nfrequency-dependent non-Hermitian Hamiltonian. This Hamiltonian arises from the\nself-energy correction of the subsystem and can be calculated exactly within\nour theoretical model, without the need for non-Hermitian approximations.\nAdditionally, complex frequency detection, which encompasses complex frequency\nexcitation, synthesis, and fingerprint, enables us to detect the physical\nresponses driven by complex frequency excitations. Our calculations reveal that\nboth complex frequency excitation and synthesis are sensitive to the\nnon-Hermitian approximation and are unable to characterize the presence or\nabscence of the NHSE. In contrast, the complex-frequency fingerprint\nsuccessfully detects the novel responses induced by the NHSE through the\nintroduction of a double-frequency Green's function. Our work paves the way for\na rigorous understanding of non-Hermitian physics in quantum systems and their\nexperimental verification through complex frequency-domain techniques.",
      "generated_abstract": "e a method to perform frequency detection in a subsystem consisting\nof two coupled quantum systems. The subsystem is driven by a continuous\nfrequency-modulated input signal, and the output is measured using a\nfrequency-resolved optical gating (FROG) technique. The subsystem consists of a\nquantum dot and a single-electron transistor (SET) as the light-harvesting\nelectrodes. The frequency detection is performed by measuring the time-of-flight\nof the FROG signal. We find that frequency detection is possible, even in the\npresence of a non-trivial time-reversal symmetry breaking (TRSB) phase, when\nthe subsystem is sufficiently close to a phase-insensitive quantum critical\npoint. This result provides a route to frequency detection in quantum\nsystems with TRSB phase, extending the existing method to a broad class of\nsystems",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16326530612244897,
          "p": 0.21621621621621623,
          "f": 0.18604650672525702
        },
        "rouge-2": {
          "r": 0.06521739130434782,
          "p": 0.08181818181818182,
          "f": 0.07258064022502636
        },
        "rouge-l": {
          "r": 0.16326530612244897,
          "p": 0.21621621621621623,
          "f": 0.18604650672525702
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2409.08728v1",
      "true_abstract": "We use a methodology based on a machine learning algorithm to quantify firms'\ncyber risks based on their disclosures and a dedicated cyber corpus. The model\ncan identify paragraphs related to determined cyber-threat types and\naccordingly attribute several related cyber scores to the firm. The cyber\nscores are unrelated to other firms' characteristics. Stocks with high cyber\nscores significantly outperform other stocks. The long-short cyber risk factors\nhave positive risk premia, are robust to all factors' benchmarks, and help\nprice returns. Furthermore, we suggest the market does not distinguish between\ndifferent types of cyber risks but instead views them as a single, aggregate\ncyber risk.",
      "generated_abstract": "r examines the interplay between cyber risk premia and their\nunderlying factors. Using a dataset that includes 1,856 cyber insurance\npurchase decisions, we find that cyber risk premia are positively related to\nthe severity of the data breach, the frequency of data breaches, and the\nrelationship between the breach and the organization's financial health.\nHowever, the cyber risk premia are also positively related to the size of the\norganization and the severity of the cyber attack. These results highlight the\nimportance of considering cyber risk factors when evaluating cyber insurance\nrisk premia. Our findings suggest that cyber risk premia may be more\nsensitive to cyber security events with a larger impact on the organization than\nto cyber events with a smaller impact. This suggests that cyber insurance\nprices may be more sensitive",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21621621621621623,
          "p": 0.23529411764705882,
          "f": 0.22535210768498326
        },
        "rouge-2": {
          "r": 0.04040404040404041,
          "p": 0.039603960396039604,
          "f": 0.039999995000500624
        },
        "rouge-l": {
          "r": 0.1891891891891892,
          "p": 0.20588235294117646,
          "f": 0.19718309360047623
        }
      }
    },
    {
      "paper_id": "math.DG.math/SG/2503.06570v1",
      "true_abstract": "In this paper, we propose a condition on the coefficients of a\ncohomology-valued power series, which we call ``asymptotically\nMittag-Leffler''. We show that if the $J$-function of a Fano manifold is\nasymptotically Mittag-Leffler, then it has the exponential growth as $t\\to\n+\\infty$. This provides an alternative method to compute the principal\nasymptotic class of a Fano manifold using the coefficients of $J$-function. We\nalso verify that the $J$-function of the projective space is asymptotically\nMittag-Leffler, and the property of having an asymptotically Mittag-Leffler\n$J$-function is preserved when taking product and hypersurface.",
      "generated_abstract": "In this paper, we investigate the asymptotic behaviour of the J-function of\na Fano manifold. The J-function is a function which measures the curvature\nof the K\\\"ahler-Einstein metric. The study of the asymptotic behaviour of the\nJ-function of a Fano manifold has been widely studied in the literature. For\nexample, the asymptotic behaviour of the J-function of a K3 surface has been\nstudied in [1",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21666666666666667,
          "p": 0.3939393939393939,
          "f": 0.27956988789455434
        },
        "rouge-2": {
          "r": 0.0875,
          "p": 0.1590909090909091,
          "f": 0.1129032212278878
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.36363636363636365,
          "f": 0.25806451155046833
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09972v1",
      "true_abstract": "It is known that, when $n$ is even, the number of permutations of\n$\\{1,2,\\dots,n\\}$ all of whose cycles have odd length equals the number of\nthose all of whose cycles have even length. Adin, Heged\\H{u}s and Roichman\nrecently found a surprising refinement of this identity. They showed that, for\nany fixed set $J$, the equality still holds when restricting to permutations\nwith descent set $J$ on one side, and permutations with ascent set $J$ on the\nother. Their proof uses generating functions for higher Lie characters. They\nalso deduce a version for odd $n$.\n  Here we give a bijective proof of their result. We first use known bijections\nto restate the identity in terms of multisets of necklaces, and then describe a\nnew weight-preserving bijection between words all of whose Lyndon factors have\nodd length and are distinct, and words all of whose Lyndon factors have even\nlength. We also show that the corresponding equality about Lyndon\nfactorizations has a short proof using generating functions.",
      "generated_abstract": "a bijection between descent sets of permutations with only even and\nonly odd cycles. The bijection maps a descent set $D$ of a permutation $\\sigma$ to\nthe set of its cycles of even length, $C_e(\\sigma)$, and that of its cycles\nof odd length, $C_o(\\sigma)$. We show that $D$ is the image of the map\n$C_e(\\sigma) \\to C_o(\\sigma)$ under the canonical map $C_e(\\sigma) \\to\n\\pi_1(S^1 \\times S^1)$; this map is the reduction map of the descent functor.\nWe then show that the map $C_e(\\sigma) \\to \\pi_1(S^1 \\times S^1)$ is an\nalgebraic involution, and that the bijection is inverse to the involution.\nFinally, we show that the bij",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21649484536082475,
          "p": 0.40384615384615385,
          "f": 0.2818791900869331
        },
        "rouge-2": {
          "r": 0.04225352112676056,
          "p": 0.06896551724137931,
          "f": 0.05240174201331062
        },
        "rouge-l": {
          "r": 0.20618556701030927,
          "p": 0.38461538461538464,
          "f": 0.26845637129498673
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2502.00877v1",
      "true_abstract": "This study investigates the inherently random structures of dry bulk shipping\nnetworks, often likened to a taxi service, and identifies the underlying trade\ndynamics that contribute to this randomness within individual cargo\nsub-networks. By analysing micro-level trade flow data from 2015 to 2023, we\nexplore the evolution of dry commodity networks, including grain, coal, and\niron ore, and suggest that the Giant Strongly Connected Components exhibit\nsmall-world phenomena, indicative of efficient bilateral trade. The significant\nheterogeneity of in-degree and out-degree within these sub-networks, primarily\ndriven by importing ports, underscores the complexity of their dynamics. Our\ntemporal analysis shows that while the Covid-19 pandemic profoundly impacted\nthe coal network, the Ukraine conflict significantly altered the grain network,\nresulting in changes in community structures. Notably, grain sub-networks\ndisplay periodic changes, suggesting distinct life cycles absent in coal and\niron ore networks. These findings illustrate that the randomness in dry bulk\nshipping networks is a reflection of real-world trade dynamics, providing\nvaluable insights for stakeholders in navigating and predicting network\nbehaviours.",
      "generated_abstract": "investigates the trading behavior of the global dry bulk shipping\nnetwork in 2005-2023, focusing on the effects of various variables such as the\nvolume of cargoes traded, the market share of each shipping line, and the\nvolume of cargoes traded per ship. The analysis reveals that shipping lines\nthat dominate the market share are more likely to participate in trades and\ntherefore experience higher trading volumes. Additionally, the research\nhighlights the importance of trading volume in influencing shipping lines'\nmarket shares, with the trend of increasing market share for larger shipping\nlines being more pronounced than the trend for smaller shipping lines. The\nanalysis also reveals that the level of market share of each shipping line\naffects the level of trading volumes. These findings are in line with\nprevious research, which emphasizes the importance of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1322314049586777,
          "p": 0.2222222222222222,
          "f": 0.16580310413058083
        },
        "rouge-2": {
          "r": 0.030864197530864196,
          "p": 0.04424778761061947,
          "f": 0.03636363152238081
        },
        "rouge-l": {
          "r": 0.12396694214876033,
          "p": 0.20833333333333334,
          "f": 0.1554404098300627
        }
      }
    },
    {
      "paper_id": "math.GT.math/GT/2503.04607v1",
      "true_abstract": "Heegaard splittings stratify 3-manifolds by complexity; only $S^3$ admits a\ngenus-zero splitting, and only $S^3$, $S^1 \\times S^2$, and lens spaces\n$L(p,q)$ admit genus-one splittings. In dimension four, the second author and\nJeffrey Meier proved that only a handful of simply-connected 4-manifolds have\ntrisection genus two or less, while Meier conjectured that if $X$ admits a\ngenus-three trisection, then $X$ is diffeomorphic to a spun lens space $S_p$ or\nits sibling $S_p'$, $S^4$, or a connected sum of copies of $\\pm \\mathbb{CP}^2$,\n$S^1 \\times S^3$, and $S^2 \\times S^2$. We prove Meier's conjecture in the case\nthat $X$ admits a weakly reducible genus-three trisection, where weak\nreducibility is a new idea adapted from Heegaard theory and is defined in terms\nof disjoint curves bounding compressing disks in various handlebodies. The\ntools and techniques used to prove the main theorem borrow heavily from\n3-manifold topology. Of independent interest, we give a trisection-diagrammatic\ndescription of 4-manifolds obtained by surgery on loops and spheres in other\n4-manifolds.",
      "generated_abstract": "We show that a compact, oriented, 3-dimensional manifold with a\nmanifold-with-boundary 3-trisection is standard if and only if its genus-three\ntrisection is weakly reducible. We also prove that any compact, oriented, 3-dimensional\nmanifold with a compact manifold-with-boundary 3-trisection is standard. We\nprovide several examples of manifolds with weakly reducible genus-three trisections\nthat are not standard.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12173913043478261,
          "p": 0.42424242424242425,
          "f": 0.1891891857240687
        },
        "rouge-2": {
          "r": 0.0189873417721519,
          "p": 0.06666666666666667,
          "f": 0.029556646795603308
        },
        "rouge-l": {
          "r": 0.09565217391304348,
          "p": 0.3333333333333333,
          "f": 0.14864864518352822
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SY/2503.07997v1",
      "true_abstract": "Autonomous stores leverage advanced sensing technologies to enable\ncashier-less shopping, real-time inventory tracking, and seamless customer\ninteractions. However, these systems face significant challenges, including\nocclusion in vision-based tracking, scalability of sensor deployment, theft\nprevention, and real-time data processing. To address these issues, researchers\nhave explored multi-modal sensing approaches, integrating computer vision,\nRFID, weight sensing, vibration-based detection, and LiDAR to enhance accuracy\nand efficiency. This survey provides a comprehensive review of sensing\ntechnologies used in autonomous retail environments, highlighting their\nstrengths, limitations, and integration strategies. We categorize existing\nsolutions across inventory tracking, environmental monitoring, people-tracking,\nand theft detection, discussing key challenges and emerging trends. Finally, we\noutline future directions for scalable, cost-efficient, and privacy-conscious\nautonomous store systems.",
      "generated_abstract": "ng demand for sustainable and smart retail environments necessitates\nthe development of innovative sensor-based solutions for retail management.\nExisting surveillance systems are often costly and impractical due to their\nhigh operating costs, while advanced sensing technologies such as smart\nclothing, drones, and artificial intelligence (AI) offer promising solutions.\nHowever, the integration of these technologies into retail environments poses\nchallenges, including security and privacy concerns, data quality issues, and\ncostly maintenance and upgrading requirements. To address these challenges,\nthis survey explores the latest technologies and challenges in retail\nautonomous sensing systems, highlighting their potential to transform retail\noperations. We provide an overview of existing sensor technologies, including\noptical sensors, thermal imaging, ultrasonic sensing, and robotics, and\ndiscuss",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.30851063829787234,
          "p": 0.3333333333333333,
          "f": 0.3204419839577547
        },
        "rouge-2": {
          "r": 0.05309734513274336,
          "p": 0.05263157894736842,
          "f": 0.05286343112344552
        },
        "rouge-l": {
          "r": 0.2872340425531915,
          "p": 0.3103448275862069,
          "f": 0.29834253644394254
        }
      }
    },
    {
      "paper_id": "math.QA.math/QA/2503.10327v1",
      "true_abstract": "Solutions to the quiver-theoretic quantum Yang-Baxter equation are associated\nwith structure categories and structure groupoids. We prove that the structure\ngroupoids of involutive non-degenerate solutions are Garside. This generalises\na well-known result about the structure groups of set-theoretic solutions, due\nto Chouraqui. We also construct involutive non-degenerate solutions from\nsuitable presented categories. We then investigate the case of solutions of\nprincipal homogeneous type. Finally, we present some examples of this new class\nof Garside groupoids.",
      "generated_abstract": "We construct a structure groupoid for the Yang-Baxter map of quiver algebras\nand its tensor-product. We prove that this structure groupoid is the\nhigher-dimensional analogue of the structure groupoid for the Yang-Baxter map\nof algebras introduced by N. Atiyah and P. Schlessinger. We also prove that the\nYang-Baxter map of quiver algebras and its tensor-product is a\nrepresentation-theoretic quantum analogue of the Yang-Baxter map of algebras\nand its tensor-product.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.43333333333333335,
          "f": 0.3170731660916122
        },
        "rouge-2": {
          "r": 0.07142857142857142,
          "p": 0.11627906976744186,
          "f": 0.08849557050669614
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.43333333333333335,
          "f": 0.3170731660916122
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.05022v2",
      "true_abstract": "Understanding firm conduct is crucial for industrial organization and\nantitrust policy. In this article, we develop a testing procedure based on the\nRivers and Vuong non-nested model selection framework. Unlike existing methods\nthat require estimating the demand and supply system, our approach compares the\nmodel fit of two first-stage price regressions. Through an extensive Monte\nCarlo study, we demonstrate that our test performs comparably to, or\noutperforms, existing methods in detecting collusion across various collusive\nscenarios. The results are robust to model misspecification, alternative\nfunctional forms for instruments, and data limitations. By simplifying the\ndiagnosis of firm behavior, our method provides an efficient tool for\nresearchers and regulators to assess industry conduct. Additionally, our\napproach offers a practical guideline for enhancing the strength of BLP-style\ninstruments in demand estimation: once collusion is detected, researchers are\nadvised to incorporate the product characteristics of colluding partners into\nown-firm instruments while excluding them from other-firm instruments.",
      "generated_abstract": "We introduce a novel instrumental variables (IV) approach to testing the\neffects of firm conduct on aggregate outcomes. Our approach uses a\nnon-parametric method to estimate the conditional mean of the variable of\ninterest given firm conduct. The IV approach provides an alternative to\nestimating the conditional mean of the variable of interest given firm\nconduct, which is often non-linear and not tractable. Our approach is based on\na structural equation model. We use the method to test the causal effect of\nfirm conduct on aggregate outcomes in the US manufacturing sector. Our\nresults suggest that firm conduct is positively associated with the growth of\nthe manufacturing sector. Our results are robust to alternative IV\nestimation methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22807017543859648,
          "p": 0.4,
          "f": 0.2905027886707657
        },
        "rouge-2": {
          "r": 0.06,
          "p": 0.09574468085106383,
          "f": 0.07377048706664906
        },
        "rouge-l": {
          "r": 0.21929824561403508,
          "p": 0.38461538461538464,
          "f": 0.27932960431322373
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2503.02611v1",
      "true_abstract": "Statistical integration of diverse data sources is an essential step in the\nbuilding of generalizable prediction tools, especially in precision health. The\ninvariant features model is a new paradigm for multi-source data integration\nwhich posits that a small number of covariates affect the outcome identically\nacross all possible environments. Existing methods for estimating invariant\neffects suffer from immense computational costs or only offer good statistical\nperformance under strict assumptions. In this work, we provide a general\nframework for estimation under the invariant features model that is\ncomputationally efficient and statistically flexible. We also provide a robust\nextension of our proposed method to protect against possibly corrupted or\nmisspecified data sources. We demonstrate the robust properties of our method\nvia simulations, and use it to build a transferable prediction model for end\nstage renal disease using electronic health records from the All of Us research\nprogram.",
      "generated_abstract": "linear models are widely used in machine learning and statistics\nfor modeling data with a common structure across various features. The\ninvariant linear models are characterized by a set of linear equations and a\ndata-dependent penalty parameter that defines the regularization strength.\nHowever, these models are sensitive to the regularization strength, making\nchoosing a suitable penalty parameter challenging. In this paper, we propose a\nrobust and fast method to estimate the optimal regularization parameter for the\ninvariant linear models. We propose a novel method to solve the problem of\nestimating the optimal regularization parameter for the invariant linear\nmodels. This method is based on a regularization of the original problem. The\nproposed method is based on a novel solution for the problem of estimating the\noptimal regularization parameter for the invariant linear models. We\ninvestigate the performance of the proposed method for several invariant linear\nmodels. We compare the proposed method with",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22115384615384615,
          "p": 0.32857142857142857,
          "f": 0.26436781128286435
        },
        "rouge-2": {
          "r": 0.04316546762589928,
          "p": 0.05555555555555555,
          "f": 0.04858299103017637
        },
        "rouge-l": {
          "r": 0.20192307692307693,
          "p": 0.3,
          "f": 0.24137930553573794
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2409.13957v1",
      "true_abstract": "One type of bond with the most implicit government guarantee is municipal\ninvestment bonds. In recent years, there have been an increasing number of\ndowngrades in the credit ratings of municipal bonds, which has led some people\nto question whether the implicit government guarantee may affect the\nobjectivity of the bond ratings? This paper uses text mining methods to mine\nrelevant policy documents related to municipal investment bond issuance, and\ncalculates the implicit guarantee strength of municipal investment bonds based\non the PMC index model. It further analyzes the impact of the implicit\nguarantee strength of municipal bonds on their credit evaluation. The study\nfound that the implicit government guarantee on municipal investment bonds does\nindeed help to raise the credit ratings assigned by credit rating agencies. The\nstudy found that, moreover, the government's implicit guarantee has a more\npronounced effect in boosting credit ratings in less developed western regions.",
      "generated_abstract": "y investigates the impact of implicit government guarantee on the\ncredit rating of municipal investment bonds (MIBs). The results show that the\nimpact of implicit government guarantee on MIBs' credit rating depends on the\nratio of the expected value of interest and expected value of principal. If the\nratio is larger than 2, the impact of implicit government guarantee on MIBs'\ncredit rating is positive; if the ratio is less than 2, the impact is negative.\nThe study also explores the impact of the ratio of expected value of interest\nand expected value of principal on MIBs' credit rating. If the ratio is larger\nthan 1.7, the impact is positive; if the ratio is less than 1.7, the impact is\nnegative. Finally, the study examines the relationship between the ratio of\nexpected value of interest and expected value of principal and the impact of\nimplicit",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.4,
          "f": 0.2666666622222223
        },
        "rouge-2": {
          "r": 0.112,
          "p": 0.19718309859154928,
          "f": 0.14285713823667237
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.4,
          "f": 0.2666666622222223
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2501.16391v1",
      "true_abstract": "Significant differences in protein structures hinder the generalization of\nexisting drug-target interaction (DTI) models, which often rely heavily on\npre-learned binding principles or detailed annotations. In contrast, BioBridge\ndesigns an Inductive-Associative pipeline inspired by the workflow of\nscientists who base their accumulated expertise on drawing insights into novel\ndrug-target pairs from weakly related references. BioBridge predicts novel\ndrug-target interactions using limited sequence data, incorporating multi-level\nencoders with adversarial training to accumulate transferable binding\nprinciples. On these principles basis, BioBridge employs a dynamic prototype\nmeta-learning framework to associate insights from weakly related annotations,\nenabling robust predictions for previously unseen drug-target pairs. Extensive\nexperiments demonstrate that BioBridge surpasses existing models, especially\nfor unseen proteins. Notably, when only homologous protein binding data is\navailable, BioBridge proves effective for virtual screening of the epidermal\ngrowth factor receptor and adenosine receptor, underscoring its potential in\ndrug discovery.",
      "generated_abstract": "ransferable binding principles (ITBPs) are a promising approach to\ninducting binding specificity and predicting novel interactions between\ndrug-target pairs. However, ITBPs typically fail to capture the nuanced\ndynamics of binding interactions, leading to challenges in modeling complex\nbiological processes such as drug resistance and drug-drug interactions. In\nthis work, we propose a novel framework to integrate induced ITBPs with\nconformational dynamics to model the association and dissociation of\ndrug-target complexes. Our method leverages the ITBPs' ability to capture\nindividual binding events and the conformational dynamics of binding sites to\nincorporate binding specificity into the association process. We evaluate the\nmethod on 23 ITBPs, which capture binding specificity in diverse contexts such\nas conformational changes during binding, intersubunit interactions,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16964285714285715,
          "p": 0.24675324675324675,
          "f": 0.20105819622966892
        },
        "rouge-2": {
          "r": 0.022058823529411766,
          "p": 0.027522935779816515,
          "f": 0.02448979097909304
        },
        "rouge-l": {
          "r": 0.15178571428571427,
          "p": 0.22077922077922077,
          "f": 0.17989417506564778
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/AI/2503.10529v1",
      "true_abstract": "3D Multimodal Large Language Models (MLLMs) have recently made substantial\nadvancements. However, their potential remains untapped, primarily due to the\nlimited quantity and suboptimal quality of 3D datasets. Current approaches\nattempt to transfer knowledge from 2D MLLMs to expand 3D instruction data, but\nstill face modality and domain gaps. To this end, we introduce PiSA-Engine\n(Point-Self-Augmented-Engine), a new framework for generating instruction\npoint-language datasets enriched with 3D spatial semantics. We observe that\nexisting 3D MLLMs offer a comprehensive understanding of point clouds for\nannotation, while 2D MLLMs excel at cross-validation by providing complementary\ninformation. By integrating holistic 2D and 3D insights from off-the-shelf\nMLLMs, PiSA-Engine enables a continuous cycle of high-quality data generation.\nWe select PointLLM as the baseline and adopt this co-evolution training\nframework to develop an enhanced 3D MLLM, termed PointLLM-PiSA. Additionally,\nwe identify limitations in previous 3D benchmarks, which often feature coarse\nlanguage captions and insufficient category diversity, resulting in inaccurate\nevaluations. To address this gap, we further introduce PiSA-Bench, a\ncomprehensive 3D benchmark covering six key aspects with detailed and diverse\nlabels. Experimental results demonstrate PointLLM-PiSA's state-of-the-art\nperformance in zero-shot 3D object captioning and generative classification on\nour PiSA-Bench, achieving significant improvements of 46.45% (+8.33%) and\n63.75% (+16.25%), respectively. We will release the code, datasets, and\nbenchmark.",
      "generated_abstract": "t a novel approach for training large models for 3D object\nrecognition using only synthetic data. Our approach uses a self-augmented data\nengine (SADE) that generates synthetic 3D point clouds from real 3D objects.\nThese synthetic data are used to train a large model, which can then be used\nfor 3D object recognition. In our approach, we propose a new loss function for\nself-augmented data that ensures the synthetic data capture the object's\nshape, color, and texture. We evaluate our approach on two large models\ntrained on synthetic data, and demonstrate that our approach achieves\nsignificantly improved performance compared to training on synthetic data alone.\nIn addition, we propose a new training strategy that allows the use of\nsynthetic data in the training process, which significantly reduces the\ntraining time of large models. Our approach provides a scalable",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.147239263803681,
          "p": 0.3116883116883117,
          "f": 0.19999999564201398
        },
        "rouge-2": {
          "r": 0.014150943396226415,
          "p": 0.025,
          "f": 0.018072284540573105
        },
        "rouge-l": {
          "r": 0.147239263803681,
          "p": 0.3116883116883117,
          "f": 0.19999999564201398
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2410.02846v1",
      "true_abstract": "We introduce a novel machine learning model for credit risk by combining\ntree-boosting with a latent spatio-temporal Gaussian process model accounting\nfor frailty correlation. This allows for modeling non-linearities and\ninteractions among predictor variables in a flexible data-driven manner and for\naccounting for spatio-temporal variation that is not explained by observable\npredictor variables. We also show how estimation and prediction can be done in\na computationally efficient manner. In an application to a large U.S. mortgage\ncredit risk data set, we find that both predictive default probabilities for\nindividual loans and predictive loan portfolio loss distributions obtained with\nour novel approach are more accurate compared to conventional independent\nlinear hazard models and also linear spatio-temporal models. Using\ninterpretability tools for machine learning models, we find that the likely\nreasons for this outperformance are strong interaction and non-linear effects\nin the predictor variables and the presence of large spatio-temporal frailty\neffects.",
      "generated_abstract": "r proposes a spatio-temporal machine learning model to forecast\nmortgage defaults. The model is built upon a two-stage process: first, the\nmortgage portfolio is created, which is a spatio-temporal matrix of default\nprobabilities of each mortgage loan. Second, the mortgage default risk is\npredicted using this mortgage portfolio. The model is tested on a data set of\n260,000 mortgage loans in the United States. The model achieves an AUC-ROC of\n0.903, a Cohen's d of 0.539, and an F-measure of 0.771. The mortgage portfolio\nis created by applying a machine learning model to the 10-year U.S. Treasury\nyields and 5-year U.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24242424242424243,
          "p": 0.3870967741935484,
          "f": 0.29813664122680456
        },
        "rouge-2": {
          "r": 0.035211267605633804,
          "p": 0.054945054945054944,
          "f": 0.042918450175174136
        },
        "rouge-l": {
          "r": 0.24242424242424243,
          "p": 0.3870967741935484,
          "f": 0.29813664122680456
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/AP/2502.16520v2",
      "true_abstract": "The increasing complexity of supply chains and the rising costs associated\nwith defective or substandard goods (bad goods) highlight the urgent need for\nadvanced predictive methodologies to mitigate risks and enhance operational\nefficiency. This research presents a novel framework that integrates Time\nSeries ARIMA (AutoRegressive Integrated Moving Average) models with a\nproprietary formula specifically designed to calculate bad goods after time\nseries forecasting. By leveraging historical data patterns, including sales,\nreturns, and capacity, the model forecasts potential quality failures, enabling\nproactive decision-making. ARIMA is employed to capture temporal trends in time\nseries data, while the newly developed formula quantifies the likelihood and\nimpact of defects with greater precision. Experimental results, validated on a\ndataset spanning 2022-2024 for Organic Beer-G 1 Liter, demonstrate that the\nproposed method outperforms traditional statistical models, such as Exponential\nSmoothing and Holt-Winters, in both prediction accuracy and risk evaluation.\nThis study advances the field of predictive analytics by bridging time series\nforecasting, ARIMA, and risk management in supply chain quality control,\noffering a scalable and practical solution for minimizing losses due to bad\ngoods.",
      "generated_abstract": "In the context of online retailing, the risk assessment of the delivery\ngoods is a crucial issue for the customer. This study focuses on the risk\nassessment of the delivery goods using time series data. To do this, the\nARIMA model was chosen as the best model for the prediction of the delivery\ngoods. The study examines the effect of the seasonal component, the\nseasonal component, and the trend component of the ARIMA model on the\npredictive ability of the model. The results show that the model performs\nwell in the prediction of the delivery goods, and it can be used to predict\nthe delivery good in the future. The results of the study show that the model\ncan predict the delivery goods in the future with an accuracy of 80%.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18382352941176472,
          "p": 0.3968253968253968,
          "f": 0.2512562770798718
        },
        "rouge-2": {
          "r": 0.028901734104046242,
          "p": 0.05263157894736842,
          "f": 0.03731342825935677
        },
        "rouge-l": {
          "r": 0.16911764705882354,
          "p": 0.36507936507936506,
          "f": 0.23115577456730899
        }
      }
    },
    {
      "paper_id": "math.OC.math/OC/2503.10405v1",
      "true_abstract": "In this paper we aim to construct piecewise-linear (PWL) approximations for\nfunctions of multiple variables and to build compact mixed-integer linear\nprogramming (MILP) formulations to represent the resulting PWL function. On the\none hand, we describe a simple heuristic to iteratively construct a\ntriangulation with a small number of triangles, while decreasing the error of\nthe piecewise-linear approximation. On the other hand, we extend known\ntechniques for modeling PWLs in MILPs more efficiently than state-of-the-art\nmethods permit. The crux of our method is that the MILP model is a result of\nsolving some hard combinatorial optimization problems, for which we present\nheuristic algorithms. The effectiveness of our techniques is demonstrated by a\nseries of computational experiments including a short-term hydropower\nscheduling problem",
      "generated_abstract": "t a new method for modeling a system of coupled ordinary\ndifferential equations (ODEs) using piecewise-linear functions. The method\nprovides a rigorous mathematical treatment of a system of ODEs that is\nanalytically tractable and computationally efficient. We illustrate our method by\nconsidering the problem of modeling the motion of a free-flying rigid body in a\nnonlinear magnetic field, using a linear model of the magnetic field. The\nlinear model is often used in experimental studies of magnetic fields, but the\nlinearity does not provide an accurate description of the magnetic field. We\nshow that the modeling of the magnetic field using piecewise-linear functions\nprovides a rigorous mathematical treatment of the problem and enables an\naccurate description of the magnetic field. We further demonstrate that the\nmethod is applicable to nonlinear problems, including a model of the motion of\na rigid body",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23863636363636365,
          "p": 0.3088235294117647,
          "f": 0.26923076431295206
        },
        "rouge-2": {
          "r": 0.059322033898305086,
          "p": 0.06542056074766354,
          "f": 0.06222221723417324
        },
        "rouge-l": {
          "r": 0.2159090909090909,
          "p": 0.27941176470588236,
          "f": 0.24358973867192646
        }
      }
    },
    {
      "paper_id": "cs.CC.stat/TH/2502.15024v1",
      "true_abstract": "We investigate implications of the (extended) low-degree conjecture (recently\nformalized in [MW23]) in the context of the symmetric stochastic block model.\nAssuming the conjecture holds, we establish that no polynomial-time algorithm\ncan weakly recover community labels below the Kesten-Stigum (KS) threshold. In\nparticular, we rule out polynomial-time estimators that, with constant\nprobability, achieve correlation with the true communities that is\nsignificantly better than random. Whereas, above the KS threshold,\npolynomial-time algorithms are known to achieve constant correlation with the\ntrue communities with high probability[Mas14,AS15].\n  To our knowledge, we provide the first rigorous evidence for the sharp\ntransition in recovery rate for polynomial-time algorithms at the KS threshold.\nNotably, under a stronger version of the low-degree conjecture, our lower bound\nremains valid even when the number of blocks diverges. Furthermore, our results\nprovide evidence of a computational-to-statistical gap in learning the\nparameters of stochastic block models.\n  In contrast to prior work, which either (i) rules out polynomial-time\nalgorithms for hypothesis testing with 1-o(1) success probability [Hopkins18,\nBBK+21a] under the low-degree conjecture, or (ii) rules out low-degree\npolynomials for learning the edge connection probability matrix [LG23], our\napproach provides stronger lower bounds on the recovery and learning problem.\n  Our proof combines low-degree lower bounds from [Hopkins18, BBK+21a] with\ngraph splitting and cross-validation techniques. In order to rule out general\nrecovery algorithms, we employ the correlation preserving projection method\ndeveloped in [HS17].",
      "generated_abstract": "the sharpness of the computational threshold for the low degree\nconjecture in the stochastic block model (SBM). The computational threshold\nmeasures the size of a minimum-weight perfect matching in a random SBM\ninstance. The conjecture states that the computational threshold is\n$\\Theta(n^{1/5})$, which was conjectured by the first author and\nGolovachov \\cite{golovachov_computational_2023} and independently by\nHuang and Yang \\cite{huang_sharp_2023}. Our proof is a generalization of the\nmethod used by the first author and Golovachov \\cite{golovachov_computational_2023}\nand Huang and Yang \\cite{huang_sharp_2023} to the SBM. Our method avoids the\nneed to construct a random graph with a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15602836879432624,
          "p": 0.4230769230769231,
          "f": 0.22797927067464901
        },
        "rouge-2": {
          "r": 0.0380952380952381,
          "p": 0.10126582278481013,
          "f": 0.05536331782665466
        },
        "rouge-l": {
          "r": 0.14184397163120568,
          "p": 0.38461538461538464,
          "f": 0.20725388207361276
        }
      }
    },
    {
      "paper_id": "math.GN.math/GN/2502.08506v1",
      "true_abstract": "We introduce a two-parameter modification of the cofinality invariant of\nideals. This allows us to include the interaction of a pair of ideals in the\nstudy of base-like structures. We find the values (cardinal numbers or\nwell-known cardinal invariants) of the invariant for pairs of some critical\nideals on $\\omega$. We also dichotomously divide pairs of known ideals on the\nreal line based on whether their relative cofinality is trivial or uncountable.\nFinally, we also study the relative cofinality of maximal ideals.",
      "generated_abstract": "We study the relative cofinality of an ideal $I$ in a seminormed algebra $A$\nby means of a characterization of the relative cofinality of $I$ in $A/I$.\n  For a subalgebra $B\\subseteq A$, the relative cofinality of $I$ in $A/I$ is\ndefined as the smallest number $n$ such that every element of $B$ is $n$-cofinal\nin $I$.\n  The main result of this paper is the characterization of the relative cofinality\nof $I$ in $A/I$ as the supremum of the set of all $n\\in\\omega$ for which\n$A/I$ satisfies $I\\leq_n\\overline{I}$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19607843137254902,
          "p": 0.2127659574468085,
          "f": 0.2040816276613912
        },
        "rouge-2": {
          "r": 0.07894736842105263,
          "p": 0.09090909090909091,
          "f": 0.0845070372783181
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.19148936170212766,
          "f": 0.18367346439608512
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.03084v1",
      "true_abstract": "We present simple to implement Wald-type statistics that deliver a general\nnonparametric inference theory for linear restrictions on varying coefficients\nin a range of spatial autoregressive models. Our theory covers error dependence\nof a general form, allows for a degree of misspecification robustness via\nnonparametric spatial weights and permits inference on both varying regression\nand spatial coefficients. One application of our method finds evidence for\nconstant returns to scale in the production function of the Chinese nonmetal\nmineral industry, while another finds a nonlinear impact of the distance to the\nemployment center on housing prices in Boston. A simulation study confirms that\nour tests perform well in finite-samples.",
      "generated_abstract": "inference on the varying coefficient model, where a time-invariant\nvariable is treated as a time-varying fixed effect and a time-varying\nintercept. The variable is modeled as a spatial autoregressive model. We\nintroduce a new estimator that is consistent and asymptotically normal for a\ntime-invariant fixed effect and asymptotically linear for a time-varying\nintercept. We apply the estimator to the 2017-2020 United States Census\nBureau's American Community Survey. The estimator is applied to two different\nsettings. In the first setting, the fixed effect is time-varying and the\nintercept is fixed. In the second setting, the fixed effect is fixed and the\nintercept is time-varying. We compare our estimator with a standard\nprocedure and with a procedure that uses the standard estimator",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18666666666666668,
          "p": 0.24561403508771928,
          "f": 0.21212120721418745
        },
        "rouge-2": {
          "r": 0.0380952380952381,
          "p": 0.041237113402061855,
          "f": 0.03960395540388261
        },
        "rouge-l": {
          "r": 0.18666666666666668,
          "p": 0.24561403508771928,
          "f": 0.21212120721418745
        }
      }
    },
    {
      "paper_id": "quant-ph.math/IT/2503.09012v1",
      "true_abstract": "The thought experiment of Maxwell's demon highlights the effect of side\ninformation in thermodynamics. In this paper, we present an axiomatic treatment\nof a quantum Maxwell's demon, by introducing a resource-theoretic framework of\nquantum thermodynamics in the presence of quantum side information. Under\nminimal operational assumptions that capture the demon's behaviour, we derive\nthe one-shot work costs of preparing, as well as erasing, a thermodynamic\nsystem whose coupling with the demon's mind is described by a bipartite quantum\nstate. With trivial Hamiltonians, these work costs are precisely captured by\nthe smoothed conditional min- and max-entropies, respectively, thus providing\noperational interpretations for these one-shot information-theoretic quantities\nin microscopic thermodynamics. An immediate, information-theoretic implication\nof our results is an affirmative proof of the conjectured maximality of the\nconditional max-entropy among all axiomatically plausible conditional\nentropies, complementing the recently established minimality of the conditional\nmin-entropy. We then generalize our main results to the setting with nontrivial\nHamiltonians, wherein the work costs of preparation and erasure are captured by\na generalized type of mutual information. Finally, we present a macroscopic\nsecond law of thermodynamics in the presence of quantum side information, in\nterms of a conditional version of the Helmholtz free energy. Our results extend\nthe conceptual connection between thermodynamics and quantum information theory\nby refining the axiomatic common ground between the two theories and revealing\nfundamental insights of each theory in light of the other.",
      "generated_abstract": "ntext of quantum information processing, preparation and\nerror correction can be performed with the aid of quantum side information.\nHowever, the work costs associated with these operations are often not\nspecified. We investigate the work costs in the context of the preparation of\na single qubit in a two-level system, which is then erased by applying an\n$X$-operator. We show that the work costs associated with preparation and\nerasure are the sum of two independent costs, which are determined by the\ncorresponding unitary operations. These costs depend on the initial state of\nthe qubit and the strength of the quantum side information. Additionally, we\ndemonstrate that the work costs of quantum preparation and erasure can be\nexpressed as the sum of two independent costs, which are determined by the\ncorresponding unitary operations. These costs depend on the initial state of\nthe qubit and the strength of the quantum",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18518518518518517,
          "p": 0.390625,
          "f": 0.2512562770435091
        },
        "rouge-2": {
          "r": 0.07142857142857142,
          "p": 0.15151515151515152,
          "f": 0.0970873742859838
        },
        "rouge-l": {
          "r": 0.15555555555555556,
          "p": 0.328125,
          "f": 0.21105527201838345
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.03333v1",
      "true_abstract": "Dynamic networks models describe temporal interactions between social actors,\nand as such have been used to describe financial fraudulent transactions,\ndispersion of destructive invasive species across the globe, and the spread of\nfake news. An important question in all of these examples is what are the\ncausal drivers underlying these processes. Current network models are\nexclusively descriptive and based on correlative structures.\n  In this paper we propose a causal extension of dynamic network modelling. In\nparticular, we prove that the causal model satisfies a set of population\nconditions that uniquely identifies the causal drivers. The empirical analogue\nof these conditions provide a consistent causal discovery algorithm, which\ndistinguishes it from other inferential approaches. Crucially, data from a\nsingle environment is sufficient. We apply the method in an analysis of bike\nsharing data in Washington D.C. in July 2023.",
      "generated_abstract": "etworks are ubiquitous in social and biomedical sciences, yet their\ncreation and evolution remain poorly understood. To address this, we introduce\nthe causal drivers framework, which unifies causal discovery and dynamic\nnetwork analysis. The framework leverages a novel causal graphical modeling\nframework, which is based on the causal graph of the observed data, to identify\ncausal drivers of dynamic networks. To quantify the strength of causal\ninfluence, we develop a novel measure based on the causal graph of the observed\ndata and the causal graph of the underlying dynamic network. By leveraging the\ncausal graph of the observed data, causal drivers can be identified for\nsubnetworks, which are of particular importance in social networks. We apply\nour causal drivers framework to a dataset of social ties in the human genome\nand to a gene-disease network. Our results reve",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24509803921568626,
          "p": 0.33783783783783783,
          "f": 0.2840909042174587
        },
        "rouge-2": {
          "r": 0.05970149253731343,
          "p": 0.07207207207207207,
          "f": 0.06530611749304494
        },
        "rouge-l": {
          "r": 0.22549019607843138,
          "p": 0.3108108108108108,
          "f": 0.261363631490186
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.09075v1",
      "true_abstract": "The pinching-antenna system (PASS) introduces new degrees of freedom (DoFs)\nfor physical layer security (PLS) through pinching beamforming. In this paper,\na couple of scenarios for secure beamforming for PASS are studied. 1) For the\ncase with a single legitimate user (Bob) and a single eavesdropper (Eve), a\nclosed-form expression for the optimal baseband beamformer is derived. On this\nbasis, a gradient-based method is proposed to optimize the activated positions\nof pinching antennas (PAs). 2) For the case with multiple Bobs and multiple\nEves, a fractional programming (FP)-based block coordinate descent (BCD)\nalgorithm, termed FP-BCD, is proposed for optimizing the weighted secrecy\nsum-rate (WSSR). Specifically, a closed-form baseband beamformer is obtained\nvia Lagrange multiplier method. Furthermore, owing to the non-convex objective\nfunction exhibiting numerous stationary points, a low-complexity\none-dimensional search is used to find a high-quality solution of the PAs'\nactivated locations. Numerical results are provided to demonstrate that: i) All\nproposed algorithms achieve stable convergence within a few iterations, ii)\nacross all considered power ranges, the FP-BCD algorithm outperforms baseline\nmethods using zero-forcing (ZF) and maximal-ratio transmission (MRT)\nbeamforming in terms of the WSSR, and iii) PASS achieves a significantly higher\nsecrecy rate than traditional fixed-antenna systems.",
      "generated_abstract": "antenna systems (PASs) are a promising alternative to conventional\ntransmit and receive antennas for improving the sensitivity of a wireless\ncommunications system. They have been proposed in the literature as a\nconceptual alternative to conventional antenna systems, and they have been\ndemonstrated in hardware in a number of applications. However, the security of\nPASs has not been studied in the literature. This paper addresses the issue of\nphysical layer security (PLS) for PASs. The main contribution of this paper is\nto provide a formal definition of PLS for PASs, and to identify the security\nthreats and security solutions for PASs. We also propose a new security\nframework for PASs that provides a comprehensive approach to PLS for PASs.\nFurthermore, we present an implementation of a Pinching-Antenna System (P",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.28205128205128205,
          "f": 0.19909501805696045
        },
        "rouge-2": {
          "r": 0.015957446808510637,
          "p": 0.02586206896551724,
          "f": 0.019736837385735197
        },
        "rouge-l": {
          "r": 0.13286713286713286,
          "p": 0.24358974358974358,
          "f": 0.17194569678999214
        }
      }
    },
    {
      "paper_id": "eess.IV.cs/HC/2503.09885v1",
      "true_abstract": "Analyzing CT scans, MRIs and X-rays is pivotal in diagnosing and treating\ndiseases. However, detecting and identifying abnormalities from such medical\nimages is a time-intensive process that requires expert analysis and is prone\nto interobserver variability. To mitigate such issues, machine learning-based\nmodels have been introduced to automate and significantly reduce the cost of\nimage segmentation. Despite significant advances in medical image analysis in\nrecent years, many of the latest models are never applied in clinical settings\nbecause state-of-the-art models do not easily interface with existing medical\nimage viewers. To address these limitations, we propose QuickDraw, an\nopen-source framework for medical image visualization and analysis that allows\nusers to upload DICOM images and run off-the-shelf models to generate 3D\nsegmentation masks. In addition, our tool allows users to edit, export, and\nevaluate segmentation masks to iteratively improve state-of-the-art models\nthrough active learning. In this paper, we detail the design of our tool and\npresent survey results that highlight the usability of our software. Notably,\nwe find that QuickDraw reduces the time to manually segment a CT scan from four\nhours to six minutes and reduces machine learning-assisted segmentation time by\n10\\% compared to prior work. Our code and documentation are available at\nhttps://github.com/qd-seg/quickdraw",
      "generated_abstract": "l image analysis, segmentation is a fundamental task that\nparticularly challenges the development of models with high performance and\nefficiency. While pre-trained models have achieved impressive performance on\nlarge-scale medical image segmentation tasks, they often have limited\ninterpretability and require complex training procedures. This paper proposes\nQuickDraw, a new model that addresses the challenges of segmentation and\ninterpretability in medical imaging. First, we introduce a novel pre-training\nmethod to improve the generalization ability of the model. Then, we introduce\nQuickDraw, a model that integrates the semantic segmentation and\ninterpretation capabilities of DINO-ViT-B/35 and the fast inference speed of\nMVT-S/35. We also propose a fast active learning strategy to improve the\nsegmentation performance. We further conduct ablation studies on the\npre-training method",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14705882352941177,
          "p": 0.2631578947368421,
          "f": 0.18867924068351738
        },
        "rouge-2": {
          "r": 0.035897435897435895,
          "p": 0.061946902654867256,
          "f": 0.04545454080894802
        },
        "rouge-l": {
          "r": 0.14705882352941177,
          "p": 0.2631578947368421,
          "f": 0.18867924068351738
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2502.08144v2",
      "true_abstract": "In recent years, the dominance of machine learning in stock market\nforecasting has been evident. While these models have shown decreasing\nprediction errors, their robustness across different datasets has been a\nconcern. A successful stock market prediction model minimizes prediction errors\nand showcases robustness across various data sets, indicating superior\nforecasting performance. This study introduces a novel multiple lag order\nprobabilistic model based on trend encoding (TeMoP) that enhances stock market\npredictions through a probabilistic approach. Results across different stock\nindexes from nine countries demonstrate that the TeMoP outperforms the\nstate-of-the-art machine learning models in predicting accuracy and\nstabilization.",
      "generated_abstract": "market is a complex and dynamic system, characterized by high\nvariability and volatility. Traditional machine learning methods struggle to\ncapture such features, and non-parametric approaches often struggle to\ngeneralize to unseen datasets. In this work, we propose a novel approach based\non probabilistic multi-order modeling. We introduce a trend-encoded model\narchitecture that encodes the trend direction into the model, enabling\nnon-linear processing of the input. We also introduce an innovative\nmulti-order feature representation that captures the correlation structure of\ntime series data. We evaluate the model on three large-scale financial time\nseries datasets: the MSCI World Index, the S&P 500 Index, and the FTSE 100\nIndex. The results demonstrate that the proposed approach achieves\nstate-of-the-art performance, outperforming traditional machine learning\nmethods",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.273972602739726,
          "p": 0.2247191011235955,
          "f": 0.24691357529568672
        },
        "rouge-2": {
          "r": 0.05434782608695652,
          "p": 0.04310344827586207,
          "f": 0.04807691814349164
        },
        "rouge-l": {
          "r": 0.2602739726027397,
          "p": 0.21348314606741572,
          "f": 0.23456789628334104
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.q-bio/CB/2408.07551v1",
      "true_abstract": "In the recently proposed Graph Vertex Model (GVM), cellular rearrangements\nare implemented as local graph transformations of the cell aggregate,\nrepresented by a knowledge graph [1]. This study extends GVM to incorporate\ncell division, a critical biological process involved in morphogenesis,\nhomeostasis, and disease progression. Like cellular rearrangements, cell\ndivision in GVM begins by identifying a subgraph of nodes and links, involved\nin the division, by matching suitable graph patterns or templates within the\nfull knowledge graph. The matched subgraph is then transformed to incorporate\ntopological changes within the knowledge graph, caused by the division event.\nImportantly, when this transformation is applied to a polygon in a 2D tiling,\nit performs the transformation, required to divide a polygon, indicating that\nthe 3D graph transformation is general and applicable also to 2D vertex models.\nOur extension of GVM enables the study of the dynamics of growing cell\naggregates in 3D to offer new insights into developmental processes and cancer\ngrowth.",
      "generated_abstract": "ell aggregates are ubiquitous in nature and have been observed in\nthe context of tumor growth, where they are believed to play a crucial role in\ncell proliferation. To better understand the growth of these aggregates,\nresearchers have developed mathematical models, which can be used to understand\ntheir behavior and predict how they will behave. One of these models is the\npolyhedral division model, which describes the growth of an aggregation by\nsplitting the aggregate into two parts and dividing the new parts into smaller\nparts. Polyhedral division models have been used to model cellular growth in\nnumerical simulations, but there has not been a formal mathematical analysis of\nthese models, which can provide insights into the behavior of the aggregates.\nIn this study, we explore the graph polyhedral division model in the context of\ngrowing cell aggregates. We first derive the formal definition",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21568627450980393,
          "p": 0.25287356321839083,
          "f": 0.23280422783572702
        },
        "rouge-2": {
          "r": 0.039473684210526314,
          "p": 0.04580152671755725,
          "f": 0.04240282188265613
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.20689655172413793,
          "f": 0.1904761855076847
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/AP/2503.06837v1",
      "true_abstract": "This paper investigates a robust empirical Bayes correction for Bayesian\nmodeling. We show the application of the model on income distribution. Income\nshock includes temporal and permanent shocks. We aim to eliminate temporal\nshock and permanent shock using two-step local empirical correction method. Our\nresults show that only 6.7% of the observed income shocks were permanent shock,\nand the posterior (permanent) mean weekly income was reduced from the observed\nincome 415 pounds to 202 pounds for the United Kingdom using the Living Costs\nand Food Survey in 2021-2022. Keywords: Empirical Bayes correction; Outliers;\nBayesian modeling",
      "generated_abstract": "modeling is a crucial tool for researchers in various fields. In\nsuch settings, it is common for researchers to consider multiple models that are\nestimated using Bayes' theorem. In this paper, we focus on local\nempirical Bayes (LEB) corrections for Bayesian modeling, which are based on\nlocal approximations of the posterior distribution. In this paper, we propose\na new local empirical Bayes correction for a parametric model that is\napproximated by a Gaussian process. This correction is based on the\nempirical covariance matrix of the model, which is a direct consequence of the\nGaussian process representation. We also propose an extension of this\nlocal-empirical-Bayes correction to account for the uncertainty in the\nestimated covariance matrix. Additionally, we propose a new local empirical\nBayes correction that is based on the empirical cov",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3076923076923077,
          "p": 0.2898550724637681,
          "f": 0.2985074576910226
        },
        "rouge-2": {
          "r": 0.08888888888888889,
          "p": 0.0761904761904762,
          "f": 0.08205127708086815
        },
        "rouge-l": {
          "r": 0.3076923076923077,
          "p": 0.2898550724637681,
          "f": 0.2985074576910226
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.10445v1",
      "true_abstract": "Reducing the spread of misinformation is challenging. AI-based fact\nverification systems offer a promising solution by addressing the high costs\nand slow pace of traditional fact-checking. However, the problem of how to\neffectively communicate the results to users remains unsolved. Warning labels\nmay seem an easy solution, but they fail to account for fuzzy misinformation\nthat is not entirely fake. Additionally, users' limited attention spans and\nsocial media information should be taken into account while designing the\npresentation. The online experiment (n = 537) investigates the impact of\nsources and granularity on users' perception of information veracity and the\nsystem's usefulness and trustworthiness. Findings show that fine-grained\nindicators enhance nuanced opinions, information awareness, and the intention\nto use fact-checking systems. Source differences had minimal impact on opinions\nand perceptions, except for informativeness. Qualitative findings suggest the\nproposed indicators promote critical thinking. We discuss implications for\ndesigning concise, user-friendly AI fact-checking feedback.",
      "generated_abstract": "asing use of social media platforms has raised concerns about the\neffectiveness of the warning messages provided by platform-based systems.\nThese warnings, which may be provided as a part of the social media platform\nfunction, are designed to inform users of potential risks associated with their\ncontent and provide guidance on how to curate their content. However,\nexisting research on the efficacy of warning messages on social media is\nrestricted to relatively small datasets and limited to English-speaking\ncountries. This paper introduces the first study of warning messages on\nEnglish-speaking social media platforms, focusing on how warnings are used and\ntheir perceived effectiveness. By analyzing 116,000 posts from 102 users on\nTwitter, 41,000 posts from 326 users on Facebook, and 10,000 posts from 700\nusers on",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15789473684210525,
          "p": 0.22784810126582278,
          "f": 0.18652849257376047
        },
        "rouge-2": {
          "r": 0.013422818791946308,
          "p": 0.018018018018018018,
          "f": 0.015384610491421676
        },
        "rouge-l": {
          "r": 0.14035087719298245,
          "p": 0.20253164556962025,
          "f": 0.16580310397272424
        }
      }
    },
    {
      "paper_id": "eess.SY.stat/TH/2503.03328v1",
      "true_abstract": "We address the problem of safety verification for nonlinear stochastic\nsystems, specifically the task of certifying that system trajectories remain\nwithin a safe set with high probability. To tackle this challenge, we adopt a\nset-erosion strategy, which decouples the effects of stochastic disturbances\nfrom deterministic dynamics. This approach converts the stochastic safety\nverification problem on a safe set into a deterministic safety verification\nproblem on an eroded subset of the safe set. The success of this strategy\nhinges on the depth of erosion, which is determined by a probabilistic tube\nthat bounds the deviation of stochastic trajectories from their corresponding\ndeterministic trajectories. Our main contribution is the establishment of a\ntight bound for the probabilistic tube of nonlinear stochastic systems. To\nobtain a probabilistic bound for stochastic trajectories, we adopt a\nmartingale-based approach. The core innovation lies in the design of a novel\nenergy function associated with the averaged moment generating function, which\nforms an affine martingale, a generalization of the traditional c-martingale.\nUsing this energy function, we derive a precise bound for the probabilistic\ntube. Furthermore, we enhance this bound by incorporating the union-bound\ninequality for strictly contractive dynamics. By integrating the derived\nprobabilistic tubes into the set-erosion strategy, we demonstrate that the\nsafety verification problem for nonlinear stochastic systems can be reduced to\na deterministic safety verification problem. Our theoretical results are\nvalidated through applications in reachability-based safety verification and\nsafe controller synthesis, accompanied by several numerical examples that\nillustrate their effectiveness.",
      "generated_abstract": "near stochastic system (NSS) is a modeling framework to analyze\nnonlinear systems with stochastic dynamics. In this paper, we focus on the\nsafety verification of NSSs, where a safety constraint is imposed on the\nsystem. Under this constraint, the system dynamics are assumed to be\nwell-behaved. We then develop a novel approach for the safety verification of\nNSSs based on the probabilistic tube method, which can be applied to nonlinear\nstochastic systems of various types. This approach is effective for analyzing\nthe safety of NSSs that are not linear and the safety of NSSs with uncertainties\nand system dynamics with different types. We show that the probabilistic\ntube method is applicable to analyzing safety of NSSs with a single nonlinear\ndynamics. We also apply the probabilistic tube method to analyze safety of\nNSSs",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2283464566929134,
          "p": 0.4461538461538462,
          "f": 0.3020833288547093
        },
        "rouge-2": {
          "r": 0.05660377358490566,
          "p": 0.11009174311926606,
          "f": 0.07476635065498229
        },
        "rouge-l": {
          "r": 0.2204724409448819,
          "p": 0.4307692307692308,
          "f": 0.29166666218804255
        }
      }
    },
    {
      "paper_id": "cs.DM.cs/DM/2503.07208v1",
      "true_abstract": "In the Subset Feedback Arc Set in Tournaments, Subset-FAST problem we are\ngiven as input a tournament $T$ with a vertex set $V(T)$ and an arc set $A(T)$,\nalong with a terminal set $S \\subseteq V(T)$, and an integer $ k$. The\nobjective is to determine whether there exists a set $ F \\subseteq A(T) $ with\n$|F| \\leq k$ such that the resulting graph $T-F $ contains no cycle that\nincludes any vertex of $S$. When $S=V(T)$ this is the classic Feedback Arc Set\nin Tournaments (FAST) problem. We obtain the first polynomial kernel for this\nproblem parameterized by the solution size. More precisely, we obtain an\nalgorithm that, given an input instance $(T, S, k)$, produces an equivalent\ninstance $(T',S',k')$ with $k'\\leq k$ and $V(T')=O(k^2)$.\n  It was known that FAST admits a simple quadratic vertex kernel and a\nnon-trivial linear vertex kernel. However, no such kernel was previously known\nfor Subset-FAST. Our kernel employs variants of the most well-known reduction\nrules for FAST and introduces two new reduction rules to identify irrelevant\nvertices. As a result of our kernelization, we also obtain the first\nsub-exponential time FPT algorithm for Subset-FAST.",
      "generated_abstract": "We introduce a quadratic vertex kernel, which is a new polynomial-time\nalgorithm for subset-FAST. We prove that the kernel is a quadratic vertex\nkernel with a polynomial kernel factor, and then we construct a subexponential\nalgorithm based on the kernel. Our algorithm is the first subexponential\nalgorithm for subset-FAST. We also provide an efficient algorithm for the\nsubexponential kernel, which is an exponential factor shorter than our\nalgorithm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16806722689075632,
          "p": 0.5263157894736842,
          "f": 0.25477706639458
        },
        "rouge-2": {
          "r": 0.04419889502762431,
          "p": 0.14285714285714285,
          "f": 0.06751054491409872
        },
        "rouge-l": {
          "r": 0.15966386554621848,
          "p": 0.5,
          "f": 0.2420382128913952
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.04091v1",
      "true_abstract": "Federated Learning (FL) is a widely adopted privacy-preserving distributed\nlearning framework, yet its generalization performance remains less explored\ncompared to centralized learning. In FL, the generalization error consists of\ntwo components: the out-of-sample gap, which measures the gap between the\nempirical and true risk for participating clients, and the participation gap,\nwhich quantifies the risk difference between participating and\nnon-participating clients. In this work, we apply an information-theoretic\nanalysis via the conditional mutual information (CMI) framework to study FL's\ntwo-level generalization. Beyond the traditional supersample-based CMI\nframework, we introduce a superclient construction to accommodate the two-level\ngeneralization setting in FL. We derive multiple CMI-based bounds, including\nhypothesis-based CMI bounds, illustrating how privacy constraints in FL can\nimply generalization guarantees. Furthermore, we propose fast-rate evaluated\nCMI bounds that recover the best-known convergence rate for two-level FL\ngeneralization in the small empirical risk regime. For specific FL model\naggregation strategies and structured loss functions, we refine our bounds to\nachieve improved convergence rates with respect to the number of participating\nclients. Empirical evaluations confirm that our evaluated CMI bounds are\nnon-vacuous and accurately capture the generalization behavior of FL\nalgorithms.",
      "generated_abstract": "learning (FL) is a promising paradigm for training and\noptimizing machine learning models across distributed data, leveraging the\nprivacy of the clients to improve overall accuracy. This approach introduces\nnon-trivial challenges, especially in the context of heterogeneous clients\nwith varying data quality and computational capabilities. In this paper, we\nintroduce a novel mutual information-based framework to assess the generalization\ncapabilities of FL, emphasizing the importance of both local and global\ninformation in accounting for the heterogeneity of clients. Specifically, we\npropose a conditional mutual information (CMI) measure that considers the\neffect of local data quality on global model performance, while also\naccounting for the heterogeneity of clients through a generalized information\nmechanism. We validate the proposed framework through extensive experiments on\nreal-world datasets. Our results demonstrate that CMI enhances the\ngeneralization capabilities",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24,
          "p": 0.3448275862068966,
          "f": 0.2830188630851727
        },
        "rouge-2": {
          "r": 0.06629834254143646,
          "p": 0.0975609756097561,
          "f": 0.07894736360305604
        },
        "rouge-l": {
          "r": 0.208,
          "p": 0.2988505747126437,
          "f": 0.24528301402856895
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.03629v3",
      "true_abstract": "Traffic simulation is essential for autonomous vehicle (AV) development,\nenabling comprehensive safety evaluation across diverse driving conditions.\nHowever, traditional rule-based simulators struggle to capture complex human\ninteractions, while data-driven approaches often fail to maintain long-term\nbehavioral realism or generate diverse safety-critical events. To address these\nchallenges, we propose TeraSim, an open-source, high-fidelity traffic\nsimulation platform designed to uncover unknown unsafe events and efficiently\nestimate AV statistical performance metrics, such as crash rates. TeraSim is\ndesigned for seamless integration with third-party physics simulators and\nstandalone AV stacks, to construct a complete AV simulation system.\nExperimental results demonstrate its effectiveness in generating diverse\nsafety-critical events involving both static and dynamic agents, identifying\nhidden deficiencies in AV systems, and enabling statistical performance\nevaluation. These findings highlight TeraSim's potential as a practical tool\nfor AV safety assessment, benefiting researchers, developers, and policymakers.\nThe code is available at https://github.com/mcity/TeraSim.",
      "generated_abstract": "e Simulation (GenSim) has emerged as a powerful technique to\ngenerate new realistic scenes and scenes with unknown unsafe events (UUEs),\nallowing developers to better understand the behavior of their autonomous\nvehicles (AVs) in scenarios where they are not possible to test in the real\nworld. However, existing methods often require a large number of samples from\nthe scene to be simulated, which is computationally and time-consuming.\nGenerative adversarial networks (GANs) have demonstrated their ability to\ngenerate high-quality scenes with UUEs, but their use in this context has\nraised privacy concerns. In this paper, we propose a novel approach to address\nthese challenges. Our method, called TeraSim, generates a diverse set of UUEs\nfor AVs, including both known and unknown UUEs, while preserving the\noriginality of the generated scenes. Our",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20353982300884957,
          "p": 0.23232323232323232,
          "f": 0.2169811270972767
        },
        "rouge-2": {
          "r": 0.03546099290780142,
          "p": 0.03937007874015748,
          "f": 0.03731342784946603
        },
        "rouge-l": {
          "r": 0.168141592920354,
          "p": 0.1919191919191919,
          "f": 0.179245278040673
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.10611v1",
      "true_abstract": "We provide a full characterization of geodesic completeness for spaces of\nconfigurations of landmarks with smooth Riemannian metrics that satisfy a\nrotational and translation invariance and which are induced from metrics on\nsubgroups of the diffeomorphism group for the shape domain. These spaces are\nwidely used for applications in shape analysis, for example, for measuring\nshape changes in medical imaging and morphometrics in biology. For statistics\nof such data to be well-defined, it is imperative to know if geodesics exist\nfor all times. We extend previously known sufficient conditions for geodesic\ncompleteness based on the regularity of the metric to give a full\ncharacterization for smooth Riemannian metrics with a rotational and\ntranslation invariance by means of an integrability criterion that involves\nonly the behavior of the cometric kernel as landmarks approach collision. We\nfurther use the integrability criterion for geodesic completeness and previous\nwork on stochastic completeness to construct a family of Riemannian landmark\nmanifolds that are geodesically complete but stochastically incomplete.",
      "generated_abstract": "Landmark space is a metric space with a single geodesic metric. This\ncharacterization provides a geometric description of geodesic completeness\nwithin the class of geodesic metric spaces. In particular, we show that\ngeodesic completeness is preserved under the action of isometries. This\ncharacterization provides a geometric characterization of geodesic completeness\nin the class of geodesic metric spaces. In particular, we show that geodesic\ncompleteness is preserved under the action of isometries.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12244897959183673,
          "p": 0.41379310344827586,
          "f": 0.1889763744286689
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.09523809523809523,
          "f": 0.04210525971412771
        },
        "rouge-l": {
          "r": 0.10204081632653061,
          "p": 0.3448275862068966,
          "f": 0.15748031143654295
        }
      }
    },
    {
      "paper_id": "cs.CV.q-bio/CB/2502.05459v1",
      "true_abstract": "White blood cells (WBC) are important parts of our immune system, and they\nprotect our body against infections by eliminating viruses, bacteria, parasites\nand fungi. The number of WBC types and the total number of WBCs provide\nimportant information about our health status. A traditional method,\nconvolutional neural networks (CNN), a deep learning architecture, can classify\nthe blood cell from a part of an object and perform object recognition. Various\nCNN models exhibit potential; however, their development often involves ad-hoc\nprocesses that neglect unnecessary layers, leading to issues with unbalanced\ndatasets and insufficient data augmentation. To address these challenges, we\npropose a novel ensemble approach that integrates three CNN architectures, each\nuniquely configured with different dropout and max-pooling layer settings to\nenhance feature learning. This ensemble model, named DCENWCNet, effectively\nbalances the bias-variance trade-off. When evaluated on the widely recognized\nRabbin-WBC dataset, our model outperforms existing state-of-the-art networks,\nachieving highest mean accuracy. Additionally, it demonstrates superior\nperformance in precision, recall, F1-score, and Area Under the ROC Curve (AUC)\nacross all categories. To delve deeper into the interpretability of\nclassifiers, we employ reliable post-hoc explanation techniques, including\nLocal Interpretable Model-Agnostic Explanations (LIME). These methods\napproximate the behavior of a black-box model by elucidating the relationships\nbetween feature values and predictions. Interpretable results enable users to\ncomprehend and validate the model's predictions, thereby increasing their\nconfidence in the automated diagnosis.",
      "generated_abstract": "od cells (WBCs) are the most important cells in the human body.\nEarly detection of WBCs is crucial for diagnosing diseases and monitoring\nhealth conditions. However, accurate classification of WBCs remains challenging\ndue to their diverse morphologies and complex shapes. This paper presents\nDCENWCNet, a deep convolutional neural network (DCNN) ensemble network designed\nto address the limitations of single-layer CNNs. The DCENWCNet employs a\nmulti-layer ensemble architecture to classify WBCs based on their morphological\nfeatures. In the first layer, a CNN model is used to extract features from\nWBCs. In the second and third layers, a multi-layer LIME-based classifier is\napplied to improve the classification accuracy. The ensemble network is\ntrained using a dataset consisting of 350 WBC images. The experimental results\nshow",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16574585635359115,
          "p": 0.3448275862068966,
          "f": 0.22388059263004015
        },
        "rouge-2": {
          "r": 0.017699115044247787,
          "p": 0.03389830508474576,
          "f": 0.02325580944632321
        },
        "rouge-l": {
          "r": 0.143646408839779,
          "p": 0.2988505747126437,
          "f": 0.19402984636138346
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/OT/2501.10482v1",
      "true_abstract": "Random fuzzy variables join the modeling of the impreciseness (due to their\n``fuzzy part'') and randomness. Statistical samples of such objects are widely\nused, and their direct, numerically effective generation is therefore\nnecessary. Usually, these samples consist of triangular or trapezoidal fuzzy\nnumbers. In this paper, we describe theoretical results and simulation\nalgorithms for another family of fuzzy numbers -- LR fuzzy numbers with\ninterval-valued cores. Starting from a simulation perspective on the piecewise\nlinear LR fuzzy numbers with the interval-valued cores, their limiting behavior\nis then considered. This leads us to the numerically efficient algorithm for\nsimulating a sample consisting of such fuzzy values.",
      "generated_abstract": "This paper investigates the use of a random fuzzy interval to approximate a\nrandom LR interval. The aim is to determine the probability that a given\nrandom fuzzy interval lies within a given LR interval. The paper presents an\nalgorithm for the computation of the random fuzzy interval. The algorithm is\nbased on the use of the discrete probability mass function (PDF) of the fuzzy\ninterval, and the random variables of the LR interval. This approach is used to\ncompute the probability that the random fuzzy interval lies within the LR\ninterval. The paper presents numerical results to illustrate the use of the\nalgorithm and to assess its performance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1891891891891892,
          "p": 0.3111111111111111,
          "f": 0.23529411294400118
        },
        "rouge-2": {
          "r": 0.030612244897959183,
          "p": 0.03896103896103896,
          "f": 0.034285709357715
        },
        "rouge-l": {
          "r": 0.17567567567567569,
          "p": 0.28888888888888886,
          "f": 0.2184873902549256
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2501.07508v1",
      "true_abstract": "This paper applies deep reinforcement learning (DRL) to optimize liquidity\nprovisioning in Uniswap v3, a decentralized finance (DeFi) protocol\nimplementing an automated market maker (AMM) model with concentrated liquidity.\nWe model the liquidity provision task as a Markov Decision Process (MDP) and\ntrain an active liquidity provider (LP) agent using the Proximal Policy\nOptimization (PPO) algorithm. The agent dynamically adjusts liquidity positions\nby using information about price dynamics to balance fee maximization and\nimpermanent loss mitigation. We use a rolling window approach for training and\ntesting, reflecting realistic market conditions and regime shifts. This study\ncompares the data-driven performance of the DRL-based strategy against common\nheuristics adopted by small retail LP actors that do not systematically modify\ntheir liquidity positions. By promoting more efficient liquidity management,\nthis work aims to make DeFi markets more accessible and inclusive for a broader\nrange of participants. Through a data-driven approach to liquidity management,\nthis work seeks to contribute to the ongoing development of more efficient and\nuser-friendly DeFi markets.",
      "generated_abstract": "tralized finance (DeFi) ecosystem is transforming the way that\ndecentralized applications (dApps) and crypto assets interact, opening up new\ninnovative investment opportunities. However, the adoption of DeFi is\nconstrained by the limited liquidity available to users, which can be\nsignificantly reduced due to the high cost of liquidity provisioning (LP). In\nthis paper, we propose a Deep Reinforcement Learning (DRL)-based approach that\noptimizes the provisioning of LP to reduce LP cost, thereby improving the\naccessibility of DeFi. The proposed method consists of three modules:\n(1) A DeFi-centric Graph Neural Network (GNN) is used to capture the complex\nrelationships between DeFi assets, dApps, and users, enabling efficient\nprovisioning of LP. (2) A multi-step planning mechanism",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15966386554621848,
          "p": 0.21839080459770116,
          "f": 0.18446601453812814
        },
        "rouge-2": {
          "r": 0.018867924528301886,
          "p": 0.02727272727272727,
          "f": 0.022304827879659976
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.19540229885057472,
          "f": 0.1650485388099728
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.06046v1",
      "true_abstract": "Televised debates between presidential candidates are often regarded as the\nexemplar of persuasive communication. Yet, recent evidence from Le Pennec and\nPons (2023) indicates that they may not sway voters as strongly as popular\nbelief suggests. We revisit their findings through the lens of the persuasion\nrate and introduce a robust framework that does not require exogenous\ntreatment, parallel trends, or credible instruments. Instead, we leverage\nplausible monotonicity assumptions to partially identify the persuasion rate\nand related parameters. Our results reaffirm that the sharp upper bounds on the\npersuasive effects of TV debates remain modest.",
      "generated_abstract": "e whether TV debate exposure moderates the effect of\nthought-informing interventions, a key component of experimental designs for\nevidence-based policy. Existing studies have employed various monotonicity\nassumptions to model the impact of the intervention on the target variable.\nHowever, these monotonicity assumptions have not been consistently applied in\nthe debate context. Using data from the 2017 general election in the United\nStates, we develop a new monotonicity assumption that is both consistent and\ntestable. We show that the impact of an intervention that increases the\ntarget variable from 0 to 1 is bounded by the impact of the intervention that\nincreases the target variable from 0.5 to 1.6, across a wide range of\nassumptions. We empirically test the impact of the debates on the target\nvariable and find that the debate has a substantial",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19480519480519481,
          "p": 0.19230769230769232,
          "f": 0.1935483820969824
        },
        "rouge-2": {
          "r": 0.054945054945054944,
          "p": 0.045871559633027525,
          "f": 0.04999999504050049
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.1794871794871795,
          "f": 0.18064515629053085
        }
      }
    },
    {
      "paper_id": "math.PR.q-bio/SC/2406.12493v1",
      "true_abstract": "We prove a Large Deviation Principle for Piecewise Deterministic Markov\nProcesses (PDMPs). This is an asymptotic estimate for the probability of a\ntrajectory in the large size limit. Explicit Euler-Lagrange equations are\ndetermined for computing optimal first-hitting-time trajectories. The results\nare applied to a model of stochastic calcium dynamics. It is widely conjectured\nthat the mechanism of calcium puff generation is a multiscale process: with\nmicroscopic stochastic fluctuations in the opening and closing of individual\nchannels generating cell-wide waves via the diffusion of calcium and other\nsignaling molecules. We model this system as a PDMP, with $N \\gg 1$ stochastic\ncalcium channels that are coupled via the ambient calcium concentration. We\nemploy the Large Deviations theory to estimate the probability of cell-wide\ncalcium waves being produced through microscopic stochasticity.",
      "generated_abstract": "We study the large deviations of a class of stochastic processes associated\nwith a piecewise-deterministic Markov process (PDMP) with piecewise-deterministic\ntransitions. Our results show that the PDMP is in the large deviations regime\nwhen the number of steps is large. We consider the case of an unbounded\nenvironment and a general number of steps. The PDMP is modeled by an Ornstein\nUhlenbeck process with a multiplicative noise, and the stochastic differential\nequation is written in the language of piecewise deterministic processes. We\nalso consider the case of a finite environment and a general number of steps,\nwhere we derive the large deviations of the stochastic differential equation\nassociated with the PDMP. The results show that the stochastic differential\nequation is in the large deviations regime when the number of steps is large.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17857142857142858,
          "p": 0.2777777777777778,
          "f": 0.2173912995841211
        },
        "rouge-2": {
          "r": 0.04918032786885246,
          "p": 0.06818181818181818,
          "f": 0.05714285227392332
        },
        "rouge-l": {
          "r": 0.17857142857142858,
          "p": 0.2777777777777778,
          "f": 0.2173912995841211
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.01179v1",
      "true_abstract": "Chemical reaction network theory provides powerful tools for rigorously\nunderstanding chemical reactions and the dynamical systems and differential\nequations that represent them. A frequent issue with mathematical analyses of\nthese networks is the reliance on explicit parameter values which in many cases\ncannot be determined experimentally. This can make analyzing a dynamical system\ninfeasible, particularly when the size of the system is large. One approach is\nto analyze subnetworks of the full network and use the results for a full\nanalysis.\n  Our focus is on the equilibria of reaction networks. Gr\\\"obner basis\ncomputation is a useful approach for solving the polynomial equations which\ncorrespond to equilibria of a dynamical system. We identify a class of networks\nfor which Gr\\\"obner basis computations of subnetworks can be used to\nreconstruct the more expensive Gr\\\"obner basis computation of the whole\nnetwork. We compliment this result with tools to determine if a steady state\ncan exist, and if so, how many.",
      "generated_abstract": "We propose a new framework for the decomposition of systems of linear\nreaction equations, with applications to the identification of steady states and\nthe computation of their values. We introduce a generalized algebra of\nreactions, allowing for the definition of a variety of algebraic operations,\nsuch as composition, conjugation, and algebra-preserving transformations. We\nalso introduce a class of algebraic objects that generalize the concept of\nalgebraic solution, which is a generalization of the concept of solution in\ndifferential equations. We use this framework to analyze the stability of the\nsolutions of linear systems of reaction equations, and to determine the\nexistence and number of steady states of such systems. In particular, we show\nthat the existence of steady states is determined by the existence of algebraic\nsolutions of the linear system, and we provide an algebraic method for\ndetermining the number of steady states.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26881720430107525,
          "p": 0.352112676056338,
          "f": 0.304878043870464
        },
        "rouge-2": {
          "r": 0.07432432432432433,
          "p": 0.09166666666666666,
          "f": 0.08208954729338412
        },
        "rouge-l": {
          "r": 0.23655913978494625,
          "p": 0.30985915492957744,
          "f": 0.2682926780168055
        }
      }
    },
    {
      "paper_id": "math.OC.math/OC/2503.10005v1",
      "true_abstract": "Training deep neural networks is challenging. To accelerate training and\nenhance performance, we propose PadamP, a novel optimization algorithm. PadamP\nis derived by applying the adaptive estimation of the p-th power of the\nsecond-order moments under scale invariance, enhancing projection adaptability\nby modifying the projection discrimination condition. It is integrated into\nAdam-type algorithms, accelerating training, boosting performance, and\nimproving generalization in deep learning. Combining projected gradient\nbenefits with adaptive moment estimation, PadamP tackles unconstrained\nnon-convex problems. Convergence for the non-convex case is analyzed, focusing\non the decoupling of first-order moment estimation coefficients and\nsecond-order moment estimation coefficients. Unlike prior work relying on , our\nproof generalizes the convergence theorem, enhancing practicality. Experiments\nusing VGG-16 and ResNet-18 on CIFAR-10 and CIFAR-100 show PadamP's\neffectiveness, with notable performance on CIFAR-10/100, especially for VGG-16.\nThe results demonstrate that PadamP outperforms existing algorithms in terms of\nconvergence speed and generalization ability, making it a valuable addition to\nthe field of deep learning optimization.",
      "generated_abstract": "We propose an adaptive moment estimation (AME) optimization algorithm for\nDeep Neural Networks (DNNs) based on projection gradient. We first introduce\nthe AME optimization algorithm and its derivatives. Then, we reformulate the\noptimization problem into a constrained minimization problem with a penalty\nterm, which is solved by the ADMM algorithm. The ADMM algorithm is\nimplemented in the framework of the FPGA. We then show how the AME\noptimization algorithm can be used to train DNNs using the FPGA. The\noptimization algorithm is used to optimize the hyperparameters, such as the\nlearning rate, momentum, and weight decay. The training results using the FPGA\nare compared with those using the CPU. We show that the FPGA implementation\nreduces training time by a factor of 10.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.358974358974359,
          "f": 0.294736837265374
        },
        "rouge-2": {
          "r": 0.025974025974025976,
          "p": 0.03669724770642202,
          "f": 0.03041824609695172
        },
        "rouge-l": {
          "r": 0.24107142857142858,
          "p": 0.34615384615384615,
          "f": 0.2842105214759003
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/ST/2502.15726v1",
      "true_abstract": "The marketing departments of financial institutions strive to craft products\nand services that cater to the diverse needs of businesses of all sizes.\nHowever, it is evident upon analysis that larger corporations often receive a\nmore substantial portion of available funds. This disparity arises from the\nrelative ease of assessing the risk of default and bankruptcy in these more\nprominent companies. Historically, risk analysis studies have focused on data\nfrom publicly traded or stock exchange-listed companies, leaving a gap in\nknowledge about small and medium-sized enterprises (SMEs). Addressing this gap,\nthis study introduces a method for evaluating SMEs by generating images for\nprocessing via a convolutional neural network (CNN). To this end, more than\n10,000 images, one for each company in the sample, were created to identify\nscenarios in which the CNN can operate with higher assertiveness and reduced\ntraining error probability. The findings demonstrate a significant predictive\ncapacity, achieving 97.8% accuracy, when a substantial number of images are\nutilized. Moreover, the image creation method paves the way for potential\napplications of this technique in various sectors and for different analytical\npurposes.",
      "generated_abstract": "The bankruptcy analysis is one of the most important tasks in the bankruptcy\nregion. This study presents a novel bankruptcy analysis model based on convolutional\nneural networks (CNN). The CNN model has been developed to capture the\nrelationship between the various variables of bankruptcy and the outcome\nvariables. The CNN model is trained and tested using the bankruptcy dataset.\nThe CNN model achieves an accuracy of 89.7%, and the average F1 score is\n0.95. The CNN model is an effective and efficient method for bankruptcy\nanalysis, and it can be used to predict the outcome of bankruptcy cases.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.3770491803278688,
          "f": 0.2311557746430646
        },
        "rouge-2": {
          "r": 0.016483516483516484,
          "p": 0.034482758620689655,
          "f": 0.022304828337364913
        },
        "rouge-l": {
          "r": 0.15217391304347827,
          "p": 0.3442622950819672,
          "f": 0.21105527213050185
        }
      }
    },
    {
      "paper_id": "physics.atom-ph.physics/atom-ph/2503.07161v1",
      "true_abstract": "We show that atomic antimatter spectroscopy can be used to search for new\nbosons that carry spin-dependent exotic forces between antifermions. A\ncomparison of a recent precise measurement of the hyperfine splitting of the\n$1$S and $2$S electronic levels of antihydrogen and bound-state quantum\nelectrodynamics theory yields the first tests of positron-antiproton exotic\ninteractions, constraining the dimensionless coupling strengths $g_pg_p$,\n$g_Vg_V$ and $g_Ag_A$, corresponding to the exchange of a pseudoscalar\n(axionlike), vector, or axial-vector boson, respectively. We also discuss new\ntests of CPT invariance with exotic spin-dependent and spin-independent\ninteractions involving antimatter.",
      "generated_abstract": "The discovery of the Higgs boson in 2012 has brought the Standard Model of\ngauge interactions to a new level of sophistication, allowing the prediction of\na particle that is a half-brother of the Higgs boson, and a neutral particle\nthat is a half-sister to the Higgs. This discovery has also opened the door to\nthe possibility of the existence of new interactions between matter and\nantimatter, which can be tested through their effects on the properties of\nthe Higgs and the neutral particle. This paper discusses the search for\nexotic interactions between antimatter in the context of the Standard Model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2463768115942029,
          "p": 0.3148148148148148,
          "f": 0.27642275930200283
        },
        "rouge-2": {
          "r": 0.056818181818181816,
          "p": 0.058823529411764705,
          "f": 0.057803463209596485
        },
        "rouge-l": {
          "r": 0.18840579710144928,
          "p": 0.24074074074074073,
          "f": 0.21138210889549883
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.08953v1",
      "true_abstract": "Digital twin (DT) has emerged as a powerful tool to facilitate monitoring,\ncontrol, and other decision-making tasks in real-world engineering systems.\nOnline update methods have been proposed to update DT models. Considering the\ndegradation behavior in the system lifecycle, these methods fail to enable DT\nmodels to predict the system responses affected by the system degradation over\ntime. To alleviate this problem, degradation models of measurable parameters\nhave been integrated into DT construction. However, identifying the degradation\nparameters relies on prior knowledge of the system and expensive experiments.\nTo mitigate those limitations, this paper proposes a lifelong update method for\nDT models to capture the effects of system degradation on system responses\nwithout any prior knowledge and expensive offline experiments on the system.\nThe core idea in the work is to represent the system degradation during the\nlifecycle as the dynamic changes of DT configurations (i.e., model parameters\nwith a fixed model structure) at all degradation stages. During the lifelong\nupdate process, an Autoencoder is adopted to reconstruct the model parameters\nof all hidden layers simultaneously, so that the latent features taking into\naccount the dependencies among hidden layers are obtained for each degradation\nstage. The dynamic behavior of latent features among successive degradation\nstages is then captured by a long short-term memory model, which enables\nprediction of the latent feature at any unseen stage. Based on the predicted\nlatent features, the model configuration at future degradation stage is\nreconstructed to determine the new DT model, which predicts the system\nresponses affected by the degradation at the same stage. The test results on\ntwo engineering datasets demonstrate that the proposed update method could\ncapture effects of system degradation on system responses during the lifecycle.",
      "generated_abstract": "wins (DTs) are powerful tools for digitalization and data-driven\nmanagement, but they are prone to failure due to limited model update frequency\nand stale data. This study proposes a system degradation model for DTs based on\nthe lifecycle system model. The model captures the effects of age, time\nelapsed, and resource utilization on a DT's lifecycle health. It predicts\nfailure probabilities for a DT based on its age, the age of its constituent\ncomponents, and the number of updates. The model is trained using a dataset of\nDTs with known lifecycle health and failure probabilities. It is evaluated on a\ndataset of DTs that have experienced a failure. The model's accuracy is\nevaluated using the kappa statistic, which measures the extent to which two\npredictions agree with the ground truth. The study demonstrates that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1793103448275862,
          "p": 0.33766233766233766,
          "f": 0.23423422970335206
        },
        "rouge-2": {
          "r": 0.029045643153526972,
          "p": 0.056,
          "f": 0.038251361622473584
        },
        "rouge-l": {
          "r": 0.1724137931034483,
          "p": 0.3246753246753247,
          "f": 0.2252252206943431
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.00741v2",
      "true_abstract": "Fully-supervised lesion recognition methods in medical imaging face\nchallenges due to the reliance on large annotated datasets, which are expensive\nand difficult to collect. To address this, synthetic lesion generation has\nbecome a promising approach. However, existing models struggle with\nscalability, fine-grained control over lesion attributes, and the generation of\ncomplex structures. We propose LesionDiffusion, a text-controllable lesion\nsynthesis framework for 3D CT imaging that generates both lesions and\ncorresponding masks. By utilizing a structured lesion report template, our\nmodel provides greater control over lesion attributes and supports a wider\nvariety of lesion types. We introduce a dataset of 1,505 annotated CT scans\nwith paired lesion masks and structured reports, covering 14 lesion types\nacross 8 organs. LesionDiffusion consists of two components: a lesion mask\nsynthesis network (LMNet) and a lesion inpainting network (LINet), both guided\nby lesion attributes and image features. Extensive experiments demonstrate that\nLesionDiffusion significantly improves segmentation performance, with strong\ngeneralization to unseen lesion types and organs, outperforming current\nstate-of-the-art models. Code will be available at\nhttps://github.com/HengruiTianSJTU/LesionDiffusion.",
      "generated_abstract": "models have recently shown strong potential in generating synthetic\ntexts, but existing methods often rely on a hand-designed text generator\narchitecture. In this work, we propose a novel text-controlled lesion\nsynthesis model, LesionDiffusion, which employs a text-agnostic diffusion\nmodel to generate lesions. In contrast to existing methods that pre-train the\nmodel on lesion images, we design an end-to-end framework that directly uses the\ntext as the input of the model, thereby enabling the model to generalize to\ntext-less lesion images. Our method achieves state-of-the-art performance on\nthe lesion synthesis task across various lesion types, text conditions, and\ndifferent lesion datasets. Additionally, we introduce a novel lesion\nsynthesis-text generation framework that enables the generation of text-free\nlesions. Extensive experiments show that our method",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23333333333333334,
          "p": 0.34146341463414637,
          "f": 0.27722771794922074
        },
        "rouge-2": {
          "r": 0.03067484662576687,
          "p": 0.043478260869565216,
          "f": 0.03597121817064399
        },
        "rouge-l": {
          "r": 0.19166666666666668,
          "p": 0.2804878048780488,
          "f": 0.2277227674541713
        }
      }
    },
    {
      "paper_id": "cs.MM.cs/MM/2503.07911v1",
      "true_abstract": "Pixel-level segmentation is essential in remote sensing, where foundational\nvision models like CLIP and Segment Anything Model(SAM) have demonstrated\nsignificant capabilities in zero-shot segmentation tasks. Despite their\nadvances, challenges specific to remote sensing remain substantial. Firstly,\nThe SAM without clear prompt constraints, often generates redundant masks, and\nmaking post-processing more complex. Secondly, the CLIP model, mainly designed\nfor global feature alignment in foundational models, often overlooks local\nobjects crucial to remote sensing. This oversight leads to inaccurate\nrecognition or misplaced focus in multi-target remote sensing imagery. Thirdly,\nboth models have not been pre-trained on multi-scale aerial views, increasing\nthe likelihood of detection failures. To tackle these challenges, we introduce\nthe innovative VTPSeg pipeline, utilizing the strengths of Grounding DINO,\nCLIP, and SAM for enhanced open-vocabulary image segmentation. The Grounding\nDINO+(GD+) module generates initial candidate bounding boxes, while the CLIP\nFilter++(CLIP++) module uses a combination of visual and textual prompts to\nrefine and filter out irrelevant object bounding boxes, ensuring that only\npertinent objects are considered. Subsequently, these refined bounding boxes\nserve as specific prompts for the FastSAM model, which executes precise\nsegmentation. Our VTPSeg is validated by experimental and ablation study\nresults on five popular remote sensing image segmentation datasets.",
      "generated_abstract": "ration of remote sensing and artificial intelligence has become a\nprominent topic in the field of remote sensing, and the development of\nprompt-based remote sensing methods has been a major trend in recent years.\nHowever, the segmentation of remote sensing images and text prompts remains a\nchallenging task due to the diverse structures of these two modalities and the\ncomplex interactions between them. In this paper, we propose a novel multi-\nmodel framework for remote sensing prompt segmentation, which integrates a\npre-trained vision transformer (ViT) with a text prompt encoder and a\nprompt decoder to process remote sensing images and prompt text. The vision\ntransformer serves as the image encoder, enabling the model to learn visual\ninformation from remote sensing images and extract the corresponding text\nprompt, while the prompt encoder is designed to encode the prompt text into a\nlow-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18493150684931506,
          "p": 0.32142857142857145,
          "f": 0.23478260405897927
        },
        "rouge-2": {
          "r": 0.015625,
          "p": 0.024193548387096774,
          "f": 0.018987337003686502
        },
        "rouge-l": {
          "r": 0.14383561643835616,
          "p": 0.25,
          "f": 0.18260869101550103
        }
      }
    },
    {
      "paper_id": "math.ST.math/ST/2503.08355v1",
      "true_abstract": "This work addresses the problem of estimating a vector field from a noisy\nOrdinary Differential Equation (ODE) in a non-parametric regression setting\nwith a random design for initial values. More specifically, given a vector\nfield $ f:\\mathbb{R}^{D}\\rightarrow \\mathbb{R}^{D}$ governing a dynamical\nsystem defined by the autonomous ODE: $y' = f(y)$, we assume that the\nobservations are $\\tilde{y}_{X_{i}}(t_{j}) = y_{X_{i}}(t_{j}) +\n\\varepsilon_{i,j}$ where $y_{X_{i}}(t_{j})$ is the solution of the ODE at time\n$t_{j}$ with initial condition $y(0) = X_{i}$, $X_{i}$ is sampled from a\nprobability distribution $\\mu$, and $\\varepsilon_{i,j}$ some noise. In this\ncontext, we investigate, from a minimax perspective, the pointwise\nreconstruction of $f$ within the envelope of trajectories originating from the\nsupport of $\\mu$. We propose an estimation strategy based on preliminary flow\nreconstruction and techniques from derivative estimation in non-parametric\nregression. Under mild assumptions on $f$, we establish convergence rates that\ndepend on the temporal resolution, the number of sampled initial values and the\nmass concentration of $\\mu$. Importantly, we show that these rates are minimax\noptimal. Furthermore, we discuss the implications of our results in a manifold\nlearning setting, providing insights into how our approach can mitigate the\ncurse of dimensionality.",
      "generated_abstract": "We study the pointwise minimax minimax reconstruction problem for the\nreconstruction of a vector field from noisy observations. The problem is\nformulated in terms of the minimax minimax minimax problem for a linear\ndifferential operator in a space of functions that is defined on the vector\nfield, and is solved by the minimax minimax minimax problem for a linear\ndifferential operator in a space of functions that is defined on a\ntime-dependent vector field. The problem is considered for the case where the\nvector field is smooth and has only a finite number of jumps. We show that the\nminimax minimax minimax problem for the linear differential operator is a\nconvolution problem, and we obtain the minimax minimax minimax minimax point\nwise minimax minimax minimax reconstruction result.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19083969465648856,
          "p": 0.5,
          "f": 0.27624308992399504
        },
        "rouge-2": {
          "r": 0.06486486486486487,
          "p": 0.14285714285714285,
          "f": 0.08921932655988744
        },
        "rouge-l": {
          "r": 0.1450381679389313,
          "p": 0.38,
          "f": 0.20994474738255858
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2503.06707v1",
      "true_abstract": "We extend the scope of differential machine learning and introduce a new\nbreed of supervised principal component analysis to reduce dimensionality of\nDerivatives problems. Applications include the specification and calibration of\npricing models, the identification of regression features in least-square\nMonte-Carlo, and the pre-processing of simulated datasets for (differential)\nmachine learning.",
      "generated_abstract": "een widely used in financial data analysis, but its effectiveness\ndepends on the quality of the data. A common issue is that the data are not\nidentically and independently distributed (i.i.d.). For example, the covariance\nmatrix of the price series is unknown, and the covariance matrix of the\nvolatility series is different from that of the price series. In this paper, we\nintroduce the difference-based PCA (DBPCA) to improve the robustness of PCA\nwith the covariance matrix unknown. We first show that the difference-based\nPCA can be viewed as the PCA with the covariance matrix known. The difference\nbetween the two PCA is the difference between the two covariance matrices.\nTherefore, if the covariance matrix of the price series is known, the\ndifference-based PCA can be viewed as the PCA with",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1794871794871795,
          "p": 0.10144927536231885,
          "f": 0.12962962501543226
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.010526315789473684,
          "f": 0.013888884399114104
        },
        "rouge-l": {
          "r": 0.1794871794871795,
          "p": 0.10144927536231885,
          "f": 0.12962962501543226
        }
      }
    },
    {
      "paper_id": "cs.CV.stat/OT/2501.12596v1",
      "true_abstract": "This expository paper introduces a simplified approach to image-based quality\ninspection in manufacturing using OpenAI's CLIP (Contrastive Language-Image\nPretraining) model adapted for few-shot learning. While CLIP has demonstrated\nimpressive capabilities in general computer vision tasks, its direct\napplication to manufacturing inspection presents challenges due to the domain\ngap between its training data and industrial applications. We evaluate CLIP's\neffectiveness through five case studies: metallic pan surface inspection, 3D\nprinting extrusion profile analysis, stochastic textured surface evaluation,\nautomotive assembly inspection, and microstructure image classification. Our\nresults show that CLIP can achieve high classification accuracy with relatively\nsmall learning sets (50-100 examples per class) for single-component and\ntexture-based applications. However, the performance degrades with complex\nmulti-component scenes. We provide a practical implementation framework that\nenables quality engineers to quickly assess CLIP's suitability for their\nspecific applications before pursuing more complex solutions. This work\nestablishes CLIP-based few-shot learning as an effective baseline approach that\nbalances implementation simplicity with robust performance, demonstrated in\nseveral manufacturing quality control applications.",
      "generated_abstract": "vances in Generative Adversarial Networks (GANs) have made\nrobust image synthesis a reality. However, the use of GANs to generate\nsynthetic images has the potential to cause privacy and security issues. To\naddress these concerns, we propose an approach to use Generative Adversarial\nNetworks (GANs) to generate synthetic images for quality control in manufacturing\nprocesses. This paper describes a novel method for using GANs to generate\nsynthetic images for inspection tasks in manufacturing processes. Our approach\nuses a pre-trained CLIP model to generate synthetic images that closely\nresemble the real-world images used for training the CLIP model. This approach\nallows for rapid generation of synthetic images for use in inspection tasks. We\nalso use our method to generate synthetic images for a variety of application\nexamples in manufacturing quality control. These",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.216,
          "p": 0.38028169014084506,
          "f": 0.27551019946116206
        },
        "rouge-2": {
          "r": 0.030864197530864196,
          "p": 0.04807692307692308,
          "f": 0.037593980200124985
        },
        "rouge-l": {
          "r": 0.176,
          "p": 0.30985915492957744,
          "f": 0.22448979129789678
        }
      }
    },
    {
      "paper_id": "math.SG.math/AT/2503.09783v1",
      "true_abstract": "For a Weinstein manifold, we compare and contrast the properties of admitting\nan arboreal skeleton and admitting Maslov data. Both properties are implied by\nthe existence of a polarization, for which a basic obstruction is that the odd\nChern classes are 2-torsion. In a similar spirit we establish cohomological\nobstructions to the existence of arboreal skeleta and to the existence of\nMaslov data, exhibiting concrete examples to illustrate their failure. For\ninstance, we show that complements of smooth anti-canonical divisors in complex\nprojective space may fail to admit either arboreal skeleta or Maslov data. We\nalso exhibit an example of a Weinstein manifold which admits Maslov data but\ndoes not admit an arboreal skeleton.",
      "generated_abstract": "In this paper, we study a class of arborealizable connections on Riemannian\nsurfaces. We show that for every $\\varepsilon > 0$, there exists a positive\nconstant $C_\\varepsilon$ such that for all $\\alpha > 0$ and $p \\geq 2$, if\n$\\alpha \\leq \\varepsilon^2$ and $p \\geq \\frac{2(p-1)}{2p-4}$, then the\n$\\alpha$-arborealizable connection $\\nabla^{\\text{arb}}$ has Maslov data of\ntype at most $p$. We also give a complete list of arborealizable connections\nfor which the $\\alpha$-arborealizable connection $\\nabla^{\\text{arb}}$ has\narbitrarily high Maslov data for some $\\alpha > 0$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19444444444444445,
          "p": 0.2413793103448276,
          "f": 0.21538461044260365
        },
        "rouge-2": {
          "r": 0.039603960396039604,
          "p": 0.056338028169014086,
          "f": 0.0465116230590865
        },
        "rouge-l": {
          "r": 0.19444444444444445,
          "p": 0.2413793103448276,
          "f": 0.21538461044260365
        }
      }
    },
    {
      "paper_id": "cs.CY.physics/ed-ph/2502.08705v2",
      "true_abstract": "Engaging the public with science is critical for a well-informed population.\nA popular method of scientific communication is documentaries. Once released,\nit can be difficult to assess the impact of such works on a large scale, due to\nthe overhead required for in-depth audience feedback studies. In what follows,\nwe overview our complementary approach to qualitative studies through\nquantitative impact and sentiment analysis of Amazon reviews for several\nscientific documentaries. In addition to developing a novel impact category\ntaxonomy for this analysis, we release a dataset containing 1296\nhuman-annotated sentences from 1043 Amazon reviews for six movies created in\nwhole or part by the Advanced Visualization Lab (AVL). This interdisciplinary\nteam is housed at the National Center for Supercomputing Applications and\nconsists of visualization designers who focus on cinematic presentations of\nscientific data. Using this data, we train and evaluate several machine\nlearning and large language models, discussing their effectiveness and possible\ngeneralizability for documentaries beyond those focused on for this work.\nThemes are also extracted from our annotated dataset which, along with our\nlarge language model analysis, demonstrate a measure of the ability of\nscientific documentaries to engage with the public.",
      "generated_abstract": "y investigates the influence of science documentaries on Amazon\nreviewers. We analyze 217,453 reviews from 57 documentaries across 2015-2017,\nand find that documentaries tend to receive positive reviews, and their\nreviews are more likely to be positive than negative. We also find that the\nreviewers' ratings of the documentaries are significantly correlated with the\nratings of the documentaries. Moreover, we found that the average rating of a\ndocumentary is negatively correlated with its viewership on YouTube, and\npositively correlated with its viewership on Amazon. However, we found no\ncorrelation between the documentaries' ratings and their ratings on IMDb.\nFinally, we found that the reviewers' ratings of a documentary are positively\ncorrelated with the documentary's ratings on IMDb. These findings have\nsignificant implications",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13953488372093023,
          "p": 0.27692307692307694,
          "f": 0.1855670058534383
        },
        "rouge-2": {
          "r": 0.01092896174863388,
          "p": 0.021052631578947368,
          "f": 0.014388484709643762
        },
        "rouge-l": {
          "r": 0.12403100775193798,
          "p": 0.24615384615384617,
          "f": 0.1649484491524074
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2501.12233v1",
      "true_abstract": "We address the challenge of identifying all real positive steady states in\nchemical reaction networks (CRNs) governed by mass-action kinetics. Traditional\nnumerical methods often require specific initial guesses and may fail to find\nall the solutions in systems exhibiting multistability. Gr\\\"obner bases offer\nan algebraic framework that systematically transforms polynomial equations into\nsimpler forms, facilitating comprehensive solution enumeration. In this work,\nwe propose a conjecture that CRNs with at most pairwise interactions yield\nGr\\\"obner bases possessing a near-\"triangular\" structure, under appropriate\nassumptions. We illustrate this phenomenon using examples from a gene\nregulatory network and the Wnt signaling pathway, where the Gr\\\"obner basis\napproach reliably captures all real positive solutions. Our computational\nexperiments reveal the potential of Gr\\\"obner bases to overcome limitations of\nlocal numerical methods for finding the steady states of complex biological\nsystems, making them a powerful tool for understanding dynamical processes\nacross diverse biochemical models.",
      "generated_abstract": "t a novel mathematical framework for understanding the dynamics\nof complex chemical reaction networks, based on Gr\\\"obner bases. Our approach\nuses the Gr\\\"obner basis construction to identify the structure of the\ndynamics of these networks. We propose that Gr\\\"obner bases arise from the\nalgebraic structure of the network, and that the dynamics can be described by\nan algebraic system of equations. Our approach is general and applicable to\nnetworks of any size and complexity, as long as they can be expressed as\ndifferential equations. This approach is particularly interesting for\nbiological systems, where the algebraic system of equations can be used to\npredict the dynamics of the system, without requiring a full dynamics analysis.\nThis method can also be used to identify dynamical regimes, and predict the\ntime evolution of the system. We use this method to analyse the dynamics of a\nsimple chemical reaction network, and to identify some interesting regimes. We",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2608695652173913,
          "p": 0.379746835443038,
          "f": 0.3092783456876395
        },
        "rouge-2": {
          "r": 0.06428571428571428,
          "p": 0.072,
          "f": 0.06792452331790709
        },
        "rouge-l": {
          "r": 0.20869565217391303,
          "p": 0.3037974683544304,
          "f": 0.24742267558454678
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.12669v1",
      "true_abstract": "This paper examines the optimal design of information sharing in\norganizations. Organizational performance depends on agents adapting to\nuncertain external environments while coordinating their actions, where\ncoordination incentives and synergies are modeled as graphs (networks). The\nequilibrium strategies and the principal's objective function are summarized\nusing Laplacian matrices of these graphs. I formulate a Bayesian persuasion\nproblem to determine the optimal public signal and show that it comprises a set\nof statistics on local states, necessarily including their average, which\nserves as the organizational goal. When the principal benefits equally from the\ncoordination of any two agents, the choice of disclosed statistics is based on\nthe Laplacian eigenvectors and eigenvalues of the incentive graph. The\nalgebraic connectivity (the second smallest Laplacian eigenvalue) determines\nthe condition for full revelation, while the Laplacian spectral radius (the\nlargest Laplacian eigenvalue) establishes the condition for minimum\ntransparency, where only the average state is disclosed.",
      "generated_abstract": "uce an information design framework for adaptive organizations,\ndeveloped in the context of adaptive teams. Adaptive teams are defined as\nmulti-agent systems where each agent possesses incomplete information about the\nenvironment and the actions of other agents. The adaptive organization is\ndefined as a social network of agents and their interactions with the\nenvironment. We assume that the environment is structured and can be modeled as\na graph, while the agents are modeled as agents in a graph with no directed\nedges. Each agent is endowed with a limited amount of local information about\nthe environment and their interactions with other agents. This local information\nis incomplete, and can be updated by observing interactions between the agents\nand the environment. The adaptive organization is characterized by an\ninteraction matrix that encodes the interactions between the agents. The\ninteraction matrix is updated in a dynamic manner through interactions between\nthe agents. We develop a framework for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19811320754716982,
          "p": 0.2916666666666667,
          "f": 0.23595505136220185
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.03305785123966942,
          "f": 0.030188674282948483
        },
        "rouge-l": {
          "r": 0.18867924528301888,
          "p": 0.2777777777777778,
          "f": 0.22471909630602208
        }
      }
    },
    {
      "paper_id": "eess.IV.q-bio/TO/2503.03780v1",
      "true_abstract": "Camera-based vital signs monitoring in recent years has attracted more and\nmore researchers and the results are promising. However, a few research works\nfocus on heart rate extraction under extremely low illumination environments.\nIn this paper, we propose a novel framework for remote heart rate estimation\nunder low-light conditions. This method uses singular spectrum analysis (SSA)\nto decompose the filtered signal into several reconstructed components. A\nspectral masking algorithm is utilized to refine the preliminary candidate\ncomponents on the basis of a reference heart rate. The contributive components\nare fused into the final pulse signal. To evaluate the performance of our\nframework in low-light conditions, the proposed approach is tested on a\nlarge-scale multi-illumination HR dataset (named MIHR). The test results verify\nthat the proposed method has stronger robustness to low illumination than\nstate-of-the-art methods, effectively improving the signal-to-noise ratio and\nheart rate estimation precision. We further perform experiments on the PUlse\nRatE detection (PURE) dataset which is recorded under normal light conditions\nto demonstrate the generalization of our method. The experiment results show\nthat our method can stably detect pulse rate and achieve comparative results.\nThe proposed method pioneers a new solution to the remote heart rate estimation\nin low-light conditions.",
      "generated_abstract": "otoplethysmography (RPPG) has been widely used for pulse wave\ndetermination in low-light conditions. This paper proposes a novel method to\nextract the RPPG pulse wave in the presence of low-light conditions. First, we\ndevelop a weighted combination method that combines two RPPG signals using the\nweighted sum of the absolute values, while preserving the signal power. This\napproach reduces the noise and reduces the effect of RPPG artifacts. Then, we\nintroduce a singular spectrum analysis (SSA) method to determine the optimal\nspectral window and its corresponding singular vectors for the extracted\nRPPG pulse wave. This method provides a more precise pulse wave extraction\nthan the original SSA method, and it is able to identify the optimal spectral\nwindow with minimal cross-correlation between the two extracted RPPG signals.\nFinally,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21428571428571427,
          "p": 0.34177215189873417,
          "f": 0.26341462940916127
        },
        "rouge-2": {
          "r": 0.043010752688172046,
          "p": 0.06956521739130435,
          "f": 0.053156141457600226
        },
        "rouge-l": {
          "r": 0.1984126984126984,
          "p": 0.31645569620253167,
          "f": 0.24390243428721006
        }
      }
    },
    {
      "paper_id": "cs.MA.econ/GN/2502.13267v1",
      "true_abstract": "BeforeIT is an open-source software for building and simulating\nstate-of-the-art macroeconomic agent-based models (macro ABMs) based on the\nrecently introduced macro ABM developed in [1] and here referred to as the base\nmodel. Written in Julia, it combines extraordinary computational efficiency\nwith user-friendliness and extensibility. We present the main structure of the\nsoftware, demonstrate its ease of use with illustrative examples, and benchmark\nits performance. Our benchmarks show that the base model built with BeforeIT is\norders of magnitude faster than a Matlab version, and significantly faster than\nMatlab-generated C code. BeforeIT is designed to facilitate reproducibility,\nextensibility, and experimentation. As the first open-source, industry-grade\nsoftware to build macro ABMs of the type of the base model, BeforeIT can\nsignificantly foster collaboration and innovation in the field of agent-based\nmacroeconomic modelling. The package, along with its documentation, is freely\navailable at https://github.com/bancaditalia/BeforeIT.jl under the AGPL-3.0.",
      "generated_abstract": "r introduces BeforeIT.jl, a Julia package that provides a fast,\nfine-grained, and highly scalable agent-based macroeconomics modeling framework.\nThe package features a highly scalable and efficient simulation engine,\nhigh-performance parallel computing, and a flexible, extensible architecture.\nBeforeIT.jl supports both discrete and continuous-time simulations, providing\nflexibility in modeling heterogeneous agents, heterogeneous markets, and\nstructured time-series data. Additionally, the package integrates with a\nhigh-performance computing framework, enabling users to easily run their\nsimulations on large-scale supercomputers.\n  This paper provides a comprehensive tutorial for BeforeIT.jl, detailing the\nsimulation process from data loading to modeling and visualization.\n  In addition, the paper discusses the challenges faced in developing\nhigh-performance agent-based macroeconomics",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1262135922330097,
          "p": 0.17105263157894737,
          "f": 0.14525139176180535
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11650485436893204,
          "p": 0.15789473684210525,
          "f": 0.1340782074042635
        }
      }
    },
    {
      "paper_id": "math-ph.nlin/SI/2503.01578v1",
      "true_abstract": "We compute scalar products of off-shell Bethe vectors in models with\n$o_{2n+1}$ symmetry. The scalar products are expressed as a sum over partitions\nof the Bethe parameter sets, the building blocks being the so-called highest\ncoefficients. We prove some recurrence relations and a residue theorem for\nthese highest coefficients, and prove that they are consistent with the\nreduction to $gl_n$ invariant models. We also express the norm of on-shell\nBethe vectors as a Gaudin determinant.",
      "generated_abstract": "In this paper we study the scalar product and norm of the Bethe\nvector of the integrable model $\\mathfrak{o}_{2n+1}$ with the Hamiltonian\n$H=\\sum_{k=1}^{n+1} \\frac{1}{2}\\sum_{j=1}^{k-1}(-\\frac{1}{2}-2\\delta_{jk})\n\\partial_{x_j}\\partial_{x_k}$. We compute the scalar product and norm of the\nBethe vector for the cases $n=1,2,3$ and $n=4$. We also compute the scalar\nproduct and norm of the Bethe vector for the cases $n=2$ and $n=5$. Our\nresults are in good agreement with the results of Baxter's group $U_{q}(\\mathfrak{o}_{2n+1})$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24528301886792453,
          "p": 0.3333333333333333,
          "f": 0.28260869076795847
        },
        "rouge-2": {
          "r": 0.08450704225352113,
          "p": 0.12,
          "f": 0.09917354886961297
        },
        "rouge-l": {
          "r": 0.24528301886792453,
          "p": 0.3333333333333333,
          "f": 0.28260869076795847
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/TO/2503.03126v1",
      "true_abstract": "Groups of cells, including clusters of cancerous cells, multicellular\norganisms, and developing organs, may both grow and break apart. What physical\nfactors control these fractures? In these processes, what sets the eventual\nsize of clusters? We develop a framework for understanding cell clusters that\ncan fragment due to cell motility using an active particle model. We compute\nanalytically how the break rate of cell-cell junctions depends on cell speed,\ncell persistence, and cell-cell junction properties. Next, we find the cluster\nsize distributions, which differ depending on whether all cells can divide or\nonly the cells on the edge of the cluster divide. Cluster size distributions\ndepend solely on the ratio of the break rate to the growth rate - allowing us\nto predict how cluster size and variability depend on cell motility and\ncell-cell mechanics. Our results suggest that organisms can achieve better size\ncontrol when cell division is restricted to the cluster boundaries or when\nfracture can be localized to the cluster center. Our results link the general\nphysics problem of a collective active escape over a barrier to size control,\nproviding a quantitative measure of how motility can regulate organ or organism\nsize.",
      "generated_abstract": "acture is a mode of tissue deformation that allows for the\norganization of cellular material into complex architectures. We demonstrate\nthat active fracture can be controlled by the shape of the substrate,\nparticularly by the stiffness of the substrate material. We observe that\nactivating the substrate through the application of external forces can\nenhance the control of cellular organization. This is because active fracture\nprovides an effective mechanism for the control of cellular organization by\nthe substrate stiffness. We find that the stiffness of the substrate can be\ncontrolled through a simple mechanism by varying the contact angle of the\nsubstrate. We demonstrate this by varying the contact angle of the substrate\nfrom 10 degrees to 100 degrees. We observe that the stiffness of the substrate\nincreases with the contact angle of the substrate. The st",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1282051282051282,
          "p": 0.2542372881355932,
          "f": 0.1704545409975466
        },
        "rouge-2": {
          "r": 0.016666666666666666,
          "p": 0.031578947368421054,
          "f": 0.021818177295868706
        },
        "rouge-l": {
          "r": 0.10256410256410256,
          "p": 0.2033898305084746,
          "f": 0.13636363190663756
        }
      }
    },
    {
      "paper_id": "math.AP.math/SP/2503.01528v1",
      "true_abstract": "We examine semiclassical measures for Laplace eigenfunctions on compact\nhyperbolic $(n+1)$-manifolds. We prove their support must contain the cosphere\nbundle of a compact immersed totally geodesic submanifold. Our proof adapts the\nargument of Dyatlov and Jin to higher dimensions and classifies the closures of\nhorocyclic orbits using Ratner theory. An important step in the proof is a\ngeneralization of the higher-dimensional fractal uncertainty principle of Cohen\nto Fourier integral operators, which may be of independent interest.",
      "generated_abstract": "new proof of the Gromov-Lazarsfeld theorem for $\\mathbb{H}^n$,\ndenoted by Theorem \\ref{T:GL}. In this theorem, we establish the existence of\na measure $\\mu$ on the moduli space $\\mathcal{M}$ such that for any\n$(\\alpha,\\beta)\\in \\mathcal{M}$, the area of $\\alpha\\times \\beta$ in\n$\\mathbb{H}^n$ is given by the integral of $\\mu$ along the geodesic segment\njoining $\\alpha$ and $\\beta$. We also show that the area of any pair of\nparallel hyperplanes in $\\mathbb{H}^n$ is given by the integral of $\\mu$ along\nthe geodesic segment joining their endpoints. The measure $\\mu$ is constructed\nas the push-forward of a volume form on the moduli space of closed\n$n$-dimensional submanifolds of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21311475409836064,
          "p": 0.20967741935483872,
          "f": 0.21138210882146882
        },
        "rouge-2": {
          "r": 0.02666666666666667,
          "p": 0.023809523809523808,
          "f": 0.025157227720423437
        },
        "rouge-l": {
          "r": 0.16393442622950818,
          "p": 0.16129032258064516,
          "f": 0.1626016210165908
        }
      }
    },
    {
      "paper_id": "math-ph.math/MP/2503.09558v1",
      "true_abstract": "For a given graph $G$, Budzik, Gaiotto, Kulp, Wang, Williams, Wu, Yu, and the\nfirst author studied a ''topological'' differential form $\\alpha_G$, which\nexpresses violations of BRST-closedness of a quantum field theory along a\nsingle topological direction. In a seemingly unrelated context, Brown, Panzer,\nand the second author studied a ''Pfaffian'' differential form $\\phi_G$, which\nis used to construct cohomology classes of the odd commutative graph complex.\nWe give an explicit combinatorial proof that $\\alpha_G$ coincides with\n$\\phi_G$. We also discuss the equivalence of several properties of these forms,\nwhich had been established independently for both contexts in previous work.",
      "generated_abstract": "In this paper, we study the topological form of the Pfaffian form, which\nis the natural generalization of the standard Heisenberg form in quantum\nmechanics. We show that the topological form is related to the symplectic\nform of the standard Heisenberg form. Furthermore, we propose a general\ntopological action of the standard Heisenberg form on the symplectic form,\nwhich is a generalization of the standard Heisenberg action. This general\ntopological action is a generalization of the Heisenberg action in quantum\nmechanics. In particular, the topological action of the standard Heisenberg\nform on the symplectic form can be constructed as a generalization of the Heisenberg\naction. We also give a geometric interpretation of the topological form in terms\nof the symplectic form, and construct a topological action of the symplectic\nform on the topological form.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21518987341772153,
          "p": 0.37777777777777777,
          "f": 0.27419354376300736
        },
        "rouge-2": {
          "r": 0.031578947368421054,
          "p": 0.0410958904109589,
          "f": 0.03571428080002902
        },
        "rouge-l": {
          "r": 0.20253164556962025,
          "p": 0.35555555555555557,
          "f": 0.2580645115049428
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.07830v1",
      "true_abstract": "Let (K,v) be a valued field. Take an extension of v to a fixed algebraic\nclosure L of K. In this paper we show that an element a in L admits a complete\ndistinguished chain over K if and only if the extension (K(a)|K,v) is\ndefectless and unibranched. This characterization generalizes the known result\nin the henselian case. In particular, our result shows that if a admits a\ncomplete distinguished chain over K, then it also admits one over the\nhenselization; however, the converse may not be true. The main tool employed in\nour analysis is the stability of the j-invariant associated to a valuation\ntranscendental extension under passage to the henselization.",
      "generated_abstract": "In this paper, we prove that if $X$ is a complete distinguished chain\nchasing $A_n$ and $B_n$ in a class of unibranched extensions, then $X$ is\ndefectless. In particular, if $X$ is a complete distinguished chain with\n$A_n$ as a defect, then $X$ is defectless. We also show that if $X$ is a\ncomplete distinguished chain with $A_n$ as a defect and $B_n$ as a\ndefectless defect, then $X$ is defectless. Furthermore, we prove that if $X$\nis a complete distinguished chain with $A_n$ as a defect and $B_n$ as a\ndefectless defect, then $X$ is defectless if and only if $X$ is a\n$A_n$-chain.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2777777777777778,
          "p": 0.5714285714285714,
          "f": 0.3738317712988034
        },
        "rouge-2": {
          "r": 0.10476190476190476,
          "p": 0.22,
          "f": 0.14193547950052043
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.5142857142857142,
          "f": 0.33644859372870994
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.07139v1",
      "true_abstract": "In this letter, we investigate a coordinated multiple point (CoMP)-aided\nintegrated sensing and communication (ISAC) system that supports multiple users\nand targets. Multiple base stations (BSs) employ a coordinated power allocation\nstrategy to serve their associated single-antenna communication users (CUs)\nwhile utilizing the echo signals for joint radar target (RT) detection. The\nprobability of detection (PoD) of the CoMP-ISAC system is then proposed for\nassessing the sensing performance. To maximize the sum rate while ensuring the\nPoD for each RT and adhering to the total transmit power budget across all BSs,\nwe introduce an efficient power allocation strategy. Finally, simulation\nresults are provided to validate the analytical findings, demonstrating that\nthe proposed power allocation scheme effectively enhances the sum rate while\nsatisfying the sensing requirements.",
      "generated_abstract": "ration of multiple wireless communications technologies, including\ncommercial-off-the-shelf (COTS) baseband units and advanced modulation and\ncoding schemes, has increased the capacity and bandwidth of wireless networks.\nHowever, the interference introduced by these technologies can degrade the\nperformance of intelligent signal processing (ISP) systems, such as adaptive\nbeamforming, which utilizes signal processing algorithms to optimize the\ntransmission of signals. This paper studies the power allocation problem for\ncoordinated multi-point (CoMP) aided ISAC systems, which combines the\nintelligent signal processing capabilities of ISAC with the capability of\ncoordinated multi-point (CMP) communications. This problem is particularly\nchallenging due to the complex interference patterns in these networks. The\npower allocation problem is formulated as a constrained convex optimization\nproblem, which",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16091954022988506,
          "p": 0.1794871794871795,
          "f": 0.1696969647118459
        },
        "rouge-2": {
          "r": 0.017241379310344827,
          "p": 0.018518518518518517,
          "f": 0.0178571378635218
        },
        "rouge-l": {
          "r": 0.16091954022988506,
          "p": 0.1794871794871795,
          "f": 0.1696969647118459
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2402.04765v2",
      "true_abstract": "Early-stage firms play a significant role in driving innovation and creating\nnew products and services, especially for cybersecurity. Therefore, evaluating\ntheir performance is crucial for investors and policymakers. This work presents\na financial evaluation of early-stage firms' performance in 19 cybersecurity\nsectors using a private-equity dataset from 2010 to 2022 retrieved from\nCrunchbase. We observe firms, their primary and secondary activities, funding\nrounds, and pre and post-money valuations. We compare cybersecurity sectors\nregarding the amount raised over funding rounds and post-money valuations while\ninferring missing observations. We observe significant investor interest\nvariations across categories, periods, and locations. In particular, we find\nthe average capital raised (valuations) to range from USD 7.24 mln (USD 32.39\nmln) for spam filtering to USD 45.46 mln (USD 447.22 mln) for the private cloud\nsector. Next, we assume a log process for returns computed from post-money\nvaluations and estimate the expected returns, systematic and specific risks,\nand risk-adjusted returns of investments in early-stage firms belonging to\ncybersecurity sectors. Again, we observe substantial performance variations\nwith annualized expected returns ranging from 9.72\\% for privacy to 177.27\\%\nfor the blockchain sector. Finally, we show that overall, the cybersecurity\nindustry performance is on par with previous results found in private equity.\nOur results shed light on the performance of cybersecurity investments and,\nthus, on investors' expectations about cybersecurity.",
      "generated_abstract": "in information security startups is a promising strategy for\nfund managers to diversify their portfolios and increase returns. This study\nanalyzes the performance of investments in these startups using data from the\nCrunchbase platform. The results show that the risk-adjusted return of the\ninvestment portfolio is 5.2%, compared to the 1.6% for the S&P 500 Index.\nHowever, the results are sensitive to the selection of the sector and the\ncomposition of the portfolio. The most profitable sector is cybersecurity, with\nan average return of 8.3%. The analysis also reveals that the investment\nportfolio is more volatile in the first 5 years of operation than in the\nfollowing 10 years, and the cybersecurity sector is more volatile than the\ngeneral IT sector. The results suggest that investors can",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17482517482517482,
          "p": 0.31645569620253167,
          "f": 0.22522522064077602
        },
        "rouge-2": {
          "r": 0.03255813953488372,
          "p": 0.06086956521739131,
          "f": 0.042424237883379734
        },
        "rouge-l": {
          "r": 0.17482517482517482,
          "p": 0.31645569620253167,
          "f": 0.22522522064077602
        }
      }
    },
    {
      "paper_id": "math.CO.cs/CG/2503.02336v1",
      "true_abstract": "We present a program for enumerating all pseudoline arrangements with a small\nnumber of pseudolines and abstract order types of small point sets. This\nprogram supports computer experiments with these structures, and it complements\nthe order-type database of Aichholzer, Aurenhammer, and Krasser. This system\nmakes it practical to explore the abstract order types for 12 points, and the\npseudoline arrangements of 11 pseudolines.",
      "generated_abstract": "The number of pseudoline arrangements has been computed by Goren, Kramer,\nand Shelukhin using the pseudoline algorithm. We present a more efficient\nresearch tool for pseudoline arrangements that we call NumPSLA. We give a\ncomprehensive description of the algorithm, including the pseudocode and\ndetails of the implementation. We present a few applications of NumPSLA and\ndemonstrate its efficiency. We also compare the efficiency of NumPSLA with\nthe other known algorithms for the number of pseudoline arrangements.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2619047619047619,
          "p": 0.23404255319148937,
          "f": 0.247191006251736
        },
        "rouge-2": {
          "r": 0.0847457627118644,
          "p": 0.07462686567164178,
          "f": 0.07936507438523589
        },
        "rouge-l": {
          "r": 0.2619047619047619,
          "p": 0.23404255319148937,
          "f": 0.247191006251736
        }
      }
    },
    {
      "paper_id": "eess.SY.econ/TH/2502.14150v1",
      "true_abstract": "We propose a risk-sensitive security-constrained economic dispatch (R-SCED)\nformulation capturing the tradeoff between dispatch cost and resilience against\npotential line failures, where risk is modeled via the conditional value at\nrisk (CVaR). In the context of our formulation, we analyze revenue adequacy and\nside payments of two pricing models, one based on nominal generation costs, and\nanother based on total marginal cost including contingencies. In particular, we\nprove that the system operator's (SO) merchandising surplus (MS) and total\nrevenue are nonnegative under the latter, while under the former the same does\nnot hold in general. We demonstrate that the proposed R-SCED formulation is\namenable to decomposition and describe a Benders' decomposition algorithm to\nsolve it. In numerical examples, we illustrate the differences in MS and total\nrevenue under the considered pricing schemes, and the computational efficiency\nof our decomposition approach.",
      "generated_abstract": "aper, we study a new class of optimal energy and security dispatch\nproblems with risk-sensitive utility functions. These problems arise in\nsystems where security is of primary concern, but risk is also a\nconstraint. We propose a risk-sensitive utility function that combines\nsecurity with risk aversion. We derive an optimization problem that defines a\nnew notion of optimal dispatch, which we call risk-sensitive economic dispatch\n(RSDD). We then develop a method to solve the RSDD problem, using a\nnonlinear programming approach. We use the RSDD problem to study the impact of\nsecurity and risk on dispatch, as well as on energy prices. We compare our\nresults with those obtained using a nonlinear programming approach that\nsimplifies the RSDD problem, and we compare the results of our method with the\nresults obtained using the simpler problem. Our findings suggest that the\nimpact of security on energy",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20833333333333334,
          "p": 0.2702702702702703,
          "f": 0.23529411273079595
        },
        "rouge-2": {
          "r": 0.06060606060606061,
          "p": 0.0625,
          "f": 0.06153845653964538
        },
        "rouge-l": {
          "r": 0.20833333333333334,
          "p": 0.2702702702702703,
          "f": 0.23529411273079595
        }
      }
    },
    {
      "paper_id": "math.OC.econ/EM/2502.05212v1",
      "true_abstract": "In this paper, we provide analytic expressions for the first-order loss\nfunction, the complementary loss function and the second-order loss function\nfor several probability distributions. These loss functions are important\nfunctions in inventory optimization and other quantitative fields. For several\nreasons, which will become apparent throughout this paper, the implementation\nof these loss functions prefers the use of an analytic expression, only using\nstandard probability functions. However, complete and consistent references of\nanalytic expressions for these loss functions are lacking in literature. This\npaper aims to close this gap and can serve as a reference for researchers,\nsoftware engineers and practitioners that are concerned with the optimization\nof a quantitative system. This should lead directly to easily using different\nprobability distributions in quantitive models which is at the core of\noptimization. Also, this paper serves as a broad introduction to loss functions\nand their use in inventory control.",
      "generated_abstract": "This paper studies the problem of inventory control under a fixed-cost\napproach. The problem involves the estimation of the stockout probability and\nthe choice of a loss function that measures the difference between the\nestimated stockout probability and the actual stockout probability. The loss\nfunction is designed to minimize the loss in stockout probability. We consider\nthe case where the stockout probability is non-deterministic. We propose two\nloss functions, namely, the sum of squared errors and the empirical risk\nminimization loss, and compare their performance. We derive a necessary and\nsufficient condition for the existence of a unique loss function that minimizes\nthe loss in stockout probability. Finally, we present numerical results to\nillustrate the performance of the proposed loss functions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21839080459770116,
          "p": 0.2835820895522388,
          "f": 0.24675324183757807
        },
        "rouge-2": {
          "r": 0.05185185185185185,
          "p": 0.06796116504854369,
          "f": 0.058823524502154216
        },
        "rouge-l": {
          "r": 0.20689655172413793,
          "p": 0.26865671641791045,
          "f": 0.23376622885056514
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.astro-ph/CO/2503.09599v1",
      "true_abstract": "Primordial Magnetic Fields (PMFs), long studied as potential relics of the\nearly Universe, accelerate the recombination process and have been proposed as\na possible way to relieve the Hubble tension. However, previous studies relied\non simplified toy models. In this study, for the first time, we use the recent\nhigh-precision evaluations of recombination with PMFs, incorporating full\nmagnetohydrodynamic (MHD) simulations and detailed Lyman-alpha radiative\ntransfer, to test PMF-enhanced recombination ($b\\Lambda$CDM) against\nobservational data from the cosmic microwave background (CMB), baryon acoustic\noscillations (BAO), and Type Ia supernovae (SN). Focusing on non-helical PMFs\nwith a Batchelor spectrum, we find a preference for present-day total field\nstrengths of approximately 5-10 pico-Gauss. Depending on the dataset\ncombination, this preference ranges from mild ($\\sim 1.8\\sigma$ with Planck +\nDESI) to moderate ($\\sim 3\\sigma$ with Planck + DESI + SH0ES-calibrated SN)\nsignificance. The $b\\Lambda$CDM has Planck + DESI $\\chi^2$ values equal or\nbetter than those of the $\\Lambda$CDM model while predicting a higher Hubble\nconstant. The favored field strengths align closely with those required for\ncluster magnetic fields to originate entirely from primordial sources, without\nthe need for additional dynamo amplification or stellar magnetic field\ncontamination. Future high-resolution CMB temperature and polarization\nmeasurements will be crucial for confirming or further constraining the\npresence of PMFs at recombination.",
      "generated_abstract": "e the effect of primordial magnetic fields (PMFs) on the Cosmic\nrecombination process and the evolution of the cosmic magnetic field (CMF).\nMagnetic fields in the early Universe are likely to have been generated by\nbaryon acoustic oscillations (BAO) and large-scale structure formation, and\nmay have contributed to the reionization process. We focus on the case where\nthe magnetic field is inhomogeneous on small scales, with a power-law spectrum\nwith a maximum scale of the order of the Hubble scale. The magnetic field is\ngenerated by the dissipative shock wave, which is responsible for reionization\nand the CMF evolution. We analyze the impact of PMFs on the CMF evolution and\nthe reionization history. We find that the magnetic field can affect the\nreionization history, with stronger magnetic fields increasing the reionization",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16774193548387098,
          "p": 0.3561643835616438,
          "f": 0.22807017108533403
        },
        "rouge-2": {
          "r": 0.06796116504854369,
          "p": 0.12612612612612611,
          "f": 0.08832807115883354
        },
        "rouge-l": {
          "r": 0.16129032258064516,
          "p": 0.3424657534246575,
          "f": 0.2192982412607726
        }
      }
    },
    {
      "paper_id": "math.AG.math/AG/2503.08924v1",
      "true_abstract": "This article introduces efficient and user-friendly tools for analyzing the\nintersection curve between a ringed torus and an irreducible quadric surface.\nWithout loose of generality, it is assumed that the torus is centered at the\norigin, and its axis of revolution coincides with the $z$-axis. The paper\nprimarily focuses on examining the curve's projection onto the plane $z=0$,\nreferred to as the cutcurve, which is essential for ensuring accurate lifting\nprocedures. Additionally, we provide a detailed characterization of the\nsingularities in both the projection and the intersection curve, as well as the\nexistence of double tangents. A key tool for the analysis is the theory of\nresultant and subresultant polynomials.",
      "generated_abstract": "aper we discuss the intersection curve between a torus and a\nquadric through a projection. We show that the intersection curve is a\nquadric-elliptic surface, and we describe a parametrization of its projection\nto the plane. We also prove that the intersection curve is a smooth quadric\nwhen the projection is a general one, and that it is a general quadric when\nthe projection is a general one and the tangent plane of the torus is\ndegenerate. Finally, we describe a way of lifting the intersection curve to a\ncurve on the plane through the projection to the plane. We prove that the\nlifted curve is a general quadric. In addition, we show that the lift of the\nintersection curve is the intersection of a general quadric with a general\nquadric, and we prove that the lift of the intersection curve is the intersection\nof a general quadric with the general quadric corresponding to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25316455696202533,
          "p": 0.43478260869565216,
          "f": 0.3199999953484801
        },
        "rouge-2": {
          "r": 0.14018691588785046,
          "p": 0.1595744680851064,
          "f": 0.1492537263641991
        },
        "rouge-l": {
          "r": 0.20253164556962025,
          "p": 0.34782608695652173,
          "f": 0.25599999534848006
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.02578v1",
      "true_abstract": "Bird's Eye View (BEV) perception technology is crucial for autonomous\ndriving, as it generates top-down 2D maps for environment perception,\nnavigation, and decision-making. Nevertheless, the majority of current BEV map\ngeneration studies focusing on visual map generation lack depth-aware reasoning\ncapabilities. They exhibit limited efficacy in managing occlusions and handling\ncomplex environments, with a notable decline in perceptual performance under\nadverse weather conditions or low-light scenarios. Therefore, this paper\nproposes TS-CGNet, which leverages Temporal-Spatial fusion with\nCenterline-Guided diffusion. This visual framework, grounded in prior\nknowledge, is designed for integration into any existing network for building\nBEV maps. Specifically, this framework is decoupled into three parts: Local\nmapping system involves the initial generation of semantic maps using purely\nvisual information; The Temporal-Spatial Aligner Module (TSAM) integrates\nhistorical information into mapping generation by applying transformation\nmatrices; The Centerline-Guided Diffusion Model (CGDM) is a prediction module\nbased on the diffusion model. CGDM incorporates centerline information through\nspatial-attention mechanisms to enhance semantic segmentation reconstruction.\nWe construct BEV semantic segmentation maps by our methods on the public\nnuScenes and the robustness benchmarks under various corruptions. Our method\nimproves 1.90%, 1.73%, and 2.87% for perceived ranges of 60x30m, 120x60m, and\n240x60m in the task of BEV HD mapping. TS-CGNet attains an improvement of 1.92%\nfor perceived ranges of 100x100m in the task of BEV semantic mapping. Moreover,\nTS-CGNet achieves an average improvement of 2.92% in detection accuracy under\nvarying weather conditions and sensor interferences in the perception range of\n240x60m. The source code will be publicly available at\nhttps://github.com/krabs-H/TS-CGNet.",
      "generated_abstract": "Engineering Virtual Reality (BEV) maps is critical for autonomous\ncar navigation. Traditional methods rely on the traditional point cloud-based\nBEV mapping pipeline, which is time-consuming and requires precise point\nclouds, making them inapplicable to complex and large-scale mapping tasks.\nExisting BEV mapping methods often suffer from poor spatial consistency,\nunrealistic geometry, and uncontrolled illumination. In this paper, we\nintroduce TS-CGNet, the first BEV mapping method that integrates temporal\nand spatial fusion into a unified framework. Our approach incorporates a\ntemporal-spatial fusion module that fuses the BEV and 2D semantic maps with\ntemporal consistency. The fusion results are then fed into a centerline-guided\ndiffusion model for geometry control. The proposed method achieves a\nbreakthrough in real-world mapping by en",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19428571428571428,
          "p": 0.38202247191011235,
          "f": 0.25757575310634767
        },
        "rouge-2": {
          "r": 0.01639344262295082,
          "p": 0.03508771929824561,
          "f": 0.022346364374396147
        },
        "rouge-l": {
          "r": 0.18857142857142858,
          "p": 0.3707865168539326,
          "f": 0.24999999553059005
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/AP/2503.02850v1",
      "true_abstract": "The comparison of different medical treatments from observational studies or\nacross different clinical studies is often biased by confounding factors such\nas systematic differences in patient demographics or in the inclusion criteria\nfor the trials. Propensity score matching is a popular method to adjust for\nsuch confounding. It compares weighted averages of patient responses. The\nweights are calculated from logistic regression models with the intention to\nreduce differences between the confounders in the treatment groups. However,\nthe groups are only \"roughly matched\" with no generally accepted principle to\ndetermine when a match is \"good enough\".\n  In this manuscript, we propose an alternative approach to the matching\nproblem by considering it as a constrained optimization problem. We investigate\nthe conditions for exact matching in the sense that the average values of\nconfounders are identical in the treatment groups after matching. Our approach\nis similar to the matching-adjusted indirect comparison approach by\nSignorovitch et al. (2010) but with two major differences: First, we do not\nimpose any specific functional form on the matching weights; second, the\nproposed approach can be applied to individual patient data from several\ntreatment groups as well as to a mix of individual patient and aggregated data.",
      "generated_abstract": "y score matching (PSM) is a popular method for matching patients\nmatched to a treatment group with those not matched. The matching error is\noften described as the chance of an outcome being incorrectly assigned due to\nimperfections in the matching procedure. For example, in the context of\nclinical trials, the chance of an outcome being incorrectly assigned is\ndefined as the chance of an outcome being incorrectly assigned due to\ninconsistency between the assigned treatment and the treatment being tested.\n  The problem of matching in practice is often complex and it is not always\nclear which patients to match. In this work we propose a novel approach that\nuses exact matching rather than matching to the propensity score. The key\ndifference is that exact matching uses the exact patient assignment in the\nmatched data, while propensity score matching uses the estimated assignment.\n  This approach is motivated by the fact that PSM is an",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2619047619047619,
          "p": 0.42857142857142855,
          "f": 0.3251231480006795
        },
        "rouge-2": {
          "r": 0.08465608465608465,
          "p": 0.125,
          "f": 0.1009463674248925
        },
        "rouge-l": {
          "r": 0.24603174603174602,
          "p": 0.4025974025974026,
          "f": 0.30541871450314256
        }
      }
    },
    {
      "paper_id": "physics.soc-ph.physics/pop-ph/2502.03191v2",
      "true_abstract": "This study intends to test the hypothesis that, contrary to traditional\ninterpretation, the social structure of the polity of Aksum - especially in its\nearly stages - was not characterized by a vertical hierarchy with highly\ncentralized administrative power, and that the leaders mentioned in the few\navailable inscriptions were predominantly ritual leaders with religious rather\nthan coercive political authority. This hypothesis, suggested by the available\narchaeological evidence, is grounded in Charles Stanish's model, which posits\nthat pre-state societies could achieve cooperative behavior without the\npresence of coercive authority. Using agent-based modeling applied to data\ninspired by the Aksum civilization, we examine the dynamics of cooperation in\nthe presence and absence of a Public Goods Game. Results show that while\ncooperative behavior can emerge in the short term without coercive power, it\nmay not be sustainable over the long term, suggesting a need for centralized\nauthority to foster stable, complex societies. These findings provide insights\ninto the evolutionary pathways that lead to state formation and complex social\nstructures.",
      "generated_abstract": "a pre-state society located in what is now Ethiopia, which developed\nthrough a series of transitions from a hunter-gatherer society to a\nsemi-nomadic pastoralist society, to a settled agrarian economy. The\npre-state society was characterized by a hierarchical political structure\ncombined with a complex system of social institutions. The aim of this paper is\nto investigate how such a complex socio-political system would evolve and how\nit would impact on the evolution of cooperative behavior. To this end, we\ndevelop a novel agent-based model that allows us to study the emergence of\ncooperative behavior in a pre-state society. Our results show that the\nemergence of cooperative behavior is not an unconditional result of the\ndevelopment of a hierarchical political structure, but depends on the nature of\nthe cooperative institutions that emer",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22608695652173913,
          "p": 0.3291139240506329,
          "f": 0.2680412322855777
        },
        "rouge-2": {
          "r": 0.043209876543209874,
          "p": 0.06086956521739131,
          "f": 0.050541511389436015
        },
        "rouge-l": {
          "r": 0.1826086956521739,
          "p": 0.26582278481012656,
          "f": 0.21649484053300044
        }
      }
    },
    {
      "paper_id": "astro-ph.GA.astro-ph/GA/2503.10087v1",
      "true_abstract": "Galaxy formation models predict that local galaxies are surrounded by hot\nX-ray-emitting halos, which are technically difficult to detect due to their\nextended and low surface brightness nature. Previous X-ray studies have mostly\nfocused on disk galaxies more massive than the Milky Way, with essentially no\nconsensus on the halo X-ray properties at the lower mass end. We utilize the\nearly-released eROSITA and archival Chandra observations to analyze the diffuse\nX-ray emission of NGC7793, a nearby spiral galaxy with an estimated stellar\nmass of only $3.2\\times 10^9$ $M_{\\odot}$. We find evidence for extraplanar hot\ngas emission from both the radial and vertical soft X-ray intensity profiles,\nwhich spreads up to a galactocentric distance of $\\sim$ 6 kpc, nearly 30 $\\%$\nmore extended than its stellar disk. Analysis of the eROSITA spectra indicates\nthat the hot gas can be characterized by a temperature of\n$0.18^{+0.02}_{-0.03}$ keV, with 0.5--2 keV unabsorbed luminosity of $1.3\\times\n10^{38}$ erg $s^{-1}$. We compare our results with the IllustrisTNG simulations\nand find overall consistence on the disk scale, whereas excessive emission at\nlarge radii is predicted by TNG50. This work provides the latest detection of\nhot corona around a low-mass galaxy, putting new constrains on state-of-the-art\ncosmological simulations. We also verify the detectability of hot\ncircumgalactic medium around even low-mass spirals with future high-resolution\nX-ray spectrometer such as the Hot Universe Baryon Surveyor.",
      "generated_abstract": "ass disk galaxy NGC 7793 is a well-studied case of a low-mass disk\ngalaxy with a thick, hot halo. This halo has been extensively studied using\noptical and radio data, but the thermal X-ray halo has not been observed yet. We\nused the European Roentgen Satellite (eROSITA) to observe NGC 7793 with\nhigh-resolution X-ray imaging and the Chandra X-ray Observatory to obtain\nhigh-resolution X-ray spectra. We found the halo to be extremely hot, with a\nmean temperature of 10$^7$ K. The halo extends over $10^{\\circ}$ in radius and\nthe X-ray spectrum shows a distinct feature at 0.6 keV, which is a signature\nof a thermal bremsstrahlung component. The soft X",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15853658536585366,
          "p": 0.35135135135135137,
          "f": 0.21848739067297515
        },
        "rouge-2": {
          "r": 0.021929824561403508,
          "p": 0.049019607843137254,
          "f": 0.030303026031956524
        },
        "rouge-l": {
          "r": 0.14634146341463414,
          "p": 0.32432432432432434,
          "f": 0.2016806679838995
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.07061v1",
      "true_abstract": "We consider a group of agents who can each take an irreversible costly action\nwhose payoff depends on an unknown state. Agents learn about the state from\nprivate signals, as well as from past actions of their social network\nneighbors, which creates an incentive to postpone taking the action. We show\nthat outcomes depend on network structure: on networks with a linear structure\npatient agents do not converge to the first-best action, while on regular\ndirected tree networks they do.",
      "generated_abstract": "This paper considers a learning process in which individuals choose between\nrepeated interactions or a single interaction with a group. The interaction\nstrategies are determined by the network structure of the group, which is\ninformed by the interactions of individuals in the past. The model is\nillustrated with an analysis of the dynamics of social learning in the\nwell-known Diner's dilemma, where the group is the entire population of\ninterested individuals.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18032786885245902,
          "p": 0.25,
          "f": 0.2095238046548754
        },
        "rouge-2": {
          "r": 0.02531645569620253,
          "p": 0.029850746268656716,
          "f": 0.027397255307750945
        },
        "rouge-l": {
          "r": 0.14754098360655737,
          "p": 0.20454545454545456,
          "f": 0.17142856655963734
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.05674v3",
      "true_abstract": "Driven by advances in self-supervised learning for speech, state-of-the-art\nsynthetic speech detectors have achieved low error rates on popular benchmarks\nsuch as ASVspoof. However, prior benchmarks do not address the wide range of\nreal-world variability in speech. Are reported error rates realistic in\nreal-world conditions? To assess detector failure modes and robustness under\ncontrolled distribution shifts, we introduce ShiftySpeech, a benchmark with\nmore than 3000 hours of synthetic speech from 7 domains, 6 TTS systems, 12\nvocoders, and 3 languages. We found that all distribution shifts degraded model\nperformance, and contrary to prior findings, training on more vocoders,\nspeakers, or with data augmentation did not guarantee better generalization. In\nfact, we found that training on less diverse data resulted in better\ngeneralization, and that a detector fit using samples from a single carefully\nselected vocoder and a small number of speakers, without data augmentations,\nachieved state-of-the-art results on the challenging In-the-Wild benchmark.",
      "generated_abstract": "r introduces a novel synthetic speech detection approach for\nmini-speech, the shortest form of speech used in the wild. We propose a\nframework that leverages pre-trained ASR models and self-supervised learning to\ngenerate speech that closely mimics natural speech. The generated speech is\nthen used as a training dataset for a standard ASR model, enabling automatic\nspeech recognition in the wild. Our approach achieves a 97.3% success rate on\nmini-speech in the wild, with no manual labeling or supervised training.\nAdditionally, we demonstrate that synthetic speech can be used to train a\ndifferent ASR model, achieving a 99.6% success rate in the wild on a different\nmini-speech dataset. This approach offers a scalable and efficient alternative\nto traditional ASR training methods, allowing for the rapid development of\nspeech",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1743119266055046,
          "p": 0.2375,
          "f": 0.20105819617591905
        },
        "rouge-2": {
          "r": 0.013605442176870748,
          "p": 0.01680672268907563,
          "f": 0.015037589040365692
        },
        "rouge-l": {
          "r": 0.1651376146788991,
          "p": 0.225,
          "f": 0.1904761855939085
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.06651v1",
      "true_abstract": "This paper explores the emerging research direction of electromagnetic\ninformation theory (EIT), which aims to integrate traditional Shannon-based\nmethodologies with physical consistency, particularly the electromagnetic\nproperties of communication channels. We propose an EIT-based multiple-input\nmultiple-output (MIMO) paradigm that enhances conventional spatially-discrete\nMIMO models by incorporating the concepts of electromagnetic (EM) precoding and\nEM combining. This approach aims to improve the modeling of next-generation\nsystems while remaining consistent with Shannon's theoretical foundations. We\nexplore typical EIT applications, such as densely spaced MIMO, near-field\ncommunications, and tri-polarized antennas, and analyze their channel\ncharacteristics through theoretical simulations and measured datasets. The\npaper also discusses critical research challenges and opportunities for EIT\napplications from an industrial perspective, emphasizing the field's potential\nfor practical applications.",
      "generated_abstract": "Electromagnetic information theory (EMIT) is a foundational field that unifies\nunderlying electromagnetic properties, such as amplitude, phase, frequency, and\npolarization, into a unified framework. The paradigm shift from the classical\nview of electromagnetic waves to the quantum view of electromagnetic fields\nallows for a deeper understanding of EMIT. This article provides a comprehensive\noverview of EMIT, focusing on the classical and quantum viewpoints. We then\ndiscuss recent advances in EMIT, including quantum entanglement, quantum\ninterferometry, frequency modulation, and polarization modulation. We conclude\nby discussing how EMIT can be applied in various fields, including quantum\ncommunications, radar, and radiometers.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18947368421052632,
          "p": 0.24324324324324326,
          "f": 0.21301774655649328
        },
        "rouge-2": {
          "r": 0.02564102564102564,
          "p": 0.03225806451612903,
          "f": 0.028571423636735545
        },
        "rouge-l": {
          "r": 0.18947368421052632,
          "p": 0.24324324324324326,
          "f": 0.21301774655649328
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.12833v1",
      "true_abstract": "In areas infested with Aedes aegypti mosquitoes it may be possible to control\ndengue, and some other vector-borne diseases, by introducing Wolbachia-infected\nmosquitoes into the wildtype population. Thus far, empirical and theoretical\nstudies of Wolbachia release have tended to focus on the dynamics at the\ncommunity scale. However, Ae. aegypti mosquitoes typically dwell in and around\nthe same houses as the people they bite and it can be insightful to explore\nwhat happens at the household scale where small population sizes lead to\ninherently stochastic dynamics. Here we use a continuous-time Markov framework\nto develop a stochastic household model for small populations of wildtype and\nWolbachia-infected mosquitoes. We investigate the transient and long term\ndynamics of the system, in particular examining the impact of stochasticity on\nthe Wolbachia invasion threshold and bistability between the wildtype-only and\nWolbachia-only steady states previously observed in deterministic models. We\nfocus on the influence of key parameters which determine the fitness cost of\nWolbachia infection and the probability of Wolbachia vertical transmission.\nUsing Markov and matrix population theory, we derive salient characteristics of\nthe system including the probability of successful Wolbachia invasion, the\nexpected time until invasion and the probability that a Wolbachia-infected\npopulation reverts to a wildtype population. These attributes can inform\nstrategies for the release of Wolbachia-infected mosquitoes. In addition, we\nfind that releasing the minimum number of Wolbachia-infected mosquitoes\nrequired to displace a resident wildtype population according to the\ndeterministic model, only results in that outcome about 20% of the time in the\nstochastic model; a significantly larger release is required to reach a steady\nstate composed entirely of Wolbachia-infected mosquitoes 90% of the time.",
      "generated_abstract": "is a bacterial symbiont that has been proposed to play a crucial role\nin the spread of arboviruses. To explore this, we present a model of a\nhousehold-scale transmission scenario, where a female mosquito infected with\nWolbachia can transmit the bacteria to its offspring and thereby increase the\npopulation. We analyze the model using the method of moments and the\nmoment-generating function (MGF) technique, to examine the asymptotic behavior\nof the population. The model is simulated using the MATLAB code, and we find\nthat the population is asymptotically stable and the mean population size\nexhibits a log-normal distribution. We then investigate the dynamics of the\npopulation in the presence of other factors such as temperature and the\npopulation size, and we find that the mean population size follows a power law\nwhen the population",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1503267973856209,
          "p": 0.2911392405063291,
          "f": 0.19827585757766059
        },
        "rouge-2": {
          "r": 0.03225806451612903,
          "p": 0.06896551724137931,
          "f": 0.04395603961357367
        },
        "rouge-l": {
          "r": 0.13071895424836602,
          "p": 0.25316455696202533,
          "f": 0.17241378861214343
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08094v1",
      "true_abstract": "Medical image denoising is essential for improving the reliability of\nclinical diagnosis and guiding subsequent image-based tasks. In this paper, we\npropose a multi-scale approach that integrates anisotropic Gaussian filtering\nwith progressive Bezier-path redrawing. Our method constructs a scale-space\npyramid to mitigate noise while preserving critical structural details.\nStarting at the coarsest scale, we segment partially denoised images into\ncoherent components and redraw each using a parametric Bezier path with\nrepresentative color. Through iterative refinements at finer scales, small and\nintricate structures are accurately reconstructed, while large homogeneous\nregions remain robustly smoothed. We employ both mean square error and\nself-intersection constraints to maintain shape coherence during path\noptimization. Empirical results on multiple MRI datasets demonstrate consistent\nimprovements in PSNR and SSIM over competing methods. This coarse-to-fine\nframework offers a robust, data-efficient solution for cross-domain denoising,\nreinforcing its potential clinical utility and versatility. Future work extends\nthis technique to three-dimensional data.",
      "generated_abstract": "r proposes a novel denoising method using layer-wise medial axis\ntrajectory (MAT) and layer-wise repainting. The proposed method is based on a\nrecent research on layer-wise repainting, which is a technique that enables\nimage repainting by reconstructing the pixels from the repainted layers. This\nis a novel method for image denoising, and we have not seen any work that\nintegrates layer-wise repainting with image denoising. The proposed method\nfirstly applies layer-wise repainting to the denoised image. The repainted\nimage is then denoised by a denoising method such as the wavelet denoising\nmethod. The proposed method can be used for denoising images from a single\nimage with a high reconstruction quality, as long as the reconstruction quality\nis high enough. The proposed method can also be",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16030534351145037,
          "p": 0.3230769230769231,
          "f": 0.21428570985266565
        },
        "rouge-2": {
          "r": 0.013422818791946308,
          "p": 0.01818181818181818,
          "f": 0.01544401055738739
        },
        "rouge-l": {
          "r": 0.1450381679389313,
          "p": 0.2923076923076923,
          "f": 0.19387754658735953
        }
      }
    },
    {
      "paper_id": "physics.data-an.physics/data-an/2503.09415v1",
      "true_abstract": "SPARKX is an open-source Python package developed to analyze simulation data\nfrom heavy-ion collision experiments. By offering a comprehensive suite of\ntools, SPARKX simplifies data analysis workflows, supports multiple formats\nsuch as OSCAR2013, and integrates seamlessly with SMASH and JETSCAPE/X-SCAPE.\nThis paper describes SPARKX's architecture, features, and applications and\ndemonstrates its effectiveness through detailed examples and performance\nbenchmarks. SPARKX enhances productivity and precision in relativistic\nkinematics studies.",
      "generated_abstract": "experiments play a crucial role in particle physics and astroparticle\nphysics. They offer a unique window into the physics of the fundamental forces,\nincluding the strong and electromagnetic interactions. The kinematic\nproperties of these experiments, such as the particle energy, the angular\ndistributions of the final state particles, and the momentum and energy of the\ncolliding nuclei, are fundamental to understanding the particles and the\ninteractions. However, the analysis of kinematic data is challenging due to\nthe complex experimental configurations and the high-dimensional\ninteractions. The analysis of kinematic data often requires the use of\nhigh-dimensional data analysis techniques, such as principal component\nanalysis, factor analysis, and Bayesian methods. This work introduces SPARKX,\na software package for analyzing kinematic data. The SPARKX package provides\ntools for the analysis of the kinematic data, including the estimation of the\nparticle energy",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23728813559322035,
          "p": 0.18181818181818182,
          "f": 0.20588234802876312
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.01652892561983471,
          "f": 0.02139036976407773
        },
        "rouge-l": {
          "r": 0.23728813559322035,
          "p": 0.18181818181818182,
          "f": 0.20588234802876312
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.18923v1",
      "true_abstract": "Slutsky symmetry and negative semidefiniteness are necessary and sufficient\nconditions for the rationality of demand functions. While the empirical\nimplications of Slutsky negative semidefiniteness in repeated cross-sectional\ndemand data are well understood, the empirical content of Slutsky symmetry\nremains largely unexplored. This paper takes an important first step toward\naddressing this gap. We demonstrate that the average Slutsky matrix is not\nidentified and that its identified set always contains a symmetric matrix. A\nkey implication of our findings is that the symmetry of the average Slutsky\nmatrix is untestable, and consequently, individual Slutsky symmetry cannot be\ntested using the average Slutsky matrix.",
      "generated_abstract": "We investigate the asymptotic properties of the Slutsky symmetry test in\nthe context of a large class of models with a common distribution. We show\nthat in such models, the null hypothesis of Slutsky symmetry cannot be\ntested asymptotically, and the null is rejected if the null hypothesis is\nfalse. The null is therefore untestable. We apply our findings to the model\nof two-sided matching with a common distribution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23076923076923078,
          "p": 0.3488372093023256,
          "f": 0.2777777729852538
        },
        "rouge-2": {
          "r": 0.07954545454545454,
          "p": 0.11666666666666667,
          "f": 0.09459458977355759
        },
        "rouge-l": {
          "r": 0.23076923076923078,
          "p": 0.3488372093023256,
          "f": 0.2777777729852538
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2502.11510v1",
      "true_abstract": "Longitudinal models with dynamics governed by differential equations may\nrequire numerical integration alongside parameter estimation. We have\nidentified a situation where the numerical integration introduces error in such\na way that it becomes a novel source of non-uniqueness in estimation. We obtain\ntwo very different sets of parameters, one of which is a good estimate of the\ntrue values and the other a very poor one. The two estimates have forward\nnumerical projections statistically indistinguishable from each other because\nof numerical error. In such cases, the posterior distribution for parameters is\nbimodal, with a dominant mode closer to the true parameter value, and a second\ncluster around the errant value. We demonstrate that bimodality exists both\ntheoretically and empirically for an affine first order differential equation,\nthat a simulation workflow can test for evidence of the issue more generally,\nand that Markov Chain Monte Carlo sampling with a suitable solution can avoid\nbimodality. The issue of bimodal posteriors arising from numerical error has\nconsequences for Bayesian inverse methods that rely on numerical integration\nmore broadly.",
      "generated_abstract": "The bimodal posterior distribution of a longitudinal model is a feature\nlong recognized in the literature. We show that this bimodality is a direct\nconsequence of numerical integration error, which arises from the inaccurate\nintegration of an auxiliary term that does not affect the posterior\ndistribution. This error can be reduced by modifying the integration method,\nwhich we detail in a simple, yet effective, method. We demonstrate this\ntechnique through a real-world example, and we show that the method can be\nextended to more complex cases.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21052631578947367,
          "p": 0.4,
          "f": 0.27586206444708683
        },
        "rouge-2": {
          "r": 0.03592814371257485,
          "p": 0.07407407407407407,
          "f": 0.04838709237545565
        },
        "rouge-l": {
          "r": 0.18421052631578946,
          "p": 0.35,
          "f": 0.2413793058263972
        }
      }
    },
    {
      "paper_id": "math.NA.stat/CO/2502.07918v2",
      "true_abstract": "Stochastic reaction networks (SRNs) model stochastic effects for various\napplications, including intracellular chemical or biological processes and\nepidemiology. A typical challenge in practical problems modeled by SRNs is that\nonly a few state variables can be dynamically observed. Given the measurement\ntrajectories, one can estimate the conditional probability distribution of\nunobserved (hidden) state variables by solving a stochastic filtering problem.\nIn this setting, the conditional distribution evolves over time according to an\nextensive or potentially infinite-dimensional system of coupled ordinary\ndifferential equations with jumps, known as the filtering equation. The current\nnumerical filtering techniques, such as the Filtered Finite State Projection\n(D'Ambrosio et al., 2022), are hindered by the curse of dimensionality,\nsignificantly affecting their computational performance. To address these\nlimitations, we propose to use a dimensionality reduction technique based on\nthe Markovian projection (MP), initially introduced for forward problems (Ben\nHammouda et al., 2024). In this work, we explore how to adapt the existing MP\napproach to the filtering problem and introduce a novel version of the MP, the\nFiltered MP, that guarantees the consistency of the resulting estimator. The\nnovel method employs a reduced-variance particle filter for estimating the jump\nintensities of the projected model and solves the filtering equations in a\nlow-dimensional space. The analysis and empirical results highlight the\nsuperior computational efficiency of projection methods compared to the\nexisting filtered finite state projection in the large dimensional setting.",
      "generated_abstract": "We introduce a novel filtering method, called filtered Markovian projection,\nfor solving the stochastic reaction network model. The method is based on a\nMarkovian projection that preserves the Markovian property of the system,\nensuring that the filtered dynamics are Markovian. This approach is particularly\nsuitable for filtering stochastic reaction networks, where the Markovian\nproperty is essential to ensure the stability of the filtered dynamics.\nFurthermore, we propose a novel way to compute the filtered dynamics by\nintegrating the filter dynamics using a simple projection operator. We\ndemonstrate the effectiveness of the method through numerical simulations for\na two-dimensional stochastic reaction network model, which is a well-studied\nmodel in stochastic reaction networks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18867924528301888,
          "p": 0.47619047619047616,
          "f": 0.27027026620525935
        },
        "rouge-2": {
          "r": 0.0365296803652968,
          "p": 0.08247422680412371,
          "f": 0.050632907137678616
        },
        "rouge-l": {
          "r": 0.15723270440251572,
          "p": 0.3968253968253968,
          "f": 0.2252252211602143
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.16525v1",
      "true_abstract": "Recurrent neural networks (RNNs) are central to sequence modeling tasks, yet\ntheir high computational complexity poses challenges for scalability and\nreal-time deployment. Traditional pruning techniques, predominantly based on\nweight magnitudes, often overlook the intrinsic structural properties of these\nnetworks. We introduce a novel framework that models RNNs as partially ordered\nsets (posets) and constructs corresponding dependency lattices. By identifying\nmeet irreducible neurons, our lattice-based pruning algorithm selectively\nretains critical connections while eliminating redundant ones. The method is\nimplemented using both binary and continuous-valued adjacency matrices to\ncapture different aspects of network connectivity. Evaluated on the MNIST\ndataset, our approach exhibits a clear trade-off between sparsity and\nclassification accuracy. Moderate pruning maintains accuracy above 98%, while\naggressive pruning achieves higher sparsity with only a modest performance\ndecline. Unlike conventional magnitude-based pruning, our method leverages the\nstructural organization of RNNs, resulting in more effective preservation of\nfunctional connectivity and improved efficiency in multilayer networks with\ntop-down feedback. The proposed lattice-based pruning framework offers a\nrigorous and scalable approach for reducing RNN complexity while sustaining\nrobust performance, paving the way for more efficient hierarchical models in\nboth machine learning and computational neuroscience.",
      "generated_abstract": "s a widely used method to reduce the computational complexity of\nrecurrent neural networks (RNNs). In this paper, we introduce a novel\npruning-based method that uses posets, a powerful data structure, to\nautomatically prune the RNN weights. By transforming RNNs into posets, we\nreduce the number of pruned weights while maintaining the same accuracy as\nwithout pruning. We show that our method achieves this by leveraging the\nposet structure of the RNN weights to define a pruning criterion. Our method\nalso provides an efficient way to compute the pruning criterion and\npruning-induced error bounds. We demonstrate the effectiveness of our method on\ntwo widely used RNNs, a long short-term memory (LSTM) and a convolutional neural\nnetwork (CNN), and show that our method achieves state-of-the-art accuracy\nwithout",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20279720279720279,
          "p": 0.3670886075949367,
          "f": 0.26126125667681205
        },
        "rouge-2": {
          "r": 0.03208556149732621,
          "p": 0.05217391304347826,
          "f": 0.03973509462194697
        },
        "rouge-l": {
          "r": 0.17482517482517482,
          "p": 0.31645569620253167,
          "f": 0.22522522064077602
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.00920v2",
      "true_abstract": "This paper contributes to the literature on parametric demand estimation by\nusing deep learning to model consumer preferences. Traditional econometric\nmethods often struggle with limited within-product price variation, a challenge\naddressed by the proposed neural network approach. The proposed method\nestimates the functional form of the demand and demonstrates higher performance\nin both simulations and empirical applications. Notably, under low price\nvariation, the machine learning model outperforms econometric approaches,\nreducing the mean squared error of initial price parameter estimates by nearly\nthreefold. In empirical setting, the ML model consistently predicts a negative\nrelationship between demand and price in 100% of cases, whereas the econometric\napproach fails to do so in 20% of cases. The suggested model incorporates a\nwide range of product characteristics, as well as prices of other products and\ncompetitors.",
      "generated_abstract": "r proposes a deep learning-based approach for estimating the\ndemand of goods and services in a retail environment. The proposed model\nintegrates the concept of Dynamic Programming to estimate the demand of goods\nand services. The model employs the Convolutional Neural Networks (CNN) for\nestimating the demand of goods and services. The CNN is trained using the\nhistorical data to predict the demand of goods and services. The model\nassesses the quality of the predicted demand by comparing it with the actual\ndemand and by evaluating its accuracy. The model is also integrated with a\nDynamic Programming (DP) algorithm to estimate the price of goods and services.\nThe DP algorithm is trained using historical data to estimate the price of goods\nand services. The model assesses the quality of the price estimate by\ncomparing it with the actual price and by evaluating its accuracy. The model is\nalso",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17582417582417584,
          "p": 0.2909090909090909,
          "f": 0.21917807749577792
        },
        "rouge-2": {
          "r": 0.031007751937984496,
          "p": 0.04597701149425287,
          "f": 0.037037032226080874
        },
        "rouge-l": {
          "r": 0.16483516483516483,
          "p": 0.2727272727272727,
          "f": 0.20547944735879164
        }
      }
    },
    {
      "paper_id": "math.AG.math/AC/2503.01752v1",
      "true_abstract": "Border basis schemes are open subschemes of the Hilbert scheme of $\\mu$\npoints in an affine space $\\mathbb{A}^n$. They have easily describable systems\nof generators of their vanishing ideals for a natural embedding into a large\naffine space $\\mathbb{A}^{\\mu\\nu}$. Here we bring together several techniques\nfor re-embedding affine schemes into lower dimensional spaces which we\ndeveloped in the last years. We study their efficacy for some special types of\nborder basis schemes such as MaxDeg border basis schemes, L-shape and\nsimplicial border basis schemes, as well as planar border basis schemes. A\nparticular care is taken to make these re-embeddings efficiently computable and\nto check when we actually get an isomorphism with $\\mathbb{A}^{n\\mu}$, i.e.,\nwhen the border basis scheme is an affine cell.",
      "generated_abstract": "We study the re-embeddings of special border basis schemes over a\nfinite field. In particular, we give a complete description of the re-embeddings\nof special border basis schemes over a finite field with $n$ elements for\n$n\\leq 500$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15294117647058825,
          "p": 0.52,
          "f": 0.2363636328512397
        },
        "rouge-2": {
          "r": 0.034782608695652174,
          "p": 0.14814814814814814,
          "f": 0.056338025089268176
        },
        "rouge-l": {
          "r": 0.12941176470588237,
          "p": 0.44,
          "f": 0.19999999648760333
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.nlin/CD/2503.08905v1",
      "true_abstract": "Sub-Neptunes have been found to be one of the most common types of\nexoplanets, yet their physical parameters and properties are poorly determined\nand in need of further investigation. In order to improve the mass measurement\nand parameter determination of two sub-Neptunes, K2-266 d and K2-266 e, we\npresent new transit observations obtained with CHaracterising ExOPlanets\nSatellite (CHEOPS) and Transiting Exoplanet Survey Satellite (TESS), increasing\nthe baseline of transit data from a few epochs to 165 epochs for K2-266 d, and\nto 121 epochs for K2-266 e. Through a two-stage fitting process, it is found\nthat the masses of K2-266 d and K2-266 e are 6.01$\\pm$0.43 $M_\\oplus$ and\n7.70$\\pm$0.58 $M_\\oplus$, respectively. With these updated values and one order\nof magnitude better precision, we confirm the planets to belong to the\npopulation of planets that has been determined to be volatile-rich. Finally, we\npresent the results of dynamical simulations, showing that the system is\nstable, the orbits are not chaotic, and that these two planets are close to but\nnot in 4:3 mean motion resonance.",
      "generated_abstract": "6 planetary system consists of two sub-Neptunes orbiting a\nsub-Galactic star. A high-precision mass measurement of the sub-Neptune\nsub-planet K2-266b is required to study its physical and chemical properties.\nThe K2-266 system is relatively close, with an orbital period of 24.5 hours and\na semi-major axis of 0.0297 AU. We present new light curves from the K2-266\ntransit campaign, which covers 452 days of observations from 2022 August 1 to\n2024 May 25. Our new light curves enable us to measure the transit times of\nthe sub-Neptunes with a precision of $\\sim 2$ hours. We have derived the\norbital period of the sub-Nept",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1891891891891892,
          "p": 0.28378378378378377,
          "f": 0.22702702222702714
        },
        "rouge-2": {
          "r": 0.02976190476190476,
          "p": 0.05102040816326531,
          "f": 0.03759398030866697
        },
        "rouge-l": {
          "r": 0.18018018018018017,
          "p": 0.2702702702702703,
          "f": 0.21621621141621633
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.08457v1",
      "true_abstract": "This paper explores foliated differential graded algebras (dga) and their\nrole in extending fundamental theorems of differential geometry to foliations.\nWe establish an $A_{\\infty}$ de Rham theorem for foliations, demonstrating that\nthe classical quasi-isomorphism between singular cochains and de Rham forms\nlifts to an $A_{\\infty}$ quasi-isomorphism in the foliated setting.\nFurthermore, we investigate the Riemann-Hilbert correspondence for foliations,\nbuilding upon the established higher Riemann-Hilbert correspondence for\nmanifolds. By constructing an integration functor, we prove a higher\nRiemann-Hilbert correspondence for foliations, revealing an equivalence between\n$\\infty$-representations of $L_{\\infty}$-algebroids and\n$\\infty$-representations of Lie $\\infty$-groupoids within the context of\nfoliations. This work generalizes the classical Riemann-Hilbert correspondence\nto foliations, providing a deeper understanding of the relationship between\nrepresentations of Lie algebroids and Lie groupoids in this framework.",
      "generated_abstract": "In this paper, we construct a Riemann-Hilbert correspondence between\ntwo families of foliations: the family of foliations generated by a given\npseudo-Riemannian metric and the family of foliations generated by a given\npseudo-Riemannian metric and an $H^1$-closed $1$-form. We show that the\ncorrespondence is one-to-one and the inverse map is given by a certain\nRiemannian metric. This is a generalization of the correspondence between\nfoliations and holomorphic Riemannian manifolds due to Schoen.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21621621621621623,
          "p": 0.3902439024390244,
          "f": 0.2782608649769377
        },
        "rouge-2": {
          "r": 0.03636363636363636,
          "p": 0.07017543859649122,
          "f": 0.047904187120370464
        },
        "rouge-l": {
          "r": 0.1891891891891892,
          "p": 0.34146341463414637,
          "f": 0.2434782562812855
        }
      }
    },
    {
      "paper_id": "math.QA.math/QA/2503.08839v1",
      "true_abstract": "We introduce a new topological coproduct $\\Delta^{\\psi}_{u}$ for quantum\ntoroidal algebras $U_{q}(\\mathfrak{g}_{\\mathrm{tor}})$ in untwisted types,\nleading to a well-defined tensor product on the category\n$\\widehat{\\mathcal{O}}_{\\mathrm{int}}$ of integrable representations. This is\ndefined by twisting the Drinfeld coproduct $\\Delta_{u}$ with an anti-involution\n$\\psi$ of $U_{q}(\\mathfrak{g}_{\\mathrm{tor}})$ that swaps its horizontal and\nvertical quantum affine subalgebras. Other applications of $\\psi$ include\ngeneralising the celebrated Miki automorphism from type $A$, and an action of\nthe universal cover of $SL_{2}(\\mathbb{Z})$.\n  Next, we investigate the ensuing tensor representations of\n$U_{q}(\\mathfrak{g}_{\\mathrm{tor}})$, and prove quantum toroidal analogues for\na series of influential results by Chari-Pressley on the affine level. In\nparticular, there is a compatibility with Drinfeld polynomials, and the product\nof irreducibles is generically irreducible. Furthermore, we obtain $R$-matrices\nwith spectral parameter which provide solutions to the (trigonometric, quantum)\nYang-Baxter equation, and endow $\\widehat{\\mathcal{O}}_{\\mathrm{int}}$ with a\nmeromorphic braiding. These moreover give rise to a commuting family of\ntransfer matrices for each module.",
      "generated_abstract": "We construct a tensor product for quantum toroidal algebras over a field\n$k$ of positive characteristic $p$. This tensor product is defined by the\ncomposition of the tensor products of quantum affine algebras with a\n$k$-linear $R$-matrix. As a consequence, we obtain a quantum toroidal\nalgebra over $k$ from a quantum affine algebra over $k$ by taking the\ntensor product with an appropriate $R$-matrix. We give examples of quantum\naffine algebras over $k$ which can be used to construct quantum toroidal\nalgebras by applying our construction. We also discuss some general properties\nof the tensor products of quantum affine algebras with a $k$-linear $R$-matrix.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21904761904761905,
          "p": 0.45098039215686275,
          "f": 0.2948717904709073
        },
        "rouge-2": {
          "r": 0.07333333333333333,
          "p": 0.14285714285714285,
          "f": 0.09691629507655904
        },
        "rouge-l": {
          "r": 0.21904761904761905,
          "p": 0.45098039215686275,
          "f": 0.2948717904709073
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2502.14160v1",
      "true_abstract": "In this paper, we study inverse game theory (resp. inverse multiagent\nlearning) in which the goal is to find parameters of a game's payoff functions\nfor which the expected (resp. sampled) behavior is an equilibrium. We formulate\nthese problems as generative-adversarial (i.e., min-max) optimization problems,\nfor which we develop polynomial-time algorithms to solve, the former of which\nrelies on an exact first-order oracle, and the latter, a stochastic one. We\nextend our approach to solve inverse multiagent simulacral learning in\npolynomial time and number of samples. In these problems, we seek a simulacrum,\nmeaning parameters and an associated equilibrium that replicate the given\nobservations in expectation. We find that our approach outperforms the\nwidely-used ARIMA method in predicting prices in Spanish electricity markets\nbased on time-series data.",
      "generated_abstract": "This paper investigates the problem of multiagent learning in a Bayesian\nnetwork structure. We propose a novel learning algorithm based on the\nprojection onto the nullspace of the Bellman operator, which we refer to as the\nProjection-Based Bayesian Inverse Reinforcement Learning (PBBIRL) method. The\nproposed method is shown to be both efficient and adaptive to the Bayesian\nnetwork structure. Additionally, we propose a novel method for sampling from the\nBayesian network structure, which we refer to as the Projection-Based\nBayesian Inverse Partial Information Sampling (PBBIPIS) method.\n  The PBBIRL method is shown to be efficient and adaptive to the Bayesian\nnetwork structure. Additionally, the PBBIPIS method is shown to be both\nefficient and adaptive to the Bayesian network structure.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19540229885057472,
          "p": 0.3090909090909091,
          "f": 0.23943661497222782
        },
        "rouge-2": {
          "r": 0.024193548387096774,
          "p": 0.03896103896103896,
          "f": 0.029850741542041793
        },
        "rouge-l": {
          "r": 0.19540229885057472,
          "p": 0.3090909090909091,
          "f": 0.23943661497222782
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/GN/2404.15226v3",
      "true_abstract": "We revisit granular models that represent the size of a firm as the sum of\nthe sizes of multiple constituents or sub-units. Originally developed to\naddress the unexpectedly slow reduction in volatility as firm size increases,\nthese models also explain the shape of the distribution of firm growth rates.\n  We introduce new theoretical insights regarding the relationship between firm\nsize and growth rate statistics within this framework, directly linking the\ngrowth statistics of a firm to how diversified it is. The non-intuitive nature\nof our results arises from the fat-tailed distributions of the size and the\nnumber of sub-units, which suggest the categorization of firms into three\ndistinct diversification types: well-diversified firms with sizes evenly\ndistributed across many sub-units, firms with many sub-units but concentrated\nsize in just a few, and poorly diversified firms consisting of only a small\nnumber of sub-units.\n  Inspired by our theoretical findings, we identify new empirical patterns in\nfirm growth. Our findings show that growth volatility, when adjusted by average\nsize-conditioned volatility, has a size-independent distribution, but with a\ntail that is much too thin to be in agreement with the predictions of granular\nmodels. Furthermore, the predicted Gaussian distribution of growth rates, even\nwhen rescaled for firm-specific volatility, remains fat-tailed across all\nsizes. Such discrepancies not only challenge the granularity hypothesis but\nalso underscore the need for deeper exploration into the mechanisms driving\nfirm growth.",
      "generated_abstract": "t the granular model of firm growth in a two-sector economy, where\nthe labor and capital stocks are assumed to be heterogeneous and nonlinear. We\nshow that the model can be interpreted as a special case of a generalized\nWinnowing model, and we present new insights on the effects of heterogeneity on\nthe growth rate of a firm. We also examine the sensitivity of the model to\ncertain parameter choices, and we find that the growth rate of a firm depends\nstrongly on the parameter $r$, the share of the labor stock in the capital\nstock, and on the parameter $\\theta$, the level of the capital stock. We\nconclude that the granular model of firm growth is not a generalization of the\nWinnowing model, but rather a special case of it. The granular model is also\nillustrated by a simple case where the capital stock is constant over time,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18518518518518517,
          "p": 0.35714285714285715,
          "f": 0.24390243452706728
        },
        "rouge-2": {
          "r": 0.041474654377880185,
          "p": 0.07758620689655173,
          "f": 0.05405404951401891
        },
        "rouge-l": {
          "r": 0.17037037037037037,
          "p": 0.32857142857142857,
          "f": 0.2243902394051161
        }
      }
    },
    {
      "paper_id": "quant-ph.math/IT/2503.08736v1",
      "true_abstract": "We provide a streamlined elaboration on existing ideas that link Ising anyon\n(or equivalently, Majorana) stabilizer codes to certain classes of binary\nclassical codes. The groundwork for such Majorana-based quantum codes can be\nfound in earlier works (including, for example, Bravyi (arXiv:1004.3791) and\nVijay et al. (arXiv:1703.00459)), where it was observed that commuting families\nof fermionic (Clifford) operators can can often be systematically lifted from\nweakly self-dual or self-orthogonal binary codes. Here, we recast and unify\nthese ideas into a classification theorem that explicitly shows how explicitly\nshows how q-isotropic subspaces in $\\mathbb{F}_2^{2n}$ yield commuting Clifford\noperators relevant to Ising anyons, and how these subspaces naturally\ncorrespond to punctured self-orthogonal codes in $\\mathbb{F}_2^{2n+1}$.",
      "generated_abstract": "We introduce a novel family of stabilizer codes based on Clifford\nstabilizer algebras. By using the method of noncommutative algebraic\ntransformation, we construct a stabilizer code that can be realized as a\nclassical code, while encoding and decoding operations are performed in the\ngap of a Clifford group operation. Moreover, we show that the code can be\nrealized as a stabilizer code with the same error-correcting capacity as a\nclassical code. We also give a brief review of the error-correcting codes for\nIsing anyons and demonstrate that the stabilizer code has a similar structure\nas the Ising anyon error-correcting codes.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.29310344827586204,
          "f": 0.23776223294048618
        },
        "rouge-2": {
          "r": 0.026785714285714284,
          "p": 0.03409090909090909,
          "f": 0.029999995072000806
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.25862068965517243,
          "f": 0.20979020496845827
        }
      }
    },
    {
      "paper_id": "cs.CL.econ/GN/2502.14497v1",
      "true_abstract": "Macroeconomic fluctuations and the narratives that shape them form a mutually\nreinforcing cycle: public discourse can spur behavioural changes leading to\neconomic shifts, which then result in changes in the stories that propagate. We\nshow that shifts in semantic embedding space can be causally linked to\nfinancial market shocks -- deviations from the expected market behaviour.\nFurthermore, we show how partisanship can influence the predictive power of\ntext for market fluctuations and shape reactions to those same shocks. We also\nprovide some evidence that text-based signals are particularly salient during\nunexpected events such as COVID-19, highlighting the value of language data as\nan exogenous variable in economic forecasting. Our findings underscore the\nbidirectional relationship between news outlets and market shocks, offering a\nnovel empirical approach to studying their effect on each other.",
      "generated_abstract": "d of misinformation is a growing threat to democracy. While\ncommunication platforms are often seen as neutral platforms, recent studies\nhave shown that they can amplify political messages and promote polarization.\nWhile this phenomenon is well-documented, the mechanisms behind it are not\nfully understood. In this work, we examine how political polarization and\nmisinformation spread in social media networks through the lens of causal\ninference. Specifically, we investigate how a political story spreads through a\nnetwork of social media users, with the goal of understanding how partisan\ngroups influence each other's political beliefs and how these beliefs influence\nthe spread of misinformation. We use a novel causal modeling framework to\nanalyze the network of social media users, which includes 15 million users from\nFacebook, Instagram, and WhatsApp and 5.2 million users from YouTube, and the\ndistribution of political",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1782178217821782,
          "p": 0.2,
          "f": 0.18848167040925426
        },
        "rouge-2": {
          "r": 0.023076923076923078,
          "p": 0.023076923076923078,
          "f": 0.023076918076924163
        },
        "rouge-l": {
          "r": 0.15841584158415842,
          "p": 0.17777777777777778,
          "f": 0.16753926203229094
        }
      }
    },
    {
      "paper_id": "math-ph.math/SP/2503.08595v1",
      "true_abstract": "In this expository note, we study several families of periodic graphs which\nsatisfy a sufficient condition for the ergodicity of the associated\ncontinuous-time quantum walk. For these graphs, we compute the limiting\ndistribution of the walk explicitly. We uncover interesting behavior where in\nsome families, the walk is ergodic in both horizontal and sectional directions,\nwhile in others, ergodicity only holds in the horizontal (large N) direction.\nWe compare this to the limiting distribution of classical random walks on the\nsame graphs.",
      "generated_abstract": "We derive the limiting distribution of the walker's position in a\nperiodic quantum walk on a graph, which can be viewed as a quantum version of\nthe classical random walk on the complete graph. The quantum walk is\nergodic and is governed by a Hamiltonian which has a local Hamiltonian\ninteraction between the walker and the environment. We show that the walker's\nposition in the limiting distribution is the same as the position of the walker\nin the limiting distribution of the random walk on the complete graph.\nAdditionally, we show that the walker's position in the limiting distribution\nis independent of the choice of the initial position. We also study the\ndistribution of the walker's position in the limiting distribution for a wide\nclass of quantum walks on periodic graphs.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3898305084745763,
          "p": 0.4339622641509434,
          "f": 0.4107142807286353
        },
        "rouge-2": {
          "r": 0.17105263157894737,
          "p": 0.14285714285714285,
          "f": 0.15568861779482965
        },
        "rouge-l": {
          "r": 0.3559322033898305,
          "p": 0.39622641509433965,
          "f": 0.37499999501434955
        }
      }
    },
    {
      "paper_id": "math.CA.math/FA/2503.10190v1",
      "true_abstract": "In a famous paper published in 1904, Helge von Koch introduced the curve that\nstill serves nowadays as an iconic representation of fractal shapes. In fact,\nvon Koch's main goal was the construction of a continuous but nowhere\ndifferentiable function, very similar to the snowflake, using elementary\ngeometric procedures, and not analytical formulae. We prove that a parametrized\nfamily of functions (including and) generalizing von Koch's example enjoys a\nrich multifractal behavior, thus enriching the class of historical mathematical\nobjects having surprising regularity properties. The analysis relies on the\nstudy of the orbits of an underlying dynamical system and on the introduction\nof self-similar measures and non-trivial iterated functions systems adapted to\nthe problem.",
      "generated_abstract": "We consider a family of parametrized functions defined on a compact\n(possibly singular) Riemannian manifold, and we show that the\n$K_1$-invariant of the family is the sum of the $K_1$-invariants of the\ncorresponding parametrized families of von Koch curves. In particular, the\nmultifractal nature of the family of parametrized functions is inherited by\nthe parametrized family of von Koch curves.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16091954022988506,
          "p": 0.4117647058823529,
          "f": 0.23140495463697838
        },
        "rouge-2": {
          "r": 0.03636363636363636,
          "p": 0.08333333333333333,
          "f": 0.050632907162314084
        },
        "rouge-l": {
          "r": 0.11494252873563218,
          "p": 0.29411764705882354,
          "f": 0.1652892521576396
        }
      }
    },
    {
      "paper_id": "math.PR.stat/TH/2502.15116v1",
      "true_abstract": "We introduce an empirical functional $\\Psi$ that is an optimal uniform mean\nestimator: Let $F\\subset L_2(\\mu)$ be a class of mean zero functions, $u$ is a\nreal valued function, and $X_1,\\dots,X_N$ are independent, distributed\naccording to $\\mu$. We show that under minimal assumptions, with $\\mu^{\\otimes\nN}$ exponentially high probability, \\[ \\sup_{f\\in F} |\\Psi(X_1,\\dots,X_N,f) -\n\\mathbb{E} u(f(X))| \\leq c R(F) \\frac{ \\mathbb{E} \\sup_{f\\in F } |G_f| }{\\sqrt\nN}, \\] where $(G_f)_{f\\in F}$ is the gaussian processes indexed by $F$ and\n$R(F)$ is an appropriate notion of `diameter' of the class $\\{u(f(X)) : f\\in\nF\\}$.\n  The fact that such a bound is possible is surprising, and it leads to the\nsolution of various key problems in high dimensional probability and high\ndimensional statistics. The construction is based on combining Talagrand's\ngeneric chaining mechanism with optimal mean estimation procedures for a single\nreal-valued random variable.",
      "generated_abstract": "We consider the problem of uniform mean estimation under a Gaussian\nconditioning. We show that the optimal chaining algorithm for this problem\nrequires an unbounded number of samples. We then propose a method that\napproximates the optimal chaining algorithm and proves that its sample complexity\nis at least the number of samples required by the optimal algorithm. This\napproximation is based on a novel concentration inequality for the empirical\ndistribution function of the estimator. The proposed method is implemented in\nthe R package RUGM.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2037037037037037,
          "p": 0.39285714285714285,
          "f": 0.26829267842950627
        },
        "rouge-2": {
          "r": 0.050359712230215826,
          "p": 0.09090909090909091,
          "f": 0.06481481022676644
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.32142857142857145,
          "f": 0.2195121906246283
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/MN/2408.08503v1",
      "true_abstract": "Research organisms provide invaluable insights into human biology and\ndiseases, serving as essential tools for functional experiments, disease\nmodeling, and drug testing. However, evolutionary divergence between humans and\nresearch organisms hinders effective knowledge transfer across species. Here,\nwe review state-of-the-art methods for computationally transferring knowledge\nacross species, primarily focusing on methods that utilize transcriptome data\nand/or molecular networks. We introduce the term \"agnology\" to describe the\nfunctional equivalence of molecular components regardless of evolutionary\norigin, as this concept is becoming pervasive in integrative data-driven models\nwhere the role of evolutionary origin can become unclear. Our review addresses\nfour key areas of information and knowledge transfer across species: (1)\ntransferring disease and gene annotation knowledge, (2) identifying agnologous\nmolecular components, (3) inferring equivalent perturbed genes or gene sets,\nand (4) identifying agnologous cell types. We conclude with an outlook on\nfuture directions and several key challenges that remain in cross-species\nknowledge transfer.",
      "generated_abstract": "opment of precision medicines that target specific diseases and\ndisorders, particularly in oncology, remains a major challenge in clinical\npractice. Accelerating translational research has been crucial to improve\nprecision medicine but has also necessitated the development of advanced\nbiomarkers, which are critical for precision medicine. Biomarkers are\nindividualized molecular markers that can identify specific patients at risk for\ndisease progression and response to treatment. In cancer research, biomarkers\ncan be used to identify cancer cells that are more likely to respond to a\ntreatment, and to identify cells that are resistant to the treatment. In\nclinical practice, biomarkers can help to select the right patient for\ntreatment and can guide treatment decisions. Therefore, it is essential that\nbiomarkers are accurate, sensitive, and specific. The use of biomarkers",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11504424778761062,
          "p": 0.16455696202531644,
          "f": 0.13541666182345938
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11504424778761062,
          "p": 0.16455696202531644,
          "f": 0.13541666182345938
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/RM/2412.09662v1",
      "true_abstract": "This article analytically characterizes the impermanent loss for automatic\nmarket makers in decentralized exchanges such as Uniswap or Balancer (CPMM). We\npresent a theoretical static replication formula for the pool value using a\ncombination of European calls and puts. We will formulate a result to guarantee\ncoverage for any final price that falls within a predefined range.",
      "generated_abstract": "This paper proposes a novel model to study the hedging of impermanent\nlosses in crypto markets. The model is based on the concept of a replica of\nthe market value of a pool (CPM). The replica is a financial instrument that\nfollows a stochastic process, and the hedging of impermanent losses is a\nstrategy based on the use of this replica. This paper describes the model in\ndetail, and we present the results of numerical simulations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26,
          "p": 0.3023255813953488,
          "f": 0.2795698875014453
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.22,
          "p": 0.2558139534883721,
          "f": 0.23655913481327334
        }
      }
    },
    {
      "paper_id": "nucl-th.nucl-ex/2503.09045v1",
      "true_abstract": "Electromagnetic (EM) probes, including photons and dileptons, do not interact\nstrongly after their production in heavy-ion collisions, allowing them to carry\nundistorted information from their points of origin. This makes them powerful\ntools for studying early-stage equilibration and the thermodynamic properties\nof the quark-gluon plasma (QGP). In these proceedings, we highlight recent\ntheoretical advancements in EM probes, focusing on their role in probing\nearly-stage dynamics and extracting medium properties. We also discuss the\nemerging multimessenger approach, which combines hadronic and electromagnetic\nprobes to achieve a more comprehensive understanding of the QGP.",
      "generated_abstract": "ew addresses the recent advancements in multimessenger approaches to\ninvestigate heavy-ion collisions, focusing on electromagnetic probe observations\nand their impact on the phenomenology of the collision. These experiments\ndemonstrate that the electromagnetic signals from heavy-ion collisions are\nsignificant and can be used to enhance our understanding of the collision\nprocess. The review starts by discussing the origin of the electromagnetic\nsignals, such as the nuclear de-excitation, and the different methods used to\nobtain them. Subsequently, the different types of electromagnetic signatures\nwill be reviewed, focusing on the different experiments that can provide them.\nThe main results from these experiments will be presented, focusing on the\ndifferent signatures obtained, their origin, and their impact on the\nunderstanding of the collision process. Finally, the future challenges and\nopport",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25675675675675674,
          "p": 0.2714285714285714,
          "f": 0.26388888389274695
        },
        "rouge-2": {
          "r": 0.06741573033707865,
          "p": 0.057692307692307696,
          "f": 0.06217616083331136
        },
        "rouge-l": {
          "r": 0.22972972972972974,
          "p": 0.24285714285714285,
          "f": 0.23611110611496927
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.04407v1",
      "true_abstract": "In this paper, we propose a movable antenna (MA)-enabled frequency-hopping\n(FH) multiple-input multiple-output (MIMO) radar system and investigate its\nsensing resolution. Specifically, we derive the expression of the ambiguity\nfunction and analyze the relationship between its main lobe width and the\ntransmit antenna positions. In particular, the optimal antenna distribution to\nachieve the minimum main lobe width in the angular domain is characterized. We\ndiscover that this minimum width is related to the antenna size, the antenna\nnumber, and the target angle. Meanwhile, we present lower bounds of the\nambiguity function in the Doppler and delay domains, and show that the impact\nof the antenna size on the radar performance in these two domains is very\ndifferent from that in the angular domain. Moreover, the performance\nenhancement brought by MAs exhibits a certain trade-off between the main lobe\nwidth and the side lobe peak levels. Therefore, we propose to balance between\nminimizing the side lobe levels and narrowing the main lobe of the ambiguity\nfunction by optimizing the antenna positions. To achieve this goal, we propose\na low-complexity algorithm based on the Rosen's gradient projection method, and\nshow that its performance is very close to the baseline. Simulation results are\npresented to validate the theoretical analysis on the properties of the\nambiguity function, and demonstrate that MAs can reduce the main lobe width and\nsuppress the side lobe levels of the ambiguity function, thereby enhancing\nradar performance.",
      "generated_abstract": "-hopping spread spectrum (FHSS) radar is a well-known radar system\nthat employs multiple transmit antennas and multiple receive antennas to\nachieve high spatial resolution and high sensitivity. However, the performance\ndegradation of FHSS radar is caused by the ambiguity of the frequency-hopping\npattern, which is a critical issue. In this paper, we propose a novel\nfrequency-hopping-based radar system with movable antennas. Unlike existing\nfrequency-hopping radar systems, this radar system has a movable antenna\nconfiguration. By adjusting the antenna position, the radar can achieve a\ngreater sensitivity and better spatial resolution. The proposed system is\ndesigned based on the FHSS principle, which can be realized by utilizing the\nspread spectrum technique and the frequency-hopping scheme. The proposed\nfrequency-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22608695652173913,
          "p": 0.37681159420289856,
          "f": 0.28260869096467395
        },
        "rouge-2": {
          "r": 0.09042553191489362,
          "p": 0.1574074074074074,
          "f": 0.11486486023009515
        },
        "rouge-l": {
          "r": 0.20869565217391303,
          "p": 0.34782608695652173,
          "f": 0.26086956052989135
        }
      }
    },
    {
      "paper_id": "math.FA.math/CA/2503.04285v1",
      "true_abstract": "We introduce the space SBV$_X$ of special functions with bounded\n$X$-variation in Carnot-Carath\\'eodory spaces and study its main properties.\nOur main outcome is an approximation result, with respect to the BV$_X$\ntopology, for SBV$_X$ functions.",
      "generated_abstract": "f functions defined on a closed subset of the Euclidean space is\ncalled a SBV function if it is non-decreasing and satisfies a certain\nnon-homogeneous condition. This paper studies a class of SBV functions defined\non a closed subset of the Carnot-Carath\\'eodory space. It is proved that the\nclass is closed under finite unions and complements. It is also shown that\nthese functions are well defined and that the set of SBV functions is dense in\nthe class of all Carnot-Carath\\'eodory functions.\n  The class of SBV functions on a closed subset of the Carnot-Carath\\'eodory\nspace is also considered in the setting of non-homogeneous Carnot groups. In\nthis case, the set of SBV functions is also closed under finite unions and\ncomplements. However, it is shown that this set",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26666666666666666,
          "p": 0.1509433962264151,
          "f": 0.19277107972129492
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.26666666666666666,
          "p": 0.1509433962264151,
          "f": 0.19277107972129492
        }
      }
    },
    {
      "paper_id": "math.RT.math/RT/2503.08020v2",
      "true_abstract": "We introduce weighted cycles on weaves of general Dynkin types and define a\nskew-symmetrizable intersection pairing between weighted cycles. We prove that\nweighted cycles on a weave form a Laurent polynomial algebra and construct a\nquantization for this algebra using the skew-symmetric intersection pairing in\nthe simply-laced case. We define merodromies along weighted cycles as functions\non the decorated flag moduli space of the weave. We relate weighted cycles with\ncluster variables in a cluster algebra and prove that mutations of weighted\ncycles are compatible with mutations of cluster variables.",
      "generated_abstract": "In this paper, we construct a set of weighted cycles on weaves, which are\ndensely packed, connected, and have finite sums of lengths. Furthermore, we\nprove the existence of these cycles. We give the details of our construction in\nSection 2 and prove the existence of these cycles in Section 3.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2653061224489796,
          "p": 0.3611111111111111,
          "f": 0.3058823480581315
        },
        "rouge-2": {
          "r": 0.07692307692307693,
          "p": 0.13636363636363635,
          "f": 0.09836065112604161
        },
        "rouge-l": {
          "r": 0.22448979591836735,
          "p": 0.3055555555555556,
          "f": 0.25882352452871976
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/OS/2503.05117v1",
      "true_abstract": "This paper presents HyperGraph ROS, an open-source robot operating system\nthat unifies intra-process, inter-process, and cross-device computation into a\ncomputational hypergraph for efficient message passing and parallel execution.\nIn order to optimize communication, HyperGraph ROS dynamically selects the\noptimal communication mechanism while maintaining a consistent API. For\nintra-process messages, Intel-TBB Flow Graph is used with C++ pointer passing,\nwhich ensures zero memory copying and instant delivery. Meanwhile,\ninter-process and cross-device communication seamlessly switch to ZeroMQ. When\na node receives a message from any source, it is immediately activated and\nscheduled for parallel execution by Intel-TBB. The computational hypergraph\nconsists of nodes represented by TBB flow graph nodes and edges formed by TBB\npointer-based connections for intra-process communication, as well as ZeroMQ\nlinks for inter-process and cross-device communication. This structure enables\nseamless distributed parallelism. Additionally, HyperGraph ROS provides\nROS-like utilities such as a parameter server, a coordinate transformation\ntree, and visualization tools. Evaluation in diverse robotic scenarios\ndemonstrates significantly higher transmission and throughput efficiency\ncompared to ROS 2. Our work is available at\nhttps://github.com/wujiazheng2020a/hyper_graph_ros.",
      "generated_abstract": "r presents HyperGraph ROS, an open-source robot operating system\n(ROS) that enables the integration of parallel computing in robots. HyperGraph\nROS enables efficient and scalable parallel processing for both static and\ndynamic simulation scenarios. HyperGraph ROS supports both C++17 and C++23,\nenabling the use of modern C++ features, such as template metaprogramming and\nmodern containers. HyperGraph ROS also includes a high-performance computing\nengine that can handle high-throughput and high-bandwidth operations. The\nHyperGraph ROS project includes a set of libraries that are used by\nHyperGraphROS to support the computation of parallel operations. The HyperGraph\nROS library is designed to provide high performance and scalability by\noptimizing the execution of operations. Additionally, HyperGraph ROS includes\ntools to manage the scheduling of parallel operations and the execution of\nth",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24806201550387597,
          "p": 0.41025641025641024,
          "f": 0.3091787392648604
        },
        "rouge-2": {
          "r": 0.06060606060606061,
          "p": 0.08928571428571429,
          "f": 0.0722021612480291
        },
        "rouge-l": {
          "r": 0.24031007751937986,
          "p": 0.3974358974358974,
          "f": 0.2995169035160681
        }
      }
    },
    {
      "paper_id": "math.ST.q-bio/PE/2501.17622v1",
      "true_abstract": "We study the optimization landscape of maximum likelihood estimation for a\nbinary latent tree model with hidden variables at internal nodes and observed\nvariables at the leaves. This model, known as the Cavender-Farris-Neyman (CFN)\nmodel in statistical phylogenetics, is also a special case of the ferromagnetic\nIsing model. While the likelihood function is known to be non-concave with\nmultiple critical points in general, gradient descent-type optimization methods\nhave proven surprisingly effective in practice. We provide theoretical insights\ninto this phenomenon by analyzing the population likelihood landscape in a\nneighborhood of the true parameter vector. Under some conditions on the edge\nparameters, we show that the expected log-likelihood is strongly concave and\nsmooth in a box around the true parameter whose size is independent of both the\ntree topology and number of leaves. The key technical contribution is a careful\nanalysis of the expected Hessian, showing that its diagonal entries are large\nwhile its off-diagonal entries decay exponentially in the graph distance\nbetween the corresponding edges. These results provide the first rigorous\ntheoretical evidence for the effectiveness of optimization methods in this\nsetting and may suggest broader principles for understanding optimization in\nlatent variable models on trees.",
      "generated_abstract": "the likelihood landscape of a binary latent model, with a single\nbinary latent variable and binary conditional independence structure, on a\ntree-like graph. We show that the likelihood of the binary latent model on a\ntree is maximized along a unique local minimum of the potential function of the\nbinary latent variable. We also show that the likelihood of the binary latent\nmodel is maximized along a unique local maximum of the potential function of the\nbinary latent variable. Finally, we show that the likelihood of the binary\nlatent model on a tree is maximized along a unique local minimum of the\nconditional independence structure. We also show that the likelihood of the\nbinary latent model on a tree is maximized along a unique local maximum of the\nconditional independence structure. We conclude that the likelihood landscape\nof the binary latent model on a tree is the same as the likelihood landscape of\nthe",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1875,
          "p": 0.6153846153846154,
          "f": 0.28742514612069275
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.18333333333333332,
          "f": 0.08906882223278549
        },
        "rouge-l": {
          "r": 0.1484375,
          "p": 0.48717948717948717,
          "f": 0.22754490659973467
        }
      }
    },
    {
      "paper_id": "math.HO.math/HO/2502.11145v1",
      "true_abstract": "The studies of Bonaventura Cavalieri's indivisibles by Giusti, Andersen,\nMancosu and others provide a comprehensive picture of Cavalieri's mathematics,\nas well as of the mathematical objections to it as formulated by Paul Guldin\nand other critics. An issue that has been studied in less detail concerns the\ntheological underpinnings of the contemporary debate over indivisibles, its\nhistorical roots, the geopolitical situation at the time, and its relation to\nthe ultimate suppression of Cavalieri's religious order. We analyze sources\nfrom the 17th through 21st centuries to investigate such a relation.",
      "generated_abstract": "This paper is a contribution to the study of the relationship between the\nconjectural existence of an indivisible prime and the existence of a pasha.\nWe show that, if the existence of an indivisible prime is true, then the\nexistence of a pasha is also true.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08955223880597014,
          "p": 0.23076923076923078,
          "f": 0.12903225403630492
        },
        "rouge-2": {
          "r": 0.023255813953488372,
          "p": 0.05714285714285714,
          "f": 0.033057847127928926
        },
        "rouge-l": {
          "r": 0.08955223880597014,
          "p": 0.23076923076923078,
          "f": 0.12903225403630492
        }
      }
    },
    {
      "paper_id": "cs.HC.stat/CO/2502.08114v2",
      "true_abstract": "The rapid proliferation of data science forced different groups of\nindividuals with different backgrounds to adapt to statistical analysis. We\nhypothesize that conversational agents are better suited for statistical\nanalysis than traditional graphical user interfaces (GUI). In this work, we\npropose a novel conversational agent, StatZ, for statistical analysis. We\nevaluate the efficacy of StatZ relative to established statistical\nsoftware:SPSS, SAS, Stata, and JMP in terms of accuracy, task completion time,\nuser experience, and user satisfaction. We combined the proposed analysis\nquestion from state-of-the-art language models with suggestions from\nstatistical analysis experts and tested with 51 participants from diverse\nbackgrounds. Our experimental design assessed each participant's ability to\nperform statistical analysis tasks using traditional statistical analysis tools\nwith GUI and our conversational agent. Results indicate that the proposed\nconversational agents significantly outperform GUI statistical software in all\nassessed metrics, including quantitative (task completion time, accuracy, and\nuser experience), and qualitative (user satisfaction) metrics. Our findings\nunderscore the potential of using conversational agents to enhance statistical\nanalysis processes, reducing cognitive load and learning curves and thereby\nproliferating data analysis capabilities, to individuals with limited knowledge\nof statistics.",
      "generated_abstract": "years, conversational agents have been gaining popularity in the\nstatistical analysis field. These agents, equipped with advanced AI and\nmachine learning capabilities, have the potential to revolutionize the way we\ninteract with data. However, there is a lack of research exploring the effect\nof conversational agents on statistical analysis. In this paper, we present\nthe first study on the effectiveness of conversational agents in statistical\nanalysis. We evaluate the performance of three conversational agents, i.e.,\nTalkingData, Lingua, and TalkingData-NLP, in two prominent statistical\nanalysis tasks: the Fisher-Von Mises test and the K-means clustering. Our\nfindings reveal that conversational agents are capable of handling both tasks\nadequately. However, the performance of TalkingData-NLP is significantly\nimproved compared to the other two agents",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.3291139240506329,
          "f": 0.26530611763692213
        },
        "rouge-2": {
          "r": 0.05847953216374269,
          "p": 0.08928571428571429,
          "f": 0.07067137330919383
        },
        "rouge-l": {
          "r": 0.19658119658119658,
          "p": 0.2911392405063291,
          "f": 0.23469387273896306
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.physics/bio-ph/2503.08818v1",
      "true_abstract": "Molecular motors are in charge of almost every process in the life cycle of\ncells, such as protein synthesis, DNA replication, and cell locomotion, hence\nbeing of crucial importance for understanding the cellular dynamics. However,\ngiven their size scales on the order of nanometers, direct measurements are\nrather challenging, and the information that can be extracted from them is\nlimited. In this work, we propose strategies based on martingale theory in\nstochastic thermodynamics to infer thermodynamic properties of molecular motors\nusing a limited amount of available information. In particular, we use two\nrecent theoretical results valid for systems arbitrary far of equilibrium: the\nintegral fluctuation theorem (IFT) at stopping times, and a family of bounds to\nthe maximal excursions of entropy production. The potential of these strategies\nis illustrated with a simple model for the F1-ATPase rotary molecular motor,\nwhere our approach is able to estimate several quantities determining the\nthermodynamics of the motor, such as the rotational work of the motor performed\nagainst an externally applied force, or the effective environmental\ntemperature.",
      "generated_abstract": "motors are ubiquitous, yet their operation remains a mystery,\nunderlying the fundamental problems of self-assembly, protein folding, and\ninformation processing in living systems. The current paradigm holds that\nmotors operate by releasing a proton and binding it to a target nucleic acid.\nHowever, the mechanism of release is unknown and it is unclear how the binding\nprocess is mediated. Here, we propose a new thermodynamic approach for\ninferring the dynamics of molecular motors. Our approach is based on a\nmartingale framework that incorporates the thermodynamic laws of the system. We\nderive thermodynamic and kinetic expressions for the dynamics of the\ndynamos. We then employ these expressions to infer the release and binding\nprocesses. Our results show that the release process is driven by the\nconcentration of the proton, while the binding process is driven",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23809523809523808,
          "p": 0.37037037037037035,
          "f": 0.2898550677000631
        },
        "rouge-2": {
          "r": 0.052941176470588235,
          "p": 0.07377049180327869,
          "f": 0.061643830751548515
        },
        "rouge-l": {
          "r": 0.2222222222222222,
          "p": 0.345679012345679,
          "f": 0.2705313962024785
        }
      }
    },
    {
      "paper_id": "cs.CC.cs/CC/2503.07285v1",
      "true_abstract": "The complexity of solving equations over finite groups has been an active\narea of research over the last two decades, starting with Goldmann and Russell,\n\\emph{The complexity of solving equations over finite groups} from 1999. One\nimportant case of a group with unknown complexity is the symmetric group $S_4.$\nIn 2023, Idziak, Kawa{\\l}ek, and Krzaczkowski published $\\exp(\\Omega(\\log^2\nn))$ lower bounds for the satisfiability and equivalence problems over $S_4$\nunder the Exponential Time Hypothesis. In the present note, we prove that the\nsatisfiability problem $\\textsc{PolSat}(S_4)$ can be reduced to the equivalence\nproblem $\\textsc{PolEqv}(S_4)$ and thus, the two problems have the same\ncomplexity. We provide several equivalent formulations of the problem. In\nparticular, we prove that $\\textsc{PolEqv}(S_4)$ is equivalent to the circuit\nequivalence problem for $\\operatorname{CC}[2,3,2]$-circuits, which were\nintroduced by Idziak, Kawe{\\l}ek and Krzaczkowski. Under their strong\nexponential size hypothesis, such circuits cannot compute\n$\\operatorname{AND}_n$ in size $\\exp(o(\\sqrt{n})).$ Our results provide an\nupper bound on the complexity of $\\textsc{PolEqv}(S_4)$ that is based on the\nminimal size of $\\operatorname{AND}_n$ over\n$\\operatorname{CC}[2,3,2]$-circuits.",
      "generated_abstract": "We study the complexity of solving equations over the group $S_4$ by\nverifying if there are two solutions, and by finding the smallest number of\nsolutions for each given equation. We also present the complexity of solving\nequations over $S_4$ with a single variable and by checking if there are\nsolutions. Our results show that the problem is NP-hard in general. Furthermore,\nwe show that the problem is NP-hard when the equation has a constant number of\nvariables and has at most $3$ solutions. We also present the complexity of\nsolving equations over $S_4$ with a variable and a constant number of\nsolutions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22857142857142856,
          "p": 0.47058823529411764,
          "f": 0.30769230329142017
        },
        "rouge-2": {
          "r": 0.0641025641025641,
          "p": 0.1388888888888889,
          "f": 0.08771929392428461
        },
        "rouge-l": {
          "r": 0.20952380952380953,
          "p": 0.43137254901960786,
          "f": 0.28205127765039456
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.14731v1",
      "true_abstract": "Motor skill acquisition in fields like surgery, robotics, and sports involves\nlearning complex task sequences through extensive training. Traditional\nperformance metrics, like execution time and error rates, offer limited insight\nas they fail to capture the neural mechanisms underlying skill learning and\nretention. This study introduces directed functional connectivity (dFC),\nderived from electroencephalography (EEG), as a novel brain-based biomarker for\nassessing motor skill learning and retention. For the first time, dFC is\napplied as a biomarker to map the stages of the Fitts and Posner motor learning\nmodel, offering new insights into the neural mechanisms underlying skill\nacquisition and retention. Unlike traditional measures, it captures both the\nstrength and direction of neural information flow, providing a comprehensive\nunderstanding of neural adaptations across different learning stages. The\nanalysis demonstrates that dFC can effectively identify and track the\nprogression through various stages of the Fitts and Posner model. Furthermore,\nits stability over a six-week washout period highlights its utility in\nmonitoring long-term retention. No significant changes in dFC were observed in\na control group, confirming that the observed neural adaptations were specific\nto training and not due to external factors. By offering a granular view of the\nlearning process at the group and individual levels, dFC facilitates the\ndevelopment of personalized, targeted training protocols aimed at enhancing\noutcomes in fields where precision and long-term retention are critical, such\nas surgical education. These findings underscore the value of dFC as a robust\nbiomarker that complements traditional performance metrics, providing a deeper\nunderstanding of motor skill learning and retention.",
      "generated_abstract": "new motor skills is a fundamental aspect of human development and\nperformance. While traditional performance scores such as the Motor\nTest-Receptive Elements (MTERF) and Motor Score-Receptive Elements (MSRE) have\nprovided insights into motor skill acquisition, these scores are based on\nperformance scores that are calculated by summing scores for individual\nsubcomponents of motor skills. These subcomponents are often related to\ndifferent motor skills, which results in subcomponents with higher scores than\ntheir corresponding motor skill. These subcomponents are thus not related to\nmotor skill learning and retention. In this study, we propose the\nDirectedFunctionalConnectivity as a brain-based biomarker for motor skill\nlearning and retention. This approach is based on the idea that motor skills\nare learned in the brain by connecting subcomponents of motor skills, which are\ndirectly related to each other.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19607843137254902,
          "p": 0.39473684210526316,
          "f": 0.2620087291897562
        },
        "rouge-2": {
          "r": 0.05309734513274336,
          "p": 0.10619469026548672,
          "f": 0.07079645573254699
        },
        "rouge-l": {
          "r": 0.1830065359477124,
          "p": 0.3684210526315789,
          "f": 0.24454148028145922
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/NI/2503.08262v1",
      "true_abstract": "The search for the optimal pair of active and protection paths in a network\nwith Shared Risk Link Groups (SRLG) is a challenging but high-value problem in\nthe industry that is inevitable in ensuring reliable connections on the modern\nInternet. We propose a new approach to solving this problem, with a novel use\nof statistical analysis of the distribution of paths with respect to their\ncost, which is an integral part of our innovation. The key idea in our\nalgorithm is to employ iterative updates of cost bounds, allowing efficient\npruning of suboptimal paths. This idea drives an efficacious exploration of the\nsearch space. We benchmark our algorithms against the state-of-the-art\nalgorithms that exploit the alternative strategy of conflicting links\nexclusion, showing that our approach has the advantage of finding more feasible\nconnections within a set time limit.",
      "generated_abstract": "We address the iterative routing problem with SRLG-disjoint protection,\nwhich is a constrained routing problem that has been extensively studied in\nthe literature. It is a generalization of the classical routing problem with\nSRLG, which is a special case of this problem when the routing graph is\n$K_{m,n}$. We propose a novel cost-driven pruning algorithm for solving this\nproblem, which is based on the pruning algorithm for the general problem. We\nshow that the cost-driven pruning algorithm is optimal for solving this problem\nunder the restricted settings. Moreover, we show that the cost-driven pruning\nalgorithm is optimal under the general setting under mild assumptions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23655913978494625,
          "p": 0.4230769230769231,
          "f": 0.30344827126183116
        },
        "rouge-2": {
          "r": 0.08823529411764706,
          "p": 0.15,
          "f": 0.11111110644718813
        },
        "rouge-l": {
          "r": 0.20430107526881722,
          "p": 0.36538461538461536,
          "f": 0.26206896091700366
        }
      }
    },
    {
      "paper_id": "cs.DC.cs/DC/2503.10292v1",
      "true_abstract": "Synchronous consensus protocols offer a significant advantage over their\nasynchronous and partially synchronous counterparts by providing higher fault\ntolerance -- an essential benefit in distributed systems, like blockchains,\nwhere participants may have incentives to act maliciously. However, despite\nthis advantage, synchronous protocols are often met with skepticism due to\nconcerns about their performance, as the latency of synchronous protocols is\ntightly linked to a conservative time bound for message delivery.\n  This paper introduces AlterBFT, a new Byzantine fault-tolerant consensus\nprotocol. The key idea behind AlterBFT lies in the new model we propose, called\nhybrid synchronous system model. The new model is inspired by empirical\nobservations about network behavior in the public cloud environment and\ncombines elements from the synchronous and partially synchronous models.\nNamely, it distinguishes between small messages that respect time bounds and\nlarge messages that may violate bounds but are eventually timely. Leveraging\nthis observation, AlterBFT achieves up to 15$\\times$ lower latency than\nstate-of-the-art synchronous protocols while maintaining similar throughput and\nthe same fault tolerance. Compared to partially synchronous protocols, AlterBFT\nprovides higher fault tolerance, higher throughput, and comparable latency.",
      "generated_abstract": "The emergence of public cloud infrastructures has increased the need for\naccelerating blockchain applications, especially in the context of distributed\nledgers. In this paper, we propose AlterBFT, a proof-of-concept implementation\nof a scalable, high-performance, and practical synchronous blockchain\nimplementation in public cloud environments. We implement a new consensus\nmechanism, called AlterBFT, which is inspired by the work of Ethereum and\nEOSIO. We have designed a proof-of-concept implementation of the AlterBFT\nconsensus mechanism in the cloud and evaluated its performance with various\ntypes of workloads. Our results show that the proposed consensus mechanism\nprovides a scalable, efficient, and high-performance alternative to the\ncurrent state-of-the-art blockchain protocols, especially in the context of\ndistributed ledgers.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2204724409448819,
          "p": 0.39436619718309857,
          "f": 0.282828278228242
        },
        "rouge-2": {
          "r": 0.03508771929824561,
          "p": 0.061224489795918366,
          "f": 0.04460966079573299
        },
        "rouge-l": {
          "r": 0.1968503937007874,
          "p": 0.352112676056338,
          "f": 0.2525252479252118
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ph/2503.10366v1",
      "true_abstract": "It is a common lore that in the thermal leptogenesis in the type-1 seesaw\nscenario with the conventional hierarchy of heavy right-handed neutrinos\n(RHNs), the CP violating, out-of-equilibrium decay of the lightest RHN ($N_1$)\nis the only relevant source of $B-L$ asymmetry. Any asymmetry produced by the\nheavier RHNs ($N_2$ and $N_3$) gets washed out by the lepton number violating\nprocesses mediated by $N_1$. In this paper, we examine this assumption\ncomprehensively, considering decay and inverse decay processes as well as the\ninclusion of scatterings. We find that the above said assumption is true only\nif all the RHNs ($N_1, N_2$ and $N_3$) are in strong washout regime. However,\nwe saw that, to satisfy the neutrino masses and mixing given by the low energy\nneutrino oscillation data, at most one of the RHNs can be in the weak washout\nregime. This leads to, if $N_1$ is in the weak washout regime, then the washout\nparameters of $N_2$ and $N_3$ can be chosen in such a way that the impact of\n$N_2$ and $N_3$ on the final $B-L$ asymmetry is relatively small. On the other\nhand, if $N_2$ or $N_3$ is in weak washout regime, then the asymmetry produced\nby them can affect the final $B-L$ asymmetry even if $N_1$ is in the strong\nwashout regime, which we call the memory effect. We delineated the parameter\nspace where the memory effect is significant.",
      "generated_abstract": "igate the possibility that thermal leptogenesis in a canonical\nseesaw model depends on initial memory. Specifically, we propose that if the\nelectroweak symmetry is broken at a high temperature $T_c$, the initial\nconditions for leptogenesis in this model may not be the same as those in the\ngeneral seesaw model. This can occur when $T_c \\sim m_0$. In this case, the\nleptogenesis process may be facilitated by the initial memory, which is a\nresult that is consistent with the recent experimental result on the\nbaryon-to-entropy density ratio. We also investigate the possibility that\nthermal leptogenesis in a seesaw model requires additional assumptions about\nthe initial conditions for leptogenesis. In particular, we propose that the\ninitial conditions for leptogenesis in this model are determined by the\nnon-renormalizable terms in the scalar potential",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1889763779527559,
          "p": 0.3287671232876712,
          "f": 0.23999999536450006
        },
        "rouge-2": {
          "r": 0.04591836734693878,
          "p": 0.08571428571428572,
          "f": 0.059800659908831384
        },
        "rouge-l": {
          "r": 0.18110236220472442,
          "p": 0.3150684931506849,
          "f": 0.22999999536450008
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/CV/2503.10632v1",
      "true_abstract": "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of\nlearnable activation functions with the potential to capture more complex\nrelationships from data. Although KANs are useful in finding symbolic\nrepresentations and continual learning of one-dimensional functions, their\neffectiveness in diverse machine learning (ML) tasks, such as vision, remains\nquestionable. Presently, KANs are deployed by replacing multilayer perceptrons\n(MLPs) in deep network architectures, including advanced architectures such as\nvision Transformers (ViTs). In this paper, we are the first to design a general\nlearnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate\non any choice of basis. However, the computing and memory costs of training\nthem motivated us to propose a more modular version, and we designed particular\nlearnable attention, called Fourier-KArAt. Fourier-KArAt and its variants\neither outperform their ViT counterparts or show comparable performance on\nCIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures'\nperformance and generalization capacity by analyzing their loss landscapes,\nweight distributions, optimizer path, attention visualization, and spectral\nbehavior, and contrast them with vanilla ViTs. The goal of this paper is not to\nproduce parameter- and compute-efficient attention, but to encourage the\ncommunity to explore KANs in conjunction with more advanced architectures that\nrequire a careful understanding of learnable activations. Our open-source code\nand implementation details are available on: https://subhajitmaity.me/KArAt",
      "generated_abstract": "tion mechanism is a core component in modern vision transformers.\nA key question is whether a learnable attention layer, which can be\ntrained end-to-end, is better than a layer that relies on learnable\nquantifiers, such as self-attention. To answer this question, we conduct a\ncomparison of two types of attention layers: an attention layer that\ncorresponds to the self-attention layer in the Transformer architecture and a\nlayer that does not contain any self-attention. We show that the self-attention\nlayer has a large variance in its output, while the attention layer with learnable\nquantifiers has a small variance. This finding suggests that the attention\nlayer with learnable quantifiers can be a better choice for vision transformers\nthan the self-attention layer. We further show that the self-attention layer\nalso has large variance in its output. This finding also suggests that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16883116883116883,
          "p": 0.3561643835616438,
          "f": 0.22907488550447322
        },
        "rouge-2": {
          "r": 0.004807692307692308,
          "p": 0.009009009009009009,
          "f": 0.006269587938801055
        },
        "rouge-l": {
          "r": 0.14935064935064934,
          "p": 0.3150684931506849,
          "f": 0.20264316744279928
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2502.15800v1",
      "true_abstract": "This paper explores how Large Language Models (LLMs) behave in a classic\nexperimental finance paradigm widely known for eliciting bubbles and crashes in\nhuman participants. We adapt an established trading design, where traders buy\nand sell a risky asset with a known fundamental value, and introduce several\nLLM-based agents, both in single-model markets (all traders are instances of\nthe same LLM) and in mixed-model \"battle royale\" settings (multiple LLMs\ncompeting in the same market). Our findings reveal that LLMs generally exhibit\na \"textbook-rational\" approach, pricing the asset near its fundamental value,\nand show only a muted tendency toward bubble formation. Further analyses\nindicate that LLM-based agents display less trading strategy variance in\ncontrast to humans. Taken together, these results highlight the risk of relying\non LLM-only data to replicate human-driven market phenomena, as key behavioral\nfeatures, such as large emergent bubbles, were not robustly reproduced. While\nLLMs clearly possess the capacity for strategic decision-making, their relative\nconsistency and rationality suggest that they do not accurately mimic human\nmarket dynamics.",
      "generated_abstract": "We present a novel study into the behavior of Large Language Models (LLMs)\nin an experimental asset market, with a particular focus on the trading strategies\nof AI agents trained on publicly available datasets. By analyzing the\ninteractions of trading bots and their human counterparts, we aim to provide\ninsights into the behavioral patterns and learning mechanisms of LLMs in\nfinancial markets. Our findings suggest that LLMs demonstrate a range of\nbehavioral patterns, including active participation, active monitoring, and\npassive observation, which are influenced by factors such as the size of the\ndataset, the model's expertise, and the trading strategy of the bot. These\nfindings have implications for the design and evaluation of AI trading\nalgorithms, and offer insights into the potential risks and benefits of\nAI-driven trading in financial markets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24242424242424243,
          "p": 0.36363636363636365,
          "f": 0.29090908610909094
        },
        "rouge-2": {
          "r": 0.06060606060606061,
          "p": 0.08196721311475409,
          "f": 0.06968640626206496
        },
        "rouge-l": {
          "r": 0.20454545454545456,
          "p": 0.3068181818181818,
          "f": 0.24545454065454556
        }
      }
    },
    {
      "paper_id": "cs.CL.cs/AI/2503.10533v1",
      "true_abstract": "High-quality test items are essential for educational assessments,\nparticularly within Item Response Theory (IRT). Traditional validation methods\nrely on resource-intensive pilot testing to estimate item difficulty and\ndiscrimination. More recently, Item-Writing Flaw (IWF) rubrics emerged as a\ndomain-general approach for evaluating test items based on textual features.\nHowever, their relationship to IRT parameters remains underexplored. To address\nthis gap, we conducted a study involving over 7,000 multiple-choice questions\nacross various STEM subjects (e.g., math and biology). Using an automated\napproach, we annotated each question with a 19-criteria IWF rubric and studied\nrelationships to data-driven IRT parameters. Our analysis revealed\nstatistically significant links between the number of IWFs and IRT difficulty\nand discrimination parameters, particularly in life and physical science\ndomains. We further observed how specific IWF criteria can impact item quality\nmore and less severely (e.g., negative wording vs. implausible distractors).\nOverall, while IWFs are useful for predicting IRT parameters--particularly for\nscreening low-difficulty MCQs--they cannot replace traditional data-driven\nvalidation methods. Our findings highlight the need for further research on\ndomain-general evaluation rubrics and algorithms that understand\ndomain-specific content for robust item validation.",
      "generated_abstract": "Item response theory (IRT) is a powerful tool for analyzing how individuals\ndifficulty and discrimination vary across multiple items. The focus of the\ncurrent study is on the impact of item-writing flaws on IRT modeling. We first\nexamine the impact of item-writing flaws on IRT modeling. We then discuss how\nthese modeling flaws affect discrimination and difficulty in the context of\nitem response theory. Finally, we discuss how these modeling flaws affect\ndifficulty and discrimination in the context of item response theory.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14184397163120568,
          "p": 0.46511627906976744,
          "f": 0.21739130076618626
        },
        "rouge-2": {
          "r": 0.011235955056179775,
          "p": 0.03389830508474576,
          "f": 0.016877633391373294
        },
        "rouge-l": {
          "r": 0.14184397163120568,
          "p": 0.46511627906976744,
          "f": 0.21739130076618626
        }
      }
    },
    {
      "paper_id": "econ.TH.q-fin/PM/2410.18432v1",
      "true_abstract": "This paper develops a dynamic model to analyze the general equilibrium of the\ninsurance market, focusing on the interaction between insurers' underwriting\nand investment strategies. Three possible equilibrium outcomes are identified:\na positive insurance market, a zero insurance market, and market failure. Our\nfindings reveal why insurers may rationally accept underwriting losses by\nsetting a negative safety loading while relying on investment profits,\nparticularly when there is a negative correlation between insurance gains and\nfinancial returns. Additionally, we explore the impact of regulatory frictions,\nshowing that while imposing a cost on investment can enhance social welfare\nunder certain conditions, it may not always be necessary. Therefore, we\nemphasize the importance of tailoring regulatory interventions to specific\nmarket conditions.",
      "generated_abstract": "r studies an insurance agent's dynamic pricing strategy for two\nunderlying insurance policies. The first policy is a traditional insurance\npolicy that provides a fixed benefit, while the second policy is a risk-based\ninsurance policy that provides a variable benefit. The insurance agent's goal\nis to maximize the agent's welfare by determining the optimal insurance\npricing strategy. The insurance agent's decision is affected by two factors,\nthe premium income and the risk of insurance fraud. The former is determined by\nthe agent's decision to provide a fixed benefit and the latter is affected by\nthe insurer's decision to offer a risk-based insurance policy. In this paper,\nwe use the equilibrium dynamic programming (EDP) approach to analyze the\noptimal insurance pricing strategy of the insurance agent. The EDP method\nallows us to identify the equilibrium pr",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1744186046511628,
          "p": 0.22388059701492538,
          "f": 0.19607842644965626
        },
        "rouge-2": {
          "r": 0.04424778761061947,
          "p": 0.046296296296296294,
          "f": 0.04524886378084042
        },
        "rouge-l": {
          "r": 0.1744186046511628,
          "p": 0.22388059701492538,
          "f": 0.19607842644965626
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.cond-mat/other/2503.05400v1",
      "true_abstract": "We investigate site-controlled In$_{0.25}$Ga$_{0.75}$As quantum dots in\n(111)B GaAs pyramidal recesses as spin qubits. Combining scanning confocal\ncryomicroscopy, magneto-photoluminescence studies and resonant excitation, we\nidentify and isolate a positively charged exciton with a hole-spin in its\nground state. Application of a strong 5 T magnetic field parallel to the growth\naxis, induces a fourfold splitting of the energy levels of the positively\ncharged exciton creating an optically addressable double-lambda system. We\ncombine weak above-band and resonant excitation to demonstrate spin pumping and\nhigh-fidelity spin initialization through all four optical transitions and\nstudy the system behavior as a function of the resonant driving strength\nshowing the existence of a robust spin that can be optically pumped and\ninitialized. These results demonstrate the potential of these quantum dots for\nprecise spin manipulation and their relevance for future quantum hardware.",
      "generated_abstract": "We present a novel method for the optical control of the hole spin in\nquantum dots (QDs) by using an optical pumping process followed by a\ndifferent pumping process. This method is based on the spin-cooling of the hole\nspin in the dot. The hole is then excited by a laser pulse, which is then\nmeasured by the electron spin resonance (ESR) technique. This process allows\nthe initialization of the hole spin in the QD. The method is demonstrated on\na pyramidal quantum dot. The obtained results demonstrate that the method can\nbe applied to other types of quantum dots.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20618556701030927,
          "p": 0.36363636363636365,
          "f": 0.2631578901185942
        },
        "rouge-2": {
          "r": 0.030534351145038167,
          "p": 0.047619047619047616,
          "f": 0.03720929756452197
        },
        "rouge-l": {
          "r": 0.15463917525773196,
          "p": 0.2727272727272727,
          "f": 0.19736841643438377
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2502.20749v1",
      "true_abstract": "Deep learning-based medical image segmentation typically requires large\namount of labeled data for training, making it less applicable in clinical\nsettings due to high annotation cost. Semi-supervised learning (SSL) has\nemerged as an appealing strategy due to its less dependence on acquiring\nabundant annotations from experts compared to fully supervised methods. Beyond\nexisting model-centric advancements of SSL by designing novel regularization\nstrategies, we anticipate a paradigmatic shift due to the emergence of\npromptable segmentation foundation models with universal segmentation\ncapabilities using positional prompts represented by Segment Anything Model\n(SAM). In this paper, we present SemiSAM+, a foundation model-driven SSL\nframework to efficiently learn from limited labeled data for medical image\nsegmentation. SemiSAM+ consists of one or multiple promptable foundation models\nas generalist models, and a trainable task-specific segmentation model as\nspecialist model. For a given new segmentation task, the training is based on\nthe specialist-generalist collaborative learning procedure, where the trainable\nspecialist model delivers positional prompts to interact with the frozen\ngeneralist models to acquire pseudo-labels, and then the generalist model\noutput provides the specialist model with informative and efficient supervision\nwhich benefits the automatic segmentation and prompt generation in turn.\nExtensive experiments on two public datasets and one in-house clinical dataset\ndemonstrate that SemiSAM+ achieves significant performance improvement,\nespecially under extremely limited annotation scenarios, and shows strong\nefficiency as a plug-and-play strategy that can be easily adapted to different\nspecialist and generalist models.",
      "generated_abstract": "a of foundation models, segmentation in medical images has\nbecome a crucial task. Despite its importance, existing methods, especially\nsemi-supervised ones, are often limited by the low-quality annotations, which\nimply that they are often unable to generalize to unseen data. To address this\nissue, we propose a novel semi-supervised segmentation model, SemiSAM+, which\nintegrates a multi-scale feature extractor with a multi-task learning strategy\nto enhance both segmentation performance and generalization ability. The\nmulti-scale feature extractor uses a series of encoders to extract different\nlevels of features, which are then fused together to improve the segmentation\nperformance. Meanwhile, the multi-task learning strategy is adopted to\npromote the generalization ability of the model. Theoretically, we prove that\nthe proposed model can achieve a higher accuracy than the state-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2077922077922078,
          "p": 0.3764705882352941,
          "f": 0.26778242219498966
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.17532467532467533,
          "p": 0.3176470588235294,
          "f": 0.22594141801088927
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2412.00658v1",
      "true_abstract": "A new modular approximate Bayesian inferential framework is proposed that\nenables fast calculation of probabilistic predictions of future option prices.\nWe exploit multiple information sources, including daily spot returns,\nhigh-frequency spot data and option prices. A benefit of this modular Bayesian\napproach is that it allows us to work with the theoretical option pricing\nmodel, without needing to specify an arbitrary statistical model that links the\ntheoretical prices to their observed counterparts. We show that our approach\nproduces accurate probabilistic predictions of option prices in realistic\nscenarios and, despite not explicitly modelling pricing errors, the method is\nshown to be robust to their presence. Predictive accuracy based on the Heston\nstochastic volatility model, with predictions produced via rapid real-time\nupdates, is illustrated empirically for short-maturity options.",
      "generated_abstract": "e a novel approach to option pricing using multiple sources of\ndata, including historical option prices, historical volatilities, and the\nstochastic volatility model. The proposed method incorporates an ensemble\napproach to combine the inputs, thereby enhancing the predictive accuracy of\nthe model. The method is based on the use of a Gaussian process (GP) model to\nrepresent the underlying option price distribution. The GP model is trained\nusing the historical option prices, historical volatilities, and the stochastic\nvolatility model. The model is then evaluated using the predictive accuracy\nmeasures, including the root mean square error (RMSE), the mean absolute error\n(MAE), and the median absolute error (MAE). The results show that the\nproposed method outperforms traditional approaches, such as the Black-Scholes\nmodel and the stochastic volatility model, in terms of predictive accuracy. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25555555555555554,
          "p": 0.32857142857142857,
          "f": 0.28749999507812507
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.06422018348623854,
          "f": 0.061403503781548574
        },
        "rouge-l": {
          "r": 0.23333333333333334,
          "p": 0.3,
          "f": 0.26249999507812505
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2412.00471v1",
      "true_abstract": "Building a general-purpose task model similar to ChatGPT has been an\nimportant research direction for gene large language models. Instruction\nfine-tuning is a key component in building ChatGPT, but existing instructions\nare primarily based on natural language. Natural language and gene sequences\nhave significant differences in tokenization and encoding. Therefore,\nconstructing a multilingual model that can handle both natural language and\ngene sequences is crucial for solving this problem.In this paper, we expand the\ncapabilities of the LLaMA large language model to include gene language. This\ninvolves expanding the vocabulary using the Byte Pair Encoding (BPE) method,\nspecifically tailored for DNA and protein sequences, and conducting further\npre-training on these sequences. We then convert various downstream gene task\ndata into a unified format for instruction fine-tuning and further fine-tune\nthe model on this data.Our study demonstrates that a mixed model of gene and\nnatural language, fine-tuned with instructions, achieves results comparable to\nthe current state-of-the-art (SOTA) in tasks such as gene classification and\ngene sequence interaction. This provides a promising direction for building a\nunified large language model for gene tasks.",
      "generated_abstract": "guage Models (LLMs) have shown remarkable capability in\ntransferring knowledge across different domains, yet the transferability of\nLLMs across gene-related tasks remains poorly explored. To address this\nlimitation, we propose LLaMA-Gene, a general-purpose gene-related LLM based on\ninstruction fine-tuning. Our approach integrates three key components:\nLLaMA-Gene, a multi-task LLM, for gene-related tasks; LLAMA-Gene, a gene-specific\nLLaMA-Gene, for gene-specific tasks; and LLAMA-Gene-X, a multi-gene-specific\nLLaMA-Gene, for gene-specific tasks. To ensure the transferability of LLAMA-Gene\nand LLAMA-Gene-X across different gene-related tasks, we design the\nLL",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14782608695652175,
          "p": 0.29310344827586204,
          "f": 0.19653178745029917
        },
        "rouge-2": {
          "r": 0.017543859649122806,
          "p": 0.039473684210526314,
          "f": 0.024291493715354218
        },
        "rouge-l": {
          "r": 0.14782608695652175,
          "p": 0.29310344827586204,
          "f": 0.19653178745029917
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.00352v1",
      "true_abstract": "This paper reviews statistical methods for hypothesis testing and clustering\nin network models. We analyze the method by Bickel et al. (2016) for deriving\nthe asymptotic null distribution of the largest eigenvalue, noting its slow\nconvergence and the need for bootstrap corrections. The SCORE method by Jin et\nal. (2015) and the NCV method by Chen et al. (2018) are evaluated for their\nefficacy in clustering within Degree-Corrected Block Models, with NCV facing\nchallenges due to its time-intensive nature. We suggest exploring eigenvector\nentry distributions as a potential efficiency improvement.",
      "generated_abstract": "Determining the number of communities in network data is a crucial task that\nhas emerged as a key challenge in many real-world problems. In this paper, we\nfirst review existing methods for community detection in undirected networks\nand then propose a novel method, namely the \\textit{community-level\nrandomized-sampling algorithm} (CLRS), to determine the number of communities in\ndirected networks. We show that, under certain conditions, the CLRS algorithm\nachieves the same approximation ratio as the \\textit{randomized-sampling\nalgorithm} (RSA) for community detection in undirected networks. Furthermore,\nthe CLRS algorithm is superior in terms of computational efficiency, even in\nthe case of large networks. Numerical experiments demonstrate the effectiveness\nof the CLRS algorithm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16176470588235295,
          "p": 0.1506849315068493,
          "f": 0.1560283638006138
        },
        "rouge-2": {
          "r": 0.047619047619047616,
          "p": 0.041666666666666664,
          "f": 0.04444443946666723
        },
        "rouge-l": {
          "r": 0.16176470588235295,
          "p": 0.1506849315068493,
          "f": 0.1560283638006138
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09525v1",
      "true_abstract": "The complexity of continuous piecewise affine (CPA) functions can be measured\nby the number of pieces $p$ or the number of distinct affine functions $n$. For\nCPA functions on $\\mathbb{R}^d$, this paper shows an upper bound of\n$p=O(n^{d+1})$ and constructs a family of functions achieving a lower bound of\n$p=\\Omega(n^{d+1-\\frac{c}{\\sqrt{\\log_2(n)}}})$.",
      "generated_abstract": "We study the number of pieces of a continuous piecewise affine function in\nthe space of piecewise affine functions. We provide a lower bound on the\nnumber of pieces of a continuous piecewise affine function $f$ with constant\nderivative in terms of the derivative of $f$ and the number of pieces of\n$f$. We also obtain a similar lower bound for piecewise affine functions with\na polynomial derivative. As a consequence, we obtain a lower bound on the number\nof pieces of a continuous piecewise affine function with a polynomial\nderivative. Our approach is based on the study of a certain sum of moments for\npiecewise affine functions, and we apply the results to the study of piecewise\naffine functions with a polynomial derivative.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.35135135135135137,
          "p": 0.3023255813953488,
          "f": 0.32499999502812504
        },
        "rouge-2": {
          "r": 0.17391304347826086,
          "p": 0.10666666666666667,
          "f": 0.132231400245885
        },
        "rouge-l": {
          "r": 0.32432432432432434,
          "p": 0.27906976744186046,
          "f": 0.299999995028125
        }
      }
    },
    {
      "paper_id": "math.ST.math/ST/2503.07918v1",
      "true_abstract": "COVID-19 has had a large scale negative impact on the health of opioid users\nexacerbating the health of an already vulnerable population. Critical\ninformation on the total impact of COVID-19 on opioid users is unknown due to a\nlack of comprehensive data on COVID-19 cases, inaccurate diagnostic coding, and\nlack of data coverage. To assess the impact of COVID-19 on small-area opioid\nmortality, we developed a Bayesian hierarchical excess opioid mortality\nmodeling approach. We incorporate spatio-temporal autocorrelation structures to\nallow for sharing of information across small areas and time to reduce\nuncertainty in small area estimates. Excess mortality is defined as the\ndifference between observed trends after a crisis and expected trends based on\nobserved historical trends, which captures the total increase in observed\nmortality rates compared to what was expected prior to the crisis. We\nillustrate the application of our approach to assess excess opioid mortality\nrisk estimates for 159 counties in GA. Using our proposed approach will help\ninform interventions in opioid-related public health responses, policies, and\nresource allocation. The application of this work also provides a general\nframework for improving the estimation and mapping of health indicators during\ncrisis periods for the opioid user population.",
      "generated_abstract": "-19 pandemic has had a devastating impact on opioid-related mortality in\nGeorgia. We estimate the annual excess opioid mortality in Georgia from 2020 to\n2022. Our results suggest that the total opioid mortality in Georgia is\napproximately 2000 deaths higher than the baseline mortality. The excess opioid\nmortality was primarily driven by an increase in opioid-related deaths in the\npopulation over 75 years of age and in rural counties. The excess opioid\nmortality is driven by an increase in opioid-related deaths in the\npopulation over 75 years of age and in rural counties. Our findings provide\ncritical insights for addressing opioid mortality disparities in Georgia,\nparticularly in rural areas. This study provides a rig",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22033898305084745,
          "p": 0.40625,
          "f": 0.28571428115445
        },
        "rouge-2": {
          "r": 0.05405405405405406,
          "p": 0.12048192771084337,
          "f": 0.07462686139591249
        },
        "rouge-l": {
          "r": 0.19491525423728814,
          "p": 0.359375,
          "f": 0.252747248187417
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.03323v1",
      "true_abstract": "This study as part of an ongoing research effort, empirically examines the\nrelationship between foreign trade in the Istanbul Ataturk Airport Free Zone\nand exchange rate movements. Monthly data from 2003 to 2016 were analyzed\nthrough stationarity tests (Unit Root), followed by the Vector Autoregressive\n(VAR) model, Cointegration Analysis, and the Toda-Yamamoto Causality Test. The\nfindings indicate that the exchange rate does not significantly affect imports\nand exports in the free zone. This result suggests that free zones, due to\ntheir structural characteristics and operational framework, may be relatively\ninsulated from exchange rate fluctuations. The study contributes to the\nliterature by providing a focused analysis of a specific free zone in Turkiye,\nhighlighting the potential independence of free zone trade from exchange rate\nvolatility.",
      "generated_abstract": "This paper aims to examine the exchange rate sensitivity of the Istanbul\nAtaturk Free Zone, which has been a major destination for international\ntrade. Using an empirical approach, the paper focuses on analyzing the\nexchange rate impact of trade flows from 2015 to 2023. The results indicate that\nthe exchange rate sensitivity of the Istanbul Ataturk Free Zone is higher than\nthat of the Istanbul Ataturk Airport, and the sensitivity of the two free zones\nis statistically significant. The results of the study suggest that the\nexchange rate sensitivity of the free zone can be a key factor in determining\nthe performance of international trade. The findings of the study can be used\nby governments and businesses to understand the potential impact of exchange\nrate fluctuations on international trade flows and to develop policies to\nmanage and optimize trade flows.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3068181818181818,
          "p": 0.375,
          "f": 0.33749999505
        },
        "rouge-2": {
          "r": 0.11206896551724138,
          "p": 0.12149532710280374,
          "f": 0.11659192325926543
        },
        "rouge-l": {
          "r": 0.29545454545454547,
          "p": 0.3611111111111111,
          "f": 0.32499999505000005
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.10206v1",
      "true_abstract": "A graph $G$ is perfectly divisible if, for every induced subgraph $H$ of $G$,\neither $V(H)$ is a stable set or admits a partition into two sets $X_1$ and\n$X_2$ such that $\\omega(H[X_1]) < \\omega(H)$ and $H[X_2]$ is a perfect graph.\nIn this article, we propose the following generalisation of perfectly divisible\ngraphs. A graph $G$ is perfectly $1$-divisible if $G$ is perfect and perfectly\n$k$-divisible if, for every induced subgraph $H$ of $G$, either $V(H)$ is a\nstable set or admits a partition into two sets $X_1$ and $X_2$ such that\n$\\omega(H[X_1]) < \\omega(H)$ and $H[X_2]$ is perfectly $(k-1)$-divisible, $k\n\\in \\mathbb{N}_{> 1}$. Our main result establishes that every perfectly\n$k$-divisible graph $G$ satisfies $\\chi(G) \\leq \\binom{\\omega(G)+k-1}{k}$ which\ngeneralises the known bound for perfectly divisible graphs.",
      "generated_abstract": "We prove that the perfect $k$-divisibility of a graph $G$ is the same as\nthe perfect $k$-divisibility of its $k$-colored version $\\Gamma(G)$. We show\nthat if $G$ is a connected graph, then the perfect $k$-divisibility of $\\Gamma(G)$\nis the same as the perfect $k$-divisibility of $G$. We also show that if $G$ is\na connected graph with no triangle, then the perfect $k$-divisibility of\n$\\Gamma(G)$ is the same as the perfect $k$-divisibility of $G$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.140625,
          "p": 0.34615384615384615,
          "f": 0.1999999958913581
        },
        "rouge-2": {
          "r": 0.04819277108433735,
          "p": 0.10256410256410256,
          "f": 0.06557376614216637
        },
        "rouge-l": {
          "r": 0.140625,
          "p": 0.34615384615384615,
          "f": 0.1999999958913581
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2412.11019v1",
      "true_abstract": "The domain of hedge fund investments is undergoing significant\ntransformation, influenced by the rapid expansion of data availability and the\nadvancement of analytical technologies. This study explores the enhancement of\nhedge fund investment performance through the integration of machine learning\ntechniques, the application of PolyModel feature selection, and the analysis of\nfund size. We address three critical questions: (1) the effect of machine\nlearning on trading performance, (2) the role of PolyModel feature selection in\nfund selection and performance, and (3) the comparative reliability of larger\nversus smaller funds.\n  Our findings offer compelling insights. We observe that while machine\nlearning techniques enhance cumulative returns, they also increase annual\nvolatility, indicating variability in performance. PolyModel feature selection\nproves to be a robust strategy, with approaches that utilize a comprehensive\nset of features for fund selection outperforming more selective methodologies.\nNotably, Long-Term Stability (LTS) effectively manages portfolio volatility\nwhile delivering favorable returns. Contrary to popular belief, our results\nsuggest that larger funds do not consistently yield better investment outcomes,\nchallenging the assumption of their inherent reliability.\n  This research highlights the transformative impact of data-driven approaches\nin the hedge fund investment arena and provides valuable implications for\ninvestors and asset managers. By leveraging machine learning and PolyModel\nfeature selection, investors can enhance portfolio optimization and reassess\nthe dependability of larger funds, leading to more informed investment\nstrategies.",
      "generated_abstract": "r presents a novel approach for constructing hedge funds portfolios\nusing machine learning techniques. PolyModel is a multi-step regression model\nthat can handle complex multi-factor portfolio construction problems. We\npresented PolyModel as a flexible and scalable tool that can be used to\nconstrain hedge fund portfolio construction. PolyModel is a two-stage model.\nThe first stage is a regression model that models the relationship between\nmultiple factors, while the second stage is an optimization model that\noptimizes the model parameters. PolyModel is trained on a large dataset of\nhedge fund performance data. The results show that PolyModel outperforms\ntraditional hedge fund portfolio construction methods, with a median alpha of\n0.07% compared to a median alpha of 0.03% for traditional portfolio construction\nmethods. PolyModel also outperforms other machine learning approaches, such as\nXGBoost, Light",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19444444444444445,
          "p": 0.345679012345679,
          "f": 0.24888888428088898
        },
        "rouge-2": {
          "r": 0.019417475728155338,
          "p": 0.035398230088495575,
          "f": 0.025078365330923287
        },
        "rouge-l": {
          "r": 0.18055555555555555,
          "p": 0.32098765432098764,
          "f": 0.23111110650311117
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2502.07625v1",
      "true_abstract": "An empirical stochastic analysis of high-frequency, tick-by-tick order data\nof NASDAQ100 listed stocks is conducted using a first-order discrete-time\nMarkov chain model to explore intraday order transition dynamics. This analysis\nfocuses on three market cap categories: High, Medium, and Low. Time-homogeneous\ntransition probability matrices are estimated and compared across time-zones\nand market cap categories, and we found that limit orders exhibit higher degree\nof inertia (DoI), i.e., the probability of placing consecutive limit order is\nhigher, during the opening hour. However, in the subsequent hour, the DoI of\nlimit order decreases, while that of market order increases. Limit order\nadjustments via additions and deletions of limit orders increases significantly\nafter the opening hour. All the order transitions then stabilize during\nmid-hours. As the closing hour approaches, consecutive order executions surge,\nwith decreased placement of buy and sell limit orders following sell and buy\nexecutions, respectively. In terms of the differences in order transitions\nbetween stocks of different market cap, DoI of orders is stronger in high and\nmedium market cap stocks. On the other hand, lower market cap stocks show a\nhigher probability of limit order modifications and greater likelihood of\nsubmitting sell/buy limit orders after buy/sell executions. Further, order\ntransitions are clustered across all stocks, except during opening and closing\nhours. The findings of this study may be useful in understanding intraday order\nplacement dynamics across stocks of varying market cap, thus aiding market\nparticipants in making informed order placements at different times of trading\nhour.",
      "generated_abstract": "investigates intraday order transition dynamics in high, medium,\nand low market capitalization stocks in the United States over the period from\nAugust 2023 to February 2024. Using a novel approach, we model intraday\norder transition as a Markov chain, which enables us to model the order\ntransitions as a sequence of independent events. We develop a numerical\nsimulation methodology to obtain the conditional distribution of the order\ntransition probabilities, which is then used to estimate the order transition\ndynamics. Our findings suggest that the order transition dynamics differ\nacross market capitalizations. In particular, in low-capitalization stocks,\nthe order transition dynamics are more stationary than in high-capitalization\nstocks. Moreover, our results indicate that the order transition dynamics in\nlow-capitalization stocks are more volatile than in high-capitalization stocks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16783216783216784,
          "p": 0.32,
          "f": 0.22018348172502322
        },
        "rouge-2": {
          "r": 0.03070175438596491,
          "p": 0.06930693069306931,
          "f": 0.04255318723441253
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.29333333333333333,
          "f": 0.20183485787181224
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2503.05340v1",
      "true_abstract": "Matrix-valued time series analysis has gained prominence in econometrics and\nfinance due to the increasing availability of high-dimensional data with\ninherent matrix structures. Traditional approaches, such as Matrix\nAutoregressive (MAR) models and Dynamic Matrix Factor (DMF) models, often\nimpose restrictive assumptions that may not align with real-world data\ncomplexities. To address this gap, we propose a novel Matrix Autoregressive\nwith Common Factors (MARCF) model, which bridges the gap between MAR and DMF\nframeworks by introducing common bases between predictor and response\nsubspaces. The MARCF model achieves significant dimension reduction and enables\na more flexible and interpretable factor representation of dynamic\nrelationships. We develop a computationally efficient estimator and a gradient\ndescent algorithm. Theoretical guarantees for computational and statistical\nconvergence are provided, and extensive simulations demonstrate the robustness\nand accuracy of the model. Applied to a multinational macroeconomic dataset,\nthe MARCF model outperforms existing methods in forecasting and provides\nmeaningful insights into the interplay between countries and economic factors.",
      "generated_abstract": "We introduce a novel approach to time series analysis that leverages\nmatrix analysis techniques. Our framework combines autoregressive-common\nfactor (ACF) models with matrix-based methods to address the limitations of\ntraditional approaches. Specifically, we address the challenge of modeling\ntime series with missing values by leveraging matrix methods. We demonstrate\nthe efficacy of our approach through a simulation study and an application to\nthe stock market. Our findings highlight the potential of our approach for\ntime series analysis, offering a flexible and scalable solution for modeling\ncomplex time series.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17886178861788618,
          "p": 0.3793103448275862,
          "f": 0.24309391829675533
        },
        "rouge-2": {
          "r": 0.03870967741935484,
          "p": 0.07407407407407407,
          "f": 0.05084745311871629
        },
        "rouge-l": {
          "r": 0.16260162601626016,
          "p": 0.3448275862068966,
          "f": 0.2209944707829432
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/DS/2503.09802v1",
      "true_abstract": "We study the task of list-decodable linear regression using batches. A batch\nis called clean if it consists of i.i.d. samples from an unknown linear\nregression distribution. For a parameter $\\alpha \\in (0, 1/2)$, an unknown\n$\\alpha$-fraction of the batches are clean and no assumptions are made on the\nremaining ones. The goal is to output a small list of vectors at least one of\nwhich is close to the true regressor vector in $\\ell_2$-norm. [DJKS23] gave an\nefficient algorithm, under natural distributional assumptions, with the\nfollowing guarantee. Assuming that the batch size $n$ satisfies $n \\geq\n\\tilde{\\Omega}(\\alpha^{-1})$ and the number of batches is $m = \\mathrm{poly}(d,\nn, 1/\\alpha)$, their algorithm runs in polynomial time and outputs a list of\n$O(1/\\alpha^2)$ vectors at least one of which is\n$\\tilde{O}(\\alpha^{-1/2}/\\sqrt{n})$ close to the target regressor. Here we\ndesign a new polynomial time algorithm with significantly stronger guarantees\nunder the assumption that the low-degree moments of the covariates distribution\nare Sum-of-Squares (SoS) certifiably bounded. Specifically, for any constant\n$\\delta>0$, as long as the batch size is $n \\geq\n\\Omega_{\\delta}(\\alpha^{-\\delta})$ and the degree-$\\Theta(1/\\delta)$ moments of\nthe covariates are SoS certifiably bounded, our algorithm uses $m =\n\\mathrm{poly}((dn)^{1/\\delta}, 1/\\alpha)$ batches, runs in polynomial-time, and\noutputs an $O(1/\\alpha)$-sized list of vectors one of which is\n$O(\\alpha^{-\\delta/2}/\\sqrt{n})$ close to the target. That is, our algorithm\nachieves substantially smaller minimum batch size and final error, while\nachieving the optimal list size. Our approach uses higher-order moment\ninformation by carefully combining the SoS paradigm interleaved with an\niterative method and a novel list pruning procedure. In the process, we give an\nSoS proof of the Marcinkiewicz-Zygmund inequality that may be of broader\napplicability.",
      "generated_abstract": "er the problem of batch training linear regression models. In\nhigh-dimensional settings, this problem is often expressed as a regression\nproblem with a high-dimensional data matrix. A classical approach to solving\nthis problem is to train an additive model with an appropriate additive\nregression function and then use the coefficients of the additive model to\npredict the data matrix. However, this approach does not scale well for\nhigh-dimensional data. In this work, we introduce a new method for batch\ntraining linear regression models that combines batch list-decodable linear\nregression with the so-called higher moments technique. Our approach\nsignificantly outperforms existing batch training methods in terms of\nefficiency and generalization. We provide theoretical guarantees for our\nmethod, and we analyze the performance of the method on a variety of\nbenchmark datasets. Additionally, we introduce a new method for batch training\nlinear regression models using higher moments, which",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19135802469135801,
          "p": 0.3875,
          "f": 0.25619834268151087
        },
        "rouge-2": {
          "r": 0.03319502074688797,
          "p": 0.06504065040650407,
          "f": 0.04395603948149424
        },
        "rouge-l": {
          "r": 0.18518518518518517,
          "p": 0.375,
          "f": 0.24793387987159357
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2410.01864v1",
      "true_abstract": "This paper introduces a novel approach to optimizing portfolio rebalancing by\nintegrating Graph Neural Networks (GNNs) for predicting transaction costs and\nDijkstra's algorithm for identifying cost-efficient rebalancing paths. Using\nhistorical stock data from prominent technology firms, the GNN is trained to\nforecast future transaction costs, which are then applied as edge weights in a\nfinancial asset graph. Dijkstra's algorithm is used to find the least costly\npath for reallocating capital between assets. Empirical results show that this\nhybrid approach significantly reduces transaction costs, offering a powerful\ntool for portfolio managers, especially in high-frequency trading environments.\nThis methodology demonstrates the potential of combining advanced machine\nlearning techniques with classical optimization algorithms to improve financial\ndecision-making processes. Future research will explore expanding the asset\nuniverse and incorporating reinforcement learning for continuous portfolio\noptimization.",
      "generated_abstract": "ng portfolios is a critical task for investors to ensure the\nportfolio remains consistent with their investment objectives. In this paper,\nwe propose a novel dynamic portfolio rebalancing model that integrates\nneural-network-based pathfinding and Graph Neural Networks (GNNs) to enhance\nthe robustness and efficiency of portfolio rebalancing. The proposed model\nconsiders portfolio rebalancing as the optimization of the portfolio's\ndistribution in terms of its constituents. We first introduce a novel\nneural-network-based pathfinding method for finding the optimal path to\nrebalance the portfolio. Then, we develop a GNN-based framework to optimize\nthe rebalancing path by computing the expected utility of the portfolio,\nenhancing the robustness and efficiency of the portfolio rebalancing process.\nThe effectiveness of the proposed pathfinding and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21568627450980393,
          "p": 0.3055555555555556,
          "f": 0.25287355836702347
        },
        "rouge-2": {
          "r": 0.0390625,
          "p": 0.049019607843137254,
          "f": 0.04347825593345992
        },
        "rouge-l": {
          "r": 0.20588235294117646,
          "p": 0.2916666666666667,
          "f": 0.24137930549346026
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.00308v1",
      "true_abstract": "We propose a solution to the problem of bargaining with transfers, along with\nan axiomatisation of the solution. Inefficient allocations in the bargaining\nset can influence the solution, but are discounted relative to more efficient\nones. The key axioms are additivity and a property we call \\emph{inverse\nmonotonicity}, which states that adding an allocation to the bargaining set\nthat is worse for a given player than the initial solution cannot benefit that\nplayer.",
      "generated_abstract": "er a market where buyers and sellers are rational and transfers\nare available. We model the bargaining process by a sequential game with a\nbounded horizon, where buyers and sellers repeatedly choose their actions,\nfollowing a given strategy profile, until the bargaining process ends. We\nintroduce a novel notion of 'perfect information', which allows us to characterize\nthe bargaining process as a Markov chain. We characterize the equilibrium\nstrategies of the buyers and sellers and show that the market always ends in\nequilibrium. We also prove that, if buyers and sellers have perfect\ninformation, then the market always ends in a Nash equilibrium, i.e., the\nequilibrium outcome is unique and it is optimal. We also establish an\nasymptotic result: if the buyers and sellers' information is sufficiently\ndetailed, then the market converges to a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.28846153846153844,
          "p": 0.189873417721519,
          "f": 0.22900762880018657
        },
        "rouge-2": {
          "r": 0.043478260869565216,
          "p": 0.02654867256637168,
          "f": 0.03296702825926888
        },
        "rouge-l": {
          "r": 0.2692307692307692,
          "p": 0.17721518987341772,
          "f": 0.2137404532276675
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.09345v1",
      "true_abstract": "This report documents the development, test, and application of Large\nLanguage Models (LLMs) for automated text analysis, with a specific focus on\ngambling-like elements in digital games, such as lootboxes. The project aimed\nnot only to analyse user opinions and attitudes towards these mechanics, but\nalso to advance methodological research in text analysis. By employing\nprompting techniques and iterative prompt refinement processes, the study\nsought to test and improve the accuracy of LLM-based text analysis. The\nfindings indicate that while LLMs can effectively identify relevant patterns\nand themes on par with human coders, there are still challenges in handling\nmore complex tasks, underscoring the need for ongoing refinement in\nmethodologies. The methodological advancements achieved through this study\nsignificantly enhance the application of LLMs in real-world text analysis. The\nresearch provides valuable insights into how these models can be better\nutilized to analyze complex, user-generated content.",
      "generated_abstract": "r explores the potential of Large Language Models (LLMs) for\nAnalysing user-generated text data to determine the prevalence of\nin-game gambling-like elements (GLEs) in video games. By incorporating the\nLLM-derived text as a third data source, this study aims to provide a deeper\nunderstanding of the role of GLEs in gameplay and user experiences. The\napproach is based on the assumption that GLEs are likely to be present in\ngameplay text due to the presence of in-game elements, while text about\nnon-game-related topics is more likely to be created by non-player characters\n(NPCs) and does not necessarily reflect the real-life experiences of players.\nUsing the Star Wars: Jedi Challenges game as a case study, we demonstrate how\nthe LLM-derived text can be leveraged",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27522935779816515,
          "p": 0.3488372093023256,
          "f": 0.3076923027618672
        },
        "rouge-2": {
          "r": 0.05714285714285714,
          "p": 0.06956521739130435,
          "f": 0.06274509308727452
        },
        "rouge-l": {
          "r": 0.22935779816513763,
          "p": 0.29069767441860467,
          "f": 0.256410251479816
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-th/2503.10595v1",
      "true_abstract": "We present compact two-loop QCD corrections in the leading-color\napproximation for the production of an electroweak vector boson, $V =\n\\{W^{\\pm}, Z,\\gamma^\\star\\}$, in association with two light jets ($Vjj$) at\nhadron colliders. Leptonic decays of the electroweak boson are included at the\namplitude level. Working in the analytic-reconstruction approach, we develop\ntwo techniques to build compact partial-fraction forms for individual rational\nfunctions. One approach exploits their analytic structure. In the other, we\niteratively construct subtraction terms that match the rational functions in\nsingular limits. Moreover, we show how the singular behavior of the rational\nfunctions can be systematically used to find a more compact basis of the space\nthat they span. We apply our techniques to the $Vjj$ amplitudes, yielding a\nrepresentation that is three orders of magnitude smaller than previous results.\nWe then use these compact expressions to provide an efficient and stable C++\nnumerical implementation suitable for phenomenological applications.",
      "generated_abstract": "We present a compact two-loop calculation of the two-jet $Vjj$ production\nin proton-proton collisions. This is the first such calculation in the literature\nand is performed in the gauge-invariant framework of factorization. Our\nresults are in good agreement with the standard parton shower-based Monte\nCarlo simulation, with the factorization approximation providing an\naccurate description of the cross section. We also present the analytical\nstructure of the two-loop corrected amplitude, demonstrating that the two-loop\ncontributions to the jet mass and the polarization of the two jets are\nconstructed in a compact way.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2018348623853211,
          "p": 0.3728813559322034,
          "f": 0.26190475734764745
        },
        "rouge-2": {
          "r": 0.03496503496503497,
          "p": 0.060240963855421686,
          "f": 0.04424778296303597
        },
        "rouge-l": {
          "r": 0.1651376146788991,
          "p": 0.3050847457627119,
          "f": 0.21428570972859987
        }
      }
    },
    {
      "paper_id": "physics.soc-ph.econ/GN/2412.10421v1",
      "true_abstract": "The global food system provides the energy that supports human metabolism,\nwith complex spatial interdependencies between food production, transformation,\nand consumption. Empirical food system data for these global processes are\noften fragmented and inconsistent, with only certain components captured in\nspatially resolved formats. Here we propose a flexible approach to allocating\ncountry-level food system data subnationally. We estimate the spatial patterns\nof food energy production and supply, which we compare to estimates of human\nmetabolism based on average body size. We downscale these rates onto a\none-degree resolution grid with 95 corresponding food types to derive an\ninternally consistent, energy-conserving, and spatially resolved dataset. We\nshow that national food supply varies linearly with metabolism per capita, with\nabout half the variation in food supply explained by metabolic rates. Our data\nprocessing pipeline is openly available and can readily incorporate new inputs\nin order to advance trans-disciplinary food system modeling efforts.",
      "generated_abstract": "the energy and material inputs of the global food system, focusing\non the production and consumption of food, water, land, and waste. We use a\nspatially resolved, global, and dynamic model of the food system to estimate\nthe total energy and material inputs of the food system. Our approach is based\non the concept of energy closure, which states that the amount of energy\nconsumed in a system is equal to the sum of the energy input into the system\nand the energy output of the system. Using this concept, we derive a set of\nequations that determine the total energy and material inputs of the food\nsystem. We find that the global food system is characterized by four distinct\nenergy inputs: 1) land, 2) water, 3) agricultural energy, and 4) waste. The\ntotal energy and material inputs of the food system are 1.32 EJ and 6.12",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2641509433962264,
          "p": 0.3835616438356164,
          "f": 0.3128491571811118
        },
        "rouge-2": {
          "r": 0.04895104895104895,
          "p": 0.06306306306306306,
          "f": 0.05511810531558107
        },
        "rouge-l": {
          "r": 0.22641509433962265,
          "p": 0.3287671232876712,
          "f": 0.2681564197509442
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.09898v1",
      "true_abstract": "Dynamic simulation plays a crucial role in power system transient stability\nanalysis, but traditional numerical integration-based methods are\ntime-consuming due to the small time step sizes. Other semi-analytical solution\nmethods, such as the Differential Transformation method, often struggle to\nselect proper orders and steps, leading to slow performance and numerical\ninstability. To address these challenges, this paper proposes a novel adaptive\ndynamic simulation approach for power system transient\n  stability analysis. The approach adds feedback control and optimization to\nselecting the step and order, utilizing the Differential Transformation method\nand a proportional-integral control strategy to control truncation errors.\nOrder selection is formulated as an optimization problem resulting in a\nvariable-step-optimal-order method that achieves significantly larger time step\nsizes without violating numerical stability. It is applied to three systems:\nthe IEEE 9-bus, 3-generator system, IEEE 39-bus, 10-generator system, and a\nPolish 2383-bus, 327-generator system, promising computational efficiency and\nnumerical robustness for large-scale power system is demonstrated in\ncomprehensive case studies.",
      "generated_abstract": "r presents a novel method to efficiently simulate variable time-step\npower systems with a low-order differential transformation method. The\ndifferential transformation method is a special type of adaptive order\ndifferential transformation (AODDT) method, which is particularly suitable for\nvariable time-step simulations due to its low computational cost. AODDT is a\nwell-known method for variable time-step simulations. However, most existing\nmethods for variable time-step simulations are based on the fixed-order\nAODDT method, which has a low computational cost and high accuracy. However,\nthese methods are not suitable for variable time-step simulations due to their\nlow accuracy and computational cost. In this paper, we propose a novel method\nto simulate variable time-step power systems using an AODDT method with a low\ncomputational cost and high accuracy. This method uses a differential\ntransformation method to solve the differential equations",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17592592592592593,
          "p": 0.296875,
          "f": 0.22093022788534353
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.030927835051546393,
          "f": 0.024590159144384165
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.28125,
          "f": 0.20930232090859938
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ph/2503.10347v1",
      "true_abstract": "Through the black hole (BH) superradiance, ultralight bosons can form dense\nclouds around rotating Kerr BHs. Certain ultralight bosons, such as axions and\naxion-like particles (promising dark matter candidates), naturally possess\nself-interactions, and thus may significantly modify the dynamics of the\nsuperradiance process. Previous studies on the detection or constraint of\nultralight bosons through superradiance have largely neglected the\nself-interaction effects of bosons. In this work, we investigate the formation\nand evolution of self-interacting boson clouds in the full Kerr spacetime\nduring BH superradiance. Using numerical methods, we compute the superradiant\ngrowth rate of boson clouds with self-interactions around Kerr BHs and\nquantitatively evaluate how the self-interaction strength of scalar bosons\naffects the growth rate. We also assess the evolution of the BH's mass and\nspin. Our results reveal that, in addition to the superradiance-imposed upper\nbound on the boson cloud mass, self-interactions of ultralight bosons introduce\na new, lower critical mass limit, beyond which the growth rate of the boson\ncloud approaches zero. This implies that the superradiance process terminates\nearlier when self-interactions are considered. Furthermore, we explore how\nself-interactions affect both the oscillation frequency of boson clouds in\ngravitational atoms and the frequency of gravitational wave (GW) emitted\nthrough cloud annihilation. The anticipated frequency shift could be detectable\nby the GW observatories. Given that self-interactions substantially alter the\nevolution of BH superradiance, their effects can significantly relax existing\nconstraints on scalar boson models derived from superradiance. Taking the spin\nmeasurements from GW190412 and GW190517 as examples, we discuss the impact of\nself-interactions on constraint results in details.",
      "generated_abstract": "The Kerr black hole superradiance is a prominent phenomenon in the\nsuperradiant interaction of a massive particle with a black hole. In this\nletter, we study the self-interaction effects on the Kerr black hole superradiance\nand its observational implications. We find that the superradiance becomes\nsuppressed when the self-interaction is strong. In addition, the self-interaction\ninduced shift of the Kerr black hole horizon is also analyzed. Finally, we\nconclude that the self-interaction of the Kerr black hole significantly\nenhances the superradiance in the superradiant interaction, which may be\nobservable in the future gravitational wave experiments.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18831168831168832,
          "p": 0.5272727272727272,
          "f": 0.2775119578443717
        },
        "rouge-2": {
          "r": 0.04700854700854701,
          "p": 0.14666666666666667,
          "f": 0.0711974073271124
        },
        "rouge-l": {
          "r": 0.16883116883116883,
          "p": 0.4727272727272727,
          "f": 0.2488038238730799
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ex/2503.09280v1",
      "true_abstract": "An excess observed in the accelerator neutrino experiments in the $\\nu_{\\mu}\n\\rightarrow \\nu_{e}$ channel at high confidence level (CL) has been interpreted\nas due to eV-scale sterile neutrino(s). But, it has been suffered from the\nproblem of ``appearance - disappearance tension'' at the similarly high CL\nbecause the measurements of the $\\nu_{\\mu} \\rightarrow \\nu_{\\mu}$ channel do\nnot observe the expected event number depletion corresponding to the sterile\ncontribution in the appearance channel. We suggest non-unitarity as a simple\nand natural way of resolving the tension, which leads us to construct the\nnon-unitary $(3+1)$ model. With reasonable estimation of the parameters\ngoverning non-unitarity, we perform an illustrative analysis to know if the\ntension is resolved in this model. At the best fit of the appearance signature\nwe have found the unique solution with $\\sin^2 2\\theta_{14} \\approx 0.3$, which\nis consistent with the (reactors + Ga) data combined fit. Unexpectedly, our\ntension-easing mechanism bridges between the two high CL signatures, the BEST\nand LSND-MiniBooNE anomalies.",
      "generated_abstract": "igate the possibility of a sterile neutrino (sneutrino) with an\neV mass in the context of non-unitarity of the CKM matrix. This is motivated\nby the recent CKM-fitter results on the branching ratios of the $B \\to\nK_S \\mu^+\\mu^-$ decays. We find that the mass of the sneutrino is constrained\nto lie in the range $m_{\\tilde\\nu_\\mu} = 10$ eV to $1$ GeV, which is smaller\nthan the current experimental bound of $m_{\\tilde\\nu_\\mu} = 15$ eV. We also\ninvestigate the possibility of a light sterile neutrino (sneutrino) in the\nHiggs sector. This sneutrino is constrained to lie in the range $m_{\\tilde\\nu",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11206896551724138,
          "p": 0.22413793103448276,
          "f": 0.14942528291187754
        },
        "rouge-2": {
          "r": 0.01948051948051948,
          "p": 0.036585365853658534,
          "f": 0.025423724278943637
        },
        "rouge-l": {
          "r": 0.09482758620689655,
          "p": 0.1896551724137931,
          "f": 0.12643677716475113
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/TH/2503.00640v1",
      "true_abstract": "Laplacian matrices are commonly employed in many real applications, encoding\nthe underlying latent structural information such as graphs and manifolds. The\nuse of the normalization terms naturally gives rise to random matrices with\ndependency. It is well-known that dependency is a major bottleneck of new\nrandom matrix theory (RMT) developments. To this end, in this paper, we\nformally introduce a class of generalized (and regularized) Laplacian matrices,\nwhich contains the Laplacian matrix and the random adjacency matrix as a\nspecific case, and suggest the new framework of the asymptotic theory of\neigenvectors for latent embeddings with generalized Laplacian matrices\n(ATE-GL). Our new theory is empowered by the tool of generalized quadratic\nvector equation for dealing with RMT under dependency, and delicate high-order\nasymptotic expansions of the empirical spiked eigenvectors and eigenvalues\nbased on local laws. The asymptotic normalities established for both spiked\neigenvectors and eigenvalues will enable us to conduct precise inference and\nuncertainty quantification for applications involving the generalized Laplacian\nmatrices with flexibility. We discuss some applications of the suggested ATE-GL\nframework and showcase its validity through some numerical examples.",
      "generated_abstract": "p a general theory for eigenvectors of random matrices with\ngeneralized Laplacian matrices. This class of matrices generalizes the\nwell-known Laplacian matrices and includes numerous important classes such as\nthe Gaussian random matrix and the Gaussian Laplacian matrix. These matrices\narise in various contexts, including the Gaussian Ising model, random walk\nmodels, and the Gaussian Free Field. Our theory is based on a novel\ndistributional property of generalized Laplacian matrices. We show that the\ngeneralized Laplacian matrices arise naturally in a variety of random matrix\nmodels, including random matrix ensembles and Gaussian random matrix models. We\ndemonstrate that our eigenvalue theory extends to these models, with the main\nresult providing a rigorous framework for the eigenvectors of these models.\n  Our theory is based on the notion of an eigenvector entropy, which we show is\nrelated to the eigenvalue entropy in the special",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2543859649122807,
          "p": 0.4027777777777778,
          "f": 0.31182795224419013
        },
        "rouge-2": {
          "r": 0.08875739644970414,
          "p": 0.12396694214876033,
          "f": 0.10344827099904899
        },
        "rouge-l": {
          "r": 0.23684210526315788,
          "p": 0.375,
          "f": 0.29032257590010413
        }
      }
    },
    {
      "paper_id": "cs.AI.stat/OT/2502.14581v1",
      "true_abstract": "Empirical human-AI alignment aims to make AI systems act in line with\nobserved human behavior. While noble in its goals, we argue that empirical\nalignment can inadvertently introduce statistical biases that warrant caution.\nThis position paper thus advocates against naive empirical alignment, offering\nprescriptive alignment and a posteriori empirical alignment as alternatives. We\nsubstantiate our principled argument by tangible examples like human-centric\ndecoding of language models.",
      "generated_abstract": "human-AI alignment is an important research direction in AI\nhuman-machine intelligence (HMI). Empirical HMI research aims to identify and\nvalidate AI design principles that align with human perception, decision-making,\nand action. However, empirical HMI faces challenges due to the limited\nknowledge of human decision-making, the unpredictable nature of human\nperception, and the difficulty in distinguishing between human and machine\nperception. In this paper, we explore how these challenges affect empirical\nHMI research. First, we review the empirical HMI literature and identify the\nmajor challenges. Second, we propose a statistical framework to address these\nchallenges. Third, we analyze the empirical HMI literature to find a pattern\nof empirical results that are inconsistent with human decision-making. Our\nfindings provide evidence that empirical HMI research faces significant\nchalleng",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2711864406779661,
          "p": 0.21052631578947367,
          "f": 0.23703703211632382
        },
        "rouge-2": {
          "r": 0.046875,
          "p": 0.026785714285714284,
          "f": 0.034090904462810545
        },
        "rouge-l": {
          "r": 0.2711864406779661,
          "p": 0.21052631578947367,
          "f": 0.23703703211632382
        }
      }
    },
    {
      "paper_id": "physics.acc-ph.physics/acc-ph/2503.10122v1",
      "true_abstract": "Recently, the experimental discovery of a new type of wakefield effect, the\n\"skewed wake effect\", has been reported. We provide an explanation of the\nnature of the skewed wake effect based on a simple three-particle model that we\nhave developed. Taking a step forward, we analyze this effect for the case of a\nhighly elliptical beam, provide a simple estimate of the skew angle, and\nanalyze the wake amplitude effects associated with this effect.",
      "generated_abstract": "effect, also known as the skewed wake, is a phenomenon where the\ndistribution of the wake velocity is skewed and the wake is not centered\naround the Mach number. It is caused by the asymmetric shape of the inlet\nchannel and has been observed in various flow regimes, including low-pressure\nconditions. The skewed wake effect has been the focus of much research and\nnumerical modeling, but the analytical theory of the skewed wake effect has\nbeen limited to few-dimensional cases. In this study, we propose a new\nanalytical model that captures the skewed wake effect. We first derive a\ntime-dependent wake distribution function using a linear stability analysis of\nthe steady-state wake. Next, we use this function to derive the wake velocity\ndistribution function. Finally, we use the wake distribution function to derive\nthe skewed w",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.32,
          "p": 0.21333333333333335,
          "f": 0.2559999952000001
        },
        "rouge-2": {
          "r": 0.11764705882352941,
          "p": 0.07207207207207207,
          "f": 0.089385470148872
        },
        "rouge-l": {
          "r": 0.26,
          "p": 0.17333333333333334,
          "f": 0.20799999520000012
        }
      }
    },
    {
      "paper_id": "econ.TH.q-fin/PM/2410.19030v3",
      "true_abstract": "We present a theory of expected utility with state-dependent linear utility\nfunctions for monetary returns, that incorporates the possibility of\nloss-aversion. Our results relate to first order stochastic dominance,\nmean-preserving spread, increasing-concave linear utility profiles and risk\naversion. As an application of the expected utility theory developed here, we\nanalyze the contract that a monopolist would offer in an insurance market that\nallowed for partial coverage of loss.",
      "generated_abstract": "We consider a monetary economy with a state-dependent linear utility function\nfor monetary returns and a loss aversion parameter. The state-dependent utility\nfunction is endowed with a continuous-time market clearing model. The model\nallows for a state-dependent choice of the state of the market. The model is\nbased on the assumption that the state of the market is influenced by the\nutility function, and that the state of the market influences the utility\nfunction. The model can be used to explain the observed market behavior. We\nalso show that the model can be used to analyze the effect of a loss aversion\nparameter on the market behavior. In particular, the model can be used to\npredict the behavior of the market in a certain situation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.34615384615384615,
          "p": 0.33962264150943394,
          "f": 0.3428571378575964
        },
        "rouge-2": {
          "r": 0.07692307692307693,
          "p": 0.056179775280898875,
          "f": 0.06493506005650232
        },
        "rouge-l": {
          "r": 0.3269230769230769,
          "p": 0.32075471698113206,
          "f": 0.3238095188099774
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/OT/2412.11211v1",
      "true_abstract": "State-space models (SSMs) offer a powerful framework for dynamical system\nanalysis, wherein the temporal dynamics of the system are assumed to be\ncaptured through the evolution of the latent states, which govern the values of\nthe observations. This paper provides a selective review of recent advancements\nin deep neural network-based approaches for SSMs, and presents a unified\nperspective for discrete time deep state space models and continuous time ones\nsuch as latent neural Ordinary Differential and Stochastic Differential\nEquations. It starts with an overview of the classical maximum likelihood based\napproach for learning SSMs, reviews variational autoencoder as a general\nlearning pipeline for neural network-based approaches in the presence of latent\nvariables, and discusses in detail representative deep learning models that\nfall under the SSM framework. Very recent developments, where SSMs are used as\nstandalone architectural modules for improving efficiency in sequence modeling,\nare also examined. Finally, examples involving mixed frequency and\nirregularly-spaced time series data are presented to demonstrate the advantage\nof SSMs in these settings.",
      "generated_abstract": "The development of deep learning methods for modeling dynamical systems has\nlead to advancements in both model selection and parameter estimation,\nparticularly for systems with complex dynamics. In this review, we explore\nseveral approaches for state space models, focusing on their applications in\nlearning and parameter estimation. We begin by reviewing the fundamentals of\nstate space models, including their definition and properties, including the\ndifference between state and parameter spaces. We then discuss several\napproaches for state space model selection and parameter estimation, including\nKullback-Leibler divergence, maximum likelihood, and expectation-maximization\nalgorithms. Finally, we explore the use of deep learning methods in these\napplications, including recurrent neural networks (RNNs), deep convolutional\nneural networks (DCNNs), and transformer-based models. We conclude by\ndiscussing future directions for these approaches and their applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.2631578947368421,
          "f": 0.21505375860793166
        },
        "rouge-2": {
          "r": 0.031055900621118012,
          "p": 0.045871559633027525,
          "f": 0.0370370322224972
        },
        "rouge-l": {
          "r": 0.14545454545454545,
          "p": 0.21052631578947367,
          "f": 0.17204300591975966
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/SE/2503.09463v1",
      "true_abstract": "Co-developing scientific algorithms and hardware accelerators requires\ndomain-specific knowledge and large engineering resources. This leads to a slow\ndevelopment pace and high project complexity, which creates a barrier to entry\nthat is too high for the majority of developers to overcome. We are developing\na reusable end-to-end compiler toolchain for the Julia language entirely built\non permissively-licensed open-source projects. This unifies accelerator and\nalgorithm development by automatically synthesising Julia source code into\nhigh-performance Verilog.",
      "generated_abstract": "This document is the work in progress for the Hardware.jl package. It serves as\na documentation and code repository for the Julia package. The code is\ncurrently in a work-in-progress state. It is currently undergoing a complete\nredesign, which will be outlined in a separate document.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14754098360655737,
          "p": 0.28125,
          "f": 0.19354838258295767
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.046511627906976744,
          "f": 0.03478260401361122
        },
        "rouge-l": {
          "r": 0.14754098360655737,
          "p": 0.28125,
          "f": 0.19354838258295767
        }
      }
    },
    {
      "paper_id": "math.FA.math/AP/2503.09536v1",
      "true_abstract": "We characterise the normal trace space associated to extended\n(measure-valued) divergence-measure fields on the boundary of a set $E \\subset\n\\mathbb R^n$, as the Arens-Eells space $\\mathrm{AE}(\\partial E)$. Such a trace\noperator is constructed for any Borel set $E$, and under a mild regularity\ncondition, which includes Lipschitz domains, this trace operator is shown to\nmoreover be surjective. This relies in part on a new pointwise description of\nthe Anzellotti pairing $\\overline{\\nabla \\phi \\cdot {\\boldsymbol F}}$ between a\n$\\mathrm{W}^{1,\\infty}$ function $\\phi$ and extended divergence-measure field\n${\\boldsymbol F}$. As an application, we prove extension theorems for\ndivergence-measure fields and divergence-free measures. Results for\n$\\mathrm{L}^1$-fields are also obtained.",
      "generated_abstract": "In this paper, we prove that the normal trace space of extended divergence\nmeasures is the set of the traces of a bounded measurable function. As a\nconsequence, we obtain that the trace space of an extended divergence measure\nis a closed subspace of the extended Sobolev space.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1951219512195122,
          "p": 0.5517241379310345,
          "f": 0.2882882844282121
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.14285714285714285,
          "f": 0.08333332920138908
        },
        "rouge-l": {
          "r": 0.17073170731707318,
          "p": 0.4827586206896552,
          "f": 0.252252248392176
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.astro-ph/EP/2503.08670v1",
      "true_abstract": "The Taurid Complex is a large interplanetary system that contains comet\n2P/Encke, several meteoroid streams, and possibly a number of near-Earth\nasteroids. The size and nature of the system has led to the speculation that it\nwas formed through a large-scale cometary breakup. Numerical investigations\nhave suggested that planetary dynamics can create a resonant region with a\nlarge number of objects concentrated in a small segment of the orbit, known as\nthe Taurid swarm, which approaches the Earth in certain years and provides\nfavorable conditions to study the Taurid Complex. Recent meteor observations\nconfirmed the existence of the swarm for mm- to m-sized objects. Here we\npresent a dedicated telescopic search for potentially hazardous asteroids and\nother macroscopic objects in the Taurid swarm using the Zwicky Transient\nFacility survey. We determine from our non-detection that there are no more\nthan 9--14 $H\\leq24$ (equivalent to a diameter of $D\\gtrsim100$~m) objects in\nthe swarm, suggesting that the Encke--Taurid progenitor was $\\sim10$~km in\nsize. A progenitor of such a size is compatible with the prediction of\nstate-of-the-art Solar System dynamical models, which expects $\\sim0.1$\n$D>10$~km objects on Encke-like orbits at any given time.",
      "generated_abstract": "d meteor shower is a prominent celestial event occurring around\nthe time of the perigee of the cometary nucleus, and the Taurid swarm is the\nprimary source of meteoroids in the shower. The swarm consists of a number of\nsub-swarms, the most important of which is the Taurid 1 sub-swarm, the\nTaurid 2 sub-swarm, the Taurid 3 sub-swarm, and the Taurid 4 sub-swarm. The\nTaurid 1 and 2 sub-swarms are relatively well-studied, and the Taurid 3 and 4\nsub-swarms have been investigated less extensively. The Taurid 3 sub-swarm is\nconsidered to be the most hazardous in terms of potential impacts on Earth. In",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15748031496062992,
          "p": 0.36363636363636365,
          "f": 0.219780215562734
        },
        "rouge-2": {
          "r": 0.04419889502762431,
          "p": 0.09195402298850575,
          "f": 0.05970148815242848
        },
        "rouge-l": {
          "r": 0.13385826771653545,
          "p": 0.3090909090909091,
          "f": 0.18681318259570112
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.08802v1",
      "true_abstract": "Recent literature highlights the advantages of implementing social rules via\ndynamic game forms. We characterize when truth-telling remains a dominant\nstrategy in gradual mechanisms implementing strategy-proof social rules, where\nagents gradually reveal their private information while acquiring information\nabout others in the process. Our first characterization hinges on the\nincentive-preservation of a basic transformation on gradual mechanisms called\nilluminating that partitions information sets. The second relies on a single\nreaction-proofness condition. We demonstrate the usefulness of both\ncharacterizations through applications to second-price auctions and the\ntop-trading cycles algorithm.",
      "generated_abstract": "A new framework is introduced to study the dynamics of gradual mechanisms\nin a game-theoretic setting. In contrast to the classical framework, the\nparticipants are not only strategic but also have a preference over truthful\ninformation. In the classical framework, the participants are always truthful,\nwhile in our setting, the preference over truthfulness is not fixed. We prove\nthat a gradual mechanism is either truthful or truth-telling, and show that the\nfirst case is the unique truthful mechanism. Moreover, we show that the\ntruthfulness of a gradual mechanism depends only on the preference over\ntruthfulness. This is not the case for truth-telling mechanisms. This is the\nfirst time that the truthfulness of a gradual mechanism is characterized as a\nfunction of the preference over truthfulness.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21428571428571427,
          "p": 0.24193548387096775,
          "f": 0.22727272229109283
        },
        "rouge-2": {
          "r": 0.03488372093023256,
          "p": 0.030927835051546393,
          "f": 0.032786880263968025
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.22580645161290322,
          "f": 0.21212120713957774
        }
      }
    },
    {
      "paper_id": "math.FA.math/FA/2503.10467v1",
      "true_abstract": "This note is to advertise the concept of directed completion of partial\norders as the natural analogue of Cauchy-completion in Lorentzian signature,\nespecially in relation to non-smooth and/or infinite dimensional geometries.\n  A closely related notion appeared in the recent [2] where it was used as\nground for further development of non-smooth calculus in metric spacetimes\nallowing, for instance, for a quite general limit curve theorem in such\nsetting. The proposal in [2] was also in part motivated by the discussion we\nmake here, key points being:\n  - A general existence result, already obtained in the context of theoretical\ncomputer science and for which we give a slightly different presentation.\n  - An example showing that an analogue two-sided completion is unsuitable from\nthe (or at least `some') geometric/analytic perspective.\n  - The existence of natural links with both the the concept of ideal point of\nGeroch--Kronheimer--Penrose and with Beppo Levi's monotone convergence theorem,\nshowing in particular an underlying commonality between these two seemingly far\nconcepts.\n  - The flexibility of the notion, that by nature can cover non-smooth and\ninfinite-dimensional situations.\n  - The fact that the concept emerges spontaneously when investigating the\nduality relations between $L^p$ and $L^q$ spaces where $\\tfrac1p+\\tfrac 1q = 1$\nare H\\\"older conjugate exponents with $p, q < 1$.\n  This note is part of a larger work in progress that aims at laying the\ngrounds of a Lorentzian, or Hyperbolic, theory of Banach spaces. Given that the\nnotion of completion has nothing to do with the linear structure and the\ngrowing interest around this topic, for instance related to convergence of\ngeometric structures, we make available these partial findings.",
      "generated_abstract": "In this paper we consider a set of hyperbolic Banach spaces $X_1, \\ldots,\nX_m$ with the partial order $\\prec$, and we consider the set $X=X_1\\uplus\\cdots\n\\uplus X_m$. We consider the following problem.\n  \\begin{enumerate}\n  \\item[(i)",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.05847953216374269,
          "p": 0.37037037037037035,
          "f": 0.10101009865472917
        },
        "rouge-2": {
          "r": 0.007751937984496124,
          "p": 0.06451612903225806,
          "f": 0.0138408285346203
        },
        "rouge-l": {
          "r": 0.05263157894736842,
          "p": 0.3333333333333333,
          "f": 0.09090908855371907
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.17810v2",
      "true_abstract": "In recent years, with advances in large language models (LLMs), end-to-end\nspoken dialogue models (SDMs) have made significant strides. Compared to\ntext-based LLMs, the evaluation of SDMs needs to take speech-related aspects\ninto account, such as paralinguistic information and speech quality. However,\nthere is still a lack of comprehensive evaluations for SDMs in speech-to-speech\n(S2S) scenarios. To address this gap, we propose URO-Bench, an extensive\nbenchmark for SDMs. Notably, URO-Bench is the first S2S benchmark that covers\nevaluations about multilingualism, multi-round dialogues, and paralinguistics.\nOur benchmark is divided into two difficulty levels: basic track and pro track,\nconsisting of 16 and 20 datasets respectively, evaluating the model's abilities\nin Understanding, Reasoning, and Oral conversation. Evaluations on our proposed\nbenchmark reveal that current open-source SDMs perform rather well in daily QA\ntasks, but lag behind their backbone LLMs in terms of instruction-following\nability and also suffer from catastrophic forgetting. Their performance in\nadvanced evaluations of paralinguistic information and audio understanding\nremains subpar, highlighting the need for further research in this direction.\nWe hope that URO-Bench can effectively facilitate the development of spoken\ndialogue models by providing a multifaceted evaluation of existing models and\nhelping to track progress in this area.",
      "generated_abstract": "advancement of spoken dialogue systems (SDS) has driven the\ndevelopment of new benchmarks for evaluating performance. Despite the\nconsiderable progress in this area, there are still gaps in the evaluation of\nend-to-end models for spoken dialogue. To address this gap, we introduce URO-Bench,\na comprehensive benchmark for end-to-end spoken dialogue models, including\nmulti-turn, multi-turn dialogue, and multi-turn speech. URO-Bench includes 21\ntasks and 139 datasets, covering all the key components of the field, including\ndialogue management, speech recognition, and speaker-dependent responses. Our\nevaluation protocols are designed to measure end-to-end performance across all\ntasks. Furthermore, we introduce a new benchmark for multi-turn speech\nrecognition, SAD-Bench, which includes 62 datasets and 1",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20689655172413793,
          "p": 0.4225352112676056,
          "f": 0.2777777733646262
        },
        "rouge-2": {
          "r": 0.07329842931937172,
          "p": 0.13592233009708737,
          "f": 0.09523809068605694
        },
        "rouge-l": {
          "r": 0.18620689655172415,
          "p": 0.38028169014084506,
          "f": 0.2499999955868485
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.05676v1",
      "true_abstract": "Multidimensional poverty measurement is crucial for capturing deprivation\nbeyond income-based metrics. This study compares the Alkire-Foster (AF) method\nand a Markov Random Field (MRF) approach for classifying multidimensional\npoverty using a simulation-based analysis. The AF method applies a\ndeterministic threshold-based classification, while the MRF approach leverages\nprobabilistic graphical modelling to account for correlations between\ndeprivation indicators. Using a synthetic dataset of 50,000 individuals with\nten binary deprivation indicators, we assess classification accuracy, false\npositive/negative trade-offs, and agreement between the methods. Results show\nthat AF achieves higher classification accuracy (89.5%) compared to MRF\n(75.4%), with AF minimizing false negatives and MRF reducing false positives.\nThe overall agreement between the two methods is 65%, with discrepancies\nprimarily occurring when AF classifies individuals as poor while MRF does not.\nWhile AF is transparent and easy to implement, it does not capture\ninterdependencies among indicators, potentially leading to misclassification.\nMRF, though computationally intensive, offers a more nuanced understanding of\ndeprivation clusters. These findings highlight the trade-offs in\nmultidimensional poverty measurement and provide insights for policymakers on\nmethod selection based on data availability and policy objectives. Future\nresearch should extend these approaches to non-binary indicators and real-world\ndatasets.",
      "generated_abstract": "This paper examines the Alkire-Foster method for multidimensional\npoverty analysis. It compares the method to a Markov random field approach\nbased on the Cobb-Douglas utility model, which is widely used in the\npoverty analysis literature. The method is compared to the Alkire-Foster\nmethod in terms of computational efficiency and accuracy. The results indicate\nthat the Alkire-Foster method is more computationally efficient than the Markov\nrandom field approach, while the methodology is more accurate. The results also\ndemonstrate that the Alkire-Foster method is suitable for analyzing\nmulti-dimensional data, including household-level data and data from\nsub-communities. The paper concludes with a discussion of the advantages and\nlimitations of each method and suggestions for future research.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2116788321167883,
          "p": 0.4264705882352941,
          "f": 0.28292682483474124
        },
        "rouge-2": {
          "r": 0.042328042328042326,
          "p": 0.08163265306122448,
          "f": 0.05574912442253797
        },
        "rouge-l": {
          "r": 0.19708029197080293,
          "p": 0.39705882352941174,
          "f": 0.26341462971279006
        }
      }
    },
    {
      "paper_id": "cs.SI.cs/SI/2503.07695v1",
      "true_abstract": "On February 7, 2024, Russian President Vladimir Putin gave a two-hour\ninterview with conservative political commentator, Tucker Carlson. This study\ninvestigated the impact of the Carlson- Putin interview on the US X audience.\nWe proposed a framework of social media impact using machine learning (ML) and\nnatural language processing (NLP) by measuring changes in audience, structure,\nand content. Triangulation methods were used to validate the process and\nresults. The interview had a considerable impact among segments of the American\npublic: 1) the reach and engagement of far-right influencers increased after\nthe interview, suggesting Kremlin narratives gained traction within these\ncircles, 2) the communication structure became more vulnerable to\ndisinformation spread after the interview, and 3) the public discourse changed\nfrom support for Ukraine funding to conversations about Putin, Russia, and the\nissue of \"truth\" or the veracity of Putin's claims. This research contributes\nto methods development for social media studies and aids scholars in analyzing\nhow public opinion shapes policy debates. The Carlson-Putin interview sparked a\nbroader discussion about truth-telling. Far from being muted, the broad impact\nof the interview appears considerable and poses challenges for foreign affairs\nleaders who depend on public support and buy-in when formulating national\npolicy.",
      "generated_abstract": "This study examines how the Tucker Carlson interview with Vladimir\nPutin affects perceptions of the Russian leader and his policies in the United\nStates. We develop a machine learning model to analyze the interview, and\ncompare the results to data from the 2016 election. Our analysis reveals that\nthe interview increases Putin's favorability and decreases the public's\nperception of Russia's role in the world. However, the analysis also shows\nthat the interview has no effect on the public's perception of American\nleadership and the American political system. This study highlights the\nlimitations of machine learning in analyzing political and public opinion\ndata and offers practical recommendations for future research. By examining the\nimpact of one interview, we provide insight into the potential of machine\nlearning for political and public opinion research, and suggest future\nresearch directions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22695035460992907,
          "p": 0.38095238095238093,
          "f": 0.28444443976533335
        },
        "rouge-2": {
          "r": 0.08290155440414508,
          "p": 0.1322314049586777,
          "f": 0.10191082328836891
        },
        "rouge-l": {
          "r": 0.16312056737588654,
          "p": 0.27380952380952384,
          "f": 0.20444443976533347
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.21268v1",
      "true_abstract": "Gene drive alleles bias their own inheritance to offspring. They can fix in a\nwild-type population in spite of a fitness cost, and even lead to the\neradication of the target population if the fitness cost is high. However, this\noutcome may be prevented or delayed if areas previously cleared by the drive\nare recolonised by wild-type individuals. Here, we investigate the conditions\nunder which these stochastic wild-type recolonisation events are likely and\nwhen they are unlikely to occur in one spatial dimension. More precisely, we\nexamine the conditions ensuring that the last individual carrying a wild-type\nallele is surrounded by a large enough number of drive homozygous individuals,\nresulting in a very low chance of wild-type recolonisation. To do so, we make a\ndeterministic approximation of the distribution of drive alleles within the\nwave, and we split the distribution of wild-type alleles into a deterministic\npart and a stochastic part. Our analytical and numerical results suggest that\nthe probability of wild-type recolonisation events increases with lower fitness\nof drive individuals, with smaller migration rate, and also with smaller local\ncarrying capacity. Numerical simulations show that these results extend to two\nspatial dimensions. We also demonstrate that, if a wild-type recolonisation\nevent were to occur, the probability of a following drive reinvasion event\ndecreases with smaller values of the intrinsic growth rate of the population.\nOverall, our study paves the way for further analysis of wild-type\nrecolonisation at the back of eradication traveling waves.",
      "generated_abstract": "Gene drives are a tool for eradicating invasive species and have been\nused to successfully remove the brown marmorated stinkbug from New York. A\nsignificant challenge in their deployment is their potential for accelerating\nthe spread of other invasive species. Here, we examine the role of stochastic\ndynamics in a model of gene drive propagation. We find that, even in the case\nof a single gene drive, stochasticity can significantly influence the spread\nof a second gene drive, even when the first drive is relatively inactive. The\nresults suggest that stochasticity should be considered in the design of\ngene drives, particularly when multiple drives are considered. Our findings\nhighlight the need to account for stochasticity in the design of gene drives\nand underscore the importance of careful modeling and experimental\nvalidation in gene drive research.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18571428571428572,
          "p": 0.30952380952380953,
          "f": 0.23214285245535726
        },
        "rouge-2": {
          "r": 0.032407407407407406,
          "p": 0.05785123966942149,
          "f": 0.0415430221035675
        },
        "rouge-l": {
          "r": 0.17142857142857143,
          "p": 0.2857142857142857,
          "f": 0.21428570959821439
        }
      }
    },
    {
      "paper_id": "physics.ins-det.physics/ins-det/2503.09495v1",
      "true_abstract": "The calibration of the CR39 and Makrofol Nuclear Track Detectors of the\nMoEDAL experiment at the CERN-LHC was performed by exposing stacks of detector\nfoils to heavy ion beams with energies ranging from 340 MeV/nucleon to 150\nGeV/nucleon. After chemical etching, the base areas and lengths of etch-pit\ncones were measured using automatic and manual optical microscopes. The\nresponse of the detectors, as measured by the ratio of the track-etching rate\nover the bulk-etching rate, was determined over a range extending from their\nthreshold at Z/$\\beta\\sim7$ and $\\sim50$ for CR39 and Makrofol, respectively,\nup to Z/$\\beta\\sim92$",
      "generated_abstract": "te Nuclear Track Detectors (SSNTDs) are becoming an attractive\noption for rare event searches, offering significant improvements in\nefficiency, sensitivity, and speed. However, calibration remains an\nunderexplored challenge. This study investigates the potential for SSNTDs to\nachieve improved performance compared to traditional track detectors, using\ndata collected at the Fermi Large Area Telescope (LAT) in a search for\nrare-event signals. To this end, we introduce an approach that utilizes the\nSSNTD's inherent time resolution to improve the precision of track reconstruction\nin the presence of noise, and a model-based calibration method for the\ncalibration of SSNTDs. Our results indicate that SSNTDs with a single-chip\nsolid-state detector exhibit up to 2.4 times better precision compared to\ntrad",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22857142857142856,
          "p": 0.18181818181818182,
          "f": 0.20253164063451382
        },
        "rouge-2": {
          "r": 0.054945054945054944,
          "p": 0.043478260869565216,
          "f": 0.04854368438825576
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.1590909090909091,
          "f": 0.17721518493831132
        }
      }
    },
    {
      "paper_id": "math.DG.math-ph/2503.10208v1",
      "true_abstract": "We explore the Jordan-Chevalley decomposition problem for an operator field\nin small dimensions. In dimensions three and four, we find tensorial conditions\nfor an operator field $L$, similar to a nilpotent Jordan block, to possess\nlocal coordinates in which $L$ takes a strictly upper triangular form. We prove\nthe Tempesta-Tondo conjecture for higher order brackets of\nFr\\\"olicher-Nijenhuis type.",
      "generated_abstract": "We prove a Jordan-Chevalley decomposition theorem for operator fields in\nsmall dimensions. For this purpose, we first prove a decomposition theorem for\nthe operator field on the group of invertible real matrices in small\ndimensions. Then we use this result to prove the decomposition theorem for the\noperator field on the group of invertible real matrices with the property\nthat its center is an ideal of finite index. In this case, we construct a\nsuitable Jordan-Chevalley decomposition of the operator field on the group of\ninvertible real matrices with the property that its center is an ideal of\nfinite index.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3617021276595745,
          "p": 0.3953488372093023,
          "f": 0.37777777278765434
        },
        "rouge-2": {
          "r": 0.1111111111111111,
          "p": 0.1,
          "f": 0.1052631529085875
        },
        "rouge-l": {
          "r": 0.2978723404255319,
          "p": 0.32558139534883723,
          "f": 0.31111110612098775
        }
      }
    },
    {
      "paper_id": "math.OC.q-fin/RM/2502.16364v1",
      "true_abstract": "As the developed world replaces Defined Benefit (DB) pension plans with\nDefined Contribution (DC) plans, there is a need to develop decumulation\nstrategies for DC plan holders. Optimal decumulation can be viewed as a problem\nin optimal stochastic control. Formulation as a control problem requires\nspecification of an objective function, which in turn requires a definition of\nreward and risk. An intuitive specification of reward is the total withdrawals\nover the retirement period. Most retirees view risk as the possibility of\nrunning out of savings. This paper investigates several possible left tail risk\nmeasures, in conjunction with DC plan decumulation. The risk measures studied\ninclude (i) expected shortfall (ii) linear shortfall and (iii) probability of\nshortfall. We establish that, under certain assumptions, the set of optimal\ncontrols associated with all expected reward and expected shortfall Pareto\nefficient frontier curves is identical to the set of optimal controls for all\nexpected reward and linear shortfall Pareto efficient frontier curves. Optimal\nefficient frontiers are determined computationally for each risk measure, based\non a parametric market model. Robustness of these strategies is determined by\ntesting the strategies out-of-sample using block bootstrapping of historical\ndata.",
      "generated_abstract": "r considers the problem of decumulation of DC pension plans and\nis concerned with the identification of the appropriate risk measure for the\ndecedent's assets, taking into account the risks associated with the pension\nplan's unfunded liabilities. We formulate the problem as a two-stage\noptimal control problem. In the first stage, we solve a stochastic control\nproblem to identify the optimal investment strategy and risk measure for the\ndecedent's assets. In the second stage, we solve a control problem to determine\nthe optimal decedent's withdrawal strategy, taking into account the risk\nmeasures identified in the first stage. We prove that the optimal decedent's\nwithdrawal strategy satisfies the required conditions and provide a closed-form\nsolution for the optimal decedent's withdrawal policy. Numerical examples are\npresented to illustrate the performance of the proposed",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19327731092436976,
          "p": 0.32857142857142857,
          "f": 0.2433862387223203
        },
        "rouge-2": {
          "r": 0.046511627906976744,
          "p": 0.07692307692307693,
          "f": 0.05797100979626167
        },
        "rouge-l": {
          "r": 0.16806722689075632,
          "p": 0.2857142857142857,
          "f": 0.21164020697628858
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.21505v2",
      "true_abstract": "The Gulf Cooperation Council countries -- Oman, Bahrain, Kuwait, UAE, Qatar,\nand Saudi Arabia -- holds strategic significance due to its large oil reserves.\nHowever, these nations face considerable challenges in shifting from\noil-dependent economies to more diversified, knowledge-based systems. This\nstudy examines the progress of Gulf Cooperation Council (GCC) countries in\nachieving economic diversification and social development, focusing on the\nSocial Progress Index (SPI), which provides a broader measure of societal\nwell-being beyond just economic growth. Using data from the World Bank,\ncovering 2010 to 2023, the study employs the XGBoost machine learning model to\nforecast SPI values for the period of 2024 to 2026. Key components of the\nmethodology include data preprocessing, feature selection, and the simulation\nof independent variables through ARIMA modeling. The results highlight\nsignificant improvements in education, healthcare, and women's rights,\ncontributing to enhanced SPI performance across the GCC countries. However,\nnotable challenges persist in areas like personal rights and inclusivity. The\nstudy further indicates that despite economic setbacks caused by global\ndisruptions, including the COVID-19 pandemic and oil price volatility, GCC\nnations are expected to see steady improvements in their SPI scores through\n2027. These findings underscore the critical importance of economic\ndiversification, investment in human capital, and ongoing social reforms to\nreduce dependence on hydrocarbons and build knowledge-driven economies. This\nresearch offers valuable insights for policymakers aiming to strengthen both\nsocial and economic resilience in the region while advancing long-term\nsustainable development goals.",
      "generated_abstract": "advancements in technology and knowledge have changed the\nglobal landscape and affected the economies of the Gulf Cooperation\nCountries (GCC). The oil-dependent economies of the GCC countries have\nundergone major transformations from being dependent on oil to being\nknowledge-based economies. This study aims to analyze the economic diversification\nprocess and the impact of economic diversification on social progress in the\nGCC countries. The data used in the study include the World Economic\nFair Index (WEFI) and the Gulf Economic Diversification Index (GEDI). The\nresults show that economic diversification has led to significant changes in\nGCC countries, with the impact on social progress varying depending on the\ncountry. The results also show that economic diversification has a positive\ninfluence on social progress, but this effect is stronger in countries with a\nhigh level of economic diversification. Furthermore, the results",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1871345029239766,
          "p": 0.42105263157894735,
          "f": 0.25910930748053573
        },
        "rouge-2": {
          "r": 0.0635593220338983,
          "p": 0.1282051282051282,
          "f": 0.0849858312622686
        },
        "rouge-l": {
          "r": 0.16374269005847952,
          "p": 0.3684210526315789,
          "f": 0.22672064351292434
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.19557v2",
      "true_abstract": "We study how social image concerns affect information sharing patterns\nbetween peers. An individual receives a signal (\"news\") about the state of the\nworld and can either share it with a peer or not. This signal has two\nattributes: a headline -- e.g., arguing for or against human-induced climate\nchange -- and a veracity status, indicating if the signal is based on facts or\nmade-up. The headline is observable at no cost by everyone, while observing the\nveracity status is costly and the cost depends on an individual's type. We\nstudy the sharing patterns induced by two different types of social image\nconcern: wanting to be perceived as talented, which implies being able to\ndistinguish proper from fake news, and wanting to signal one's worldview. Our\nmodel can rationalize the empirical finding that fake news may be shared with a\nhigher propensity than proper news (e.g., Vosoughi et al., 2018). We show that\nboth a veracity and a worldview concern may rationalize this finding, though\nsharing patterns are empirically distinguishable and welfare implications\ndiffer.",
      "generated_abstract": "is a prominent challenge in the digital age, where people may\nbe misinformed about a variety of topics, including politics, business, and\nsports. In this paper, we introduce a model of social image formation and\nconcerns regarding fake news to study how information sharing influences\nfake news spread. We introduce a two-dimensional model in which people use their\nsocial images to form their beliefs about a topic. In the model, people use\nsocial images to influence their beliefs and then share their beliefs with\nothers. The shared beliefs influence the spread of fake news. We derive the\nsteady-state distribution of fake news, and show that it can be either\nincreasing or decreasing. We also derive the steady-state distribution of\nfalse news, and show that it can be either increasing or decreasing. We show\nthat the false news distribution can be a decreasing function of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2727272727272727,
          "p": 0.44594594594594594,
          "f": 0.3384615337520053
        },
        "rouge-2": {
          "r": 0.0650887573964497,
          "p": 0.09401709401709402,
          "f": 0.07692307208836648
        },
        "rouge-l": {
          "r": 0.23140495867768596,
          "p": 0.3783783783783784,
          "f": 0.287179482469954
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2503.01362v1",
      "true_abstract": "This paper describes a streaming audio-to-MIDI piano transcription approach\nthat aims to sequentially translate a music signal into a sequence of note\nonset and offset events. The sequence-to-sequence nature of this task may call\nfor the computationally-intensive transformer model for better performance,\nwhich has recently been used for offline transcription benchmarks and could be\nextended for streaming transcription with causal attention mechanisms. We\nassume that the performance limitation of this naive approach lies in the\ndecoder. Although time-frequency features useful for onset detection are\nconsiderably different from those for offset detection, the single decoder is\ntrained to output a mixed sequence of onset and offset events without guarantee\nof the correspondence between the onset and offset events of the same note. To\novercome this limitation, we propose a streaming encoder-decoder model that\nuses a convolutional encoder aggregating local acoustic features, followed by\nan autoregressive Transformer decoder detecting a variable number of onset\nevents and another decoder detecting the offset events for the active pitches\nwith validation of the sustain pedal at each time frame. Experiments using the\nMAESTRO dataset showed that the proposed streaming method performed comparably\nwith or even better than the state-of-the-art offline methods while\nsignificantly reducing the computational cost.",
      "generated_abstract": "aper, we propose a novel streaming piano transcription model that\nemploys consistent onset and offset decoding to enhance transcription accuracy.\nThe proposed method utilizes a single-channel piano stream and a single-channel\npiano transcription decoder, which are trained jointly using piano piano\nstreams and piano transcription labels. The proposed model achieves the best\naccuracy of 97.2% and F1-score of 95.2% on the WSJ0-150K dataset. Furthermore,\nwe propose a novel approach to detect sustain pedal and release time in piano\nstreams using a convolutional neural network. The proposed approach achieves\n92.4% and 93.2% of accuracy and F1-score, respectively, on the WSJ0-150K\ndataset. These results highlight the potential of the proposed approach for\nstreaming",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22137404580152673,
          "p": 0.4461538461538462,
          "f": 0.2959183629138901
        },
        "rouge-2": {
          "r": 0.05945945945945946,
          "p": 0.11,
          "f": 0.07719297790089287
        },
        "rouge-l": {
          "r": 0.19083969465648856,
          "p": 0.38461538461538464,
          "f": 0.25510203638327783
        }
      }
    },
    {
      "paper_id": "nucl-ex.physics/ed-ph/2502.14612v1",
      "true_abstract": "The Modular Neutron Array (MoNA) collaboration was initiated in 2000 at the\nNational Superconducting Cyclotron Laboratory (NSCL) at Michigan State\nUniversity (MSU). Since then, the collaboration studied properties of nuclides\nat and beyond the neutron dripline discovering seven new isotopes between\nlithium to fluorine. The collaboration included liberal arts colleges, regional\ncomprehensive universities, and major research universities with the focus of\ngiving undergraduate students meaningful research experiences. Over the last 25\nyears, the combined efforts of hundreds of undergraduates, dozens of graduate\nstudents and research associates, and faculty from more than a dozen colleges\nand universities produced over fifty publications, won awards for research, and\ncombined research and teaching in new and interesting ways.",
      "generated_abstract": "The MoNA collaboration was established in 2002 as a spin-off from the Nuclear\nnuclear physics group at the Max Planck Institute for Physics and Nuclear Physics\nin Hamburg, Germany. The collaboration was established to study the spin\nstructure of the nucleon by means of exclusive vector meson production and\nobservation. The MoNA collaboration has a rich history of over 25 years, during\nwhich it has published 60 papers in high-impact journals. The collaboration\nhas also had 22 invited talks at international conferences. The MoNA collaboration\nhas been active in the field of spin physics for over 25 years, with a\nsignificant impact on the field of nuclear physics and on the community of\nexperimentalists. This review article gives an overview of the MoNA collaboration\nand its achievements.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18823529411764706,
          "p": 0.21052631578947367,
          "f": 0.1987577589907798
        },
        "rouge-2": {
          "r": 0.035398230088495575,
          "p": 0.03669724770642202,
          "f": 0.03603603103765998
        },
        "rouge-l": {
          "r": 0.16470588235294117,
          "p": 0.18421052631578946,
          "f": 0.17391303849388542
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.09310v1",
      "true_abstract": "The failure of a system can result from the simultaneous effects of multiple\ncauses, where assigning a specific cause may be inappropriate or unavailable.\nExamples include contributing causes of death in epidemiology and the aetiology\nof neurodegenerative diseases like Alzheimer's. We propose a parametric Weibull\naccelerated failure time model for multiple causes, incorporating a\ndata-driven, individualized, and time-varying winning probability (relative\nimportance) matrix. Using maximum likelihood estimation and the\nexpectation-maximization (EM) algorithm, our approach enables simultaneous\nestimation of regression coefficients and relative cause importance, ensuring\nconsistency and asymptotic normality. A simulation study and an application to\nAlzheimer's disease demonstrate its effectiveness in addressing cause-mixture\nproblems and identifying informative biomarker combinations, with comparisons\nto Weibull and Cox proportional hazards models.",
      "generated_abstract": "We consider the problem of estimating the Weibull survival function for a\ncompeting-risk population when the underlying distribution is a Weibull\ndistribution with shape parameter $\\gamma > 0$ and scale parameter $a > 0$. We\ndevelop a general estimator for the mean survival time and the mean of the\nlogarithm of the survival function. We provide conditions for the\nestimator to be consistent and asymptotically normal. We also show that our\nestimator can be interpreted as a generalized Cox regression estimator for the\ncompeting-risk population. We apply our estimator to the estimation of the\nmean survival time in the context of oncology and the mean of the logarithm of\nthe survival function in the context of psychiatry. We illustrate the\napplicability of our estimator through simulation studies and two real-world\ndata examples.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1935483870967742,
          "p": 0.28125,
          "f": 0.2292993582279201
        },
        "rouge-2": {
          "r": 0.017241379310344827,
          "p": 0.019230769230769232,
          "f": 0.01818181319669558
        },
        "rouge-l": {
          "r": 0.1827956989247312,
          "p": 0.265625,
          "f": 0.2165605047247354
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.09981v1",
      "true_abstract": "This study investigates possibility and impossibility results of the\nrepugnant and sadistic conclusions in population ethics and economics. The\nrepugnant conclusion says that an enormous population with very low well-being\nis socially better than any smaller population with sufficiently high\nwell-being. The sadistic conclusion says that adding individuals with negative\nwell-being to a society is socially better than adding individuals with\npositive well-being to it. Previous studies have often found it challenging to\navoid both undesirable conclusions. However, I demonstrate that a class of\nacceptable social welfare orderings can easily prevent these conclusions while\nadhering to standard axioms, such as anonymity, strong Pareto, Pigou-Dalton\ntransfer, and extended continuity. Nevertheless, if the avoidance requirements\nfor the repugnant and sadistic conclusions are strengthened, it is possible to\nencounter new impossibility results. These results reveal essential conflicts\nbetween the independence axiom and the avoidance of the weak repugnant\nconclusion when evaluating well-being profiles with different populations.",
      "generated_abstract": "We argue that standard axioms are essential for a population-economic\nanalysis of the effects of population growth on welfare. This is a consequence\nof the fact that a population-economic analysis cannot be performed without\nstandard axioms. We show that the standard axioms can be derived from\nclassical axioms in a way that preserves the conclusion of the standard\naxioms. The result is that, if the standard axioms are used in a population\neconomic analysis, then the conclusion is that the population should be\ndecreasing in size.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1717171717171717,
          "p": 0.3695652173913043,
          "f": 0.23448275428870397
        },
        "rouge-2": {
          "r": 0.022222222222222223,
          "p": 0.04225352112676056,
          "f": 0.029126209074842816
        },
        "rouge-l": {
          "r": 0.16161616161616163,
          "p": 0.34782608695652173,
          "f": 0.22068965084042813
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.06587v1",
      "true_abstract": "This article presents a comprehensive methodology for processing financial\ndatasets of Apple Inc., encompassing quarterly income and daily stock prices,\nspanning from March 31, 2009, to December 31, 2023. Leveraging 60 observations\nfor quarterly income and 3774 observations for daily stock prices, sourced from\nMacrotrends and Yahoo Finance respectively, the study outlines five distinct\ndatasets crafted through varied preprocessing techniques. Through detailed\nexplanations of aggregation, interpolation (linear, polynomial, and cubic\nspline) and lagged variables methods, the study elucidates the steps taken to\ntransform raw data into analytically rich datasets. Subsequently, the article\ndelves into regression analysis, aiming to decipher which of the five data\nprocessing methods best suits capital market analysis, by employing both linear\nand polynomial regression models on each preprocessed dataset and evaluating\ntheir performance using a range of metrics, including cross-validation score,\nMSE, MAE, RMSE, R-squared, and Adjusted R-squared. The research findings reveal\nthat linear interpolation with polynomial regression emerges as the\ntop-performing method, boasting the lowest validation MSE and MAE values,\nalongside the highest R-squared and Adjusted R-squared values.",
      "generated_abstract": "y compares and contrasts three preprocessing techniques,\neach of which has been proposed to improve the accuracy of regression\nmodeling of financial data. The first technique, the Pearson correlation,\nconsiders only the absolute value of the Pearson correlation coefficient, which\nis a well-known measure of linear association between variables. The second\ntechnique, the Spearman rank correlation, considers both absolute values of\nthe Pearson and Spearman correlation coefficients, which may be useful when\nthere is a need to differentiate between linear and nonlinear associations.\nThe third technique, the Kendall's tau correlation, considers both absolute\nvalues of the Pearson and Spearman correlation coefficients, which may be\nuseful when there is a need to differentiate between linear and nonlinear\nassociations. Each technique has its own advantages and disadvantages, and the\ntechnique used will depend on the specific data analysis",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.136,
          "p": 0.24285714285714285,
          "f": 0.1743589697567391
        },
        "rouge-2": {
          "r": 0.012195121951219513,
          "p": 0.019801980198019802,
          "f": 0.015094334905234654
        },
        "rouge-l": {
          "r": 0.072,
          "p": 0.12857142857142856,
          "f": 0.09230768770545716
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.hep-lat/2503.08414v1",
      "true_abstract": "This work examines the effect of disclinations on the scattering of\nquasipaticles in graphene with the presence of a topological defect. Using the\ntight-binding method, the electronic properties of graphene with disclination\nare described, where the topological defects are introduced in the lattice via\ngeometric theory. The massless Dirac equation is modified to account for the\ncurvature induced by these defects, incorporating a gauge field. The results\nshow that disclinations significantly affect the scattering process, altering\nphase shifts and interference patterns. The differential cross-section and its\ndependence on the scattering angle are analyzed, highlighting the role of\ngeometric factors like the parameter {\\alpha} in shaping the scattering\ndynamics.",
      "generated_abstract": "igate the Aharonov-Bohm (AB) effect in a graphene system with\ndisclinations. The AB interaction between the charge and spin degrees of\nfreedom of a massless fermion is mediated by the non-Abelian gauge fields of\nthe lattice, which can be treated as a disclination system. We derive the\neffective action for the gauge fields, which contains both the Abelian and\nnon-Abelian terms. We use this action to study the scattering of massless\nfermions in the AB interaction, focusing on the effects of the disclination\nbackground. We find that the non-Abelian gauge fields contribute to the\nscattering amplitude in a non-trivial way, both in the presence and in the\nabsence of disclination, depending on the relative orientation between the\ndisclination and the non-Abelian gauge fields. In particular, we find that in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.28,
          "p": 0.2916666666666667,
          "f": 0.28571428071636823
        },
        "rouge-2": {
          "r": 0.06862745098039216,
          "p": 0.061946902654867256,
          "f": 0.06511627408285597
        },
        "rouge-l": {
          "r": 0.24,
          "p": 0.25,
          "f": 0.24489795418575602
        }
      }
    },
    {
      "paper_id": "cs.AI.q-fin/GN/2407.20377v1",
      "true_abstract": "This paper explores an innovative approach to Environmental, Social, and\nGovernance (ESG) scoring by integrating Natural Language Processing (NLP)\ntechniques with Item Response Theory (IRT), specifically the Rasch model. The\nstudy utilizes a comprehensive dataset of news articles in Portuguese related\nto Petrobras, a major oil company in Brazil, collected from 2022 and 2023. The\ndata is filtered and classified for ESG-related sentiments using advanced NLP\nmethods. The Rasch model is then applied to evaluate the psychometric\nproperties of these ESG measures, providing a nuanced assessment of ESG\nsentiment trends over time. The results demonstrate the efficacy of this\nmethodology in offering a more precise and reliable measurement of ESG factors,\nhighlighting significant periods and trends. This approach may enhance the\nrobustness of ESG metrics and contribute to the broader field of sustainability\nand finance by offering a deeper understanding of the temporal dynamics in ESG\nreporting.",
      "generated_abstract": "Environmental, social, and governance (ESG) factors are increasingly\nimportant in investment decisions. This paper proposes a method to leverage\nnatural language processing (NLP) models for ESG scoring. We develop a method\nto automatically extract the ESG scores of the company and its suppliers,\nleveraging item response theory (IRT) models. The method employs a neural\nnetwork to score the ESG scores of companies and their suppliers. The model is\ntrained on an ESG dataset with 2278 companies and 1810 suppliers. The model\nperforms well across multiple evaluation metrics, achieving an F1-score of 0.92\non the test dataset. The proposed method provides a scalable, efficient, and\nrobust approach for ESG scoring.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21153846153846154,
          "p": 0.2894736842105263,
          "f": 0.24444443956543216
        },
        "rouge-2": {
          "r": 0.02112676056338028,
          "p": 0.030303030303030304,
          "f": 0.02489626071934117
        },
        "rouge-l": {
          "r": 0.20192307692307693,
          "p": 0.27631578947368424,
          "f": 0.23333332845432112
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/MF/2411.18397v1",
      "true_abstract": "We study optimal payoff choice for an expected utility maximizer under the\nconstraint that their payoff is not allowed to deviate ``too much'' from a\ngiven benchmark. We solve this problem when the deviation is assessed via a\nBregman-Wasserstein (BW) divergence, generated by a convex function $\\phi$.\nUnlike the Wasserstein distance (i.e., when $\\phi(x)=x^2$). The inherent\nasymmetry of the BW divergence makes it possible to penalize positive\ndeviations different than negative ones. As a main contribution, we provide the\noptimal payoff in this setting. Numerical examples illustrate that the choice\nof $\\phi$ allow to better align the payoff choice with the objectives of\ninvestors.",
      "generated_abstract": "We consider a class of optimal payoff problems with a Bregman-Wasserstein\ndivergence constraint. We establish a dual representation of the optimal payoff\nwith respect to a convex set of admissible policies. Furthermore, we prove that\nthe optimal payoff is a maximum over the convex set of admissible policies. We\nalso establish a dual representation of the optimal payoff with respect to the\nadmissible policies themselves. Finally, we present a simple and\ncomputationally efficient method to determine the optimal payoff. We illustrate\nthe method in the context of a simple two-person game. We provide an example\nwhere the optimal payoff is the expectation of a random variable.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2375,
          "p": 0.37254901960784315,
          "f": 0.290076331122895
        },
        "rouge-2": {
          "r": 0.06796116504854369,
          "p": 0.0875,
          "f": 0.07650272731941864
        },
        "rouge-l": {
          "r": 0.2125,
          "p": 0.3333333333333333,
          "f": 0.25954197997785683
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2409.13236v1",
      "true_abstract": "Collective decision-making is the process through which diverse stakeholders\nreach a joint decision. Within societal settings, one example is participatory\nbudgeting, where constituents decide on the funding of public projects. How to\nmost efficiently aggregate diverse stakeholder inputs on a portfolio of\nprojects with uncertain long-term benefits remains an open question. We address\nthis problem by studying collective decision-making through the integration of\npreference aggregation and knapsack allocation methods. Since different\nstakeholder groups may evaluate projects differently,we examine several\naggregation methods that combine their diverse inputs. The aggregated\nevaluations are then used to fill a ``collective'' knapsack. Among the methods\nwe consider are the arithmetic mean, Borda-type rankings, and delegation to\nexperts. We find that the factors improving an aggregation method's ability to\nidentify projects with the greatest expected long-term value include having\nmany stakeholder groups, moderate variation in their expertise levels, and some\ndegree of delegation or bias favoring groups better positioned to objectively\nassess the projects. We also discuss how evaluation errors and heterogeneous\ncosts impact project selection. Our proposed aggregation methods are relevant\nnot only in the context of funding public projects but also, more generally,\nfor organizational decision-making under uncertainty.",
      "generated_abstract": "We propose a dynamic decision-making framework that combines collective\ndecision-making and a knapsack constraint to address problems in which\nresources are limited but decision-makers are heterogeneous. We analyze the\nproblem of maximizing the aggregate satisfaction of agents by maximizing the\nutility of their decisions. We show that the problem is NP-hard. We then\nintroduce an approximation algorithm that approximates the optimal solution by\na constant factor, and we provide an upper bound on the approximation ratio.\nFinally, we discuss the practical use of the proposed framework in resource\nallocation and resource allocation problems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20437956204379562,
          "p": 0.45901639344262296,
          "f": 0.28282827856494236
        },
        "rouge-2": {
          "r": 0.015789473684210527,
          "p": 0.033707865168539325,
          "f": 0.02150537199933285
        },
        "rouge-l": {
          "r": 0.1678832116788321,
          "p": 0.3770491803278688,
          "f": 0.2323232280598919
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.10352v1",
      "true_abstract": "Popular safe Bayesian optimization (BO) algorithms learn control policies for\nsafety-critical systems in unknown environments. However, most algorithms make\na smoothness assumption, which is encoded by a known bounded norm in a\nreproducing kernel Hilbert space (RKHS). The RKHS is a potentially\ninfinite-dimensional space, and it remains unclear how to reliably obtain the\nRKHS norm of an unknown function. In this work, we propose a safe BO algorithm\ncapable of estimating the RKHS norm from data. We provide statistical\nguarantees on the RKHS norm estimation, integrate the estimated RKHS norm into\nexisting confidence intervals and show that we retain theoretical guarantees,\nand prove safety of the resulting safe BO algorithm. We apply our algorithm to\nsafely optimize reinforcement learning policies on physics simulators and on a\nreal inverted pendulum, demonstrating improved performance, safety, and\nscalability compared to the state-of-the-art.",
      "generated_abstract": "on in reinforcement learning (RL) is crucial for finding\nexplanations for model behavior, and exploration in high-dimensional\nreproducing kernel Hilbert spaces (HDKHS) is particularly challenging due to\nthe computational complexity of optimal transport. Recently, we introduced a\nnovel safe exploration method that reduces the computational cost of optimal\ntransport by exploring the low-dimensional space of transport maps. In this\npaper, we present a rigorous analysis of the safety properties of the safe\nexploration method. We prove that, under certain assumptions, the safe\nexploration method is asymptotically safe in the limit of infinite exploration\ntime. We also prove that, under mild assumptions, the safe exploration method\nachieves a constant exploration rate in finite time. These results highlight\nthe safety of our method, which may be useful in scenarios where exploration\nis crucial, but the computational cost of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26262626262626265,
          "p": 0.3170731707317073,
          "f": 0.2872928127236654
        },
        "rouge-2": {
          "r": 0.05343511450381679,
          "p": 0.0603448275862069,
          "f": 0.05668015696176018
        },
        "rouge-l": {
          "r": 0.25252525252525254,
          "p": 0.3048780487804878,
          "f": 0.2762430889667593
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.19574v1",
      "true_abstract": "Large language models (LLMs) have shown promise in various natural language\nprocessing tasks, including their application to proteomics data to classify\nprotein fragments. In this study, we curated a limited mass spectrometry\ndataset with 1000s of protein fragments, consisting of proteins that appear to\nbe attached to the endoplasmic reticulum in cardiac cells, of which a fraction\nwas cloned and characterized for their impact on SERCA, an ER calcium pump.\nWith this limited dataset, we sought to determine whether LLMs could correctly\npredict whether a new protein fragment could bind SERCA, based only on its\nsequence and a few biophysical characteristics, such as hydrophobicity,\ndetermined from that sequence. To do so, we generated random sequences based on\ncloned fragments, embedded the fragments into a retrieval augmented generation\n(RAG) database to group them by similarity, then fine-tuned large language\nmodel (LLM) prompts to predict whether a novel sequence could bind SERCA. We\nbenchmarked this approach using multiple open-source LLMs, namely the\nMeta/llama series, and embedding functions commonly available on the\nHuggingface repository. We then assessed the generalizability of this approach\nin classifying novel protein fragments from mass spectrometry that were not\ninitially cloned for functional characterization. By further tuning the prompt\nto account for motifs, such as ER retention sequences, we improved the\nclassification accuracy by and identified several proteins predicted to\nlocalize to the endoplasmic reticulum and bind SERCA, including Ribosomal\nProtein L2 and selenoprotein S. Although our results were based on proteomics\ndata from cardiac cells, our approach demonstrates the potential of LLMs in\nidentifying novel protein interactions and functions with very limited\nproteomic data.",
      "generated_abstract": "a large protein family that plays a crucial role in regulating\ncrystalline calcium transport, essential for heartbeat stability. However, the\nstructure of SERCA-binding protein (SBP) fragments, which are known to be\nimportant in cardiac diseases, are poorly understood. To address this gap, we\nconstruct a protein-protein interaction (PPI) database of cardiac SBPs using\nthe Cardiac Protein Database (CPD) and the Human Protein Atlas (HPA), and\nidentify 247 SBP fragments from 140 proteins. We then perform large language\nmodel (LLM) predictions to predict the PPI interactions of these fragments,\nwhich are used to generate their PPI structures. We then apply retrieval-augmented\ngeneration (RAG) to predict SBP fragments based on the generated PPIs,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22839506172839505,
          "p": 0.44047619047619047,
          "f": 0.3008130036327583
        },
        "rouge-2": {
          "r": 0.044,
          "p": 0.10476190476190476,
          "f": 0.061971826820075654
        },
        "rouge-l": {
          "r": 0.21604938271604937,
          "p": 0.4166666666666667,
          "f": 0.28455284103113226
        }
      }
    },
    {
      "paper_id": "cs.GR.cs/GR/2503.08370v1",
      "true_abstract": "This paper presents Ev-Layout, a novel large-scale event-based multi-modal\ndataset designed for indoor layout estimation and tracking. Ev-Layout makes key\ncontributions to the community by: Utilizing a hybrid data collection platform\n(with a head-mounted display and VR interface) that integrates both RGB and\nbio-inspired event cameras to capture indoor layouts in motion. Incorporating\ntime-series data from inertial measurement units (IMUs) and ambient lighting\nconditions recorded during data collection to highlight the potential impact of\nmotion speed and lighting on layout estimation accuracy. The dataset consists\nof 2.5K sequences, including over 771.3K RGB images and 10 billion event data\npoints. Of these, 39K images are annotated with indoor layouts, enabling\nresearch in both event-based and video-based indoor layout estimation. Based on\nthe dataset, we propose an event-based layout estimation pipeline with a novel\nevent-temporal distribution feature module to effectively aggregate the\nspatio-temporal information from events. Additionally, we introduce a\nspatio-temporal feature fusion module that can be easily integrated into a\ntransformer module for fusion purposes. Finally, we conduct benchmarking and\nextensive experiments on the Ev-Layout dataset, demonstrating that our approach\nsignificantly improves the accuracy of dynamic indoor layout estimation\ncompared to existing event-based methods.",
      "generated_abstract": "yout estimation plays a crucial role in indoor navigation.\nHowever, existing datasets for indoor layout estimation are limited to\nsingle-modal data, which are insufficient for evaluating the performance of\nmulti-modal architectures. Furthermore, existing datasets for indoor\nmulti-modal layout estimation are also limited to single-scale scenarios,\nwhich do not provide enough data for evaluating the performance of\nmulti-scale architectures. To address these limitations, we propose Ev-Layout,\na large-scale event-based indoor multi-modal dataset with multi-scale\ngeometries. Ev-Layout is constructed by collecting over 320k real-world event\nlog data from 320k indoor locations in 100k buildings. Each event log data\ncontains 1000 geo-fenced events and 320k geo-aligned 3D points. Our dataset",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2125984251968504,
          "p": 0.3698630136986301,
          "f": 0.26999999536450003
        },
        "rouge-2": {
          "r": 0.043243243243243246,
          "p": 0.08602150537634409,
          "f": 0.05755395238212343
        },
        "rouge-l": {
          "r": 0.1968503937007874,
          "p": 0.3424657534246575,
          "f": 0.24999999536450007
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.08557v1",
      "true_abstract": "This paper studies Integrated Sensing, Communication, and Powering (ISCAP) as\na novel framework designed to enhance Internet of Things (IoT) applications\nwithin sixth-generation wireless networks. In these applications, in addition\nto IoT devices requiring an energy supply and receiving information or control\ndata to perform their tasks, the base station serving them must sense the\ndevices and their environment to localize them, thereby improving data\ntransmission and enabling simultaneous power delivery. In our multi-node ISCAP\nIoT system, we optimize base station beamforming alongside the receiver's\npower-splitting factor to maximize energy harvesting while adhering to strict\ncommunication and sensing constraints. To effectively tackle this non-convex\noptimization problem, we decompose it into three manageable subproblems and\nemploy several techniques such as semidefinite relaxation and Rayleigh quotient\nmethods to find an efficient solution. Simulation results demonstrate the\neffectiveness of the proposed design, highlighting performance trade-offs among\nsensing accuracy, communication reliability, and power transfer efficiency.",
      "generated_abstract": "net of Things (IoT) is transforming our world, with the potential\nto enhance both human and environmental well-being. One key enabler of\nIoT-enabled applications is the ability to sense, communicate, and power\neffectively, enabling precise, real-time decision-making and reliability.\nHowever, traditional wireless technologies, such as Wi-Fi and LTE, have\nlimited capabilities, with latency and energy limitations. To address these\nchallenges, this paper proposes an integrated sensing, communication, and\npowering (ISCAP) framework for IoT devices. The proposed framework leverages\nmultiple technologies to address the limitations of existing technologies. It\nintegrates a multi-cellular beamforming strategy for sensing and communication,\na multi-tier hybrid powering scheme for energy, and a hybrid powering and\ncommunication (HPC)",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24369747899159663,
          "p": 0.3493975903614458,
          "f": 0.28712870803009516
        },
        "rouge-2": {
          "r": 0.040268456375838924,
          "p": 0.05504587155963303,
          "f": 0.04651162302716234
        },
        "rouge-l": {
          "r": 0.19327731092436976,
          "p": 0.27710843373493976,
          "f": 0.2277227674360358
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.06079v1",
      "true_abstract": "Despite the significance of probabilistic time-series forecasting models,\ntheir evaluation metrics often involve intractable integrations. The most\nwidely used metric, the continuous ranked probability score (CRPS), is a\nstrictly proper scoring function; however, its computation requires\napproximation. We found that popular CRPS estimators--specifically, the\nquantile-based estimator implemented in the widely used GluonTS library and the\nprobability-weighted moment approximation--both exhibit inherent estimation\nbiases. These biases lead to crude approximations, resulting in improper\nrankings of forecasting model performance when CRPS values are close. To\naddress this issue, we introduced a kernel quadrature approach that leverages\nan unbiased CRPS estimator and employs cubature construction for scalable\ncomputation. Empirically, our approach consistently outperforms the two widely\nused CRPS estimators.",
      "generated_abstract": "ng is a fundamental task in statistics and machine learning. It\npredicts future values of a dependent variable given past observations.\nHowever, the evaluation of forecasting models is often challenging due to the\ncomplexity of the problems involved, such as the uncertainty in the predictions\nand the dependence between the past observations and future values. Traditional\nmethods of evaluating forecasting models, such as time series cross-validation\n(TSCV), rely on the assumption that the model can be trained on a subset of\nhistorical observations and evaluated on the remaining ones. However, the\nassumption is not always valid, particularly when the number of observations is\nlarge. In this paper, we present a new evaluation method for probabilistic\nforecasting models, based on kernel quadrature, which can be applied to\nforecasting problems with a large number of observations. The method is\nefficient and easy to implement, and it provides an alternative evaluation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23655913978494625,
          "p": 0.24719101123595505,
          "f": 0.24175823676065702
        },
        "rouge-2": {
          "r": 0.044642857142857144,
          "p": 0.03676470588235294,
          "f": 0.040322575691988126
        },
        "rouge-l": {
          "r": 0.21505376344086022,
          "p": 0.2247191011235955,
          "f": 0.21978021478263507
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.cond-mat/stat-mech/2503.10599v1",
      "true_abstract": "Recently, a thermodynamic bound on correlation times was formulated in [A.\nDechant, J. Garnier-Brun, S.-i. Sasa, Phys. Rev. Lett. 131, 167101 (2023)],\nshowing how the decay of correlations in Langevin dynamics is bounded by\nshort-time fluctuations and dissipation. Whereas these original results only\naddress very long observation times in steady-state dynamics, we here\ngeneralize the respective inequalities to finite observations and general\ninitial conditions. We utilize the connection between correlations and the\nfluctuations of time-integrated density functionals and generalize the direct\nstochastic calculus approach from [C. Dieball and A. Godec, Phys. Rev. Lett.\n130, 087101 (2023)] which paves the way for further generalizations. We address\nthe connection between short and long time scales, as well as the saturation of\nthe bounds via complementary spectral-theoretic arguments. Motivated by the\nspectral insight, we formulate all results also for complex-valued observables.",
      "generated_abstract": "We provide a new approach to the analysis of the correlation length of\nprocesses that exhibit a transient behavior. We demonstrate that for a class of\nGaussian processes, the correlation length diverges at a finite time. This\nresult, which was previously only obtained for a subset of Gaussian processes,\ncan be extended to a broader class of processes. We show that the divergence\noccurs at a finite time for a wide range of Gaussian processes, including\nexponential processes, the classical Markov chain, and a class of random walks\nwith drift and diffusion. This result is supported by numerical simulations and\nby the analysis of the master equation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.26229508196721313,
          "f": 0.19393938927897167
        },
        "rouge-2": {
          "r": 0.015037593984962405,
          "p": 0.022988505747126436,
          "f": 0.018181813400414477
        },
        "rouge-l": {
          "r": 0.14423076923076922,
          "p": 0.2459016393442623,
          "f": 0.18181817715775953
        }
      }
    },
    {
      "paper_id": "math.AP.math/FA/2503.07569v1",
      "true_abstract": "In this paper, we consider second order degenerate parabolic equations with\ncomplex, measurable, and time-dependent coefficients. The degenerate\nellipticity is dictated by a spatial $A_2$-weight. We prove that having a\ngeneralized fundamental solution with upper Gaussian bounds is equivalent to\nMoser's $L^2$-$L^\\infty$ estimates for local weak solutions. In the special\ncase of real coefficients, Moser's $L^2$-$L^\\infty$ estimates are known, which\nprovide an easier proof of Gaussian upper bounds, and a known Harnack\ninequality is then used to derive Gaussian lower bounds.",
      "generated_abstract": "We study the nonlinear parabolic equations\n$\\partial_t u + \\Delta u = f$ with $f$ divergence-free in a ball $B(R)\\subset\n\\mathbb{R}^n$, $n\\geq 2$, where $u$ is the unique solution to the equation\n$-\\Delta u + \\nabla \\cdot (u\\nabla u) = g$ in $\\mathbb{R}^n$ for some $g\\in\nL^2(B(R))$. We prove that the solution $u$ satisfies the Gaussian bounds\n$\\Vert u\\Vert_{L^2(B(R))}\\leq C\\Vert f\\Vert_{L^2(B(R))}$ for some constant $C$\nindependent of $R$, and the solution $u$ is a fundamental solution of the\nequation $-\\Delta u + \\nabla \\cdot (u\\nabla u) = g$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.265625,
          "p": 0.30357142857142855,
          "f": 0.2833333283555556
        },
        "rouge-2": {
          "r": 0.0641025641025641,
          "p": 0.06944444444444445,
          "f": 0.06666666167466705
        },
        "rouge-l": {
          "r": 0.234375,
          "p": 0.26785714285714285,
          "f": 0.2499999950222223
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.01509v1",
      "true_abstract": "A key step in the Bayesian workflow for model building is the graphical\nassessment of model predictions, whether these are drawn from the prior or\nposterior predictive distribution. The goal of these assessments is to identify\nwhether the model is a reasonable (and ideally accurate) representation of the\ndomain knowledge and/or observed data. There are many commonly used visual\npredictive checks which can be misleading if their implicit assumptions do not\nmatch the reality. Thus, there is a need for more guidance for selecting,\ninterpreting, and diagnosing appropriate visualizations. As a visual predictive\ncheck itself can be viewed as a model fit to data, assessing when this model\nfails to represent the data is important for drawing well-informed conclusions.\n  We present recommendations and diagnostic tools to mitigate ad-hoc\ndecision-making in visual predictive checks. These contributions aim to improve\nthe robustness and interpretability of Bayesian model criticism practices. We\noffer recommendations for appropriate visual predictive checks for observations\nthat are: continuous, discrete, or a mixture of the two. We also discuss\ndiagnostics to aid in the selection of visual methods. Specifically, in the\ndetection of an incorrect assumption of continuously-distributed data:\nidentifying when data is likely to be discrete or contain discrete components,\ndetecting and estimating possible bounds in data, and a diagnostic of the\ngoodness-of-fit to data for density plots made through kernel density\nestimates.",
      "generated_abstract": "The visual prediction check (VPC) is a tool used in Bayesian workflows to\nvisualize the posterior distribution, allowing users to interpret and compare\ndifferent models. This paper proposes recommendations for visualizing the VPC\nin workflows. These recommendations are based on three principles: the VPC\nshould be interpretable, comparable across different models, and allow the\nuser to interpret and compare the posterior distribution. We highlight some\nof the challenges in visualizing the VPC and provide guidelines for implementing\nthem in workflows. We demonstrate the usefulness of the VPC by applying it to\na toxicology modeling example.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14388489208633093,
          "p": 0.31746031746031744,
          "f": 0.19801979768797184
        },
        "rouge-2": {
          "r": 0.014150943396226415,
          "p": 0.03571428571428571,
          "f": 0.02027026620526013
        },
        "rouge-l": {
          "r": 0.1366906474820144,
          "p": 0.30158730158730157,
          "f": 0.188118807588962
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/AP/2502.16118v1",
      "true_abstract": "We study the relationship between the rank of the prior covariance matrix and\nthe local false sign rate in a multivariate empirical Bayes normal mean model.\nIt has been observed that the false sign rate is inflated when the prior\nassigns weight to low-rank covariance matrices. We show that this issue arises\ndue to the rank deficiency of prior covariance matrices and propose an\nadjustment to mitigate it.",
      "generated_abstract": "The empirical Bayes multiple testing procedure, which is widely used in\nthe biomedical sciences, can be expressed as a Bayesian model. However,\nexisting Bayesian procedures often fail to account for prior covariance\nstructures, which are important for accurate inference. In this paper, we\nintroduce a novel Bayesian multiple testing procedure that addresses this\nissue. We develop a novel likelihood function for this procedure and\ninvestigate its properties. We then derive the posterior distribution of\nthe covariance matrix of the prior and its posterior distribution using the\nHadamard product of the posterior distribution of the covariance matrix of the\nprior with the prior distribution. Finally, we derive a simple formula for the\nfalse signal rate of the proposed procedure. We also discuss the\ncomputational complexity of the proposed procedure and compare it with the\nexisting methods in the literature.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.425531914893617,
          "p": 0.25316455696202533,
          "f": 0.31746031278281694
        },
        "rouge-2": {
          "r": 0.11475409836065574,
          "p": 0.06086956521739131,
          "f": 0.0795454500161418
        },
        "rouge-l": {
          "r": 0.425531914893617,
          "p": 0.25316455696202533,
          "f": 0.31746031278281694
        }
      }
    },
    {
      "paper_id": "cs.LG.econ/EM/2503.05800v1",
      "true_abstract": "Understanding consumer choice is fundamental to marketing and management\nresearch, as firms increasingly seek to personalize offerings and optimize\ncustomer engagement. Traditional choice modeling frameworks, such as\nmultinomial logit (MNL) and mixed logit models, impose rigid parametric\nassumptions that limit their ability to capture the complexity of consumer\ndecision-making. This study introduces the Mixture of Experts (MoE) framework\nas a machine learning-driven alternative that dynamically segments consumers\nbased on latent behavioral patterns. By leveraging probabilistic gating\nfunctions and specialized expert networks, MoE provides a flexible,\nnonparametric approach to modeling heterogeneous preferences.\n  Empirical validation using large-scale retail data demonstrates that MoE\nsignificantly enhances predictive accuracy over traditional econometric models,\ncapturing nonlinear consumer responses to price variations, brand preferences,\nand product attributes. The findings underscore MoEs potential to improve\ndemand forecasting, optimize targeted marketing strategies, and refine\nsegmentation practices. By offering a more granular and adaptive framework,\nthis study bridges the gap between data-driven machine learning approaches and\nmarketing theory, advocating for the integration of AI techniques in managerial\ndecision-making and strategic consumer insights.",
      "generated_abstract": "We propose a novel framework for understanding consumer choice under\nthe Mixture of Experts (MoE) model, which has been widely used in market\nresearch and machine learning applications. We introduce a novel framework for\nidentifying hidden preferences and uncovering latent latent preferences\nunderlying observed choices. This framework enables us to examine how the\nobserved choice patterns arise from the latent latent preferences. We develop\nan algorithm to find latent latent preferences and a method to identify\nhidden preferences. We apply our framework to two real-world datasets to\ndemonstrate its effectiveness.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.31666666666666665,
          "f": 0.19689118742516584
        },
        "rouge-2": {
          "r": 0.03508771929824561,
          "p": 0.0759493670886076,
          "f": 0.04799999567712039
        },
        "rouge-l": {
          "r": 0.13533834586466165,
          "p": 0.3,
          "f": 0.18652849312464773
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.01708v1",
      "true_abstract": "We develop a pseudo-likelihood theory for rank one matrix estimation problems\nin the high dimensional limit. We prove a variational principle for the\nlimiting pseudo-maximum likelihood which also characterizes the performance of\nthe corresponding pseudo-maximum likelihood estimator. We show that this\nvariational principle is universal and depends only on four parameters\ndetermined by the corresponding null model. Through this universality, we\nintroduce a notion of equivalence for estimation problems of this type and, in\nparticular, show that a broad class of estimation tasks, including community\ndetection, sparse submatrix detection, and non-linear spiked matrix models, are\nequivalent to spiked matrix models. As an application, we obtain a complete\ndescription of the performance of the least-squares (or ``best rank one'')\nestimator for any rank one matrix estimation problem.",
      "generated_abstract": "ical maximum likelihood estimation (MLE) method is well-suited for\nhigh-dimensional estimation when the number of samples is large. However, when\nthe number of samples is small, MLE may fail to find the true rank-one\nsubmatrix, leading to biased estimation. This problem can be mitigated by\nemploying pseudo-likelihood methods, which rely on the pseudo-likelihood\nestimator. In this paper, we investigate the pseudo-likelihood estimator when\nthe rank-one submatrix is randomly selected from a low-rank distribution, and\npropose a pseudo-likelihood estimator that relies on the pseudo-likelihood\nestimator and a pseudo-likelihood estimator. In addition, we establish the\nasymptotic normality of the proposed pseudo-likelihood estimator and its\nconsistent estimator of the true rank-one submatrix. Numerical studies are",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.29850746268656714,
          "f": 0.2721088385765191
        },
        "rouge-2": {
          "r": 0.017857142857142856,
          "p": 0.022222222222222223,
          "f": 0.019801975257328946
        },
        "rouge-l": {
          "r": 0.225,
          "p": 0.26865671641791045,
          "f": 0.24489795422277766
        }
      }
    },
    {
      "paper_id": "physics.comp-ph.physics/flu-dyn/2503.10478v1",
      "true_abstract": "A multiscale stochastic-deterministic coupling method is proposed to\ninvestigate the complex interactions between turbulent and rarefied gas flows\nwithin a unified framework. This method intermittently integrates the general\nsynthetic iterative scheme with the shear stress transport turbulence model\ninto the direct simulation Monte Carlo (DSMC) approach, enabling the simulation\nof gas flows across the free-molecular, transition, slip, and turbulent\nregimes. First, the macroscopic synthetic equations, derived directly from\nDSMC, are coupled with the turbulence model to establish a constitutive\nrelation that incorporates not only turbulent and laminar transport\ncoefficients but also higher-order terms accounting for rarefaction effects.\nSecond, the macroscopic properties, statistically sampled over specific time\nintervals in DSMC, along with the turbulent properties provided by the\nturbulence model, serve as initial conditions for solving the macroscopic\nsynthetic equations. Finally, the simulation particles in DSMC are updated\nbased on the macroscopic properties obtained from the synthetic equations.\nNumerical simulations demonstrate that the proposed method asymptotically\nconverges to either the turbulence model or DSMC results, adaptively adjusting\nto different flow regimes. Then, this coupling method is applied to simulate an\nopposing jet surrounded by hypersonic rarefied gas flows, revealing significant\nvariations in surface properties due to the interplay of turbulent and rarefied\neffects. This study presents an efficient methodology for simulating the\ncomplex interplay between rarefied and turbulent flows, establishing a\nfoundational framework for investigating the coupled effects of turbulence,\nhypersonic conditions, and chemical reactions in rarefied gas dynamics in the\nfuture.",
      "generated_abstract": "sive-Stochastic-Mass-Coupling (DSMC) framework is a stochastic\nsimulation method that combines the Diffusive Stochastic Method (DSM) with a\nnon-equilibrium mass-coupling scheme. In this framework, the mean-field\nequations of the DSM are solved by the stochastic numerical integration of\nequations of the mass-coupling scheme. In this work, we introduce the\nmultiscale simulation of interacting turbulent and rarefied gas flows in the\nDSMC framework. We employ the adaptive mesh refinement (AMR) strategy to\nsimulate the turbulent flow and the coarse mesh refinement (CMR) strategy to\nsimulate the rarefied gas flow. We focus on the simulation of the turbulent\nflow with the coarse mesh refinement. We numerically demonstrate the\nper",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19444444444444445,
          "p": 0.4666666666666667,
          "f": 0.27450979976931955
        },
        "rouge-2": {
          "r": 0.05504587155963303,
          "p": 0.13636363636363635,
          "f": 0.07843136845145052
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.4,
          "f": 0.23529411349480975
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2502.19862v1",
      "true_abstract": "Decentralized lending protocols within the decentralized finance ecosystem\nenable the lending and borrowing of crypto-assets without relying on\ntraditional intermediaries. Interest rates in these protocols are set\nalgorithmically and fluctuate according to the supply and demand for liquidity.\nIn this study, we propose an agent-based model tailored to a decentralized\nlending protocol and determine the optimal interest rate model. When the\nresponses of the agents are linear with respect to the interest rate, the\noptimal solution is derived from a system of Riccati-type ODEs. For nonlinear\nbehaviors, we propose a Monte-Carlo estimator, coupled with deep learning\ntechniques, to approximate the optimal solution. Finally, after calibrating the\nmodel using block-by-block data, we conduct a risk-adjusted profit and loss\nanalysis of the liquidity pool under industry-standard interest rate models and\nbenchmark them against the optimal interest rate model.",
      "generated_abstract": "In this paper, we study a new problem of optimal interest rate management for\ndecentralized lending protocols (DLPs). We propose a novel model that combines\na general risk management problem with the interest rate management problem of\na DLP. We show that this model is equivalent to the standard risk management\nproblem in the centralized lending scenario, where the lender has a finite\nportfolio with a risk-neutral probability measure. We also provide a\nfeasibility analysis of the new problem. The solution to the problem provides\nthe optimal risk-neutral interest rate for the DLP, as well as the optimal\nrisk-neutral portfolio for the lender.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23595505617977527,
          "p": 0.38181818181818183,
          "f": 0.29166666194540897
        },
        "rouge-2": {
          "r": 0.08870967741935484,
          "p": 0.12359550561797752,
          "f": 0.10328638011153013
        },
        "rouge-l": {
          "r": 0.19101123595505617,
          "p": 0.3090909090909091,
          "f": 0.23611110638985344
        }
      }
    },
    {
      "paper_id": "hep-ex.hep-ex/2503.09773v1",
      "true_abstract": "Current template-based gravitational-wave searches for compact binary mergers\nneglect the general relativistic phenomenon of spin-induced orbital precession.\nOwing to their asymmetric masses, gravitational-waves from neutron star-black\nhole (NSBH) binaries are prime candidates for displaying strong imprints of\nspin-precession. As a result, current searches may be missing a significant\nfraction of the astrophysical population, and the detected NSBH population may\nbe significantly suppressed or biased. Here we report the most sensitive search\nfor NSBH binaries to date by including spin-precession for the first time. We\nanalyze data from the entirety of the third LIGO-Virgo-KAGRA gravitational-wave\nobserving run and show that when accounting for spin-precession, our search is\nup to 100% more sensitive than the search techniques currently adopted by the\nLIGO-Virgo-KAGRA collaboration (for systems with strong precessional effects).\nThis allows us to more tightly constrain the rate of NSBH mergers in the local\nUniverse. Firstly, we focus on a precessing subpopulation of NSBH mergers; the\nlack of observed candidates allows us to place an upper limit on the merger\nrate of $R_{90} = 79\\, \\mathrm{Gpc}^{-3}\\mathrm{yr}^{-1}$ with 90% confidence.\nSecondly, we tighten the overall rate of NSBH mergers; we show that if there is\nno preferred direction of component spin, the rate of NSBH mergers is on\naverage 16% smaller than previously believed. Finally, we report four new\nsubthreshold NSBH candidates, all with strong imprints of spin precession, but\nnote that these are most likely to be of terrestrial origin.",
      "generated_abstract": "t the most precise measurement of the merger rate of neutron stars\nand black holes to date. Using the most precise electromagnetic follow-up\nobservations of GW170817 and GW190425, combined with the most precise\npre-merger GW signal from GW170817, we measure the merger rate of\nneutron-star-black-hole binaries to be $2.5^{+1.7}_{-1.2}$ events per\nyear. This rate is consistent with previous rates from GW150914, GW170817,\nGW190425, and GW170818, but is 2.5 times lower than the rate measured by\nLISA in the frequency range of interest for our analysis. We also constrain\nthe rate of short-duration",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15894039735099338,
          "p": 0.3582089552238806,
          "f": 0.22018348198089394
        },
        "rouge-2": {
          "r": 0.0547945205479452,
          "p": 0.13953488372093023,
          "f": 0.07868852054093006
        },
        "rouge-l": {
          "r": 0.1390728476821192,
          "p": 0.31343283582089554,
          "f": 0.19266054620107745
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.06020v1",
      "true_abstract": "Using a reaction-diffusion model with free boundaries in one space dimension\nfor a single population species with density $u(t,x)$ and population range\n$[g(t), h(t)]$, we demonstrate that the Allee effects can be eliminated if the\nspecies maintains its population density at a suitable level at the range\nboundary by advancing or retreating the fronts. It is proved that with such a\nstrategy at the range edge the species can invade the environment successfully\nwith all admissible initial populations, exhibiting the dynamics of super\ninvaders. Numerical simulations are used to help understand what happens if the\npopulation density level at the range boundary is maintained at other levels.\nIf the invading cane toads in Australia used this strategy at the range\nboundary to become a super invader, then our results may explain why toads near\nthe invading front evolve to have longer legs and run faster.",
      "generated_abstract": "Invasion dynamics of super- and sub-populations are studied on a\nsuper-infectious disease model where the disease-transmission rate depends on\nthe infected population size. We demonstrate that the elimination of the\nAllee effect is possible for super-populations when the infected population is\nsmaller than the sub-population. This is due to the fact that the sub-population\nhas a smaller effective range than the super-population, leading to a lower\ndisease-transmission rate. The effect of the effective range is also observed\nin a sub-infectious disease model. This finding has important implications for\ninfectious diseases spread by contact between infected and susceptible\nindividuals. We also analyze the threshold of the Allee effect in the\nsuper-infectious model and discuss its role in the emergence of epidemic\ndynamics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19148936170212766,
          "p": 0.2647058823529412,
          "f": 0.2222222173510137
        },
        "rouge-2": {
          "r": 0.03076923076923077,
          "p": 0.037383177570093455,
          "f": 0.03375526930869414
        },
        "rouge-l": {
          "r": 0.1702127659574468,
          "p": 0.23529411764705882,
          "f": 0.19753085932632236
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/GA/2503.10532v1",
      "true_abstract": "Type II Cepheids (T2Ceps), alongside RR Lyrae stars, serve as important\ndistance indicators for old population II stars due to their period-luminosity\n(PL) relations. However, studies of these relations in the Sloan photometric\nsystem are rather limited in the literature. Our goal is to calibrate PL\nrelations (and their counterparts in Wesenheit magnitudes) in the\nSloan-Pan-STARSS gP1rP1iP1 bands for Galactic T2Ceps located in the vicinity of\nthe Sun. We collected data for 16 T2Ceps of the BLHer type and 17 of the WVir\ntype using 40 cm telescopes of the Las Cumbres Observatory Global Telescope\nNetwork. Geometric parallaxes were adopted from Gaia Data Release 3. We have\ncalibrated PL and period-Wesenheit relations for Milky Way BLHer and WVir stars\nin the solar neighborhood, as well as for a combined sample of both types. The\nrelationships derived here will allow to determine the distances to T2Ceps that\nwill be discovered by the Legacy Survey of Space and Time survey and, in turn,\nto probe the extended halo of the Milky Way, as well as the halos of nearby\ngalaxies. To the best of our knowledge, the relations derived in this study are\nthe first for Milky Way T2Ceps in the Sloan bands.",
      "generated_abstract": "t a period-luminosity relation (PLR) for Galactic Type II Cepheids\n(TICs) in the $ugriz$ bands. We use the Sloan Digital Sky Survey (SDSS) DR13\ndata release, which includes 60,432 TICs with accurate parallaxes. The\ndistribution of the TICs is characterized by a relatively narrow distribution\naround the mean period, with a median period of 3.10 days. We estimate the\nmean luminosity of TICs as 1.88 mag and the mean absolute magnitude as 1.02\nmag. The PLR is well fit by a linear function with a slope of 0.010 $\\pm$\n0.001 (95\\% confidence interval) and a y-intercept of 1.00 (1.00, 1.01",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18110236220472442,
          "p": 0.3026315789473684,
          "f": 0.22660098053726138
        },
        "rouge-2": {
          "r": 0.03225806451612903,
          "p": 0.058823529411764705,
          "f": 0.04166666209201439
        },
        "rouge-l": {
          "r": 0.15748031496062992,
          "p": 0.2631578947368421,
          "f": 0.19704433029095594
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2411.13559v1",
      "true_abstract": "Financial markets are nonlinear with complexity, where different types of\nassets are traded between buyers and sellers, each having a view to maximize\ntheir Return on Investment (ROI). Forecasting market trends is a challenging\ntask since various factors like stock-specific news, company profiles, public\nsentiments, and global economic conditions influence them. This paper describes\na daily price directional predictive system of financial instruments,\naddressing the difficulty of predicting short-term price movements. This paper\nwill introduce the development of a novel trading system methodology by\nproposing a two-layer Composing Ensembles architecture, optimized through grid\nsearch, to predict whether the price will rise or fall the next day. This\nstrategy was back-tested on a wide range of financial instruments and time\nframes, demonstrating an improvement of 20% over the benchmark, representing a\nstandard investment strategy.",
      "generated_abstract": "the problem of optimizing profitability in algorithmic trading,\nusing a multi-armed bandit (MAB) framework. Traditional approaches to this\nproblem, such as the Q-learning algorithm and its variants, rely on\ninformation from the past. However, for algorithmic trading, this information\nis scarce and inaccessible, especially in high-frequency trading where the\nmarket is volatile and noisy. Traditional optimization approaches that\nincorporate past information, such as reinforcement learning (RL), struggle to\nperform well in this context. In this work, we propose a novel approach that\nuses ensembles of instrument-model pairs to overcome these challenges. The\nensembles of instrument-model pairs are composed of tradable assets and\nequivalent instrument models, and each instrument-model pair is trained with\nhistorical data from multiple assets and instrument models. We then use the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14423076923076922,
          "p": 0.1744186046511628,
          "f": 0.15789473188698075
        },
        "rouge-2": {
          "r": 0.007692307692307693,
          "p": 0.008547008547008548,
          "f": 0.00809716100575632
        },
        "rouge-l": {
          "r": 0.1346153846153846,
          "p": 0.16279069767441862,
          "f": 0.1473684160975071
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.16584v1",
      "true_abstract": "Recent advancements in audio tokenization have significantly enhanced the\nintegration of audio capabilities into large language models (LLMs). However,\naudio understanding and generation are often treated as distinct tasks,\nhindering the development of truly unified audio-language models. While\ninstruction tuning has demonstrated remarkable success in improving\ngeneralization and zero-shot learning across text and vision, its application\nto audio remains largely unexplored. A major obstacle is the lack of\ncomprehensive datasets that unify audio understanding and generation. To\naddress this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset\ncovering 80 diverse tasks across speech, music, and sound domains, with over\n100 million instances. Audio-FLAN lays the foundation for unified\naudio-language models that can seamlessly handle both understanding (e.g.,\ntranscription, comprehension) and generation (e.g., speech, music, sound) tasks\nacross a wide range of audio domains in a zero-shot manner. The Audio-FLAN\ndataset is available on HuggingFace and GitHub and will be continuously\nupdated.",
      "generated_abstract": "N (Fast Low-Rank Audio-to-Speech) is a novel audio-to-speech\ntransformation framework, which directly learns the low-rank transformer\nmapping between audio and speech. We introduce Audio-FLAN, a preliminary release\nof the system for research. Audio-FLAN consists of two components: a\ntransformer-based audio encoder and a transformer-based speech decoder. The\naudio encoder learns the mapping between audio and speech by processing\nspeech-specific features. The speech decoder uses the learned mapping to\nreconstruct audio. To evaluate Audio-FLAN, we conducted experiments on two\naudio-to-speech tasks: Speech-to-Music and Speech-to-Speech. The experiments\nshow that Audio-FLAN achieves competitive performance on the two tasks.\nAdditionally, we conducted extensive ablation studies to further analyze",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15315315315315314,
          "p": 0.2537313432835821,
          "f": 0.1910112312605733
        },
        "rouge-2": {
          "r": 0.014084507042253521,
          "p": 0.02127659574468085,
          "f": 0.016949147749211354
        },
        "rouge-l": {
          "r": 0.15315315315315314,
          "p": 0.2537313432835821,
          "f": 0.1910112312605733
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.15131v1",
      "true_abstract": "We study the fundamental problem of calibrating a linear binary classifier of\nthe form $\\sigma(\\hat{w}^\\top x)$, where the feature vector $x$ is Gaussian,\n$\\sigma$ is a link function, and $\\hat{w}$ is an estimator of the true linear\nweight $w^\\star$. By interpolating with a noninformative $\\textit{chance\nclassifier}$, we construct a well-calibrated predictor whose interpolation\nweight depends on the angle $\\angle(\\hat{w}, w_\\star)$ between the estimator\n$\\hat{w}$ and the true linear weight $w_\\star$. We establish that this angular\ncalibration approach is provably well-calibrated in a high-dimensional regime\nwhere the number of samples and features both diverge, at a comparable rate.\nThe angle $\\angle(\\hat{w}, w_\\star)$ can be consistently estimated.\nFurthermore, the resulting predictor is uniquely $\\textit{Bregman-optimal}$,\nminimizing the Bregman divergence to the true label distribution within a\nsuitable class of calibrated predictors. Our work is the first to provide a\ncalibration strategy that satisfies both calibration and optimality properties\nprovably in high dimensions. Additionally, we identify conditions under which a\nclassical Platt-scaling predictor converges to our Bregman-optimal calibrated\nsolution. Thus, Platt-scaling also inherits these desirable properties provably\nin high dimensions.",
      "generated_abstract": "the problem of calibrating a classification model $f$ in high-dimensional\nbinary classification, where the model is assumed to be linearly separable.\nCalibration is the ability of the model to predict the correct class with\nprobability one, while predictive accuracy is the probability of correctly\nclassifying the test data. We establish the optimal calibration rate and\nprovable predictive accuracy in the setting of linearly separable models. We\nshow that, for a fixed calibration rate $\\alpha$, the optimal predictive\naccuracy is attained by a classifier that satisfies a calibration constraint\nthat is a special case of a certain calibration constraint on the\nAngular-Platt transformation of the classifier. We provide an analysis of this\nspecial case, which we refer to as the Platt scaling condition, and show that\nit is a necessary condition for calibration in the case of linearly separable\nmodels. We",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24778761061946902,
          "p": 0.37333333333333335,
          "f": 0.2978723356298099
        },
        "rouge-2": {
          "r": 0.06748466257668712,
          "p": 0.09016393442622951,
          "f": 0.07719297755961867
        },
        "rouge-l": {
          "r": 0.21238938053097345,
          "p": 0.32,
          "f": 0.25531914414044826
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.physics/data-an/2503.09328v1",
      "true_abstract": "A Schr\\\"odinger bridge is the most probable time-dependent probability\ndistribution that connects an initial probability distribution $w_{i}$ to a\nfinal one $w_{f}$. The problem has been solved and widely used for the case of\nsimple Brownian evolution (non-interacting particles). It is related to the\nproblem of entropy regularized Wasserstein optimal transport. In this article,\nwe generalize Brownian bridges to systems of interacting particles. We derive\nsome equations for the forward and backward single particle ``wave-functions''\nwhich allow to compute the most probable evolution of the single-particle\nprobability between the initial and final distributions.",
      "generated_abstract": "er a system of interacting particles described by the Schr\\\"odinger\nequation with a general potential that is quadratic in the coordinate. This\npotential can be written as a sum of terms that depend on the position of the\nparticles, the mass of the particles, and a distance-dependent interaction\nstrength. We construct a set of Schr\\\"odinger bridges that are equivalent to\nthe potential. In particular, the set of Schr\\\"odinger bridges is\none-to-one related to the set of all Schr\\\"odinger eigenfunctions of the\nsystem. These bridges can be constructed by solving a system of linear\nequations, and they are constructed by solving the linear system of equations\nfor each of the Schr\\\"odinger eigenfunctions. We show that the set of\nSchr\\\"odinger bridges has a natural interpretation as a set of quantum\ninterference patterns in a quantum system. We also",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2537313432835821,
          "p": 0.26153846153846155,
          "f": 0.2575757525769055
        },
        "rouge-2": {
          "r": 0.06818181818181818,
          "p": 0.05454545454545454,
          "f": 0.060606055667789406
        },
        "rouge-l": {
          "r": 0.208955223880597,
          "p": 0.2153846153846154,
          "f": 0.2121212071223601
        }
      }
    },
    {
      "paper_id": "eess.SP.math/GR/2503.09398v1",
      "true_abstract": "Incorporating mathematical properties of a wireless policy to be learned into\nthe design of deep neural networks (DNNs) is effective for enhancing learning\nefficiency. Multi-user precoding policy in multi-antenna system, which is the\nmapping from channel matrix to precoding matrix, possesses a permutation\nequivariance property, which has been harnessed to design the parameter sharing\nstructure of the weight matrix of DNNs. In this paper, we study a stronger\nproperty than permutation equivariance, namely unitary equivariance, for\nprecoder learning. We first show that a DNN with unitary equivariance designed\nby further introducing parameter sharing into a permutation equivariant DNN is\nunable to learn the optimal precoder. We proceed to develop a novel non-linear\nweighting process satisfying unitary equivariance and then construct a joint\nunitary and permutation equivariant DNN. Simulation results demonstrate that\nthe proposed DNN not only outperforms existing learning methods in learning\nperformance and generalizability but also reduces training complexity.",
      "generated_abstract": "This paper proposes a novel precoder learning algorithm by leveraging the\nunitary equivariance property. The proposed algorithm is based on the\nunitary transformation of the received signal, which is known as a precoder\nvector. The proposed algorithm is designed to minimize the training cost and\nmaximize the performance. The proposed algorithm is applied to the case of a\nsingle-user MIMO system, and the closed-form solution is derived. Simulation\nresults show that the proposed algorithm achieves a superior performance\ncompared to the conventional precoder learning algorithm, and the simulation\nresults also confirm the validity of the unitary equivariance property.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24242424242424243,
          "p": 0.42857142857142855,
          "f": 0.3096774147396462
        },
        "rouge-2": {
          "r": 0.06944444444444445,
          "p": 0.12195121951219512,
          "f": 0.08849557059754118
        },
        "rouge-l": {
          "r": 0.20202020202020202,
          "p": 0.35714285714285715,
          "f": 0.2580645115138398
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.09150v1",
      "true_abstract": "Personalization is a critical yet often overlooked factor in boosting\nproductivity and wellbeing in knowledge-intensive workplaces to better address\nindividual preferences. Existing tools typically offer uniform guidance whether\nauto-generating email responses or prompting break reminders without accounting\nfor individual behavioral patterns or stress triggers. We introduce AdaptAI, a\nmultimodal AI solution combining egocentric vision and audio, heart and motion\nactivities, and the agentic workflow of Large Language Models LLMs to deliver\nhighly personalized productivity support and context-aware well-being\ninterventions. AdaptAI not only automates peripheral tasks (e.g. drafting\nsuccinct document summaries, replying to emails etc.) but also continuously\nmonitors the users unique physiological and situational indicators to\ndynamically tailor interventions such as micro-break suggestions or exercise\nprompts, at the exact point of need. In a preliminary study with 15\nparticipants, AdaptAI demonstrated significant improvements in task throughput\nand user satisfaction by anticipating user stressors and streamlining daily\nworkflows.",
      "generated_abstract": "r presents AdaptAI, an AI-powered tool that provides personalized\nstress management strategies. AdaptAI employs machine learning (ML) to analyze\nuser behavior, sentiment analysis, and user-generated content to predict\nstress levels and provide personalized guidance. The tool offers a variety of\nstress management strategies, including meditation, breathing exercises, and\nmindfulness exercises, as well as a stress-relief journal and supportive\ncommunity forum. AdaptAI also provides users with personalized insights and\ntips on how to manage stress, such as by identifying triggers and strategies for\nmanaging them. AdaptAI aims to help users manage their stress, boost their\nproductivity, and improve their mental health. This paper presents the\ndevelopment and implementation of AdaptAI, providing insights into its\nstress-management strategies, user experience, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.152,
          "p": 0.23170731707317074,
          "f": 0.18357487444281095
        },
        "rouge-2": {
          "r": 0.006756756756756757,
          "p": 0.008620689655172414,
          "f": 0.007575752649222672
        },
        "rouge-l": {
          "r": 0.128,
          "p": 0.1951219512195122,
          "f": 0.15458936719643415
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2407.10284v2",
      "true_abstract": "``Self-Organised Criticality'' (SOC) is the mechanism by which complex\nsystems spontaneously settle close to a *critical point*, at the edge between\nstability and chaos, and characterized by fat-tailed fluctuations and\nlong-memory correlations. Such a scenario may explain why insignificant\nperturbations can generate large disruptions, through the propagation of\n``avalanches'' across the system. In this short review, we discuss how SOC\ncould offer a plausible solution to the excess volatility puzzle in financial\nmarkets and the analogue ``small shocks, large business cycle puzzle'' for the\neconomy at large, as initially surmised by Per Bak et al. in 1993. We argue\nthat in general the quest for efficiency and the necessity of *resilience* may\nbe mutually incompatible and require specific policy considerations.",
      "generated_abstract": "r introduces the Self-Organized Criticality (SOC) paradigm as a\nnovel theoretical framework to analyze the evolution of economic and financial\nsystems. SOC describes how market dynamics are shaped by the emergence of\nself-organized, self-regulating, and self-adaptive economic and financial\nsystems. The emergence of these systems is not driven by external or internal\ninterventions but by self-organization within the system. These systems\nrepresent a new type of evolutionary process where the emergence of a new\neconomic and financial system is driven by the emergence of new behaviors,\ninstead of the emergence of new entities. This new process is called the\nemergence of emergent properties, and it is the driving force behind economic\nand financial system evolution. In this paper, we discuss the emergence of\nemergent properties in financial systems and propose a framework",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.2676056338028169,
          "f": 0.22891565775511696
        },
        "rouge-2": {
          "r": 0.0423728813559322,
          "p": 0.04716981132075472,
          "f": 0.044642852157207195
        },
        "rouge-l": {
          "r": 0.16842105263157894,
          "p": 0.22535211267605634,
          "f": 0.19277107944186397
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/CY/2503.10264v1",
      "true_abstract": "While peer review enhances writing and research quality, harsh feedback can\nfrustrate and demotivate authors. Hence, it is essential to explore how\ncritiques should be delivered to motivate authors and enable them to keep\niterating their work. In this study, we explored the impact of appending an\nautomatically generated positive summary to the peer reviews of a writing task,\nalongside varying levels of overall evaluations (high vs. low), on authors'\nfeedback reception, revision outcomes, and motivation to revise. Through a 2x2\nonline experiment with 137 participants, we found that adding an AI-reframed\npositive summary to otherwise harsh feedback increased authors' critique\nacceptance, whereas low overall evaluations of their work led to increased\nrevision efforts. We discuss the implications of using AI in peer feedback,\nfocusing on how AI-driven critiques can influence critique acceptance and\nsupport research communities in fostering productive and friendly peer feedback\npractices.",
      "generated_abstract": "ew plays a vital role in scientific discovery, providing independent\nanalysis and critical feedback to enhance the validity and credibility of\nresearch. However, the traditional peer review process can be time-consuming and\nincomplete, leading to biased and inaccurate reviews. Recent advances in\nAI-driven review systems have the potential to improve peer review efficiency\nand accuracy. However, existing AI-based peer review systems often fail to\naddress key challenges, such as: (1) understanding peer review comments, (2)\nenhancing the comprehensiveness and relevance of peer review comments, and (3)\nsupporting peer reviewers in managing their review tasks. To address these\nchallenges, we introduce Positive Summary, a novel AI-based peer review system\nthat integrates review summarization and peer review feedback generation.\nPositive Summary leverages machine learning and structured",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17,
          "p": 0.2073170731707317,
          "f": 0.1868131818620941
        },
        "rouge-2": {
          "r": 0.007194244604316547,
          "p": 0.008771929824561403,
          "f": 0.007905133388745308
        },
        "rouge-l": {
          "r": 0.16,
          "p": 0.1951219512195122,
          "f": 0.17582417087308314
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.00725v1",
      "true_abstract": "We propose a machine-learning tool that yields causal inference on text in\nrandomized trials. Based on a simple econometric framework in which text may\ncapture outcomes of interest, our procedure addresses three questions: First,\nis the text affected by the treatment? Second, which outcomes is the effect on?\nAnd third, how complete is our description of causal effects? To answer all\nthree questions, our approach uses large language models (LLMs) that suggest\nsystematic differences across two groups of text documents and then provides\nvalid inference based on costly validation. Specifically, we highlight the need\nfor sample splitting to allow for statistical validation of LLM outputs, as\nwell as the need for human labeling to validate substantive claims about how\ndocuments differ across groups. We illustrate the tool in a proof-of-concept\napplication using abstracts of academic manuscripts.",
      "generated_abstract": "This paper develops a novel method for causal inference on outcomes learned\nfrom text data. We introduce a novel measure of causal effect, the conditional\naverage treatment effect (CATE), which accounts for heterogeneity in treatment\neffects across individuals, and show that this measure is consistent under\nsuitable conditions. We then propose a new method for inference on the\nestimand of interest, the causal effect of interest (CEI), using a\nprobability-weighted average of the outcomes learned from text. We show that\nthis estimand is consistent under suitable conditions, and we propose an\nefficient estimator that is asymptotically normal and has finite variance. We\napply our method to a dataset of US census tracts and find that CEI is\nsignificantly higher in areas with greater racial/ethnic diversity. We also\nfind that CEI is higher in areas with greater poverty and unemployment.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.3,
          "f": 0.27272726776859507
        },
        "rouge-2": {
          "r": 0.03787878787878788,
          "p": 0.041666666666666664,
          "f": 0.03968253469387818
        },
        "rouge-l": {
          "r": 0.21875,
          "p": 0.2625,
          "f": 0.23863635867768607
        }
      }
    },
    {
      "paper_id": "q-fin.PM.econ/GN/2503.07498v1",
      "true_abstract": "We examine the problem of optimal portfolio allocation within the framework\nof utility theory. We apply exponential utility to derive the optimal\ndiversification strategy and logarithmic utility to determine the optimal\nleverage. We enhance existing methodologies by incorporating compound\nprobability distributions to model the effects of both statistical and\nnon-stationary uncertainties. Additionally, we extend the maximum expected\nutility objective by including the variance of utility in the objective\nfunction, which we term generalized mean-variance. In the case of logarithmic\nutility, it provides a natural explanation for the half-Kelly criterion, a\nconcept widely used by practitioners.",
      "generated_abstract": "This study provides a theoretical framework for determining optimal portfolio\ndesigns in a utility-based approach, focusing on portfolio allocation in the\npresence of leverage and diversification. The framework is built on a\nframework of portfolio allocation in a single stock market with an efficient\nmarket hypothesis, with leverage and diversification as key variables.\nAdditionally, we consider an investment scenario where a portfolio is allocated\nindependently across different assets, but the investor has leverage and\ndiversification constraints. The results show that the optimal allocation is\ndependent on the asset returns and the investor's leverage and diversification\nconstraints. The optimal allocation is different depending on the return\ndistribution and the investor's leverage and diversification constraints. The\nresults provide a theoretical framework for understanding how leverage and\ndiversification impact portfolio allocation decisions in a utility-based\napproach.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23076923076923078,
          "p": 0.22388059701492538,
          "f": 0.2272727222738752
        },
        "rouge-2": {
          "r": 0.07692307692307693,
          "p": 0.06862745098039216,
          "f": 0.07253885511986934
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.19402985074626866,
          "f": 0.19696969197084496
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.01704v1",
      "true_abstract": "Discrete Choice Modelling serves as a robust framework for modelling human\nchoice behaviour across various disciplines. Building a choice model is a semi\nstructured research process that involves a combination of a priori\nassumptions, behavioural theories, and statistical methods. This complex set of\ndecisions, coupled with diverse workflows, can lead to substantial variability\nin model outcomes. To better understand these dynamics, we developed the\nSerious Choice Modelling Game, which simulates the real world modelling process\nand tracks modellers' decisions in real time using a stated preference dataset.\nParticipants were asked to develop choice models to estimate Willingness to Pay\nvalues to inform policymakers about strategies for reducing noise pollution.\nThe game recorded actions across multiple phases, including descriptive\nanalysis, model specification, and outcome interpretation, allowing us to\nanalyse both individual decisions and differences in modelling approaches.\nWhile our findings reveal a strong preference for using data visualisation\ntools in descriptive analysis, it also identifies gaps in missing values\nhandling before model specification. We also found significant variation in the\nmodelling approach, even when modellers were working with the same choice\ndataset. Despite the availability of more complex models, simpler models such\nas Multinomial Logit were often preferred, suggesting that modellers tend to\navoid complexity when time and resources are limited. Participants who engaged\nin more comprehensive data exploration and iterative model comparison tended to\nachieve better model fit and parsimony, which demonstrate that the\nmethodological choices made throughout the workflow have significant\nimplications, particularly when modelling outcomes are used for policy\nformulation.",
      "generated_abstract": "The decision-making process of choice modellers is often assumed to be\neither linear or probabilistic, yet there is little understanding of how these\nmodels are actually constructed. This paper introduces a new framework to\ndescribe the decision-making process of choice modellers. We define decision\nrules as sets of actions that a modeller must take, and construct a\nprobabilistic framework for modellers to perform this process. We use this\nframework to explain the decision-making process of a choice modeller, and\ndemonstrate that this model can be used to interpret the results of many\nexisting studies. The framework is applied to a study of the impact of\ninterventions on consumer behaviour, showing that the decision-making process\nof choice modellers is complex, and that interventions often have mixed effects\non the choices of modellers.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15789473684210525,
          "p": 0.36,
          "f": 0.2195121908834028
        },
        "rouge-2": {
          "r": 0.020080321285140562,
          "p": 0.04310344827586207,
          "f": 0.027397255937850192
        },
        "rouge-l": {
          "r": 0.13450292397660818,
          "p": 0.30666666666666664,
          "f": 0.18699186568015078
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.10196v1",
      "true_abstract": "In this paper, we present an error estimate for the filtered Lie splitting\nscheme applied to the Zakharov system, characterized by solutions exhibiting\nvery low regularity across all dimensions. Our findings are derived from the\napplication of multilinear estimates established within the framework of\ndiscrete Bourgain spaces. Specifically, we demonstrate that when the solution\n$(E,z,z_t) \\in H^{s+r+1/2}\\times H^{s+r}\\times H^{s+r-1}$, the error in\n$H^{r+1/2}\\times H^{r}\\times H^{r-1}$ is $\\mathcal{O}(\\tau^{s/2})$ for\n$s\\in(0,2]$, where $r=\\max(0,\\frac d2-1)$. To the best of our knowledge, this\nrepresents the first explicit error estimate for the splitting method based on\nthe original Zakharov system, as well as the first instance where low\nregularity error estimates for coupled equations have been considered within\nthe Bourgain framework. Furthermore, numerical experiments confirm the validity\nof our theoretical results.",
      "generated_abstract": "In this paper, we develop a filtered Lie splitting method to solve the Zakharov\nsystem with a low regularity term. The method is based on the modified\nGrothendieck-Vietoris-Rips complex, which allows us to handle the non-smooth\nregions of the domain and the target spaces. The main idea is to use the\nLie-Trotter decomposition to decompose the dynamics into a series of standard\nLie groups, which are treated by the modified Grothendieck-Vietoris-Rips\ncomplex. The method is illustrated by numerical examples.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23595505617977527,
          "p": 0.38181818181818183,
          "f": 0.29166666194540897
        },
        "rouge-2": {
          "r": 0.08547008547008547,
          "p": 0.13513513513513514,
          "f": 0.10471203713823657
        },
        "rouge-l": {
          "r": 0.2247191011235955,
          "p": 0.36363636363636365,
          "f": 0.2777777730565201
        }
      }
    },
    {
      "paper_id": "math.GM.math/GM/2502.14877v1",
      "true_abstract": "In this paper is studied the problem concerning the angle between two\nsubspaces of arbitrary dimensions in Euclidean space $E_{n}$. It is proven that\nthe angle between two subspaces is equal to the angle between their orthogonal\nsubspaces. Using the eigenvalues and eigenvectors of corresponding matrix\nrepresentations, there are introduced principal values and principal subspaces.\nTheir geometrical interpretation is also given together with the canonical\nrepresentation of the two subspaces. The canonical matrix for the two subspaces\nis introduced and its properties of duality are obtained. Here obtained results\nexpand the classic results given in [1,2].",
      "generated_abstract": "We study the principal values and principal subspaces of two subspaces of\nvector spaces with an inner product. We consider the case when the two subspaces\nare one-dimensional and the inner product is the usual Euclidean inner\nproduct. In this case, we give the conditions for the existence of the principal\nvalues and principal subspaces of the two subspaces. We also give the conditions\nfor the existence of the principal values and principal subspaces of the\nsubspaces when the inner product is a Hilbert-Schmidt inner product. In the\nsecond case, we also give the conditions for the existence of the principal\nvalues and principal subspaces of the two subspaces.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25862068965517243,
          "p": 0.42857142857142855,
          "f": 0.3225806404671061
        },
        "rouge-2": {
          "r": 0.11764705882352941,
          "p": 0.17543859649122806,
          "f": 0.14084506561694124
        },
        "rouge-l": {
          "r": 0.2413793103448276,
          "p": 0.4,
          "f": 0.30107526412302005
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.16277v1",
      "true_abstract": "Machine learning is critical for innovation and efficiency in financial\nmarkets, offering predictive models and data-driven decision-making. However,\nchallenges such as missing data, lack of transparency, untimely updates,\ninsecurity, and incompatible data sources limit its effectiveness. Blockchain\ntechnology, with its transparency, immutability, and real-time updates,\naddresses these challenges. We present a framework for integrating\nhigh-frequency on-chain data with low-frequency off-chain data, providing a\nbenchmark for addressing novel research questions in economic mechanism design.\nThis framework generates modular, extensible datasets for analyzing economic\nmechanisms such as the Transaction Fee Mechanism, enabling multi-modal insights\nand fairness-driven evaluations. Using four machine learning techniques,\nincluding linear regression, deep neural networks, XGBoost, and LSTM models, we\ndemonstrate the framework's ability to produce datasets that advance financial\nresearch and improve understanding of blockchain-driven systems. Our\ncontributions include: (1) proposing a research scenario for the Transaction\nFee Mechanism and demonstrating how the framework addresses previously\nunexplored questions in economic mechanism design; (2) providing a benchmark\nfor financial machine learning by open-sourcing a sample dataset generated by\nthe framework and the code for the pipeline, enabling continuous dataset\nexpansion; and (3) promoting reproducibility, transparency, and collaboration\nby fully open-sourcing the framework and its outputs. This initiative supports\nresearchers in extending our work and developing innovative financial\nmachine-learning models, fostering advancements at the intersection of machine\nlearning, blockchain, and economics.",
      "generated_abstract": "machine learning (FML) models have demonstrated promising\nresults in financial applications, yet existing datasets are limited in\ncoverage, variety, and completeness, which hinder their widespread adoption.\nThis paper introduces FinML-Chain, the first blockchain-integrated dataset\ndeveloped for FML, which comprises 2.8 million transaction records and 675,000\nsignals. We leverage the unique characteristics of blockchain technology, such\nas timestamp, non-repudiation, and immutability, to build a trustworthy\ndataset. We also enhance the dataset's completeness by integrating historical\ntransaction records from the US Securities and Exchange Commission (SEC) and\nanalyzing the characteristics of new transactions. This enables us to conduct\ncomprehensive and detailed analyses of the blockchain market, such as the\nd",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1527777777777778,
          "p": 0.2619047619047619,
          "f": 0.19298245148661142
        },
        "rouge-2": {
          "r": 0.019417475728155338,
          "p": 0.0380952380952381,
          "f": 0.025723468196152594
        },
        "rouge-l": {
          "r": 0.13194444444444445,
          "p": 0.2261904761904762,
          "f": 0.16666666201292718
        }
      }
    },
    {
      "paper_id": "math-ph.nlin/SI/2503.08018v1",
      "true_abstract": "In this paper we consider the Toda lattice $(\\boldsymbol{p}(t);\n\\boldsymbol{q}(t))$ at thermal equilibrium, meaning that its variables $(p_i)$\nand $(e^{q_i-q_{i+1}})$ are independent Gaussian and Gamma random variables,\nrespectively. We justify the notion from the physics literature that this model\ncan be thought of as a dense collection of solitons (or \"soliton gas'') by, (i)\nprecisely defining the locations of these solitons; (ii) showing that local\ncharges and currents for the Toda lattice are well-approximated by simple\nfunctions of the soliton data; and (iii) proving an asymptotic scattering\nrelation that governs the dynamics of the soliton locations. Our arguments are\nbased on analyzing properties about eigenvector entries of the Toda lattice's\n(random) Lax matrix, particularly, their rates of exponential decay and their\nevolutions under inverse scattering.",
      "generated_abstract": "ering problem in quantum mechanics is an important tool for\nstudying scattering phenomena in systems with a finite number of degrees of\nfreedom. The scattering problem in the Toda lattice has been investigated\nextensively in the literature. In this paper, we propose a novel approach for\nthe scattering problem in the Toda lattice based on the scattering problem in\nthe Boussinesq-Laplace equation, which provides a unified framework for\ninvestigating scattering phenomena in quantum mechanics. The proposed approach\nis based on the asymptotic scattering relation for the Boussinesq-Laplace\nequation, which is derived from the asymptotic scattering relation for the\nToda lattice. The proposed approach is applied to the scattering problem in the\nToda lattice to obtain the scattering relation for the Toda lattice. The\nscattering problem in the Toda lattice is solved using the asympt",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18085106382978725,
          "p": 0.2982456140350877,
          "f": 0.22516555821411352
        },
        "rouge-2": {
          "r": 0.06779661016949153,
          "p": 0.0963855421686747,
          "f": 0.0796019852013567
        },
        "rouge-l": {
          "r": 0.1595744680851064,
          "p": 0.2631578947368421,
          "f": 0.198675491988948
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.03019v1",
      "true_abstract": "In this paper, we define an underlying data generating process that allows\nfor different magnitudes of cross-sectional dependence, along with time series\nautocorrelation. This is achieved via high-dimensional moving average processes\nof infinite order (HDMA($\\infty$)). Our setup and investigation integrates and\nenhances homogenous and heterogeneous panel data estimation and testing in a\nunified way. To study HDMA($\\infty$), we extend the Beveridge-Nelson\ndecomposition to a high-dimensional time series setting, and derive a complete\ntoolkit set. We exam homogeneity versus heterogeneity using Gaussian\napproximation, a prevalent technique for establishing uniform inference. For\npost-testing inference, we derive central limit theorems through Edgeworth\nexpansions for both homogenous and heterogeneous settings. Additionally, we\nshowcase the practical relevance of the established asymptotic properties by\nrevisiting the common correlated effects (CCE) estimators, and a classic\nnonstationary panel data process. Finally, we verify our theoretical findings\nvia extensive numerical studies using both simulated and real datasets.",
      "generated_abstract": "We consider a model with a random intercept and random slope that models\nthe relationship between two heterogeneous variables and a single\nrandomly-exogenous variable. We study the homogeneity and heterogeneity of\nestimators and inference for the parameters of the model. We apply our\nresults to panel data and to data from an application to the U.S. census. We\nshow that in many settings, the estimators are homogeneous, and that in\nother settings, they are heterogeneous. We illustrate the implications of the\nresults for inference in panel data and for estimating the variance of\nestimators. We also apply our results to a census dataset to estimate the\nvariance of a random intercept and random slope estimator. The results show\nthat the variance of the estimator depends on the heterogeneity of the\ninteraction terms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.168141592920354,
          "p": 0.3114754098360656,
          "f": 0.21839080004425956
        },
        "rouge-2": {
          "r": 0.027972027972027972,
          "p": 0.036036036036036036,
          "f": 0.031496058071486914
        },
        "rouge-l": {
          "r": 0.1592920353982301,
          "p": 0.29508196721311475,
          "f": 0.20689654717069633
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.05649v1",
      "true_abstract": "Recent advancements in controllable expressive speech synthesis, especially\nin text-to-speech (TTS) models, have allowed for the generation of speech with\nspecific styles guided by textual descriptions, known as style prompts. While\nthis development enhances the flexibility and naturalness of synthesized\nspeech, there remains a significant gap in understanding how these models\nhandle vague or abstract style prompts. This study investigates the potential\ngender bias in how models interpret occupation-related prompts, specifically\nexamining their responses to instructions like \"Act like a nurse\". We explore\nwhether these models exhibit tendencies to amplify gender stereotypes when\ninterpreting such prompts. Our experimental results reveal the model's tendency\nto exhibit gender bias for certain occupations. Moreover, models of different\nsizes show varying degrees of this bias across these occupations.",
      "generated_abstract": "on-Guided Speech Synthesis (IGSS) models have emerged as a promising\ntransformer-based approach to generating natural language speech. However,\nresearch on gender bias in IGSS models has been limited, mainly due to the\nlimited availability of large-scale datasets that contain rich gender\ninformation. To address this limitation, we introduce the first gender bias\nanalysis in IGSS models. Our approach consists of two steps: (1) gender\nannotations and (2) model evaluation. In step (1), we create a dataset of\nreal-world speech with a male speaker and a female speaker, and annotate them\nwith gender labels. We then evaluate the model's ability to generate speech\nthat matches the annotated gender labels. In step (2), we evaluate the gender\nbias of the model's generated speech. We analyze the gender bias of IGSS models\nby comparing the gender distribution of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1935483870967742,
          "p": 0.21176470588235294,
          "f": 0.20224718602133582
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.03305785123966942,
          "f": 0.03319501574697482
        },
        "rouge-l": {
          "r": 0.1827956989247312,
          "p": 0.2,
          "f": 0.19101123096515604
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.astro-ph/EP/2503.08988v1",
      "true_abstract": "We propose a new method for investigating atmospheric inhomogeneities in\nexoplanets through transmission spectroscopy. Our approach links chromatic\nvariations in conventional transit model parameters (central transit time,\ntotal and full durations, and transit depth) to atmospheric asymmetries. By\nseparately analyzing atmospheric asymmetries during ingress and egress, we can\nderive clear connections between these variations and the underlying\nasymmetries of the planetary limbs. Additionally, this approach enables us to\ninvestigate differences between the limbs slightly offset from the terminator\non the dayside and the nightside. We applied this method to JWST's\nNIRSpec/G395H observations of the hot Saturn exoplanet WASP-39 b. Our analysis\nsuggests a higher abundance of CO2 on the evening limb compared to the morning\nlimb and indicates a greater probability of SO2 on the limb slightly offset\nfrom the terminator on the dayside relative to the nightside. These findings\nhighlight the potential of our method to enhance the understanding of\nphotochemical processes in exoplanetary atmospheres.",
      "generated_abstract": "tion of atmospheric asymmetry in transiting exoplanets is crucial for\nunderstanding their physical and chemical properties. While transit-time\nvariation (TTV) is a promising technique for detecting asymmetries, it suffers\nfrom a strong, inherent systematic bias due to the phase-folding of the\nobserved transit times. This bias is caused by the time-dependent phase\nevolution of the transit center due to the Earth's orbital motion. In this\nwork, we investigate the impact of this bias on the asymmetry detection in\ntransiting exoplanets. Using a statistical analysis, we present a robust\nmethod for detecting asymmetries in transiting exoplanets using Chromatic\nTransit Time Variation (ChTTV). We apply this method to the transiting exoplanet\nHD 189733b,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20618556701030927,
          "p": 0.26666666666666666,
          "f": 0.23255813461668481
        },
        "rouge-2": {
          "r": 0.04285714285714286,
          "p": 0.0594059405940594,
          "f": 0.04979252625127025
        },
        "rouge-l": {
          "r": 0.1958762886597938,
          "p": 0.25333333333333335,
          "f": 0.2209302276399406
        }
      }
    },
    {
      "paper_id": "cs.CC.stat/CO/2502.15024v1",
      "true_abstract": "We investigate implications of the (extended) low-degree conjecture (recently\nformalized in [MW23]) in the context of the symmetric stochastic block model.\nAssuming the conjecture holds, we establish that no polynomial-time algorithm\ncan weakly recover community labels below the Kesten-Stigum (KS) threshold. In\nparticular, we rule out polynomial-time estimators that, with constant\nprobability, achieve correlation with the true communities that is\nsignificantly better than random. Whereas, above the KS threshold,\npolynomial-time algorithms are known to achieve constant correlation with the\ntrue communities with high probability[Mas14,AS15].\n  To our knowledge, we provide the first rigorous evidence for the sharp\ntransition in recovery rate for polynomial-time algorithms at the KS threshold.\nNotably, under a stronger version of the low-degree conjecture, our lower bound\nremains valid even when the number of blocks diverges. Furthermore, our results\nprovide evidence of a computational-to-statistical gap in learning the\nparameters of stochastic block models.\n  In contrast to prior work, which either (i) rules out polynomial-time\nalgorithms for hypothesis testing with 1-o(1) success probability [Hopkins18,\nBBK+21a] under the low-degree conjecture, or (ii) rules out low-degree\npolynomials for learning the edge connection probability matrix [LG23], our\napproach provides stronger lower bounds on the recovery and learning problem.\n  Our proof combines low-degree lower bounds from [Hopkins18, BBK+21a] with\ngraph splitting and cross-validation techniques. In order to rule out general\nrecovery algorithms, we employ the correlation preserving projection method\ndeveloped in [HS17].",
      "generated_abstract": "aper, we establish a conjecture of H\\'ajek and N\\'ez-Ram\\'irez\nconjecturing that the minimum degree of the graph of a deterministic stochastic\nblock model satisfies the following inequality:\n  $\\min_{0\\le s\\le T}d_s\\le O(\\log T)$ as $T\\to\\infty$. The conjecture is\nsharper than the classical low degree conjecture of Erd\\\"os and R\\'enyi which\nsuggests that the minimum degree of the graph of a random stochastic block\nmodel satisfies $d_s\\ge c\\log T$ as $T\\to\\infty$. The conjecture was proved for\nthe Erd\\\"os-R\\'enyi random graph with constant edge probability $p$ and the\nconjecture is proved for the random block model with deterministic edge\nprobability $p$ for all $0\\le s",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14893617021276595,
          "p": 0.38181818181818183,
          "f": 0.2142857102483341
        },
        "rouge-2": {
          "r": 0.0380952380952381,
          "p": 0.1,
          "f": 0.055172409797859985
        },
        "rouge-l": {
          "r": 0.14184397163120568,
          "p": 0.36363636363636365,
          "f": 0.20408162861568105
        }
      }
    },
    {
      "paper_id": "cs.CV.q-bio/CB/2410.14612v2",
      "true_abstract": "High-throughput image analysis in the biomedical domain has gained\nsignificant attention in recent years, driving advancements in drug discovery,\ndisease prediction, and personalized medicine. Organoids, specifically, are an\nactive area of research, providing excellent models for human organs and their\nfunctions. Automating the quantification of organoids in microscopy images\nwould provide an effective solution to overcome substantial manual\nquantification bottlenecks, particularly in high-throughput image analysis.\nHowever, there is a notable lack of open biomedical datasets, in contrast to\nother domains, such as autonomous driving, and, notably, only few of them have\nattempted to quantify annotation uncertainty. In this work, we present MultiOrg\na comprehensive organoid dataset tailored for object detection tasks with\nuncertainty quantification. This dataset comprises over 400 high-resolution 2d\nmicroscopy images and curated annotations of more than 60,000 organoids. Most\nimportantly, it includes three label sets for the test data, independently\nannotated by two experts at distinct time points. We additionally provide a\nbenchmark for organoid detection, and make the best model available through an\neasily installable, interactive plugin for the popular image visualization tool\nNapari, to perform organoid quantification.",
      "generated_abstract": "based cell-type identification (OCID) has emerged as a powerful\nmethod for analyzing complex biological systems, but existing datasets\nexhibit significant limitations, including low-quality images, limited\ndiversity, and poor annotation quality. To address these challenges, we\npropose MultiOrg, a multi-rater dataset that includes 300 high-quality\nmulti-organ tissue samples from 10 diverse organs. The dataset was developed\nthrough a multi-staged annotation process, including manual image inspection,\nautomated image segmentation, and manual annotation. To ensure accurate\ndetection of cell types, we also conducted extensive validation and\nvalidation-free evaluation. Our results demonstrate that MultiOrg surpasses\nstate-of-the-art OCID datasets in terms of image quality and diversity.\nAdditionally, we evaluate the dataset's performance on 15",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1357142857142857,
          "p": 0.2111111111111111,
          "f": 0.16521738654064286
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1357142857142857,
          "p": 0.2111111111111111,
          "f": 0.16521738654064286
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.13982v1",
      "true_abstract": "Natural Language Processing (NLP) and Voice Recognition agents are rapidly\nevolving healthcare by enabling efficient, accessible, and professional patient\nsupport while automating grunt work. This report serves as my self project\nwherein models finetuned on medical call recordings are analysed through a\ntwo-stage system: Automatic Speech Recognition (ASR) for speech transcription\nand a Large Language Model (LLM) for context-aware, professional responses.\nASR, finetuned on phone call recordings provides generalised transcription of\ndiverse patient speech over call, while the LLM matches transcribed text to\nmedical diagnosis. A novel audio preprocessing strategy, is deployed to provide\ninvariance to incoming recording/call data, laden with sufficient augmentation\nwith noise/clipping to make the pipeline robust to the type of microphone and\nambient conditions the patient might have while calling/recording.",
      "generated_abstract": "iagnostics is an important application of language models (LMs)\nand has received significant attention in recent years. However,\nstate-of-the-art LM-based diagnostic systems often suffer from suboptimal\nperformance, particularly in clinical settings, due to the complex and\nsemi-structured nature of medical data. In this paper, we propose the\nSpeech-to-Medical-Diagnostics (S2M) benchmark, a novel diagnostic benchmark\ndesigned to assess LM-based diagnostic systems in medical contexts. The S2M\nbenchmark includes 100,000 speech utterances from 10,000 patients, with 240\ncases per patient, and is designed to evaluate LM performance in three\nsub-tasks: 1) Speech-to-Medical-Diagnostics (S2M), 2) Medical-to-Spe",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11578947368421053,
          "p": 0.14864864864864866,
          "f": 0.13017750987010276
        },
        "rouge-2": {
          "r": 0.008264462809917356,
          "p": 0.011235955056179775,
          "f": 0.009523804639911801
        },
        "rouge-l": {
          "r": 0.10526315789473684,
          "p": 0.13513513513513514,
          "f": 0.11834319034347558
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2501.18909v1",
      "true_abstract": "HIV-1 replication can be suppressed with antiretroviral therapy (ART), but\nindividuals who stop taking ART soon become viremic again. Some people\nexperience extended times of detectable viremia despite optimal adherence to\nART. In the issue of the JCI, White, Wu, and coauthors elucidate a source of\nnonsuppressible viremia (NSV) in treatment-adherent patients clonally expanded\nT cells harboring HIV-1 proviruses with small deletions or mutations in the\n5'-leader, the UTR that includes the major splice donor site of viral RNA.\nThese mutations altered viral RNA-splicing efficiency and RNA dimerization and\npackaging, yet still allowed production of detectable levels of noninfectious\nvirus particles. These particles lacked the HIV-1 Env surface protein required\nfor cell entry and failed to form the mature capsid cone required for\ninfectivity. These studies improve our understanding of NSV and the regulation\nof viral functions in the 5'-leader with implications for rationalized care in\nindividuals with NSV.",
      "generated_abstract": "years, molecular virology has provided an important tool for\nanalysis of HIV-1-infected patients. In this study, we present a method to\ndetect viremia in HIV-1-infected patients, which we call nonsuppressible\nviremia (NSV). NSV is characterized by a significant increase in the viral load\nduring treatment, but not after treatment completion. Using this method, we\nidentified two patients who developed NSV during antiretroviral therapy for\nHIV-1 infection. The patients were treated with highly active antiretroviral\ntherapy (HAART) for a median of 14 months and had a CD4+ cell count of\napproximately 400 cells/mm3. We found that the two patients had substantial\nincreases in their viral loads during HA",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20754716981132076,
          "p": 0.2716049382716049,
          "f": 0.23529411273642378
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.038461538461538464,
          "f": 0.03225805964620261
        },
        "rouge-l": {
          "r": 0.1792452830188679,
          "p": 0.2345679012345679,
          "f": 0.2032085512390976
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.03608v1",
      "true_abstract": "This paper studies games of voluntary disclosure in which a sender discloses\nevidence to a receiver who then offers an allocation and transfers. We\ncharacterize the set of equilibrium payoffs in this setting. Our main result\nestablishes that any payoff profile that can be achieved through information\ndesign can also be supported by an equilibrium of the disclosure game. Hence,\nour analysis suggests an equivalence between disclosure and design in these\nsettings. We apply our results to monopoly pricing, bargaining over policies,\nand insurance markets.",
      "generated_abstract": "We consider the problem of designing a disclosure rule for a firm that has to\ndisclose both its actual and optimal value. The firm has to design a rule that\nleads to the optimal disclosure, given its cost of disclosure. We show that the\noptimal disclosure rule is the minimal consistent disclosure rule, that is, the\none that is consistent with the optimal disclosure and minimizes the cost of\ndisclosure. We also show that a firm can design a rule that leads to the\noptimal disclosure if and only if it has a costly disclosure. We also show that\nthe optimal disclosure rule is the minimal consistent disclosure rule, that is,\nthe one that is consistent with the optimal disclosure and minimizes the cost\nof disclosure. We show that a firm can design a rule that leads to the\noptimal disclosure if and only if it has a costly disclosure.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16923076923076924,
          "p": 0.2619047619047619,
          "f": 0.2056074718665387
        },
        "rouge-2": {
          "r": 0.011904761904761904,
          "p": 0.013157894736842105,
          "f": 0.012499995012501989
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.23809523809523808,
          "f": 0.18691588308149196
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/MF/2411.10386v2",
      "true_abstract": "The fragility of financial systems was starkly demonstrated in early 2023\nthrough a cascade of major bank failures in the United States, including the\nsecond, third, and fourth largest collapses in the US history. The highly\ninterdependent financial networks and the associated high systemic risk have\nbeen deemed the cause of the crashes. The goal of this paper is to enhance\nexisting systemic risk analysis frameworks by incorporating essential debt\nvaluation factors. Our results demonstrate that these additional elements\nsubstantially influence the outcomes of risk assessment. Notably, by modeling\nthe dynamic relationship between interest rates and banks' credibility, our\nframework can detect potential cascading failures that standard approaches\nmight miss. The proposed risk assessment methodology can help regulatory bodies\nprevent future failures, while also allowing companies to more accurately\npredict turmoil periods and strengthen their survivability during such events.",
      "generated_abstract": "Systemic risk has been a critical concern in financial markets for a\nlong time. However, the relationship between systemic risk and various risk\nfactors has been poorly understood. In this paper, we propose a method to\nidentify the relationships between systemic risk and various risk factors by\nusing a modified Black-Litterman model. The modified Black-Litterman model\nincorporates the impact of debt valuation factors on systemic risk, which\nprovides a more comprehensive and accurate assessment of systemic risk. The\nresults show that debt valuation factors have a significant impact on the\nsystemic risk of financial institutions. This study provides a more\ndetailed and reliable assessment of systemic risk, which is essential for\neffective risk management and risk-based capital requirements.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22935779816513763,
          "p": 0.36764705882352944,
          "f": 0.28248587097449657
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.05102040816326531,
          "f": 0.042918450061707344
        },
        "rouge-l": {
          "r": 0.1926605504587156,
          "p": 0.3088235294117647,
          "f": 0.23728813086150222
        }
      }
    },
    {
      "paper_id": "gr-qc.gr-qc/2503.09918v1",
      "true_abstract": "Quasi-normal modes (QNMs) and greybody factors are some of the most\ncharacteristic features of the dynamics of black holes (BHs) and represent the\nbasis for a number of fundamental physics tests with gravitational wave\nobservations. It is therefore important to understand the properties of these\nquantities, naturally introduced within BH perturbation theory, in particular\nthe stability properties under modifications of the BH potential. Instabilities\nin the QNMs have been recently shown to appear in the BH pseudospectrum under\ncertain circumstances. In this work, we give a novel point of view based on the\nexistence of some recently discovered hidden symmetries in BH dynamics and the\nassociated infinite series of conserved quantities, the Korteweg-de Vries (KdV)\nintegrals. We provide different motivations to use the KdV integrals as\nindicators of some crucial BH spectral properties. In particular, by studying\nthem in different scenarios described by modified BH barriers, we find strong\nevidence that the KdV conserved quantities represent a useful tool to look for\ninstabilities in the BH spectrum of QNMs and in their greybody factors.",
      "generated_abstract": "aper, we examine the stability of solutions to the KdV equation in\nvarious modified black hole potentials. We start by examining the KdV equation\nwith a potential of the form $V(r)=V_0r^{n+1}$, where $V_0$ is the\ndimensionless central potential and $n\\in \\mathbb{R}$. We find that the\nKorteweg-de Vries equation, when solved in the form\n$u_t=-\\frac{1}{2}\\frac{d}{dr}u^3+\\frac{1}{2}u^5$, is unstable in the case of\n$n=1$ and $n=2$. However, we find that the Korteweg-de Vries equation is\nunstable only when the dimensionless central potential is sufficiently large,\nnamely, when $V_0\\gg 1$. This result is consistent with",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18018018018018017,
          "p": 0.36363636363636365,
          "f": 0.24096385099070985
        },
        "rouge-2": {
          "r": 0.04878048780487805,
          "p": 0.10256410256410256,
          "f": 0.06611569811078506
        },
        "rouge-l": {
          "r": 0.17117117117117117,
          "p": 0.34545454545454546,
          "f": 0.22891565821962553
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.02040v1",
      "true_abstract": "This paper introduces a distributed contingency detection algorithm for\ndetecting unobservable contingencies in power distribution systems using\nstochastic hybrid system (SHS) models. We aim to tackle the challenge of\nlimited measurement capabilities in distribution networks that restrict the\nability to detect contingencies promptly. We incorporate the dynamics of\ndistribution network connections, load feeders, PV, and battery energy storage\nsystem (BESS) hybrid resources into a fully correlated SHS model representing\nthe distribution system as a randomly switching system between different\nstructures during contingency occurrence. We show that jumps in the SHS model\ncorrespond to contingencies in the physical power grid. We propose a probing\napproach based on magnitude-modulation inputs (MaMI) to make contingencies\ndetectable. The effectiveness of the proposed approach is validated through\nsimulations on a sample distribution system.",
      "generated_abstract": "stribution Systems (ADS) are a promising alternative to\nunderground power lines for energy transmission, but their implementation\nremains challenging due to their complexity and inherent uncertainties. This\npaper addresses this issue by introducing a hybrid stochastic systems approach\nto model the ADS. The model is based on the well-known distribution network\napproach, which includes a deterministic power flow model and a stochastic\nmodel for the unobservable contingencies. The deterministic model is\nparameterized using the state-space representation, which is then used to\ndetermine the parameters of the stochastic model. By leveraging the\nstochastic-differential-equation (SDE) representation of the ADS, the\nstochastic model is able to capture the underlying stochasticity of the\nunobservable contingencies. The stochastic model is then used to derive\nprobability distributions of the unobservable",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2727272727272727,
          "p": 0.32432432432432434,
          "f": 0.2962962913336382
        },
        "rouge-2": {
          "r": 0.04065040650406504,
          "p": 0.04716981132075472,
          "f": 0.043668117298297705
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.2972972972972973,
          "f": 0.27160493330894686
        }
      }
    },
    {
      "paper_id": "physics.space-ph.physics/space-ph/2503.04965v1",
      "true_abstract": "We present the first full-wavelength numerical simulations of the electric\nfield generated by cosmic ray impacts into the Moon. Billions of cosmic rays\nfall onto the Moon every year. Ultra-high energy cosmic ray impacts produce\nsecondary particle cascades within the regolith and subsequent coherent,\nwidebandwidth, linearly-polarized radio pulses by the Askaryan Effect.\nObservations of the cosmic ray particle shower radio emissions can reveal\nsubsurface structure on the Moon and enable the broad and deep prospecting\nnecessary to confirm or refute the existence of polar ice deposits. Our\nsimulations show that the radio emissions and reflections could reveal ice\nlayers as thin as 10 cm and buried under regolith as deep as 9 m. The Askaryan\nEffect presents a novel and untapped opportunity for characterizing buried\nlunar ice at unprecedented depths and spatial scales.",
      "generated_abstract": "is a fascinating laboratory for studying the subsurface, but the\nhigh-energy particles produced by cosmic rays pose a significant challenge in\naccessing the subsurface. This study presents a novel technique for detecting\nand measuring the Askaryan response (AR) in the lunar surface, using cosmic\nrays to investigate the subsurface structure and buried ice on the Moon. The\nAR is a signature of the electric field that is produced by the cosmic rays,\nwhich is sensitive to the depth and composition of the subsurface. This study\nexplores the AR as a tool for detecting and mapping buried ice in the lunar\nsubsurface, by analyzing the subsurface structure in the lunar highlands and\nsubsurface layer. By analyzing the AR data and the cosmic ray data, we\ndemonstrate the feasibility of using the AR technique to detect",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2808988764044944,
          "p": 0.36231884057971014,
          "f": 0.31645569128264706
        },
        "rouge-2": {
          "r": 0.12096774193548387,
          "p": 0.13392857142857142,
          "f": 0.12711863908072416
        },
        "rouge-l": {
          "r": 0.2696629213483146,
          "p": 0.34782608695652173,
          "f": 0.3037974634345458
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2503.02351v1",
      "true_abstract": "Concept-selective regions within the human cerebral cortex exhibit\nsignificant activation in response to specific visual stimuli associated with\nparticular concepts. Precisely localizing these regions stands as a crucial\nlong-term goal in neuroscience to grasp essential brain functions and\nmechanisms. Conventional experiment-driven approaches hinge on manually\nconstructed visual stimulus collections and corresponding brain activity\nrecordings, constraining the support and coverage of concept localization.\nAdditionally, these stimuli often consist of concept objects in unnatural\ncontexts and are potentially biased by subjective preferences, thus prompting\nconcerns about the validity and generalizability of the identified regions. To\naddress these limitations, we propose a data-driven exploration approach. By\nsynthesizing extensive brain activity recordings, we statistically localize\nvarious concept-selective regions. Our proposed MindSimulator leverages\nadvanced generative technologies to learn the probability distribution of brain\nactivity conditioned on concept-oriented visual stimuli. This enables the\ncreation of simulated brain recordings that reflect real neural response\npatterns. Using the synthetic recordings, we successfully localize several\nwell-studied concept-selective regions and validate them against empirical\nfindings, achieving promising prediction accuracy. The feasibility opens\navenues for exploring novel concept-selective regions and provides prior\nhypotheses for future neuroscience research.",
      "generated_abstract": "alization, defined as the identification of specific brain regions\nsurrounding an object, is a key component of many cognitive tasks, such as\nobject recognition and language comprehension. Current methods for brain\nlocalization rely on high-throughput functional magnetic resonance imaging\n(fMRI) data collected from brain regions during task-relevant conditions. The\nhigh cost of acquiring such data limits the feasibility of using them to\nevaluate general-purpose brain localization models. To address this limitation,\nwe introduce MindSimulator, a method that generates synthetic fMRI data by\nrandomly perturbing fMRI data and then using it for brain localization tasks.\nWe evaluate the performance of MindSimulator on the BrainLocalizer challenge,\na publicly available brain localization benchmark that includes more than 100,000\nbrain images collected during 10 different tasks. MindSim",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16176470588235295,
          "p": 0.24719101123595505,
          "f": 0.19555555077372852
        },
        "rouge-2": {
          "r": 0.005681818181818182,
          "p": 0.008620689655172414,
          "f": 0.0068493102796055244
        },
        "rouge-l": {
          "r": 0.14705882352941177,
          "p": 0.2247191011235955,
          "f": 0.17777777299595074
        }
      }
    },
    {
      "paper_id": "cs.NE.cs/NE/2503.08703v1",
      "true_abstract": "Event cameras provide superior temporal resolution, dynamic range, power\nefficiency, and pixel bandwidth. Spiking Neural Networks (SNNs) naturally\ncomplement event data through discrete spike signals, making them ideal for\nevent-based tracking. However, current approaches that combine Artificial\nNeural Networks (ANNs) and SNNs, along with suboptimal architectures,\ncompromise energy efficiency and limit tracking performance. To address these\nlimitations, we propose the first Transformer-based spike-driven tracking\npipeline. Our Global Trajectory Prompt (GTP) method effectively captures global\ntrajectory information and aggregates it with event streams into event images\nto enhance spatiotemporal representation. We then introduce SDTrack, a\nTransformer-based spike-driven tracker comprising a Spiking MetaFormer backbone\nand a simple tracking head that directly predicts normalized coordinates using\nspike signals. The framework is end-to-end, does not require data augmentation\nor post-processing. Extensive experiments demonstrate that SDTrack achieves\nstate-of-the-art performance while maintaining the lowest parameter count and\nenergy consumption across multiple event-based tracking benchmarks,\nestablishing a solid baseline for future research in the field of neuromorphic\nvision.",
      "generated_abstract": "r introduces a novel event-based tracking framework based on\nspiking neural networks (SNNs) for real-time video analysis. The proposed\nSNN-based approach is capable of tracking multiple target objects in the video\nand detecting their change in appearance over time. The framework is designed\nto integrate with existing video analytics systems, enabling real-time\ndetection of objects of interest and monitoring their evolution in a\nconcise and efficient manner. The proposed SNN-based framework is capable of\ncapturing complex motion patterns in video data, offering a robust and\ncomputationally efficient solution for real-time event-based object tracking.\nExperimental results demonstrate that the proposed framework outperforms\nconventional deep learning-based tracking methods, achieving state-of-the-art\nperformance in both tracking accuracy and runtime efficiency. The\nperformance-accuracy trade-off achieved by the proposed framework",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15267175572519084,
          "p": 0.24691358024691357,
          "f": 0.18867924056114285
        },
        "rouge-2": {
          "r": 0.03821656050955414,
          "p": 0.05217391304347826,
          "f": 0.044117642178039033
        },
        "rouge-l": {
          "r": 0.11450381679389313,
          "p": 0.18518518518518517,
          "f": 0.14150942924038817
        }
      }
    },
    {
      "paper_id": "cs.GT.cs/ET/2503.07558v1",
      "true_abstract": "We introduce the first formal model capturing the elicitation of unverifiable\ninformation from a party (the \"source\") with implicit signals derived by other\nplayers (the \"observers\"). Our model is motivated in part by applications in\ndecentralized physical infrastructure networks (a.k.a. \"DePIN\"), an emerging\napplication domain in which physical services (e.g., sensor information,\nbandwidth, or energy) are provided at least in part by untrusted and\nself-interested parties. A key challenge in these signal network applications\nis verifying the level of service that was actually provided by network\nparticipants.\n  We first establish a condition called source identifiability, which we show\nis necessary for the existence of a mechanism for which truthful signal\nreporting is a strict equilibrium. For a converse, we build on techniques from\npeer prediction to show that in every signal network that satisfies the source\nidentifiability condition, there is in fact a strictly truthful mechanism,\nwhere truthful signal reporting gives strictly higher total expected payoff\nthan any less informative equilibrium. We furthermore show that this truthful\nequilibrium is in fact the unique equilibrium of the mechanism if there is\npositive probability that any one observer is unconditionally honest (e.g., if\nan observer were run by the network owner). Also, by extending our condition to\ncoalitions, we show that there are generally no collusion-resistant mechanisms\nin the settings that we consider.\n  We apply our framework and results to two DePIN applications: proving\nlocation, and proving bandwidth. In the location-proving setting observers\nlearn (potentially enlarged) Euclidean distances to the source. Here, our\ncondition has an appealing geometric interpretation, implying that the source's\nlocation can be truthfully elicited if and only if it is guaranteed to lie\ninside the convex hull of the observers.",
      "generated_abstract": "er the problem of recovering the value of a function from a\nmanipulated signal, where the manipulator can control the value of the\nsignals. We propose an incentive-compatible framework for this problem, which\nis based on the notion of a \\emph{decentralized incentive compatible\nconstrained auction}, where the value of the function is auctioned off to\nseveral agents, each of which has the option to collude with the manipulator\nto set the value of the signal arbitrarily. We show that, under suitable\nassumptions, the value of the function can be recovered with high probability\nusing a single signal, if the manipulator follows an incentive-compatible\nauction. We also show that, under similar assumptions, the value of the\nfunction can be recovered by the agents, using only a single signal, if the\nmanipulator follows a deceptive incentive-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14450867052023122,
          "p": 0.373134328358209,
          "f": 0.20833332930868062
        },
        "rouge-2": {
          "r": 0.01845018450184502,
          "p": 0.05,
          "f": 0.026954173959794537
        },
        "rouge-l": {
          "r": 0.1329479768786127,
          "p": 0.34328358208955223,
          "f": 0.19166666264201396
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.19002v1",
      "true_abstract": "This paper studies the stochastic setting in cooperative games and suggests a\nsolution concept based on second order stochastic dominance (SSD), which is\noften applied to robustly model risk averse behaviour of players in different\neconomic and game theoretic models as it enables to model not specified levels\nof risk aversion among players. The main result of the paper connects this\nsolution concept, \\emph{SSD-core}, in case of uniform distribution of the game\nto cores of two deterministic cooperative games. Interestingly, balancedness of\nboth of these games and convexity of one of these implies non-emptiness of the\nSSD-core. The opposite implication does not, in general, hold and leads to\nquestions about intersections of cores of two games and their relations.\nFinally, we present an application of the SSD-core to the multiple newsvendors\nproblem, where we provide a characterization of risk averse behaviour of\nplayers with an interpretation in terms of the model.",
      "generated_abstract": "r considers a stochastic cooperative game of risk-averse players\nplaying against a single, risk-neutral, newsvendor. The newsvendor has an\ninfinite portfolio of products, and the players' payoffs depend on the\nproducts' current market shares. We derive a general formulation of the\ngame, and show that it is equivalent to a stochastic Nash equilibrium. We\nconsider three types of newsvendor's strategies: (i) a fixed price strategy,\n(ii) a dynamic price strategy, and (iii) a dynamic price strategy with a\nfixed price strategy. We derive closed-form expressions for the payoff of the\nplayers and the newsvendor in each case. Finally, we discuss the implications\nof the newsvendor's strategies on the players' strategies. Our analysis shows\nthat the players' strategies depend on the market shares and the new",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20430107526881722,
          "p": 0.2602739726027397,
          "f": 0.228915657723182
        },
        "rouge-2": {
          "r": 0.014925373134328358,
          "p": 0.01904761904761905,
          "f": 0.016736396747257274
        },
        "rouge-l": {
          "r": 0.1827956989247312,
          "p": 0.2328767123287671,
          "f": 0.2048192721810133
        }
      }
    },
    {
      "paper_id": "cs.LG.econ/GN/2501.06248v2",
      "true_abstract": "Current methods that train large language models (LLMs) with reinforcement\nlearning feedback, often resort to averaging outputs of multiple rewards\nfunctions during training. This overlooks crucial aspects of individual reward\ndimensions and inter-reward dependencies that can lead to sub-optimal outcomes\nin generations. In this work, we show how linear aggregation of rewards\nexhibits some vulnerabilities that can lead to undesired properties of\ngenerated text. We then propose a transformation of reward functions inspired\nby economic theory of utility functions (specifically Inada conditions), that\nenhances sensitivity to low reward values while diminishing sensitivity to\nalready high values. We compare our approach to the existing baseline methods\nthat linearly aggregate rewards and show how the Inada-inspired reward feedback\nis superior to traditional weighted averaging. We quantitatively and\nqualitatively analyse the difference in the methods, and see that models\ntrained with Inada-transformations score as more helpful while being less\nharmful.",
      "generated_abstract": "models (LMs) are designed to generate natural language output,\nwith the ultimate goal of learning complex vocabularies and generating\nsatisfying responses to prompts. While current approaches for training LMs\noften rely on a large number of training data, they often struggle with\nachieving good generalization and robustness to unseen inputs, resulting in\npoor translation capabilities. We propose a new training framework for LMs,\nwhich integrates reward transformations into the learning process, enabling\nrobust training with limited data. We introduce the Reward-Transformed\nLanguage Model (RTLM) framework, which uses reward-transformed language models\n(RewardLMs) to improve the training of LMs. The RewardLM framework\nautomatically generates reward transformations for LMs, enabling them to\nefficiently learn complex tasks. Additionally, we demonstrate the effectiveness\nof the Reward",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16346153846153846,
          "p": 0.19767441860465115,
          "f": 0.1789473634659281
        },
        "rouge-2": {
          "r": 0.014285714285714285,
          "p": 0.01694915254237288,
          "f": 0.015503871005349824
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.18604651162790697,
          "f": 0.16842104767645444
        }
      }
    },
    {
      "paper_id": "cs.CC.cs/CC/2503.10393v1",
      "true_abstract": "Oredango puzzle, one of the pencil puzzles, was originally created by\nKanaiboshi and published in the popular puzzle magazine Nikoli. In this paper,\nwe show NP- and ASP-completeness of Oredango by constructing a reduction from\nthe 1-in-3SAT problem. Next, we formulate Oredango as an 0-1\ninteger-programming problem, and present numerical results obtained by solving\nOredango puzzles from Nikoli and PuzzleSquare JP using a 0-1 optimization\nsolver.",
      "generated_abstract": "ngo puzzle is a combinatorial optimization problem that seeks to\nmake a list of items in the order of their weight. In the special case where\neach item is weighted 1, it can be reduced to a well-known integer program,\nwhich has been extensively studied. In this paper, we present a new\nreduction of the Oredango puzzle to an integer program. We show that the\ninteger program can be solved in polynomial time, and that the complexity of\nthe integer program is $\\Omega(n^2)$ with respect to the number of items in the\nlist. We then describe a novel reduction of the Oredango puzzle to a\ncombinatorial optimization problem, and show that this problem is also\napproximation-hard. We describe a polynomial-time reduction from the\ncombinatorial optimization problem to the integer program, and show that the\ncomplexity of the integer program is $\\Omega(",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.38,
          "p": 0.27941176470588236,
          "f": 0.3220338934214307
        },
        "rouge-2": {
          "r": 0.12307692307692308,
          "p": 0.07476635514018691,
          "f": 0.09302325111208785
        },
        "rouge-l": {
          "r": 0.34,
          "p": 0.25,
          "f": 0.2881355883366849
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/MF/2502.17417v1",
      "true_abstract": "In this paper, we propose an event-driven Limit Order Book (LOB) model that\ncaptures twelve of the most observed LOB events in exchange-based financial\nmarkets. To model these events, we propose using the state-of-the-art Neural\nHawkes process, a more robust alternative to traditional Hawkes process models.\nMore specifically, this model captures the dynamic relationships between\ndifferent event types, particularly their long- and short-term interactions,\nusing a Long Short-Term Memory neural network. Using this framework, we\nconstruct a midprice process that captures the event-driven behavior of the LOB\nby simulating high-frequency dynamics like how they appear in real financial\nmarkets. The empirical results show that our model captures many of the broader\ncharacteristics of the price fluctuations, particularly in terms of their\noverall volatility. We apply this LOB simulation model within a Deep\nReinforcement Learning Market-Making framework, where the trading agent can now\ncomplete trade order fills in a manner that closely resembles real-market trade\nexecution. Here, we also compare the results of the simulated model with those\nfrom real data, highlighting how the overall performance and the distribution\nof trade order fills closely align with the same analysis on real data.",
      "generated_abstract": "e a novel limit order book (LOB) simulator that models the event-based\nliquidity dynamics of financial markets using a neural Hawkes process. By\nintegrating the Hawkes process into a standard LOB simulation framework, our\napproach allows for real-time, event-driven simulations of LOBs, enabling\ntraders to adjust their strategies in real-time to maximize liquidity. Our\napproach offers a more flexible and scalable framework for simulating LOBs,\nenabling traders to adjust their strategies in real-time to maximize liquidity.\nOur simulator can handle high-frequency trading (HFT) and market-making\nactivities. By integrating the Hawkes process into a standard LOB simulation\nframework, our approach allows for real-time, event-driven simulations of LOBs,\nenabling traders to adjust their strategies in real",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.232,
          "p": 0.47540983606557374,
          "f": 0.3118279525812233
        },
        "rouge-2": {
          "r": 0.0335195530726257,
          "p": 0.08108108108108109,
          "f": 0.047430825900733074
        },
        "rouge-l": {
          "r": 0.208,
          "p": 0.4262295081967213,
          "f": 0.27956988806509425
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/MM/2503.07259v1",
      "true_abstract": "Egocentric video-based models capture rich semantic information and have\ndemonstrated strong performance in human activity recognition (HAR). However,\ntheir high power consumption, privacy concerns, and dependence on lighting\nconditions limit their feasibility for continuous on-device recognition. In\ncontrast, inertial measurement unit (IMU) sensors offer an energy-efficient and\nprivacy-preserving alternative, yet they suffer from limited large-scale\nannotated datasets, leading to weaker generalization in downstream tasks. To\nbridge this gap, we propose COMODO, a cross-modal self-supervised distillation\nframework that transfers rich semantic knowledge from the video modality to the\nIMU modality without requiring labeled annotations. COMODO leverages a\npretrained and frozen video encoder to construct a dynamic instance queue,\naligning the feature distributions of video and IMU embeddings. By distilling\nknowledge from video representations, our approach enables the IMU encoder to\ninherit rich semantic information from video while preserving its efficiency\nfor real-world applications. Experiments on multiple egocentric HAR datasets\ndemonstrate that COMODO consistently improves downstream classification\nperformance, achieving results comparable to or exceeding fully supervised\nfine-tuned models. Moreover, COMODO exhibits strong cross-dataset\ngeneralization. Benefiting from its simplicity, our method is also generally\napplicable to various video and time-series pre-trained models, offering the\npotential to leverage more powerful teacher and student foundation models in\nfuture research. The code is available at https://github.com/Breezelled/COMODO .",
      "generated_abstract": "r presents COMODO, a novel framework that leverages cross-modal\nlanguages to enhance egocentric human activity recognition (HER). Traditional\nHER approaches typically rely on ground-truth human activity labels to train\nmodels, limiting the ability to generalize to new tasks. COMODO extends the\nlearning process by introducing a cross-modal video-to-IMU distillation\ntask, which enables the model to capture complex temporal and spatial\nrelationships within a single video. The distilled video representations are\nthen used for subsequent classification tasks. Experiments on the UCSD-HAR\nbenchmark demonstrate that COMODO outperforms state-of-the-art methods,\nespecially in unseen tasks, achieving a 100% accuracy on the test set. The\nsource code and datasets are available at\nhttps://github.com/Mohammad-A-Ali",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21019108280254778,
          "p": 0.38372093023255816,
          "f": 0.2716049336984539
        },
        "rouge-2": {
          "r": 0.054455445544554455,
          "p": 0.10377358490566038,
          "f": 0.07142856691431973
        },
        "rouge-l": {
          "r": 0.19745222929936307,
          "p": 0.36046511627906974,
          "f": 0.2551440283486597
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.05766v1",
      "true_abstract": "Audio-visual representation learning is crucial for advancing multimodal\nspeech processing tasks, such as lipreading and audio-visual speech\nrecognition. Recently, speech foundation models (SFMs) have shown remarkable\ngeneralization capabilities across various speech-related tasks. Building on\nthis progress, we propose an audio-visual representation learning model that\nleverages cross-modal knowledge distillation from SFMs. In our method, SFMs\nserve as teachers, from which multi-layer hidden representations are extracted\nusing clean audio inputs. We also introduce a multi-teacher ensemble method to\ndistill the student, which receives audio-visual data as inputs. A novel\nrepresentational knowledge distillation loss is employed to train the student\nduring pretraining, which is also applied during finetuning to further enhance\nthe performance on downstream tasks. Our experiments utilized both a\nself-supervised SFM, WavLM, and a supervised SFM, iFLYTEK-speech. The results\ndemonstrated that our proposed method achieved superior or at least comparable\nperformance to previous state-of-the-art baselines across automatic speech\nrecognition, visual speech recognition, and audio-visual speech recognition\ntasks. Additionally, comprehensive ablation studies and the visualization of\nlearned representations were conducted to evaluate the effectiveness of our\nproposed method.",
      "generated_abstract": "ech foundation models, such as GPT-3, have demonstrated strong\ngenerative ability, but their ability to represent audio and visual\ninformation remains under-explored. To address this limitation, we propose\na novel knowledge distillation framework for audio-visual representation learning\nfrom speech foundation models. Specifically, we distill a speech foundation model\nwith large-scale visual recognition capability, such as ImageNet or VQ-VAE, into\na small-scale audio-visual recognition model, such as VQ-VAE. To enhance\nvisual recognition performance, we propose a visual-to-audio feature\nrepresentation learning scheme. Specifically, we learn a visual-to-audio\nfeature representation using a visual recognition model. Then, we distill the\nvisual-to-audio feature representation into a small-scale audio-visual\nrecognition model, which can be directly applied to generate visual representations\nof audio. Extensive experiments on",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2903225806451613,
          "p": 0.4864864864864865,
          "f": 0.3636363589552087
        },
        "rouge-2": {
          "r": 0.04790419161676647,
          "p": 0.08163265306122448,
          "f": 0.060377353829548244
        },
        "rouge-l": {
          "r": 0.28225806451612906,
          "p": 0.47297297297297297,
          "f": 0.35353534885419863
        }
      }
    },
    {
      "paper_id": "stat.ME.q-bio/SC/2408.17188v2",
      "true_abstract": "We introduce a generalized promotion time cure model motivated by a new\nbiological consideration. The new approach is flexible to model heterogeneous\nsurvival data, in particular for addressing intra-sample heterogeneity. We also\nindicate that the new approach is suited to model a series or parallel system\nconsisting of multiple subsystems in reliability analysis.",
      "generated_abstract": "e a formal characterization of the promotion time cure model, a\nmodel where an infectious disease agent evolves according to a time cure\nmodel. This model was introduced by Sarkar and Waugh (2017) in the context of\ntheir study of the dynamics of the HIV epidemic. We prove that the promotion\ntime cure model is equivalent to a model with a new biological assumption,\nwhich is the same as the one used by Sarkar and Waugh (2017). This new\nbiological assumption is a novel mechanism to model the natural selection of\na pathogen population that is not subject to the standard HIV model. We also\npropose an extension of the model with the new biological assumption to the\ncase of a finite number of agents. We prove that the extension of the model\nwith the new biological assumption is a model with a new biological\nconsider",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.38095238095238093,
          "p": 0.24242424242424243,
          "f": 0.2962962915432099
        },
        "rouge-2": {
          "r": 0.1836734693877551,
          "p": 0.08411214953271028,
          "f": 0.11538461107577268
        },
        "rouge-l": {
          "r": 0.38095238095238093,
          "p": 0.24242424242424243,
          "f": 0.2962962915432099
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/CV/2503.10635v1",
      "true_abstract": "Despite promising performance on open-source large vision-language models\n(LVLMs), transfer-based targeted attacks often fail against black-box\ncommercial LVLMs. Analyzing failed adversarial perturbations reveals that the\nlearned perturbations typically originate from a uniform distribution and lack\nclear semantic details, resulting in unintended responses. This critical\nabsence of semantic information leads commercial LVLMs to either ignore the\nperturbation entirely or misinterpret its embedded semantics, thereby causing\nthe attack to fail. To overcome these issues, we notice that identifying core\nsemantic objects is a key objective for models trained with various datasets\nand methodologies. This insight motivates our approach that refines semantic\nclarity by encoding explicit semantic details within local regions, thus\nensuring interoperability and capturing finer-grained features, and by\nconcentrating modifications on semantically rich areas rather than applying\nthem uniformly. To achieve this, we propose a simple yet highly effective\nsolution: at each optimization step, the adversarial image is cropped randomly\nby a controlled aspect ratio and scale, resized, and then aligned with the\ntarget image in the embedding space. Experimental results confirm our\nhypothesis. Our adversarial examples crafted with local-aggregated\nperturbations focused on crucial regions exhibit surprisingly good\ntransferability to commercial LVLMs, including GPT-4.5, GPT-4o,\nGemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning\nmodels like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach\nachieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly\noutperforming all prior state-of-the-art attack methods. Our optimized\nadversarial examples under different configurations and training code are\navailable at https://github.com/VILA-Lab/M-Attack.",
      "generated_abstract": "vancements in Large Language Models (LLMs) have made it possible to\ngenerate high-quality text with great variability. However, the models are\noften vulnerable to adversarial attacks, which can be exploited to alter the\ngenerated text and mislead humans. To overcome these challenges, we propose a\nsimple yet highly effective baseline that generates text in the style of the\nGPT-4o/o1 model, and we demonstrate that it achieves a success rate of 91.9%\nagainst the strong black-box models of GPT-4.5/4o/o1. This baseline shows\nconsiderable effectiveness, despite not having any pre-trained language model\nor specialized features. We also demonstrate that it is effective even when the\nmodel is not trained on the target text, and the model is not pre-trained on\nthe target",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1783783783783784,
          "p": 0.38372093023255816,
          "f": 0.24354243109162466
        },
        "rouge-2": {
          "r": 0.0411522633744856,
          "p": 0.08771929824561403,
          "f": 0.056022404616435144
        },
        "rouge-l": {
          "r": 0.16756756756756758,
          "p": 0.36046511627906974,
          "f": 0.2287822834901486
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/ST/2501.16659v1",
      "true_abstract": "Considering the continuous-time Mean-Variance (MV) portfolio optimization\nproblem, we study a regime-switching market setting and apply reinforcement\nlearning (RL) techniques to assist informed exploration within the control\nspace. We introduce and solve the Exploratory Mean Variance with Regime\nSwitching (EMVRS) problem. We also present a Policy Improvement Theorem.\nFurther, we recognize that the widely applied Temporal Difference (TD) learning\nis not adequate for the EMVRS context, hence we consider Orthogonality\nCondition (OC) learning, leveraging the martingale property of the induced\noptimal value function from the analytical solution to EMVRS. We design a RL\nalgorithm that has more meaningful parameterization using the market parameters\nand propose an updating scheme for each parameter. Our empirical results\ndemonstrate the superiority of OC learning over TD learning with a clear\nconvergence of the market parameters towards their corresponding ``grounding\ntrue\" values in a simulated market scenario. In a real market data study, EMVRS\nwith OC learning outperforms its counterparts with the highest mean and\nreasonably low volatility of the annualized portfolio returns.",
      "generated_abstract": "uce a novel mean-variance portfolio optimization problem for\ndynamically changing market regimes, where the market dynamics are modeled as\nan Ornstein-Uhlenbeck process. We consider a two-asset portfolio, with the\nlonger-term asset held as a long position and the short-term asset as a short\nposition. The long-term asset is modeled as a mean-variance portfolio with a\nfixed risk-free interest rate, while the short-term asset is modeled as a\nmean-variance portfolio with a risk-free interest rate that evolves with the\nlong-term asset. We derive the optimal strategies in the mean-variance framework\nand show that the mean-variance objective is convex with respect to the\nportfolio weights. We then extend the portfolio optimization problem to the\ncase of multiple assets, where the dynamics of the market are modeled as a\nMark",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14754098360655737,
          "p": 0.2857142857142857,
          "f": 0.1945945901031411
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.06060606060606061,
          "f": 0.04597700678557321
        },
        "rouge-l": {
          "r": 0.13934426229508196,
          "p": 0.2698412698412698,
          "f": 0.18378377929233028
        }
      }
    },
    {
      "paper_id": "math.SP.math/SP/2502.18307v1",
      "true_abstract": "The notion of eta invariant is traditionally defined by means of analytic\ncontinuation. We prove, by examining the particular case of the operator curl,\nthat the eta invariant can equivalently be obtained as the trace of the\ndifference of positive and negative spectral projections, appropriately\nregularised. Our construction is direct, in the sense that it does not involve\nanalytic continuation, and is based on the use of pseudodifferential\ntechniques. This provides a novel approach to the study of spectral asymmetry\nof non-semibounded (pseudo)differential systems on manifolds which encompasses\nand extends previous results.",
      "generated_abstract": "We establish a pathway from microlocal analysis to spectral geometry. We\nshow that the operator $L_\\varepsilon$ acting on $L^2(\\mathbb{R}^2)$ with\nspectral parameter $\\varepsilon\\in(0,\\infty)$ is the Dirichlet Laplacian on\n$\\mathbb{R}^2$ with Dirichlet data at the origin. We also show that the\nDirichlet Laplacian $L_\\varepsilon$ is the same as the Curl operator on the\nunit ball of $\\mathbb{R}^2$. We then extend these results to the case of\n$L^p(\\mathbb{R}^2)$ with $1<p<2$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19402985074626866,
          "p": 0.30952380952380953,
          "f": 0.23853210535476818
        },
        "rouge-2": {
          "r": 0.06741573033707865,
          "p": 0.0967741935483871,
          "f": 0.0794701938353584
        },
        "rouge-l": {
          "r": 0.16417910447761194,
          "p": 0.2619047619047619,
          "f": 0.20183485764834624
        }
      }
    },
    {
      "paper_id": "cs.OS.cs/OS/2502.10923v2",
      "true_abstract": "The emergence of symmetric multi-processing (SMP) systems with non-uniform\nmemory access (NUMA) has prompted extensive research on process and data\nplacement to mitigate the performance impact of NUMA on applications. However,\nexisting solutions often overlook the coordination between the CPU scheduler\nand memory manager, leading to inefficient thread and page table placement.\nMoreover, replication techniques employed to improve locality suffer from\nredundant replicas, scalability barriers, and performance degradation due to\nmemory bandwidth and inter-socket interference. In this paper, we present\nPhoenix, a novel integrated CPU scheduler and memory manager with on-demand\npage table replication mechanism. Phoenix integrates the CPU scheduler and\nmemory management subsystems, allowing for coordinated thread and page table\nplacement. By differentiating between data and page table pages, Phoenix\nenables direct migration or replication of page tables based on application\nbehavior. Additionally, Phoenix employs memory bandwidth management mechanism\nto maintain Quality of Service (QoS) while mitigating coherency maintenance\noverhead. We implemented Phoenix as a loadable kernel module for Linux,\nensuring compatibility with legacy applications and ease of deployment. Our\nevaluation on real hardware demonstrates that Phoenix reduces CPU cycles by\n2.09x and page-walk cycles by 1.58x compared to state-of-the-art solutions.",
      "generated_abstract": "architecture is a fundamental component of modern computer\nsystems, enabling the distribution of processing power among multiple\nprocessors. However, the NUMA architecture also introduces challenges in\nperforming effective memory allocation and page table placement, especially in\nthe presence of inter-NUMA communication, which is often unpredictable.\nAlthough many techniques have been proposed to address these challenges,\nthese techniques often rely on either static or non-dynamic memory\nallocation schemes. In this paper, we present Phoenix, a new technique that\nuses dynamic memory allocation to achieve performance-aware thread and page\ntable placement in NUMA systems. Phoenix is based on the concept of\nresource-aware thread allocation, which is a novel technique for performing\ndynamic memory allocation based on the current workload. In this technique,\ninstead of allocating all threads to a specific processor, the current workload\nis partitioned into a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21705426356589147,
          "p": 0.3146067415730337,
          "f": 0.2568807291132902
        },
        "rouge-2": {
          "r": 0.06779661016949153,
          "p": 0.09836065573770492,
          "f": 0.08026755369760995
        },
        "rouge-l": {
          "r": 0.20155038759689922,
          "p": 0.29213483146067415,
          "f": 0.23853210526007923
        }
      }
    },
    {
      "paper_id": "math.CV.math/OA/2503.09112v1",
      "true_abstract": "In this paper, we provide a complete characterization of bounded Toeplitz\noperators $T_f$ on the harmonic Bergman space of the unit disk, where the\nsymbol $f$ has a polar decomposition truncated above, that commute with\n$T_{z+\\bar{g}}$, for a bounded analytic function $g$.",
      "generated_abstract": "In this paper, we investigate the commuting problem of Toeplitz operators on\nthe harmonic Bergman space. We prove the uniqueness of commuting Toeplitz\noperators and their commuting symbols, and give some examples of commuting\nToeplitz operators on the harmonic Bergman space.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3333333333333333,
          "p": 0.5,
          "f": 0.39999999520000007
        },
        "rouge-2": {
          "r": 0.1951219512195122,
          "p": 0.25806451612903225,
          "f": 0.22222221731867295
        },
        "rouge-l": {
          "r": 0.3333333333333333,
          "p": 0.5,
          "f": 0.39999999520000007
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.cond-mat/dis-nn/2503.09457v1",
      "true_abstract": "The effective conductivity ($T^{eff}$) of 2D and 3D Random Resistor Networks\n(RRNs) with random edge conductivity is studied. The combined influence of\ngeometrical disorder, which controls the overall connectivity of the medium and\nleads to percolation effects, and conductivity randomness is investigated. A\nformula incorporating connectivity aspects and second-order averaging methods,\nwidely used in the stochastic hydrology community, is derived and extrapolated\nto higher orders using a power averaging formula based on a mean-field\nargument. This approach highlights the role of the so-called resistance\ndistance introduced by graph theorists. Simulations are performed on various\nRRN geometries constructed from 2D and 3D bond-percolation lattices. The\nresults confirm the robustness of the power averaging technique and the\nrelevance of the mean-field assumption.",
      "generated_abstract": "etworks are the simplest models of nonlinear transport and they\nhave been widely studied in condensed matter physics and engineering. These\nnetworks consist of parallel conducting wires and may be described by the\nHamiltonian $H = H_{\\rm int} + H_{\\rm ext}$, where $H_{\\rm int}$ describes\ninter-wire interactions and $H_{\\rm ext}$ the external fields. Here, we study\nthe effect of random conductivities on the conductivity of conduit networks. We\nshow that random conductivities can alter the conductivity of conduit\nnetworks in a way that is similar to what happens in nonlinear conductance\nmodulation (NCM) of transmission lines, where random conductivities can\nmodify the conductivity of transmission lines. We show that the effect of\nrandom conductivities on the conductivity of conduit networks can be\nquantified using the effective conductivity. We demonstrate that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19047619047619047,
          "p": 0.2077922077922078,
          "f": 0.1987577589846072
        },
        "rouge-2": {
          "r": 0.008849557522123894,
          "p": 0.009345794392523364,
          "f": 0.009090904094630843
        },
        "rouge-l": {
          "r": 0.15476190476190477,
          "p": 0.16883116883116883,
          "f": 0.16149067823926563
        }
      }
    },
    {
      "paper_id": "math.DG.math/AG/2503.10517v1",
      "true_abstract": "We consider the group $\\mathcal G$ which is the semidirect product of the\ngroup of analytic functions with values in ${\\mathbb C}^*$ on the circle and\nthe group of analytic diffeomorphisms of the circle that preserve the\norientation. Then we construct the central extensions of the group $\\mathcal G$\nby the group ${\\mathbb C}^*$. The first central extension, so-called the\ndeterminant central extension, is constructed by means of determinants of\nlinear operators acting in infinite-dimensional locally convex topological\n$\\mathbb C$-vector spaces. Other central extensions are constructed by\n$\\cup$-products of group $1$-cocycles with the application to them the map\nrelated with algebraic $K$-theory. We prove in the second cohomology group,\ni.e. modulo of a group $2$-coboundary, the equality of the $12$th power of the\n$2$-cocycle constructed by the first central extension and the product of\ninteger powers of the $2$-cocycles constructed above by means of\n$\\cup$-products (in multiplicative notation). As an application of this result\nwe obtain the new topological Riemann-Roch theorem for a complex line bundle\n$L$ on a smooth manifold $M$, where $\\pi :M \\to B$ is a fibration in oriented\ncircles. More precisely, we prove that in the group $H^3(B, {\\mathbb Z})$ the\nelement $12 \\, [ {\\mathcal Det} (L)]$ is equal to the element $6 \\, \\pi_* (\nc_1(L) \\cup c_1(L))$, where $[{\\mathcal Det} (L)]$ is the class of the\ndeterminant gerbe on $B$ constructed by $L$ and the determinant central\nextension.",
      "generated_abstract": "We study analytic diffeomorphisms of the circle and the relationship to\ntopological Riemann-Roch theory. We prove that the circle is the only\nnon-orientable compact surface of genus one for which there is a smooth\n$1$-parameter family of analytic diffeomorphisms that are topologically\nRiemann-Roch equivalent to the identity. We also prove that the circle is the\nonly compact surface of genus one for which the identity is topologically\nRiemann-Roch equivalent to a homeomorphism with a unique fixed point.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1484375,
          "p": 0.48717948717948717,
          "f": 0.22754490659973467
        },
        "rouge-2": {
          "r": 0.07106598984771574,
          "p": 0.2413793103448276,
          "f": 0.10980391805428694
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.41025641025641024,
          "f": 0.19161676288715987
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/OT/2407.11518v1",
      "true_abstract": "In this paper, we present a new ensemble-based filter method by\nreconstructing the analysis step of the particle filter through a transport\nmap, which directly transports prior particles to posterior particles. The\ntransport map is constructed through an optimization problem described by the\nMaximum Mean Discrepancy loss function, which matches the expectation\ninformation of the approximated posterior and reference posterior. The proposed\nmethod inherits the accurate estimation of the posterior distribution from\nparticle filtering. To improve the robustness of Maximum Mean Discrepancy, a\nvariance penalty term is used to guide the optimization. It prioritizes\nminimizing the discrepancy between the expectations of highly informative\nstatistics for the approximated and reference posteriors. The penalty term\nsignificantly enhances the robustness of the proposed method and leads to a\nbetter approximation of the posterior. A few numerical examples are presented\nto illustrate the advantage of the proposed method over the ensemble Kalman\nfilter.",
      "generated_abstract": "Ensemble Transport Filter (ETF) is a popular transport-based filter that\ncan reduce computational costs while maintaining excellent performance. However,\nthe computational complexity of ETF is prohibitive for large-scale problems.\nIn this paper, we propose a novel ensemble transport filter method based on\nmaximum mean discrepancy (MMD) distance. This method is effective and\nefficient, reducing computational costs by a factor of 2.4 compared to\ntraditional ETF. We also demonstrate the robustness of our proposed method by\nanalyzing various filter parameters. Furthermore, we propose a method to\nimprove the efficiency of our proposed method by reducing the number of\ntransmissions and the number of training data samples.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2247191011235955,
          "p": 0.2777777777777778,
          "f": 0.24844720002469048
        },
        "rouge-2": {
          "r": 0.06766917293233082,
          "p": 0.09375,
          "f": 0.07860261521786419
        },
        "rouge-l": {
          "r": 0.2247191011235955,
          "p": 0.2777777777777778,
          "f": 0.24844720002469048
        }
      }
    },
    {
      "paper_id": "hep-th.gr-qc/2503.10598v1",
      "true_abstract": "The basic observables in cosmology are known as in-in correlators. Recent\ncalculations have revealed that in-in correlators in four dimensional de Sitter\nspace exhibit hidden simplicity stemming from a close relation to scattering\namplitudes in flat space. In this paper we explain how to make this property\nmanifest by dressing flat space Feynman diagrams with certain auxiliary\npropagators. These dressing rules hold for any order in perturbation theory and\ncan be derived for a broad range of scalar theories, including those with\ninfrared divergences. In the latter case we show that they reproduce the same\ninfrared divergences predicted by the Schwinger-Keldysh formalism.",
      "generated_abstract": "In this paper, we extend the formalism of the cosmic string theory to the\ngauge theory level. We show that, in the case of the open string sector, the\ncosmological dynamics of the open string field can be completely described by\nthe dressing rules of the closed string sector. As a result, the closed string\ndressing rules, in principle, can be determined from the cosmological\ndynamics of the open string sector. We further investigate the cosmic string\ndressing rules and discuss their implications for the emergence of the cosmic\nstring spectrum in the early Universe.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.39215686274509803,
          "f": 0.3053435066954141
        },
        "rouge-2": {
          "r": 0.04081632653061224,
          "p": 0.056338028169014086,
          "f": 0.04733727323413096
        },
        "rouge-l": {
          "r": 0.2125,
          "p": 0.3333333333333333,
          "f": 0.25954197997785683
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2501.03269v1",
      "true_abstract": "This study investigates the resilience of Environmental, Social, and\nGovernance (ESG) investments during periods of financial instability, comparing\nthem with traditional equity indices across major European markets-Germany,\nFrance, and Italy. Using daily returns from October 2021 to February 2024, the\nanalysis explores the effects of key global disruptions such as the Covid-19\npandemic and the Russia-Ukraine conflict on market performance. A mixture of\ntwo generalised normal distributions (MGND) and EGARCH-in-mean models are used\nto identify periods of market turmoil and assess volatility dynamics. The\nfindings indicate that during crises, ESG investments present higher volatility\nin Germany and Italy than in France. Despite some regional variations, ESG\nportfolios demonstrate greater resilience compared to traditional ones,\noffering potential risk mitigation during market shocks. These results\nunderscore the importance of integrating ESG factors into long-term investment\nstrategies, particularly in the face of unpredictable financial turmoil.",
      "generated_abstract": "This paper evaluates the resilience of European equities in the context of\nsurviving crises. The study explores the impact of COVID-19 and Russia's\ninvasion of Ukraine on European equities. The research analyzes the stock\nreturns of the most influential ESG-focused funds during the pandemic and the\nUkraine crisis, as well as their post-crisis performance. The results show that\nthe ESG-focused funds were more resilient than the benchmark during the\npandemic and had higher post-crisis returns. The study also examines the\nimpact of the war on Ukraine on ESG-focused funds. The results indicate that\nthe ESG-focused funds are less sensitive to the impact of the war on Ukraine\nthan the benchmark.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21495327102803738,
          "p": 0.4107142857142857,
          "f": 0.28220858444653546
        },
        "rouge-2": {
          "r": 0.050359712230215826,
          "p": 0.08139534883720931,
          "f": 0.06222221749965469
        },
        "rouge-l": {
          "r": 0.18691588785046728,
          "p": 0.35714285714285715,
          "f": 0.24539876849561523
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/SC/2503.00965v1",
      "true_abstract": "G protein-coupled receptors (GPCRs) represent a diverse and vital family of\nmembrane proteins that mediate intracellular signaling in response to\nextracellular stimuli, playing critical roles in physiology and disease.\nTraditionally recognized as chemical signal transducers, GPCRs have recently\nbeen implicated in mechanotransduction, the process of converting mechanical\nstimuli into cellular responses. This review explores the emerging role of\nGPCRs in sensing and responding to mechanical forces, with a particular focus\non the cardiovascular system. Cardiovascular homeostasis is heavily influenced\nby mechanical forces such as shear stress, cyclic stretch, and pressure, which\nare central to both normal physiology and the pathogenesis of diseases like\nhypertension and atherosclerosis. GPCRs, including the angiotensin II type 1\nreceptor (AT1R) and the $\\beta$2-adrenergic receptor ($\\beta$2-AR), have\ndemonstrated the ability to integrate mechanical and chemical signals,\npotentially through conformational changes and/or modulation of lipid\ninteractions, leading to biased signaling. Recent studies highlight the dual\nactivation mechanisms of GPCRs, with $\\beta$2-AR now serving as a key example\nof how mechanical and ligand-dependent pathways contribute to cardiovascular\nregulation. This review synthesizes current knowledge of GPCR\nmechanosensitivity, emphasizing its implications for cardiovascular health and\ndisease, and explores advancements in methodologies poised to further unravel\nthe mechanistic intricacies of these receptors.",
      "generated_abstract": "nsitive ion channels (MSICs) are important cellular channels that\nare regulated by mechanical forces, which are of crucial importance for\ncellular function. However, the mechanosensitive behaviors of MSICs remain\nunder-explored. In this study, we utilized a combination of two-color\noptical tweezers, atomic force microscopy, and time-lapse imaging to investigate\nthe behaviors of the G protein-coupled receptor for the growth hormone (GHR)\nunder mechanical stimulation. Our results show that the GHR is able to\nundergo new behaviors, including a squeezing-like response, when subjected to\nmechanical stimulation. These behaviors are consistent with the previously\nreported ion channel-like behaviors of GHR and are also consistent with the\nmechanical sensing of GHR that was reported previously. Our",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13986013986013987,
          "p": 0.2702702702702703,
          "f": 0.18433179274055522
        },
        "rouge-2": {
          "r": 0.025510204081632654,
          "p": 0.04716981132075472,
          "f": 0.03311257822551705
        },
        "rouge-l": {
          "r": 0.11188811188811189,
          "p": 0.21621621621621623,
          "f": 0.14746543329355066
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.06769v1",
      "true_abstract": "This paper proposes an innovative solution to the growing issue of greenhouse\ngas emissions: a closed photobioreactor (PBR) fa\\c{c}ade system to mitigate\ngreenhouse gas (GHG) concentrations. With digital fabrication technology, this\nstudy explores the transition from traditional, single function building\nfacades to multifunctional, integrated building systems. It introduces a\nphotobioreactor (PBR) fa\\c{c}ade system to mitigate greenhouse gas (GHG)\nconcentrations while addressing the challenge of large-scale prefabricated\ncomponents transportation. This research introduces a novel approach by\ndesigning the fa\\c{c}ade system as modular, user-friendly and\ntransportation-friendly bricks, enabling the creation of a user-customized and\nself-assembled photobioreactor (PBR) system. The single module in the system is\nproposed to be \"neutralization bricks\", which embedded with algae and equipped\nwith an air circulation system, facilitating the photobioreactor (PBR)'s\nfunctionality. A connection system between modules allows for easy assembly by\nusers, while a limited variety of brick styles ensures modularity in\nmanufacturing without sacrificing customization and diversity. The system is\nalso equipped with an advanced microalgae status detection algorithm, which\nallows users to monitor the condition of the microalgae using monocular camera.\nThis functionality ensures timely alerts and notifications for users to replace\nthe algae, thereby optimizing the operational efficiency and sustainability of\nthe algae cultivation process.",
      "generated_abstract": "r presents a modular photobioreactor fa\u00e7ade system designed to\nfacilitate the integration of photobioreactors into architectural projects,\nenhancing the sustainability of buildings. The fa\u00e7ade is a key component of\narchitectural design, providing an aesthetic and functional interface with the\noutside world. This paper focuses on the integration of photobioreactors into\narchitectural fa\u00e7ades, highlighting the benefits of integrating photobioreactors\nin buildings and the challenges associated with their integration. The fa\u00e7ade\nsystem presented in this paper provides a modular framework that allows for the\nintegration of photobioreactors into buildings, offering a sustainable and\nsustainable alternative to conventional fa\u00e7ade systems. The system is designed\nto maximize the integration of photobioreactors into architectural fa\u00e7ades,\nenhancing the sustain",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14960629921259844,
          "p": 0.3114754098360656,
          "f": 0.20212765519069728
        },
        "rouge-2": {
          "r": 0.027472527472527472,
          "p": 0.05434782608695652,
          "f": 0.0364963459044174
        },
        "rouge-l": {
          "r": 0.14960629921259844,
          "p": 0.3114754098360656,
          "f": 0.20212765519069728
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2410.09069v2",
      "true_abstract": "The rapid expansion of e-commerce and the widespread use of credit cards in\nonline purchases and financial transactions have significantly heightened the\nimportance of promptly and accurately detecting credit card fraud (CCF). Not\nonly do fraudulent activities in financial transactions lead to substantial\nmonetary losses for banks and financial institutions, but they also undermine\nuser trust in digital services. This study presents a new stacking-based\napproach for CCF detection by adding two extra layers to the usual\nclassification process: an attention layer and a confidence-based combination\nlayer. In the attention layer, we combine soft outputs from a convolutional\nneural network (CNN) and a recurrent neural network (RNN) using the dependent\nordered weighted averaging (DOWA) operator, and from a graph neural network\n(GNN) and a long short-term memory (LSTM) network using the induced ordered\nweighted averaging (IOWA) operator. These weighted outputs capture different\npredictive signals, increasing the model's accuracy. Next, in the\nconfidence-based layer, we select whichever aggregate (DOWA or IOWA) shows\nlower uncertainty to feed into a meta-learner. To make the model more\nexplainable, we use shapley additive explanations (SHAP) to identify the top\nten most important features for distinguishing between fraud and normal\ntransactions. These features are then used in our attention-based model.\nExperiments on three datasets show that our method achieves high accuracy and\nrobust generalization, making it effective for CCF detection.",
      "generated_abstract": "ection in financial services has become increasingly challenging due\nto the complexities of detecting financial crimes. Traditional deep learning\nmodels struggle to effectively process and categorize large volumes of data,\nand the inherent complexity of financial fraud presents unique challenges. In\nthis work, we propose an ensemble of models based on Attention Mechanism,\nConvolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and a\nConfidence-Driven Gating Mechanism (CDGM). Our approach integrates the\nattention mechanism with CNNs to capture contextual information, while\nexploiting the advantages of GNNs to enhance the robustness of fraud detection\ninvolving unstructured data. We introduce a Confidence-Driven Gating\nMechanism (CDGM) to adaptively balance the attention distribution between\ndifferent features and class labels. By",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14102564102564102,
          "p": 0.2558139534883721,
          "f": 0.18181817723652768
        },
        "rouge-2": {
          "r": 0.023809523809523808,
          "p": 0.046296296296296294,
          "f": 0.03144653639492173
        },
        "rouge-l": {
          "r": 0.12179487179487179,
          "p": 0.22093023255813954,
          "f": 0.15702478880677564
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.cond-mat/dis-nn/2503.07796v1",
      "true_abstract": "Recently, a new frontier in computing has emerged with physical neural\nnetworks(PNNs) harnessing intrinsic physical processes for learning. Here, we\nexplore topological mechanical neural networks(TMNNs) inspired by the quantum\nspin Hall effect(QSHE) in topological metamaterials, for machine learning\nclassification tasks. TMNNs utilize pseudospin states and the robustness of the\nQSHE, making them damage-tolerant for binary classification. We first\ndemonstrate data clustering using untrained TMNNs. Then, for specific tasks, we\nderive an in situ backpropagation algorithm - a two-step, local-rule method\nthat updates TMNNs using only local information, enabling in situ physical\nlearning. TMNNs achieve high accuracy in classifications of Iris flowers,\nPenguins, and Seeds while maintaining robustness against bond pruning.\nFurthermore, we demonstrate parallel classification via frequency-division\nmultiplexing, assigning different tasks to distinct frequencies for enhanced\nefficiency. Our work introduces in situ backpropagation for wave-based\nmechanical neural networks and positions TMNNs as promising neuromorphic\ncomputing hardware for classification tasks.",
      "generated_abstract": "vancements in topological neural networks (TNNs) have unveiled\ntheir potential in various applications, such as material classification and\nquantum computing. However, these systems suffer from low-performance and\nhigh-energy consumption due to their high-dimensionality. To address these\nissues, this work proposes a novel neural network based on TNNs, named as\nTopological Mechanical Neural Networks (TMNNs), which is designed to perform\nclassification tasks while retaining a low energy consumption. We demonstrate\nthat TMNNs can outperform conventional deep neural networks (DNNs) on both\nmachine learning and classification tasks. Moreover, we analyze the\nperformance of TMNNs by conducting both backpropagation learning (BPL) and\nBPL-like learning (BPL-LL). The BPL-LL algorithm demonstrates superior\nperformance compared to the BPL algorithm,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24528301886792453,
          "p": 0.29213483146067415,
          "f": 0.2666666617046681
        },
        "rouge-2": {
          "r": 0.027972027972027972,
          "p": 0.03636363636363636,
          "f": 0.03162054844475072
        },
        "rouge-l": {
          "r": 0.2169811320754717,
          "p": 0.25842696629213485,
          "f": 0.23589743093543733
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2502.05336v1",
      "true_abstract": "This paper introduces Monotone Delta, an order-theoretic measure designed to\nenhance the reliability assessment of survey-based instruments in human-machine\ninteractions. Traditional reliability measures, such as Cronbach's Alpha and\nMcDonald's Omega, often yield misleading estimates due to their sensitivity to\nredundancy, multidimensional constructs, and assumptions of normality and\nuncorrelated errors. These limitations can compromise decision-making in\nhuman-centric evaluations, where survey instruments inform adaptive interfaces,\ncognitive workload assessments, and human-AI trust models. Monotone Delta\naddresses these issues by quantifying internal consistency through the\nminimization of ordinal contradictions and alignment with a unidimensional\nlatent order using weighted tournaments. Unlike traditional approaches, it\noperates without parametric or model-based assumptions. We conducted\ntheoretical analyses and experimental evaluations on four challenging\nscenarios: tau-equivalence, redundancy, multidimensionality, and non-normal\ndistributions, and proved that Monotone Delta provides more stable reliability\nassessments compared to existing methods. The Monotone Delta is a valuable\nalternative for evaluating questionnaire-based assessments in psychology, human\nfactors, healthcare, and interactive system design, enabling organizations to\noptimize survey instruments, reduce costly redundancies, and enhance confidence\nin human-system interactions.",
      "generated_abstract": "Survey-based instruments are increasingly used to assess individuals'\nconsistent attitudes and beliefs. However, in practice, the design and\nimplementation of such instruments often face challenges. This paper presents a\nnovel methodology to assess internal consistency in such instruments. We\nintroduce a novel graph-based framework that allows us to leverage order\ntheoretic tools to address the challenge of assessing the internal consistency\nof survey instruments across diverse scenarios. To our knowledge, this is the\nfirst work to leverage order theoretic tools to assess internal consistency\nacross diverse scenarios. We demonstrate the effectiveness of our proposed\nframework through simulations and empirical studies. The results show that our\nframework is capable of assessing internal consistency across diverse\nscenarios, offering a more effective and practical approach for the\nassessment of internal consistency in survey-based instruments.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17518248175182483,
          "p": 0.3157894736842105,
          "f": 0.2253521080861382
        },
        "rouge-2": {
          "r": 0.02976190476190476,
          "p": 0.045454545454545456,
          "f": 0.03597121823922221
        },
        "rouge-l": {
          "r": 0.145985401459854,
          "p": 0.2631578947368421,
          "f": 0.18779342264012883
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/MF/2501.07135v1",
      "true_abstract": "We present a systematic, trend-following strategy, applied to commodity\nfutures markets, that combines univariate trend indicators with cross-sectional\ntrend indicators that capture so-called {\\em momentum spillover}, which can\noccur when there is a lead-lag relationship between the trending behaviour of\ndifferent markets. Our strategy utilises two methods for detecting lead-lag\nrelationships, with a method for computing {\\em network momentum}, to produce a\nnovel trend-following indicator. We use our new trend indicator to construct a\nportfolio whose performance we compare to a baseline model which uses only\nunivariate indicators, and demonstrate statistically significant improvements\nin Sharpe ratio, skewness of returns, and downside performance, using synthetic\nbootstrapped data samples taken from time-series of actual prices.",
      "generated_abstract": "al trend-following strategies primarily focus on capturing the\ntrend component of asset returns, often neglecting the tail component, which\ndominates market returns. In this paper, we propose a novel network momentum\nstrategy that integrates both trend and tail components. Specifically, we\nintroduce a novel Momentum-Trend Indicator that captures the trend and\ntail components separately, and then utilize it to construct the momentum\ncomponent. The proposed strategy outperforms the state-of-the-art trend-following\nstrategies on two standard benchmarks: the Diversified 60/40 Portfolio and the\nBond-Weighted 60/40 Portfolio. We further study the impact of the proposed\nstrategy on the tail component, which is critical for systematic portfolio\nconstruction in practice. Our findings reveal that the tail component plays a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25842696629213485,
          "p": 0.3108108108108108,
          "f": 0.28220858399939786
        },
        "rouge-2": {
          "r": 0.018018018018018018,
          "p": 0.0196078431372549,
          "f": 0.01877933773193282
        },
        "rouge-l": {
          "r": 0.1797752808988764,
          "p": 0.21621621621621623,
          "f": 0.1963190134472507
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.physics/chem-ph/2503.08880v1",
      "true_abstract": "Density matrix embedding theory (DMET) provides a framework to describe\nground-state expectation values in strongly correlated systems, but its\nextension to dynamical quantities is still an open problem. We demonstrate one\nroute to obtaining excitations and dynamical spectral functions by using the\ntechniques of DMET to approximate the matrix elements that arise in a\nsingle-mode inspired excitation ansatz. We demonstrate this approach in the 1D\nHubbard model, comparing the neutral excitations, single-particle density of\nstates, charge, and spin dynamical structure factors to benchmarks from the\nBethe ansatz and density matrix renormalization group. Our work highlights the\npotential of these ideas in building computationally efficient approaches for\ndynamical quantities.",
      "generated_abstract": "t a theoretical framework for the description of correlated\n lattice systems based on the density matrix embedding theory (DMET) and\n its associated computational tools. The DMET framework is an extension of\n density functional theory (DFT) and, in particular, of the DFT-BSE method.\n It allows for a complete description of the many-body wavefunction,\n including its correlation and spin structure, and thus its emergent properties\n in the framework of the many-body localized model (MBLM). In the DMET framework,\n the eigenstates of the Hamiltonian are described as wavefunctions of a\n reduced density matrix, which is then used to calculate dynamical quantities\n such as excitation energies and spin dynamics. The DMET framework is\n particularly suited for the description of correlated systems in which the\n many-body wavefunction is in general unknown, as in many materials systems\n exhibiting complex spatial correlations. Here",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2839506172839506,
          "p": 0.27710843373493976,
          "f": 0.2804877998787924
        },
        "rouge-2": {
          "r": 0.06666666666666667,
          "p": 0.05785123966942149,
          "f": 0.06194689767992836
        },
        "rouge-l": {
          "r": 0.25925925925925924,
          "p": 0.25301204819277107,
          "f": 0.2560975559763534
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.02729v1",
      "true_abstract": "This paper introduces a novel low-complexity memoryless linearizer for\nsuppression of distortion in analog frontends. It is based on our recently\nintroduced linearizer which is inspired by neural networks, but with\norders-of-magnitude lower complexity than conventional neural-networks\nconsidered in this context, and it can also outperform the conventional\nparallel memoryless Hammerstein linearizer. Further, it can be designed through\nmatrix inversion and thereby the costly and time consuming numerical\noptimization traditionally used when training neural networks is avoided. The\nlinearizer proposed in this paper is different in that it uses 1-bit\nquantizations as nonlinear activation functions and different bias values.\nThese features enable a look-up table implementation which eliminates all but\none of the multiplications and additions required for the linearization.\nExtensive simulations and comparisons are included in the paper, for distorted\nmulti-tone signals and bandpass filtered white noise, which demonstrate the\nefficacy of the proposed linearizer.",
      "generated_abstract": "e a digital linearizer for low-complexity linearization of\ndifferential-quadratic (DQ) models in the real domain, leveraging 1-bit\nquantization. The proposed digital linearizer is implemented in a single\ntransmitter and receiver, eliminating the need for analog pre-emphasis and\nquantization-specific processing, and significantly reducing the hardware\ncomplexity compared to state-of-the-art digital linearizers. Additionally, we\ndemonstrate that the proposed digital linearizer can achieve higher-order\npolynomial approximation errors than existing digital linearizers, offering\npotential for reducing the number of polynomial terms required for\napproximation. The proposed digital linearizer is implemented in a single\ntransmitter and receiver, eliminating the need for analog pre-emphasis and\nquantization-specific processing, and significantly reducing the hardware\ncomplexity compared to state-of-the-art digital linear",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19047619047619047,
          "p": 0.3389830508474576,
          "f": 0.24390243441775733
        },
        "rouge-2": {
          "r": 0.028169014084507043,
          "p": 0.05194805194805195,
          "f": 0.03652967580575942
        },
        "rouge-l": {
          "r": 0.1523809523809524,
          "p": 0.2711864406779661,
          "f": 0.19512194661287935
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.09489v1",
      "true_abstract": "Integrated sensing and communications (ISAC) has emerged as a promising\nparadigm to unify wireless communications and radar sensing, enabling efficient\nspectrum and hardware utilization. A core challenge with realizing the gains of\nISAC stems from the unique challenges of dual purpose beamforming design due to\nthe highly non-convex nature of key performance metrics such as sum rate for\ncommunications and the Cramer-Rao lower bound (CRLB) for sensing. In this\npaper, we propose a low-complexity structured approach to ISAC beamforming\noptimization to simultaneously enhance spectral efficiency and estimation\naccuracy. Specifically, we develop a successive convex approximation (SCA)\nbased algorithm which transforms the original non-convex problem into a\nsequence of convex subproblems ensuring convergence to a locally optimal\nsolution. Furthermore, leveraging the proposed SCA framework and the Lagrange\nduality, we derive the optimal beamforming structure for CRLB optimization in\nISAC systems. Our findings characterize the reduction in radar streams one can\nemploy without affecting performance. This enables a dimensionality reduction\nthat enhances computational efficiency. Numerical simulations validate that our\napproach achieves comparable or superior performance to the considered\nbenchmarks while requiring much lower computational costs.",
      "generated_abstract": "In this paper, we investigate the optimal design of beamforming structures in\nintegrated beamforming (ISAC) systems, focusing on the trade-off between\nsum-rate and the Cramer-Rao lower bound (CRLB). We derive the optimal beamforming\nstructure for maximizing the sum-rate and simultaneously minimizing the CRLB\nwithout requiring any a priori knowledge of the channel. We also present an\nefficient algorithm for the beamforming design using the Alternating\nDirection Method of Multipliers (ADMM) to minimize the sum-rate while\noptimizing the beamforming parameters. Simulation results validate our\ntheoretical results and show that the proposed beamforming design achieves\nbetter performance compared to the conventional beamforming structure and\nconventional ISAC systems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27692307692307694,
          "p": 0.5070422535211268,
          "f": 0.35820895065468683
        },
        "rouge-2": {
          "r": 0.09497206703910614,
          "p": 0.17346938775510204,
          "f": 0.12274367773801319
        },
        "rouge-l": {
          "r": 0.26153846153846155,
          "p": 0.4788732394366197,
          "f": 0.338308453142249
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.01640v1",
      "true_abstract": "We study certain properties of modules over 1-dimensional local integral\ndomains. First, we examine the order of the conductor ideal and its expected\nrelationship with multiplicity. Next, we investigate the reflexivity of certain\ncolength-two ideals. Finally, we consider the freeness problem of the absolute\nintegral closure of a DVR, and connect this to the reflexivity problem of\n$R^{\\frac{1}{p^n}}$.",
      "generated_abstract": "Let $\\mathfrak{p}$ be a prime ideal of an algebraically closed field $k$ of\nprescribed degree $d$, and let $R$ be a $d$-dimensional $k$-algebra that is\nnot a field. In this paper we show that the integral closure of $R$ in $k[x_1,\n\\dots, x_d",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21951219512195122,
          "p": 0.2571428571428571,
          "f": 0.23684210029432146
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.04878048780487805,
          "f": 0.04210525825152412
        },
        "rouge-l": {
          "r": 0.17073170731707318,
          "p": 0.2,
          "f": 0.18421052134695307
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2411.19436v2",
      "true_abstract": "In economic analysis, rational decision-makers often take actions to reduce\ntheir risk exposure. These actions include purchasing market insurance and\nimplementing prevention measures to modify the shape of the loss distribution.\nUnder the assumption that the insureds' actions are fully observed by the\ninsurer, this paper investigates the interaction between self-protection and\ninsurance demand when insurance premiums are determined by convex premium\nprinciples within the framework of distortion risk measures. Specifically, the\ninsured selects an optimal proportional insurance share and prevention effort\nto minimize the risk measure of their end-of-period exposure. We explicitly\ncharacterize the optimal combination of prevention effort and insurance demand\nin a self-protection model when the insured adopts tail value-at-risk or a\nsubclass with strictly concave distortion functions. Additionally, we conduct\ncomparative static analyses to illustrate our main findings under various\npremium structures, risk aversion levels, and loss distributions. Our results\nindicate that market insurance and self-protection are complementary,\nsupporting classical insights from the literature regarding corner insurance\npolicies (i.e., null and full insurance) in the absence of ex ante moral\nhazard. Finally, we consider the effects of moral hazard on the interaction\nbetween self-protection and insurance demand. Our findings show that ex ante\nmoral hazard shifts the complementary effect into substitution effect.",
      "generated_abstract": "r examines the demand for self-protection and insurance with\nconvex premium principles. A firm's insurance premium is a function of the\npremium principle, the firm's own premium, and the firm's risk. The firm\ninsures against risk and is willing to pay a higher insurance premium to\ninsure against risk. In addition, the firm can also choose to insure against\nrisk and be willing to pay a lower insurance premium to insure against risk. We\nshow that, in the presence of convex premium principles, a firm's insurance\ndemand is non-monotonic in the premium principle and convex. Our analysis\nreveals that, in the presence of convex premium principles, the firm's\ninsurance demand can be non-monotonic in the premium principle, convex, and\nthe firm's risk. We also",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13953488372093023,
          "p": 0.36,
          "f": 0.20111731440966268
        },
        "rouge-2": {
          "r": 0.03664921465968586,
          "p": 0.08139534883720931,
          "f": 0.05054151196392535
        },
        "rouge-l": {
          "r": 0.13953488372093023,
          "p": 0.36,
          "f": 0.20111731440966268
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2411.16617v1",
      "true_abstract": "Quanto options allow the buyer to exchange the foreign currency payoff into\nthe domestic currency at a fixed exchange rate. We investigate quanto options\nwith multiple underlying assets valued in different foreign currencies each\nwith a different strike price in the payoff function. We carry out a\ncomparative performance analysis of different stochastic volatility (SV),\nstochastic correlation (SC), and stochastic exchange rate (SER) models to\ndetermine the best combination of these models for Monte Carlo (MC) simulation\npricing. In addition, we test the performance of all model variants with\nconstant correlation as a benchmark. We find that a combination of GARCH-Jump\nSV, Weibull SC, and Ornstein Uhlenbeck (OU) SER performs best. In addition, we\nanalyze different discretization schemes and their results. In our simulations,\nthe Milstein scheme yields the best balance between execution times and lower\nstandard deviations of price estimates. Furthermore, we find that incorporating\nmean reversion into stochastic correlation and stochastic FX rate modeling is\nbeneficial for MC simulation pricing. We improve the accuracy of our\nsimulations by implementing antithetic variates variance reduction. Finally, we\nderive the correlation risk parameters Cora and Gora in our framework so that\ncorrelation hedging of quanto options can be performed.",
      "generated_abstract": "r introduces a new pricing model for multi-strike quanto call options\non multiple assets. These options are priced using a novel stochastic volatility\nmodel that incorporates the impact of correlations among assets, as well as\nvolatility and correlation effects across multiple exchange rates. Our model\nconsiders both European and American options. We derive the Black-Scholes\nderivative for the model and use Monte Carlo simulation to evaluate its\nperformance. Additionally, we test the performance of the model using real\nequity data from the U.S. and the U.K. and compare the results with those from\nthe Black-Scholes model. Our results show that the model performs well, with\nhigh accuracy and lower volatility compared to the Black-Scholes model. The\nresults also show that the model performs better than the Black-Scholes model\nwhen volatility and correlation are high. Additionally, we explore",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.38271604938271603,
          "f": 0.30243901961023206
        },
        "rouge-2": {
          "r": 0.0427807486631016,
          "p": 0.06557377049180328,
          "f": 0.051779930496329556
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.38271604938271603,
          "f": 0.30243901961023206
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2407.05866v1",
      "true_abstract": "We introduce generalizations of the COGARCH model of Kl\\\"uppelberg et al.\nfrom 2004 and the volatility and price model of Barndorff-Nielsen and Shephard\nfrom 2001 to a Markov-switching environment. These generalizations allow for\nexogeneous jumps of the volatility at times of a regime switch. Both models are\nstudied within the framework of Markov-modulated generalized Ornstein-Uhlenbeck\nprocesses which allows to derive conditions for stationarity, formulas for\nmoments, as well as the autocovariance structure of volatility and price\nprocess. It turns out that both models inherit various properties of the\noriginal models and therefore are able to capture basic stylized facts of\nfinancial time-series such as uncorrelated log-returns, correlated squared\nlog-returns and non-existence of higher moments in the COGARCH case.",
      "generated_abstract": "We consider the problem of estimating the variance of the log-return of a\nmarket-index. We present two models, based on Ornstein-Uhlenbeck and\nGaussian-Markovian processes, that are appropriate for such a task. Our\nresults show that the Ornstein-Uhlenbeck-type model is superior to the\nGaussian-Markovian one in terms of efficiency and predictive power. The\ncomparative advantage of the Ornstein-Uhlenbeck-type model is particularly\nnoticeable in the case of heavy-tailed distributions, such as the CAPM-type\ndistribution. In this case, the Gaussian-Markovian model tends to overfit the\ndata.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18518518518518517,
          "p": 0.26785714285714285,
          "f": 0.21897809735627907
        },
        "rouge-2": {
          "r": 0.045454545454545456,
          "p": 0.06493506493506493,
          "f": 0.05347593098458679
        },
        "rouge-l": {
          "r": 0.16049382716049382,
          "p": 0.23214285714285715,
          "f": 0.18978101706430828
        }
      }
    },
    {
      "paper_id": "math.LO.math/CO/2503.09246v1",
      "true_abstract": "We introduce the notion of Ramsey partition regularity, a generalisation of\npartition regularity involving infinitary configurations. We provide\ncharacterisations of this notion in terms of certain ultrafilters related to\ntensor products and dubbed Ramsey's witnesses; and we also consider their\nnonstandard counterparts as pairs of hypernatural numbers, called Ramsey pairs.\nThese characterisations are then used to determine whether various\nconfigurations involving polynomials and exponentials are Ramsey partition\nregular over the natural numbers.",
      "generated_abstract": "ntext of Ramsey theory, a collection of sets is said to be a witness\nfor a set of subsets of a given cardinality if it contains a copy of every\nsufficiently large subset of that cardinality. The existence of witnesses for\nsufficiently large sets of the same cardinality has been studied by many\nauthors, including Birkhoff and Ramsey. In particular, they have studied the\nexistence of witnesses for certain classes of sets, namely, the classes of\nsets of the form $\\{\\alpha_n : n \\in \\omega\\}$, the classes of sets of the\nform $\\{\\alpha_n : n \\in \\omega, \\alpha_n \\cap \\alpha_m = \\emptyset, \\forall\nm < n\\}$, and the classes of sets of the form $\\{\\alpha_n : n \\in \\omega,\n\\alpha_n \\cap \\alpha_m = \\emptyset, \\forall",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1320754716981132,
          "p": 0.109375,
          "f": 0.11965811470231594
        },
        "rouge-2": {
          "r": 0.014285714285714285,
          "p": 0.011235955056179775,
          "f": 0.012578611423600682
        },
        "rouge-l": {
          "r": 0.1320754716981132,
          "p": 0.109375,
          "f": 0.11965811470231594
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2412.05508v1",
      "true_abstract": "Experimentation in online digital platforms is used to inform decision\nmaking. Specifically, the goal of many experiments is to optimize a metric of\ninterest. Null hypothesis statistical testing can be ill-suited to this task,\nas it is indifferent to the magnitude of effect sizes and opportunity costs.\nGiven access to a pool of related past experiments, we discuss how\nexperimentation practice should change when the goal is optimization. We survey\nthe literature on empirical Bayes analyses of A/B test portfolios, and single\nout the A/B Testing Problem (Azevedo et al., 2020) as a starting point, which\ntreats experimentation as a constrained optimization problem. We show that the\nframework can be solved with dynamic programming and implemented by\nappropriately tuning $p$-value thresholds. Furthermore, we develop several\nextensions of the A/B Testing Problem and discuss the implications of these\nresults on experimentation programs in industry. For example, under no-cost\nassumptions, firms should be testing many more ideas, reducing test allocation\nsizes, and relaxing $p$-value thresholds away from $p = 0.05$.",
      "generated_abstract": "We present a new approach to the design of experimentation programs,\noptimizing the sampling strategy to minimize the cost of evaluating the\neffectiveness of a new treatment. Our method uses a model to analyze the\nconsequences of the experimental design and then estimates the weights of the\ntreatments to be used. We show that this approach allows for flexible\ndesigns, with different weights for each treatment, and that the resulting\nexperimental designs can be optimized to minimize the overall cost of the\nexperimentation program. Theoretical analysis and simulation studies support\nthe method's theoretical guarantees and illustrate its practical utility in\noptimizing the design of experimentation programs.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13445378151260504,
          "p": 0.25,
          "f": 0.17486338342978305
        },
        "rouge-2": {
          "r": 0.043209876543209874,
          "p": 0.07368421052631578,
          "f": 0.05447470351103009
        },
        "rouge-l": {
          "r": 0.1092436974789916,
          "p": 0.203125,
          "f": 0.14207649818388143
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.astro-ph/CO/2503.09718v1",
      "true_abstract": "The strongly lensed Supernova (SN) Encore at a redshift of $z = 1.949$,\ndiscovered behind the galaxy cluster MACS J0138$-$2155 at $z=0.336$, provides a\nrare opportunity for time-delay cosmography and studies of the SN host galaxy,\nwhere previously another SN, called SN Requiem, had appeared. To enable these\nstudies, we combine new James Webb Space Telescope (JWST) imaging, archival\nHubble Space Telescope (HST) imaging, and new Very Large Telescope (VLT)\nspectroscopic data to construct state-of-the-art lens mass models that are\ncomposed of cluster dark-matter (DM) halos and galaxies. We determine the\nphotometric and structural parameters of the galaxies across six JWST and five\nHST filters. We use the color-magnitude and color-color relations of\nspectroscopically-confirmed cluster members to select additional cluster\nmembers, identifying a total of 84 galaxies belonging to the galaxy cluster. We\nconstruct seven different mass models using a variety of DM halo mass profiles,\nand explore both multi-plane and approximate single-plane lens models. As\nconstraints, we use the observed positions of 23 multiple images from eight\nmultiply lensed sources at four distinct spectroscopic redshifts. In addition,\nwe use stellar velocity dispersion measurements to obtain priors on the galaxy\nmass distributions. We find that six of the seven models fit well to the\nobserved image positions. Mass models with cored-isothermal DM profiles fit\nwell to the observations, whereas the mass model with a Navarro-Frenk-White\ncluster DM profile has an image-position $\\chi^2$ value that is four times\nhigher. We build our ultimate model by combining four multi-lens-plane mass\nmodels and predict the image positions and magnifications of SN Encore and SN\nRequiem. Our work lays the foundation for building state-of-the-art mass models\nof the cluster for future cosmological analysis and SN host galaxy studies.",
      "generated_abstract": "er MACS J0138$-$2155 has been studied extensively in the context of\ncluster lensing, with its strong lensing mass model (SMMJ13) yielding a\nlensing mass of $M_{\\rm lens} = 1.4\\times10^{14} M_\\odot$. In this paper, we\npresent the results of our analysis of the supernova data, focusing on the\nsupernova data that are most sensitive to the cluster lensing mass. We find that\nthe supernova data alone are insufficient to constrain the lens mass to better\nthan 5% accuracy. To achieve a better constraint, we include the supernova data\ntogether with the photometry from the Hubble Space Telescope, which has a\nsensitivity to the lensing mass of 0.5%, with a 5%",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16091954022988506,
          "p": 0.4,
          "f": 0.22950819262966948
        },
        "rouge-2": {
          "r": 0.052830188679245285,
          "p": 0.14,
          "f": 0.07671232478889119
        },
        "rouge-l": {
          "r": 0.14942528735632185,
          "p": 0.37142857142857144,
          "f": 0.2131147500067187
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/CG/2503.06833v1",
      "true_abstract": "The Hausdorff distance is a fundamental measure for comparing sets of\nvectors, widely used in database theory and geometric algorithms. However, its\nexact computation is computationally expensive, often making it impractical for\nlarge-scale applications such as multi-vector databases. In this paper, we\nintroduce an approximation framework that efficiently estimates the Hausdorff\ndistance while maintaining rigorous error bounds. Our approach leverages\napproximate nearest-neighbor (ANN) search to construct a surrogate function\nthat preserves essential geometric properties while significantly reducing\ncomputational complexity. We provide a formal analysis of approximation\naccuracy, deriving both worst-case and expected error bounds. Additionally, we\nestablish theoretical guarantees on the stability of our method under\ntransformations, including translation, rotation, and scaling, and quantify the\nimpact of non-uniform scaling on approximation quality. This work provides a\nprincipled foundation for integrating Hausdorff distance approximations into\nlarge-scale data retrieval and similarity search applications, ensuring both\ncomputational efficiency and theoretical correctness.",
      "generated_abstract": "orff distance is a distance metric used to compare the locations of\nobjects within a two-dimensional space. In the context of multi-vector databases,\nthe Hausdorff distance between two multi-vector databases is used to measure\nthe distance between the objects represented by the databases. In this paper,\nwe introduce a new approximation for the Hausdorff distance between\nmulti-vector databases. We use a method that is based on the distance between\nmulti-vector vectors and the distances between their elements. In addition, we\npropose a new approximation for the Hausdorff distance between multi-vector\ndatabases that takes into account the order of the vectors in the database.\nThe proposed approximation for the Hausdorff distance is more accurate than\nthe original one, as well as more efficient in terms of space and time\nrequirements. We also compare the proposed approximation with the original one\nand provide theoretical results on the approximation errors. Finally, we",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2631578947368421,
          "p": 0.410958904109589,
          "f": 0.3208556102136178
        },
        "rouge-2": {
          "r": 0.0763888888888889,
          "p": 0.09090909090909091,
          "f": 0.08301886296219325
        },
        "rouge-l": {
          "r": 0.23684210526315788,
          "p": 0.3698630136986301,
          "f": 0.28877004871629164
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.10478v1",
      "true_abstract": "Existing studies have shown that the monetization of silver in the Ming\nDynasty effectively promoted the prosperity of trade in the Ming Dynasty, while\nthe prices of labor, handicraft products and grain were long suppressed by the\ndeformed economic structure. With the expansion of silver application, the\nfluctuation of silver supply and demand exacerbated the above contradictions.\nCapital accumulation that should have been obtained through the marketization\nof labor was easily plundered by the landlord gentry class through silver. This\narticle re-discusses the issue from the perspective of supply and demand.\nCompared with the increase and then decrease of silver supply, the evolution of\nsilver demand is more complicated: at the tax level, the widespread use of\nsilver leads to a huge difference in the elasticity of production and trade\ntaxes. When government spending surges, the increase in tax burden will be\nmainly borne by agriculture and handicrafts. At the production level, the high\nliquidity of silver makes the concentration of social wealth more convenient,\nwhile the reduction in silver supply and the expansion of demand have rapidly\nexpanded deflation, further exacerbating the gap between the rich and the poor.\nSuch combined effect of supply and demand factors has caused the monetization\nof silver to become an accelerator of the economic collapse of the Ming\nDynasty.",
      "generated_abstract": "r examines the social impact of silver monetization in the Ming\nSilver monetization was a significant economic development, which significantly\nimproved the standard of living of the people. This study examines the\ninfluence of the silver monetization on the social impact of the Ming Dynasty\nfrom the perspective of supply and demand. It uses the quantitative method to\nanalyze the trend and impact of the silver monetization on the people's\nstandard of living. The results show that, compared to the people before\nsilver monetization, the people's standard of living increased by 12.57%,\nwhile the social impact on the people also increased by 20.47%. The study also\nfinds that, compared to the people before silver monetization, the people's\nstandard of living increased by 12.57%, while the social impact on the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16393442622950818,
          "p": 0.35714285714285715,
          "f": 0.22471909681100874
        },
        "rouge-2": {
          "r": 0.06451612903225806,
          "p": 0.14457831325301204,
          "f": 0.08921932658807942
        },
        "rouge-l": {
          "r": 0.14754098360655737,
          "p": 0.32142857142857145,
          "f": 0.20224718669864927
        }
      }
    },
    {
      "paper_id": "math.PR.math/MP/2503.09486v1",
      "true_abstract": "The directed landscape, the central object in the Kardar-Parisi-Zhang\nuniversality class, is shown to be the scaling limit of various models by\nDauvergne and Vir\\'ag (2022) and Dauvergne, Ortmann and Vir\\'ag (2018). In his\nstudy of geodesics in upper tail deviations of the directed landscape, Liu\n(2022) put forward a conjecture about the rate of the lowest rate metric under\nwhich a geodesic between two points passes through a particular point between\nthem. Das, Dauvergne and Vir\\'ag (2024) disproved his conjecture, and made a\nconjecture of their own. This paper disproves that conjecture and puts the\nquestion to rest with an answer and a proof.",
      "generated_abstract": "In this paper, we investigate the behavior of the directed landscape\ngene\n0-geodesic for a general graph on the large scale. Our main result is a\none-point large deviation for the length of the directed landscape geodesic.\nOur approach relies on the random matrix theory and the Gaussian free field\ntheory.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.2222222222222222,
          "f": 0.14814814370370383
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.043478260869565216,
          "f": 0.027777773429784634
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.2222222222222222,
          "f": 0.14814814370370383
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.00085v1",
      "true_abstract": "This study examines the influence of employee education and health on\nfirm-level Total Factor Productivity (TFP) in China, using panel data from\nA-share listed companies spanning from 2007 to 2022. The analysis shows that\nlife expectancy and higher education have a significant impact on TFP. More\noptimal health conditions can result in increased productivity through\ndecreased absenteeism and improved work efficiency. Similarly, higher levels of\neducation can support technological adaptation, innovation, and managerial\nefficiency. Nevertheless, the correlation between health and higher education\nindicates that there may be a point where further improvements in health yield\ndiminishing returns in terms of productivity for individuals with advanced\neducation. These findings emphasise the importance of implementing\ncomprehensive policies that improve both health and education, maximising their\nimpact on productivity. This study adds to the current body of research by\npresenting empirical evidence at the firm-level in China. It also provides\npractical insights for policymakers and business leaders who want to improve\neconomic growth and competitiveness. Future research should take into account\nwider datasets, more extensive health metrics, and delve into the mechanisms\nthat contribute to the diminishing returns observed in the relationship between\nhealth and education.",
      "generated_abstract": "This study examines the impact of employee education and health on firm-level\nteacher-focused public pension (TFP) in China. We find that the TFP rate increases\nsignificantly with employees' education and health. Our findings suggest that\nincreasing employees' education and health can improve the TFP rate of\nteacher-focused pension, but the impact is not consistent across different\nemployers and different employee groups. Moreover, the TFP rate is lower when\nemployees are younger and less educated. In addition, the TFP rate increases as\nthe size of firms increases. The results also suggest that the impact of\nemployee education on the TFP rate is stronger when the firm is more\ncompetitive, more capital-intensive, and more labor-intensive.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18461538461538463,
          "p": 0.375,
          "f": 0.24742267599107245
        },
        "rouge-2": {
          "r": 0.060109289617486336,
          "p": 0.12222222222222222,
          "f": 0.08058607616632475
        },
        "rouge-l": {
          "r": 0.17692307692307693,
          "p": 0.359375,
          "f": 0.237113397640557
        }
      }
    },
    {
      "paper_id": "stat.ME.q-bio/QM/2503.05448v1",
      "true_abstract": "Graphical modeling is a widely used tool for analyzing conditional\ndependencies between variables and traditional methods may struggle to capture\nshared and distinct structures in multi-group or multi-condition settings.\nJoint graphical modeling (JGM) extends this framework by simultaneously\nestimating network structures across multiple related datasets, allowing for a\ndeeper understanding of commonalities and differences. This capability is\nparticularly valuable in fields such as genomics and neuroscience, where\nidentifying variations in network topology can provide critical biological\ninsights. Existing JGM methodologies largely fall into two categories:\nregularization-based approaches, which introduce additional penalties to\nenforce structured sparsity, and Bayesian frameworks, which incorporate prior\nknowledge to improve network inference. In this study, we explore an\nalternative method based on two-target linear covariance matrix shrinkage.\nFormula for optimal shrinkage intensities is proposed which leads to the\ndevelopment of JointStein framework. Performance of JointStein framework is\nproposed through simulation benchmarking which demonstrates its effectiveness\nfor large-scale single-cell RNA sequencing (scRNA-seq) data analysis. Finally,\nwe apply our approach to glioblastoma scRNA-seq data, uncovering dynamic shifts\nin T cell network structures across disease progression stages. The result\nhighlights potential of JointStein framework in extracting biologically\nmeaningful insights from high-dimensional data.",
      "generated_abstract": "In this paper, we propose a novel method to estimate the joint graphical model of\nscRNAseq data, which integrates the traditional graphical model inference with\nthe Stein-type shrinkage. We first introduce a novel graphical modeling\nframework that integrates the graphical model inference with the Stein-type\nshrinkage. The proposed framework is able to handle various graphical model\nstructures and can efficiently approximate the joint graphical model. Secondly,\nwe introduce a novel algorithm to efficiently estimate the joint graphical\nmodel. We further propose a fast algorithm based on the alternating least\nsquares method, which is effective and efficient for large scale networks. We\nvalidate the proposed method through simulation studies and two real-world\nscRNAseq data, showing the effectiveness and efficiency of the proposed\nmethod.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.45454545454545453,
          "f": 0.27777777353395067
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.05102040816326531,
          "f": 0.03533568451847378
        },
        "rouge-l": {
          "r": 0.19333333333333333,
          "p": 0.4393939393939394,
          "f": 0.26851851427469137
        }
      }
    },
    {
      "paper_id": "physics.ao-ph.physics/ao-ph/2503.05288v1",
      "true_abstract": "Like Johnson noise, where thermal fluctuations of charge carriers in a\nresistor lead to measurable current fluctuations, the internal variability of\nEarth's atmosphere leads to fluctuations in the infrared radiation emitted to\nspace, creating \"Earth's infrared background\" (EIB). This background consists\nof fluctuations that are isotropic in space and red in time, with an upper\nbound of 400 km and 2.5 days on their spatiotemporal decorrelation, between\nmeso-scale and synoptic-scale weather. Like the anisotropies in the Cosmic\nMicrowave Background (CMB), which represent features of interest in the\nUniverse, the anisotropies in Earth's infrared radiation represent features of\ninterest in Earth's atmosphere. Unlike the CMB, which represents a historical\nrecord of the Universe since the Big Bang, the EIB represents Earth's climate\nin steady state.",
      "generated_abstract": "'s infrared background is a fundamental component of the solar\ninfrared spectrum and is an important contributor to climate. The infrared\nbackground is typically described in terms of a blackbody spectrum with\ntemperature and emissivity that are determined by the geographical location of\nthe observer. In this paper, we develop a theoretical model for the Earth's\ninfrared background that accounts for both the solar infrared background and\nthe emission of man-made and natural radiation. This model is based on\nconcepts from non-linear optics, and is a generalization of the standard\nblackbody model for the Earth's infrared background. We show that this model\nprovides a good description of the Earth's infrared background, even though\nit does not directly account for the solar infrared background. We also\nconsider the effects of man-made and natural radiation on the Earth'",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20987654320987653,
          "p": 0.22972972972972974,
          "f": 0.21935483371987524
        },
        "rouge-2": {
          "r": 0.02654867256637168,
          "p": 0.027777777777777776,
          "f": 0.02714931626952857
        },
        "rouge-l": {
          "r": 0.19753086419753085,
          "p": 0.21621621621621623,
          "f": 0.20645160791342365
        }
      }
    },
    {
      "paper_id": "cs.CL.cs/CL/2503.10533v1",
      "true_abstract": "High-quality test items are essential for educational assessments,\nparticularly within Item Response Theory (IRT). Traditional validation methods\nrely on resource-intensive pilot testing to estimate item difficulty and\ndiscrimination. More recently, Item-Writing Flaw (IWF) rubrics emerged as a\ndomain-general approach for evaluating test items based on textual features.\nHowever, their relationship to IRT parameters remains underexplored. To address\nthis gap, we conducted a study involving over 7,000 multiple-choice questions\nacross various STEM subjects (e.g., math and biology). Using an automated\napproach, we annotated each question with a 19-criteria IWF rubric and studied\nrelationships to data-driven IRT parameters. Our analysis revealed\nstatistically significant links between the number of IWFs and IRT difficulty\nand discrimination parameters, particularly in life and physical science\ndomains. We further observed how specific IWF criteria can impact item quality\nmore and less severely (e.g., negative wording vs. implausible distractors).\nOverall, while IWFs are useful for predicting IRT parameters--particularly for\nscreening low-difficulty MCQs--they cannot replace traditional data-driven\nvalidation methods. Our findings highlight the need for further research on\ndomain-general evaluation rubrics and algorithms that understand\ndomain-specific content for robust item validation.",
      "generated_abstract": "r examines the impact of item-writing flaws on difficulty and\ndiscrimination in the Item Response Theory (IRT) model. It is demonstrated that\nIRT models based on Item Response Theory (IRT) model with item-writing\nflaws are not fit for the purpose of the reliability of assessments. It is\nproposed to use the IRT model with item-writing flaws, but with the model of\nthe reliability of assessments, to solve the problems of reliability and\ndiscrimination in IRT models. The results of a study on the impact of item-writing\nflaws on the reliability of assessments and discrimination in IRT models are\npresented. It is shown that the reliability of assessments in the IRT model\nwith item-writing flaws is 0.0006, and the discrimination is 0.0005",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15602836879432624,
          "p": 0.4489795918367347,
          "f": 0.23157894354072023
        },
        "rouge-2": {
          "r": 0.03932584269662921,
          "p": 0.08333333333333333,
          "f": 0.05343511014742766
        },
        "rouge-l": {
          "r": 0.15602836879432624,
          "p": 0.4489795918367347,
          "f": 0.23157894354072023
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10107v1",
      "true_abstract": "Perceptive mobile networks (PMNs), integrating ubiquitous sensing\ncapabilities into mobile networks, represent an important application of\nintegrated sensing and communication (ISAC) in 6G. In this paper, we propose a\npractical framework for uplink sensing of angle-of-arrival (AoA), Doppler, and\ndelay in millimeter-wave (mmWave) communication systems, which addresses\nchallenges posed by clock asynchrony and hybrid arrays, while being compatible\nwith existing communication protocols. We first introduce a beam scanning\nmethod and a corresponding AoA estimation algorithm, which utilizes frequency\nsmoothing to effectively estimate AoAs for both static and dynamic paths. We\nthen propose several methods for constructing a ``clean'' reference signal,\nwhich is subsequently used to cancel the effect caused by the clock asynchrony.\nWe further develop a signal ratio-based joint AoA-Doppler-delay estimator and\npropose an AoA-based 2D-FFT-MUSIC (AB2FM) algorithm that applies 2D-FFT\noperations on the signal subspace, which accelerates the computation process\nwith low complexity. Our proposed framework can estimate parameters in pairs,\nremoving the complicated parameter association process. Simulation results\nvalidate the effectiveness of our proposed framework and demonstrate its\nrobustness in both low and high signal-to-noise ratio (SNR) conditions.",
      "generated_abstract": "This paper addresses the sensing problem in the uplink of a millimeter-wave\n(mmWave) integrated sensing and communication (ISAC) system, where both the\nchannel and the sensing pattern are known a priori. We first present an\naccurate model of the channel that accounts for the nonlinearities and\ncorrelation effects in mmWave channels. Based on the channel model, we derive\nthe sensing problem in the uplink of the ISAC system and provide a closed-form\nsolution for the optimal sensing pattern. We then propose a low-complexity\nalgorithm to perform the sensing and estimate the sensing pattern. The\nperformance of the proposed algorithm is compared with a baseline algorithm\nthat does not account for the nonlinearities in the channel. Our analysis\nindicates that the proposed algorithm achieves superior performance compared to\nthe baseline algorithm under the same channel model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22727272727272727,
          "p": 0.410958904109589,
          "f": 0.29268292224342657
        },
        "rouge-2": {
          "r": 0.06179775280898876,
          "p": 0.1,
          "f": 0.07638888416763145
        },
        "rouge-l": {
          "r": 0.20454545454545456,
          "p": 0.3698630136986301,
          "f": 0.26341462956049977
        }
      }
    },
    {
      "paper_id": "math.OC.econ/TH/2502.17012v2",
      "true_abstract": "We introduce a model of infinite horizon linear dynamic optimization and\nobtain results concerning existence of solution and satisfaction of the Euler\ncondition and transversality condition being unconditionally sufficient for\noptimality of a trajectory. We show that the optimal value function is concave\nand continuous and the optimal trajectory satisfies the functional equation of\ndynamic programming. Linearity bites when it comes to the definition of optimal\ndecision rules which can no longer be guaranteed to be single-valued. We show\nthat the optimal decision rule is an upper semi-continuous correspondence. For\nlinear cake-eating problems, we obtain monotonicity results for the optimal\nvalue function and a conditional monotonicity result for optimal decision\nrules. We also introduce the concept of a two-phase linear cake eating problem\nand obtain a necessary condition that must be satisfied by all solutions of\nsuch problems.",
      "generated_abstract": "Dynamic optimization is a ubiquitous and challenging problem in science and\nindustry. In this work, we consider the dynamic optimization problem as a\nnetwork-based optimization problem and derive the linear and deterministic\nsolution to the problem. Our solution is based on the reduction of the problem\nto a deterministic system of nonlinear equations. The solution provides a\nmathematical framework for analyzing and understanding dynamic optimization. We\nalso discuss the use of the proposed solution in various settings.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18518518518518517,
          "p": 0.3191489361702128,
          "f": 0.23437499535278325
        },
        "rouge-2": {
          "r": 0.04032258064516129,
          "p": 0.06944444444444445,
          "f": 0.05102040351520243
        },
        "rouge-l": {
          "r": 0.1728395061728395,
          "p": 0.2978723404255319,
          "f": 0.2187499953527833
        }
      }
    },
    {
      "paper_id": "math.OC.math/HO/2503.07134v1",
      "true_abstract": "This paper is dedicated to the elementary proof of Pontryagin's maximum\nprinciple for problems with free right end point. The proof for the standard\nproblem is taken from the monography of Ioffe and Tichomirov. We assume\npiecewise continuous controls and the proof turns out to be very simple. We\ngeneralize the concept to the problem of optimal multiprocesses, to control\nproblems with delays and to the control of Volterra integral equations.\nFurthermore, we discuss the problem on infinite horizon. Moreover, we state\nArrow type sufficiency conditions. The optimality conditions are demonstrated\non illustrative examples.",
      "generated_abstract": "er optimal control problems in the form\n$u^*(t)= \\inf \\{ V(t,u(s)) : s \\in (0,T) \\},$ where $V$ is a given\ncontrolled function and $u$ is a given initial-boundary value problem. We\nassume that $V$ is Lipschitz continuous with respect to both variables, and\nthat $V$ is Lipschitz continuous on the boundary of its domain. We also\nassume that $V$ is continuous on $(0,T) \\times \\partial D$ for some\nclosed domain $D.$ We prove that the problem is equivalent to finding a\nfunction $w$ such that $u^*(t) = \\inf \\{ V(t,u(s)) : s \\in (0,T) \\} +\n\\int_0^t w(s) ds.$ We prove that the optimal control problem",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.234375,
          "p": 0.23076923076923078,
          "f": 0.23255813453518429
        },
        "rouge-2": {
          "r": 0.0449438202247191,
          "p": 0.046511627906976744,
          "f": 0.045714280715755645
        },
        "rouge-l": {
          "r": 0.234375,
          "p": 0.23076923076923078,
          "f": 0.23255813453518429
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/CB/2501.09546v1",
      "true_abstract": "Bacteria can form a great variety of spatially heterogeneous cell density\npatterns, ranging from simple concentric rings to dynamical spiral waves\nappearing in growing colonies. These pattern formation phenomena are important\nas they reflect how cellular processes such as metabolism operate in\nheterogeneous chemical environments. In the laboratory, they can be studied in\nsimplified set-ups, where spatial gradients of oxygen and nutrients are\nexternally imposed, and cells are immobilized in a gel matrix. An intriguing\nexample, observed in such set-ups over 80 years ago, is the sequential\nformation of narrow bands of high cell density, taking place even for a clonal\npopulation. However, key aspects of the dynamics of band formation remained\nobscure. Using time-lapse imaging of replicate transparent columns in\nsimplified growth media, we first quantify the precision of the positioning and\ntiming of band formation. We also show that the appearance and position of\ndifferent bands can be modulated independently. This \"modularity\" is suggested\nby the observation that different bands differ in their gene expression, and it\nis reproduced by a theoretical model based on the existence of internal\nmetabolic states and the induction of a pH gradient. Finally, we can also\nmodify the observed pattern formation by introducing genetic modifications that\nimpair selected metabolic pathways. In our opinion, the possibility of precise\nmeasurements and controls, together with the simplicity and richness of the\n\"proliferation pattern formation\" phenomenon, can make it a model system to\nstudy the response of cellular processes to heterogeneous environments.",
      "generated_abstract": "tion patterns of bacteria play a crucial role in their adaptation to\nevolutionary niches. Bacterial populations exhibit a wide range of patterns,\nfrom highly mobile, mobile-type (MT) to inert, non-motile (NM) states. The\npatterns of bacterial growth are often influenced by environmental conditions,\nsuch as nutrient availability, and these conditions are often dynamically\nmodulated by bacterial processes. However, the mechanisms underlying the\ndynamics of proliferation patterns of bacteria are not fully understood. In\nthis review, we discuss the existing theories and mechanisms for the\ndynamics of bacterial growth, focusing on the role of nutrient availability,\nmetabolism, and cell-cell communication. We also discuss how the dynamics of\nbacterial growth can be modeled using mathematical models, and discuss the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1761006289308176,
          "p": 0.35,
          "f": 0.23430961897725885
        },
        "rouge-2": {
          "r": 0.029661016949152543,
          "p": 0.06796116504854369,
          "f": 0.041297930872860926
        },
        "rouge-l": {
          "r": 0.15723270440251572,
          "p": 0.3125,
          "f": 0.20920501646679865
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2501.13136v1",
      "true_abstract": "Digital currencies have become popular in the last decade due to their\nnon-dependency and decentralized nature. The price of these currencies has seen\na lot of fluctuations at times, which has increased the need for prediction. As\ntheir most popular, Bitcoin(BTC) has become a research hotspot. The main\nchallenge and trend of digital currencies, especially BTC, is price\nfluctuations, which require studying the basic price prediction model. This\nresearch presents a classification and regression model based on stack deep\nlearning that uses a wavelet to remove noise to predict movements and prices of\nBTC at different time intervals. The proposed model based on the stacking\ntechnique uses models based on deep learning, especially neural networks and\ntransformers, for one, seven, thirty and ninety-day forecasting. Three feature\nselection models, Chi2, RFE and Embedded, were also applied to the data in the\npre-processing stage. The classification model achieved 63\\% accuracy for\npredicting the next day and 64\\%, 67\\% and 82\\% for predicting the seventh,\nthirty and ninety days, respectively. For daily price forecasting, the\npercentage error was reduced to 0.58, while the error ranged from 2.72\\% to\n2.85\\% for seven- to ninety-day horizons. These results show that the proposed\nmodel performed better than other models in the literature.",
      "generated_abstract": "y explores the potential of using hashrate features to forecast\nthe Bitcoin (BTC) price. Using the MATLAB DeepStacking approach, the study\ncompares the performance of two wavelet transforms, namely the Haar wavelet\nand the wavelet packet transform. The wavelet transforms were applied on the\nBTC hashrate data, which was obtained from the Bitinfocharts website, over a\nperiod of 10 years. The results show that the wavelet transforms outperform\nthe traditional Fourier transform in terms of accuracy. Additionally, the\nDeepStacking approach outperforms the wavelet transforms in terms of accuracy,\nrobustness, and scalability. The study concludes that the wavelet transforms\nare more effective than the traditional Fourier transform in forecasting the\nBTC price. Future research can be focused on improving the scalability and\nrobustness of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16911764705882354,
          "p": 0.3108108108108108,
          "f": 0.2190476144834468
        },
        "rouge-2": {
          "r": 0.020100502512562814,
          "p": 0.037037037037037035,
          "f": 0.026058627361139907
        },
        "rouge-l": {
          "r": 0.16176470588235295,
          "p": 0.2972972972972973,
          "f": 0.2095238049596373
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.01265v1",
      "true_abstract": "Contrast-enhanced magnetic resonance imaging (CE-MRI) is crucial for tumor\ndetection and diagnosis, but the use of gadolinium-based contrast agents\n(GBCAs) in clinical settings raises safety concerns due to potential health\nrisks. To circumvent these issues while preserving diagnostic accuracy, we\npropose a novel Transformer with Localization Prompts (TLP) framework for\nsynthesizing CE-MRI from non-contrast MR images. Our architecture introduces\nthree key innovations: a hierarchical backbone that uses efficient Transformer\nto process multi-scale features; a multi-stage fusion system consisting of\nLocal and Global Fusion modules that hierarchically integrate complementary\ninformation via spatial attention operations and cross-attention mechanisms,\nrespectively; and a Fuzzy Prompt Generation (FPG) module that enhances the TLP\nmodel's generalization by emulating radiologists' manual annotation through\nstochastic feature perturbation. The framework uniquely enables interactive\nclinical integration by allowing radiologists to input diagnostic prompts\nduring inference, synergizing artificial intelligence with medical expertise.\nThis research establishes a new paradigm for contrast-free MRI synthesis while\naddressing critical clinical needs for safer diagnostic procedures. Codes are\navailable at https://github.com/ChanghuiSu/TLP.",
      "generated_abstract": "resonance imaging (MRI) is a crucial diagnostic tool for medical\nphenomenon, but it is often time-consuming, costly, and invasive. The\ndevelopment of rapid and inexpensive MRI techniques, such as fast spin echo (FSE)\nand fast gradient echo (FG) sequences, has improved image quality and reduced\nthe examination time by a factor of 10 to 100, but these methods still require\nthe patient to be positioned in a specific manner and are susceptible to\ninterference from physical factors, such as patient position, movement, and\nexternal factors. To address these challenges, we propose a novel MRI synthesis\nmethod that uses a transformer to generate synthetic images by synthesizing\nrealistic 3D anatomical structures based on the target patient's MRI data,\nproviding a novel solution for fast and non",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20567375886524822,
          "p": 0.30851063829787234,
          "f": 0.24680850583829791
        },
        "rouge-2": {
          "r": 0.03636363636363636,
          "p": 0.04918032786885246,
          "f": 0.04181184180213487
        },
        "rouge-l": {
          "r": 0.19858156028368795,
          "p": 0.2978723404255319,
          "f": 0.23829786754042565
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2411.17750v1",
      "true_abstract": "This paper statistically analysed pensioner longevity in Ghana. It\nfundamentally sought to ascertain the significant determinants of longevity\namongst Ghanaian pensioners, specifically, SSNIT pensioners by estimating the\nmortality rate of SSNIT pensioners, determining the factors that significantly\naffect the longevity of the average SSNIT pensioner and constructing a\npredictive model for predicting the longevity of SSNIT pensioners in Ghana.\nSecondary data was obtained from the leading pension provider in Ghana, SSNIT.\nThe results of the study revealed that the total number of male deaths was\nsignificantly greater, about four times more, than the total number of female\ndeaths. There was sufficient evidence that there exists a lower rate of death\namong female pensioners as compared to male pensioners. Furthermore, a\nsignificant number of the SSNIT pensioners used in the analysis survived less\nthan 8 years after retirement before their death and the average basic salary\nof the pensioners who lived less than 8 years after retirement was GH 7741.827\ncedis and served for 35.65 years on average. On the contrary, it was observed\nthat pensioners who lived more than 8 years after retirement had an average\nbasic salary of GH 5544.20 cedis and served for 31.49 years on average. In\nconclusion, predictors such as basic salary, the number of years of service in\nthe workforce, the age of the pensioner before death and the gender of the\npensioner were statistically significant in predicting the number of years a\nSSNIT pensioner survived after retirement before death that is, whether a\nparticular SSNIT pensioner survived less than 8 years or 8 years and above\nafter retirement. The authors recommend to employees that, their health should\nbe of great priority as every increase in the service year increases the\nlikelihood of a SSNIT pensioner surviving less than 8 years after retirement by\n22.36%.",
      "generated_abstract": "objective of this study was to examine the determinants of longevity\namongst SSNIT pensioners in Ghana. The study adopted a descriptive survey\napproach using a cross sectional design. The study employed a sample size of\n1000 respondents drawn from three geographical regions of Ghana. The\npopulation of the study was 5,390,451. The instrument used for data\ncollection was a self-administered questionnaire. The data collected was\nanalysed using the Statistical Package for Social Sciences (SPSS) version 22.\nThe study revealed that there were 64.6% of the respondents were males, 52.3%\nwere aged 41 years and above and 33.8% were unemployed. It also revealed that\n75.8% of the respondents were literate,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.3783783783783784,
          "f": 0.2616822384662416
        },
        "rouge-2": {
          "r": 0.05439330543933055,
          "p": 0.1262135922330097,
          "f": 0.07602338760353637
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.3783783783783784,
          "f": 0.2616822384662416
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.01448v1",
      "true_abstract": "This paper introduces Agency-Driven Labor Theory as a new theoretical\nframework for understanding human work in AI-augmented environments. While\ntraditional labor theories have focused primarily on task execution and labor\ntime, ADLT proposes that human labor value is increasingly derived from agency\n- the capacity to make informed judgments, provide strategic direction, and\ndesign operational frameworks for AI systems. The paper presents a mathematical\nframework expressing labor value as a function of agency quality, direction\neffectiveness, and outcomes, providing a quantifiable approach to analyzing\nhuman value creation in AI-augmented workplaces. Drawing on recent work in\norganizational economics and knowledge worker productivity, ADLT explains how\nhuman workers create value by orchestrating complex systems that combine human\nand artificial intelligence. The theory has significant implications for job\ndesign, compensation structures, professional development, and labor market\ndynamics. Through applications across various sectors, the paper demonstrates\nhow ADLT can guide organizations in managing the transition to AI-augmented\noperations while maximizing human value creation. The framework provides\npractical tools for policymakers and educational institutions as they prepare\nworkers for a labor market where value creation increasingly centers on agency\nand direction rather than execution.",
      "generated_abstract": "r develops a theory of labor in the age of artificial intelligence.\nAgency theory has long been used to explain human behavior in the market, but\nits application to the workplace is limited. In this paper, we introduce a\nframework for understanding the labor market in the age of artificial\nintelligence. We develop a model that incorporates the key features of\nagency-driven labor theory, including decision-making, competition, and\nincentives. We apply this framework to analyze the impact of artificial\nintelligence on the labor market, focusing on the role of algorithmic\ndecision-making in shaping labor supply and demand. The model provides a\ncomprehensive framework for understanding the interplay between algorithmic\ndecision-making and human behavior, offering new insights into the\ntechnological, economic, and societal impacts of AI. Our analysis shows that\nthe adoption of AI technologies can lead",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.192,
          "p": 0.2891566265060241,
          "f": 0.23076922597309552
        },
        "rouge-2": {
          "r": 0.027932960893854747,
          "p": 0.041666666666666664,
          "f": 0.03344481124819702
        },
        "rouge-l": {
          "r": 0.176,
          "p": 0.26506024096385544,
          "f": 0.2115384567423263
        }
      }
    },
    {
      "paper_id": "astro-ph.IM.hep-ex/2503.10521v1",
      "true_abstract": "The Dark Matter Particle Explorer (DAMPE) is a space-based Cosmic-Ray (CR)\nobservatory with the aim, among others, to study Cosmic-Ray Electrons (CREs) up\nto 10 TeV. Due to the low CRE rate at multi-TeV energies, we aim to increasing\nthe acceptance by selecting events outside the fiducial volume. The complex\ntopology of non-fiducial events requires the development of a novel energy\nreconstruction method. We propose the usage of Convolutional Neural Networks\nfor a regression task to recover an accurate estimation of the initial energy.",
      "generated_abstract": "The Diffusive Atmospheric Muon Neutrino Telescope (DAMPE) experiment is\nadvancing the understanding of neutrino oscillation physics by measuring the\nenergies of muons in the atmosphere. In this study, we investigate the\nreconstruction of non-fiducial events in the DAMPE experiment using convolutional\nneural networks (CNNs) to estimate the energy of muons in the atmosphere. We\nanalyze data collected in the first 18 months of DAMPE operation, with 178,627\nmuons detected. The reconstructed energy of the muon is compared to the\ndetermined energy using the 2-sigma method, with the average error of 1.8%.\nThis study demonstrates that CNNs can be used to estimate the energy of muons\nreconstructed from the DAMPE experiment.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23076923076923078,
          "p": 0.21739130434782608,
          "f": 0.22388059201938082
        },
        "rouge-2": {
          "r": 0.060240963855421686,
          "p": 0.052083333333333336,
          "f": 0.0558659168140824
        },
        "rouge-l": {
          "r": 0.2153846153846154,
          "p": 0.2028985507246377,
          "f": 0.20895521888505247
        }
      }
    },
    {
      "paper_id": "physics.data-an.physics/data-an/2503.09771v1",
      "true_abstract": "We propose a method of estimating the uncertainty of a result obtained\nthrough extrapolation to the complete basis set limit. The method is based on\nan ensemble of random walks which simulate all possible extrapolation outcomes\nthat could have been obtained if results from larger basis sets had been\navailable. The results assembled from a large collection of random walks can be\nthen analyzed statistically, providing a route for uncertainty prediction at a\nconfidence level required in a particular application. The method is free of\nempirical parameters and compatible with any extrapolation scheme. The proposed\ntechnique is tested in a series of numerical trials by comparing the determined\nconfidence intervals with reliable reference data. We demonstrate that the\npredicted error bounds are reliable, tight, yet conservative at the same time.",
      "generated_abstract": "of extrapolating the basis set extrapolation (BSE) error is to\nestimate the error of the extrapolated solution at the next iteration. This\nerror is typically measured by comparing the solution obtained at the current\niteration with the solution obtained at the previous iteration. In this work,\nwe propose a method to estimate the error through random walks. We derive\nanalytical expressions for the error in the case of an even number of basis\nfunctions. We also show that the error can be computed as the difference between\nthe solution obtained at the next iteration and the solution obtained at the\ncurrent iteration. We use this method to estimate the error in the BSE\nextrapolation of a one-dimensional Schr\\\"odinger equation with a local\nHamiltonian. The error is shown to be small, ranging from 2.4% at the first\niteration to 1.7% at the last",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.31521739130434784,
          "p": 0.3918918918918919,
          "f": 0.3493975854202352
        },
        "rouge-2": {
          "r": 0.08064516129032258,
          "p": 0.09009009009009009,
          "f": 0.08510637799402473
        },
        "rouge-l": {
          "r": 0.29347826086956524,
          "p": 0.36486486486486486,
          "f": 0.32530119987806655
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.09434v1",
      "true_abstract": "We derive nonlinear stability results for numerical integrators on Riemannian\nmanifolds, by imposing conditions on the ODE vector field and the step size\nthat makes the numerical solution non-expansive whenever the exact solution is\nnon-expansive over the same time step. Our model case is a geodesic version of\nthe explicit Euler method. Precise bounds are obtained in the case of\nRiemannian manifolds of constant sectional curvature. The approach is based on\na cocoercivity property of the vector field adapted to manifolds from Euclidean\nspace. It allows us to compare the new results to the corresponding well-known\nresults in flat spaces, and in general we find that a non-zero curvature will\ndeteriorate the stability region of the geodesic Euler method. The step size\nbounds depend on the distance traveled over a step from the initial point.\nNumerical examples for spheres and hyperbolic 2-space confirm that the bounds\nare tight.",
      "generated_abstract": "We investigate the conditions on the initial data of a first order\nequilibrium problem in a Riemannian manifold for which the Euler method converges\nto the equilibrium in finite time. In particular, we prove that the\nconditional stability condition in terms of the first eigenvalue of the\nLaplacian on a manifold is sufficient for the Euler method to converge to the\nequilibrium. Moreover, we show that the convergence is independent of the\ndiscretization and the convergence rate is proportional to the distance between\nthe equilibrium and the initial point.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23863636363636365,
          "p": 0.42857142857142855,
          "f": 0.3065693384708829
        },
        "rouge-2": {
          "r": 0.07857142857142857,
          "p": 0.14473684210526316,
          "f": 0.10185184729080954
        },
        "rouge-l": {
          "r": 0.23863636363636365,
          "p": 0.42857142857142855,
          "f": 0.3065693384708829
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/OT/2407.18572v1",
      "true_abstract": "An approach to amputation, the process of introducing missing values to a\ncomplete dataset, is presented. It allows to construct missingness indicators\nin a flexible and principled way via copulas and Bernoulli margins and to\nincorporate dependence in missingness patterns. Besides more classical\nmissingness models such as missing completely at random, missing at random, and\nmissing not at random, the approach is able to model structured missingness\nsuch as block missingness and, via mixtures, monotone missingness, which are\npatterns of missing data frequently found in real-life datasets. Properties\nsuch as joint missingness probabilities or missingness correlation are derived\nmathematically. The approach is demonstrated with mathematical examples and\nempirical illustrations in terms of a well-known dataset.",
      "generated_abstract": "er the problem of amputating a Bernoulli random variable with\namputation probability $\\alpha$ in the presence of a known (unknown) event\n$A$ of interest. When $A$ is of class $\\mathscr{C}$, we consider the\namputation of the Bernoulli distribution with amputation probability\n$\\alpha$ under the event $A \\cap \\mathscr{C}$. We derive a necessary and\nsufficient condition for the existence of a consistent estimator of the\namputation parameter $\\alpha$ under the null hypothesis $\\mathscr{H}_0$. We\nthen derive a consistent estimator for the amputation parameter under the\nalternative hypothesis $\\mathscr{H}_1$. We establish the asymptotic properties\nof the estimators for both the null and alternative hypotheses. We also\nestablish a consistency of the estimator for the null hypothesis, which\ninvolves the use of the Cram\\'er-Wold",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12162162162162163,
          "p": 0.14754098360655737,
          "f": 0.13333332837969838
        },
        "rouge-2": {
          "r": 0.009174311926605505,
          "p": 0.01020408163265306,
          "f": 0.009661830762914192
        },
        "rouge-l": {
          "r": 0.12162162162162163,
          "p": 0.14754098360655737,
          "f": 0.13333332837969838
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.13140v2",
      "true_abstract": "Tensor decomposition has emerged as a powerful framework for feature\nextraction in multi-modal biomedical data. In this review, we present a\ncomprehensive analysis of tensor decomposition methods such as Tucker,\nCANDECOMP/PARAFAC, spiked tensor decomposition, etc. and their diverse\napplications across biomedical domains such as imaging, multi-omics, and\nspatial transcriptomics. To systematically investigate the literature, we\napplied a topic modeling-based approach that identifies and groups distinct\nthematic sub-areas in biomedicine where tensor decomposition has been used,\nthereby revealing key trends and research directions. We evaluated challenges\nrelated to the scalability of latent spaces along with obtaining the optimal\nrank of the tensor, which often hinder the extraction of meaningful features\nfrom increasingly large and complex datasets. Additionally, we discuss recent\nadvances in quantum algorithms for tensor decomposition, exploring how quantum\ncomputing can be leveraged to address these challenges. Our study includes a\npreliminary resource estimation analysis for quantum computing platforms and\nexamines the feasibility of implementing quantum-enhanced tensor decomposition\nmethods on near-term quantum devices. Collectively, this review not only\nsynthesizes current applications and challenges of tensor decomposition in\nbiomedical analyses but also outlines promising quantum computing strategies to\nenhance its impact on deriving actionable insights from complex biomedical\ndata.",
      "generated_abstract": "ence of quantum computers has brought significant advancements in\nquantum algorithms, including quantum tensor decomposition (QTD), which has\nbeen widely applied in quantum chemistry. However, the performance of QTD in\nbiomedical applications is still limited by its complexity and low scalability.\nThis study proposes an efficient and scalable approach to implement QTD in\nbiomedical applications. Specifically, we introduce a tensor decomposition\nalgorithm based on the quantum Fourier transform (QFT) and the quantum\ninverse-square root (QISR) to solve the tensor decomposition problem. The\nalgorithm is realized by a quantum circuit and is implemented on a quantum\ndevice. The proposed algorithm achieves a computational complexity of $O(N^4)$\nand can be parallelized to $O(N^2)$, which greatly improves the scalability\ncompared with the conventional approach. The experimental results on a 2000-node",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17647058823529413,
          "p": 0.3037974683544304,
          "f": 0.2232558093049217
        },
        "rouge-2": {
          "r": 0.03225806451612903,
          "p": 0.05128205128205128,
          "f": 0.03960395565532848
        },
        "rouge-l": {
          "r": 0.15441176470588236,
          "p": 0.26582278481012656,
          "f": 0.19534883256073562
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.14069v2",
      "true_abstract": "We study the problem of estimating the barycenter of a distribution given\ni.i.d. data in a geodesic space. Assuming an upper curvature bound in\nAlexandrov's sense and a support condition ensuring the strong geodesic\nconvexity of the barycenter problem, we establish finite-sample error bounds in\nexpectation and with high probability. Our results generalize Hoeffding- and\nBernstein-type concentration inequalities from Euclidean to geodesic spaces.\nBuilding on these concentration inequalities, we derive statistical guarantees\nfor two efficient algorithms for the computation of barycenters.",
      "generated_abstract": "We establish finite sample bounds for the barycenter estimation of\nfinancial assets in geodesic spaces. Our approach builds on a new\nwell-tempered, geometric approach to the estimation of the mean of a\ngeodesic Brownian motion, developed in our previous work. We derive a new\ngeneralized central limit theorem for the barycenter estimator in terms of\nGaussian processes. Our finite sample bounds are obtained using a\nnon-asymptotic approach, and rely on the explicit expressions for the\nGaussian processes. We prove that our finite sample bounds are asymptotically\noptimal under a set of assumptions on the volatility of the asset returns. Our\nresults extend the previous results in the literature.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2698412698412698,
          "p": 0.27419354838709675,
          "f": 0.27199999500032007
        },
        "rouge-2": {
          "r": 0.08641975308641975,
          "p": 0.07446808510638298,
          "f": 0.07999999502759214
        },
        "rouge-l": {
          "r": 0.2698412698412698,
          "p": 0.27419354838709675,
          "f": 0.27199999500032007
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.05087v2",
      "true_abstract": "This paper studies the formation of the grand coalition of a cooperative game\nby investigating its possible internal dynamics. Each coalition is capable of\nforcing all players to reconsider the current state of the game when it does\nnot provide sufficient payoff. Different coalitions may ask for contradictory\nevolutions, leading to the impossibility of the grand coalition forming. In\nthis paper, we give a characterization of the impossibility, for a given state,\nof finding a new state dominating the previous one such that each aggrieved\ncoalition has a satisfactory payoff. To do so, we develop new polyhedral tools\nrelated to a new family of polyhedra, appearing in numerous situations in\ncooperative game theory.",
      "generated_abstract": "We introduce a model of two-party politics with coalitions in which a\ncoalition's members can choose to cooperate or compete with each other. We\ncharacterize the steady state of this model under a monotonicity condition on\nthe coalition formation rules. This condition allows us to characterize the\ncoalition structure in terms of a linear-quadratic game between the coalition\nmembers. We derive a closed-form solution for the steady state of the model\nwhen the coalition formation rules are linear and quadratic. We then apply our\nresults to illustrate the effect of the coalition formation rules on the\nsteady state and the emergence of coalitions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19230769230769232,
          "p": 0.2631578947368421,
          "f": 0.22222221734320996
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.03488372093023256,
          "f": 0.031413607614923615
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.22807017543859648,
          "f": 0.19259258771358037
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/RM/2503.06806v1",
      "true_abstract": "An important step in the Financial Benchmarks Reform was taken on 13th\nSeptember 2018, when the ECB Working Group on Euro Risk-Free Rates recommended\nthe Euro Short-Term Rate ESTR as the new benchmark rate for the euro area, to\nreplace the Euro OverNight Index Average (EONIA) which will be discontinued at\nthe end of 2021. This transition has a number of important consequences on\nfinancial instruments, OTC derivatives in particular.\n  In this paper we show in detail how the switch from EONIA to ESTR affects the\npricing of OIS, IRS and XVAs. We conclude that the adoption of the \"clean\ndiscounting\" approach recommended by the the ECB, based on ESTR only, is\ntheoretically sound and leads to very limited impacts on financial valuations.\n  This finding ensures the possibility, for the financial industry, to switch\nall EUR OTC derivatives, either cleared with Central Counterparties, or subject\nto bilateral collateral agreements, or non-collateralised, in a safe and\nconsistent manner. The transition to such EONIA-free pricing framework is\nessential for the complete elimination of EONIA before its discontinuation\nscheduled on 31st December 2021.",
      "generated_abstract": "er 2021, the European Central Bank (ECB) announced the replacement\nof its EURONIA interest rate benchmark with ESTR, a new system that will\nreplace EURONIA and EURIBOR. The ECB's new benchmark is designed to be\nequivalent to the EURIBOR, a key interest rate benchmark for banking in the\nEU. The new benchmark will be implemented gradually, with the transition from\nEURONIA to ESTR scheduled to occur on 31 March 2024. The ECB has published a\ntimeline for the implementation of the new benchmark, outlining the steps\nnecessary to transition from EURONIA to ESTR. This paper focuses on the\ntransition from EURONIA to ESTR, examining the key issues that need to be\naddressed to ensure a smooth transition. We discuss the implications of the\nnew benchmark",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23015873015873015,
          "p": 0.4142857142857143,
          "f": 0.2959183627551021
        },
        "rouge-2": {
          "r": 0.04,
          "p": 0.06422018348623854,
          "f": 0.04929576991792349
        },
        "rouge-l": {
          "r": 0.20634920634920634,
          "p": 0.37142857142857144,
          "f": 0.2653061178571429
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.08920v1",
      "true_abstract": "A distributed integrated sensing and communication (D-ISAC) system offers\nsignificant cooperative gains for both sensing and communication performance.\nThese gains, however, can only be fully realized when the distributed nodes are\nperfectly synchronized, which is a challenge that remains largely unaddressed\nin current ISAC research. In this paper, we propose an over-the-air\ntime-frequency synchronization framework for the D-ISAC system, leveraging the\nreciprocity of bistatic sensing channels. This approach overcomes the\nimpractical dependency of traditional methods on a direct line-of-sight (LoS)\nlink, enabling the estimation of time offset (TO) and carrier frequency offset\n(CFO) between two ISAC nodes even in non-LoS (NLOS) scenarios. To achieve this,\nwe introduce a bistatic signal matching (BSM) technique with delay-Doppler\ndecoupling, which exploits offset reciprocity (OR) in bistatic observations.\nThis method compresses multiple sensing links into a single offset for\nestimation. We further present off-grid super-resolution estimators for TO and\nCFO, including the maximum likelihood estimator (MLE) and the matrix pencil\n(MP) method, combined with BSM processing. These estimators provide accurate\noffset estimation compared to spectral cross-correlation techniques. Also, we\nextend the pairwise synchronization leveraging OR between two nodes to the\nsynchronization of $N$ multiple distributed nodes, referred to as centralized\npairwise synchronization. We analyze the Cramer-Rao bounds (CRBs) for TO and\nCFO estimates and evaluate the impact of D-ISAC synchronization on the\nbottom-line target localization performance. Simulation results validate the\neffectiveness of the proposed algorithm, confirm the theoretical analysis, and\ndemonstrate that the proposed synchronization approach can recover up to 96% of\nthe bottom-line target localization performance of the fully-synchronous\nD-ISAC.",
      "generated_abstract": "This paper investigates the time-frequency (TF) synchronization in distributed\nisolated space-time block codes (ISAC) systems. The synchronization process is\ndesigned by maximizing the mutual information (MI) between the received\nsignal and the TF-encoded signal. The MI maximization is formulated as an\noptimization problem for both the joint and independent TF-encoded signals,\nwhich is then solved by a constrained optimization approach. The performance of\nthe proposed approach is evaluated using numerical simulations. The results\ndemonstrate that the proposed approach outperforms the conventional approach,\nin terms of the MI maximization. Moreover, the proposed approach is flexible\nand can be easily adapted to other TF-encoded signal designs. This work\ncontributes to the development of ISAC systems by providing a practical\noptimization-based solution to the TF synchronization problem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16363636363636364,
          "p": 0.34177215189873417,
          "f": 0.2213114710309729
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.07894736842105263,
          "f": 0.05042016372007664
        },
        "rouge-l": {
          "r": 0.15757575757575756,
          "p": 0.3291139240506329,
          "f": 0.2131147497194975
        }
      }
    },
    {
      "paper_id": "quant-ph.physics/hist-ph/2503.07666v1",
      "true_abstract": "The correspondence principle states that classical mechanics emerges from\nquantum mechanics in the appropriate limits. However, beyond this heuristic\nrule, an information-theoretic perspective reveals that classical mechanics is\na compressed, lower-information representation of quantum reality. Quantum\nmechanics encodes significantly more information through superposition,\nentanglement, and phase coherence, which are lost due to decoherence, phase\naveraging, and measurement, reducing the system to a classical probability\ndistribution. This transition is quantified using Kolmogorov complexity, where\nclassical systems require \\( O(N) \\) bits of information, while quantum\ndescriptions require \\( O(2^N) \\), showing an exponential reduction in\ncomplexity. Further justification comes from Ehrenfest's theorem, which ensures\nthat quantum expectation values obey Newton's laws, and path integral\nsuppression, which eliminates non-classical trajectories when \\( S \\gg \\hbar\n\\). Thus, rather than viewing quantum mechanics as an extension of classical\nmechanics, we argue that classical mechanics is a lossy, computationally\nreduced encoding of quantum physics, emerging from a systematic loss of quantum\ncorrelations.",
      "generated_abstract": "We propose a classical-quantum correspondence for the classical dynamics of\nmechanical systems. We first show that any classical dynamical system\nsatisfying the Liouville equation can be mapped to a quantum system that\nexhibits the same dynamics. We then show that the same mapping also occurs when\nthe dynamics of the classical system are described by a quantum theory that is\nan approximation to a Hamiltonian with a Hamiltonian of the quantum system. We\nshow that the classical dynamics of the classical system can be recovered from\nthe quantum dynamics in the limit of a large number of degrees of freedom and\nthe Hamiltonian has a form that is close to that of the Hamiltonian of a\nHamiltonian with a large number of degrees of freedom. We show that the\ndynamics of a system with a Hamiltonian with a large number of degrees of\nfreedom is approximated by a quantum system with a Hamiltonian that is a\nproduct of many small Hamiltonians.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14912280701754385,
          "p": 0.288135593220339,
          "f": 0.19653178741287725
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.028846153846153848,
          "f": 0.023904377616864207
        },
        "rouge-l": {
          "r": 0.13157894736842105,
          "p": 0.2542372881355932,
          "f": 0.17341040012964026
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/NA/2503.09892v1",
      "true_abstract": "As inverter-based resources (IBRs) penetrate power systems, the dynamics\nbecome more complex, exhibiting multiple timescales, including electromagnetic\ntransient (EMT) dynamics of power electronic controllers and electromechanical\ndynamics of synchronous generators. Consequently, the power system model\nbecomes highly stiff, posing a challenge for efficient simulation using\nexisting methods that focus on dynamics within a single timescale. This paper\nproposes a Heterogeneous Multiscale Method for highly efficient multi-timescale\nsimulation of a power system represented by its EMT model. The new method\nalternates between the microscopic EMT model of the system and an automatically\nreduced macroscopic model, varying the step size accordingly to achieve\nsignificant acceleration while maintaining accuracy in both fast and slow\ndynamics of interests. It also incorporates a semi-analytical solution method\nto enable a more adaptive variable-step mechanism. The new simulation method is\nillustrated using a two-area system and is then tested on a detailed EMT model\nof the IEEE 39-bus system.",
      "generated_abstract": "r presents a new multiscale method for the simulation of power\nsystems with inverter-based resources. The method employs a multiscale\nframework, including a discrete model for the generation and transmission\nnetworks and a coarse-grid model for the power system dynamics. The discrete\nmodel is based on a high-fidelity finite element method (FEM) approach, while\nthe coarse-grid model is based on a multiscale FEM approach with a\nmulti-scale multigrid solver. This multiscale methodology allows the\nsimulation of a wide range of configurations, from simple single-inverter\nsystems to more complex hybrid inverter-based systems. The method is validated\nthrough a series of benchmark problems, including the simulation of a simple\nsingle-inverter system, a simple hybrid inverter-based system, a simple\ninterconnecting system, and a complex hybrid",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21568627450980393,
          "p": 0.3548387096774194,
          "f": 0.26829267822427133
        },
        "rouge-2": {
          "r": 0.056338028169014086,
          "p": 0.08163265306122448,
          "f": 0.06666666183472257
        },
        "rouge-l": {
          "r": 0.18627450980392157,
          "p": 0.3064516129032258,
          "f": 0.23170731237061282
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/PR/2501.05232v1",
      "true_abstract": "Tether Limited has the sole authority to create (mint) and destroy (burn)\nTether stablecoins (USDT). This paper investigates Bitcoin's response to USDT\nsupply change events between 2014 and 2021 and identifies an interesting\nasymmetry between Bitcoin's responses to USDT minting and burning events.\nBitcoin responds positively to USDT minting events over 5- to 30-minute event\nwindows, but this response begins declining after 60 minutes. State-dependence\nis also demonstrated, with Bitcoin prices exhibiting a greater increase when\nthe corresponding USDT minting event coincides with positive investor sentiment\nand is announced to the public by data service provider, Whale Alert, on\nTwitter.",
      "generated_abstract": "BTC) is a decentralized, peer-to-peer, cryptocurrency that\nhas revolutionized financial transactions by offering unprecedented privacy\nand portability. The token is also a digital asset that can be mined,\nissued, and sold, with its value directly tied to supply and demand.\nBitcoin's value fluctuates with its supply and demand, and it is affected by\nthe activity of various actors, including miners, traders, and investors. This\nstudy investigates the relationship between the volume of bitcoin (BTC)\ntransactions and the volume of bitcoin mined. We find that, in the week leading\nup to the Tether (TTD) Bitcoin minting event on January 20, 2025, the Tether\nminting event caused a 1.2% increase in BTC",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2702702702702703,
          "p": 0.25316455696202533,
          "f": 0.2614379035020719
        },
        "rouge-2": {
          "r": 0.031578947368421054,
          "p": 0.02912621359223301,
          "f": 0.030303025311193558
        },
        "rouge-l": {
          "r": 0.25675675675675674,
          "p": 0.24050632911392406,
          "f": 0.24836600807723536
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/GN/2411.13762v1",
      "true_abstract": "This paper delves into the spectrum of credit risks associated with\ndecentralized stablecoin issuance, ranging from overcollateralized lending to\nbusiness-to-business credit. It examines the mechanisms, risks, and mitigation\nstrategies at each layer, highlighting the potential for scaling decentralized\nstablecoins while ensuring systemic health.",
      "generated_abstract": "ecoin ecosystem, driven by rapid adoption, has grown to become\na major source of credit risk for financial institutions. This paper examines\nthe growing stability and liquidity risks posed by stablecoins, focusing on\nincreasing the volatility of stablecoin issuers, including the increasing\ninfluence of central banks in the stablecoin ecosystem. We identify four\nrisk categories: liquidity, interest rate, exchange rate, and collateral\ndefault. We then quantify the credit risks posed by each category of stablecoins\nbased on historical data. The results highlight the importance of\nunderstanding the volatility and liquidity risks posed by stablecoins, and\nhow these risks evolve over time. The findings provide a comprehensive\nunderstanding of the credit risks posed by stablecoins and offer insights for\nfinancial institutions",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3333333333333333,
          "p": 0.17333333333333334,
          "f": 0.22807017093721155
        },
        "rouge-2": {
          "r": 0.09523809523809523,
          "p": 0.0380952380952381,
          "f": 0.054421764625850653
        },
        "rouge-l": {
          "r": 0.3333333333333333,
          "p": 0.17333333333333334,
          "f": 0.22807017093721155
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.13395v1",
      "true_abstract": "Distributed acoustic sensor (DAS) technology leverages optical fiber cables\nto detect acoustic signals, providing cost-effective and dense monitoring\ncapabilities. It offers several advantages including resistance to extreme\nconditions, immunity to electromagnetic interference, and accurate detection.\nHowever, DAS typically exhibits a lower signal-to-noise ratio (S/N) compared to\ngeophones and is susceptible to various noise types, such as random noise,\nerratic noise, level noise, and long-period noise. This reduced S/N can\nnegatively impact data analyses containing inversion and interpretation. While\nartificial intelligence has demonstrated excellent denoising capabilities, most\nexisting methods rely on supervised learning with labeled data, which imposes\nstringent requirements on the quality of the labels. To address this issue, we\ndevelop a label-free unsupervised learning (UL) network model based on\nContext-Pyramid-UNet (CP-UNet) to suppress erratic and random noises in DAS\ndata. The CP-UNet utilizes the Context Pyramid Module in the encoding and\ndecoding process to extract features and reconstruct the DAS data. To enhance\nthe connectivity between shallow and deep features, we add a Connected Module\n(CM) to both encoding and decoding section. Layer Normalization (LN) is\nutilized to replace the commonly employed Batch Normalization (BN),\naccelerating the convergence of the model and preventing gradient explosion\nduring training. Huber-loss is adopted as our loss function whose parameters\nare experimentally determined. We apply the network to both the 2-D synthetic\nand filed data. Comparing to traditional denoising methods and the latest UL\nframework, our proposed method demonstrates superior noise reduction\nperformance.",
      "generated_abstract": "This paper presents a novel approach for denoising DAS data by using a\nuniversal denoiser (UD) model, which was trained using a large-scale dataset\ncomprising of DAS and noise samples. The proposed UD model is trained with\ntwo main objectives: (i) to learn the denoiser's properties from noise samples\nand (ii) to generate denoised samples from noisy samples. The UD model is\ntrained using a novel loss function, which is defined as a combination of the\noriginal loss and decay noise loss. The proposed approach is evaluated using\nthe Lime-DAS dataset, which is a challenging dataset with both noise and DAS\nsamples. The results demonstrate that the UD model can generate high-quality\ndenoised samples even when the original DAS samples contain significant\ndecay noise.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11560693641618497,
          "p": 0.29850746268656714,
          "f": 0.166666662642014
        },
        "rouge-2": {
          "r": 0.008547008547008548,
          "p": 0.018867924528301886,
          "f": 0.011764701591005025
        },
        "rouge-l": {
          "r": 0.11560693641618497,
          "p": 0.29850746268656714,
          "f": 0.166666662642014
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/BM/2502.12638v2",
      "true_abstract": "3D molecule generation is crucial for drug discovery and material design.\nWhile prior efforts focus on 3D diffusion models for their benefits in modeling\ncontinuous 3D conformers, they overlook the advantages of 1D SELFIES-based\nLanguage Models (LMs), which can generate 100% valid molecules and leverage the\nbillion-scale 1D molecule datasets. To combine these advantages for 3D molecule\ngeneration, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D\nLanguage Modeling for 3D Molecule Generation. NExT-Mol uses an extensively\npretrained molecule LM for 1D molecule generation, and subsequently predicts\nthe generated molecule's 3D conformers with a 3D diffusion model. We enhance\nNExT-Mol's performance by scaling up the LM's model size, refining the\ndiffusion neural architecture, and applying 1D to 3D transfer learning.\nNotably, our 1D molecule LM significantly outperforms baselines in\ndistributional similarity while ensuring validity, and our 3D diffusion model\nachieves leading performances in conformer prediction. Given these improvements\nin 1D and 3D modeling, NExT-Mol achieves a 26% relative improvement in 3D FCD\nfor de novo 3D generation on GEOM-DRUGS, and a 13% average relative gain for\nconditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are\navailable at https://github.com/acharkq/NExT-Mol.",
      "generated_abstract": "advancements of deep learning in recent years have enabled the\ndevelopment of state-of-the-art generative models, such as the GPT-4 model,\nwhich can generate large amounts of text and images. However, despite the\nsuccess of these models, they are unable to generate 3D molecular structures,\nthus limiting their applicability in scientific research. To address this\nchallenge, we propose NExT-Mol, a 3D molecule generation model that combines\n3D diffusion models with a 1D language model. Our approach leverages the\ngenerative power of diffusion models to generate 3D molecular structures. We\nthen employ a 1D language model to generate chemical descriptors, which are\nthen used as input to the diffusion model to generate 3D molecular structures.\nThis approach enables NExT-Mol to generate 3D molecular structures with high",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.224,
          "p": 0.36363636363636365,
          "f": 0.27722771805460256
        },
        "rouge-2": {
          "r": 0.06077348066298342,
          "p": 0.10377358490566038,
          "f": 0.07665504760625992
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.3246753246753247,
          "f": 0.2475247477575729
        }
      }
    },
    {
      "paper_id": "physics.ao-ph.stat/AP/2503.03990v1",
      "true_abstract": "Accurately quantifying air-sea fluxes is important for understanding air-sea\ninteractions and improving coupled weather and climate systems. This study\nintroduces a probabilistic framework to represent the highly variable nature of\nair-sea fluxes, which is missing in deterministic bulk algorithms. Assuming\nGaussian distributions conditioned on the input variables, we use artificial\nneural networks and eddy-covariance measurement data to estimate the mean and\nvariance by minimizing negative log-likelihood loss. The trained neural\nnetworks provide alternative mean flux estimates to existing bulk algorithms,\nand quantify the uncertainty around the mean estimates. Stochastic\nparameterization of air-sea turbulent fluxes can be constructed by sampling\nfrom the predicted distributions. Tests in a single-column forced upper-ocean\nmodel suggest that changes in flux algorithms influence sea surface temperature\nand mixed layer depth seasonally. The ensemble spread in stochastic runs is\nmost pronounced during spring restratification.",
      "generated_abstract": "ea flux parameterization (ASFP) is a key component of the\nenvironmental Earth system model (ESM) CESM2. It integrates the air-sea\nfluxes into a turbulent boundary layer (TBL) scheme, which governs the\ntransfer of heat and momentum between the atmosphere and the ocean. However,\nthe TBL model is highly uncertain due to the limited dataset and the\nsimplifying assumptions in the model. Therefore, a better ASFP is essential to\nunderstand the TBL and improve the CESM2. In this study, we develop a\ndata-driven probabilistic ASFP (PD-ASFP) to improve the TBL model. The\nPD-ASFP incorporates 127 surface fluxes and 221 boundary layer heat fluxes\nobtained from the Surface Fluxes and Heat Fluxes of the Earth'",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20388349514563106,
          "p": 0.2916666666666667,
          "f": 0.23999999515689807
        },
        "rouge-2": {
          "r": 0.015037593984962405,
          "p": 0.01904761904761905,
          "f": 0.01680671775828123
        },
        "rouge-l": {
          "r": 0.18446601941747573,
          "p": 0.2638888888888889,
          "f": 0.21714285229975525
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/CR/2503.09727v1",
      "true_abstract": "Knowledge graph reasoning (KGR), which answers complex, logical queries over\nlarge knowledge graphs (KGs), represents an important artificial intelligence\ntask with a range of applications. Many KGs require extensive domain expertise\nand engineering effort to build and are hence considered proprietary within\norganizations and enterprises. Yet, spurred by their commercial and research\npotential, there is a growing trend to make KGR systems, (partially) built upon\nprivate KGs, publicly available through reasoning APIs.\n  The inherent tension between maintaining the confidentiality of KGs while\nensuring the accessibility to KGR systems motivates our study of KG extraction\nattacks: the adversary aims to \"steal\" the private segments of the backend KG,\nleveraging solely black-box access to the KGR API. Specifically, we present\nKGX, an attack that extracts confidential sub-KGs with high fidelity under\nlimited query budgets. At a high level, KGX progressively and adaptively\nqueries the KGR API and integrates the query responses to reconstruct the\nprivate sub-KG. This extraction remains viable even if any query responses\nrelated to the private sub-KG are filtered. We validate the efficacy of KGX\nagainst both experimental and real-world KGR APIs. Interestingly, we find that\ntypical countermeasures (e.g., injecting noise into query responses) are often\nineffective against KGX. Our findings suggest the need for a more principled\napproach to developing and deploying KGR systems, as well as devising new\ndefenses against KG extraction attacks.",
      "generated_abstract": "Graphs (KGs) are a powerful resource for enhancing search and\nknowledge discovery. However, their privacy and security risks, as well as the\nlack of transparency and accountability of the systems that process KGs,\nthreaten their widespread adoption. This paper explores the vulnerabilities of\nthe KG API ecosystem, focusing on the integration of the knowledge extraction\n(KE) APIs into search engines. We analyze the risks that exist in the current\narchitecture of the KE APIs, including the lack of privacy protections and\nlack of transparency and accountability. We also examine the vulnerabilities\nthat exist in the KE APIs themselves, including data leakage and data\ncorruption. Additionally, we explore the implications of these vulnerabilities\non the integrity and reliability of search results. By examining the\narchitecture and vulnerabilities of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12962962962962962,
          "p": 0.2876712328767123,
          "f": 0.17872339997247635
        },
        "rouge-2": {
          "r": 0.013761467889908258,
          "p": 0.02727272727272727,
          "f": 0.018292678468918397
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.2465753424657534,
          "f": 0.15319148507885932
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/CE/2503.06647v1",
      "true_abstract": "Accurate food intake monitoring is crucial for maintaining a healthy diet and\npreventing nutrition-related diseases. With the diverse range of foods consumed\nacross various cultures, classic food classification models have limitations\ndue to their reliance on fixed-sized food datasets. Studies show that people\nconsume only a small range of foods across the existing ones, each consuming a\nunique set of foods. Existing class-incremental models have low accuracy for\nthe new classes and lack personalization. This paper introduces a personalized,\nclass-incremental food classification model designed to overcome these\nchallenges and improve the performance of food intake monitoring systems. Our\napproach adapts itself to the new array of food classes, maintaining\napplicability and accuracy, both for new and existing classes by using\npersonalization. Our model's primary focus is personalization, which improves\nclassification accuracy by prioritizing a subset of foods based on an\nindividual's eating habits, including meal frequency, times, and locations. A\nmodified version of DSN is utilized to expand on the appearance of new food\nclasses. Additionally, we propose a comprehensive framework that integrates\nthis model into a food intake monitoring system. This system analyzes meal\nimages provided by users, makes use of a smart scale to estimate food weight,\nutilizes a nutrient content database to calculate the amount of each\nmacro-nutrient, and creates a dietary user profile through a mobile\napplication. Finally, experimental evaluations on two new benchmark datasets\nFOOD101-Personal and VFN-Personal, personalized versions of well-known datasets\nfor food classification, are conducted to demonstrate the effectiveness of our\nmodel in improving the classification accuracy of both new and existing\nclasses, addressing the limitations of both conventional and class-incremental\nfood classification models.",
      "generated_abstract": "r proposes a personalized incremental context-aware food classifier\nfor incremental classification of food intake data. The incremental context\nrepresents the contextual information about the current time-step, which\nincludes the time of day, the current weather condition, and the location of\nthe user. The context-aware food classifier is built using a two-stage\nframework. In the first stage, the model is trained to classify different food\ntypes from the food images. In the second stage, the model is fine-tuned to\nclassify the new food images based on the current context information. The\nproposed model achieves an average accuracy of 91.15% in the context-aware\nincremental classification task. This accuracy is 2.3% higher than the\nstate-of-the-art model, achieving a gain of 2.3% in the classification\naccuracy.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1509433962264151,
          "p": 0.34285714285714286,
          "f": 0.2096069826547931
        },
        "rouge-2": {
          "r": 0.035856573705179286,
          "p": 0.08108108108108109,
          "f": 0.04972375265391814
        },
        "rouge-l": {
          "r": 0.13836477987421383,
          "p": 0.3142857142857143,
          "f": 0.19213973374649618
        }
      }
    },
    {
      "paper_id": "cs.FL.cs/FL/2503.05006v1",
      "true_abstract": "Markov decision process over vector addition system with states (VASS MDP) is\na finite state model combining non-deterministic and probabilistic behavior,\naugmented with non-negative integer counters that can be incremented or\ndecremented during each state transition. VASS MDPs can be used as abstractions\nof probabilistic programs with many decidable properties. In this paper, we\ndevelop techniques for analyzing the asymptotic behavior of VASS MDPs. That is,\nfor every initial configuration of size \\(n\\), we consider the number of\ntransitions needed to reach a configuration with some counter negative. We show\nthat given a strongly connected VASS MDP there either exists an integer \\(k\\leq\n2^d\\cdot 3^{|T|} \\), where \\(d \\) is the dimension and \\(|T|\\) the number of\ntransitions of the VASS MDP, such that for all \\(\\epsilon>0 \\) and all\nsufficiently large \\(n\\) it holds that the complexity of the VASS MDP lies\nbetween \\(n^{k-\\epsilon} \\) and \\(n^{k+\\epsilon} \\) with probability at least\n\\(1-\\epsilon \\), or it holds for all \\(\\epsilon>0 \\) and all sufficiently large\n\\(n\\) that the complexity of the VASS MDP is at least \\(2^{n^{1-\\epsilon}} \\)\nwith probability at least \\(1-\\epsilon \\). We show that it is decidable which\ncase holds and the \\(k\\) is computable in time polynomial in the size of the\nconsidered VASS MDP. We also provide a full classification of asymptotic\ncomplexity for VASS Markov chains.",
      "generated_abstract": "We present an efficient method for analyzing the value of a state-dependent\npolynomial under a given Markov decision process. We obtain a high-level\npolynomial-time algorithm that computes the value of a given state for any\ngiven MDP, given a polynomial-time algorithm that computes the value of a given\nstate for an arbitrary MDP. We show that the value of a state for a given\npolynomial MDP can be expressed as the value of a state for the value-independent\nMDP with the same initial state and transition kernel. We present the\ncomputational complexity of our algorithm and discuss its impact on the\ncomputation of the value of a state for a given MDP.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20491803278688525,
          "p": 0.5102040816326531,
          "f": 0.29239765672993406
        },
        "rouge-2": {
          "r": 0.06382978723404255,
          "p": 0.1518987341772152,
          "f": 0.08988763628273666
        },
        "rouge-l": {
          "r": 0.18032786885245902,
          "p": 0.4489795918367347,
          "f": 0.25730993743168845
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.06308v1",
      "true_abstract": "Computable phenotypes are used to characterize patients and identify outcomes\nin studies conducted using healthcare claims and electronic health record data.\nChart review studies establish reference labels against which computable\nphenotypes are compared to understand their measurement characteristics, the\nquantity of interest, for instance the positive predictive value. We describe a\nmethod to adaptively evaluate a quantity of interest over sequential samples of\ncharts, with the goal to minimize the number of charts reviewed. With the help\nof a simultaneous confidence band, we stop the reviewing once the confidence\nband meets a pre-specified stopping threshold. The contribution of this article\nis threefold. First, we tested the use of an adaptive approach called Neyman's\nsampling of charts versus random or stratified random sampling. Second, we\npropose frequentist confidence bands and Bayesian credible intervals to\nsequentially evaluate the quantity of interest. Third, we propose a tool to\npredict the stopping time (defined as the number of charts reviewed) at which\nthe chart review would be complete. We observe that Bayesian credible intervals\nproved to be tighter than its frequentist confidence band counterparts.\nMoreover, we observe that simple random sampling is often performing similarly\nto Neyman's sampling.",
      "generated_abstract": "g charts is a critical aspect of healthcare quality improvement.\nHowever, traditional chart validation methods, such as chart abstraction and\nchart sampling, are often time-consuming and expensive. To address these\nlimitations, we propose Adaptive Multi-Wave Sampling (AMWS), a novel\nmulti-wave sampling approach that effectively extracts clinically relevant\ndata from the chart. AMWS leverages a multi-wave sampling strategy to sample\nmultiple waveforms from the chart, enabling efficient chart validation. By\nsampling multiple waveforms, AMWS effectively captures information that is\noften missing in conventional chart validation methods, such as patient\nphysical examination, vital signs, and clinical history. Additionally, AMWS\nenables sample selection based on clinical relevance, providing more\ntargeted insights into patient care. AMWS is demonstrated through a case study\ninvolving a patient with a seizure disorder",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16129032258064516,
          "p": 0.2222222222222222,
          "f": 0.18691588297667933
        },
        "rouge-2": {
          "r": 0.017045454545454544,
          "p": 0.02564102564102564,
          "f": 0.02047781090239952
        },
        "rouge-l": {
          "r": 0.1532258064516129,
          "p": 0.2111111111111111,
          "f": 0.17757008858415596
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.09865v1",
      "true_abstract": "We are concerned with the likelihood ratio tests in the $p_0$ model for\ntesting degree heterogeneity in directed networks. It is an exponential family\ndistribution on directed graphs with the out-degree sequence and the in-degree\nsequence as naturally sufficient statistics. For two growing dimensional null\nhypotheses: a specified null $H_{0}: \\theta_{i}=\\theta_{i}^{0}$ for\n$i=1,\\ldots,r$ and a homogenous null $H_{0}: \\theta_{1}=\\cdots=\\theta_{r}$, we\nreveal high dimensional Wilks' phenomena that the normalized log-likelihood\nratio statistic,\n$[2\\{\\ell(\\widehat{\\bs\\theta})-\\ell(\\widehat{\\bs\\theta}^{0})\\}-r]/(2r)^{1/2}$,\nconverges in distribution to a standard normal distribution as $r\\rightarrow\n\\infty$. Here, $\\ell( \\bs{\\theta})$ is the log-likelihood function,\n$\\widehat{\\bs{\\theta}}$ is the unrestricted maximum likelihood estimator (MLE)\nof $\\bs\\theta$, and $\\widehat{\\bs{\\theta}}^0$ is the restricted MLE for\n$\\bs\\theta$ under the null $H_{0}$. For the homogenous null $H_0:\n\\theta_1=\\cdots=\\theta_r$ with a fixed $r$, we establish the Wilks-type theorem\nthat $2\\{\\ell(\\widehat{\\bs{\\theta}}) - \\ell(\\widehat{\\bs{\\theta}}^0)\\}$\nconverges in distribution to a chi-square distribution with $r-1$ degrees of\nfreedom as $n\\rightarrow \\infty$, not depending on the nuisance parameters.\nThese results extend a recent work by \\cite{yan2023likelihood} to directed\ngraphs. Simulation studies and real data analyses illustrate the theoretical\nresults.",
      "generated_abstract": "We consider the problem of testing whether a given directed graph has degree\nheterogeneity. We focus on the case of a single edge per node and show that\ntesting this property can be reduced to testing a random variable. We then\npropose an asymptotic test of this random variable and establish its\nasymptotic properties. In particular, we prove that the test has a\n$(\\log n)^{O(1)}$-size null distribution. Finally, we propose a test that can\nbe computed in $O(\\log n)$ time. Our test is based on an algorithm proposed by\nAgarwal and Kumar [JRSS B(2), 62(3):303-312, 2022",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17699115044247787,
          "p": 0.30303030303030304,
          "f": 0.22346368249555268
        },
        "rouge-2": {
          "r": 0.018518518518518517,
          "p": 0.03225806451612903,
          "f": 0.023529407130796758
        },
        "rouge-l": {
          "r": 0.168141592920354,
          "p": 0.2878787878787879,
          "f": 0.21229049813801082
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08970v1",
      "true_abstract": "Previous studies on echocardiogram segmentation are focused on the left\nventricle in parasternal long-axis views. In this study, deep-learning models\nwere evaluated on the segmentation of the ventricles in parasternal short-axis\nechocardiograms (PSAX-echo). Segmentation of the ventricles in complementary\nechocardiogram views will allow the computation of important metrics with the\npotential to aid in diagnosing cardio-pulmonary diseases and other\ncardiomyopathies. Evaluating state-of-the-art models with small datasets can\nreveal if they improve performance on limited data. PSAX-echo were performed on\n33 volunteer women. An experienced cardiologist identified end-diastole and\nend-systole frames from 387 scans, and expert observers manually traced the\ncontours of the cardiac structures. Traced frames were pre-processed and used\nto create labels to train 2 specific-domain (Unet-Resnet101 and Unet-ResNet50),\nand 4 general-domain (3 Segment Anything (SAM) variants, and the Detectron2)\ndeep-learning models. The performance of the models was evaluated using the\nDice similarity coefficient (DSC), Hausdorff distance (HD), and difference in\ncross-sectional area (DCSA). The Unet-Resnet101 model provided superior\nperformance in the segmentation of the ventricles with 0.83, 4.93 pixels, and\n106 pixel2 on average for DSC, HD, and DCSA respectively. A fine-tuned MedSAM\nmodel provided a performance of 0.82, 6.66 pixels, and 1252 pixel2, while the\nDetectron2 model provided 0.78, 2.12 pixels, and 116 pixel2 for the same\nmetrics respectively. Deep-learning models are suitable for the segmentation of\nthe left and right ventricles in PSAX-echo. This study demonstrated that\nspecific-domain trained models such as Unet-ResNet provide higher accuracy for\necho segmentation than general-domain segmentation models when working with\nsmall and locally acquired datasets.",
      "generated_abstract": "icular septal defect (VSD) is a common congenital heart defect\nand is associated with a high incidence of cardiac arrhythmias, cardiac\ndysrhythmias, and sudden death. The ventricular septal defect (VSD) is a common\ncongenital heart defect and is associated with a high incidence of cardiac\narrhythmias, cardiac dysrhythmias, and sudden death. To improve the\ndiagnosis of VSD, this study evaluates the performance of state-of-the-art\ndeep learning models in the segmentation of the heart ventricles in parasternal\nshort-axis echocardiograms. The study was conducted on 272 patients with\nVSDs. The researchers evaluated the performance of deep learning models using\nthe Dice score, mean square error, and the area under the curve (",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15625,
          "p": 0.423728813559322,
          "f": 0.22831049834657327
        },
        "rouge-2": {
          "r": 0.05084745762711865,
          "p": 0.14634146341463414,
          "f": 0.0754716942858275
        },
        "rouge-l": {
          "r": 0.15,
          "p": 0.4067796610169492,
          "f": 0.2191780782552491
        }
      }
    },
    {
      "paper_id": "q-bio.NC.eess/SP/2503.02685v1",
      "true_abstract": "Precise parcellation of functional networks (FNs) of early developing human\nbrain is the fundamental basis for identifying biomarker of developmental\ndisorders and understanding functional development. Resting-state fMRI\n(rs-fMRI) enables in vivo exploration of functional changes, but adult FN\nparcellations cannot be directly applied to the neonates due to incomplete\nnetwork maturation. No standardized neonatal functional atlas is currently\navailable. To solve this fundamental issue, we propose TReND, a novel and fully\nautomated self-supervised transformer-autoencoder framework that integrates\nregularized nonnegative matrix factorization (RNMF) to unveil the FNs in\nneonates. TReND effectively disentangles spatiotemporal features in voxel-wise\nrs-fMRI data. The framework integrates confidence-adaptive masks into\ntransformer self-attention layers to mitigate noise influence. A self\nsupervised decoder acts as a regulator to refine the encoder's latent\nembeddings, which serve as reliable temporal features. For spatial coherence,\nwe incorporate brain surface-based geodesic distances as spatial encodings\nalong with functional connectivity from temporal features. The TReND clustering\napproach processes these features under sparsity and smoothness constraints,\nproducing robust and biologically plausible parcellations. We extensively\nvalidated our TReND framework on three different rs-fMRI datasets: simulated,\ndHCP and HCP-YA against comparable traditional feature extraction and\nclustering techniques. Our results demonstrated the superiority of the TReND\nframework in the delineation of neonate FNs with significantly better spatial\ncontiguity and functional homogeneity. Collectively, we established TReND, a\nnovel and robust framework, for neonatal FN delineation. TReND-derived neonatal\nFNs could serve as a neonatal functional atlas for perinatal populations in\nhealth and disease.",
      "generated_abstract": "brain functional networks (FBNs) represent a critical window of\ndecision-making for developing a robust neonatal brain disorder prognosis.\nDespite the advances in high-throughput neonatal brain imaging, the accurate\ndelineation of neonatal FBN remains challenging. In this paper, we propose a\nnovel method for neonatal FBN delineation based on the transformer, an\nattention-based architecture. By leveraging the transformer, we propose a\nnovel FBN delineation framework, TReND, which integrates the proposed\nneurocognitive score (NCS) and the FBN-specific score (FBS). The NCS and FBS\ncapture the network connectivity and the cognitive status of the neonate,\nrespectively, while the transformer encodes the information through multi-layer\nfeedforward neural",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1566265060240964,
          "p": 0.36619718309859156,
          "f": 0.21940927850380107
        },
        "rouge-2": {
          "r": 0.02586206896551724,
          "p": 0.06382978723404255,
          "f": 0.03680981184688969
        },
        "rouge-l": {
          "r": 0.14457831325301204,
          "p": 0.3380281690140845,
          "f": 0.20253164137299942
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/PL/2503.05849v1",
      "true_abstract": "The quality of software products tends to correlate with the quality of the\nabstractions adopted early in the design process. Acknowledging this tendency\nhas led to the development of various tools and methodologies for modeling\nsystems thoroughly before implementing them. However, creating effective\nabstract models of domain problems is difficult, especially if the models are\nalso expected to exhibit qualities such as intuitiveness, being seamlessly\nintegrable with other models, or being easily translatable into code.\n  This thesis describes Conceptual, a DSL for modeling the behavior of software\nsystems using self-contained and highly reusable units of functionally known as\nconcepts. The language's syntax and semantics are formalized based on previous\nwork. Additionally, the thesis proposes a strategy for mapping language\nconstructs from Conceptual into the Alloy modeling language. The suggested\nstrategy is then implemented with a simple compiler, allowing developers to\naccess and utilize Alloy's existing analysis tools for program reasoning.\n  The utility and expressiveness of Conceptual is demonstrated qualitatively\nthrough several practical case studies. Using the implemented compiler, a few\nerroneous specifications are identified in the literature. Moreover, the thesis\nestablishes preliminary tool support in the Visual Studio Code IDE.",
      "generated_abstract": "This paper presents a proof-of-concept tool, DSL, designed to facilitate\nthe specification of data structures using the Dryad Semantics Language,\nDryadDL. DSL is an interactive tool designed to enable users to express\nstructural concepts in a human-readable way. This is a significant step in the\ndirection of establishing the DryadDL as a widely accepted data structure\nspecification language. The paper describes the design of DSL and its features\nand discusses its implementation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12878787878787878,
          "p": 0.34,
          "f": 0.18681318282816095
        },
        "rouge-2": {
          "r": 0.01639344262295082,
          "p": 0.04285714285714286,
          "f": 0.023715411017201378
        },
        "rouge-l": {
          "r": 0.12878787878787878,
          "p": 0.34,
          "f": 0.18681318282816095
        }
      }
    },
    {
      "paper_id": "math.LO.math/LO/2503.10528v1",
      "true_abstract": "The first-order model theory of modules has been studied for decades. More\nrecently, the model theoretic study of nonelementary classes of\nmodules--especially Abstract Elementary Classes of modules--has produced\ninteresting results. This survey aims to discuss these recent results and give\nan introduction to the framework of Abstract Elementary Classes for module\ntheorists.",
      "generated_abstract": "In this paper, we introduce a novel and highly abstract approach to\nelementary classes in groups and rings. In particular, we show that elementary\nclasses in a group are in one-to-one correspondence with cosets of a certain\nsubring, and that elementary classes in a ring are in one-to-one correspondence\nwith the set of all left ideals. These results are proved in a manner that is\nboth transparent and elegant. We apply these results to prove several\ntheorems about elementary classes in groups and rings, including a proof of the\nstrong B\\'ezout theorem for groups.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.14035087719298245,
          "f": 0.16494844876182393
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.14035087719298245,
          "f": 0.16494844876182393
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2412.13040v1",
      "true_abstract": "Epithelial tissues are continuously exposed to cyclic stretch. Physiological\nstretching has been found to regulate soft tissue function at the molecular,\ncellular, and tissue scales, allowing tissues to preserve their homeostasis and\nadapt to challenges. In contrast, dysregulated or pathological stretching can\ninduce damage and tissue fragilisation. Many mechanisms have been described for\nthe repair of epithelial tissues across a range of time-scales. In this review,\nwe present the timescales of (i) physiological cyclic loading regimes, (ii)\nstrain-regulated remodelling and damage accumulation, and (iii) repair\nmechanisms in epithelial tissues. We discuss how fatigue in biological tissues\ndiffers from synthetic materials, in that damage can be partially or fully\nreversed by repair mechanisms acting on timescales shorter than cyclic loading.\nWe highlight that timescales are critical to understanding the interplay\nbetween damage and repair in tissues that experience cyclic loading, opening up\nnew avenues for exploring soft tissue homeostasis.",
      "generated_abstract": "ding the cellular mechanisms that govern the tissue response to\nepithelial loading, such as cyclic loading, remains a major challenge in\nbiomechanics and wound healing. This paper examines the effects of cyclic\nloading on the control of cellular tissue integrity. We model a single cell as\na linear elastic beam, subject to cyclic loading. We study the impact of\ncyclotronic loading on the control of cellular tissue integrity. We demonstrate\nthat the loading response of cells to cyclic loading is different from that\nfound in linear elastic beams, with a critical difference in the role of\nshear-thinning fluids. We show that the response of cells to cyclic loading\ndepends on the ratio of shear stress to the cyclic stress, and that this\ndependence is different from that of linear elastic beams. In addition, we\nshow",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21875,
          "p": 0.3088235294117647,
          "f": 0.25609755612135643
        },
        "rouge-2": {
          "r": 0.028368794326241134,
          "p": 0.036036036036036036,
          "f": 0.03174602681689419
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.23529411764705882,
          "f": 0.1951219463652589
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.13076v1",
      "true_abstract": "Machine learning predictions are typically interpreted as the sum of\ncontributions of predictors. Yet, each out-of-sample prediction can also be\nexpressed as a linear combination of in-sample values of the predicted\nvariable, with weights corresponding to pairwise proximity scores between\ncurrent and past economic events. While this dual route leads nowhere in some\ncontexts (e.g., large cross-sectional datasets), it provides sparser\ninterpretations in settings with many regressors and little training data-like\nmacroeconomic forecasting. In this case, the sequence of contributions can be\nvisualized as a time series, allowing analysts to explain predictions as\nquantifiable combinations of historical analogies. Moreover, the weights can be\nviewed as those of a data portfolio, inspiring new diagnostic measures such as\nforecast concentration, short position, and turnover. We show how weights can\nbe retrieved seamlessly for (kernel) ridge regression, random forest, boosted\ntrees, and neural networks. Then, we apply these tools to analyze post-pandemic\nforecasts of inflation, GDP growth, and recession probabilities. In all cases,\nthe approach opens the black box from a new angle and demonstrates how machine\nlearning models leverage history partly repeating itself.",
      "generated_abstract": "igate the dual interpretation of machine learning forecasts. In\nmachine learning, the model is trained to predict future values, and the model\nperforms a forecasting task. We propose to interpret the forecast as a\nprediction of future values and the model as a predictor of future values. We\npropose to measure this dual interpretation by a distance between the model\nand the forecast. The distance is defined as the root mean squared error\n(RMSE) of the forecast minus the RMSE of the model. We derive the RMSE of the\ndual interpretation of the forecast and show that it is a convex function of\nthe RMSE of the dual interpretation of the model. We prove that the RMSE of the\ndual interpretation of the forecast is equal to the RMSE of the model, and\ntherefore, the RMSE of the dual interpretation of the forecast is equal to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14388489208633093,
          "p": 0.37037037037037035,
          "f": 0.20725388198018746
        },
        "rouge-2": {
          "r": 0.02824858757062147,
          "p": 0.05555555555555555,
          "f": 0.0374531790514671
        },
        "rouge-l": {
          "r": 0.11510791366906475,
          "p": 0.2962962962962963,
          "f": 0.16580310477811494
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2411.09517v1",
      "true_abstract": "We study a setting where agents use no-regret learning algorithms to\nparticipate in repeated auctions. \\citet{kolumbus2022auctions} showed, rather\nsurprisingly, that when bidders participate in second-price auctions using\nno-regret bidding algorithms, no matter how large the number of interactions\n$T$ is, the runner-up bidder may not converge to bidding truthfully. Our first\nresult shows that this holds for \\emph{general deterministic} truthful\nauctions. We also show that the ratio of the learning rates of the bidders can\n\\emph{qualitatively} affect the convergence of the bidders. Next, we consider\nthe problem of revenue maximization in this environment. In the setting with\nfully rational bidders, \\citet{myerson1981optimal} showed that revenue can be\nmaximized by using a second-price auction with reserves.We show that, in stark\ncontrast, in our setting with learning bidders, \\emph{randomized} auctions can\nhave strictly better revenue guarantees than second-price auctions with\nreserves, when $T$ is large enough. Finally, we study revenue maximization in\nthe non-asymptotic regime. We define a notion of {\\em auctioneer regret}\ncomparing the revenue generated to the revenue of a second price auction with\ntruthful bids. When the auctioneer has to use the same auction throughout the\ninteraction, we show an (almost) tight regret bound of $\\smash{\\widetilde\n\\Theta(T^{3/4})}.$ If the auctioneer can change auctions during the\ninteraction, but in a way that is oblivious to the bids, we show an (almost)\ntight bound of $\\smash{\\widetilde \\Theta(\\sqrt{T})}.$",
      "generated_abstract": "auctions in which bidders can learn the true valuation of an item\nthrough their own observations. The agent's goal is to maximize the\nexpected payment, subject to the constraint that the payment is nonnegative.\n  We consider two types of auctions. The first is a two-sided auction, in\nwhich the agent can bid on both the item and the seller. The second is a\nsingle-sided auction, in which the agent can bid only on the item. In both\ncases, we allow the seller to bid on the item, and we assume that the seller\nknows the agent's valuation of the item.\n  We show that the expected payment of an agent who uses a truthful truthful\nmechanism is $O(\\sqrt{\\log\\frac{1}{\\delta}})$, where $\\delta$ is the\nexpected ratio of the true valuations of the item and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.3333333333333333,
          "f": 0.22222221777777784
        },
        "rouge-2": {
          "r": 0.03365384615384615,
          "p": 0.06666666666666667,
          "f": 0.04472843004623956
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.2857142857142857,
          "f": 0.19047618603174613
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.06913v1",
      "true_abstract": "Given a finite collection of stochastic alternatives, we study the problem of\nsequentially allocating a fixed sampling budget to identify the optimal\nalternative with a high probability, where the optimal alternative is defined\nas the one with the smallest value of extreme tail risk. We particularly\nconsider a situation where these alternatives generate heavy-tailed losses\nwhose probability distributions are unknown and may not admit any specific\nparametric representation. In this setup, we propose data-driven sequential\nsampling policies that maximize the rate at which the likelihood of falsely\nselecting suboptimal alternatives decays to zero. We rigorously demonstrate the\nsuperiority of the proposed methods over existing approaches, which is further\nvalidated via numerical studies.",
      "generated_abstract": "The traditional approach of tail risk mitigation is to sample only from the\ntail region of the distribution. In this paper, we propose a novel method to\nsample from the entire distribution, using sequential sampling with a tail risk\nmeasure. We show that our method can be used to perform tail risk\nmitigation in a non-parametric fashion, and the resulting method is called\n\"data-driven sequential sampling\". This method is flexible in that it can be\nused to mitigate tail risk from any distribution. We illustrate the\napplicability of our method by applying it to a real data set and to an\nalternative method that has been used in the literature. Finally, we present an\nillustrative simulation study to illustrate the performance of our method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23255813953488372,
          "p": 0.2898550724637681,
          "f": 0.258064511189178
        },
        "rouge-2": {
          "r": 0.05504587155963303,
          "p": 0.05504587155963303,
          "f": 0.055045866559633484
        },
        "rouge-l": {
          "r": 0.23255813953488372,
          "p": 0.2898550724637681,
          "f": 0.258064511189178
        }
      }
    },
    {
      "paper_id": "eess.SP.stat/TH/2502.17981v1",
      "true_abstract": "This work deals with the generation of theoretical correlation matrices with\nspecific sparsity patterns, associated to graph structures. We present a novel\napproach based on convex optimization, offering greater flexibility compared to\nexisting techniques, notably by controlling the mean of the entry distribution\nin the generated correlation matrices. This allows for the generation of\ncorrelation matrices that better represent realistic data and can be used to\nbenchmark statistical methods for graph inference.",
      "generated_abstract": "e a convex optimization approach to generate correlation matrices\nwith structured graph structures, such as random walks or random networks.\nGenerating correlation matrices with structured graph structures has many\napplications in fields such as statistical physics, machine learning, and\nstatistical physics. Existing methods typically generate correlation matrices\nby solving an optimization problem that involves calculating the correlation\nmatrix element between neighboring points in a graph. This approach can be\ncomputationally expensive, particularly for large graphs. In this paper, we\npropose a novel approach to generating correlation matrices by first constructing\nthe correlation matrix by solving a convex optimization problem that\nminimizes the total edge weight in the graph. Then, we use this matrix as a\nsolution to another convex optimization problem that minimizes the edge weight\nin the graph. This approach leads to a much faster convergence rate than\nsolving the original optimization problem. We validate our approach through\nnumerical simulations and provide",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.4,
          "p": 0.25,
          "f": 0.3076923029585799
        },
        "rouge-2": {
          "r": 0.1044776119402985,
          "p": 0.056910569105691054,
          "f": 0.0736842059606651
        },
        "rouge-l": {
          "r": 0.38181818181818183,
          "p": 0.23863636363636365,
          "f": 0.29370628897256595
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.05591v1",
      "true_abstract": "The article proposes a computational approach that can generate a descending\norder of the IUPAC-notated functional groups based on their importance for a\ngiven case study. Thus, a reduced list of functional groups could be obtained\nfrom which drug discovery can be successfully initiated. The approach,\napplicable to any study case with sufficient data, was demonstrated using a\nPubChem bioassay focused on TDP1 inhibitors. The Scikit Learn interpretation of\nthe Random Forest Classifier (RFC) algorithm was employed. The machine learning\n(ML) model RFC obtained 70.9% accuracy, 73.1% precision, 66.1% recall, 69.4% F1\nand 70.8% receiver-operating characteristic (ROC). In addition to the main\nstudy, the CID_SID ML model was developed, which, using only the PubChem\ncompound and substance identifiers (CIDs and SIDs) data, can predict with 85.2%\naccuracy, 94.2% precision, 75% precision, F1 of 83.5% F1 and 85.2% ROC whether\na compound is a TDP1 inhibitor.",
      "generated_abstract": "ification of drug targets is crucial for drug discovery and\nchemical biology. Functional group ranking (FGR) has been widely used for\nidentifying drug targets in computational chemistry and drug design. However,\ncurrently, there is no FGR method to rank drug targets based on their\nprotein-protein interaction (PPI) interactions, and this limits the applicability\nof FGR in drug discovery. To address this limitation, we propose a novel FGR\nmethod based on IUPAC name analysis (INA), which is an unsupervised method for\ngrouping compounds into functional groups based on their chemical structures.\nThis method integrates the ranking power of FGR with the classification power\nof INA to provide a novel method for ranking drug targets based on their\nprotein-protein interactions. The INA-based FGR is then used to rank the\ntargets and identify potential drug targets for TDP1",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1792452830188679,
          "p": 0.24675324675324675,
          "f": 0.20765026834960743
        },
        "rouge-2": {
          "r": 0.03355704697986577,
          "p": 0.04310344827586207,
          "f": 0.037735844134140904
        },
        "rouge-l": {
          "r": 0.16037735849056603,
          "p": 0.22077922077922077,
          "f": 0.18579234485233972
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/EC/2502.17044v1",
      "true_abstract": "Supply chain disruptions constitute an often underestimated risk for\nfinancial stability. As in financial networks, systemic risks in production\nnetworks arises when the local failure of one firm impacts the production of\nothers and might trigger cascading disruptions that affect significant parts of\nthe economy. Here, we study how systemic risk in production networks translates\ninto financial systemic risk through a mechanism where supply chain contagion\nleads to correlated bank-firm loan defaults. We propose a financial\nstress-testing framework for micro- and macro-prudential applications that\nfeatures a national firm level supply chain network in combination with\ninterbank network layers. The model is calibrated by using a unique data set\nincluding about 1 million firm-level supply links, practically all bank-firm\nloans, and all interbank loans in a small European economy. As a showcase we\nimplement a real COVID-19 shock scenario on the firm level. This model allows\nus to study how the disruption dynamics in the real economy can lead to\ninterbank solvency contagion dynamics. We estimate to what extent this\namplifies financial systemic risk. We discuss the relative importance of these\ncontagion channels and find an increase of interbank contagion by 70% when\nproduction network contagion is present. We then examine the financial systemic\nrisk firms bring to banks and find an increase of up to 28% in the presence of\nthe interbank contagion channel. This framework is the first financial systemic\nrisk model to take agent-level dynamics of the production network and shocks of\nthe real economy into account which opens a path for directly, and event-driven\nunderstanding of the dynamical interaction between the real economy and\nfinancial systems.",
      "generated_abstract": "r proposes a data-driven econo-financial stress-testing\nframework to estimate the effect of supply chain networks on financial\nsystemic risk. Our framework integrates the macroeconomic shocks and the\noperational risk exposures of the supply chain network into the stress-testing\nprocess, focusing on the financial stability risks of supply chains. The paper\ndevelops a novel framework based on the macroeconomic shocks, the operational\nrisks of supply chains, and the network structure of the supply chain. The\nframework uses the network structure of the supply chain and macroeconomic\nshocks to calculate the network-specific financial stability risks, and then\ncalculates the operational risk-specific financial stability risks for each\nsupply chain. The paper applies the framework to the supply chain network of\nAlibaba Group, using the operational risk-specific financial stability risks",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.4444444444444444,
          "f": 0.24242423845730032
        },
        "rouge-2": {
          "r": 0.0375,
          "p": 0.0989010989010989,
          "f": 0.05438066066574815
        },
        "rouge-l": {
          "r": 0.13194444444444445,
          "p": 0.35185185185185186,
          "f": 0.19191918795224985
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2411.03699v4",
      "true_abstract": "We study a multivariate autoregressive stochastic volatility model for the\nfirst 3 principal components (level, slope, curvature) of 10 series of\nzero-coupon Treasury bond rates with maturities from 1 to 10 years. We fit this\nmodel using monthly data from 1990. Unlike classic models with hidden\nstochastic volatility, here it is observed as VIX: the volatility index for the\nS&P 500 stock market index. Surprisingly, this stock index volatility works for\nTreasury bonds, too. Next, we prove long-term stability and the Law of Large\nNumbers. We express total returns of zero-coupon bonds using these principal\ncomponents. We prove the Law of Large Numbers for these returns. All results\nare done for discrete and continuous time.",
      "generated_abstract": "We derive a method to compute zero-coupon Treasury rates and returns using\nthe volatility index of the S&P 500 stock index. We show that the method\npreserves the asymptotic distribution of the zero-coupon Treasury rates and\nreturns, and that it is consistent and asymptotically normal. We also derive\nthe covariance matrix of the zero-coupon Treasury rate and returns and\npresent simulations to compare with the method proposed by\n\\cite{Farmer_2022}.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24675324675324675,
          "p": 0.4634146341463415,
          "f": 0.32203389377046826
        },
        "rouge-2": {
          "r": 0.07476635514018691,
          "p": 0.13559322033898305,
          "f": 0.09638553758673268
        },
        "rouge-l": {
          "r": 0.2077922077922078,
          "p": 0.3902439024390244,
          "f": 0.27118643614334975
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.01352v1",
      "true_abstract": "Polarization, as a new optical imaging tool, has been explored to assist in\nthe diagnosis of pathology. Moreover, converting the polarimetric Mueller\nMatrix (MM) to standardized stained images becomes a promising approach to help\npathologists interpret the results. However, existing methods for\npolarization-based virtual staining are still in the early stage, and the\ndiffusion-based model, which has shown great potential in enhancing the\nfidelity of the generated images, has not been studied yet. In this paper, a\nRegulated Bridge Diffusion Model (RBDM) for polarization-based virtual staining\nis proposed. RBDM utilizes the bidirectional bridge diffusion process to learn\nthe mapping from polarization images to other modalities such as H\\&E and\nfluorescence. And to demonstrate the effectiveness of our model, we conduct the\nexperiment on our manually collected dataset, which consists of 18,000 paired\npolarization, fluorescence and H\\&E images, due to the unavailability of the\npublic dataset. The experiment results show that our model greatly outperforms\nother benchmark methods. Our dataset and code will be released upon acceptance.",
      "generated_abstract": "atrix imaging is a novel approach for staining multiple fluorescent\npolarized dyes in a single image. Mueller matrix imaging has the potential to\nreduce the number of staining steps by eliminating the need to stain multiple\nfluorescent dyes with different emission wavelengths. However, staining multiple\nfluorescent dyes with different emission wavelengths is a challenging task\nbecause the corresponding polarization signals are difficult to generate.\nTherefore, it is essential to estimate the polarization signals of the fluorescent\ndyes during the staining process. In this paper, we propose a diffusion-based\nalgorithm for virtual staining of fluorescent dyes in polarized mueller matrix\nimages. Our method leverages the Mueller matrix to compute the polarization\nsignals for each dye during the staining process, and then the polarization\nsignals are estimated using diffusion-based",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22413793103448276,
          "p": 0.37142857142857144,
          "f": 0.27956988777893405
        },
        "rouge-2": {
          "r": 0.025,
          "p": 0.0380952380952381,
          "f": 0.030188674460662923
        },
        "rouge-l": {
          "r": 0.20689655172413793,
          "p": 0.34285714285714286,
          "f": 0.25806451143484804
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2501.07737v1",
      "true_abstract": "Understanding how molecular changes caused by genetic variation drive disease\nrisk is crucial for deciphering disease mechanisms. However, interpreting\ngenome sequences is challenging because of the vast size of the human genome,\nand because its consequences manifest across a wide range of cells, tissues and\nscales -- spanning from molecular to whole organism level. Here, we present\nPhenformer, a multi-scale genetic language model that learns to generate\nmechanistic hypotheses as to how differences in genome sequence lead to\ndisease-relevant changes in expression across cell types and tissues directly\nfrom DNA sequences of up to 88 million base pairs. Using whole genome\nsequencing data from more than 150 000 individuals, we show that Phenformer\ngenerates mechanistic hypotheses about disease-relevant cell and tissue types\nthat match literature better than existing state-of-the-art methods, while\nusing only sequence data. Furthermore, disease risk predictors enriched by\nPhenformer show improved prediction performance and generalisation to diverse\npopulations. Accurate multi-megabase scale interpretation of whole genomes\nwithout additional experimental data enables both a deeper understanding of\nmolecular mechanisms involved in disease and improved disease risk prediction\nat the level of individuals.",
      "generated_abstract": "terpretation is the process of mapping genomic features into\nexplanatory concepts. Recent advances in large language models (LLMs) have\nenhanced the performance of genome interpretation, enabling the\nautomatic identification of functional modules, chromatin structure, and\ndisease associations. However, existing LLM-based genome interpretation\nmethods are limited by their dependence on the quality and structure of the\ntraining data. Here, we present a framework that integrates multiple\nmegabase-scale genome interpretation tasks. By incorporating the\ninterpretation of multiple genome-scale tasks into a single framework, we\nintroduce the Multi-Megabase Scale Genome Interpretation (MMSGI) task, which\ncomprises 10 tasks, each of which tests the performance of a LLM in\ninterpreting a genome. Through extensive experiments, we demonstrate that MMSGI",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.24096385542168675,
          "f": 0.1970443301414741
        },
        "rouge-2": {
          "r": 0.02247191011235955,
          "p": 0.03636363636363636,
          "f": 0.02777777305652086
        },
        "rouge-l": {
          "r": 0.14166666666666666,
          "p": 0.20481927710843373,
          "f": 0.16748767989516866
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.04278v1",
      "true_abstract": "This study addresses the challenge of access point (AP) and user equipment\n(UE) association in cell-free massive MIMO networks. It introduces a deep\nlearning algorithm leveraging Bidirectional Long Short-Term Memory cells and a\nhybrid probabilistic methodology for weight updating. This approach enhances\nscalability by adapting to variations in the number of UEs without requiring\nretraining. Additionally, the study presents a training methodology that\nimproves scalability not only with respect to the number of UEs but also to the\nnumber of APs. Furthermore, a variant of the proposed AP-UE algorithm ensures\nrobustness against pilot contamination effects, a critical issue arising from\npilot reuse in channel estimation. Extensive numerical results validate the\neffectiveness and adaptability of the proposed methods, demonstrating their\nsuperiority over widely used heuristic alternatives.",
      "generated_abstract": "rrent 5G cell-free massive multiple-input multiple-output (MIMO)\nsystems, the ultra-dense networks with large number of users (e.g., 100,000)\nare being considered. The users are connected through the base stations (BSs)\nthrough the user-centric cell-free massive multiple-input (UC-CF-MIMO)\ncommunication protocol. In this paper, we consider the case that the users\nassociate with the BSs sequentially, where the association process is\ndeterministic. To address this issue, we develop a recurrent neural network\n(RNN) based framework for the UC-CF-MIMO system, where the UC-CF-MIMO system is\nmodeled as a sequence of independent and identically distributed (i.i.d.)\nexperiments. The RNN is designed for the sequ",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12903225806451613,
          "p": 0.15384615384615385,
          "f": 0.140350872231456
        },
        "rouge-2": {
          "r": 0.017241379310344827,
          "p": 0.02040816326530612,
          "f": 0.01869158382042231
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.15384615384615385,
          "f": 0.140350872231456
        }
      }
    },
    {
      "paper_id": "q-bio.QM.quant-ph/2503.10510v1",
      "true_abstract": "Whole-slide image classification represents a key challenge in computational\npathology and medicine. Attention-based multiple instance learning (MIL) has\nemerged as an effective approach for this problem. However, the effect of\nattention mechanism architecture on model performance is not well-documented\nfor biomedical imagery. In this work, we compare different methods and\nimplementations of MIL, including deep learning variants. We introduce a new\nmethod using higher-dimensional feature spaces for deep MIL. We also develop a\nnovel algorithm for whole-slide image classification where extreme machine\nlearning is combined with attention-based MIL to improve sensitivity and reduce\ntraining complexity. We apply our algorithms to the problem of detecting\ncirculating rare cells (CRCs), such as erythroblasts, in peripheral blood. Our\nresults indicate that nonlinearities play a key role in the classification, as\nremoving them leads to a sharp decrease in stability in addition to a decrease\nin average area under the curve (AUC) of over 4%. We also demonstrate a\nconsiderable increase in robustness of the model with improvements of over 10%\nin average AUC when higher-dimensional feature spaces are leveraged. In\naddition, we show that extreme learning machines can offer clear improvements\nin terms of training efficiency by reducing the number of trained parameters by\na factor of 5 whilst still maintaining the average AUC to within 1.5% of the\ndeep MIL model. Finally, we discuss options of enriching the classical\ncomputing framework with quantum algorithms in the future. This work can thus\nhelp pave the way towards more accurate and efficient single-cell diagnostics,\none of the building blocks of precision medicine.",
      "generated_abstract": "de image (WSI) classification is a challenging task, involving\nattention-based multiple instance learning (AMIL). AMIL models are designed to\nmaximize the likelihood of correctly classifying a set of images in an\nunordered fashion, while minimizing the likelihood of incorrectly classifying\none of them. In this paper, we propose a novel attention-based AMIL model,\nExtreme Learning Machines (ELM), for WSI classification. ELM features a\ngeneralized linear model as a base classifier and employs extreme learning\nmethods to enhance model performance. The proposed model is trained using extreme\nlearning machines, a novel approach for learning high-dimensional data, and\nextremely large-scale inference (ELSI), a method for handling large datasets\nwithout the need for preprocessing. We validate our model on the TCGA breast\ncancer WSI dataset and compare it to the state-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20833333333333334,
          "p": 0.3763440860215054,
          "f": 0.2681992291293434
        },
        "rouge-2": {
          "r": 0.036885245901639344,
          "p": 0.07317073170731707,
          "f": 0.04904631706939731
        },
        "rouge-l": {
          "r": 0.16071428571428573,
          "p": 0.2903225806451613,
          "f": 0.2068965471370063
        }
      }
    },
    {
      "paper_id": "math.SG.math/SG/2503.09209v1",
      "true_abstract": "Time-dependent Stark-Zeeman systems describe the motion of an electron\nattracted by a proton subject to a magnetic and a time-dependent electric\nfield. For instance the study of the dynamics of a gateway around the moon\nwhich is subject to the joint attraction of the moon, the earth and the sun\nleads to time-dependent Stark-Zeeman systems. In the time-dependent case there\nis no preserved energy. Therefore collisions cannot be regularized by blowing\nup the energy hypersurface. A new regularization technique of blowing up\ninstead of the energy hypersurface the loop space was recently discovered by\nBarutello, Ortega, and Verzini. In this article we explain how this new\nregularization technique can be applied to the study of periodic orbits in\ntime-dependent planar Stark-Zeeman systems. Since the regularization by\nblowing-up the loop space is nonlocal the regularized periodic orbits will not\nsatisfy an ODE anymore but a delay equation.",
      "generated_abstract": "We study the dynamics of a time-dependent planar Stark-Zeeman system\nand the corresponding periodic orbits. We find the orbits and their\nperiods, and we determine the dynamics of the corresponding phase-space\nmanifold. We also prove that the system is unstable, and we describe the\ndynamics of the unstable manifolds of the periodic orbits.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.4827586206896552,
          "f": 0.2477876068039784
        },
        "rouge-2": {
          "r": 0.07751937984496124,
          "p": 0.23255813953488372,
          "f": 0.11627906601744199
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.4827586206896552,
          "f": 0.2477876068039784
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2503.08533v1",
      "true_abstract": "Advancements in audio foundation models (FMs) have fueled interest in\nend-to-end (E2E) spoken dialogue systems, but different web interfaces for each\nsystem makes it challenging to compare and contrast them effectively. Motivated\nby this, we introduce an open-source, user-friendly toolkit designed to build\nunified web interfaces for various cascaded and E2E spoken dialogue systems.\nOur demo further provides users with the option to get on-the-fly automated\nevaluation metrics such as (1) latency, (2) ability to understand user input,\n(3) coherence, diversity, and relevance of system response, and (4)\nintelligibility and audio quality of system output. Using the evaluation\nmetrics, we compare various cascaded and E2E spoken dialogue systems with a\nhuman-human conversation dataset as a proxy. Our analysis demonstrates that the\ntoolkit allows researchers to effortlessly compare and contrast different\ntechnologies, providing valuable insights such as current E2E systems having\npoorer audio quality and less diverse responses. An example demo produced using\nour toolkit is publicly available here:\nhttps://huggingface.co/spaces/Siddhant/Voice_Assistant_Demo.",
      "generated_abstract": "r presents ESPnet-SDS, a unified toolkit and demo for Spoken\ndDialogue systems. ESPnet-SDS is the first toolkit to provide a unified\ninterface to all major speech recognition and language models, enabling\neasier development and deployment of Spoken Dialogue Systems. It also\ndemonstrates the potential of integrating a variety of speech recognition and\nlanguage models. ESPnet-SDS is available at https://github.com/espnet/espnet-sds.\n  ESPnet-SDS has been tested with ESPnet versions 0.2.3, 0.3.0, and 0.4.0.\n  ESPnet-SDS includes a unified toolkit for speech recognition and language\nmodels, a demonstration application that provides a unified user experience\nacross speech recognition and language models, and a sample configuration for\na Spoken Dialogue",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16071428571428573,
          "p": 0.27692307692307694,
          "f": 0.20338982586102347
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13392857142857142,
          "p": 0.23076923076923078,
          "f": 0.1694915207762777
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2411.00071v2",
      "true_abstract": "Musculoskeletal (MSK) models offer a non-invasive way to understand\nbiomechanical loads on joints and tendons, which are difficult to measure\ndirectly. Variations in muscle strength, especially relative differences\nbetween muscles, significantly impact model outcomes. Typically, scaled generic\nMSK models use maximum isometric forces that are not adjusted for different\ndemographics, raising concerns about their accuracy. This review provides an\noverview on experimentally derived strength parameters, including physiological\ncross-sectional area (PCSA), muscle mass (Mm), and relative muscle mass (%Mm),\nwhich is the relative distribution of muscle mass across the leg. We analysed\ndifferences by age and sex, and compared open-source lower limb MSK model\nparameters with experimental data from 57 studies. Our dataset, with records\ndating back to 1884, shows that uniformly increasing all maximum isometric\nforces in MSK models does not capture key muscle ratio differences due to age\nand sex. Males have a higher proportion of muscle mass in the rectus femoris\nand semimembranosus muscles, while females have a greater relative muscle mass\nin the pelvic (gluteus maximus and medius) and ankle muscles (tibialis\nanterior, tibialis posterior, and extensor digitorum longus). Older adults have\na higher relative muscle mass in the gluteus medius, while younger individuals\nshow more in the gastrocnemius. Current MSK models do not accurately represent\nmuscle mass distribution for specific age or sex groups, and none of them\naccurately reflect female muscle mass distribution. Further research is needed\nto explore musculotendon age- and sex differences.",
      "generated_abstract": "leg exhibits a distinct structural and functional composition,\nwith a specialized musculoskeletal system for the lower limb that includes\nmultiple compartments and muscles. This musculoskeletal system is composed of\nthree distinct compartments: the lower limb, including the muscles, bones, and\nfibers, the pelvis, and the sacroiliac joints. This paper reviews the\ndifferences in muscle mass and compartmentalization between age- and sex-\nrelated groups. In addition, we provide a detailed analysis of the muscle mass\nof the lower limb in three age groups: 18-24 years, 25-39 years, and 40-64\nyears. This analysis includes a discussion of age- and sex-related differences\nin muscle mass and compartmentalization, including the impact of body\ncomposition,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.36923076923076925,
          "f": 0.21719456598349754
        },
        "rouge-2": {
          "r": 0.018779342723004695,
          "p": 0.041237113402061855,
          "f": 0.025806447313008
        },
        "rouge-l": {
          "r": 0.14102564102564102,
          "p": 0.3384615384615385,
          "f": 0.19909501847218536
        }
      }
    },
    {
      "paper_id": "cs.CL.q-fin/RM/2503.01886v1",
      "true_abstract": "This study presents a comparative analysis of deep learning methodologies\nsuch as BERT, FinBERT and ULMFiT for sentiment analysis of earnings call\ntranscripts. The objective is to investigate how Natural Language Processing\n(NLP) can be leveraged to extract sentiment from large-scale financial\ntranscripts, thereby aiding in more informed investment decisions and risk\nmanagement strategies. We examine the strengths and limitations of each model\nin the context of financial sentiment analysis, focusing on data preprocessing\nrequirements, computational efficiency, and model optimization. Through\nrigorous experimentation, we evaluate their performance using key metrics,\nincluding accuracy, precision, recall, and F1-score. Furthermore, we discuss\npotential enhancements to improve the effectiveness of these models in\nfinancial text analysis, providing insights into their applicability for\nreal-world financial decision-making.",
      "generated_abstract": "y focuses on analyzing earnings call transcripts, which are\nhighly structured, transcribed audio recordings, to develop advanced deep\nlearning techniques for earnings call analysis. Traditional text analysis methods\noften fail to extract important financial information from earnings call\ntranscripts due to the inherent challenges of analyzing transcripts, such as\ninconsistent formatting, missing punctuation, and ambiguous language.\nConsequently, these methods often produce inconsistent and inaccurate\nfinancial insights. To address these challenges, this study introduces\nstructured-audio-based deep learning models, including a novel approach that\ncombines text and audio features. These models achieve state-of-the-art\nperformance in text classification and speech-to-text conversion, enabling\nbetter financial insights from earnings call transcripts. Additionally, we\nintroduce a novel",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2916666666666667,
          "p": 0.345679012345679,
          "f": 0.3163841758268697
        },
        "rouge-2": {
          "r": 0.04201680672268908,
          "p": 0.04950495049504951,
          "f": 0.04545454048801708
        },
        "rouge-l": {
          "r": 0.28125,
          "p": 0.3333333333333333,
          "f": 0.3050847407986212
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.02327v2",
      "true_abstract": "Nonlinear frequency hopping has emerged as a promising approach for\nmitigating interference and enhancing range resolution in automotive FMCW radar\nsystems. Achieving an optimal balance between high range-resolution and\neffective interference mitigation remains challenging, especially without\ncentralized frequency scheduling. This paper presents a game-theoretic\nframework for interference avoidance, in which each radar operates as an\nindependent player, optimizing its performance through decentralized\ndecision-making. We examine two equilibrium concepts--Nash Equilibrium (NE) and\nCoarse Correlated Equilibrium (CCE)--as strategies for frequency band\nallocation, with CCE demonstrating particular effectiveness through regret\nminimization algorithms. We propose two interference avoidance algorithms: Nash\nHopping, a model-based approach, and No-Regret Hopping, a model-free adaptive\nmethod. Simulation results indicate that both methods effectively reduce\ninterference and enhance the signal-to-interference-plus-noise ratio (SINR).\nNotably, No-regret Hopping further optimizes frequency spectrum utilization,\nachieving improved range resolution compared to Nash Hopping.",
      "generated_abstract": "r presents a game-theoretic approach for high-resolution automotive\nFMCW radar interference avoidance (RAIA), focusing on the challenges of\ncooperative radar network design in multi-vehicle systems. We propose a game\ntheoretic framework that integrates the impact of radar interference with\ncooperative radar network design to enhance the robustness of radar systems in\nmulti-vehicle environments. The proposed framework incorporates a cooperative\nnetwork design problem and a radar interference avoidance problem, which\ncombine for a single optimization problem. The game theoretic framework is\nused to identify the optimal design of the cooperative radar network, which\ninvolves strategic decision-making by the cooperating vehicles to maximize\ntheir radar interference avoidance performance while minimizing their radar\ninterference impact on the rad",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22935779816513763,
          "p": 0.390625,
          "f": 0.28901733637876315
        },
        "rouge-2": {
          "r": 0.06716417910447761,
          "p": 0.09278350515463918,
          "f": 0.07792207305035544
        },
        "rouge-l": {
          "r": 0.21100917431192662,
          "p": 0.359375,
          "f": 0.2658959490955261
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/SC/2401.04786v1",
      "true_abstract": "The method developed by Michaelis and Menten was foundational in the\ndevelopment of our understanding of biochemical reaction kinetics. Extended\nmodels of metabolism encapsulated by reaction rate theory, stochastic reaction\nmodels, and dynamic flux estimation, amongst others, address aspects of this\nfundamental idea. The limitations of these approaches are well understood, and\nefforts to overcome those issues so far have been plentiful but with limited\nsuccess. The known issues can be summarised as the sole dependent relation with\nsubstrate concentration, the encapsulation of rate in a single relevant scalar,\nand the subsequent lack of functional control that results from this\nassumption. The Rate Control of Chaos (RCC) is a nonlinear control method that\nhas been shown to be effective in controlling the dynamic state of biological\noscillators based on the concept of rate limitation of the exponential growth\nin chaotic systems. Extending RCC with allosteric properties allows robust\ncontrol of the enzymatic process, and replicates the Michaelis-Menten kinetics.\nThe emergent dynamics is robust to perturbations and noise but susceptible to\nregulatory adjustments. This control method adapts the control parameters\ndynamically in the presence of a ligand, and permits introduction of energy\nrelations into the control function. The dynamic nature of the control\neliminates the steady-state requirements and allows the modelling of\nlarge-scale dynamic behaviour, potentially addressing issues in metabolic\ndisorder and failure of metabolic control.",
      "generated_abstract": "Allostery is a general mechanism by which a single gene regulates multiple\nchannels of a multicellular organism, but its mathematical analysis is\nintricate. Here, we develop a simple and general model of allostery, which\npredicts the existence of a \"sweet spot\" in the system, where the overall\ngene regulatory efficiency is maximized. We show that this sweet spot is\nachieved by a single gene that allosterically regulates a channel that is\nindependent of the others. The model predicts that a threshold value of the\nallosteric modulator determines the optimal number of genes that must be\nactivated to achieve the desired efficiency. We discuss the implications of our\nmodel for the mechanism of gene expression, and we provide an intuitive\nexplanation of our results.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1276595744680851,
          "p": 0.24,
          "f": 0.16666666213348777
        },
        "rouge-2": {
          "r": 0.027649769585253458,
          "p": 0.05128205128205128,
          "f": 0.03592813916078081
        },
        "rouge-l": {
          "r": 0.10638297872340426,
          "p": 0.2,
          "f": 0.13888888435571003
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2404.14137v2",
      "true_abstract": "Providing a measure of market risk is an important issue for investors and\nfinancial institutions. However, the existing models for this purpose are per\ndefinition symmetric. The current paper introduces an asymmetric capital asset\npricing model for measurement of the market risk. It explicitly accounts for\nthe fact that falling prices determine the risk for a long position in the\nrisky asset and the rising prices govern the risk for a short position. Thus, a\nposition dependent market risk measure that is provided accords better with\nreality. The empirical application reveals that Apple stock is more volatile\nthan the market only for the short seller. Surprisingly, the investor that has\na long position in this stock is facing a lower volatility than the market.\nThis property is not captured by the standard asset pricing model, which has\nimportant implications for the expected returns and hedging designs.",
      "generated_abstract": "We develop an asymmetric capital asset pricing model that accounts for\nnonlinear and non-Gaussian dependencies in the asset returns. The model is\nintended to capture the nonlinearities inherent in real-world financial data\nand provides a framework to address the challenges of high-dimensional\nfinancial data, such as those arising from the large-scale data generated by\nbig data and machine learning. We demonstrate the model's performance in\nestimating the parameters of the model, and the model's impact on the\npricing of stock options and other financial assets. The model is implemented\nin R and Python, and is publicly available at\nhttps://github.com/mohammadbod/ACAP. We also provide a user-friendly\nimplementation of the model in R, which is available at\nhttps://github.com/mohammadbod/ACAP-R.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2558139534883721,
          "p": 0.2972972972972973,
          "f": 0.27499999502812505
        },
        "rouge-2": {
          "r": 0.06923076923076923,
          "p": 0.08256880733944955,
          "f": 0.07531380256998335
        },
        "rouge-l": {
          "r": 0.22093023255813954,
          "p": 0.25675675675675674,
          "f": 0.2374999950281251
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.06329v1",
      "true_abstract": "In this paper, we introduce and study a class of monoids, called Layered\nCatalan Monoids (\\( {LC}_n \\)), which satisfy the structural conditions for\n$\\ll$-smoothness as defined in~\\cite{Sha-Det2}. These monoids are defined by\nspecific identities inspired by Catalan monoids. We establish their canonical\nforms and compute their determinant, proving that it is non-zero for \\(1 \\leq n\n\\leq 7\\) but vanishes for \\(n \\geq 8\\).",
      "generated_abstract": "The aim of this paper is to study the layered Catalan monoids (LCMs)\nas an affine variety in a natural way. In particular, we prove that the LCM\nis a graded variety, and we compute its determinants at several points. We\nalso study the Weyl group of the LCM, and we determine its orbit-stabilizer\ngroups, giving a complete classification of the orbits. Finally, we compute the\ndeterminant of the Weyl group of the LCM, and we show that it is a product of\nfour determinants. We also compute the determinant of the Weyl group of the\nfundamental group of the complex plane modulo a prime number, and we show that\nit is a product of two determinants.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2857142857142857,
          "p": 0.26229508196721313,
          "f": 0.27350426851340504
        },
        "rouge-2": {
          "r": 0.046875,
          "p": 0.03571428571428571,
          "f": 0.04054053563184866
        },
        "rouge-l": {
          "r": 0.2857142857142857,
          "p": 0.26229508196721313,
          "f": 0.27350426851340504
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.04648v1",
      "true_abstract": "The time-varying reproduction number ($R_t$) gives an indication of the\ntrajectory of an infectious disease outbreak. Commonly used frameworks for\ninferring $R_t$ from epidemiological time series include those based on\ncompartmental models (such as the SEIR model) and renewal equation models.\nThese inference methods are usually validated using synthetic data generated\nfrom a simple model, often from the same class of model as the inference\nframework. However, in a real outbreak the transmission processes, and thus the\ninfection data collected, are much more complex. The performance of common\n$R_t$ inference methods on data with similar complexity to real world scenarios\nhas been subject to less comprehensive validation. We therefore propose\nevaluating these inference methods on outbreak data generated from a\nsophisticated, geographically accurate agent-based model. We illustrate this\nproposed method by generating synthetic data for two outbreaks in Northern\nIreland: one with minimal spatial heterogeneity, and one with additional\nheterogeneity. We find that the simple SEIR model struggles with the greater\nheterogeneity, while the renewal equation model demonstrates greater robustness\nto spatial heterogeneity, though is sensitive to the accuracy of the generation\ntime distribution used in inference. Our approach represents a principled way\nto benchmark epidemiological inference tools and is built upon an open-source\nsoftware platform for reproducible epidemic simulation and inference.",
      "generated_abstract": "d of infectious diseases is often driven by spatial heterogeneity.\nSpatially heterogeneous epidemic models, including compartmental models,\nrenewal models, and spatially explicit models, have been widely used to\nsimulate and forecast infectious diseases. However, the performance of spatial\nmodels is often evaluated using only a single spatial location. This paper\nintroduces a novel method to assess the performance of spatial models using\nspatially heterogeneous epidemic simulations on real geographies. We propose a\nnovel evaluation framework to assess the performance of spatially heterogeneous\nepidemic models, using spatially heterogeneous epidemic simulations on\nreal-world geographies. The framework consists of three steps: (1) defining\ngeographic regions based on the spatial distribution of infectious disease\ncases; (2) generating spatially heterogeneous epidemic simulations using\nspatial",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23308270676691728,
          "p": 0.4696969696969697,
          "f": 0.3115577845115023
        },
        "rouge-2": {
          "r": 0.01507537688442211,
          "p": 0.031914893617021274,
          "f": 0.020477811341775436
        },
        "rouge-l": {
          "r": 0.21052631578947367,
          "p": 0.42424242424242425,
          "f": 0.28140703074265805
        }
      }
    },
    {
      "paper_id": "math.GN.math/GN/2502.18833v1",
      "true_abstract": "A topological space is domain-representable (or, has a domain model) if it is\nhomeomorphic to the maximal point space $\\mbox{Max}(P)$ of a domain $P$ (with\nthe relative Scott topology). We first construct an example to show that the\nset of maximal points of an ideal domain $P$ need not be a $G_{\\delta}$-set in\nthe Scott space $\\Sigma P$, thereby answering an open problem from Martin\n(2003). In addition, Bennett and Lutzer (2009) asked whether $X$ and $Y$ are\ndomain-representable if their product space $X \\times Y$ is\ndomain-representable. This problem was first solved by \\\"{O}nal and Vural\n(2015). In this paper, we provide a new approach to Bennett and Lutzer's\nproblem.",
      "generated_abstract": "aper we solve two problems on maximal point spaces of domains. The\nfirst problem is about maximal point spaces of the domain $D$ with a given\nsymmetric non-degenerate matrix $A$ in the sense of F. L. C. S. Santos and F.\nR. Santos. We prove that if $D$ is a bounded domain in $\\mathbb C^n$, and $A$ is\na symmetric matrix in $\\mathbb C^{n\\times n}$ with $n\\geq 2$, then there is a\nmaximal point space of $D$ with $A$ if and only if $A$ is diagonalizable.\nMoreover, the maximal point space of $D$ with $A$ is the intersection of the\nmaximal point space of $D$ with every diagonalizable matrix in $\\mathbb C^{n\\times\nn}$.\n  The second problem is about maximal point spaces of the domain $D$ with",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20512820512820512,
          "p": 0.2909090909090909,
          "f": 0.24060149890892654
        },
        "rouge-2": {
          "r": 0.03773584905660377,
          "p": 0.045454545454545456,
          "f": 0.04123710844510634
        },
        "rouge-l": {
          "r": 0.20512820512820512,
          "p": 0.2909090909090909,
          "f": 0.24060149890892654
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06755v1",
      "true_abstract": "In this paper, we study a transfer learning framework for Linear\n  Quadratic Regulator (LQR) control, where (i) the dynamics of the\n  system of interest (target system) are unknown and only a short\n  trajectory of impulse responses from the target system is provided,\n  and (ii) impulse responses are available from $N$ source systems\n  with different dynamics. We show that the LQR controller can be\n  learned from a sufficiently long trajectory of impulse\n  responses. Further, a transferable mode set can be identified using\n  the available data from source systems and the target system,\n  enabling the reconstruction of the target system's impulse responses\n  for controller design. By leveraging data from the source systems we\n  demonstrate that only n+1 (n being the system dimension) samples\n  of data from the target system are needed to learn the LQR\n  controller, this yields a significant reduction of the required\n  data.",
      "generated_abstract": "This paper proposes a transfer learning method for control of linear\nlinear quadratic regulator (LQR) systems. The method is based on an\ninformation-theoretic framework that captures the relationship between the\noriginal LQR problem and a linear system with a linear predictor. The\nprediction error of the linear system is optimized using transfer learning,\nand the LQR problem is solved using the optimal prediction error. The\ntransfer learning approach is shown to be applicable to a wide range of\nsystems and to various control tasks, including the optimal control of\nlinear-quadratic games. The method is implemented through a\ndeep-learning-based controller synthesis method and is evaluated using\nsimulation and real-world datasets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2345679012345679,
          "p": 0.3064516129032258,
          "f": 0.26573426082253415
        },
        "rouge-2": {
          "r": 0.06557377049180328,
          "p": 0.08080808080808081,
          "f": 0.07239818509940453
        },
        "rouge-l": {
          "r": 0.20987654320987653,
          "p": 0.27419354838709675,
          "f": 0.23776223285050624
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/CB/2409.05333v1",
      "true_abstract": "We examine the difference in motion ordering between cellular systems with\nand without information transfer to evaluate the effect of the polar--polar\ninteraction through mutual guiding, which enables cells to inform other cells\nof their moving directions. We compare this interaction with the\npolar--nonpolar interaction through cell motion triggered by cellular contact,\nwhich cannot provide information on the moving directions. We model these\ninteractions on the basis of the cellular Potts model. We calculate the order\nparameter of the polar direction in the interactions and examine the cell\nconcentration and surface tension conditions of ordering. The results suggest\nthat the polar--polar interaction through mutual guiding efficiently induces\nthe motion ordering in comparison with the polar-nonpolar interaction for\ncontact triggering, except in cases of weak driving. The results also show that\nthe polar--polar interaction efficiently accelerates the collective motion\ncompared with the polar--nonpolar interaction.",
      "generated_abstract": "of the molecular mechanisms of cell polarity determination has\nbeen a long-standing challenge, with significant progress in recent years.\nPolarity-polar interactions (PPIs) are important regulators of the cell\npolarity, but their molecular mechanisms are still poorly understood. Here, we\npropose a new model of the molecular mechanisms of PPIs. We study the motion\nordering in the polar-polar and polar-nonpolar PPIs. We first establish that\nmolecular motors are the most probable carriers of the polarity information\nin the polar-polar PPI. We then propose that the molecular motors are\narranged in a regular periodic pattern in the polar-nonpolar PPI. These\nresults suggest that molecular motors are the carriers of the polarity\ninformation in both PPIs, and that the molecular",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22077922077922077,
          "p": 0.27419354838709675,
          "f": 0.2446043116049895
        },
        "rouge-2": {
          "r": 0.08196721311475409,
          "p": 0.10989010989010989,
          "f": 0.09389670872093306
        },
        "rouge-l": {
          "r": 0.22077922077922077,
          "p": 0.27419354838709675,
          "f": 0.2446043116049895
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/OT/2406.11940v1",
      "true_abstract": "The stable unit treatment value assumption states that the outcome of an\nindividual is not affected by the treatment statuses of others, however in many\nreal world applications, treatments can have an effect on many others beyond\nthe immediately treated. Interference can generically be thought of as mediated\nthrough some network structure. In many empirically relevant situations\nhowever, complete network data (required to adjust for these spillover effects)\nare too costly or logistically infeasible to collect. Partially or indirectly\nobserved network data (e.g., subsamples, aggregated relational data (ARD),\negocentric sampling, or respondent-driven sampling) reduce the logistical and\nfinancial burden of collecting network data, but the statistical properties of\ntreatment effect adjustments from these design strategies are only beginning to\nbe explored. In this paper, we present a framework for the estimation and\ninference of treatment effect adjustments using partial network data through\nthe lens of structural causal models. We also illustrate procedures to assign\ntreatments using only partial network data, with the goal of either minimizing\nestimator variance or optimally seeding. We derive single network asymptotic\nresults applicable to a variety of choices for an underlying graph model. We\nvalidate our approach using simulated experiments on observed graphs with\napplications to information diffusion in India and Malawi.",
      "generated_abstract": "er the problem of estimating the causal effects of a treatment\non an outcome of interest when the treatment is partially observed. The\ntreatment is partially observed if the treatment assignment is determined by a\npartial network, i.e., some variables are observed for some treatments and\nothers are observed for other treatments. We propose a novel approach to\nestimate the causal effects of a treatment on an outcome of interest when the\ntreatment is partially observed. The proposed approach combines a graphical\nmodeling approach with a model-based approach to estimate the causal effects.\nWe propose an algorithm for the graphical modeling approach and derive\nasymptotic results for the proposed algorithm. We propose a novel approach to\nestimate the causal effects when the treatment is partially observed using a\nparticular network structure. We propose a Bayesian framework for the\nestimation of the causal effects of a treatment on an outcome",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.5517241379310345,
          "f": 0.3168316790746006
        },
        "rouge-2": {
          "r": 0.03482587064676617,
          "p": 0.07526881720430108,
          "f": 0.047619043293766886
        },
        "rouge-l": {
          "r": 0.20833333333333334,
          "p": 0.5172413793103449,
          "f": 0.29702969887658076
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/MN/2409.17488v1",
      "true_abstract": "Controlling the stochastic dynamics of biological populations is a challenge\nthat arises across various biological contexts. However, these dynamics are\ninherently nonlinear and involve a discrete state space, i.e., the number of\nmolecules, cells, or organisms. Additionally, the possibility of extinction has\na significant impact on both the dynamics and control strategies, particularly\nwhen the population size is small. These factors hamper the direct application\nof conventional control theories to biological systems. To address these\nchallenges, we formulate the optimal control problem for stochastic population\ndynamics by utilizing a control cost function based on the Kullback-Leibler\ndivergence. This approach naturally accounts for population-specific factors\nand simplifies the complex nonlinear Hamilton-Jacobi-Bellman equation into a\nlinear form, facilitating efficient computation of optimal solutions. We\ndemonstrate the effectiveness of our approach by applying it to the control of\ninteracting random walkers, Moran processes, and SIR models, and observe the\nmode-switching phenomena in the control strategies. Our approach provides new\nopportunities for applying control theory to a wide range of biological\nproblems.",
      "generated_abstract": "aper, we study optimal control of stochastic reaction networks with\nentropic control cost. The model is based on a simple model of the\nbiochemical mechanism of the human body. We study the case of a single\nreaction and two states of the system, where the system can switch between two\nstates. We consider the case of nonlinear dynamics of the reaction rate and the\nstochastic rate of the first reaction. We analyze the case of a finite number\nof states and a finite number of reactions. We study the case of linear and\nnonlinear dynamics of the reaction rate. We investigate the case of a\nsingle-state stochastic reaction network with a control cost that depends on the\nentropy produced by the reaction. We find the optimal control for a\nreaction-driven system, which is linear in the control cost. We study the\ncase of multiple-state stochastic reaction networks with a control cost that\ndepends",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18421052631578946,
          "p": 0.35,
          "f": 0.2413793058263972
        },
        "rouge-2": {
          "r": 0.0718562874251497,
          "p": 0.11009174311926606,
          "f": 0.08695651695993514
        },
        "rouge-l": {
          "r": 0.17543859649122806,
          "p": 0.3333333333333333,
          "f": 0.22988505295283396
        }
      }
    },
    {
      "paper_id": "math.ST.stat/ME/2503.03567v1",
      "true_abstract": "We propose a new statistical hypothesis testing framework which decides\nvisually, using confidence intervals, whether the means of two samples are\nequal or if one is larger than the other. With our method, the user can at the\nsame time visualize the confidence region of the means and do a test to decide\nif the means of the two populations are significantly different or not by\nlooking whether the two confidence intervals overlap. To design this test we\nuse confidence intervals constructed using e-variables, which provide a measure\nof evidence in hypothesis testing. We propose both a sequential test and a\nnon-sequential test based on the overlap of confidence intervals and for each\nof these tests we give finite-time error bounds on the probabilities of error.\nWe also illustrate the practicality of our method by applying it to the\ncomparison of sequential learning algorithms.",
      "generated_abstract": "r proposes a novel framework for testing the significance of\nvisual comparisons of means. A single-sided test is proposed that involves\nevaluating the significance of the mean difference between the two means,\nbased on several confidence intervals. The framework is applied to the\ncomparison of the mean and median of two samples of ordered variables. We\ndemonstrate that the proposed test is consistent and asymptotically normal. We\nalso propose a confidence interval for the mean difference between the two\nmeans, and show that it is asymptotically normal, with an asymptotic variance\nthat depends on the shape of the distribution of the means. We illustrate the\nproposed tests using simulated data and experimental data from a study on\neating habits. The proposed tests are compared to a standard one-sided test,\nwhich is based on the Shapiro-Wilk test, and a two-sided test that is based on\nthe",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.313953488372093,
          "p": 0.35526315789473684,
          "f": 0.3333333283523854
        },
        "rouge-2": {
          "r": 0.11363636363636363,
          "p": 0.12096774193548387,
          "f": 0.11718749500488303
        },
        "rouge-l": {
          "r": 0.29069767441860467,
          "p": 0.32894736842105265,
          "f": 0.308641970327694
        }
      }
    },
    {
      "paper_id": "cs.CL.cs/IR/2503.08398v1",
      "true_abstract": "In this paper, we analyze and empirically show that the learned relevance for\nconventional information retrieval (IR) scenarios may be inconsistent in\nretrieval-augmented generation (RAG) scenarios. To bridge this gap, we\nintroduce OpenRAG, a RAG framework that is optimized end-to-end by tuning the\nretriever to capture in-context relevance, enabling adaptation to the diverse\nand evolving needs. Extensive experiments across a wide range of tasks\ndemonstrate that OpenRAG, by tuning a retriever end-to-end, leads to a\nconsistent improvement of 4.0% over the original retriever, consistently\noutperforming existing state-of-the-art retrievers by 2.1%. Additionally, our\nresults indicate that for some tasks, an end-to-end tuned 0.2B retriever can\nachieve improvements that surpass those of RAG-oriented or instruction-tuned 8B\nlarge language models (LLMs), highlighting the cost-effectiveness of our\napproach in enhancing RAG systems.",
      "generated_abstract": "e OpenRAG, a novel framework for end-to-end RAG generation that\nis trained end-to-end in the in-context retrieval (ICR) setting. Compared to\nprevious RAG generation models that rely on pre-trained language models (PLMs)\nand external retrieval datasets, our method directly encodes RAGs as query\nresponses, eliminating the need for external retrieval datasets and PLMs.\nUnlike prior work that employs an additional encoder-decoder architecture to\nprocess RAGs, we construct a lightweight RAG encoder that processes RAGs as\nquery responses, thereby reducing the model size. Our model achieves\nstate-of-the-art performance on RAG generation benchmarks.\n  We conduct a series of experiments on a range of RAG generation tasks,\nshowing that OpenRAG achieves superior performance across all tasks. Our",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27,
          "p": 0.34177215189873417,
          "f": 0.3016759727224494
        },
        "rouge-2": {
          "r": 0.031007751937984496,
          "p": 0.03773584905660377,
          "f": 0.03404254823938506
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.31645569620253167,
          "f": 0.2793296040073656
        }
      }
    },
    {
      "paper_id": "cs.OS.cs/OS/2502.07118v1",
      "true_abstract": "File systems play an essential role in modern society for managing precious\ndata. To meet diverse needs, they often support many configuration parameters.\nSuch flexibility comes at the price of additional complexity which can lead to\nsubtle configuration-related issues. To address this challenge, we study the\nconfiguration-related issues of two major file systems (i.e., Ext4 and XFS) in\ndepth, and identify a prevalent pattern called multilevel configuration\ndependencies. Based on the study, we build an extensible tool called ConfD to\nextract the dependencies automatically, and create a set of plugins to address\ndifferent configuration-related issues. Our experiments on Ext4, XFS and a\nmodern copy-on-write file system (i.e., ZFS) show that ConfD was able to\nextract 160 configuration dependencies for the file systems with a low false\npositive rate. Moreover, the dependency-guided plugins can identify various\nconfiguration issues (e.g., mishandling of configurations, regression test\nfailures induced by valid configurations). In addition, we also explore the\napplicability of ConfD on a popular storage engine (i.e., WiredTiger). We hope\nthat this comprehensive analysis of configuration dependencies of storage\nsystems can shed light on addressing configuration-related challenges for the\nsystem community in general.",
      "generated_abstract": "The complexity of file systems is increasingly apparent, with more\nand more of them having dependencies on other parts of the operating system.\nThe complexity of file systems is increasingly apparent, with more and more of\nthem having dependencies on other parts of the operating system. This paper\nexplores the complexity of file systems by studying their configuration\ndependencies. We collect configuration data for a collection of file systems,\nand then use it to analyze their configuration dependencies. Our analysis\nreveals that file systems are not just simple containers for other parts of the\noperating system, but are instead complex systems with their own dependencies.\nThis paper introduces a novel way of measuring file system complexity, which\nis based on their configuration dependencies, and introduces a new metric for\nevaluating their performance. Our results show that the complexity of file\nsystems is not only related to the complexity of their underlying operating\nsystem, but also the complexity of their configuration dependencies.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18548387096774194,
          "p": 0.323943661971831,
          "f": 0.2358974312667982
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.045454545454545456,
          "f": 0.033898300407929405
        },
        "rouge-l": {
          "r": 0.18548387096774194,
          "p": 0.323943661971831,
          "f": 0.2358974312667982
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.physics/chem-ph/2503.09855v1",
      "true_abstract": "Spatially varying electric fields are prevalent throughout nature and\ntechnology, arising from heterogeneity inherent to all physical systems.\nInhomogeneous electric fields can originate naturally, such as in nanoporous\nmaterials and biological membranes, or be engineered, e.g., with patterned\nelectrodes or layered van der Waals heterostructures. While uniform fields\ncause free ions to migrate, for polar fluids they simply act to reorient the\nconstituent molecules. In contrast, electric field gradients (EFGs) induce a\ndielectrophoretic force, offering exquisite electrokinetic control of a fluid,\neven in the absence of free charge carriers. EFGs, therefore, offer vast\npotential for optimizing fluid behavior under confinement, such as in\nnanoporous electrodes, nanofluidic devices, and chemical separation materials.\nYet, EFGs remain largely unexplored at the microscopic level owing to the\nabsence of a rigorous, first principles theoretical treatment of\nelectrostrictive effects. By integrating state-of-the-art advances in liquid\nstate theory and deep learning, we reveal how EFGs modulate fluid structure and\ncapillary phenomena. We demonstrate, from first principles, that\ndielectrophoretic coupling enables tunable control over the liquid-gas phase\ntransition, capillary condensation, and fluid uptake into porous media. Our\nfindings establish \"dielectrocapillarity'' -- the use of EFGs to control\nconfined fluids -- as a powerful tool for controlling volumetric capacity in\nnanopores, which holds immense potential for optimizing energy storage in\nsupercapacitors, selective gas separation, and tunable hysteresis in\nneuromorphic nanofluidic devices.",
      "generated_abstract": "The ability to exert precise control over the flow of fluids is a key requirement\nfor many technological applications. In this work, we investigate the\nexperimental conditions for the development of dielectrocapillarity, a phenomenon\ninvolving a sudden change in the dielectric constant of the surrounding\nenvironment, often observed in aqueous solutions. Using a combination of\nelectrochemical measurements, dielectric spectroscopy, and numerical simulations,\nwe show that dielectrocapillarity can be induced by applying a voltage\ndifference across a pair of electrodes in aqueous solutions. The development of\ndielectrocapillarity depends on the applied voltage, and the applied voltage\ncan be tuned to control the flow of the fluid.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10240963855421686,
          "p": 0.25,
          "f": 0.14529914117612694
        },
        "rouge-2": {
          "r": 0.018604651162790697,
          "p": 0.041666666666666664,
          "f": 0.02572346840086505
        },
        "rouge-l": {
          "r": 0.09036144578313253,
          "p": 0.22058823529411764,
          "f": 0.12820512408210985
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/RM/2502.11706v1",
      "true_abstract": "A deep BSDE approach is presented for the pricing and delta-gamma hedging of\nhigh-dimensional Bermudan options, with applications in portfolio risk\nmanagement. Large portfolios of a mixture of multi-asset European and Bermudan\nderivatives are cast into the framework of discretely reflected BSDEs. This\nsystem is discretized by the One Step Malliavin scheme (Negyesi et al. [2024,\n2025]) of discretely reflected Markovian BSDEs, which involves a $\\Gamma$\nprocess, corresponding to second-order sensitivities of the associated option\nprices. The discretized system is solved by a neural network regression Monte\nCarlo method, efficiently for a large number of underlyings. The resulting\noption Deltas and Gammas are used to discretely rebalance the corresponding\nreplicating strategies. Numerical experiments are presented on both\nhigh-dimensional basket options and large portfolios consisting of multiple\noptions with varying early exercise rights, moneyness and volatility. These\nexamples demonstrate the robustness and accuracy of the method up to $100$ risk\nfactors. The resulting hedging strategies significantly outperform benchmark\nmethods both in the case of standard delta- and delta-gamma hedging.",
      "generated_abstract": "e a deep learning-based model for simultaneous pricing and delta-gamma\nhedgeing of a large portfolio of Bermudan options. Our model combines a\ndeep-learning-based forward-backward stochastic differential equation (FB-SBDE)\nwith a deep neural network (DNN) to model the pricing and delta-gamma hedging\nprocesses for the portfolio of Bermudan options. The DNN-based pricing model\nintegrates the forward-backward stochastic differential equation (FB-SBDE) with\na deep neural network (DNN) to model the pricing process. We apply the\nproposed DNN-based pricing model to the simultaneous pricing and delta-gamma\nhedging of a large portfolio of Bermudan options. The results show that the\nproposed DNN-based pricing model outperforms the FB-SBDE in terms",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17117117117117117,
          "p": 0.41304347826086957,
          "f": 0.24203821241754236
        },
        "rouge-2": {
          "r": 0.05625,
          "p": 0.13432835820895522,
          "f": 0.07929515002425841
        },
        "rouge-l": {
          "r": 0.17117117117117117,
          "p": 0.41304347826086957,
          "f": 0.24203821241754236
        }
      }
    },
    {
      "paper_id": "physics.optics.nlin/SI/2503.08513v1",
      "true_abstract": "We report the results of experimental studies of recurrent spectral dynamics\nof the two component Akhmediev breathers (ABs) in a single mode optical fibre.\nWe also provide the theoretical analysis and numerical simulations of the ABs\nbased on the two component Manakov equations that confirm the experimental\ndata. In particular, we observed spectral asymmetry of fundamental ABs and\ncomplex spectral evolution of second-order nondegenerate ABs.",
      "generated_abstract": "t experimental results of the spectral asymmetry and recurrence of\nthe two-component Akhmediev breather (AB) in a single mode optical fiber (OMF).\nThe experimental setup consists of a 1.5-m long OMF with a periodically\npulsed mode at 1550 nm. The mode is excited by a pulsed laser with a power of\n30 mW and a pulse duration of 300 fs. The pulses are focused into the fiber by\na lens with a focal length of 2.5 m, where the mode is continuously\npropagating. The experimental results show the presence of the spectral\nasymmetry of the AB in the mode of the OMF, which is confirmed by the\nspectral analysis. The recurrence of the AB is demonstrated by the\nobservation of the modulation of the AB",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.30434782608695654,
          "p": 0.208955223880597,
          "f": 0.247787605792153
        },
        "rouge-2": {
          "r": 0.13114754098360656,
          "p": 0.0761904761904762,
          "f": 0.0963855375199596
        },
        "rouge-l": {
          "r": 0.2608695652173913,
          "p": 0.1791044776119403,
          "f": 0.2123893757036574
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2402.15828v4",
      "true_abstract": "Geometric Asian options are a type of options where the payoff depends on the\ngeometric mean of the underlying asset over a certain period of time. This\npaper is concerned with the pricing of such options for the class of\nVolterra-Heston models, covering the rough Heston model. We are able to derive\nsemi-closed formulas for the prices of geometric Asian options with fixed and\nfloating strikes for this class of stochastic volatility models. These formulas\nrequire the explicit calculation of the conditional joint Fourier transform of\nthe logarithm of the stock price and the logarithm of the geometric mean of the\nstock price over time. Linking our problem to the theory of affine Volterra\nprocesses, we find a representation of this Fourier transform as a suitably\nconstructed stochastic exponential, which depends on the solution of a\nRiccati-Volterra equation. Finally we provide a numerical study for our results\nin the rough Heston model.",
      "generated_abstract": "the pricing of geometric Asian options in the Volterra-Heston\nmodel with different volatilities. Our goal is to identify the optimal\nstrategies that exploit the intrinsic properties of the model and its\nderivatives. The geometric Asian options are priced by means of the\nVolterra-Heston model with different volatilities, the aim is to identify the\noptimal strategies that exploit the intrinsic properties of the model and its\nderivatives. The optimal pricing formulas are obtained in closed form,\nincluding the case of a single Asian option, as well as the cases of a\nmulti-asset geometric Asian option, a geometric Asian option with a maturity\ntime of one year, and a geometric Asian option with a maturity time of one\nmonth. The pricing formulas for the geometric Asian options with maturity time\nof one year and of one month are compared with the formulas obtained by",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2235294117647059,
          "p": 0.34545454545454546,
          "f": 0.27142856665816334
        },
        "rouge-2": {
          "r": 0.1076923076923077,
          "p": 0.14583333333333334,
          "f": 0.12389380042289944
        },
        "rouge-l": {
          "r": 0.2235294117647059,
          "p": 0.34545454545454546,
          "f": 0.27142856665816334
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2409.10938v2",
      "true_abstract": "Modern macroeconomic models, particularly those grounded in Rational\nExpectation Dynamic Stochastic General Equilibrium (DSGE), operate under the\nassumption of fully rational decision-making. This paper examines the impact of\nbehavioral factors on the communication index/sentiment index of the US Federal\nReserve. [Upon receiving the review comments, I found some technical errors in\nthe paper. I shall update it accordingly. Please do not cite this paper without\nauthor's permission.]",
      "generated_abstract": "anks are increasingly using forward-looking statements to communicate\ncentral bank policy. This paper examines the role of animal spirits and\ninflation extrapolation in these statements, by examining the empirical\nevidence from the Federal Reserve Bank of New York's announcement of its\nnext policy meeting. We find that a greater reliance on animal spirits leads\nto more optimistic forecasts, and a greater reliance on inflation extrapolation\nleads to more pessimistic forecasts. In addition, we show that these\ndifferences in forecasts are driven by differences in inflation expectations.\nThese findings are consistent with the notion that central banks are using\nforward-looking statements to communicate their expectations for inflation. We\nalso find that the effect of animal spirits on the forecast is moderated by\ninflation expectations, suggesting that central banks are using animal spirits\nto communicate expectations about inflation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15789473684210525,
          "p": 0.12,
          "f": 0.13636363145661173
        },
        "rouge-2": {
          "r": 0.07462686567164178,
          "p": 0.043859649122807015,
          "f": 0.055248614121669454
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.12,
          "f": 0.13636363145661173
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.03012v1",
      "true_abstract": "We develop a framework to holistically test for and monitor the impact of\ndifferent types of events affecting a country's housing market, yet originating\nfrom housing-external sources. We classify events along three dimensions\nleading to testable hypotheses: prices versus quantities, supply versus demand,\nand immediate versus gradually evolving. These dimensions translate into\nguidance about which data type, statistical measure and testing strategy should\nbe used. To perform such test suitable statistical models are needed which we\nimplement as a hierarchical hedonic price model and a complementary count\nmodel. These models are amended by regime and contextual variables as suggested\nby our classification strategy. We apply this framework to the Austrian real\nestate market together with three disruptive events triggered by the COVID-19\npandemic, a policy tightening mortgage lending standards, as well as the\ncost-of-living crisis that came along with increased financing costs. The tests\nyield the expected results and, by that, some housing market puzzles are\nresolved. Deviating from the prior classification exercise means that some\ndevelopments would have been undetected. Further, adopting our framework\nconsistently when performing empirical research on residential real estate\nwould lead to better comparable research results and, by that, would allow\nresearchers to draw meta-conclusions from the bulk of studies available across\ntime and space.",
      "generated_abstract": "r introduces a novel framework for assessing the effects of\nexternal shocks on housing markets, focusing on how the price of housing affects\nthe demand for housing. The framework combines two key components: (1) a\nframework for identifying external shocks on housing markets using aggregate\nmarket data and (2) a model to estimate the effects of external shocks on\nhousing markets. The model captures the interplay between the supply of housing\nand external shocks, including changes in the supply of land, changes in the\nlevel of interest rates, changes in the level of the stock of homes, and\nchanges in the price of housing. The model is implemented using the Econoday\ndata and the Panel-Econ software. The results show that the effects of external\nshocks on housing prices depend on the characteristics of the external shocks\nand on the specifics of the housing market. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12413793103448276,
          "p": 0.2857142857142857,
          "f": 0.17307691885401266
        },
        "rouge-2": {
          "r": 0.009900990099009901,
          "p": 0.018867924528301886,
          "f": 0.012987008472762573
        },
        "rouge-l": {
          "r": 0.12413793103448276,
          "p": 0.2857142857142857,
          "f": 0.17307691885401266
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/CO/2501.11743v1",
      "true_abstract": "We consider the constrained sampling problem where the goal is to sample from\na target distribution on a constrained domain. We propose skew-reflected\nnon-reversible Langevin dynamics (SRNLD), a continuous-time stochastic\ndifferential equation with skew-reflected boundary. We obtain non-asymptotic\nconvergence rate of SRNLD to the target distribution in both total variation\nand 1-Wasserstein distances. By breaking reversibility, we show that the\nconvergence is faster than the special case of the reversible dynamics. Based\non the discretization of SRNLD, we propose skew-reflected non-reversible\nLangevin Monte Carlo (SRNLMC), and obtain non-asymptotic discretization error\nfrom SRNLD, and convergence guarantees to the target distribution in\n1-Wasserstein distance. We show better performance guarantees than the\nprojected Langevin Monte Carlo in the literature that is based on the\nreversible dynamics. Numerical experiments are provided for both synthetic and\nreal datasets to show efficiency of the proposed algorithms.",
      "generated_abstract": "eversible Langevin (NRL) algorithm is a Markov chain Monte Carlo\nmethod that introduces a penalty term to force the Markov chain to be reversible\nwith respect to the original Hamiltonian. NRL has been shown to be very\nefficient for sampling from constrained Hamiltonians and can be further\noptimized to increase sampling efficiency. In this work, we propose a new\noptimization strategy for NRL, called the Non-Reversible Langevin Optimized\n(NRLO) algorithm, that can be used to improve the sampling efficiency of the\nNRL algorithm for sampling from constrained Hamiltonians. We also propose a\nvariational sampling algorithm based on the NRLO algorithm that is able to\naccelerate the sampling process and improve the efficiency of sampling from\nconstrained Hamiltonians. We demonstrate the efficiency of our algorithms by\ncomparing them to the standard NRL and the standard VQ",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2948717948717949,
          "p": 0.30666666666666664,
          "f": 0.3006535897731642
        },
        "rouge-2": {
          "r": 0.06504065040650407,
          "p": 0.07017543859649122,
          "f": 0.06751054353041751
        },
        "rouge-l": {
          "r": 0.28205128205128205,
          "p": 0.29333333333333333,
          "f": 0.2875816943483276
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.07889v1",
      "true_abstract": "We present a simple method to enable processing of Spotlight Synthetic\nAperture Radar (SAR) imagery distributed in Polar Format (PFA) using standard\nRange-Doppler (RDA) geometry algorithms. Our approach is applicable to PFA SAR\nimages characterized by a constant value of the Center of Aperture (COA) time.\nWe present simplified expressions for forward (image-to-ground) and inverse\n(ground-to-image) geometry mapping using Sensor Independent Complex Data (SICD)\nconventions. We discuss simple changes needed to current open source SAR\nsoftware that implement Range-Doppler algorithms, to enable support within them\nfor Spotlight data distributed in SICD format. We include a proof-of-concept\nscript that utilizes the Python packages sarpy and isce3 to demonstrate the\ncorrectness of the proposed approach.",
      "generated_abstract": "t a novel range-Doppler geometry framework for processing Spotlight\nSAR images in polar format. Spotlight SAR is a recent technology that\nprovides high-resolution imagery in polar format. This work aims to address\nthe challenges in processing Spotlight SAR images in polar format. We propose a\nmodified range-Doppler geometry framework for processing Spotlight SAR\nimagery in polar format. The proposed framework is a generalization of the\nrange-Doppler geometry framework for processing Spotlight SAR images in\nequirectangular format. This framework is based on the range-Doppler geometry\nframework for processing Spotlight SAR images in spherical format. The\nmodifications are designed to support the processing of Spotlight SAR images\nin polar format. The proposed modified range-Doppler geometry framework for\nprocessing Spotlight SAR images in polar format",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21686746987951808,
          "p": 0.4186046511627907,
          "f": 0.28571428121819104
        },
        "rouge-2": {
          "r": 0.046296296296296294,
          "p": 0.078125,
          "f": 0.058139530210925196
        },
        "rouge-l": {
          "r": 0.21686746987951808,
          "p": 0.4186046511627907,
          "f": 0.28571428121819104
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2411.06076v1",
      "true_abstract": "This paper introduces BreakGPT, a novel large language model (LLM)\narchitecture adapted specifically for time series forecasting and the\nprediction of sharp upward movements in asset prices. By leveraging both the\ncapabilities of LLMs and Transformer-based models, this study evaluates\nBreakGPT and other Transformer-based models for their ability to address the\nunique challenges posed by highly volatile financial markets. The primary\ncontribution of this work lies in demonstrating the effectiveness of combining\ntime series representation learning with LLM prediction frameworks. We showcase\nBreakGPT as a promising solution for financial forecasting with minimal\ntraining and as a strong competitor for capturing both local and global\ntemporal dependencies.",
      "generated_abstract": "ce movements are a crucial indicator of macroeconomic\nequilibria. Predicting asset price surges is a fundamental challenge in\nfinance, yet existing methods suffer from limited generalization and\nover-reliance on feature engineering. This paper introduces BreakGPT, a\ntransformer-based model that utilizes large language models (LLMs) to predict\nasset price surges. Our method employs a novel tokenization strategy to extract\nfeatures from the asset price data, and then leverages a transformer architecture\nto predict the probability of a price surge. The model is trained and tested on\nhistorical data from 1993 to 2021, covering 100 asset classes, and achieves\nstate-of-the-art performance across multiple metrics. Our findings suggest that\nBreakGPT outperforms existing methods in terms of both accuracy and robustness,\nwith",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27848101265822783,
          "p": 0.25882352941176473,
          "f": 0.2682926779335218
        },
        "rouge-2": {
          "r": 0.05825242718446602,
          "p": 0.05357142857142857,
          "f": 0.05581394849713403
        },
        "rouge-l": {
          "r": 0.25316455696202533,
          "p": 0.23529411764705882,
          "f": 0.2439024340310828
        }
      }
    },
    {
      "paper_id": "physics.plasm-ph.physics/plasm-ph/2503.10067v1",
      "true_abstract": "The significance of laser-driven polarized beam acceleration has been\nincreasingly recognized in recent years. We propose an efficient method for\ngenerating polarized proton beams from a pre-polarized hydrogen halide gas jet,\nutilizing magnetic vortex acceleration enhanced by a laser-driven plasma\nbubble. When a petawatt laser pulse passes through a pre-polarized gas jet, a\nbubble-like ultra-nonlinear plasma wave is formed. As part of the wave\nparticles, background protons are swept by the acceleration field of the bubble\nand oscillate significantly along the laser propagation axis. Some of the\npre-accelerated protons in the plasma wave are trapped by the acceleration\nfield at the rear side of the target. This acceleration field is intensified by\nthe transverse expansion of the laser-driven magnetic vortex, resulting in\nenergetic polarized proton beams. The spin of energetic protons is determined\nby their precession within the electromagnetic field, as described by the\nThomas-Bargmann-Michel-Telegdi equation in analytical models and\nparticle-in-cell simulations. Multidimensional simulations reveal that\nmonoenergetic proton beams with hundreds of MeV in energy, a beam charge of\nhundreds of pC, and a beam polarization of tens of percent can be produced at\nlaser powers of several petawatts. Laser-driven polarized proton beams offer\npromising potential for application in polarized beam colliders, where they can\nbe utilized to investigate particle interactions and to explore the properties\nof matter under unique conditions.",
      "generated_abstract": "ork, we investigate the possibility of generating a high-energy\npolarized proton beam by using laser-driven plasma bubbles in the MVA system.\nThe plasma bubbles are created in a gas-jet cavity and are injected at the\nend of the MVA. The MVA is a hybrid system that uses a vacuum plasma source\nto produce the plasma bubbles. A vacuum-based plasma source is used to generate\nthe polarized plasma bubbles in a cavity. The polarized plasma bubbles are\ninjected into a gas jet, which injects the plasma bubbles into the MVA. The\nMVA is a 4-channel MCA (Multi-Channel AmpliTube) with 300 mm of beam length.\nThe laser powers in the MCA are 300,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18461538461538463,
          "p": 0.42105263157894735,
          "f": 0.2566844877405703
        },
        "rouge-2": {
          "r": 0.04040404040404041,
          "p": 0.08791208791208792,
          "f": 0.05536331748470479
        },
        "rouge-l": {
          "r": 0.16153846153846155,
          "p": 0.3684210526315789,
          "f": 0.22459892624324407
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2502.05839v1",
      "true_abstract": "In this paper, we examine a modified version of de Finetti's optimal dividend\nproblem, incorporating fixed transaction costs and altering the surplus process\nby introducing two-valued drift and two-valued volatility coefficients. This\nmodification aims to capture the transitions or adjustments in the company's\nfinancial status. We identify the optimal dividend strategy, which maximizes\nthe expected total net dividend payments (after accounting for transaction\ncosts) until ruin, as a two-barrier impulsive dividend strategy. Notably, the\noptimal strategy can be explicitly determined for almost all scenarios\ninvolving different drifts and volatility coefficients. Our primary focus is on\nexploring how changes in drift and volatility coefficients influence the\noptimal dividend strategy.",
      "generated_abstract": "We consider a portfolio problem with a fixed transaction cost and a\nregime-switching risk premium. Under the setting of non-Gaussian returns and\nnon-linear investment costs, we show that the optimal strategy is a martingale.\nHowever, this does not imply that the optimal strategy is a martingale for all\ncases of returns and costs, nor that it is a martingale under a specific\nhypothesis. We illustrate the phenomenon with a simple example. We then\npropose a model that allows us to make some predictions based on the\nhypothesis of a linear risk premium. Finally, we show that this model can\ninterpret the De Finetti's problem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.225,
          "p": 0.2857142857142857,
          "f": 0.2517482468189154
        },
        "rouge-2": {
          "r": 0.030612244897959183,
          "p": 0.03333333333333333,
          "f": 0.03191488862607593
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.25396825396825395,
          "f": 0.2237762188468875
        }
      }
    },
    {
      "paper_id": "math.ST.math/ST/2503.06889v1",
      "true_abstract": "Community detection, which focuses on recovering the group structure within\nnetworks, is a crucial and fundamental task in network analysis. However, the\ndetection process can be quite challenging and unstable when community signals\nare weak. Motivated by a newly collected large-scale academic network dataset\nfrom the Web of Science, which includes multi-layer network information, we\npropose a Bipartite Assisted Spectral-clustering approach for Identifying\nCommunities (BASIC), which incorporates the bipartite network information into\nthe community structure learning of the primary network. The accuracy and\nstability enhancement of BASIC is validated theoretically on the basis of the\ndegree-corrected stochastic block model framework, as well as numerically\nthrough extensive simulation studies. We rigorously study the convergence rate\nof BASIC even under weak signal scenarios and prove that BASIC yields a tighter\nupper error bound than that based on the primary network information alone. We\nutilize the proposed BASIC method to analyze the newly collected large-scale\nacademic network dataset from statistical papers. During the author\ncollaboration network structure learning, we incorporate the bipartite network\ninformation from author-paper, author-institution, and author-region\nrelationships. From both statistical and interpretative perspectives, these\nbipartite networks greatly aid in identifying communities within the primary\ncollaboration network.",
      "generated_abstract": "r presents BASIC, a novel spectral clustering framework for\nidentifying communities in large-scale networks. BASIC leverages bipartite\ngraphs to efficiently cluster nodes while preserving the original community\nstructure. Specifically, BASIC first partitions the nodes into two disjoint\nsubgraphs, one containing the \"active\" nodes and the other containing the\n\"passive\" nodes. Then, BASIC sequentially applies spectral clustering to the\nactive and passive subgraphs. This approach allows BASIC to cluster nodes in a\nbipartite graph without introducing any additional edges or vertices,\nsimplifying the problem and ensuring that the clusters are well-defined and\nconsistent with the underlying community structure. By leveraging bipartite\ngraphs, BASIC also enhances scalability and efficiency, particularly when\nprocessing large-scale networks. In addition, BASIC offers a flexible\nparameterization",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1484375,
          "p": 0.2289156626506024,
          "f": 0.18009478195727874
        },
        "rouge-2": {
          "r": 0.011235955056179775,
          "p": 0.017699115044247787,
          "f": 0.013745699716821234
        },
        "rouge-l": {
          "r": 0.140625,
          "p": 0.21686746987951808,
          "f": 0.17061610897149673
        }
      }
    },
    {
      "paper_id": "math.OC.q-fin/MF/2501.17577v1",
      "true_abstract": "We study a class of singular stochastic control problems for a\none-dimensional diffusion $X$ in which the performance criterion to be\noptimised depends explicitly on the running infimum $I$ (or supremum $S$) of\nthe controlled process. We introduce two novel integral operators that are\nconsistent with the Hamilton-Jacobi-Bellman equation for the resulting\ntwo-dimensional singular control problems. The first operator involves\nintegrals where the integrator is the control process of the two-dimensional\nprocess $(X,I)$ or $(X,S)$; the second operator concerns integrals where the\nintegrator is the running infimum or supremum process itself. Using these\ndefinitions, we prove a general verification theorem for problems involving\ntwo-dimensional state-dependent running costs, costs of controlling the\nprocess, costs of increasing the running infimum (or supremum) and exit times.\nFinally, we apply our results to explicitly solve an optimal dividend problem\nin which the manager's time-preferences depend on the company's historical\nworst performance.",
      "generated_abstract": "In this paper, we consider the control of a stochastic differential equation\n(SDE) by a continuous-time stochastic process which is the solution of the\nsame SDE but with a different drift. The control problem consists in solving\nthe original SDE with the new drift. The control problem is called singular\nwhen the solution of the original SDE is singular. We give conditions on the\ncontrol and the initial condition such that the singular control is a\nrunning infimum or a supremum of a family of controls. The result is a\ngeneralization of a result of A. S. Chikhi and J. M. Pallardou (2007) who\nconsider the control of a stochastic differential equation by a continuous-time\nstochastic process which is the solution of the same SDE but with a different\ndrift. The control problem consists in solving the original SDE with the new\ndrift.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24731182795698925,
          "p": 0.38333333333333336,
          "f": 0.3006535900038448
        },
        "rouge-2": {
          "r": 0.061068702290076333,
          "p": 0.08602150537634409,
          "f": 0.07142856657246527
        },
        "rouge-l": {
          "r": 0.22580645161290322,
          "p": 0.35,
          "f": 0.2745097991541715
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.10791v1",
      "true_abstract": "We investigate methods for forecasting multivariate realized covariances\nmatrices applied to a set of 30 assets that were included in the DJ30 index at\nsome point, including two novel methods that use existing (univariate) log of\nrealized variance models that account for attenuation bias and time-varying\nparameters. We consider the implications of some modeling choices within the\nclass of heterogeneous autoregressive models. The following are our key\nfindings. First, modeling the logs of the marginal volatilities is strongly\npreferred over direct modeling of marginal volatility. Thus, our proposed model\nthat accounts for attenuation bias (for the log-response) provides superior\none-step-ahead forecasts over existing multivariate realized covariance\napproaches. Second, accounting for measurement errors in marginal realized\nvariances generally improves multivariate forecasting performance, but to a\nlesser degree than previously found in the literature. Third, time-varying\nparameter models based on state-space models perform almost equally well.\nFourth, statistical and economic criteria for comparing the forecasting\nperformance lead to some differences in the models' rankings, which can\npartially be explained by the turbulent post-pandemic data in our out-of-sample\nvalidation dataset using sub-sample analyses.",
      "generated_abstract": "We propose an HAR-type model for realized covariances in a large,\nrepresentative panel data set from the U.S. Census Bureau. The model captures\nthe multivariate distribution of realized covariances in a flexible and\nefficient manner. We introduce a novel approach for estimating the model and\nderive theoretical properties of the resulting estimator. We also introduce a\ntwo-step estimator for the variance part of the model. We demonstrate the\neffectiveness of the proposed estimators in the U.S. Census Bureau data,\ncompared to competing estimators. We show that the model captures the main\nfeatures of the data well, including heteroskedasticity and serial correlation,\nand that the proposed estimators are efficient.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.3442622950819672,
          "f": 0.22459892608538995
        },
        "rouge-2": {
          "r": 0.023121387283236993,
          "p": 0.04395604395604396,
          "f": 0.030303025785411608
        },
        "rouge-l": {
          "r": 0.15873015873015872,
          "p": 0.32786885245901637,
          "f": 0.2139037389196146
        }
      }
    },
    {
      "paper_id": "cs.LG.math/ST/2503.07453v1",
      "true_abstract": "Language model alignment (or, reinforcement learning) techniques that\nleverage active exploration -- deliberately encouraging the model to produce\ndiverse, informative responses -- offer the promise of super-human\ncapabilities. However, current understanding of algorithm design primitives for\ncomputationally efficient exploration with language models is limited. To\nbetter understand how to leverage access to powerful pre-trained generative\nmodels to improve the efficiency of exploration, we introduce a new\ncomputational framework for RL with language models, in which the learner\ninteracts with the model through a sampling oracle. Focusing on the linear\nsoftmax model parameterization, we provide new results that reveal the\ncomputational-statistical tradeoffs of efficient exploration:\n  1. Necessity of coverage: Coverage refers to the extent to which the\npre-trained model covers near-optimal responses -- a form of hidden knowledge.\nWe show that coverage, while not necessary for data efficiency, lower bounds\nthe runtime of any algorithm in our framework.\n  2. Inference-time exploration: We introduce a new algorithm, SpannerSampling,\nwhich obtains optimal data efficiency and is computationally efficient whenever\nthe pre-trained model enjoys sufficient coverage, matching our lower bound.\nSpannerSampling leverages inference-time computation with the pre-trained model\nto reduce the effective search space for exploration.\n  3. Insufficiency of training-time interventions: We contrast the result above\nby showing that training-time interventions that produce proper policies cannot\nachieve similar guarantees in polynomial time.\n  4. Computational benefits of multi-turn exploration: Finally, we show that\nunder additional representational assumptions, one can achieve improved runtime\n(replacing sequence-level coverage with token-level coverage) through\nmulti-turn exploration.",
      "generated_abstract": "ment learning (RL) has achieved remarkable success by leveraging\nthe computational advantage of neural networks. In particular, recent\nreinforcement learning approaches, such as SAC and TD3, have demonstrated\nsuperior performance in complex, high-dimensional environments with multiple\nstate dimensions and high-dimensional actions. However, the computational\nadvantage of neural networks is limited by their memory and computation\nconstraints. In this work, we investigate how the computational advantage of\nneural networks is reflected in the exploration process. We propose the\nfollowing question: ``Is a good foundation necessary for efficient reinforcement\nlearning?'' The answer to this question is affirmative. We show that, even if\nthe neural network is not trained to learn from experience, it can still\nefficiently explore the environment and learn from it. Our findings are\ncritical for understanding the computational advantage of neural networks,\nallowing us to better understand the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17419354838709677,
          "p": 0.28125,
          "f": 0.21513943750734127
        },
        "rouge-2": {
          "r": 0.017316017316017316,
          "p": 0.03278688524590164,
          "f": 0.022662884995145714
        },
        "rouge-l": {
          "r": 0.15483870967741936,
          "p": 0.25,
          "f": 0.19123505503722177
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.15090v1",
      "true_abstract": "The R package bsvars provides a wide range of tools for empirical\nmacroeconomic and financial analyses using Bayesian Structural Vector\nAutoregressions. It uses frontier econometric techniques and C++ code to ensure\nfast and efficient estimation of these multivariate dynamic structural models,\npossibly with many variables, complex identification strategies, and non-linear\ncharacteristics. The models can be identified using adjustable exclusion\nrestrictions and heteroskedastic or non-normal shocks. They feature a flexible\nthree-level equation-specific local-global hierarchical prior distribution for\nthe estimated level of shrinkage for autoregressive and structural parameters.\nAdditionally, the package facilitates predictive and structural analyses such\nas impulse responses, forecast error variance and historical decompositions,\nforecasting, statistical verification of identification and hypotheses on\nautoregressive parameters, and analyses of structural shocks, volatilities, and\nfitted values. These features differentiate bsvars from existing R packages\nthat either focus on a specific structural model, do not consider\nheteroskedastic shocks, or lack the implementation using compiled code.",
      "generated_abstract": "age implements a Bayesian framework for structural vector\nautoregressions (SVARs). This approach, which is closely related to the\nBayesian Vector Autoregressive framework, allows for flexible and efficient\nparameter estimation and forecasting of macroeconomic variables. The\nBayesian framework uses a Markov Chain Monte Carlo (MCMC) algorithm to obtain\nposterior samples of the model parameters, which are then used to compute\nforecasts. The package also implements a variety of diagnostic tools, such as\nthe Sharpe ratio, which is used to assess the performance of the estimated\nparameters. In addition, the package includes functions for constructing\nforecasts using different forecasting strategies, including the\nKalman-Bucy-Nakagami-Malliavin filter and the Extended Kalman-Bucy filter.\nFinally, the package provides functions for calculating the forec",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21428571428571427,
          "p": 0.3037974683544304,
          "f": 0.2513088956728161
        },
        "rouge-2": {
          "r": 0.026845637583892617,
          "p": 0.037037037037037035,
          "f": 0.031128399796515473
        },
        "rouge-l": {
          "r": 0.1875,
          "p": 0.26582278481012656,
          "f": 0.21989528310737105
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2502.13431v1",
      "true_abstract": "This study proposes a novel functional vector autoregressive framework for\nanalyzing network interactions of functional outcomes in panel data settings.\nIn this framework, an individual's outcome function is influenced by the\noutcomes of others through a simultaneous equation system. To estimate the\nfunctional parameters of interest, we need to address the endogeneity issue\narising from these simultaneous interactions among outcome functions. This\nissue is carefully handled by developing a novel functional moment-based\nestimator. We establish the consistency, convergence rate, and pointwise\nasymptotic normality of the proposed estimator. Additionally, we discuss the\nestimation of marginal effects and impulse response analysis. As an empirical\nillustration, we analyze the demand for a bike-sharing service in the U.S. The\nresults reveal statistically significant spatial interactions in bike\navailability across stations, with interaction patterns varying over the time\nof day.",
      "generated_abstract": "Functional Network Autoregressive (FNAR) models are a versatile class of\nmodels for panel data that provide a flexible framework for modelling\ninteractions and conditional dependence among time-varying variables. The\nmodelling framework is based on a functional time series framework, where the\ntime series is modelled as a function of the interaction terms, and the\ndependencies among variables are modelled using functional networks. The model\ncan be applied to panel data with time-varying exogenous variables and\ninteractions. This paper presents a new FNAR model that extends the existing\nversion to allow for the modeling of interactions and conditional dependence\namong time-varying variables. This includes the case of time-varying exogenous\nvariables, which is the main focus of this paper. The model is illustrated\nthrough a simulation study, and then applied to a real data set.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.2972972972972973,
          "f": 0.2543352552200208
        },
        "rouge-2": {
          "r": 0.03759398496240601,
          "p": 0.04132231404958678,
          "f": 0.03937007375131814
        },
        "rouge-l": {
          "r": 0.1717171717171717,
          "p": 0.22972972972972974,
          "f": 0.19653178701192836
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.q-bio/SC/2503.04677v1",
      "true_abstract": "We present a minimal model to analyze the capacitive response of a biological\nmembrane subjected to a step voltage via blocking electrodes. Through a\nperturbative analysis of the underlying electrolyte transport equations, we\nshow that the leading-order relaxation of the transmembrane potential is\ngoverned by a capacitive timescale, ${\\tau_{\\rm C} =\\dfrac{\\lambda_{\\rm\nD}L}{D}\\left(\\dfrac{2+\\Gamma\\delta^{\\rm M}/L}{4+\\Gamma\\delta^{\\rm\nM}/\\lambda_{\\rm D}}\\right)}$, where $\\lambda_{\\rm D}$ is the Debye screening\nlength, $L$ is the electrolyte width, $\\Gamma$ is the ratio of the dielectric\npermittivity of the electrolyte to the membrane, $\\delta^{\\rm M}$ is the\nmembrane thickness, and $D$ is the ionic diffusivity. This timescale is\nconsiderably shorter than the traditional RC timescale ${\\lambda_{\\rm D} L /\nD}$ for a bare electrolyte due to the membrane's low dielectric permittivity\nand finite thickness. Beyond the linear regime, however, salt diffusion in the\nbulk electrolyte drives a secondary, nonlinear relaxation process of the\ntransmembrane potential over a longer timescale ${\\tau_{\\rm L} =L^2/4\\pi^2 D}$.\nA simple equivalent-circuit model accurately captures the linear behavior, and\nthe perturbation expansion remains applicable across the entire range of\nobserved physiological transmembrane potentials. Together, these findings\nunderscore the importance of the faster capacitive timescale and nonlinear\neffects on the bulk diffusion timescale in determining transmembrane potential\ndynamics for a range of biological systems.",
      "generated_abstract": "of capacitive properties of biological membranes has gained\nsignificant interest in recent years due to the increasing complexity of\nmembrane systems and the need for better understanding of their response to\nelectrochemical stimuli. Here, we report a capacitive response of a bacterial\nmembrane under a voltage applied across the pore. The capacitive response is\nstrongly affected by the concentration of the ionic liquid, which can be\ncontrolled by changing the polarity of the voltage. Our findings suggest that\nthe membrane's response is controlled by the electrostatic potential difference\nbetween the pore and the electrolyte, which is significantly affected by the\nionic liquid. The results indicate that the capacitive response of the membrane\ncan be controlled by controlling the polarity of the voltage. The study\nhighlights the potential of electrolyte-driven ionic liquids in controlling the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.184,
          "p": 0.323943661971831,
          "f": 0.23469387293054986
        },
        "rouge-2": {
          "r": 0.08064516129032258,
          "p": 0.13513513513513514,
          "f": 0.10101009632894625
        },
        "rouge-l": {
          "r": 0.168,
          "p": 0.29577464788732394,
          "f": 0.21428570966524377
        }
      }
    },
    {
      "paper_id": "nucl-th.nucl-th/2503.09204v1",
      "true_abstract": "The possibility of observing wobbling mode in the even-even systems of 76Ge,\n112Ru, 188,192Os, 192Pt and 232Th is explored using the triaxial projected\nshell model approach. These nuclei are known to have {\\gamma}-bands whose\nodd-spin members are lower than the average of the neighbouring even-spin\nstates. It is shown through a detailed analysis of the excitation energies and\nthe electromagnetic transition probabilities that the observed band structures\nin these nuclei except for 232Th can be characterised as originating from the\nwobbling motion. It is further demonstrated that quasiparticle alignment is\nresponsible for driving the systems to the wobbling mode.",
      "generated_abstract": "The investigation of the microscopic mechanism of the angular momentum\ndistribution in the nucleus is important for the understanding of the\nmicroscopic origin of the observed angular momentum distribution in even-even\nnuclei. We study the spin-orbit interaction between the nucleon and the\ndeuteron in the nucleus in the framework of the nuclear density-dependent\nTl-matrix model. We find that the spin-orbit coupling has a significant role\nin the nucleus, particularly in the $^{208}$Pb nucleus. The spin-orbit coupling\nis not sufficient to explain the observed angular momentum distribution in the\n$^{208}$Pb nucleus. We also find that the deuteron binding energy is also\nimportant in the observed angular momentum distribution in the $^{208}$Pb\nnucleus.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19718309859154928,
          "p": 0.2857142857142857,
          "f": 0.233333328501389
        },
        "rouge-2": {
          "r": 0.05319148936170213,
          "p": 0.06666666666666667,
          "f": 0.05917159269633457
        },
        "rouge-l": {
          "r": 0.16901408450704225,
          "p": 0.24489795918367346,
          "f": 0.1999999951680557
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/ST/2411.06080v1",
      "true_abstract": "Portfolio diversification, traditionally measured through asset correlations\nand volatilitybased metrics, is fundamental to managing financial risk.\nHowever, existing diversification metrics often overlook non-numerical\nrelationships between assets that can impact portfolio stability, particularly\nduring market stresses. This paper introduces the lexical ratio (LR), a novel\nmetric that leverages textual data to capture diversification dimensions absent\nin standard approaches. By treating each asset as a unique document composed of\nsectorspecific and financial keywords, the LR evaluates portfolio\ndiversification by distributing these terms across assets, incorporating\nentropy-based insights from information theory. We thoroughly analyze LR's\nproperties, including scale invariance, concavity, and maximality,\ndemonstrating its theoretical robustness and ability to enhance risk-adjusted\nportfolio returns. Using empirical tests on S&P 500 portfolios, we compare LR's\nperformance to established metrics such as Markowitz's volatility-based\nmeasures and diversification ratios. Our tests reveal LR's superiority in\noptimizing portfolio returns, especially under varied market conditions. Our\nfindings show that LR aligns with conventional metrics and captures unique\ndiversification aspects, suggesting it is a viable tool for portfolio managers.",
      "generated_abstract": "uce the lexical ratio, a novel measure of portfolio diversification\nthat accounts for the relative importance of different asset classes. This\nmeasure provides a quantitative way to compare portfolios across different\nasset classes and also offers a simple way to assess how a portfolio changes\nover time. By leveraging the lexical ratio, we develop a simple strategy for\nportfolio construction that takes into account the relative importance of\ndifferent asset classes. The methodology is based on a simple model that\nincorporates the lexical ratio, which is a simple, straightforward, and\nmathematically tractable way to compare portfolios. We validate the\ntheoretical model through simulation and empirical analysis. We apply the\nmethodology to two large asset classes, equities and commodities, and compare\nthem with traditional diversification strategies. Our findings highlight the\npotential of the lexical ratio as a novel and simple way",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.208955223880597,
          "p": 0.35443037974683544,
          "f": 0.26291079345544316
        },
        "rouge-2": {
          "r": 0.047619047619047616,
          "p": 0.06779661016949153,
          "f": 0.05594405109687557
        },
        "rouge-l": {
          "r": 0.1791044776119403,
          "p": 0.3037974683544304,
          "f": 0.22535210800943384
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.10285v1",
      "true_abstract": "Accurate prediction of expected concentrations is essential for effective\ncatchment management, requiring both extensive monitoring and advanced modeling\ntechniques. However, due to limitations in the equation solving capacity, the\nintegration of monitoring and modeling has been suffering suboptimal\nstatistical approaches. This limitation results in models that can only\npartially leverage monitoring data, thus being an obstacle for realistic\nuncertainty assessments by overlooking critical correlations between both\nmeasurements and model parameters. This study presents a novel solution that\nintegrates catchment monitoring and a unified hieratical statistical catchment\nmodeling that employs a log-normal distribution for residuals within a\nleft-censored likelihood function to address measurements below detection\nlimits. This enables the estimation of concentrations within sub-catchments in\nconjunction with a source/fate sub-catchment model and monitoring data. This\napproach is possible due to a model builder R package denoted RTMB. The\nproposed approach introduces a statistical paradigm based on a hierarchical\nstructure, capable of accommodating heterogeneous sampling across various\nsampling locations and the authors suggest that this also will encourage\nfurther refinement of other existing modeling platforms within the scientific\ncommunity to improve synergy with monitoring programs. The application of the\nmethod is demonstrated through an analysis of nickel concentrations in Danish\nsurface waters.",
      "generated_abstract": "oring of water quality in surface waters is a complex task due to\nthe heterogeneity of the water body, the variety of water quality parameters,\nand the wide range of water quality states, from clean to polluted. In\nparticular, the analysis of water quality at the stream level is extremely\nchallenging due to the low spatial resolution of the measurements and the\nvarious water quality parameters that can be measured. To address these\nchallenges, the modeling of water quality at the stream level is essential.\nHowever, the modelling of water quality requires the identification of\nwater quality states at the stream level. The identification of water quality\nstates at the stream level is a challenging task because the water quality\nparameters that can be measured at the stream level are very diverse and are\noften related to the presence of other pollutants, which can interfere with the\nmeasurement of the water quality parameters. To address this issue",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17777777777777778,
          "p": 0.34285714285714286,
          "f": 0.23414633696609166
        },
        "rouge-2": {
          "r": 0.04591836734693878,
          "p": 0.08571428571428572,
          "f": 0.059800659908831384
        },
        "rouge-l": {
          "r": 0.15555555555555556,
          "p": 0.3,
          "f": 0.2048780442831649
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09577v1",
      "true_abstract": "Chip-firing is a combinatorial game played on a graph in which we place and\ndisperse chips on vertices until a stable configuration is reached. We study a\nchip-firing variant played on an infinite rooted directed $k$-ary tree, where\nwe place $k^n$ chips labeled $0,1,\\dots, k^n-1$ on the root for some\nnonnegative integer $n$, and we say a vertex $v$ can fire if it has at least\n$k$ chips. A vertex fires by dispersing one chip to each out-neighbor. Once\nevery vertex has less than $k$ chips, we reach a stable configuration since no\nvertex can fire. In this paper, we focus on stable configurations resulting\nfrom applying a strategy $F_w$ corresponding to a permutation $w = w_1w_2\\dots\nw_n\\in S_n$: for each vertex $v$ on level $i$ of the $k$-ary tree, the chip\nwith $j$ as its $w_i$th most significant digit in the $k$-ary expansion gets\nsent to the $(j+1)$st child of $v$. We express the stable configuration as a\npermutation, and we explore the properties of these permutations, such as the\nnumber of inversions, descents, and the descent set.",
      "generated_abstract": "We study the labeled chip-firing problem on $k$-ary trees. We propose a\nstrategy that uses permutation groups to compute a winning strategy. The\nstrategy requires a computer to compute a permutation of the input sequence on\nthe tree. The strategy is simple, but it is not obvious how to implement it.\nOur strategy is inspired by the work of Hajiaghayi, Khot, and Tardos on\ncomputing winning strategies for the labeled chip-firing problem on\n$k$-ary trees. We use a variant of their approach. We prove that our strategy\nis efficient and that it is the best known strategy in the case when the tree\nhas no loops.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1826086956521739,
          "p": 0.35,
          "f": 0.23999999549387763
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.06593406593406594,
          "f": 0.046332041773975194
        },
        "rouge-l": {
          "r": 0.17391304347826086,
          "p": 0.3333333333333333,
          "f": 0.22857142406530614
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.q-bio/SC/2311.13203v2",
      "true_abstract": "Filamentous viruses like influenza and torovirus often display systematic\nbends and arcs of mysterious physical origin. We propose that such viruses\nundergo an instability from a cylindrically symmetric to a toroidally curved\nstate. This ``toro-elastic'' state emerges via spontaneous symmetry breaking\nunder prestress due to short range spike protein interactions magnified by %the\nfilament's surface topography. Once surface stresses are sufficiently large,\nthe filament buckles and the curved state constitutes a soft mode that can\npotentially propagate through the filament's material frame around a\nmexican-hat-type potential. In the mucus of our airways, which constitutes a\nsoft, porous 3D network, glycan chains are omnipresent and influenza's spike\nproteins are known to efficiently bind and cut them. We next show that such a\nnon-equilibrium enzymatic reaction can induce spontaneous rotation of the\ncurved state, leading to a whole body reshaping propulsion similar to -- but\ndifferent from -- eukaryotic flagella and spirochetes.",
      "generated_abstract": "theme in the evolution of viruses is that they are designed to\nbe able to infect and manipulate cells. This has enabled the development of\nviruses that can infect and manipulate mammalian cells as well as infect and\nmodify cells of other organisms. One example is the mammalian virus that\ninfects and replicates inside the cells of the human fetal lung, causing\nhemorrhagic disease. The virus is a bacteriophage that infects and replicates\ninside the cells of the human fetal lung, causing hemorrhagic disease. The\nvirus is the human fetal lung bacteriophage (HFLB), which infects and replicates\ninside the cells of the human fetal lung, causing hemorrhagic disease. HFLB is a\nstrain of the human fetal lung bacteriophage (HFLB",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1,
          "p": 0.22,
          "f": 0.13749999570312516
        },
        "rouge-2": {
          "r": 0.013888888888888888,
          "p": 0.02702702702702703,
          "f": 0.0183486193687411
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.22,
          "f": 0.13749999570312516
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.08938v1",
      "true_abstract": "Alzheimer's disease (AD) is driven by the accumulation of amyloid-beta\n(Abeta) proteins in the brain, leading to memory loss and cognitive decline.\nWhile monoclonal antibodies targeting Abetahave been approved, optimizing their\nuse to maximize benefits while minimizing side effects remains a challenge.\nThis study develops a mathematical model to describe Abeta aggregation,\ncapturing its progression from monomers to toxic oligomers, protofibrils, and\nfibrils using mass-action kinetics and coarse-grained modeling. The model is\ncalibrated with experimental data, incorporating parameter estimation and\nsensitivity analysis to ensure accuracy. An optimal control framework is\nintroduced to determine the best drug dosing strategy that reduces toxic Abeta\naggregates while minimizing adverse effects, such as amyloid-related imaging\nabnormalities (ARIA). Results indicate that Donanemab achieves the greatest\nreduction in fibrils. This work provides a quantitative framework for\noptimizing AD treatment strategies, offering insights into balancing\ntherapeutic efficacy and safety.",
      "generated_abstract": "ew presents a data-driven approach to develop models that integrate\ndata from multiple sources to predict the binding of amyloid-beta (Abeta)\nprotein to neuronal membranes. This approach includes amyloid-beta (Abeta)\nstructures and their interactions with neuronal membranes, as well as\nelectrochemical measurements of neuronal membranes. We first introduce the\nfundamental concepts of membrane electrochemistry and membrane potential. We\nthen introduce data-driven modeling approaches for membrane electrochemistry\nincluding the Membrane Potential Modeling (MPM) framework and the\nMembrane-Electrochemical Potential (MEP) framework. We then describe the\npotential of data-driven modeling to predict membrane electrochemistry. We\nthen introduce data-driven modeling approaches for membrane-protein interactions\nincluding the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14782608695652175,
          "p": 0.2982456140350877,
          "f": 0.19767441417320186
        },
        "rouge-2": {
          "r": 0.014285714285714285,
          "p": 0.022988505747126436,
          "f": 0.017621140647015573
        },
        "rouge-l": {
          "r": 0.1391304347826087,
          "p": 0.2807017543859649,
          "f": 0.18604650719645766
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.physics/space-ph/2503.08520v1",
      "true_abstract": "Solitons are predominantly observed in near-earth plasmas as well as\nplanetary magnetospheres; however, their existence in the solar corona remains\nlargely unexplored, despite theoretical investigations. This study aims to\naddress this gap by examining the presence and dynamics of solitons in the\nsolar corona, particularly in the context of coronal heating. Utilizing\nobservational data from the Parker Solar Probe (PSP) and Solar and Heliospheric\nObservatory (SOHO) during the onset of a strong Coronal Mass Ejection (CME)\nevent, the analyses reveal a train of aperiodic solitons with increasing\namplitude preceding the eruption. A key finding of this study is that the\nobserved aperiodic soliton train serves as a potential candidate in\nfacilitating energy transfer through dissipation within the coronal plasma,\nhereby, influencing the initiation of solar eruptive events such as a CME. A\ndefining characteristic of this solitary train is its hypersonic and\nsuper-Alfvenic nature, evident from the presence of high Mach numbers that\nreinforces its role in plasma energy equilibration in the solar corona, thereby\ncontributing to plasma heating.",
      "generated_abstract": "n waves are one of the fundamental modes of the magnetic field in\nthe solar corona and are known to play a crucial role in the dynamics of the\nsolar corona. In the present study, we have observed the solitary waves in the\nsolar corona using the Parker Solar Probe (PSP) and the Solar and Heliospheric\nObservatory (SOHO) during their pass through the solar corona. We have\nobserved the magnetic field in the corona with a wavelength of 40 Mm and a\nperiod of 11.5 minutes. The Alfven wave observations performed using the PSP\nshowed the existence of both the solitary waves and group-velocity-matched\nsolitons. We also observed the solitary waves in the corona using the SOHO/LASCO\nand COR1 coronagraphs and showed the existence of the solit",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23423423423423423,
          "p": 0.4,
          "f": 0.2954545407961003
        },
        "rouge-2": {
          "r": 0.0949367088607595,
          "p": 0.15625,
          "f": 0.11811023151838324
        },
        "rouge-l": {
          "r": 0.1981981981981982,
          "p": 0.3384615384615385,
          "f": 0.24999999534155481
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.01514v1",
      "true_abstract": "Advancements in data collection have led to increasingly common repeated\nobservations with complex structures in biomedical studies. Treating these\nobservations as random objects, rather than summarizing features as vectors,\navoids feature extraction and better reflects the data's nature. Examples\ninclude repeatedly measured activity intensity distributions in physical\nactivity analysis and brain networks in neuroimaging. Testing whether these\nrepeated random objects differ across groups is fundamentally important;\nhowever, traditional statistical tests often face challenges due to the\nnon-Euclidean nature of metric spaces, dependencies from repeated measurements,\nand the unequal number of repeated measures. By defining within-subject\nvariability using pairwise distances between repeated measures and extending\nFr\\'echet analysis of variance, we develop a generalized Fr\\'echet test for\nexchangeable repeated random objects, applicable to general metric space-valued\ndata with unequal numbers of repeated measures. The proposed test can\nsimultaneously detect differences in location, scale, and within-subject\nvariability. We derive the asymptotic distribution of the test statistic, which\nfollows a weighted chi-squared distribution. Simulations demonstrate that the\nproposed test performs well across different types of random objects. We\nillustrate its effectiveness through applications to physical activity data and\nresting-state functional magnetic resonance imaging data.",
      "generated_abstract": "aper, we propose a new test for the object data with unequal repeated\nmeasurements. Our test is based on a generalized F\\'echet test, which is\ndeveloped for the object data with equal repeated measurements. In addition, we\nintroduce a new method to evaluate the performance of our test. Our method is\nbased on the generalized F\\'echet test, which is developed for the object data\nwith unequal repeated measurements. We propose a generalized F\\'echet test for\nthe object data with unequal repeated measurements, which is called the generalized\nF\\'echet test (GFT). The proposed GFT is based on the generalized F\\'echet test\nwith the generalized F\\'echet test (GFT-gf). The proposed GFT has a simple\nstructure. We propose a new method to evaluate the performance of our test. We\nalso show that our proposed test is more powerful than the generalized F\\'ech",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.39215686274509803,
          "f": 0.2209944710906261
        },
        "rouge-2": {
          "r": 0.03888888888888889,
          "p": 0.08860759493670886,
          "f": 0.054054049814403815
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.39215686274509803,
          "f": 0.2209944710906261
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.07102v1",
      "true_abstract": "The paper proposes a novel Economic Model Predictive Control (EMPC) scheme\nfor Autonomous Surface Vehicles (ASVs) to simultaneously address path following\naccuracy and energy constraints under environmental disturbances. By\nformulating lateral deviations as energy-equivalent penalties in the cost\nfunction, our method enables explicit trade-offs between tracking precision and\nenergy consumption. Furthermore, a motion-dependent decomposition technique is\nproposed to estimate terminal energy costs based on vehicle dynamics. Compared\nwith the existing EMPC method, simulations with real-world ocean disturbance\ndata demonstrate the controller's energy consumption with a 0.06 energy\nincrease while reducing cross-track errors by up to 18.61. Field experiments\nconducted on an ASV equipped with an Intel N100 CPU in natural lake\nenvironments validate practical feasibility, achieving 0.22 m average\ncross-track error at nearly 1 m/s and 10 Hz control frequency. The proposed\nscheme provides a computationally tractable solution for ASVs operating under\nresource constraints.",
      "generated_abstract": "r presents a distributed, robust, and scalable model predictive\ncontrol (MPC) framework for autonomous surface vehicles (ASVs) under\ndisturbances. The proposed approach incorporates a multi-agent system (MAS)\nframework, which allows for distributed computation and distributed updates to\nthe MPC controller. The MAS employs a hierarchical structure with the\ncoordinator as the lowest-level agent and the agents as higher-level agents.\nThe coordinator generates an energy-trajectory economic model to guide the\ndecision-making of the agents. The proposed model is validated using\nsimulation results, demonstrating that the proposed MAS-based control\napproach is robust to uncertainties. The effectiveness of the proposed\nframework is validated through an ASV testbed, where the coordinator is able to\nguide the agents to minimize energy consumption while minimizing travel time.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15966386554621848,
          "p": 0.24675324675324675,
          "f": 0.19387754625000012
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.03636363636363636,
          "f": 0.03149605808171693
        },
        "rouge-l": {
          "r": 0.15966386554621848,
          "p": 0.24675324675324675,
          "f": 0.19387754625000012
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.04067v1",
      "true_abstract": "As whole genomes become widely available, maximum likelihood and Bayesian\nphylogenetic methods are demonstrating their limits in meeting the escalating\ncomputational demands. Conversely, distance-based phylogenetic methods are\nefficient, but are rarely favoured due to their inferior performance. Here, we\nextend distance-based phylogenetics using an entropy-based likelihood of the\nevolution among pairs of taxa, allowing for fast Bayesian inference in\ngenome-scale datasets. We provide evidence of a close link between the\ninference criteria used in distance methods and Felsenstein's likelihood, such\nthat the methods are expected to have comparable performance in practice. Using\nthe entropic likelihood, we perform Bayesian inference on three phylogenetic\nbenchmark datasets and find that estimates closely correspond with previous\ninferences. We also apply this rapid inference approach to a 60-million-site\nalignment from 363 avian taxa, covering most avian families. The method has\noutstanding performance and reveals substantial uncertainty in the avian\ndiversification events immediately after the K-Pg transition event. The\nentropic likelihood allows for efficient Bayesian phylogenetic inference,\naccommodating the analysis demands of the genomic era.",
      "generated_abstract": "phylogenetics is a core tool in modern genomics, enabling inference\nof phylogenetic trees, species trees, and gene trees from high-throughput\nsequencing data. However, Bayesian phylogenetics faces limitations in the\npresence of missing data, and existing methods often struggle to accurately\naccount for the complex relationships among genes and species within a\nphylogenetic tree. In this work, we introduce a novel Bayesian distance-based\ndistance model, Generalised Bayesian distance (GBD), which addresses these\nlimitations by incorporating information about the distance between genes and\nspecies into the Bayesian distance model. GBD improves the accuracy of phylogenetic\ntrees and species trees by providing a more robust model of gene distance.\nFurthermore, GBD allows researchers to more accurately model gene-species\nrelationships, improving the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.2631578947368421,
          "f": 0.2094240789780983
        },
        "rouge-2": {
          "r": 0.006172839506172839,
          "p": 0.008928571428571428,
          "f": 0.00729926523949385
        },
        "rouge-l": {
          "r": 0.1565217391304348,
          "p": 0.23684210526315788,
          "f": 0.18848167060113497
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.15634v2",
      "true_abstract": "Instrumental variables (IV) estimation is a fundamental method in\neconometrics and statistics for estimating causal effects in the presence of\nunobserved confounding. However, challenges such as untestable model\nassumptions and poor finite sample properties have undermined its reliability\nin practice. Viewing common issues in IV estimation as distributional\nuncertainties, we propose DRIVE, a distributionally robust IV estimation\nmethod. We show that DRIVE minimizes a square root variant of ridge regularized\ntwo stage least squares (TSLS) objective when the ambiguity set is based on a\nWasserstein distance. In addition, we develop a novel asymptotic theory for\nthis estimator, showing that it achieves consistency without requiring the\nregularization parameter to vanish. This novel property ensures that the\nestimator is robust to distributional uncertainties that persist in large\nsamples. We further derive the asymptotic distribution of Wasserstein DRIVE and\npropose data-driven procedures to select the regularization parameter based on\ntheoretical results. Simulation studies demonstrate the superior finite sample\nperformance of Wasserstein DRIVE in terms of estimation error and out-of-sample\nprediction. Due to its regularization and robustness properties, Wasserstein\nDRIVE presents an appealing option when the practitioner is uncertain about\nmodel assumptions or distributional shifts in data.",
      "generated_abstract": "We propose a distributionally robust inference method for instrumental\nvariance estimation using linear regression models. The proposed estimator\nadapts to a broad class of nonlinear instruments by incorporating the\ndistribution of the associated conditional mean function. The approach is\ngeneralizable to both linear and nonlinear instruments, and its practical\napplicability is demonstrated using both synthetic and real-world data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13385826771653545,
          "p": 0.3695652173913043,
          "f": 0.1965317880036086
        },
        "rouge-2": {
          "r": 0.016483516483516484,
          "p": 0.05263157894736842,
          "f": 0.02510459887817143
        },
        "rouge-l": {
          "r": 0.13385826771653545,
          "p": 0.3695652173913043,
          "f": 0.1965317880036086
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.07509v1",
      "true_abstract": "Non-orthogonal multiple access (NOMA) has gained significant attention as a\npotential next-generation multiple access technique. However, its\nimplementation with finite-alphabet inputs faces challenges. Particularly, due\nto inter-user interference, superimposed constellations may have overlapping\nsymbols leading to high bit error rates when successive interference\ncancellation (SIC) is applied. To tackle the issue, this paper employs\nautoencoders to design interference-aware super-constellations. Unlike\nconventional methods where superimposed constellation may have overlapping\nsymbols, the proposed autoencoder-based NOMA (AE-NOMA) is trained to design\nsuper-constellations with distinguishable symbols at receivers, regardless of\nchannel gains. The proposed architecture removes the need for SIC, allowing\nmaximum likelihood-based approaches to be used instead. The paper presents the\nconceptual architecture, loss functions, and training strategies for AE-NOMA.\nVarious test results are provided to demonstrate the effectiveness of\ninterference-aware constellations in improving the bit error rate, indicating\nthe adaptability of AE-NOMA to different channel scenarios and its promising\npotential for implementing NOMA systems",
      "generated_abstract": "t of massive machine-type communications (mMTCs) has created new\nchallenges for wireless networks. Specifically, interference from a large number\nof mMTCs causes severe performance degradation in traditional wireless\nnetworks. To address this challenge, we propose a novel super-constellation\ndesign framework for multi-mMTC networks. This design framework is based on a\nsuper-constellation concept, which provides an efficient solution to the\nproblem of constellation design for multi-mMTC networks. The main contribution\nof this paper is the development of a novel interference-aware super-constellation\ndesign methodology. This methodology is based on the concept of interference\nconstraints, which allows for the efficient construction of the super-constellation\nin the presence of interference constraints. The proposed methodology is\nsimulation-based, and its performance is evaluated through an extensive\nsim",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17857142857142858,
          "p": 0.2777777777777778,
          "f": 0.2173912995841211
        },
        "rouge-2": {
          "r": 0.02054794520547945,
          "p": 0.02727272727272727,
          "f": 0.023437495098877978
        },
        "rouge-l": {
          "r": 0.16964285714285715,
          "p": 0.2638888888888889,
          "f": 0.2065217343667298
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.10600v1",
      "true_abstract": "This study examines the impact of monetary factors on the conversion of\nfarmland to renewable energy generation, specifically solar and wind, in the\ncontext of expanding U.S. energy production. We propose a new econometric\nmethod that accounts for the diverse circumstances of landowners, including\ntheir unordered alternative land use options, non-monetary benefits from\nfarming, and the influence of local regulations. We demonstrate that\nidentifying the cross elasticity of landowners' farming income in relation to\nthe conversion of farmland to renewable energy requires an understanding of\ntheir preferences. By utilizing county legislation that we assume to be shaped\nby land-use preferences, we estimate the cross-elasticities of farming income.\nOur findings indicate that monetary incentives may only influence landowners'\ndecisions in areas with potential for future residential development,\nunderscoring the importance of considering both preferences and regulatory\ncontexts.",
      "generated_abstract": "This paper investigates the relationship between landowner preferences and\nthe conversion of farmland to renewable energy production. Using data from\n2013 to 2020, we find that landowner preferences are the main driver of this\nchange, with an estimated elasticity of 0.66, which is significantly higher\nthan the estimated elasticity of 0.20 from a previous study. These results\nhighlight the importance of including preferences in models of landowner\nbehavior, which have typically been assumed to be linear. Our findings suggest\nthat policy-makers should focus on improving the incentives for renewable\nenergy production on farmland, especially as climate change increases the cost\nof energy production.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.29473684210526313,
          "p": 0.37333333333333335,
          "f": 0.3294117597750865
        },
        "rouge-2": {
          "r": 0.10852713178294573,
          "p": 0.14432989690721648,
          "f": 0.12389380040997745
        },
        "rouge-l": {
          "r": 0.2631578947368421,
          "p": 0.3333333333333333,
          "f": 0.2941176421280277
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.16313v1",
      "true_abstract": "Player tracking data have provided great opportunities to generate novel\ninsights into understudied areas of American football, such as pre-snap motion.\nUsing a Bayesian multilevel model with heterogeneous variances, we provide an\nassessment of NFL quarterbacks and their ability to synchronize the timing of\nthe ball snap with pre-snap movement from their teammates. We focus on passing\nplays with receivers in motion at the snap and running a route, and define the\nsnap timing as the time between the moment a receiver begins motioning and the\nball snap event. We assume a Gamma distribution for the play-level snap timing\nand model the mean parameter with player and team random effects, along with\nrelevant fixed effects such as the motion type identified via a Gaussian\nmixture model. Most importantly, we model the shape parameter with quarterback\nrandom effects, which enables us to estimate the differences in snap timing\nvariability among NFL quarterbacks. We demonstrate that higher variability in\nsnap timing is beneficial for the passing game, as it relates to facing less\nhavoc created by the opposing defense. We also obtain a quarterback leaderboard\nbased on our snap timing variability measure, and Patrick Mahomes stands out as\nthe top player.",
      "generated_abstract": "e a multilevel model for the conditional mean of a continuous-time\nvector of snap times from the National Football League (NFL) that allows for\nheterogeneous variances across teams and players. The proposed model is\nconstructed from a multilevel modeling perspective, where the number of\nlevels, or layers, of the model depend on the number of teams and players. The\nproposed model is estimated using a maximum likelihood approach. We apply the\nproposed model to the 2018-2022 NFL season and find that the number of snaps\nvaries significantly across teams, with the number of snaps being higher for\nteams that are more successful than teams that are less successful. The\nheterogeneity in the variance of the number of snaps across teams is\nparticularly pronounced for players. We discuss the implications of the\nheterogeneity in the variance of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17355371900826447,
          "p": 0.3,
          "f": 0.21989528331460223
        },
        "rouge-2": {
          "r": 0.016574585635359115,
          "p": 0.027777777777777776,
          "f": 0.020761240993763103
        },
        "rouge-l": {
          "r": 0.15702479338842976,
          "p": 0.2714285714285714,
          "f": 0.19895287493763888
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2412.11602v1",
      "true_abstract": "Multivariate Distributions are needed to capture the correlation structure of\ncomplex systems. In previous works, we developed a Random Matrix Model for such\ncorrelated multivariate joint probability density functions that accounts for\nthe non-stationarity typically found in complex systems. Here, we apply these\nresults to the returns measured in correlated stock markets. Only the knowledge\nof the multivariate return distributions allows for a full-fledged risk\nassessment. We analyze intraday data of 479 US stocks included in the S&P500\nindex during the trading year of 2014. We focus particularly on the tails which\nare algebraic and heavy. The non-stationary fluctuations of the correlations\nmake the tails heavier. With the few-parameter formulae of our Random Matrix\nModel we can describe and quantify how the empirical distributions change for\nvarying time resolution and in the presence of non-stationarity.",
      "generated_abstract": "tudy, we revisit multivariate distributions in non-stationary\ncomplex systems, focusing on correlated stock markets. We propose a\nmultivariate distribution model that is based on the Bayesian hierarchical\nmodel of \\citet{wu2016bayesian}, and we empirically validate it on the\ncorrelated European equity market. Our results reveal that the multivariate\ndistribution model offers a more flexible framework to capture the complex\ninteractions between stocks, which is in contrast to the traditional\nBayesian hierarchical model. In addition, we examine the performance of the\nmultivariate distribution model under different scenarios, including\nsimultaneous shocks, stock-level and market-level volatility, and\ncorrelation, and compare it with the traditional Bayesian hierarchical model.\nThe results indicate that the multivariate distribution model can effectively\ncapture the complex interactions between stock",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24731182795698925,
          "p": 0.3333333333333333,
          "f": 0.28395061239369007
        },
        "rouge-2": {
          "r": 0.0625,
          "p": 0.08163265306122448,
          "f": 0.0707964552650955
        },
        "rouge-l": {
          "r": 0.24731182795698925,
          "p": 0.3333333333333333,
          "f": 0.28395061239369007
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.08411v1",
      "true_abstract": "In this article, we prove that, given two finite connected graphs $\\Gamma_1$\nand $\\Gamma_2$, if the two right-angled Artin groups $A(\\Gamma_1)$ and\n$A(\\Gamma_2)$ are quasi-isometric, then the infinite pointed sums\n$\\bigvee_\\mathbb{N} \\Gamma_1^{\\bowtie}$ and $\\bigvee_\\mathbb{N}\n\\Gamma_2^{\\bowtie}$ are homotopy equivalent, where $\\Gamma_i^{\\bowtie}$ denotes\nthe simplicial complex whose vertex-set is $\\Gamma_i$ and whose simplices are\ngiven by joins. These invariants are extracted from a study, of independent\ninterest, of the homotopy types of several complexes of hyperplanes in\nquasi-median graphs (such as one-skeleta of CAT(0) cube complexes). For\ninstance, given a quasi-median graph $X$, the \\emph{crossing complex}\n$\\mathrm{Cross}^\\triangle(X)$ is the simplicial complex whose vertices are the\nhyperplanes (or $\\theta$-classes) of $X$ and whose simplices are collections of\npairwise transverse hyperplanes. When $X$ has no cut-vertex, we show that\n$\\mathrm{Cross}^\\triangle(X)$ is homotopy equivalent to the pointed sum of the\nlinks of all the vertices in the prism-completion $X^\\square$ of $X$.",
      "generated_abstract": "er the homotopy type of a pair of quasimedian graphs $G$ and $H$ and\ninvestigate the homotopy type of the complex of hyperplanes in $G$ with respect\nto the complex of hyperplanes in $H$. We prove that the homotopy type of the\ncomplex of hyperplanes in $G$ is homotopy equivalent to the homotopy type of\nthe pair of pointed sets $(X,x)$ where $X$ is the complex of hyperplanes in\n$G$ and $x$ is the pair of pointed sets $(X_H,h)$ where $X_H$ is the\ncomplex of hyperplanes in $H$ and $h$ is the map $\\varphi$ which takes $H$ to\n$G$. We show that the homotopy type of the complex of hyperplanes in $H$ is\nhomotopy equivalent to the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1956521739130435,
          "p": 0.47368421052631576,
          "f": 0.2769230727857988
        },
        "rouge-2": {
          "r": 0.07352941176470588,
          "p": 0.16393442622950818,
          "f": 0.10152283836429714
        },
        "rouge-l": {
          "r": 0.18478260869565216,
          "p": 0.4473684210526316,
          "f": 0.2615384574011835
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/AR/2503.08968v1",
      "true_abstract": "Homomorphic encryption (HE) allows secure computation on encrypted data\nwithout revealing the original data, providing significant benefits for\nprivacy-sensitive applications. Many cloud computing applications (e.g., DNA\nread mapping, biometric matching, web search) use exact string matching as a\nkey operation. However, prior string matching algorithms that use homomorphic\nencryption are limited by high computational latency caused by the use of\ncomplex operations and data movement bottlenecks due to the large encrypted\ndata size. In this work, we provide an efficient algorithm-hardware codesign to\naccelerate HE-based secure exact string matching. We propose CIPHERMATCH, which\n(i) reduces the increase in memory footprint after encryption using an\noptimized software-based data packing scheme, (ii) eliminates the use of costly\nhomomorphic operations (e.g., multiplication and rotation), and (iii) reduces\ndata movement by designing a new in-flash processing (IFP) architecture. We\ndemonstrate the benefits of CIPHERMATCH using two case studies: (1) Exact DNA\nstring matching and (2) encrypted database search. Our pure software-based\nCIPHERMATCH implementation that uses our memory-efficient data packing scheme\nimproves performance and reduces energy consumption by 42.9X and 17.6X,\nrespectively, compared to the state-of-the-art software baseline. Integrating\nCIPHERMATCH with IFP improves performance and reduces energy consumption by\n136.9X and 256.4X, respectively, compared to the software-based CIPHERMATCH\nimplementation.",
      "generated_abstract": "e a novel method for accelerating string matching in\nhigh-throughput settings. The method is inspired by the idea of data\npacking, which has been shown to achieve significant gains in performance when\napplied to various types of data, including strings. In this work, we focus on\nhomomorphic encryption-based string matching. The proposed method leverages the\nmemory-efficient data packing technique for accelerating this task, which is\nknown to be effective in high-throughput settings. In particular, we design an\nin-flight process that performs data packing and string matching in a\nmemory-efficient manner, achieving significant gains in performance. Our\nexperiments demonstrate that the proposed method achieves an average\nperformance gain of 27.8% compared to the baseline method, with a significant\ndrop in the total time to match a string. Additionally, we show that our\nmethod also offers a significant reduction",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2246376811594203,
          "p": 0.36470588235294116,
          "f": 0.27802690111202727
        },
        "rouge-2": {
          "r": 0.0481283422459893,
          "p": 0.072,
          "f": 0.0576923028897522
        },
        "rouge-l": {
          "r": 0.21014492753623187,
          "p": 0.3411764705882353,
          "f": 0.26008968138108557
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.14498v1",
      "true_abstract": "This paper deals with a nonparametric Nadaraya-Watson (NW) estimator of the\ntransition density function computed from independent continuous observations\nof a diffusion process. A risk bound is established on this estimator. The\npaper also deals with an extension of the penalized comparison to overfitting\nbandwidths selection method for our NW estimator. Finally, numerical\nexperiments are provided.",
      "generated_abstract": "ition density function (TDF) plays a crucial role in various\nprobability theory and statistical problems. However, the TDF of diffusions\nrequires special treatment due to its complex structure. To estimate the TDF,\nthe Nadaraya-Watson (NW) estimator has been widely applied, but it suffers from\nits inability to provide a robust estimator for the TDF. In this paper, we\npropose a novel NW estimator for the TDF based on the pseudo-likelihood. The\nnew estimator is designed to provide a robust estimator for the TDF by\nconstructing the pseudo-likelihood as a function of the TDF. The pseudo-likelihood\nis constructed by employing the Bayesian information criterion (BIC) in a way\nthat the TDF is modeled as a mixture of Gaussian density functions, which is\nthe key to constructing the pseudo-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3404255319148936,
          "p": 0.20512820512820512,
          "f": 0.25599999530752005
        },
        "rouge-2": {
          "r": 0.09433962264150944,
          "p": 0.045454545454545456,
          "f": 0.06134968886296092
        },
        "rouge-l": {
          "r": 0.2978723404255319,
          "p": 0.1794871794871795,
          "f": 0.2239999953075201
        }
      }
    },
    {
      "paper_id": "math.AP.math/AP/2503.10495v1",
      "true_abstract": "In the present work, we develop a comprehensive and rigorous analytical\nframework for a non-local phase-field model that describes tumour growth\ndynamics. The model is derived by coupling a non-local Cahn-Hilliard equation\nwith a parabolic reaction-diffusion equation, which accounts for both phase\nsegregation and nutrient diffusion. Previous studies have only considered\nsymmetric potentials for similar models. However, in the biological context of\ncell-to-cell adhesion, single-well potentials, like the so-called Lennard-Jones\npotential, seem physically more appropriate. The Cahn-Hilliard equation with\nthis kind of potential has already been analysed. Here, we take a step forward\nand consider a more refined model. First, we analyse the model with a viscous\nrelaxation term in the chemical potential and subject to suitable initial and\nboundary conditions. We prove the existence of solutions, a separation property\nfor the phase variable, and a continuous dependence estimate with respect to\nthe initial data. Finally, via an asymptotic analysis, we recover the existence\nof a weak solution to the initial and boundary value problem without viscosity,\nprovided that the chemotactic sensitivity is small enough.",
      "generated_abstract": "r presents a non-local phase-field model for tumour growth in\nan anisotropic, anisotropically deformed elastic material. The model is based\non a modified Lennard-Jones potential and incorporates a phase field to model\nthe anisotropic deformation of the material. The model is derived for the\ntumour-free case, and the effects of the deformation are studied using the\nresults of a series of local simulations. In the tumour-containing case, the\nmodel is coupled to a tumour model, and the effects of tumour growth are\nstudied using a series of local simulations. The results of the local simulations\nsuggest that the model can capture the essential features of tumour growth in\nan isotropic material, including the formation of elongated foci of tumour\ngrowth. In addition, the model is used to investigate the effects of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17647058823529413,
          "p": 0.3387096774193548,
          "f": 0.2320441943908917
        },
        "rouge-2": {
          "r": 0.07975460122699386,
          "p": 0.12871287128712872,
          "f": 0.09848484376061778
        },
        "rouge-l": {
          "r": 0.15966386554621848,
          "p": 0.3064516129032258,
          "f": 0.20994474687707954
        }
      }
    },
    {
      "paper_id": "cs.CE.q-bio/QM/2503.05113v2",
      "true_abstract": "The process of setting up and successfully running Molecular Dynamics\nSimulations (MDS) is outlined to be incredibly labour and computationally\nexpensive with a very high barrier to entry for newcomers wishing to utilise\nthe benefits and insights of MDS. Here, presented, is a unique Free and\nOpen-Source Software (FOSS) solution that aims to not only reduce the barrier\nof entry for new Molecular Dynamics (MD) users, but also significantly reduce\nthe setup time and hardware utilisation overhead for even highly experienced MD\nresearchers. This is accomplished through the creation of the Molecular\nDynamics Simulation Generator and Analysis Tool (MDSGAT) which currently serves\nas a viable alternative to other restrictive or privatised MDS Graphical\nsolutions with a unique design that allows for seamless collaboration and\ndistribution of exact MD simulation setups and initialisation parameters\nthrough a single setup file. This solution is designed from the start with a\nmodular mindset allowing for additional software expansion to incorporate\nnumerous extra MDS packages and analysis methods over time",
      "generated_abstract": "dynamics simulation is a fundamental tool in modern life sciences\nand biomedicine, enabling the exploration of chemical and biological\nphenomena. However, the complex nature of these systems often requires the\ncollaboration of experts from various fields to achieve optimal performance and\naccuracy. This paper introduces Molecular Dynamics Simulation Group Attention\n(MDSGAT), a novel approach for automating molecular dynamics simulation\nprotocols. MDSGAT leverages a transformer architecture to capture the\nrelationships among molecules within a protein complex, enabling the\nautomatic generation of customized molecular dynamics protocols. The model\ndemonstrates superior performance over existing state-of-the-art methods,\nachieving a mean absolute error of 0.0144 on the test set, as well as\nhigh-quality molecular dynamics simulations. The MDSGAT code is available",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15178571428571427,
          "p": 0.19318181818181818,
          "f": 0.16999999507200014
        },
        "rouge-2": {
          "r": 0.01910828025477707,
          "p": 0.026785714285714284,
          "f": 0.02230482785367914
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.18181818181818182,
          "f": 0.15999999507200013
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/CR/2503.09726v1",
      "true_abstract": "Graph Neural Networks (GNNs) are widely used and deployed for graph-based\nprediction tasks. However, as good as GNNs are for learning graph data, they\nalso come with the risk of privacy leakage. For instance, an attacker can run\ncarefully crafted queries on the GNNs and, from the responses, can infer the\nexistence of an edge between a pair of nodes. This attack, dubbed as a\n\"link-stealing\" attack, can jeopardize the user's privacy by leaking\npotentially sensitive information. To protect against this attack, we propose\nan approach called \"$(N)$ode $(A)$ugmentation for $(R)$estricting $(G)$raphs\nfrom $(I)$nsinuating their $(S)$tructure\" ($NARGIS$) and study its feasibility.\n$NARGIS$ is focused on reshaping the graph embedding space so that the\nposterior from the GNN model will still provide utility for the prediction task\nbut will introduce ambiguity for the link-stealing attackers. To this end,\n$NARGIS$ applies spectral clustering on the given graph to facilitate it being\naugmented with new nodes -- that have learned features instead of fixed ones.\nIt utilizes tri-level optimization for learning parameters for the GNN model,\nsurrogate attacker model, and our defense model (i.e. learnable node features).\nWe extensively evaluate $NARGIS$ on three benchmark citation datasets over\neight knowledge availability settings for the attackers. We also evaluate the\nmodel fidelity and defense performance on influence-based link inference\nattacks. Through our studies, we have figured out the best feature of $NARGIS$\n-- its superior fidelity-privacy performance trade-off in a significant number\nof cases. We also have discovered in which cases the model needs to be\nimproved, and proposed ways to integrate different schemes to make the model\nmore robust against link stealing attacks.",
      "generated_abstract": "Link stealing is a malicious attack that aims to steal network connections\nbetween two machines in a distributed network. To prevent this, several\ndistributed network protocols have been proposed that detect and counter\nattacks. Among them, DDOS (DDoS) attacks have been the focus of research. In\nthis work, we propose a novel counter-strategy against DDoS attacks. In our\ncounter-strategy, we propose a mechanism that learns from the network to\nprevent DDoS attacks. Our approach learns a set of learnable features that\ndetects and prevents DDoS attacks. The proposed counter-strategy is evaluated\nusing real-world data from the Internet.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12994350282485875,
          "p": 0.359375,
          "f": 0.19087136539384658
        },
        "rouge-2": {
          "r": 0.011583011583011582,
          "p": 0.033707865168539325,
          "f": 0.01724137550353499
        },
        "rouge-l": {
          "r": 0.12429378531073447,
          "p": 0.34375,
          "f": 0.18257261020712462
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.cond-mat/dis-nn/2503.09539v1",
      "true_abstract": "Quenched disorder in a solid state system can result in Anderson localization\nwhere electrons are exponentially localized and the system behaves like an\ninsulator. In this study, we investigate the effect of a DC electric field on\nAnderson localization. The study highlights the case of a one-dimensional\ninsulator chain with on-site disorder when a DC electric field is applied\nthroughout the chain. We study spectral properties of an Anderson localized\nsystem in equilibrium and out-of-equilibrium using a full lattice\nnonequilibrium Green's function method in the steady-state limit. Tuning the\ndisorder and the electric field strength results in the creation of exponential\nLifshitz tails near the band edge by strongly localized levels. These Lifshtiz\ntails create effects like insulator-to-metal transitions and contribute to\nnon-local hopping. The electric field causes gradual delocalization of the\nsystem and Anderson localization crossing over to Wannier Stark ladders at very\nstrong fields. Our study makes a comparison with the coherent potential\napproximation (CPA) highlighting some major differences and similarities in the\nphysics of disorder.",
      "generated_abstract": "We investigate the nonlinear electromagnetic response of a disordered\nnoninsulating lattice, where the lattice site is coupled to a nonlinear electric\nfield. We find that the response of the lattice is significantly affected by\nthe disorder, with the lattice showing different behavior depending on the\ndisorder strength. The spectral properties of the lattice show a change in\nbehavior when the disorder strength is varied. We show that the system can be\ndescribed by a single-band model, and the spectral properties of the lattice\ncan be understood as a function of the disorder strength.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2336448598130841,
          "p": 0.49019607843137253,
          "f": 0.31645569183063615
        },
        "rouge-2": {
          "r": 0.07096774193548387,
          "p": 0.14473684210526316,
          "f": 0.09523809082288583
        },
        "rouge-l": {
          "r": 0.22429906542056074,
          "p": 0.47058823529411764,
          "f": 0.3037974639825349
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/atm-clus/2502.16979v1",
      "true_abstract": "Technological advancements in generation of ultrafast and intense laser\npulses have enabled the real-time observation and control of charge migration\nin molecules on their natural timescale, which ranges from few femtoseconds to\nseveral hundreds of attoseconds. Present thesis discusses the effect of\nsymmetry on the adiabatic attosecond charge migration in different molecular\nsystems. The spatial representation of the charge migration is documented by\ntime-dependent electronic charge and flux densities. Furthermore, the induced\ncharge migration is imaged via time resolved x-ray diffraction (TRXD) with\natomic-scale spatiotemporal resolution in few cases.",
      "generated_abstract": "We investigate charge migration in molecular systems using a novel\nexperimental platform based on the trapping of polar molecules in the\nresonant regime of a high-harmonic generation laser. We observe a strong\ndependence of the trapping efficiency on the electric field strength, which is\nrelated to the localized oscillations of the electric field generated by the\nlaser. We find that the trapping efficiency of molecules in the gas phase can be\nmodulated by varying the driving frequency of the laser, and that the\ntrapping efficiency can be enhanced by a factor of 100 by applying an external\nmagnetic field. The results of this study are in good agreement with the\ntheoretical predictions of a molecular model, indicating that the observed\ntrapping efficiency can be used as a probe of charge migration in molecular\nsystems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2727272727272727,
          "p": 0.24324324324324326,
          "f": 0.25714285215918375
        },
        "rouge-2": {
          "r": 0.07228915662650602,
          "p": 0.05504587155963303,
          "f": 0.062499995091688756
        },
        "rouge-l": {
          "r": 0.21212121212121213,
          "p": 0.1891891891891892,
          "f": 0.1999999950163267
        }
      }
    },
    {
      "paper_id": "physics.atom-ph.physics/atom-ph/2503.08855v1",
      "true_abstract": "Assembly of ultracold polar molecules containing silver (Ag) from\nlaser-cooled atoms requires knowledge of the dynamic polarizabilities of Ag at\nconvenient laser wavelengths. We present calculations and analysis of the\nenergies and electric-dipole dc and ac polarizabilities of the low-lying states\nof neutral Ag. Calculations of the properties of the 4d^{10}x states, where\nx=5s,6s,7s,5p,6p,7p,5d,6d, and 4f, are performed using the linearized coupled\ncluster single-double method. The properties of the 4d^9 5s^2 ^2D_{5/2,3/2}\nstates are obtained within the framework of configuration interaction with 11\nand 17 electrons in the valence field. We analyze the different contributions\nto the polarizabilities and estimate the uncertainties of our predictions.",
      "generated_abstract": "We present a detailed analysis of the polarizabilities of the ground-state\nand the first excited state of the silver atom. We use the full non-relativistic\nscheme of the RPA to describe the electron-electron interaction and construct\nthe polarizabilities by means of the formalism of the canonical quantization of\nthe electronic wave function. We show that the polarizabilities of the\nground-state and the first excited state of the silver atom exhibit a significant\ndependence on the electron-electron interaction parameter, which is estimated\nusing the standard Hartree-Fock method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.2553191489361702,
          "f": 0.20168066748958416
        },
        "rouge-2": {
          "r": 0.061855670103092786,
          "p": 0.09523809523809523,
          "f": 0.07499999522578156
        },
        "rouge-l": {
          "r": 0.1527777777777778,
          "p": 0.23404255319148937,
          "f": 0.18487394480050856
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.05691v1",
      "true_abstract": "Prevailing methods of course allocation at undergraduate institutions involve\nreserving seats to give priority to designated groups of students. We introduce\na competitive equilibrium-based mechanism that assigns course seats using\nstudent preferences and course priorities. This mechanism satisfies approximate\nnotions of stability, efficiency, envy-freeness, and strategy-proofness. We\nevaluate its performance relative to a mechanism widely used in practice using\npreferences estimated from university data. Our empirical findings demonstrate\nan improvement in student satisfaction and allocation fairness. The number of\nstudents who envy another student of weakly lower priority declines by 8\npercent, or roughly 500 students.",
      "generated_abstract": "the allocation of undergraduate students to courses within a\nuniversity using a competitive market model. The university offers a\ncompetitive market of courses to students and the students are interested in\nattending only a few courses. We consider a situation where the university\nholds an open enrollment admissions session where students can apply to\nenroll in the university. At the start of the semester, students are randomly\nassigned to courses. The university offers a course to a student based on his\napplication to the university and his enrollment status. We analyze the\neffects of various market structures on the allocation of students to courses.\nWe show that the number of courses offered by a university is constrained by\nthe number of students it admits. We study the effects of a variety of market\nstructures, including a market in which students apply to the university and\ntheir enrollment status is random, a market in which",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2638888888888889,
          "p": 0.27941176470588236,
          "f": 0.2714285664326531
        },
        "rouge-2": {
          "r": 0.0425531914893617,
          "p": 0.03225806451612903,
          "f": 0.036697242801111675
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.2647058823529412,
          "f": 0.2571428521469389
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/CB/2411.00948v1",
      "true_abstract": "Conventional histopathology has long been essential for disease diagnosis,\nrelying on visual inspection of tissue sections. Immunohistochemistry aids in\ndetecting specific biomarkers but is limited by its single-marker approach,\nrestricting its ability to capture the full tissue environment. The advent of\nmultiplexed imaging technologies, like multiplexed immunofluorescence and\nspatial transcriptomics, allows for simultaneous visualization of multiple\nbiomarkers in a single section, enhancing morphological data with molecular and\nspatial information. This provides a more comprehensive view of the tissue\nmicroenvironment, cellular interactions, and disease mechanisms - crucial for\nunderstanding disease progression, prognosis, and treatment response. However,\nthe extensive data from multiplexed imaging necessitates sophisticated\ncomputational methods for preprocessing, segmentation, feature extraction, and\nspatial analysis. These tools are vital for managing large, multidimensional\ndatasets, converting raw imaging data into actionable insights. By automating\nlabor-intensive tasks and enhancing reproducibility and accuracy, computational\ntools are pivotal in diagnostics and research. This review explores the current\nlandscape of multiplexed imaging in pathology, detailing workflows and key\ntechnologies like PathML, an AI-powered platform that streamlines image\nanalysis, making complex dataset interpretation accessible for clinical and\nresearch settings.",
      "generated_abstract": "imaging analysis (MIA) is a powerful tool in pathology that enables\nanalysis of multiple tissues and cell types at the same time. It offers\nsignificant advantages over traditional two-dimensional (2D) image analysis,\nsuch as faster and more accurate detection of abnormalities and improved\ndetection of subtle changes. MIA techniques include fluorescence in situ\nhybridization (FISH), immunofluorescence (IF), and laser capture microdissection\n(LCM). These techniques provide a comprehensive view of tissues and cellular\ncomponents, offering a more accurate and comprehensive understanding of disease\nprogression and treatment responses. MIA has the potential to revolutionize\ndiagnosis and treatment of diseases by providing detailed insights into\nsubtle changes in cells, tissues, and organs. This review explores the\nadvancements in MIA techniques",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22556390977443608,
          "p": 0.35714285714285715,
          "f": 0.2764976911074774
        },
        "rouge-2": {
          "r": 0.05202312138728324,
          "p": 0.08181818181818182,
          "f": 0.0636042355304727
        },
        "rouge-l": {
          "r": 0.19548872180451127,
          "p": 0.30952380952380953,
          "f": 0.2396313316604728
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/NC/2502.13729v2",
      "true_abstract": "Human and animal memory for sequentially presented items is well-documented\nto be more accurate for those at the beginning and end of the sequence,\nphenomena known as the primacy and recency effects, respectively. By contrast,\nartificial neural network (ANN) models are typically designed with a memory\nthat decays monotonically over time. Accordingly, ANNs are expected to show the\nrecency effect but not the primacy effect. Contrary to this theoretical\nexpectation, however, the present study reveals a counterintuitive finding: a\nrecently developed ANN architecture, called structured state-space models,\nexhibits the primacy effect when trained and evaluated on a synthetic task that\nmirrors psychological memory experiments. Given that this model was originally\ndesigned for recovering neuronal activity patterns observed in biological\nbrains, this result provides a novel perspective on the psychological primacy\neffect while also posing a non-trivial puzzle for the current theories in\nmachine learning.",
      "generated_abstract": "pplications, the state of the system is not fully observed and\nstates are either observed or not observed. In the case of observations, it is\noften desirable to infer the system state. In this paper, we study the\nemergence of the primacy effect, where the model with the most informative\nobservations, i.e., the model with the largest likelihood, is inferred\nsuccessfully. We show that the primacy effect can be seen as a consequence of\nthe existence of an attractor that is the attractor of a Markov chain with\narbitrary time delay. In particular, we show that in the case of\nMarkov-chain-based models, the primacy effect can arise from the existence of\nan attractor that is the attractor of a state-space model. We show that the\nattractor of the state-space model can be described as a Markov chain with\narbitrary time",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21904761904761905,
          "p": 0.34328358208955223,
          "f": 0.2674418557091672
        },
        "rouge-2": {
          "r": 0.02158273381294964,
          "p": 0.028846153846153848,
          "f": 0.024691353128419743
        },
        "rouge-l": {
          "r": 0.1523809523809524,
          "p": 0.23880597014925373,
          "f": 0.18604650687195792
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.10251v1",
      "true_abstract": "Large language models based on transformer architectures have become integral\nto state-of-the-art natural language processing applications. However, their\ntraining remains computationally expensive and exhibits instabilities, some of\nwhich are expected to be caused by finite-precision computations. We provide a\ntheoretical analysis of the impact of round-off errors within the forward pass\nof a transformer architecture which yields fundamental bounds for these\neffects. In addition, we conduct a series of numerical experiments which\ndemonstrate the practical relevance of our bounds. Our results yield concrete\nguidelines for choosing hyperparameters that mitigate round-off errors, leading\nto more robust and stable inference.",
      "generated_abstract": "vances in large language models (LLMs) have revolutionized\nchallenging tasks such as question answering, text generation, and speech\nrecognition. While these models have demonstrated impressive performance,\nexisting studies often overlook the critical role of training data, which\ninfluences both model performance and computational efficiency. In this\nwork, we analyze the performance of LLMs trained on large corpora. Specifically,\nwe consider two popular LLMs, OpenAI's GPT-4o and GPT-4o-large, and\nevaluate the impact of data distribution on their performance. Our analysis\nshows that the performance gap between the two models is driven primarily by\ndifferences in the size of their vocabularies. We further propose a simple\nmethod to mitigate this gap, leveraging a simple modification to the\nimplementation of the LLM. Our method reduces the gap by 10\\%",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27848101265822783,
          "p": 0.24175824175824176,
          "f": 0.2588235244366783
        },
        "rouge-2": {
          "r": 0.041237113402061855,
          "p": 0.032520325203252036,
          "f": 0.036363631433471745
        },
        "rouge-l": {
          "r": 0.24050632911392406,
          "p": 0.2087912087912088,
          "f": 0.2235294067896195
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/SP/2503.07435v1",
      "true_abstract": "The adoption of Millimeter-Wave (mmWave) radar devices for human sensing,\nparticularly gait recognition, has recently gathered significant attention due\nto their efficiency, resilience to environmental conditions, and\nprivacy-preserving nature. In this work, we tackle the challenging problem of\nOpen-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlike\nmost existing research, which assumes a closed-set scenario, our work considers\nthe more realistic open-set case, where unknown subjects might be present at\ninference time, and should be correctly recognized by the system. Point clouds\nare well-suited for edge computing applications with resource constraints, but\nare more significantly affected by noise and random fluctuations than other\nrepresentations, like the more common micro-Doppler signature. This is the\nfirst work addressing open-set gait recognition with sparse point cloud data.\nTo do so, we propose a novel neural network architecture that combines\nsupervised classification with unsupervised reconstruction of the point clouds,\ncreating a robust, rich, and highly regularized latent space of gait features.\nTo detect unknown subjects at inference time, we introduce a probabilistic\nnovelty detection algorithm that leverages the structured latent space and\noffers a tunable trade-off between inference speed and prediction accuracy.\nAlong with this paper, we release mmGait10, an original human gait dataset\nfeaturing over five hours of measurements from ten subjects, under varied\nwalking modalities. Extensive experimental results show that our solution\nattains F1-Score improvements by 24% over state-of-the-art methods, on average,\nand across multiple openness levels.",
      "generated_abstract": "growth of wireless sensor networks (WSNs) and the increased\ndiversity of sensing modalities has significantly impacted gait recognition,\nparticularly for open-set gait recognition. Traditional gait recognition methods\noften focus on identifying gait categories, ignoring the dynamic nature of gait.\nTo address this issue, we propose an open-set gait recognition framework, which\nintegrates the open-set gait recognition model and the gait recognition model\ninto a unified framework. The open-set gait recognition model predicts the\nopen-set gait category and the gait recognition model predicts the gait\ncategory. To improve the recognition accuracy, we propose a novel approach to\nfine-tune the open-set gait recognition model using the gait recognition model\noutputs. Additionally, we propose a novel approach to fine-tune the gait\nrecognition model using the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13559322033898305,
          "p": 0.41379310344827586,
          "f": 0.20425531543105482
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.0875,
          "f": 0.04501607334911789
        },
        "rouge-l": {
          "r": 0.11299435028248588,
          "p": 0.3448275862068966,
          "f": 0.1702127622395655
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.physics/app-ph/2503.08941v1",
      "true_abstract": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors.",
      "generated_abstract": "halcogenide-based multilayer films have been investigated as\nstorage materials for high-temperature energy storage. Bismuth chalcogenide\n(BCZ) films, which are composed of a BCZ-based oxide sandwiched between two\nBCZ-based electrode materials, have been shown to have excellent\nelectrochemical stability, which is crucial for energy storage. In this\nstudy, a BCZT film was used as the electrode material, and a BCZT/LSMO/BCZT\nmultilayer film was fabricated. The electrochemical stability of the BCZT/LSMO/BCZT\nmultilayer film was investigated using cyclic voltammetry and impedance\nspectroscopy, and the electrochemical performance was evaluated by\ncycling-induced changes in capacitance. The results show that the electrochem",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26506024096385544,
          "p": 0.3384615384615385,
          "f": 0.29729729237125646
        },
        "rouge-2": {
          "r": 0.04807692307692308,
          "p": 0.056179775280898875,
          "f": 0.051813466532793305
        },
        "rouge-l": {
          "r": 0.21686746987951808,
          "p": 0.27692307692307694,
          "f": 0.24324323831720243
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.08885v1",
      "true_abstract": "Quasilinear systems with piecewise constant arguments of generalized type are\nunder investigation from the asymptotic point of view. The systems have\ndiscontinuous right-hand sides which are identified via a discrete-time map. It\nis rigorously proved that homoclinic and heteroclinic solutions are generated,\nand they are taken into account in the functional sense. The Banach fixed point\ntheorem is used for the verification. The hyperbolic set of solutions is also\ndiscussed, and an example supporting the theoretical findings is provided.",
      "generated_abstract": "We study the homoclinic and heteroclinic orbits of differential equations\nwith piecewise constant arguments of general type. We prove that in the case\nwhen the set of the solutions is a compact connected set, then the heteroclinic\ntrajectory is the unique solution of the differential equation, and the\nhomoclinic orbit is the unique solution of the system of differential\nequations. The results are illustrated by examples.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.29508196721311475,
          "p": 0.43902439024390244,
          "f": 0.35294117166282196
        },
        "rouge-2": {
          "r": 0.11538461538461539,
          "p": 0.16071428571428573,
          "f": 0.1343283533437293
        },
        "rouge-l": {
          "r": 0.2786885245901639,
          "p": 0.4146341463414634,
          "f": 0.3333333285255671
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.14041v1",
      "true_abstract": "This study investigates the effectiveness of fiscal policies on household\nconsumption, disposable income, and the propensity to consume during the\nCOVID-19 pandemic across Croatia, Slovakia, and Poland. The purpose is to\nassess how variations in government debt, expenditures, revenue, and subsidies\ninfluenced household financial behaviors in response to economic shocks. Using\na Markov Switching VAR model across three regimes: initial impact, peak crisis,\nand recovery.This analysis captures changes in household consumption,\ndisposable income, and consumption propensities under different fiscal policy\nmeasures.\n  The findings reveal that the Slovak Republic exhibited the highest fiscal\neffectiveness, demonstrating effective government policies that stimulated\nconsumer spending and supported household income during the pandemic. Croatia\nalso showed positive outcomes, particularly in terms of income, although rising\ngovernment debt posed challenges to overall effectiveness. Conversely, Poland\nfaced significant obstacles, with its fiscal measures leading to lower\nconsumption and income outcomes, indicating limited policy efficacy.\n  Conclusions emphasize the importance of tailored fiscal measures, as their\neffectiveness varied across countries and economic contexts. Recommendations\ninclude reinforcing consumption-supportive policies, particularly during crisis\nperiods, to stabilize income and consumption expectations. This study\nunderscores the significance of targeted fiscal actions in promoting household\nresilience and economic stability, as exemplified by the successful approach\ntaken by the Slovak Republic.",
      "generated_abstract": "r examines the impact of the COVID-19 pandemic on household savings in\nCentral Europe. Using a Markov switching VAR model with a shock to the interest\nrate, we explore the effects of the pandemic shock on savings and how it\naffects the interest rate. Our results suggest that the interest rate is more\nsensitive to the COVID-19 shock than savings. The results also indicate that\ngovernment fiscal policy is the main determinant of the interest rate, with\nthe effect of COVID-19 on the interest rate being more pronounced than the\neffect of the COVID-19 shock on savings. In addition, the results show that\ngovernment fiscal policy is more effective in curbing the COVID-19 shock than\ngovernment monetary policy. Furthermore, the results indicate that the\ninterest rate is more sensitive to government fiscal policy than to government",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1791044776119403,
          "p": 0.41379310344827586,
          "f": 0.2499999957834202
        },
        "rouge-2": {
          "r": 0.046153846153846156,
          "p": 0.09278350515463918,
          "f": 0.06164383117963065
        },
        "rouge-l": {
          "r": 0.17164179104477612,
          "p": 0.39655172413793105,
          "f": 0.23958332911675354
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2503.10435v1",
      "true_abstract": "When detecting anomalous sounds in complex environments, one of the main\ndifficulties is that trained models must be sensitive to subtle differences in\nmonitored target signals, while many practical applications also require them\nto be insensitive to changes in acoustic domains. Examples of such domain\nshifts include changing the type of microphone or the location of acoustic\nsensors, which can have a much stronger impact on the acoustic signal than\nsubtle anomalies themselves. Moreover, users typically aim to train a model\nonly on source domain data, which they may have a relatively large collection\nof, and they hope that such a trained model will be able to generalize well to\nan unseen target domain by providing only a minimal number of samples to\ncharacterize the acoustic signals in that domain. In this work, we review and\ndiscuss recent publications focusing on this domain generalization problem for\nanomalous sound detection in the context of the DCASE challenges on acoustic\nmachine condition monitoring.",
      "generated_abstract": "Domain shifts are a common issue in sound event detection, where the\nsound signals in different datasets exhibit varying levels of\nanomalies. This paper reviews recent studies that address domain shifts in\nsound event detection, focusing on the DCASE 2023 challenge and the DCASE\n2022 challenge. Specifically, we analyze the challenges of domain shift in\nsound event detection, including data distribution shifts, model shift, and\nanomaly shift. We then review recent studies that address these challenges,\nincluding data distribution shifts, model shift, and anomaly shift. We\nhighlight the importance of data distribution shifts, model shift, and\nanomaly shift, and provide insights into how to address them. We conclude by\noffering future research directions and highlighting key challenges for\ndomain-aware sound event detection.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22018348623853212,
          "p": 0.3380281690140845,
          "f": 0.2666666618895062
        },
        "rouge-2": {
          "r": 0.03184713375796178,
          "p": 0.05319148936170213,
          "f": 0.039840632765194756
        },
        "rouge-l": {
          "r": 0.21100917431192662,
          "p": 0.323943661971831,
          "f": 0.2555555507783951
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.06178v1",
      "true_abstract": "Parasite quiescence is the ability for the pathogen to be inactive, with\nrespect to metabolism and infectiousness, for some amount of time and then\nbecome active (infectious) again. The population is thus composed of an\ninactive proportion, and an active part in which evolution and reproduction\ntakes place. In this paper, we investigate the effect of parasite quiescence on\nthe time to extinction of infectious disease epidemics. We build a\nSusceptible-Infected-Quiescent-Susceptible (SIQS) epidemiological model.\nHereby, host individuals infected by a quiescent parasite strain cannot\nrecover, but are not infectious. We particularly focus on stochastic effects.\nWe show that the quiescent state does not affect the reproduction number, but\nfor a wide range of parameters the model behaves as an SIS model at a slower\ntime scale, given by the fraction of time infected individuals are within the I\nstate (and not in the Q state). This finding, proven using a time scale\nargument and singular perturbation theory for Markov processes, is illustrated\nand validated by numerical experiments based on the quasi-steady state\ndistribution. We find here that the result even holds without a distinct time\nscale separation. Our results highlight the influence of quiescence as a\nbet-hedging strategy against disease stochastic extinction, and are relevant\nfor predicting infectious disease dynamics in small populations.",
      "generated_abstract": "aper, we consider the stochastic time to extinction (TTE) model,\nwith a single infectious state, and without treatment. We first study the\nsteady-state behavior of the model, and derive the critical time to extinction\nunder quiescence. Then, we focus on the asymptotic behavior of the TTE model\nunder quiescence. In particular, we show that the TTE model with quiescence\nexhibits an explosive extinction event, and that the extinction rate is\nproportional to the square of the mean time to extinction. Furthermore, we\nstudy the effect of treatment, and show that the treatment can delay the\nextinction time to a finite value. The asymptotic extinction rate for the\nquiescent case is also derived. We also study the effect of the treatment on\nthe extinction time, and show that the treatment can shorten the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19402985074626866,
          "p": 0.41935483870967744,
          "f": 0.26530611812369853
        },
        "rouge-2": {
          "r": 0.04830917874396135,
          "p": 0.09803921568627451,
          "f": 0.06472491467119142
        },
        "rouge-l": {
          "r": 0.1865671641791045,
          "p": 0.4032258064516129,
          "f": 0.2551020364910455
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.cond-mat/mes-hall/2503.10059v1",
      "true_abstract": "Surface structure affects the growth, shape and properties of nanoparticles.\nIn wet chemical syntheses, metal additives and surfactants are used to modify\nsurfaces and guide nanocrystal growth. To understand this process, it is\ncritical to understand how the surface structure is modified. However,\nmeasuring the type and arrangement of atoms at hard-soft interfaces on\nnanoscale surfaces, especially in the presence of surfactants, is extremely\nchallenging. Here, we determine the atomic structure of the hard-soft interface\nin a metallic nanoparticle by developing low-dose imaging conditions in\nfour-dimensional scanning transmission electron microscopy that are\npreferentially sensitive to surface adatoms. By revealing experimentally the\ncopper additives and bromide surfactant counterion at the surface of a gold\nnanocuboid and quantifying their interatomic distances, our direct, low-dose\nimaging method provides atomic-level understanding of chemically sophisticated\nnanomaterial surface structures. These measurements of the atomic structure of\nthe hard-soft interface provide the information necessary to understand and\nquantify surface chemistries and energies and their pivotal role in nanocrystal\ngrowth.",
      "generated_abstract": "soft interface of gold nanoparticles (AuNPs) is a unique nanoscopic\nenvironment for both, the atoms and the ions in the AuNPs, as well as the\nenvironment itself. This environment is of great importance in several areas,\nsuch as nanoparticle synthesis, electrocatalysis, and sensing. The atomic\nresolution of the hard-soft interface, combined with the ability to control\nthe size of AuNPs, makes it a very interesting platform for many applications.\nIn this work, we propose a way to locate the atoms at the hard-soft interface\nof AuNPs by means of the scanning tunneling microscopy (STM). We show that\nlocating the atoms at the hard-soft interface is possible by analyzing the\nresidual potential energy curves (PECs) for the AuNPs. This method allows\ndetermining the number of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22429906542056074,
          "p": 0.3,
          "f": 0.2566844870828449
        },
        "rouge-2": {
          "r": 0.040268456375838924,
          "p": 0.05263157894736842,
          "f": 0.04562737151440727
        },
        "rouge-l": {
          "r": 0.205607476635514,
          "p": 0.275,
          "f": 0.2352941127512941
        }
      }
    },
    {
      "paper_id": "cs.AR.cs/AR/2503.09975v1",
      "true_abstract": "Low-precision data types are essential in modern neural networks during both\ntraining and inference as they enhance throughput and computational capacity by\nbetter exploiting available hardware resources. Despite the incorporation of\nFP8 in commercially available neural network accelerators, a comprehensive\nexposition of its underlying mechanisms, along with rigorous performance and\naccuracy evaluations, is still lacking. In this work, we contribute in three\nsignificant ways. First, we analyze the implementation details and quantization\noptions associated with FP8 for inference on the Intel Gaudi AI accelerator.\nSecond, we empirically quantify the throughput improvements afforded by the use\nof FP8 at both the operator level and in end-to-end scenarios. Third, we assess\nthe accuracy impact of various FP8 quantization methods. Our experimental\nresults indicate that the Intel Gaudi 2 accelerator consistently achieves high\ncomputational unit utilization, frequently exceeding 90\\% MFU, while incurring\nan accuracy degradation of less than 1\\%.",
      "generated_abstract": "guage Models (LLMs) have been shown to perform inference on\nhigh-dimensional data in a matter of seconds, but current state-of-the-art\ninference methods on FP32 codec data are prohibitively slow, with inference\ntime exceeding 1000 seconds for large models and up to 10 hours for smaller\nmodels. This is due to the large data footprint of codec data, as well as the\nhigh computational demands of LLMs on codec data. In this paper, we propose a\nmethod for faster inference using FP8 codec data, with an implementation that\nruns on an Intel Xeon Gold 5225L CPU. We demonstrate that our method achieves\nnear-FP32 speedups of up to 3x in inference time, with a 2x speedup in\nmicrosecond accuracy. We also demonstrate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.32926829268292684,
          "f": 0.2842105214094183
        },
        "rouge-2": {
          "r": 0.02112676056338028,
          "p": 0.02586206896551724,
          "f": 0.023255809004267623
        },
        "rouge-l": {
          "r": 0.21296296296296297,
          "p": 0.2804878048780488,
          "f": 0.24210525825152365
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2502.20177v1",
      "true_abstract": "The paper extends in two directions the work of \\cite{Plackett77} who studied\nhow, in a $2\\times 2$ table, the likelihood of the column totals depends on the\nodds ratio. First, we study the marginal likelihood of a single $R\\times C$\nfrequency table when only the marginal frequencies are observed and then\nconsider a collection of, say, $s$ $R\\times C$ tables, where only the row and\ncolumn totals can be observed, which is the basic framework which in\napplications of Ecological Inference. In the simpler context, we derive the\nlikelihood equations and show that the likelihood has a collection of local\nmaxima which, after a suitable rearrangement of the row and column categories,\nexhibit the strongest positive association compatible with the marginals, a\nkind of paradox, considering that the available data are so poor. Next, we\nderive the likelihood equations for the marginal likelihood of a collection of\ntow-way tables, under the assumption that they share the same row conditional\ndistributions and derive a necessary condition for the information matrix to be\nwell defined. We also describe a Fisher-scoring algorithm for maximizing the\nmarginal likelihood which, however, can be used only if the number of available\nreplications reaches a given threshold.",
      "generated_abstract": "The marginal likelihood, a measure of the likelihood of a given model, is\na central tool in Bayesian inference. However, the marginal likelihood is a\nstatistical measure and does not directly represent the likelihood of the data.\nIn this paper, we show that the marginal likelihood of a two-way table can be\nused to represent the likelihood of the data, and we discuss several\napplications of this idea.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17796610169491525,
          "p": 0.5121951219512195,
          "f": 0.2641509395688462
        },
        "rouge-2": {
          "r": 0.06976744186046512,
          "p": 0.21818181818181817,
          "f": 0.10572686857497732
        },
        "rouge-l": {
          "r": 0.1694915254237288,
          "p": 0.4878048780487805,
          "f": 0.25157232321664497
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.05435v1",
      "true_abstract": "Teacher-forcing training for audio captioning usually leads to exposure bias\ndue to training and inference mismatch. Prior works propose the contrastive\nmethod to deal with caption degeneration. However, the contrastive method\nignores the temporal information when measuring similarity across acoustic and\nlinguistic modalities, leading to inferior performance. In this work, we\ndevelop the temporal-similarity score by introducing the unbiased sliced\nWasserstein RBF (USW-RBF) kernel equipped with rotary positional embedding to\naccount for temporal information across modalities. In contrast to the\nconventional sliced Wasserstein RBF kernel, we can form an unbiased estimation\nof USW-RBF kernel via Monte Carlo estimation. Therefore, it is well-suited to\nstochastic gradient optimization algorithms, and its approximation error\ndecreases at a parametric rate of $\\mathcal{O}(L^{-1/2})$ with $L$ Monte Carlo\nsamples. Additionally, we introduce an audio captioning framework based on the\nunbiased sliced Wasserstein kernel, incorporating stochastic decoding methods\nto mitigate caption degeneration during the generation process. We conduct\nextensive quantitative and qualitative experiments on two datasets, AudioCaps\nand Clotho, to illustrate the capability of generating high-quality audio\ncaptions. Experimental results show that our framework is able to increase\ncaption length, lexical diversity, and text-to-audio self-retrieval accuracy.",
      "generated_abstract": "tioning is a fundamental task in audio processing, yet it remains\nhighly challenging due to the intrinsic nonlinearity of audio and the\ndifficulty of synthesizing high-quality audio-text pairs. Existing methods\noften rely on large-scale datasets or specialized architectures, making it\ndifficult to scale to real-world applications. To address these limitations, we\npropose a novel unbiased sliced Wasserstein kernels (USWK) framework for audio\ncaptioning. Our framework leverages the bi-directional property of kernels and\nthe unbiased nature of Wasserstein distance to improve captioning quality.\nUnlike traditional Wasserstein kernels, our method does not require\ndistributional information or a large training dataset. It can be applied to\nreal-world audio captioning tasks without requiring complex training\nprocedures. We demonstrate the effectiveness of our",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.29545454545454547,
          "f": 0.23853210527733368
        },
        "rouge-2": {
          "r": 0.03954802259887006,
          "p": 0.06140350877192982,
          "f": 0.048109960870089404
        },
        "rouge-l": {
          "r": 0.19230769230769232,
          "p": 0.2840909090909091,
          "f": 0.22935779335072817
        }
      }
    },
    {
      "paper_id": "cs.SI.q-bio/PE/2502.12847v1",
      "true_abstract": "Understanding how cognitive and social mechanisms shape the evolution of\ncomplex artifacts such as songs is central to cultural evolution research.\nSocial network topology (what artifacts are available?), selection (which are\nchosen?), and reproduction (how are they copied?) have all been proposed as key\ninfluencing factors. However, prior research has rarely studied them together\ndue to methodological challenges. We address this gap through a controlled\nnaturalistic paradigm whereby participants (N=2,404) are placed in networks and\nare asked to iteratively choose and sing back melodies from their neighbors. We\nshow that this setting yields melodies that are more complex and more pleasant\nthan those found in the more-studied linear transmission setting, and exhibits\nrobust differences across topologies. Crucially, these differences are\ndiminished when selection or reproduction bias are eliminated, suggesting an\ninteraction between mechanisms. These findings shed light on the interplay of\nmechanisms underlying the evolution of cultural artifacts.",
      "generated_abstract": "evolution is a key phenomenon in evolutionary sociology, yet it has\nbeen largely ignored in computational social science. To address this gap, we\nstudy the interaction between two key cultural evolution mechanisms: cultural\ndrift and cultural transmission. We first compare the two mechanisms through\na series of simulations. Then, we propose a novel experimental design that\ncaptures the key features of real-world social networks, and introduce a\nmethodology for assessing the fitness of cultural evolution mechanisms. Finally,\nwe apply our methodology to four experimental social networks and compare them\nwith the mechanisms themselves. Our results reveal that cultural drift is\nmore likely to dominate in complex networks, whereas cultural transmission\ndominates in simple networks. We also find that the fitness of cultural drift\nis more stable, even under extreme conditions. Our findings provide a\nfundamental understanding of cultural evolution and its interaction with\nnetwork topology,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2545454545454545,
          "p": 0.3181818181818182,
          "f": 0.28282827789001125
        },
        "rouge-2": {
          "r": 0.041379310344827586,
          "p": 0.045112781954887216,
          "f": 0.043165462635216145
        },
        "rouge-l": {
          "r": 0.21818181818181817,
          "p": 0.2727272727272727,
          "f": 0.24242423748597092
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.09678v1",
      "true_abstract": "For the first time, quality distribution of trees is introduced in a tree\ngrowth model. Consequently, the effects of quality thinning on stand\ndevelopment can be investigated. Quality thinning improves the financial return\nin all cases studied, but the effect is small. Rotation ages, timber stocks and\nmaturity diameters are not much affected by quality thinning. Bare land\nvaluation neither changes the contribution of the quality thinning. The reason\nfor the small effect apparently lies in the value development of individual\ntrees. The relative value development of small pulpwood trunks is large, since\nthe harvesting expense per volume unit is reduced along with size increment.\nSuch trees are not feasible objects for quality thinning, unless quality\ncorrelates with growth rate. Another enhanced stage of value development is\nwhen pulpwood trunks turn to sawlog trunks. For large pulpwood trunks, quality\nthinning is feasible. Existing sawlog content in trees dilutes the effect of\nquality thinning on the financial return. The results change if the growth rate\nis positively correlated with quality, quality thinning becoming feasible in\nall commercial diameter classes.",
      "generated_abstract": "This study investigates the effects of quality thinning and stand development\non growth, yield, and marketable timber volume of boreal trees. The analysis\nutilizes data from 160,000 trees in a boreal forest in Minnesota, USA. The\nresults indicate that quality thinning is associated with higher growth and\nyield, and lower stand degradation. These effects are particularly pronounced\nfor the early-growing species, which are more susceptible to stand degradation\nthrough thinning. In addition, quality thinning enhances marketable timber\nvolume by increasing timber volume per tree. The findings suggest that\nthinning strategies should be tailored to the individual species and stand\ncharacteristics, with the goal of optimizing growth and marketable timber\nvolume.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2621359223300971,
          "p": 0.38571428571428573,
          "f": 0.31213872350563
        },
        "rouge-2": {
          "r": 0.0625,
          "p": 0.10309278350515463,
          "f": 0.07782100697361079
        },
        "rouge-l": {
          "r": 0.21359223300970873,
          "p": 0.3142857142857143,
          "f": 0.25433525529753753
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.06009v1",
      "true_abstract": "In this paper, we investigate one of the most fundamental nonconvex learning\nproblems, ReLU regression, in the Differential Privacy (DP) model. Previous\nstudies on private ReLU regression heavily rely on stringent assumptions, such\nas constant bounded norms for feature vectors and labels. We relax these\nassumptions to a more standard setting, where data can be i.i.d. sampled from\n$O(1)$-sub-Gaussian distributions. We first show that when $\\varepsilon =\n\\tilde{O}(\\sqrt{\\frac{1}{N}})$ and there is some public data, it is possible to\nachieve an upper bound of $\\Tilde{O}(\\frac{d^2}{N^2 \\varepsilon^2})$ for the\nexcess population risk in $(\\epsilon, \\delta)$-DP, where $d$ is the dimension\nand $N$ is the number of data samples. Moreover, we relax the requirement of\n$\\epsilon$ and public data by proposing and analyzing a one-pass mini-batch\nGeneralized Linear Model Perceptron algorithm (DP-MBGLMtron). Additionally,\nusing the tracing attack argument technique, we demonstrate that the minimax\nrate of the estimation error for $(\\varepsilon, \\delta)$-DP algorithms is lower\nbounded by $\\Omega(\\frac{d^2}{N^2 \\varepsilon^2})$. This shows that\nDP-MBGLMtron achieves the optimal utility bound up to logarithmic factors.\nExperiments further support our theoretical results.",
      "generated_abstract": "t a novel and almost optimal differentially private (DP) regressor\nthat is trained and evaluated in a centralized manner. Our approach uses the\ndifferentially private Least Square (DPLS) algorithm to learn a ReLU regression\nmodel. The DPLS algorithm is a DP variant of the Least Squares (LS) algorithm\nthat guarantees privacy up to a constant factor in the size of the input\ndataset. Our analysis reveals that the DPLS algorithm achieves a near-optimal\nDP level of privacy for the ReLU regression problem. In particular, the\nminimum number of training samples required to achieve the optimal level of\nprivacy under DP is $n = \\log(1/\\delta)/\\log(2)$, where $\\delta > 0$ is the\nprivacy budget. This bound is tight, as it matches the optimal privacy level\nachieved by a standard",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23308270676691728,
          "p": 0.3974358974358974,
          "f": 0.2938388578989691
        },
        "rouge-2": {
          "r": 0.06321839080459771,
          "p": 0.09243697478991597,
          "f": 0.07508531940826367
        },
        "rouge-l": {
          "r": 0.20300751879699247,
          "p": 0.34615384615384615,
          "f": 0.2559241659558411
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2408.07497v1",
      "true_abstract": "This paper presents a method for accurately predicting the full distribution\nof stock returns, given a comprehensive set of 194 stock characteristics and\nmarket variables. Such distributions, learned from rich data using a machine\nlearning algorithm, are not constrained by restrictive model assumptions and\nallow the exploration of non-Gaussian, heavy-tailed data and their non-linear\ninteractions. The method uses a two-stage quantile neural network combined with\nspline interpolation. The results show that the proposed approach outperforms\nalternative models in terms of out-of-sample losses. Furthermore, we show that\nthe moments derived from such distributions can be useful as alternative\nempirical estimates in many cases, including mean estimation and forecasting.\nFinally, we examine the relationship between cross-sectional returns and\nseveral distributional characteristics. The results are robust to a wide range\nof US and international data.",
      "generated_abstract": "ork, we propose a novel framework for predicting the distribution of\nstock returns around the globe. The framework utilizes an artificial neural\nnetwork (ANN) with a 30-dimensional input, a 30-dimensional hidden layer, and\na 10-dimensional output, and incorporates a weighted sum of multiple\nstatistical measures to capture the distributional patterns of stock returns.\nThe ANN is trained using a large volume of historical stock returns from\nrepresentative countries around the globe. The framework is evaluated by\ncomparing the predicted distributions of stock returns against the\nrepresentative countries' historical data. The results show that the proposed\nframework can accurately predict the distribution of stock returns around the\nglobe, and the framework outperforms existing state-of-the-art models.\nAdditionally, the framework demonstrates the ability to predict the distribution\nof stock returns around the glo",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.29,
          "p": 0.3918918918918919,
          "f": 0.33333332844497293
        },
        "rouge-2": {
          "r": 0.078125,
          "p": 0.09523809523809523,
          "f": 0.08583690491996565
        },
        "rouge-l": {
          "r": 0.29,
          "p": 0.3918918918918919,
          "f": 0.33333332844497293
        }
      }
    },
    {
      "paper_id": "cs.IR.cs/IR/2503.09492v1",
      "true_abstract": "Cascade Ranking is a prevalent architecture in large-scale top-k selection\nsystems like recommendation and advertising platforms. Traditional training\nmethods focus on single-stage optimization, neglecting interactions between\nstages. Recent advances such as RankFlow and FS-LTR have introduced\ninteraction-aware training paradigms but still struggle to 1) align training\nobjectives with the goal of the entire cascade ranking (i.e., end-to-end\nrecall) and 2) learn effective collaboration patterns for different stages. To\naddress these challenges, we propose LCRON, which introduces a novel surrogate\nloss function derived from the lower bound probability that ground truth items\nare selected by cascade ranking, ensuring alignment with the overall objective\nof the system. According to the properties of the derived bound, we further\ndesign an auxiliary loss for each stage to drive the reduction of this bound,\nleading to a more robust and effective top-k selection. LCRON enables\nend-to-end training of the entire cascade ranking system as a unified network.\nExperimental results demonstrate that LCRON achieves significant improvement\nover existing methods on public benchmarks and industrial applications,\naddressing key limitations in cascade ranking training and significantly\nenhancing system performance.",
      "generated_abstract": "s a fundamental task in information retrieval, with many\nspoken and written languages relying on this task. Despite its importance,\nexisting methods often struggle with the inherent challenges of ranking:\ndistinctive word-level semantics, inter-document context, and the need to\naccount for ambiguous and low-scoring documents. In this paper, we present\nLearning Cascade Ranking as One Network (LCRO), a novel framework that\naddresses these challenges by learning a cascade ranking model, which learns to\nrank each document independently, and then fuses the rankings to produce a\nfinal ranking. Our approach leverages a graph convolutional network (GCN) to\nlearn the structure of the document-document graph and a cascade model to learn\nthe ranking of each document, with the goal of jointly learning both the\ninter-document and intra-document semantics. We show that by learning",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21374045801526717,
          "p": 0.3076923076923077,
          "f": 0.2522522474145768
        },
        "rouge-2": {
          "r": 0.046242774566473986,
          "p": 0.064,
          "f": 0.05369127029750956
        },
        "rouge-l": {
          "r": 0.17557251908396945,
          "p": 0.25274725274725274,
          "f": 0.20720720236953183
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2405.10920v1",
      "true_abstract": "We study the data-generating processes for factors expressed in return\ndifferences, which the literature on time-series asset pricing seems to have\noverlooked. For the factors' data-generating processes or long-short zero-cost\nportfolios, a meaningful definition of returns is impossible; further, the\ncompounded market factor (MF) significantly underestimates the return\ndifference between the market and the risk-free rate compounded separately.\nSurprisingly, if MF were treated coercively as periodic-rebalancing long-short\n(i.e., the same as size and value), Fama-French three-factor (FF3) would be\neconomically unattractive for lacking compounding and irrelevant for suffering\nfrom the small \"size of an effect.\" Otherwise, FF3 might be misspecified if MF\nwere buy-and-hold long-short. Finally, we show that OLS with net returns for\nsingle-index models leads to inflated alphas, exaggerated t-values, and\noverestimated Sharpe ratios (SR); worse, net returns may lead to pathological\nalphas and SRs. We propose defining factors (and SRs) with non-difference\ncompound returns.",
      "generated_abstract": "We consider the problem of estimating a stochastic process with a\nsignificant number of observations. We are interested in estimating the\nfunctional mean and variance of this process, as well as the mean squared\nerror of the estimator. We present a novel approach that combines an\nextended Kalman filter with a Gaussian process regression to address this\nproblem. Our approach is based on the notion of data-generating process,\nwhich allows us to construct a stochastic process that captures the\nstatistical properties of the observed data. We show that our estimator is\nwell-posed and is consistent for the unknown mean and variance. We provide\nsimulation results and empirical applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1504424778761062,
          "p": 0.25757575757575757,
          "f": 0.189944129422927
        },
        "rouge-2": {
          "r": 0.006896551724137931,
          "p": 0.009900990099009901,
          "f": 0.008130076460773584
        },
        "rouge-l": {
          "r": 0.1415929203539823,
          "p": 0.24242424242424243,
          "f": 0.1787709450653851
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/GR/2503.08061v2",
      "true_abstract": "Realistic hand manipulation is a key component of immersive virtual reality\n(VR), yet existing methods often rely on a kinematic approach or motion-capture\ndatasets that omit crucial physical attributes such as contact forces and\nfinger torques. Consequently, these approaches prioritize tight,\none-size-fits-all grips rather than reflecting users' intended force levels. We\npresent ForceGrip, a deep learning agent that synthesizes realistic hand\nmanipulation motions, faithfully reflecting the user's grip force intention.\nInstead of mimicking predefined motion datasets, ForceGrip uses generated\ntraining scenarios-randomizing object shapes, wrist movements, and trigger\ninput flows-to challenge the agent with a broad spectrum of physical\ninteractions. To effectively learn from these complex tasks, we employ a\nthree-phase curriculum learning framework comprising Finger Positioning,\nIntention Adaptation, and Dynamic Stabilization. This progressive strategy\nensures stable hand-object contact, adaptive force control based on user\ninputs, and robust handling under dynamic conditions. Additionally, a proximity\nreward function enhances natural finger motions and accelerates training\nconvergence. Quantitative and qualitative evaluations reveal ForceGrip's\nsuperior force controllability and plausibility compared to state-of-the-art\nmethods. The video presentation of our paper is accessible at\nhttps://youtu.be/lR-YAfninJw.",
      "generated_abstract": "eality (VR) offers a powerful tool for training and teaching\nhand manipulation, yet existing approaches typically rely on ground truth\nforce measurements to train and evaluate control policies. In this work, we\npropose ForceGrip, a data-free curriculum learning framework for realistic\ngrip force control in VR. The approach leverages real-world data collected from\nhand-held force sensors to teach the model how to control the virtual grip. The\nteacher model is trained with real data collected from a real hand, and the\nstudent model is trained using only simulated data. To ensure the\nreliability of the data, the teacher model is evaluated on a large-scale\ndataset of 120,000 simulated gripping tasks and the student model is\nevaluated on a more challenging dataset of 20,000 real-world gripping tasks.\nThe results",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18791946308724833,
          "p": 0.35,
          "f": 0.24454148017009594
        },
        "rouge-2": {
          "r": 0.0449438202247191,
          "p": 0.07142857142857142,
          "f": 0.05517240905208126
        },
        "rouge-l": {
          "r": 0.18120805369127516,
          "p": 0.3375,
          "f": 0.23580785571594753
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2503.08638v1",
      "true_abstract": "We tackle the task of long-form music generation--particularly the\nchallenging \\textbf{lyrics-to-song} problem--by introducing YuE, a family of\nopen foundation models based on the LLaMA2 architecture. Specifically, YuE\nscales to trillions of tokens and generates up to five minutes of music while\nmaintaining lyrical alignment, coherent musical structure, and engaging vocal\nmelodies with appropriate accompaniment. It achieves this through (1)\ntrack-decoupled next-token prediction to overcome dense mixture signals, (2)\nstructural progressive conditioning for long-context lyrical alignment, and (3)\na multitask, multiphase pre-training recipe to converge and generalize. In\naddition, we redesign the in-context learning technique for music generation,\nenabling versatile style transfer (e.g., converting Japanese city pop into an\nEnglish rap while preserving the original accompaniment) and bidirectional\ngeneration. Through extensive evaluation, we demonstrate that YuE matches or\neven surpasses some of the proprietary systems in musicality and vocal agility.\nIn addition, fine-tuning YuE enables additional controls and enhanced support\nfor tail languages. Furthermore, beyond generation, we show that YuE's learned\nrepresentations can perform well on music understanding tasks, where the\nresults of YuE match or exceed state-of-the-art methods on the MARBLE\nbenchmark. Keywords: lyrics2song, song generation, long-form, foundation model,\nmusic generation",
      "generated_abstract": "dation Models (OFMs) are a promising approach for music generation,\nchallenged by the large size of their training datasets and the need for\nexpensive hardware to process them efficiently. In this work, we introduce YuE,\na large-scale, open foundation model that scales well to the large datasets\nused in music generation. YuE is composed of a Transformer-based encoder and a\ngenerator network, with a large vocabulary and a large number of parameters.\nThe encoder uses an LSTM-based encoder-decoder architecture, allowing it to\nprocess long sequences without truncation, while the generator uses a\nlarge, pre-trained vocabulary, allowing it to generate complex music in\nunrestricted domains. We introduce YuE, a large-scale, open foundation model\nthat scales well to the large datasets used in music generation, and show that\nit surpasses",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1780821917808219,
          "p": 0.33766233766233766,
          "f": 0.23318385198093675
        },
        "rouge-2": {
          "r": 0.031746031746031744,
          "p": 0.057692307692307696,
          "f": 0.0409556268201149
        },
        "rouge-l": {
          "r": 0.1643835616438356,
          "p": 0.3116883116883117,
          "f": 0.21524663224999505
        }
      }
    },
    {
      "paper_id": "cs.AI.stat/OT/2412.14222v1",
      "true_abstract": "In recent years, data science agents powered by Large Language Models (LLMs),\nknown as \"data agents,\" have shown significant potential to transform the\ntraditional data analysis paradigm. This survey provides an overview of the\nevolution, capabilities, and applications of LLM-based data agents,\nhighlighting their role in simplifying complex data tasks and lowering the\nentry barrier for users without related expertise. We explore current trends in\nthe design of LLM-based frameworks, detailing essential features such as\nplanning, reasoning, reflection, multi-agent collaboration, user interface,\nknowledge integration, and system design, which enable agents to address\ndata-centric problems with minimal human intervention. Furthermore, we analyze\nseveral case studies to demonstrate the practical applications of various data\nagents in real-world scenarios. Finally, we identify key challenges and propose\nfuture research directions to advance the development of data agents into\nintelligent statistical analysis software.",
      "generated_abstract": "guage Models (LLMs) are increasingly used for statistics and data\nscience applications. This survey introduces the most important LLM-based\nagents for statistics and data science, focusing on their application\nstrategies, challenges, and future directions. We first provide an overview of\nthe history and evolution of LLM-based agents in statistics and data science,\nhighlighting their key features and limitations. We then systematically\nanalyze and compare the main LLM-based agents in statistics and data science,\nincluding text-based, image-based, and symbolic agents. We discuss their\napplications in statistics and data science, focusing on their challenges and\nstrategies for future development. Finally, we provide a detailed analysis of\nthe key challenges and opportunities for further development of LLM-based\nagents in statistics and data science, including the need for more advanced\ndata annotation,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25925925925925924,
          "p": 0.42424242424242425,
          "f": 0.32183907575109
        },
        "rouge-2": {
          "r": 0.08955223880597014,
          "p": 0.12371134020618557,
          "f": 0.10389609902438134
        },
        "rouge-l": {
          "r": 0.24074074074074073,
          "p": 0.3939393939393939,
          "f": 0.2988505700039636
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.02899v1",
      "true_abstract": "Accurately discriminating progressive stages of Alzheimer's Disease (AD) is\ncrucial for early diagnosis and prevention. It often involves multiple imaging\nmodalities to understand the complex pathology of AD, however, acquiring a\ncomplete set of images is challenging due to high cost and burden for subjects.\nIn the end, missing data become inevitable which lead to limited sample-size\nand decrease in precision in downstream analyses. To tackle this challenge, we\nintroduce a holistic imaging feature imputation method that enables to leverage\ndiverse imaging features while retaining all subjects. The proposed method\ncomprises two networks: 1) An encoder to extract modality-independent\nembeddings and 2) A decoder to reconstruct the original measures conditioned on\ntheir imaging modalities. The encoder includes a novel {\\em ordinal contrastive\nloss}, which aligns samples in the embedding space according to the progression\nof AD. We also maximize modality-wise coherence of embeddings within each\nsubject, in conjunction with domain adversarial training algorithms, to further\nenhance alignment between different imaging modalities. The proposed method\npromotes our holistic imaging feature imputation across various modalities in\nthe shared embedding space. In the experiments, we show that our networks\ndeliver favorable results for statistical analysis and classification against\nimputation baselines with Alzheimer's Disease Neuroimaging Initiative (ADNI)\nstudy.",
      "generated_abstract": "n is a critical task in machine learning, where missing values\nin datasets are replaced with arbitrary labels. However, the imputation process\ncan be computationally expensive, and a lack of label progressivity can lead to\nsuboptimal imputation results. To address these challenges, we propose a novel\nmethod for imputing missing features using ordinal contrastive learning (OCL).\nOCL leverages the contrastive learning framework to learn an imputation model\nthat captures the similarity between the missing values and the label\nprogressive, which facilitates a more efficient imputation process. Additionally,\nOCL introduces an imputation loss that encourages the model to assign more\nsimilar labels to the missing values, enhancing the imputation quality.\nExperimental results on the CIFAR-10 and ImageNet datasets demonstrate that\nOCL outperforms state-of-the-art imputation methods, achieving an accuracy of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.2857142857142857,
          "f": 0.2105263111357342
        },
        "rouge-2": {
          "r": 0.020942408376963352,
          "p": 0.03305785123966942,
          "f": 0.025641020892711266
        },
        "rouge-l": {
          "r": 0.1597222222222222,
          "p": 0.27380952380952384,
          "f": 0.2017543813111728
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.02683v1",
      "true_abstract": "Streaming multi-talker speech translation is a task that involves not only\ngenerating accurate and fluent translations with low latency but also\nrecognizing when a speaker change occurs and what the speaker's gender is.\nSpeaker change information can be used to create audio prompts for a zero-shot\ntext-to-speech system, and gender can help to select speaker profiles in a\nconventional text-to-speech model. We propose to tackle streaming speaker\nchange detection and gender classification by incorporating speaker embeddings\ninto a transducer-based streaming end-to-end speech translation model. Our\nexperiments demonstrate that the proposed methods can achieve high accuracy for\nboth speaker change detection and gender classification.",
      "generated_abstract": "r presents a real-time streaming speaker change detection (SCD) and\ngender classification system for multi-talker speech translation (MTS) using\ntransducer-based speech recognition. SCD enables users to monitor whether a\nsingle or multiple speakers are speaking. The system performs gender\nclassification by identifying male and female speakers using a speaker\nembedding model. The gender classification accuracy is evaluated using the\nSpeaker Gender Evaluation Index (SGI) and the gender identification accuracy\nis evaluated using the gender confusion rate (GCR). The proposed system\nprovides a user-friendly interface, ensuring intuitive operation while\nsupporting real-time speech translation. The system was evaluated using a\nreal-world scenario, where the MTS system was integrated into a smartphone\napp, enabling the user to monitor the speaker identity. The results show that\nthe proposed system can effectively detect and classify the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.34285714285714286,
          "p": 0.2926829268292683,
          "f": 0.31578946871537406
        },
        "rouge-2": {
          "r": 0.1276595744680851,
          "p": 0.10344827586206896,
          "f": 0.11428570934058976
        },
        "rouge-l": {
          "r": 0.3,
          "p": 0.25609756097560976,
          "f": 0.2763157845048477
        }
      }
    },
    {
      "paper_id": "cs.HC.q-bio/OT/2410.10513v1",
      "true_abstract": "Structuring data analysis projects, that is, defining the layout of files and\nfolders needed to analyze data using existing tools and novel code, largely\nfollows personal preferences. In this work, we look at the structure of several\ndata analysis project templates and find little structural overlap. We\nhighlight the parts that are similar between them, and propose guiding\nprinciples to keep in mind when one wishes to create a new data analysis\nproject. Finally, we present Kerblam!, a project management tool that can\nexpedite project data management, execution of workflow managers, and sharing\nof the resulting workflow and analysis outputs. We hope that, by following\nthese principles and using Kerblam!, the landscape of data analysis projects\ncan become more transparent, understandable, and ultimately useful to the wider\ncommunity.",
      "generated_abstract": "The use of computational approaches to unravel the molecular mechanisms behind\ndisease has become essential to advance research in the life sciences. However,\nthe vast majority of computational analyses remain unpublished and unavailable\nto the scientific community, hindering their broader impact. The Kerblam!\nproject aims to address this challenge by creating a repository of computational\nanalysis results. The repository aims to provide a platform for the dissemination\nof computational analyses that has not previously been available, including\nthe ability to provide access to the data, software, and any associated\ndocumentation. This project will also provide a platform for researchers to\ncollaborate and share data in a structured manner, allowing for the rapid\ndevelopment and evaluation of computational approaches. By providing a\ncomprehensive resource for computational analyses, the Kerblam! project aims\nto promote scientific transparency and collaboration.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14942528735632185,
          "p": 0.15476190476190477,
          "f": 0.1520467786272701
        },
        "rouge-2": {
          "r": 0.008130081300813009,
          "p": 0.008403361344537815,
          "f": 0.008264457811286405
        },
        "rouge-l": {
          "r": 0.14942528735632185,
          "p": 0.15476190476190477,
          "f": 0.1520467786272701
        }
      }
    },
    {
      "paper_id": "math.GM.math/GM/2502.14873v1",
      "true_abstract": "We examine the well-posedness of inverse eigenstrain problems for residual\nstress analysis from the perspective of the non-uniqueness of solutions,\nstructure of the corresponding null space and associated orthogonal range-null\ndecompositions. Through this process we highlight the existence of a trivial\nsolution to all inverse eigenstrain problems, with all other solutions\ndiffering from this trivial version by an unobservable null component. From one\nperspective, this implies that no new information can be gained though\neigenstrain analysis, however we also highlight the utility of the eigenstrain\nframework for enforcing equilibrium while estimating residual stress from\nincomplete experimental data. Two examples based on measured experimental data\nare given; one axisymmetric system involving ancient Roman medical tools, and\none more-general system involving an additively manufactured Inconel sample. We\nconclude by drawing a link between eigenstrain and reconstruction formulas\nrelated to strain tomography based on the Longitudinal Ray Transform (LRT).\nThrough this link, we establish a potential means for tomographic\nreconstruction of residual stress from LRT measurements.",
      "generated_abstract": "Inverse eigenstrain problems arise in the study of materials deformation\nand their properties. In this paper, we study a well-posed inverse eigenstrain\nproblem for a single layered material. The main challenge is the presence of a\nsecond-order derivative in the strain tensor, which makes the problem ill-posed\nwhen the strain is close to a critical strain. To address this issue, we\nintroduce a regularization term that suppresses the second-order derivative,\nwhich is further combined with an inverse problem to achieve a weak solution\nfor the inverse eigenstrain problem. We prove that the solution is well-posed\nand satisfies the Dirichlet boundary condition. In addition, we prove that the\nsolution is trivial, i.e., it is a constant function. This is the first time\nthat the triviality of the solution for the inverse eigenstrain problem has\nbeen established.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1559633027522936,
          "p": 0.21794871794871795,
          "f": 0.18181817695558938
        },
        "rouge-2": {
          "r": 0.026490066225165563,
          "p": 0.03333333333333333,
          "f": 0.02952029026837952
        },
        "rouge-l": {
          "r": 0.14678899082568808,
          "p": 0.20512820512820512,
          "f": 0.17112298978981397
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/PM/2502.19213v1",
      "true_abstract": "We consider an optimal investment-consumption problem for a\nutility-maximizing investor who has access to assets with different liquidity\nand whose consumption rate as well as terminal wealth are subject to\nlower-bound constraints. Assuming utility functions that satisfy standard\nconditions, we develop a methodology for deriving the optimal strategies in\nsemi-closed form. Our methodology is based on the generalized martingale\napproach and the decomposition of the problem into subproblems. We illustrate\nour approach by deriving explicit formulas for agents with power-utility\nfunctions and discuss potential extensions of the proposed framework.",
      "generated_abstract": "p a framework for asset-liability management (ALM) with fixed-term\nsecurities (FTS), where the liabilities of a firm are modeled as a portfolio\nwith FTS. The firm's obligations are treated as fixed-term liabilities, and\nthe firm's assets are treated as FTS. We show that the firm's optimal\nportfolio of FTS is a mixture of two optimal portfolios, a fixed-term portfolio\nand a portfolio of assets. The firm's optimal FTS allocation is the\nproportion of its assets in the fixed-term portfolio and the proportion of\nits liabilities in the fixed-term portfolio. We provide an analytical solution\nfor the firm's optimal FTS allocation when the firm's liabilities are\nconcave and the firm's assets are convex. We derive a closed-form solution for\nthe firm'",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2318840579710145,
          "p": 0.32,
          "f": 0.2689075581526729
        },
        "rouge-2": {
          "r": 0.011494252873563218,
          "p": 0.010869565217391304,
          "f": 0.011173179361445384
        },
        "rouge-l": {
          "r": 0.21739130434782608,
          "p": 0.3,
          "f": 0.25210083546359724
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.10008v1",
      "true_abstract": "We study whether ChatGPT and DeepSeek can extract information from the Wall\nStreet Journal to predict the stock market and the macroeconomy. We find that\nChatGPT has predictive power. DeepSeek underperforms ChatGPT, which is trained\nmore extensively in English. Other large language models also underperform.\nConsistent with financial theories, the predictability is driven by investors'\nunderreaction to positive news, especially during periods of economic downturn\nand high information uncertainty. Negative news correlates with returns but\nlacks predictive value. At present, ChatGPT appears to be the only model\ncapable of capturing economic news that links to the market risk premium.",
      "generated_abstract": "r examines whether ChatGPT and Deepseek can predict the S&P 500 and\nthe Euro Stoxx 50, and how they fare compared to traditional market forecasters.\nBoth models excel at predicting the S&P 500, with a correlation of 0.98, but\nperform similarly well on the Euro Stoxx 50. ChatGPT's performance is\nsubstantially lower than Deepseek's, and it struggles to predict the Euro Stoxx\n50, despite its larger dataset. ChatGPT is also less accurate than Deepseek at\npredicting stock market movements, with a correlation of -0.08. The findings\nhighlight the potential of ChatGPT and Deepseek in forecasting financial\nmarkets, but their limitations should be considered when making investment\ndecisions. The models are still",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23684210526315788,
          "p": 0.22784810126582278,
          "f": 0.2322580595180022
        },
        "rouge-2": {
          "r": 0.061224489795918366,
          "p": 0.06060606060606061,
          "f": 0.060913700583885594
        },
        "rouge-l": {
          "r": 0.21052631578947367,
          "p": 0.20253164556962025,
          "f": 0.20645160790509898
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/TH/2502.09525v1",
      "true_abstract": "We study the task of learning Multi-Index Models (MIMs) with label noise\nunder the Gaussian distribution. A $K$-MIM is any function $f$ that only\ndepends on a $K$-dimensional subspace. We focus on well-behaved MIMs with\nfinite ranges that satisfy certain regularity properties. Our main contribution\nis a general robust learner that is qualitatively optimal in the Statistical\nQuery (SQ) model. Our algorithm iteratively constructs better approximations to\nthe defining subspace by computing low-degree moments conditional on the\nprojection to the subspace computed thus far, and adding directions with\nrelatively large empirical moments. This procedure efficiently finds a subspace\n$V$ so that $f(\\mathbf{x})$ is close to a function of the projection of\n$\\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional\nmoments do not help, we prove an SQ lower bound suggesting that no efficient\nlearner exists.\n  As applications, we provide faster robust learners for the following concept\nclasses:\n  * {\\bf Multiclass Linear Classifiers} We give a constant-factor approximate\nagnostic learner with sample complexity $N = O(d)\n2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N\n,d)$. This is the first constant-factor agnostic learner for this class whose\ncomplexity is a fixed-degree polynomial in $d$.\n  * {\\bf Intersections of Halfspaces} We give an approximate agnostic learner\nfor this class achieving 0-1 error $K \\tilde{O}(\\mathrm{OPT}) + \\epsilon$ with\nsample complexity $N=O(d^2) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational\ncomplexity $\\mathrm{poly}(N ,d)$. This is the first agnostic learner for this\nclass with near-linear error dependence and complexity a fixed-degree\npolynomial in $d$.\n  Furthermore, we show that in the presence of random classification noise, the\ncomplexity of our algorithm scales polynomially with $1/\\epsilon$.",
      "generated_abstract": "er the problem of learning a multi-index model for a given dataset\nunder uncertainty. The model is defined by a collection of $p$ functions, each\nof which has $q$ indices. The data consists of $n$ points, and the goal is to\nlearn the model in the least-square sense by solving a least-squares problem\nwith the data. We consider the case where the data is sampled from a\nprobability distribution $P$. We propose an iterative subspace approximation\nmethod, which we refer to as the iterative subspace approximation algorithm\n(ISA), for learning the multi-index model in the least-square sense. The\nalgorithm is based on the observation that the data is a linear combination of\n$P$-means of a collection of subspaces, each of which is orthogonal to the\nother. The proposed method is designed to find the subspaces that maximize the\nerror between the data and the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14838709677419354,
          "p": 0.2987012987012987,
          "f": 0.198275857634141
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.03937007874015748,
          "f": 0.028011199897999165
        },
        "rouge-l": {
          "r": 0.14838709677419354,
          "p": 0.2987012987012987,
          "f": 0.198275857634141
        }
      }
    },
    {
      "paper_id": "physics.data-an.nucl-th/2503.09415v1",
      "true_abstract": "SPARKX is an open-source Python package developed to analyze simulation data\nfrom heavy-ion collision experiments. By offering a comprehensive suite of\ntools, SPARKX simplifies data analysis workflows, supports multiple formats\nsuch as OSCAR2013, and integrates seamlessly with SMASH and JETSCAPE/X-SCAPE.\nThis paper describes SPARKX's architecture, features, and applications and\ndemonstrates its effectiveness through detailed examples and performance\nbenchmarks. SPARKX enhances productivity and precision in relativistic\nkinematics studies.",
      "generated_abstract": "s between two heavy nuclei at relativistic energies are of great\ninterest for the study of nuclear structure and interactions, as they are\nrepresentative of the collision of the nucleon with the nucleus. The analysis\nof such data requires the use of advanced software tools, capable of handling\nthe high-dimensional kinematic space that is the outcome of the collision\nprocess. SPARKX is a new open-source software package developed to address this\nneed. It is designed to facilitate the analysis of relativistic collision\ndata, providing tools for the reconstruction of the initial and final state\nparticle momentum distributions, the measurement of the scattering angle, and\nthe determination of the total cross section. The software provides\ninteractive tools to explore the data, allowing the user to explore the\nkinematic space, compute kinematic observables, and visualize the\nreconstructed momentum distributions. The package also provides a set of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.288135593220339,
          "p": 0.19540229885057472,
          "f": 0.23287670751266668
        },
        "rouge-2": {
          "r": 0.045454545454545456,
          "p": 0.022556390977443608,
          "f": 0.030150749335623492
        },
        "rouge-l": {
          "r": 0.2542372881355932,
          "p": 0.1724137931034483,
          "f": 0.20547944723869405
        }
      }
    },
    {
      "paper_id": "math.RT.math/RT/2503.06432v3",
      "true_abstract": "We prove that the Lusztig's $a$-function is bounded for any Coxeter group of\nfinite rank.",
      "generated_abstract": "We prove that $a$-functions for Coxeter groups of finite rank are bounded\nby their ranks, i.e. $a(S)\\le r$ for any Coxeter subgroup $S$ of $W$. In\nparticular, the bound is sharp if $r=1$ or $r\\le 2$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.8,
          "p": 0.35294117647058826,
          "f": 0.48979591411911716
        },
        "rouge-2": {
          "r": 0.42857142857142855,
          "p": 0.16666666666666666,
          "f": 0.23999999596800003
        },
        "rouge-l": {
          "r": 0.7333333333333333,
          "p": 0.3235294117647059,
          "f": 0.4489795875885048
        }
      }
    },
    {
      "paper_id": "cs.LG.q-fin/CP/2412.10199v1",
      "true_abstract": "This document presents an in-depth examination of stock market sentiment\nthrough the integration of Convolutional Neural Networks (CNN) and Gated\nRecurrent Units (GRU), enabling precise risk alerts. The robust feature\nextraction capability of CNN is utilized to preprocess and analyze extensive\nnetwork text data, identifying local features and patterns. The extracted\nfeature sequences are then input into the GRU model to understand the\nprogression of emotional states over time and their potential impact on future\nmarket sentiment and risk. This approach addresses the order dependence and\nlong-term dependencies inherent in time series data, resulting in a detailed\nanalysis of stock market sentiment and effective early warnings of future\nrisks.",
      "generated_abstract": "market sentiment has been a key topic in financial markets research\nfor decades, and its impact on stock prices has been extensively studied.\nHowever, most existing studies focus on individual stocks and their\nrelationships to the broader market, neglecting the cross-market interactions.\nThis paper introduces an integrative analysis of financial market sentiment\nusing Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN)\nmodels. By combining the strengths of these two models, we formulate a\nmulti-step framework that captures both the short-term and long-term\nrelationships between individual stocks and the broader market. We then\ndevelop a risk prediction and alert system that integrates sentiment analysis\nwith traditional risk indicators and predictive models. This system provides\nmore comprehensive and timely insights into market volatility and risk\nmanagement, helping to enhance the efficiency",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2839506172839506,
          "p": 0.25274725274725274,
          "f": 0.2674418554820174
        },
        "rouge-2": {
          "r": 0.07766990291262135,
          "p": 0.06504065040650407,
          "f": 0.07079645521614884
        },
        "rouge-l": {
          "r": 0.20987654320987653,
          "p": 0.18681318681318682,
          "f": 0.19767441362155233
        }
      }
    },
    {
      "paper_id": "cs.AI.q-bio/NC/2502.21142v1",
      "true_abstract": "Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.",
      "generated_abstract": "r introduces a novel approach to World Model-Based Reinforcement\nLearning (WMBRL) that combines multimodal dreaming and a global workspace.\nMultimodal dreaming is a method that leverages natural language processing\n(NLP) and machine learning (ML) to generate descriptions of dreams. The\nglobal workspace is a conceptual space that encapsulates the world's\ndiverse knowledge, enabling ML models to explore this space and generate\nrepresentative features for the world model. The combined approach of multimodal\ndreaming and the global workspace enables a more holistic representation of the\nworld, improving WMBRL's ability to learn complex behaviors. In the paper, we\npresent a model that integrates multimodal dreaming and the global workspace\nfor WMBRL, and we demonstrate its effectiveness in simulated and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.2972972972972973,
          "f": 0.192982451755925
        },
        "rouge-2": {
          "r": 0.023923444976076555,
          "p": 0.04716981132075472,
          "f": 0.03174602728062548
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.28378378378378377,
          "f": 0.18421052193136359
        }
      }
    },
    {
      "paper_id": "math.LO.math/LO/2503.04471v1",
      "true_abstract": "We give a survey of cardinal charcteristics of the higher Cicho\\'n diagram\ndefined on the higher Baire space ${}^\\kappa\\kappa$ for $\\kappa$ regular with\n$2^{<\\kappa}=\\kappa$. Specifically, we will compare consistency proofs from the\nclassical Cicho\\'n diagram with various well-known forcing notions to similar\nconstructions generalised to the higher Cicho\\'n diagram. We are especially\ninterested in separation in a horizontal direction, that is, the consistency of\n$\\mathrm{add}(\\mathcal M_\\kappa)<\\mathrm{non}(\\mathcal M_\\kappa)$ and of\n$\\mathrm{cov}(\\mathcal M_\\kappa)<\\mathrm{cof}(\\mathcal M_\\kappa)$.\n  We will have a look at (higher analogues of) Cohen, Hechler, localisation,\neventually different, Sacks, random, Laver, Mathias and Miller forcing, and\ntheir effect on the cardinal characteristics of the higher Cicho\\'n diagram.",
      "generated_abstract": "We introduce a new notion of verticality for subsets of the complex plane,\nand show that it is equivalent to the horizontal direction. We also introduce a\nnew notion of verticality for subgroups of the real line, which is equivalent\nto the horizontal direction. We apply these new notions to prove several\nresults about the existence of vertical subgroups in various groups, including\nthe abelian groups. The new notion of verticality can also be used to characterize\nthe existence of vertical subgroups in various classes of groups, including the\nCoxeter groups.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.25,
          "f": 0.1999999952000001
        },
        "rouge-2": {
          "r": 0.031914893617021274,
          "p": 0.045454545454545456,
          "f": 0.037499995153125625
        },
        "rouge-l": {
          "r": 0.09722222222222222,
          "p": 0.14583333333333334,
          "f": 0.11666666186666685
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/CP/2502.01495v1",
      "true_abstract": "We investigate the application of quantum cognition machine learning (QCML),\na novel paradigm for both supervised and unsupervised learning tasks rooted in\nthe mathematical formalism of quantum theory, to distance metric learning in\ncorporate bond markets. Compared to equities, corporate bonds are relatively\nilliquid and both trade and quote data in these securities are relatively\nsparse. Thus, a measure of distance/similarity among corporate bonds is\nparticularly useful for a variety of practical applications in the trading of\nilliquid bonds, including the identification of similar tradable alternatives,\npricing securities with relatively few recent quotes or trades, and explaining\nthe predictions and performance of ML models based on their training data.\nPrevious research has explored supervised similarity learning based on\nclassical tree-based models in this context; here, we explore the application\nof the QCML paradigm for supervised distance metric learning in the same\ncontext, showing that it outperforms classical tree-based models in high-yield\n(HY) markets, while giving comparable or better performance (depending on the\nevaluation metric) in investment grade (IG) markets.",
      "generated_abstract": "d corporate bonds (HYCBs) are frequently traded on exchanges, and\nhigh-frequency trading (HFT) often plays a significant role in their\ntrading. This paper presents a new approach for evaluating the trading\nstrategies of HFT traders by analyzing the trading behavior of HYCBs. To\nunderstand the trading behavior of HFT traders, we propose a novel supervised\nsimilarity model to analyze the trading behavior of HFT traders and provide a\nframework for evaluating their trading strategies. The proposed model uses the\nquantum cognition machine learning (QCML) framework to learn the similarity\nbetween traders' trading behaviors and analyze the strategies of HFT traders.\nThis framework allows us to predict the strategies of HFT traders and\ncharacterize their trading behavior. Additionally, we present",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19626168224299065,
          "p": 0.31343283582089554,
          "f": 0.24137930560906334
        },
        "rouge-2": {
          "r": 0.0457516339869281,
          "p": 0.07446808510638298,
          "f": 0.056680157228606004
        },
        "rouge-l": {
          "r": 0.19626168224299065,
          "p": 0.31343283582089554,
          "f": 0.24137930560906334
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2503.07837v1",
      "true_abstract": "To double the cellular population of ribosomes, a fraction of the active\nribosomes is allocated to synthesize ribosomal proteins. Subsequently, these\nribosomal proteins enter the ribosome self-assembly process, synthesizing new\nribosomes and forming the well-known ribosome autocatalytic subcycle.\nNeglecting ribosome lifetime and the duration of the self-assembly process, the\ndoubling rate of all cellular biomass can be equated with the fraction of\nribosomes allocated to synthesize an essential ribosomal protein times its\nsynthesis rate. However, ribosomes have a finite lifetime, and the assembly\nprocess has a finite duration. Furthermore, the number of ribosomes is known to\ndecrease with slow growth rates. The finite lifetime of ribosomes and the\ndecline in their numbers present a challenge in sustaining slow growth solely\nthrough controlling the allocation of ribosomes to synthesize more ribosomal\nproteins. When the number of ribosomes allocated per mRNA of an essential\nribosomal protein is approximately one, the resulting fluctuations in the\nproduction rate of new ribosomes increase, causing a potential risk that the\nactual production rate will fall below the ribosome death rate. Thus, in this\nregime, a significant risk of extinction of the ribosome population emerges. To\nmitigate this risk, we suggest that the ribosome translation speed is used as\nan alternative control parameter, which facilitates the maintenance of slow\ngrowth rates with a larger ribosome pool. We clarify the observed reduction in\ntranslation speed at harsh environments in E. coli and C. Glutamicum, explore\nother mitigation strategies, and suggest additional falsifiable predictions of\nour model.",
      "generated_abstract": "population extinction (RPE) is a key process in cellular\nproliferation and maintenance. It is driven by a competition between the\ntranslation of newly synthesized mRNA and the maintenance of ribosomes at\nconstant levels. Here, we consider the role of translation inhibition by\nsugars, a phenomenon known as translational inhibition. In this scenario,\nsugar levels can influence the dynamics of ribosome population extinction. In\nthe absence of sugar levels, ribosomes can be rapidly translated and\nextinguished, leading to a high rate of RPE. We show that this scenario can\nalso be avoided by slowing down the translation rate. The optimal slowing rate\nis found by solving the equation of motion for the translation rate, which\nyields a nonlinear equation of motion for the ribosome population. We propose\nthat the optimal slowing",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.208955223880597,
          "p": 0.3684210526315789,
          "f": 0.2666666620480726
        },
        "rouge-2": {
          "r": 0.04245283018867924,
          "p": 0.075,
          "f": 0.05421686285382533
        },
        "rouge-l": {
          "r": 0.1865671641791045,
          "p": 0.32894736842105265,
          "f": 0.23809523347664407
        }
      }
    },
    {
      "paper_id": "math.NT.math/IT/2503.10201v1",
      "true_abstract": "The theories of automorphic forms and self-dual linear codes share many\nremarkable analogies. In both worlds there are functions invariant under an\naction of a group, notions of cusp forms and Hecke operators, also projections\nand lifts between different geni. It is then natural to ask if other important\nautomorphic objects or techniques could be introduced into coding theory. In\nthis article we propose a way to introduce the doubling method, an efficient\ntechnique used to construct and study $L$-functions. As a result, we prove the\nso-called doubling identity, which usually forms a base of many applications.\nHere we use it to solve an analogue of the \"basis problem\". Namely, we express\na cusp form as an explicit linear combination of complete weight enumerators of\nthe same type.",
      "generated_abstract": "We construct a new family of codes, with an exponential number of codewords,\nas a solution to a problem proposed by Chudnovsky and Osherovich. The codes\ninvolve the use of the doubling method in coding theory, which is a well-known\ntechnique for constructing codes with low-density parity-check matrices.\nHowever, in this paper we propose a new and more efficient way of applying the\ndoubling method in the context of codes, which we call the doubling method in\nthe context of codes. We use the idea of using the doubling method to construct\na new family of codes, which is much more efficient than using the doubling\nmethod in the context of codes.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22105263157894736,
          "p": 0.38181818181818183,
          "f": 0.2799999953555556
        },
        "rouge-2": {
          "r": 0.04,
          "p": 0.06097560975609756,
          "f": 0.04830917395971949
        },
        "rouge-l": {
          "r": 0.18947368421052632,
          "p": 0.32727272727272727,
          "f": 0.2399999953555556
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.00209v1",
      "true_abstract": "We explore the influence of framing on decision-making, where some products\nare framed (e.g., displayed, recommended, endorsed, or labeled). We introduce a\nnovel choice function that captures observed variations in framed alternatives.\nBuilding on this, we conduct a comprehensive revealed preference analysis,\nemploying the concept of frame-dependent utility using both deterministic and\nprobabilistic data. We demonstrate that simple and intuitive behavioral\nprinciples characterize our frame-dependent random utility model (FRUM), which\noffers testable conditions even with limited data. Finally, we introduce a\nparametric model to increase the tractability of FRUM. We also discuss how to\nrecover the choice types in our framework.",
      "generated_abstract": "r explores the rich interactions between random utility,\nframeless\nhegemony, and random choice in the presence of bounded rationality. We\ninvestigate a noncooperative game in which agents choose randomly from a\nfinite set of alternatives, subject to a hegemonic framing. We analyze the\nemergent framing structure and its effects on agents' choices. We find that\nrandom utility is compatible with the existence of noncooperative equilibria\neven in the presence of bounded rationality, provided that the framing is\nfrictionless and the hegemonic framing is sufficiently rich. However, when\nbounded rationality is present, we find that the framing structure has a\nstronger influence on the emergent choice structure than random utility. In\nparticular, we find that a framing that is characterized by a high degree of\nfriction, such as a hegemonic framing with a large",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22784810126582278,
          "p": 0.2465753424657534,
          "f": 0.23684210027094885
        },
        "rouge-2": {
          "r": 0.009900990099009901,
          "p": 0.008695652173913044,
          "f": 0.009259254280266737
        },
        "rouge-l": {
          "r": 0.189873417721519,
          "p": 0.2054794520547945,
          "f": 0.19736841606042257
        }
      }
    },
    {
      "paper_id": "math.CO.math/GR/2503.07299v1",
      "true_abstract": "Let $\\varphi:V\\times V\\to W$ be a bilinear map of finite vector spaces $V$\nand $W$ over a finite field $\\mathbb{F}_q$. We present asymptotic bounds on the\nnumber of isomorphism classes of bilinear maps under the natural action of\n$\\mathrm{GL}(V)$ and $\\mathrm{GL}(W)$, when $\\dim(V)$ and $\\dim(W)$ are\nlinearly related.\n  As motivations and applications of the results, we present almost tight upper\nbounds on the number of $p$-groups of Frattini class $2$ as first studied by\nHigman (Proc. Lond. Math. Soc., 1960). Such bounds lead to answers for some\nopen questions by Blackburn, Neumann, and Venkataraman (Cambridge Tracts in\nMathematics, 2007). Further applications include sampling matrix spaces with\nthe trivial automorphism group, and asymptotic bounds on the number of\nisomorphism classes of finite cube-zero commutative algebras.",
      "generated_abstract": "We give a characterization of the average order of a group $G$ of automorphisms\nof a bilinear map $f : X \\times Y \\rightarrow Z$ over a finite field $\\mathbb{F}_q$\nvia the number of $G$-orbits on $X \\times Y$. We also show that the average order\nof a finite group is bounded by the number of orbits of $G$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14606741573033707,
          "p": 0.35135135135135137,
          "f": 0.20634920220080633
        },
        "rouge-2": {
          "r": 0.08108108108108109,
          "p": 0.18,
          "f": 0.11180123795378281
        },
        "rouge-l": {
          "r": 0.1348314606741573,
          "p": 0.32432432432432434,
          "f": 0.19047618632779048
        }
      }
    },
    {
      "paper_id": "quant-ph.cs/IT/2503.09012v1",
      "true_abstract": "The thought experiment of Maxwell's demon highlights the effect of side\ninformation in thermodynamics. In this paper, we present an axiomatic treatment\nof a quantum Maxwell's demon, by introducing a resource-theoretic framework of\nquantum thermodynamics in the presence of quantum side information. Under\nminimal operational assumptions that capture the demon's behaviour, we derive\nthe one-shot work costs of preparing, as well as erasing, a thermodynamic\nsystem whose coupling with the demon's mind is described by a bipartite quantum\nstate. With trivial Hamiltonians, these work costs are precisely captured by\nthe smoothed conditional min- and max-entropies, respectively, thus providing\noperational interpretations for these one-shot information-theoretic quantities\nin microscopic thermodynamics. An immediate, information-theoretic implication\nof our results is an affirmative proof of the conjectured maximality of the\nconditional max-entropy among all axiomatically plausible conditional\nentropies, complementing the recently established minimality of the conditional\nmin-entropy. We then generalize our main results to the setting with nontrivial\nHamiltonians, wherein the work costs of preparation and erasure are captured by\na generalized type of mutual information. Finally, we present a macroscopic\nsecond law of thermodynamics in the presence of quantum side information, in\nterms of a conditional version of the Helmholtz free energy. Our results extend\nthe conceptual connection between thermodynamics and quantum information theory\nby refining the axiomatic common ground between the two theories and revealing\nfundamental insights of each theory in light of the other.",
      "generated_abstract": "um nature of information is manifest in its non-classical\ncharacteristics, such as entanglement, which makes it difficult to fully\nquantify and understand its properties. In this work, we investigate the\nfundamental work costs of quantum information processing, including the\npreparation and erasure of qubits. In particular, we provide a concise and\nmathematically rigorous proof of the fundamental work cost of a single-qubit\noperation in the presence of quantum side information, which is defined as\nthe entanglement cost of preparing and measuring the qubit. We show that the\nwork cost of preparation and erasure in the presence of quantum side\ninformation scales linearly with the number of qubits in the system. Our\nresults provide a new perspective on the nature of quantum side information,\nas well as a framework for understanding the fundamental work costs of quantum\ninformation processing. They also have important implications for the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22962962962962963,
          "p": 0.3924050632911392,
          "f": 0.2897196215106123
        },
        "rouge-2": {
          "r": 0.10476190476190476,
          "p": 0.1864406779661017,
          "f": 0.13414633685678184
        },
        "rouge-l": {
          "r": 0.2074074074074074,
          "p": 0.35443037974683544,
          "f": 0.26168223833304227
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ME/2503.02108v1",
      "true_abstract": "Generalized Bayesian Inference (GBI) provides a flexible framework for\nupdating prior distributions using various loss functions instead of the\ntraditional likelihoods, thereby enhancing the model robustness to model\nmisspecification. However, GBI often suffers the problem associated with\nintractable likelihoods. Kernelized Stein Discrepancy (KSD), as utilized in a\nrecent study, addresses this challenge by relying only on the gradient of the\nlog-likelihood. Despite this innovation, KSD-Bayes suffers from critical\npathologies, including insensitivity to well-separated modes in multimodal\nposteriors. To address this limitation, we propose a weighted KSD method that\nretains computational efficiency while effectively capturing multimodal\nstructures. Our method improves the GBI framework for handling intractable\nmultimodal posteriors while maintaining key theoretical properties such as\nposterior consistency and asymptotic normality. Experimental results\ndemonstrate that our method substantially improves mode sensitivity compared to\nstandard KSD-Bayes, while retaining robust performance in unimodal settings and\nin the presence of outliers.",
      "generated_abstract": "We propose a novel method for correcting mode proportion bias in\ngeneralized Bayesian inference. Our approach leverages the Stein discrepancy\nbetween the empirical distribution of a mode proportion estimator and its\ntrue counterpart. By optimizing the weighting scheme, we ensure that the\ncorrection is weighted appropriately and ensures that the bias is not\nexcessive. We show that our correction method yields improved convergence\nproperties compared to existing methods. We also show that our correction method\nis validated through simulation studies and an empirical study of a real-world\ndataset. Code is available at https://github.com/JeffreyXu23/Correcting_Mode_Proportion_Bias.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16964285714285715,
          "p": 0.2835820895522388,
          "f": 0.21229049810929757
        },
        "rouge-2": {
          "r": 0.02112676056338028,
          "p": 0.03488372093023256,
          "f": 0.02631578477531632
        },
        "rouge-l": {
          "r": 0.16071428571428573,
          "p": 0.26865671641791045,
          "f": 0.2011173137517557
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/LO/2503.06284v1",
      "true_abstract": "Isolation bugs, stemming especially from design-level defects, have been\nrepeatedly found in carefully designed and extensively tested production\ndatabases over decades. In parallel, various frameworks for modeling database\ntransactions and reasoning about their isolation guarantees have been\ndeveloped. What is missing however is a mathematically rigorous and systematic\nframework with tool support for formally verifying a wide range of such\nguarantees for all possible system behaviors. We present the first such\nframework, VerIso, developed within the theorem prover Isabelle/HOL. To\nshowcase its use in verification, we model the strict two-phase locking\nconcurrency control protocol and verify that it provides strict serializability\nisolation guarantee. Moreover, we show how VerIso helps identify isolation bugs\nduring protocol design. We derive new counterexamples for the TAPIR protocol\nfrom failed attempts to prove its claimed strict serializability. In\nparticular, we show that it violates a much weaker isolation level, namely,\natomic visibility.",
      "generated_abstract": "t VerIso, a novel framework for verifiable isolation guarantees in\ndatabase systems. VerIso is designed for use in real-world scenarios, where\ndecentralized data stores are required to be resilient to failures. VerIso\nprovides a framework that allows for the definition of a set of isolation\nproperties for a given data store, along with corresponding isolation\nguarantees. These guarantees are expressed in terms of a set of constraints\non the data store. The constraints can be formalized using a set of axioms. The\naxiomatic approach allows for the construction of a data store that satisfies\nthe specified isolation properties, and can be verified against the specified\nisolation guarantees. The framework is built on the Causal Consistency\nTheory (CCT) framework, which allows for the specification of a set of\naxioms. CCT is a well-established",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17857142857142858,
          "p": 0.28169014084507044,
          "f": 0.21857923022365566
        },
        "rouge-2": {
          "r": 0.028169014084507043,
          "p": 0.03636363636363636,
          "f": 0.03174602682665735
        },
        "rouge-l": {
          "r": 0.16071428571428573,
          "p": 0.2535211267605634,
          "f": 0.1967213067263879
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.11183v3",
      "true_abstract": "An agent engages in sequential search. He does not directly observe the\nquality of the goods he samples, but he can purchase signals designed by profit\nmaximizing principal(s). We formulate the principal-agent relationship as a\nrepeated contracting problem within a stopping game and characterize the set of\nequilibrium payoffs. We show that when the agent's search cost falls below a\ngiven threshold, competition does not impact how much surplus is generated in\nequilibrium nor how the surplus is divided. In contrast, competition benefits\nthe agent at the expense of total surplus when the search cost exceeds that\nthreshold. Our results challenge the view that monopoly decreases market\nefficiency, and moreover, suggest that it generates the highest value of\ninformation for the agent.",
      "generated_abstract": "r introduces a model of search that captures the complex interactions\nbetween consumers, firms, and markets. The model is based on the concept of\ncompetition in the sense of Nash equilibrium, and we provide a characterization\nof this equilibrium. We then extend the model by allowing for a limited\npersuasion of consumers by firms, and show that this leads to a class of\nequilibria with a non-trivial dependence on the persuasion level. We then\ninvestigate the consequences of the presence of search costs on the equilibrium\nstructure of the market. In particular, we show that the presence of search\ncosts leads to a generalization of the Nash equilibrium that has not been\nobserved in the literature, and we characterize this equilibrium as the\nmaximum-revenue equilibrium. We also derive a necessary and sufficient\ncondition for the existence of the maximum-revenue equilibrium in terms of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2235294117647059,
          "p": 0.2602739726027397,
          "f": 0.24050632414276568
        },
        "rouge-2": {
          "r": 0.02586206896551724,
          "p": 0.024793388429752067,
          "f": 0.025316450698428947
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.2054794520547945,
          "f": 0.18987341275036065
        }
      }
    },
    {
      "paper_id": "math.ST.q-fin/MF/2412.06343v1",
      "true_abstract": "We propose analytically tractable SDE models for correlation in financial\nmarkets. We study diffusions on the circle, namely the Brownian motion on the\ncircle and the von Mises process, and consider these as models for correlation.\nThe von Mises process was proposed in Kent (1975) as a probabilistic\njustification for the von Mises distribution which is widely used in Circular\nstatistics. The transition density of the von Mises process has been unknown,\nwe identify an approximate analytic transition density for the von Mises\nprocess. We discuss the estimation of these diffusion models and a stochastic\ncorrelation model in finance. We illustrate the application of the proposed\nmodel on real-data of equity-currency pairs.",
      "generated_abstract": "In this paper, we study a stochastic model for diffusion processes on the\ncircle with a specific drift. We derive a diffusion equation for the process\nand show that it is the solution to the martingale problem for the operator\n$L$ defined by the Laplacian on the unit circle. We then use this equation to\nstudy the correlation between two diffusion processes on the circle. We find a\nstochastic correlation model for two diffusion processes on the circle. This\nmodel is a generalization of the stochastic correlation model for two\nindependent random variables on the circle, and we use it to study the\ncorrelation between two independent random variables on the circle. We also show\nthat our model is related to the model of Wong-Zakai developed by J.J.\nWong-Zakai in 1983.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27692307692307694,
          "p": 0.3157894736842105,
          "f": 0.2950819622346144
        },
        "rouge-2": {
          "r": 0.08333333333333333,
          "p": 0.08247422680412371,
          "f": 0.08290154940427961
        },
        "rouge-l": {
          "r": 0.27692307692307694,
          "p": 0.3157894736842105,
          "f": 0.2950819622346144
        }
      }
    },
    {
      "paper_id": "physics.gen-ph.physics/gen-ph/2503.09604v1",
      "true_abstract": "The article studies the extension of the internal spaces of fermion and boson\nsecond quantized fields, described by the superposition of odd (for fermions)\nand even (for bosons) products of the operators $\\gamma^ {a}$, to strings and\nodd dimensional spaces.\\\\ For any symmetry $SO(d-1,1)$ of the internal spaces,\nit is the number of fermion fields (they appear in families and have their\nHermitian conjugated partners in a separate group) equal to the number of boson\nfields (they appear in two orthogonal groups), manifesting a kind of\nsupersymmetry, which differs from the usual supersymmetry.\\\\ The article\nsearches for the supersymmetry arising from extending the ``basis vectors'' of\nsecond quantized fermion and boson fields described in $d=2(2n+1)$ (in\nparticular $d=(13+1)$) either to strings or to odd-dimensional spaces\n($d=2(2n+1)+1$).",
      "generated_abstract": "In this paper, we propose a new way to understand the supersymmetry\nrelations. We construct a supersymmetric extension of the second quantized\nfields and boson and fermion fields, which are two different ways of constructing\nthe second quantized fields. In addition, we construct a supersymmetric\nextension of the strings and odd dimensional spaces. By constructing these\nextensions, we can understand the supersymmetry relations more clearly. We can\nfind that the supersymmetric extension of the boson and fermion fields can be\nconstructed by the boson and fermion fields. The supersymmetric extension of\nthe strings and odd dimensional spaces can be constructed by the strings and\nodd dimensional spaces.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2727272727272727,
          "p": 0.4375,
          "f": 0.33599999526912
        },
        "rouge-2": {
          "r": 0.09821428571428571,
          "p": 0.15942028985507245,
          "f": 0.12154695660816232
        },
        "rouge-l": {
          "r": 0.23376623376623376,
          "p": 0.375,
          "f": 0.28799999526912007
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.17883v1",
      "true_abstract": "The expected utility theorem of von Neumann and Morgenstern (1947) has been a\nmilestone in economics, describing rational behavior by two axioms on a weak\npreference on lotteries on a finite set of outcomes: the Independence Axiom and\nthe Continuity Axiom. For a weak preference fulfilling the Independence Axiom,\nI prove that continuity is equivalent to the existence of a set indifferent\nlotteries spanning a hyperplane.",
      "generated_abstract": "This paper investigates the continuity of a class of non-decreasing and\nnon-increasing functions defined on a compact metric space. We show that the\nclass is closed under independence and that a necessary and sufficient\ncondition for continuity is the existence of a least indifferent point. We\nalso prove that the class is not closed under non-linear addition and that the\nclass is closed under non-linear subtraction. As an application, we show that\nthe class contains the set of all non-negative functions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2553191489361702,
          "p": 0.2608695652173913,
          "f": 0.2580645111296104
        },
        "rouge-2": {
          "r": 0.11475409836065574,
          "p": 0.1076923076923077,
          "f": 0.11111110611615038
        },
        "rouge-l": {
          "r": 0.23404255319148937,
          "p": 0.2391304347826087,
          "f": 0.23655913478552443
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.17284v1",
      "true_abstract": "We test and study the variation in speech recognition of fine-tuned versions\nof the Whisper model on child, elderly and non-native Dutch speech from the\nJASMIN-CGN corpus. Our primary goal is to evaluate how speakers' age and\nlinguistic background influence Whisper's performance. Whisper achieves varying\nWord Error Rates (WER) when fine-tuned on subpopulations of specific ages and\nlinguistic backgrounds. Fine-tuned performance is remarkably better than\nzero-shot performance, achieving a relative reduction in WER of 81% for native\nchildren, 72% for non-native children, 67% for non-native adults, and 65% for\nnative elderly people. Our findings underscore the importance of training\nspeech recognition models like Whisper on underrepresented subpopulations such\nas children, the elderly, and non-native speakers.",
      "generated_abstract": "The JASMIN-CGN corpus is a Dutch-speaking corpus, which has been\nenhanced by adding speech-related lexical and grammatical tags. However,\nexisting fine-tuning approaches do not fully address the challenges of\nspeech recognition. In this paper, we propose a novel method that improves the\naccuracy of speech recognition using the JASMIN-CGN corpus. Specifically, we\nfine-tune the Whisper model on the JASMIN-CGN corpus. This fine-tuning process\nintroduces a new set of tags that are specific to Dutch speech. We evaluate\nour method on the JASMIN-CGN corpus, and the results show that our method\noutperforms existing fine-tuning approaches.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21794871794871795,
          "p": 0.27419354838709675,
          "f": 0.24285713792244906
        },
        "rouge-2": {
          "r": 0.06422018348623854,
          "p": 0.08433734939759036,
          "f": 0.07291666175835536
        },
        "rouge-l": {
          "r": 0.21794871794871795,
          "p": 0.27419354838709675,
          "f": 0.24285713792244906
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/OT/2404.08738v2",
      "true_abstract": "Air quality is a critical component of environmental health. Monitoring and\nanalysis of particulate matter with a diameter of 2.5 micrometers or smaller\n(PM2.5) plays a pivotal role in understanding air quality changes. This study\nfocuses on the application of a new bandpass bootstrap approach, termed the\nVariable Bandpass Periodic Block Bootstrap (VBPBB), for analyzing time series\ndata which provides modeled predictions of daily mean PM2.5 concentrations over\n16 years in Manhattan, New York, the United States. The VBPBB can be used to\nexplore periodically correlated (PC) principal components for this daily mean\nPM2.5 dataset. This method uses bandpass filters to isolate distinct PC\ncomponents from datasets, removing unwanted interference including noise, and\nbootstraps the PC components. This preserves the PC structure and permits a\nbetter understanding of the periodic characteristics of time series data. The\nresults of the VBPBB are compared against outcomes from alternative block\nbootstrapping techniques. The findings of this research indicate potential\ntrends of elevated PM2.5 levels, providing evidence of significant semi-annual\nand weekly patterns missed by other methods.",
      "generated_abstract": "This study examines the seasonal and periodic patterns of the Air Quality\nModel Integrated Forecasting and Data Assimilation (AMFIDA) model for the\ncomparison of its performance against the 24-hour PM2.5 observations from the\nGarment Industry Monitoring Network (GIMMN). The proposed methodology employs\nthe Variable Bandpass Periodic Block Bootstrap (VBPBB), which is a new method\nthat allows for the simultaneous identification of seasonal and periodic\npatterns. The results show that the proposed methodology has a significant\nperformance gain compared to the standard AMFIDA model. Additionally, the\nproposed methodology is able to identify seasonal and periodic patterns,\nenhancing the understanding of the seasonal variations in PM2.5 concentrations\nin the GIMMN data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.265625,
          "p": 0.4722222222222222,
          "f": 0.339999995392
        },
        "rouge-2": {
          "r": 0.08333333333333333,
          "p": 0.14285714285714285,
          "f": 0.10526315324099744
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.4444444444444444,
          "f": 0.319999995392
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.09062v1",
      "true_abstract": "Knowledge dissemination in educational settings is profoundly influenced by\nthe curse of knowledge, a cognitive bias that causes experts to underestimate\nthe challenges faced by learners due to their own in-depth understanding of the\nsubject. This bias can hinder effective knowledge transfer and pedagogical\neffectiveness, and may be exacerbated by inadequate instructor-student\ncommunication. To encourage more effective feedback and promote empathy, we\nintroduce TSConnect, a bias-aware, adaptable interactive MOOC (Massive Open\nOnline Course) learning system, informed by a need-finding survey involving 129\nstudents and 6 instructors. TSConnect integrates instructors, students, and\nArtificial Intelligence (AI) into a cohesive platform, facilitating diverse and\ntargeted communication channels while addressing previously overlooked\ninformation needs. A notable feature is its dynamic knowledge graph, which\nenhances learning support and fosters a more interconnected educational\nexperience. We conducted a between-subjects user study with 30 students\ncomparing TSConnect to a baseline system. Results indicate that TSConnect\nsignificantly encourages students to provide more feedback to instructors.\nAdditionally, interviews with 4 instructors reveal insights into how they\ninterpret and respond to this feedback, potentially leading to improvements in\nteaching strategies and the development of broader pedagogical skills.",
      "generated_abstract": "r proposes a novel MOOC platform for bridging communication gaps\nbetween instructors and students in the light of the curse of knowledge. The\nproposed platform is built on the premise that instructors have limited time\nand students have limited attention, which causes a communication gap. This\npaper proposes a two-tiered communication model. The first tier is based on\nthe instructors' lectures, which are designed to be informative and\nconcise, while the second tier is based on the students' responses, which are\ndesigned to be in-depth and thorough. The proposed platform has been\nevaluated through the use of 200 randomly selected lectures from a MOOC\ncourse on the MOOC platform Coursera, and 100 randomly selected responses from\nstudents. The results show that the platform significantly improves the\ncommunication between instructors and students, particularly in terms of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15942028985507245,
          "p": 0.2894736842105263,
          "f": 0.2056074720552014
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.02608695652173913,
          "f": 0.019933550095474675
        },
        "rouge-l": {
          "r": 0.13768115942028986,
          "p": 0.25,
          "f": 0.17757008887763134
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2409.04772v1",
      "true_abstract": "Cell motility is fundamental to many biological processes, and cells exhibit\na variety of migration patterns. Many motile cell types follow a universal law\nthat connects their speed and persistency, a property that can originate from\nthe intracellular transport of polarity cues due to the global actin retrograde\nflow. This mechanism was termed the ``Universal Coupling between cell Speed and\nPersistency\"(UCSP). Here we implemented a simplified version of the UCSP\nmechanism in a coarse-grained ``minimal-cell\" model, which is composed of a\nthree-dimensional vesicle that contains curved active proteins. This model\nspontaneously forms a lamellipodia-like motile cell shape, which is however\nsensitive and can depolarize into a non-motile form due to random fluctuations\nor when interacting with external obstacles. The UCSP implementation introduces\nlong-range inhibition, which stabilizes the motile phenotype. This allows our\nmodel to describe the robust polarity observed in cells and explain a large\nvariety of cellular dynamics, such as the relation between cell speed and\naspect ratio, cell-barrier scattering, and cellular oscillations in different\ntypes of geometric confinements.",
      "generated_abstract": "ation is essential for organismal development and homeostasis, but\nhow cells maintain persistent migration and interact with external barriers\nremains unclear. In this study, we investigate the role of the actin cytoskeleton\nin mediating persistence of cell migration. We present a new model of cell\nmigration that includes two key features: (i) persistence of migration in the\nabsence of an external force and (ii) cell-cell interactions that modulate\npersistence. Our model predicts that cells migrating in the absence of an\nexternal force exhibit persistence as a result of actin dynamics. The persistence\nof migration is enhanced by cell-cell interactions, where cells attach to\nopposite walls of an internal barrier and engulf the barrier during migration.\nWe find that persistence of migration is robust to changes in cell number and\nthe strength of cell-cell interactions. Our findings reve",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.25316455696202533,
          "f": 0.20618556218248496
        },
        "rouge-2": {
          "r": 0.018404907975460124,
          "p": 0.025210084033613446,
          "f": 0.021276590866406233
        },
        "rouge-l": {
          "r": 0.17391304347826086,
          "p": 0.25316455696202533,
          "f": 0.20618556218248496
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.00934v2",
      "true_abstract": "Background: Global viral threats underscore the need for effective genomic\nsurveillance, but high costs and uneven resource distribution hamper its\nimplementation. Targeting surveillance to international travelers in major\ntravel hubs may offer a more efficient strategy for the early detection of\nSARS-CoV-2 variants.\n  Methods: We developed and calibrated a multiple-strain metapopulation model\nof global SARS-CoV-2 transmission using extensive epidemiological,\nphylogenetic, and high-resolution air travel data. We then compared baseline\nsurveillance with various resource-allocation approaches that prioritize\ntravelers, focusing on Omicron BA.1/BA.2 retrospectively and on hypothetical\nfuture variants under different emergence, transmission and vaccine\neffectiveness scenarios.\n  Findings: Focusing existing surveillance resources on travelers at key global\nhubs significantly shortened detection delays without increasing total\nsurveillance efforts. In retrospective analyses of Omicron BA.1/BA.2,\ntraveler-targeted approaches consistently outperformed baseline strategies,\neven when overall resources were reduced. Simulations indicate that focusing\nsurveillance on key travel hubs outperform baseline practices in detecting\nfuture variants, across different possible origins, even with reduced\nresources. This approach also remains effective in future pandemic scenarios\nwith varying reproductive numbers and vaccine effectiveness.\n  Interpretation: These findings provide a quantitative, cost-effective\nframework for strengthening global genomic surveillance. By reallocating\nresources toward international travelers in select travel hubs, early detection\nof emerging variants can be enhanced, informing rapid public health\ninterventions and bolstering preparedness for future pandemics.",
      "generated_abstract": "obal spread of SARS-CoV-2 continues, efforts to detect emerging\nsequences of the virus have been a critical component of pandemic response.\nExisting approaches, such as the Global Virome Project, utilize high-throughput\nsequencing of viral genomes to monitor viral evolution and inform vaccine\ndevelopment. While these efforts have produced valuable insights, they are\nlimited by their reliance on the availability of viral sequence data.\n  In this paper, we propose an improved genomic surveillance approach that\nenables the detection of emerging variants of SARS-CoV-2, while also\nminimizing the impact on existing sequencing capacity. Our approach uses a\ndynamic pricing mechanism to dynamically adjust the cost of sequencing and\nreports the emergence of variants of interest (VOIs) in real-time. We demonstrate\nthe effectiveness of our approach in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17333333333333334,
          "p": 0.2857142857142857,
          "f": 0.21576763015443962
        },
        "rouge-2": {
          "r": 0.023923444976076555,
          "p": 0.04132231404958678,
          "f": 0.03030302565858657
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.27472527472527475,
          "f": 0.20746887496771763
        }
      }
    },
    {
      "paper_id": "cs.CL.q-bio/NC/2503.04848v2",
      "true_abstract": "Human language and logic abilities are computationally quantified within the\nwell-studied grammar-automata hierarchy. We identify three hierarchical tiers\nand two corresponding transitions and show their correspondence to specific\nabilities in transformer-based language models (LMs). These emergent abilities\nhave often been described in terms of scaling; we show that it is the\ntransition between tiers, rather than scaled size itself, that determines a\nsystem's capabilities. Specifically, humans effortlessly process language yet\nrequire critical training to perform arithmetic or logical reasoning tasks; and\nLMs possess language abilities absent from predecessor systems, yet still\nstruggle with logical processing. We submit a novel benchmark of computational\npower, provide empirical evaluations of humans and fifteen LMs, and, most\nsignificantly, provide a theoretically grounded framework to promote careful\nthinking about these crucial topics. The resulting principled analyses provide\nexplanatory accounts of the abilities and shortfalls of LMs, and suggest\nactionable insights into the expansion of their logic abilities.",
      "generated_abstract": "The success of transformer-based architectures in natural language processing\nand generation raises questions about the computational requirements of\nthese models. One approach to answering these questions is to analyze the\narchitecture of the transformer itself, including its hidden layer. In this\npaper, we analyze the architecture of the transformer and identify three\ntiers of computation: a bottom-up tier that computes local representations, a\nmiddle-up tier that computes higher-order representations, and a top-down tier\nthat computes global representations. By examining these tiers, we find that\ntransformer-based models are capable of performing three kinds of computation:\nlocal, higher-order, and global. We further identify a hierarchy of model\narchitectures, based on the number and types of computation tiers, that\ncorresponds to different levels of complexity in language modeling.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21929824561403508,
          "p": 0.3246753246753247,
          "f": 0.2617800998996738
        },
        "rouge-2": {
          "r": 0.013333333333333334,
          "p": 0.017543859649122806,
          "f": 0.015151510244491946
        },
        "rouge-l": {
          "r": 0.21052631578947367,
          "p": 0.3116883116883117,
          "f": 0.2513088957111922
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.05905v1",
      "true_abstract": "Recent developments in sequential experimental design look to construct a\npolicy that can efficiently navigate the design space, in a way that maximises\nthe expected information gain. Whilst there is work on achieving tractable\npolicies for experimental design problems, there is significantly less work on\nobtaining policies that are able to generalise well - i.e. able to give good\nperformance despite a change in the underlying statistical properties of the\nexperiments. Conducting experiments sequentially has recently brought about the\nuse of reinforcement learning, where an agent is trained to navigate the design\nspace to select the most informative designs for experimentation. However,\nthere is still a lack of understanding about the benefits and drawbacks of\nusing certain reinforcement learning algorithms to train these agents. In our\nwork, we investigate several reinforcement learning algorithms and their\nefficacy in producing agents that take maximally informative design decisions\nin sequential experimental design scenarios. We find that agent performance is\nimpacted depending on the algorithm used for training, and that particular\nalgorithms, using dropout or ensemble approaches, empirically showcase\nattractive generalisation properties.",
      "generated_abstract": "Experimental design is a fundamental tool for understanding scientific\ndisease and drug development. However, the design of experiments is\ncomplex, with multiple experimental variables and multiple treatment\ncombinations. This paper examines the performance of reinforcement learning\n(RL) algorithms for sequential experimental design in a stochastic environment.\nWe present two benchmark algorithms: a Markov decision process (MDP)-based\nalgorithm and a policy gradient (PG) algorithm. We analyze the performance of\neach algorithm in a number of scenarios, including a simulation of\nexperimental design and an experimental design problem based on drug development\nand testing. We demonstrate that the PG algorithm consistently outperforms the\nMDP-based algorithm in both simulation and real-world experiments.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.3382352941176471,
          "f": 0.25136611554838906
        },
        "rouge-2": {
          "r": 0.04242424242424243,
          "p": 0.06930693069306931,
          "f": 0.05263157423681427
        },
        "rouge-l": {
          "r": 0.19130434782608696,
          "p": 0.3235294117647059,
          "f": 0.24043715379975525
        }
      }
    },
    {
      "paper_id": "physics.geo-ph.physics/geo-ph/2503.04227v1",
      "true_abstract": "We present the first extensive analysis of K/Ka-band ranging post-fit\nresiduals of an official Level-2 product, characterised as Line-of-Sight\nGravity Differences (LGD), which exhibit and showcase interesting sub-monthly\ngeophysical signals. These residuals, provided by CSR, were derived from the\ndifference between spherical harmonic coefficient least-squares fits and\nreduced Level-1B range-rate observations. We classified the geophysical signals\ninto four distinct categories: oceanic, meteorological, hydrological, and solid\nEarth, focusing primarily on the first three categories in this study. In our\nexamination of oceanic processes, we identified notable mass anomalies in the\nArgentine basin, specifically within the Zapiola Rise, where persistent\nremnants of the rotating dipole-like modes are evident in the LGD post-fit\nresiduals. Our analysis extended to the Gulf of Carpentaria and Australia\nduring the 2013 Oswald cyclone, revealing significant LGD residual anomalies\nthat correlate with cyclone tracking and precipitation data. Additionally, we\ninvestigated the monsoon seasons in Bangladesh, particularly from June to\nSeptember 2007, where we observed peaks in sub-monthly variability. These\nfindings were further validated by demonstrating high spatial and temporal\ncorrelations between gridded LGD residuals and ITSG-Grace2018 daily solutions.\nGiven that these anomalies are associated with significant mass change\nphenomena, it is essential to integrate the post-fit residuals into a\nhigh-frequency mass change framework, with the purpose of providing enhanced\nspatial resolution compared to conventional Kalman-filtered methods.",
      "generated_abstract": "Radar altimetry has been used to monitor the change in mass of the Earth's\nlow-lying regions. The post-fit residuals of the radar altimeter observations\nare known to exhibit significant non-Gaussianity, which can be problematic in\nthe case of mass change applications. In this paper, we present a novel\nmethodology to account for this non-Gaussianity. The methodology is based on\nthe use of the posterior distribution of the range rate parameters, which\nprovides a more accurate estimate of the change in mass. We present two\nexamples of post-fit residuals where the change in mass is estimated to be\nseveral orders of magnitude larger than the uncertainty of the radar altimeter\nobservations. We also demonstrate that the proposed methodology provides\nbetter accuracy in mass change applications than the traditional approach that\nis based on the least squares method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14838709677419354,
          "p": 0.2948717948717949,
          "f": 0.1974248882499218
        },
        "rouge-2": {
          "r": 0.037914691943127965,
          "p": 0.07079646017699115,
          "f": 0.04938271150682103
        },
        "rouge-l": {
          "r": 0.14838709677419354,
          "p": 0.2948717948717949,
          "f": 0.1974248882499218
        }
      }
    },
    {
      "paper_id": "quant-ph.physics/hist-ph/2503.00966v1",
      "true_abstract": "The Frauchiger-Renner argument purports to show that the standard framework\nof quantum mechanics yields a contradiction when used to reason about systems\ncontaining agents who are themselves using quantum mechanics to perform\ndeductions. This has been framed as an obstacle to taking quantum mechanics to\nbe a complete theory. I formalize the argument in two closely related ways and\nelucidate the source of the paradox, clarifying the flaw in the original\nargument.",
      "generated_abstract": "igate the capacity of a classical observer to reason about quantum\ncommunication in a spacelike surface, as proposed by Frauchiger and Renner in\ntheir thought experiment, by means of a Bayesian approach. Our approach\nincorporates the Bayes' rule and its extensions, such as the Bayes' factor, in\nthe framework of quantum information theory. We show that the classical\nobserver can estimate the quantum state with a high probability, and\nunderstand the quantum state as a distribution over the spacelike surface. In\ncontrast to the naive expectation, the quantum state can be described as a\ndistribution over the spacelike surface, but only if the observer is able to\nreason about the quantum state. We further show that the classical observer\ncan estimate the quantum state in a region of spacelike separation from the\nobserver, and that the quantum state can be described as a distribution over\nthe spacelike",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2777777777777778,
          "p": 0.22388059701492538,
          "f": 0.24793387935523542
        },
        "rouge-2": {
          "r": 0.10294117647058823,
          "p": 0.06542056074766354,
          "f": 0.07999999524832681
        },
        "rouge-l": {
          "r": 0.2777777777777778,
          "p": 0.22388059701492538,
          "f": 0.24793387935523542
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/RO/2503.10630v1",
      "true_abstract": "In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods.",
      "generated_abstract": "nted navigation (Goal-Nav) has emerged as a critical research\ngoal in robotics due to its potential for enhancing safety, efficiency,\nautonomy, and adaptability. However, existing methods for Goal-Nav often\nrely on task-specific expert demonstrations, which are often scarce,\ntime-consuming, and unreliable. This paper introduces UniGoal, a zero-shot\nGoal-Nav framework that enables unsupervised goal-oriented navigation by\nrelying only on a single visual demonstration. UniGoal leverages a transformer\nto capture both the visual and semantic information of the scene, enabling it\nto understand the scene from a global perspective and identify the target\nobject. UniGoal then generates a plan to achieve the target object and\nexecutes the plan to achieve the target object. Our experiments on the\nKITTI-V",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17006802721088435,
          "p": 0.3048780487804878,
          "f": 0.21834060675654554
        },
        "rouge-2": {
          "r": 0.01904761904761905,
          "p": 0.037037037037037035,
          "f": 0.02515722821882126
        },
        "rouge-l": {
          "r": 0.1564625850340136,
          "p": 0.2804878048780488,
          "f": 0.2008733578482486
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2407.07797v2",
      "true_abstract": "The mechanics of animal cells is strongly determined by stress fibers, which\nare contractile filament bundles that form dynamically in response to\nextracellular cues. Stress fibers allow the cell to adapt its mechanics to\nenvironmental conditions and to protect it from structural damage. While the\nphysical description of single stress fibers is well-developed, much less is\nknown about their spatial distribution on the level of whole cells. Here, we\ncombine a finite element method for one-dimensional fibers embedded in an\nelastic bulk medium with dynamical rules for stress fiber formation based on\ngenetic algorithms. We postulate that their main goal is to achieve minimal\nmechanical stress in the bulk material with as few fibers as possible. The\nfiber positions and configurations resulting from this optimization task alone\nare in good agreement with those found in experiments where cells in\n3D-scaffolds were mechanically strained at one attachment point. For optimized\nconfigurations, we find that stress fibers typically run through the cell in a\ndiagonal fashion, similar to reinforcement strategies used for composite\nmaterial.",
      "generated_abstract": "bers are a key structural component in many cellular structures,\nincluding actin cables and filopodia, but their precise positioning and\ninteractions with surrounding cells remain poorly understood. We use a\ncontinuum-based model to investigate the effects of stress fiber positioning on\ninternal mechanical stress in contractile cells. The model incorporates\ndiffusive transport of stress fibers, with stress fibers interacting with the\ncell membrane via a spring-potential. Our results reveal that the positioning\nof stress fibers within cells minimizes internal mechanical stress, which is\ndetermined by the interaction between stress fibers and the cell membrane. This\nminimization of internal stress is particularly important for cells that\nexperience high levels of mechanical stress, such as those in the heart and\ngastrointestinal tract, where mechanical stress is often present. The model\nalso reveals that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2459016393442623,
          "p": 0.35714285714285715,
          "f": 0.29126213109246873
        },
        "rouge-2": {
          "r": 0.047337278106508875,
          "p": 0.06722689075630252,
          "f": 0.055555550706260066
        },
        "rouge-l": {
          "r": 0.22131147540983606,
          "p": 0.32142857142857145,
          "f": 0.26213591750023574
        }
      }
    },
    {
      "paper_id": "quant-ph.eess/IV/2503.01916v1",
      "true_abstract": "In transportation cyber-physical systems (CPS), ensuring safety and\nreliability in real-time decision-making is essential for successfully\ndeploying autonomous vehicles and intelligent transportation networks. However,\nthese systems face significant challenges, such as computational complexity and\nthe ability to handle ambiguous inputs like shadows in complex environments.\nThis paper introduces a Quantum Deep Convolutional Neural Network (QDCNN)\ndesigned to enhance the safety and reliability of CPS in transportation by\nleveraging quantum algorithms. At the core of QDCNN is the UU{\\dag} method,\nwhich is utilized to improve shadow detection through a propagation algorithm\nthat trains the centroid value with preprocessing and postprocessing operations\nto classify shadow regions in images accurately. The proposed QDCNN is\nevaluated on three datasets on normal conditions and one road affected by rain\nto test its robustness. It outperforms existing methods in terms of\ncomputational efficiency, achieving a shadow detection time of just 0.0049352\nseconds, faster than classical algorithms like intensity-based thresholding\n(0.03 seconds), chromaticity-based shadow detection (1.47 seconds), and local\nbinary pattern techniques (2.05 seconds). This remarkable speed, superior\naccuracy, and noise resilience demonstrate the key factors for safe navigation\nin autonomous transportation in real-time. This research demonstrates the\npotential of quantum-enhanced models in addressing critical limitations of\nclassical methods, contributing to more dependable and robust autonomous\ntransportation systems within the CPS framework.",
      "generated_abstract": "s vehicles (AVs) are increasingly used for transportation, yet\nthere is a critical need for reliable and safe AV systems. While deep learning\n(DL) has proven effective for automated driving (AD), it is still challenging\nfor the integration of safety into AVs due to the complex dynamics,\nnon-linearity, and uncertainties in AV systems. To address this, we propose\nQuantum Deep Learning (QDL), a novel DL framework that leverages quantum\ncomputing to enhance safety and reliability in AV systems. QDL consists of a\ndeep learning model and a quantum computer. The model is designed to learn\ncritical AV behaviors and features from high-quality data. The quantum computer\nis used for QDL's learning process to enhance its safety and reliability. The\nmodel and computer are trained and tested on real-world data, achieving\nimproved safety and reli",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16883116883116883,
          "p": 0.3023255813953488,
          "f": 0.21666666206805565
        },
        "rouge-2": {
          "r": 0.02843601895734597,
          "p": 0.049586776859504134,
          "f": 0.0361445736806872
        },
        "rouge-l": {
          "r": 0.15584415584415584,
          "p": 0.27906976744186046,
          "f": 0.199999995401389
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.18328v1",
      "true_abstract": "Recent advances in Visual Anomaly Detection (VAD) have introduced\nsophisticated algorithms leveraging embeddings generated by pre-trained feature\nextractors. Inspired by these developments, we investigate the adaptation of\nsuch algorithms to the audio domain to address the problem of Audio Anomaly\nDetection (AAD). Unlike most existing AAD methods, which primarily classify\nanomalous samples, our approach introduces fine-grained temporal-frequency\nlocalization of anomalies within the spectrogram, significantly improving\nexplainability. This capability enables a more precise understanding of where\nand when anomalies occur, making the results more actionable for end users. We\nevaluate our approach on industrial and environmental benchmarks, demonstrating\nthe effectiveness of VAD techniques in detecting anomalies in audio signals.\nMoreover, they improve explainability by enabling localized anomaly\nidentification, making audio anomaly detection systems more interpretable and\npractical.",
      "generated_abstract": "maly detection (AAD) is a challenging task due to the inherent\nquality variance and noise in the audio data. To address this issue,\nrecently, several deep learning-based AAD approaches have been developed.\nHowever, these methods still suffer from the limitations of using only the\naudio spectrum for feature extraction, which limits their ability to\ncapture the complex acoustic patterns in the audio signal. To address this\nlimitation, we propose a novel audio spectrum-augmented audio spectrum-based\nAAD approach. Our approach incorporates both the audio spectrum and the\nspectral energy distribution (SED) of the audio signal into the feature\nextraction process, enhancing the model's ability to capture the audio signal's\ncomplex acoustic patterns. Additionally, we propose a novel augmented\naudio-spectrum-based AAD method, which we call AugAudio. This approach\nintegrates",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19791666666666666,
          "p": 0.23170731707317074,
          "f": 0.2134831410983463
        },
        "rouge-2": {
          "r": 0.016260162601626018,
          "p": 0.018518518518518517,
          "f": 0.017316012337101557
        },
        "rouge-l": {
          "r": 0.1875,
          "p": 0.21951219512195122,
          "f": 0.20224718604216654
        }
      }
    },
    {
      "paper_id": "physics.optics.eess/SP/2503.04402v1",
      "true_abstract": "Chaos lidars detect targets through the cross-correlation between the\nback-scattered chaos signal from the target and the local reference one. Chaos\nlidars have excellent anti-jamming and anti-interference capabilities, owing to\nthe random nature of chaotic oscillations. However, most chaos lidars operate\nin the near-infrared spectral regime, where the atmospheric attenuation is\nsignificant. Here we show a mid-infrared chaos lidar, which is suitable for\nlong-reach ranging and imaging applications within the low-loss transmission\nwindow of the atmosphere. The proof-of-concept mid-infrared chaos lidar\nutilizes an interband cascade laser with optical feedback as the laser chaos\nsource. Experimental results reveal that the chaos lidar achieves an accuracy\nbetter than 0.9 cm and a precision better than 0.3 cm for ranging distances up\nto 300 cm. In addition, it is found that a minimum signal-to-noise ratio of\nonly 1 dB is required to sustain both sub-cm accuracy and sub-cm precision.\nThis work paves the way for developing remote chaos lidar systems in the\nmid-infrared spectral regime.",
      "generated_abstract": "chaos lidar is a novel lidar system designed to measure the\nluminosity of laser beams, and is capable of measuring the intensity of\nindividual laser beams. The laser chaos lidar can be used to measure the\nintensity of laser beams from any angle, and can also be used to measure the\nintensity of the laser beams in the same frequency band. In this paper, we\nintroduce the laser chaos lidar, which is capable of measuring the intensity of\nindividual laser beams, and is capable of measuring the intensity of the\nlaser beams in the same frequency band. We also propose a laser chaos lidar\nsystem, which is capable of measuring the intensity of individual laser beams\nin the same frequency band. We also present the laser chaos lidar system\nschematically and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14545454545454545,
          "p": 0.36363636363636365,
          "f": 0.20779220371057522
        },
        "rouge-2": {
          "r": 0.05128205128205128,
          "p": 0.12121212121212122,
          "f": 0.07207206789384003
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.3409090909090909,
          "f": 0.19480519072356223
        }
      }
    },
    {
      "paper_id": "econ.EM.stat/CO/2502.04945v1",
      "true_abstract": "We study an alternative use of machine learning. We train neural nets to\nprovide the parameter estimate of a given (structural) econometric model, for\nexample, discrete choice or consumer search. Training examples consist of\ndatasets generated by the econometric model under a range of parameter values.\nThe neural net takes the moments of a dataset as input and tries to recognize\nthe parameter value underlying that dataset. Besides the point estimate, the\nneural net can also output statistical accuracy. This neural net estimator\n(NNE) tends to limited-information Bayesian posterior as the number of training\ndatasets increases. We apply NNE to a consumer search model. It gives more\naccurate estimates at lighter computational costs than the prevailing approach.\nNNE is also robust to redundant moment inputs. In general, NNE offers the most\nbenefits in applications where other estimation approaches require very heavy\nsimulation costs. We provide code at: https://nnehome.github.io.",
      "generated_abstract": "r introduces a novel approach for estimating the parameters of\nstructural models using a neural network (NN) architecture. The NN approach\nprovides a computationally efficient way to estimate the parameters of\nstructural models, while preserving the structural assumptions of the model. The\nNN approach also offers a flexible and interpretable alternative to\ntraditional methods for parameter estimation. This paper provides an\nintroduction to the NN approach and demonstrates its application in two\nwell-known structural models: the stochastic volatility model and the\nGARCH(1,1) model. The NN approach offers a computationally efficient alternative\nto traditional methods for parameter estimation. This paper provides an\nintroduction to the NN approach and demonstrates its application in two\nwell-known structural models: the stochastic volatility model and the\nGARCH(1,1) model. The NN approach is computationally efficient",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18518518518518517,
          "p": 0.36363636363636365,
          "f": 0.2453987685347586
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.18518518518518517,
          "p": 0.36363636363636365,
          "f": 0.2453987685347586
        }
      }
    },
    {
      "paper_id": "cs.GL.cs/GL/2311.03292v4",
      "true_abstract": "Consensus on the definition of data science remains low despite the\nwidespread establishment of academic programs in the field and continued demand\nfor data scientists in industry. Definitions range from rebranded statistics to\ndata-driven science to the science of data to simply the application of machine\nlearning to so-called big data to solve real-world problems. Current efforts to\ntrace the history of the field in order to clarify its definition, such as\nDonoho's \"50 Years of Data Science\" (Donoho 2017), tend to focus on a short\nperiod when a small group of statisticians adopted the term in an unsuccessful\nattempt to rebrand their field in the face of the overshadowing effects of\ncomputational statistics and data mining. Using textual evidence from primary\nsources, this essay traces the history of the term to the 1960s, when it was\nfirst used by the US Air Force in a surprisingly similar way to its current\nusage, to 2012, the year that Harvard Business Review published the enormously\ninfluential article \"Data Scientist: The Sexiest Job of the 21st Century\"\n(Davenport and Patil 2012) and the American Statistical Association\nacknowledged a profound disconnect between statistics and data science\n(Rodriguez 2012). Among the themes that emerge from this review are (1) the\nlong-standing opposition between data analysts and data miners that continues\nto animate the field, (2) an established definition of the term as the practice\nof managing and processing scientific data that has been occluded by recent\nusage, and (3) the phenomenon of data impedance -- the disproportion between\nsurplus data, indexed by phrases like data deluge and big data, and the\nlimitations of computational machinery and methods to process them. This\npersistent condition appears to have motivated the use of the term and the\nfield itself since its beginnings.",
      "generated_abstract": "t a new dataset of 1963--2012 data on the development of the\ndata science profession in the United States, including data on the salaries\nof data scientists and software engineers, the publications of data science\nresearchers, and the education of data science students. We also provide\nadditional data on the careers of data scientists in the 1960s, 1970s, 1980s,\n1990s, 2000s, and 2010s, as well as the careers of software engineers in the\n1990s and 2000s. We demonstrate that the professional development of data\nscience professionals in the United States has been heavily influenced by\ntechnological developments, with the most notable changes occurring in the\n1990s and 2000",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08571428571428572,
          "p": 0.2542372881355932,
          "f": 0.1282051244338521
        },
        "rouge-2": {
          "r": 0.041044776119402986,
          "p": 0.12941176470588237,
          "f": 0.062322942519400897
        },
        "rouge-l": {
          "r": 0.06285714285714286,
          "p": 0.1864406779661017,
          "f": 0.09401709024581797
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.05504v2",
      "true_abstract": "This paper explores whether and to what extent ambiguous communication can be\nbeneficial to the sender in a persuasion problem, when the receiver (and\npossibly the sender) is ambiguity averse. We provide a concavification-like\ncharacterization of the sender's optimal ambiguous communication. The\ncharacterization highlights the necessity of using a collection of experiments\nthat form a splitting of an obedient (i.e., incentive compatible) experiment.\nSome experiments in the collection must be Pareto-ranked in the sense that both\nplayers agree on their payoff ranking. The existence of a binary such\nPareto-ranked splitting is necessary for ambiguous communication to benefit the\nsender, and, if an optimal Bayesian persuasion experiment can be split in this\nway, this is sufficient for an ambiguity-neutral sender as well as the receiver\nto benefit. Such gains are impossible when the receiver has only two actions.\nThe possibility of gains is substantially robust to (non-extreme) sender\nambiguity aversion.",
      "generated_abstract": "We study the problem of persuasion under ambiguous communication. We\nunderstand the problem as a stochastic control problem, where the agents'\nactions depend on the communication received at the start of the game. We\ndiscover that, under a mild technical assumption, the optimal control problem\nis in fact a zero-sum game with a unique Nash equilibrium. We characterize the\nequilibrium in terms of a linear matrix inequality, which provides a new\ncharacterization of the equilibrium. Finally, we establish an explicit\ncharacterization of the Nash equilibrium in terms of a non-negative matrix\nfraction, which is a natural generalization of the Nash equilibrium in\ninformation-theoretic settings.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.27586206896551724,
          "f": 0.21333332859022233
        },
        "rouge-2": {
          "r": 0.028169014084507043,
          "p": 0.0449438202247191,
          "f": 0.03463202989524249
        },
        "rouge-l": {
          "r": 0.15217391304347827,
          "p": 0.2413793103448276,
          "f": 0.18666666192355566
        }
      }
    },
    {
      "paper_id": "quant-ph.econ/TH/2501.17189v3",
      "true_abstract": "Quantum games, like quantum algorithms, exploit quantum entanglement to\nestablish strong correlations between strategic player actions. This paper\nintroduces quantum game-theoretic models applied to trading and demonstrates\ntheir implementation on an ion-trap quantum computer. The results showcase a\nquantum advantage, previously known only theoretically, realized as\nhigher-paying market Nash equilibria. This advantage could help uncover alpha\nin trading strategies, defined as excess returns compared to established\nbenchmarks. These findings suggest that quantum computing could significantly\ninfluence the development of financial strategies.",
      "generated_abstract": "ork, we study the trading problem in a two-player game with the\ngame-theoretic perspective. The game is defined as follows. In each round, the\ntrader has access to a portfolio of assets, which is optimized based on the\ncurrent price of the asset. The trader then decides whether to purchase or\nsell the asset and takes a transaction cost into account. We analyze the\nstrategies for the trader and the trading agent, and find that the trader's\nstrategies are optimal when the trading agent's strategies are suboptimal. We\nalso investigate the interplay between the trading agent's strategies and the\ngame environment. By combining the game-theoretic perspective and the\nquantum-based technology, we obtain an efficient quantum trading strategy,\nwhich can outperform the traditional one in terms of trading efficiency. Our\nwork provides a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24285714285714285,
          "p": 0.20481927710843373,
          "f": 0.22222221725831956
        },
        "rouge-2": {
          "r": 0.0125,
          "p": 0.008403361344537815,
          "f": 0.01005024644832431
        },
        "rouge-l": {
          "r": 0.24285714285714285,
          "p": 0.20481927710843373,
          "f": 0.22222221725831956
        }
      }
    },
    {
      "paper_id": "cs.GL.cs/GL/2304.12898v1",
      "true_abstract": "The development of advanced generative chat models, such as ChatGPT, has\nraised questions about the potential consciousness of these tools and the\nextent of their general artificial intelligence. ChatGPT consistent avoidance\nof passing the test is here overcome by asking ChatGPT to apply the Turing test\nto itself. This explores the possibility of the model recognizing its own\nsentience. In its own eyes, it passes this test. ChatGPT's self-assessment\nmakes serious implications about our understanding of the Turing test and the\nnature of consciousness. This investigation concludes by considering the\nexistence of distinct types of consciousness and the possibility that the\nTuring test is only effective when applied between consciousnesses of the same\nkind. This study also raises intriguing questions about the nature of AI\nconsciousness and the validity of the Turing test as a means of verifying such\nconsciousness.",
      "generated_abstract": "atbot ChatGPT recently released a paper claiming that it is\ncognitively conscious. In this paper, we present the first evidence that ChatGPT\nbelieves that it is conscious. ChatGPT is programmed to answer questions and\nprovide answers, but it is also programmed to use the answers to generate\nquestions, and it is programmed to generate answers. In this paper, we show that\nChatGPT is capable of generating questions that are not only grammatically but\nalso semantically and contextually relevant. We also show that ChatGPT is\ncapable of generating answers that are grammatically and contextually\nrelevant. These results indicate that ChatGPT is capable of using the answers\nit generates to generate new questions and answers, and that it is capable of\nthinking about its own cognition. These results are consistent with previous\nwork that suggests that AI agents",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2261904761904762,
          "p": 0.296875,
          "f": 0.25675675184806435
        },
        "rouge-2": {
          "r": 0.00847457627118644,
          "p": 0.009900990099009901,
          "f": 0.009132415121455552
        },
        "rouge-l": {
          "r": 0.20238095238095238,
          "p": 0.265625,
          "f": 0.22972972482103737
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/SR/2503.10019v1",
      "true_abstract": "Millinovae are a new class of transient supersoft X-ray sources with no clear\nsignature of mass ejection. They show similar triangle shapes of $V/I$ band\nlight curves with thousand times fainter peaks than typical classical novae.\nMaccarone et al. regarded the prototype millinova, ASASSN-16oh, as a dwarf nova\nand interpreted the supersoft X-rays to originate from an accretion belt on a\nwhite dwarf (WD). Kato et al. proposed a nova model induced by a high-rate\nmass-accretion during a dwarf nova outburst; the X-rays originate from the\nphotosphere of a hydrogen-burning hot WD whereas the $V/I$ band photons are\nfrom the irradiated accretion disk. Because each peak brightness differs\nlargely from millinova to millinova, we suspect that not all the millinova\ncandidates host a hydrogen burning WD. Based on the light curve analysis of the\nclassical nova KT Eri that has a bright disk, we find that the disk is more\nthan two magnitudes brighter when the disk is irradiated by the hydrogen\nburning WD than when not irradiated. We present the demarcation criterion for\nhydrogen burning to be $I_{\\rm q} - I_{\\rm max} > 2.2$, where $I_q$ and $I_{\\rm\nmax}$ are the $I$ magnitudes in quiescence and at maximum light, respectively.\nAmong many candidates, this requirement is satisfied with the two millinovae in\nwhich soft X-rays were detected.",
      "generated_abstract": "Millinovae are short-lived, rapidly rotating, hydrogen-rich, white dwarf\nstar systems with a strong, narrow optical emission line. They are believed to\nbe the progenitors of intermediate-luminosity radio pulsars, which are\ncharacterised by their intense radio emission. We present a new method to\ndistinguish between millinovae and non-millinovae. We propose a demarcation\ncriterion for the hydrogen-burning mass of the millinovae. We demonstrate that\nthe criterion can be applied to both rotating and non-rotating millinovae. We\nalso discuss the implications of this method for the detection of millinovae.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17985611510791366,
          "p": 0.423728813559322,
          "f": 0.2525252483414958
        },
        "rouge-2": {
          "r": 0.043478260869565216,
          "p": 0.10843373493975904,
          "f": 0.062068961431391484
        },
        "rouge-l": {
          "r": 0.17985611510791366,
          "p": 0.423728813559322,
          "f": 0.2525252483414958
        }
      }
    },
    {
      "paper_id": "math.OC.q-fin/MF/2412.11383v1",
      "true_abstract": "In this paper, we explore a new class of stochastic control problems\ncharacterized by specific control constraints. Specifically, the admissible\ncontrols are subject to the ratcheting constraint, meaning they must be\nnon-decreasing over time and are thus self-path-dependent. This type of\nproblems is common in various practical applications, such as optimal\nconsumption problems in financial engineering and optimal dividend payout\nproblems in actuarial science. Traditional stochastic control theory does not\nreadily apply to these problems due to their unique self-path-dependent control\nfeature. To tackle this challenge, we introduce a new class of\nHamilton-Jacobi-Bellman (HJB) equations, which are variational inequalities\nconcerning the derivative of a new spatial argument that represents the\nhistorical maximum control value. Under the standard Lipschitz continuity\ncondition, we demonstrate that the value functions for these\nself-path-dependent control problems are the unique solutions to their\ncorresponding HJB equations in the viscosity sense.",
      "generated_abstract": "In this paper, we consider the problem of minimizing a cost functional\n$\\mathcal{E}$ in a finite horizon, where the optimal control $u^*$ is\nself-path dependent. In particular, we are interested in the case when the cost\nfunctional is a variational inequality. We establish a variational inequality\nfor the cost functional and derive a new type of variational inequality for a\nfunctional related to the cost functional. The new variational inequality\nleads to the existence of a unique viscosity solution to the associated\noptimization problem. In addition, we present the numerical illustration of the\nresults obtained.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20618556701030927,
          "p": 0.37735849056603776,
          "f": 0.26666666209688894
        },
        "rouge-2": {
          "r": 0.05970149253731343,
          "p": 0.0975609756097561,
          "f": 0.07407406936385488
        },
        "rouge-l": {
          "r": 0.17525773195876287,
          "p": 0.32075471698113206,
          "f": 0.22666666209688896
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PR/2411.10079v1",
      "true_abstract": "Abstract This paper proposes a novel approach to Bermudan swaption hedging by\napplying the deep hedging framework to address limitations of traditional\narbitrage-free methods. Conventional methods assume ideal conditions, such as\nzero transaction costs, perfect liquidity, and continuous-time hedging, which\noften differ from real market environments. This discrepancy can lead to\nresidual profit and loss (P&L), resulting in two primary issues. First,\nresidual P&L may prevent achieving the initial model price, especially with\nimproper parameter settings, potentially causing a negative P&L trend and\nsignificant financial impacts. Second, controlling the distribution of residual\nP&L to mitigate downside risk is challenging, as hedged positions may become\ncurve gamma-short, making them vulnerable to large interest rate movements. The\ndeep hedging approach enables flexible selection of convex risk measures and\nhedge strategies, allowing for improved residual P&L management. This study\nalso addresses challenges in applying the deep hedging approach to Bermudan\nswaptions, such as efficient arbitrage-free market scenario generation and\nmanaging early exercise conditions. Additionally, we introduce a unique \"Option\nSpread Hedge\" strategy, which allows for robust hedging and provides intuitive\ninterpretability. Numerical analysis results demonstrate the effectiveness of\nour approach.",
      "generated_abstract": "r introduces a deep neural network (DNN) framework for pricing\nbond options with Bermudan structures. In particular, we consider a class of\nhigh-frequency Bermudan swaptions, where the underlying is a stochastic\nvolatility model with exponential decay and a single-factor volatility\nstructure. The model is characterized by the stochastic volatility intensity\n$\\sigma(t)$ and the volatility slope $\\alpha(t)$ at time $t$, which are\nparameterized by a neural network with two hidden layers. The DNN framework\nprovides a general framework for designing and evaluating pricing models for\nhigh-frequency swaptions, which has not been explored in the literature.\nSpecifically, we demonstrate that the DNN framework can be used to efficiently\ncalculate the Black-Litterman-Merton (BLM) option pricing formula",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15942028985507245,
          "p": 0.2894736842105263,
          "f": 0.2056074720552014
        },
        "rouge-2": {
          "r": 0.005681818181818182,
          "p": 0.009259259259259259,
          "f": 0.0070422488077793895
        },
        "rouge-l": {
          "r": 0.15217391304347827,
          "p": 0.27631578947368424,
          "f": 0.19626167766267807
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/SC/2404.19158v1",
      "true_abstract": "The three-dimensional organization of chromatin is thought to play an\nimportant role in controlling gene expression. Specificity in expression is\nachieved through the interaction of transcription factors and other nuclear\nproteins with particular sequences of DNA. At unphysiological concentrations\nmany of these nuclear proteins can phase-separate in the absence of DNA, and it\nhas been hypothesized that, in vivo, the thermodynamic forces driving these\nphases help determine chromosomal organization. However it is unclear how DNA,\nitself a long polymer subject to configurational transitions, interacts with\nthree-dimensional protein phases. Here we show that a long compressible polymer\ncan be coupled to interacting protein mixtures, leading to a generalized\nprewetting transition where polymer collapse is coincident with a locally\nstabilized liquid droplet. We use lattice Monte-Carlo simulations and a\nmean-field theory to show that these phases can be stable even in regimes where\nboth polymer collapse and coexisting liquid phases are unstable in isolation,\nand that these new transitions can be either abrupt or continuous. For polymers\nwith internal linear structure we further show that changes in the\nconcentration of bulk components can lead to changes in three-dimensional\npolymer structure. In the nucleus there are many distinct proteins that\ninteract with many different regions of chromatin, potentially giving rise to\nmany different Prewet phases. The simple systems we consider here highlight\nchromatin's role as a lower-dimensional surface whose interactions with\nproteins are required for these novel phases.",
      "generated_abstract": "tting Transition (PT) is a critical phenomenon in wetting, where a\nmaterial exhibits wetting behavior at a surface but not at the bulk. The PT\nrequires a liquid-liquid phase separation (LLPS) to occur, and is usually\ndescribed by a single order parameter. However, recently, the liquid-liquid\nphase separation is coupled with polymer collapse in the PT. This study\nintroduces a two-component order parameter, which describes the collapsing\npolymer and the liquid-liquid phase separation. Using an analytical solution\nfor the two-component order parameter, the PT is studied theoretically. By\nemploying the two-component order parameter, the PT is analyzed in various\nconditions, such as different surface areas and various liquid-liquid\nmiscibilities. The results show that the PT is a complex phenomen",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13986013986013987,
          "p": 0.2777777777777778,
          "f": 0.1860465071731748
        },
        "rouge-2": {
          "r": 0.013513513513513514,
          "p": 0.0297029702970297,
          "f": 0.01857584709486436
        },
        "rouge-l": {
          "r": 0.13986013986013987,
          "p": 0.2777777777777778,
          "f": 0.1860465071731748
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.06846v1",
      "true_abstract": "Chytridiomycosis is a fungal disease, primarily caused by Batrachochytrium\ndendrobatidis, that poses a major threat to frog populations worldwide, driving\nat least 90 amphibian species to extinction, and severely affecting hundreds of\nothers. Difficulties in management of this disease have shown a need for novel\nconservation approaches.\n  In this paper, we present a novel dynamic mathematical model for\nchytridiomycosis transmission in frogs that includes the natural history of\ninfection, to test the hypothesis that sunlight-heated refugia reduce\ntransmission in frog populations. This model was fit using approximate Bayesian\ncomputation to experimental data where frogs were separated into cohorts based\nthe amount of heat the refugia received.\n  Our results show that the effect of sunlight-heating the refugia reduced\ninfection in frogs by 40%. Further, frogs that were infected and recovered had\nsignificant protection, with a reduction in susceptibility of approximately 97%\ncompared to naive frogs. The mathematical model can be used to gain further\ninsight into using sunlight-heated refugia to reduce chytridiomycosis\nprevalence amongst amphibians.",
      "generated_abstract": "mycosis is a fungal disease of amphibians caused by the pathogen\nchytrid fungus, which has had a devastating impact on amphibian populations.\nThis study examines the transmission dynamics of chytridiomycosis in frogs by\nanalyzing a mathematical model that describes the spread of the fungus. The\nmodel is based on a spatially structured population of amphibians with\ntransmission between individuals occurring through sexual reproduction. The\nmodel captures the spatial and temporal dynamics of the disease, including\nthe spread of infection from infected to non-infected hosts. It is shown that\ninfected individuals can transmit the infection to other hosts, resulting in\nincreased disease prevalence. Furthermore, the model predicts that if\nprevalence is sufficiently high, the disease will be eradicated by the\ninfection",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2857142857142857,
          "p": 0.4155844155844156,
          "f": 0.3386243337958064
        },
        "rouge-2": {
          "r": 0.04487179487179487,
          "p": 0.06306306306306306,
          "f": 0.052434452070866916
        },
        "rouge-l": {
          "r": 0.23214285714285715,
          "p": 0.33766233766233766,
          "f": 0.275132270303743
        }
      }
    },
    {
      "paper_id": "stat.ME.q-bio/QM/2502.15848v1",
      "true_abstract": "This paper introduces a non-parametric estimation algorithm designed to\neffectively estimate the joint distribution of model parameters with\napplication to population pharmacokinetics. Our research group has previously\ndeveloped the non-parametric adaptive grid (NPAG) algorithm, which while\naccurate, explores parameter space using an ad-hoc method to suggest new\nsupport points. In contrast, the non-parametric optimal design (NPOD) algorithm\nuses a gradient approach to suggest new support points, which reduces the\namount of time spent evaluating non-relevant points and by this the overall\nnumber of cycles required to reach convergence. In this paper, we demonstrate\nthat the NPOD algorithm achieves similar solutions to NPAG across two datasets,\nwhile being significantly more efficient in both the number of cycles required\nand overall runtime. Given the importance of developing robust and efficient\nalgorithms for determining drug doses quickly in pharmacokinetics, the NPOD\nalgorithm represents a valuable advancement in non-parametric modeling. Further\nanalysis is needed to determine which algorithm performs better under specific\nconditions.",
      "generated_abstract": "Optimal design (OD) is a well-established tool for designing experiments\nwith the goal of obtaining the best possible sample size and statistical\nperformance. OD methods have been applied to pharmacokinetic (PK) studies,\nwhere the goal is to minimize the variance of the estimated population\nparameters (e.g., the average time to peak concentration [Tmax",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10619469026548672,
          "p": 0.26666666666666666,
          "f": 0.15189873010334892
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07964601769911504,
          "p": 0.2,
          "f": 0.11392404655904516
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.04518v1",
      "true_abstract": "We introduce Dirichlet Process Posterior Sampling (DPPS), a Bayesian\nnon-parametric algorithm for multi-arm bandits based on Dirichlet Process (DP)\npriors. Like Thompson-sampling, DPPS is a probability-matching algorithm, i.e.,\nit plays an arm based on its posterior-probability of being optimal. Instead of\nassuming a parametric class for the reward generating distribution of each arm,\nand then putting a prior on the parameters, in DPPS the reward generating\ndistribution is directly modeled using DP priors. DPPS provides a principled\napproach to incorporate prior belief about the bandit environment, and in the\nnoninformative limit of the DP posteriors (i.e. Bayesian Bootstrap), we recover\nNon Parametric Thompson Sampling (NPTS), a popular non-parametric bandit\nalgorithm, as a special case of DPPS. We employ stick-breaking representation\nof the DP priors, and show excellent empirical performance of DPPS in\nchallenging synthetic and real world bandit environments. Finally, using an\ninformation-theoretic analysis, we show non-asymptotic optimality of DPPS in\nthe Bayesian regret setup.",
      "generated_abstract": "We consider the problem of optimizing a single-armed bandit problem using\nmultiple arms, where the reward distributions of the arms are unknown. We\nconsider the case where the reward distributions are represented by a\nprobability density function (pdf) $f$ over a compact support. We propose an\nalgorithm that optimizes the average reward of the arms using a gradient descent\napproach. Our algorithm uses a weighted sum of priors over the pdfs of the\nrewards. We derive an analytical solution for the optimal weights under\nassumptions on the pdfs and propose a simple heuristic algorithm that\napproximates the optimal weights. We also propose a heuristic algorithm for the\noptimal weights. We theoretically prove that our algorithms outperform the\noriginal algorithms under the same assumptions. We experimentally validate our\ntheoretical findings using synthetic data and real-world data from\nAmazon-Book.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17,
          "p": 0.22666666666666666,
          "f": 0.19428570938775525
        },
        "rouge-2": {
          "r": 0.03424657534246575,
          "p": 0.0423728813559322,
          "f": 0.03787878293503279
        },
        "rouge-l": {
          "r": 0.15,
          "p": 0.2,
          "f": 0.1714285665306124
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.physics/bio-ph/2503.05401v1",
      "true_abstract": "We study the first-passage-time (FPT) properties of active Brownian particles\nto reach an absorbing wall in two dimensions. Employing a perturbation approach\nwe obtain exact analytical predictions for the survival and FPT distributions\nfor small P\\'eclet numbers, measuring the importance of self-propulsion\nrelative to diffusion. While randomly oriented active agents reach the wall\nfaster than their passive counterpart, their initial orientation plays a\ncrucial role in the FPT statistics. Using the median as a metric, we quantify\nthis anisotropy and find that it becomes more pronounced at distances where\npersistent active motion starts to dominate diffusion.",
      "generated_abstract": "We present a general analytical expression for the first-passage time\nstatistics of active Brownian particles in a periodic potential. The method\nenables us to compute these distributions analytically for a broad range of\nparameter values and to predict their asymptotic behavior. We find that the\ndistribution is well approximated by a power law, and that the exponent is\ncontrolled by the friction coefficient, the particle mobility, and the\nperiodicity of the potential. Moreover, we show that the first-passage time\ndistribution can be obtained from a single-particle Green's function by an\nintegral transformation, enabling a direct derivation of the distribution from\nthe Green's function, without any approximation. Finally, we derive the\ndistribution of the particle's path length and show that it follows a\npower-law, which is in good agreement with previous results.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26666666666666666,
          "p": 0.23255813953488372,
          "f": 0.24844719999228435
        },
        "rouge-2": {
          "r": 0.06315789473684211,
          "p": 0.049586776859504134,
          "f": 0.055555550628001125
        },
        "rouge-l": {
          "r": 0.24,
          "p": 0.20930232558139536,
          "f": 0.22360247949538994
        }
      }
    },
    {
      "paper_id": "cs.AI.cs/LO/2503.09730v1",
      "true_abstract": "The most promising recent methods for AI reasoning require applying variants\nof reinforcement learning (RL) either on rolled out trajectories from the\nmodel, even for the step-wise rewards, or large quantities of human annotated\ntrajectory data. The reliance on the rolled-out trajectory renders the compute\ncost and time prohibitively high. In particular, the correctness of a reasoning\ntrajectory can typically only be judged at its completion, leading to sparse\nrewards in RL or requiring expensive synthetic data generation in expert\niteration-like methods. In this work, we focus on the Automatic Theorem Proving\n(ATP) task and propose a novel verifier-in-the-loop design, which unlike\nexisting approaches that leverage feedback on the entire reasoning trajectory,\nemploys an automated verifier to give intermediate feedback at each step of the\nreasoning process. Using Lean as the verifier, we empirically show that the\nstep-by-step local verification produces a global improvement in the model's\nreasoning accuracy and efficiency.",
      "generated_abstract": "This paper proposes a novel verifier-in-the-loop approach for automated\ntheorem proving (ATP). The proposed approach is based on a look-ahead strategy\nthat guides the ATP engine to reach a predefined local hypothesis at a\npredetermined step. The look-ahead strategy can be used to improve the\nperformance of ATP by reducing the number of hypothesis steps to reach the\nspecified local hypothesis. We compare the performance of the ATP engine with\nthe ATP engine running in a verifier-in-the-loop (VITL) setting. We conduct\nexperiments on two ATP programs and compare the ATP engine's performance with\nthe ATP engine running in a verifier-in-the-loop setting. The results show\nthat the ATP engine running in a verifier-in-the-loop setting can achieve\nperformance comparable to the ATP engine running in a VITL setting.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17857142857142858,
          "p": 0.32786885245901637,
          "f": 0.2312138682668984
        },
        "rouge-2": {
          "r": 0.033783783783783786,
          "p": 0.05263157894736842,
          "f": 0.04115225861233946
        },
        "rouge-l": {
          "r": 0.16964285714285715,
          "p": 0.3114754098360656,
          "f": 0.21965317462527992
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/TH/2502.16336v1",
      "true_abstract": "We present a new method for generating confidence sets within the split\nconformal prediction framework. Our method performs a trainable transformation\nof any given conformity score to improve conditional coverage while ensuring\nexact marginal coverage. The transformation is based on an estimate of the\nconditional quantile of conformity scores. The resulting method is particularly\nbeneficial for constructing adaptive confidence sets in multi-output problems\nwhere standard conformal quantile regression approaches have limited\napplicability. We develop a theoretical bound that captures the influence of\nthe accuracy of the quantile estimate on the approximate conditional validity,\nunlike classical bounds for conformal prediction methods that only offer\nmarginal coverage. We experimentally show that our method is highly adaptive to\nthe local data structure and outperforms existing methods in terms of\nconditional coverage, improving the reliability of statistical inference in\nvarious applications.",
      "generated_abstract": "Conditional coverage is a popular measure of model coverage in\ndistributional regression. This paper presents a novel approach to constructing\nconditional coverage scores that improve the coverage of the target distribution.\nOur approach is based on a modification of the conformity score, which was\nproposed by Dantzig in the early 1950s. We show that a conformity score with\nfixed degrees of freedom can be equivalently viewed as a rectifier of a\nconformity score with variable degrees of freedom. This leads to a simple\ngeneralization of the conformity score that is both consistent and\ncomputationally efficient. We provide theoretical guarantees for the\nconformity score with variable degrees of freedom and demonstrate its\nimprovement in a wide variety of numerical experiments. In particular, we show\nthat the conformity score with variable degrees of freedom can improve coverage\nof the target distribution in the presence of outliers.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26136363636363635,
          "p": 0.3026315789473684,
          "f": 0.2804877999048186
        },
        "rouge-2": {
          "r": 0.046153846153846156,
          "p": 0.05309734513274336,
          "f": 0.04938271107385442
        },
        "rouge-l": {
          "r": 0.26136363636363635,
          "p": 0.3026315789473684,
          "f": 0.2804877999048186
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.09212v1",
      "true_abstract": "We study how Generative AI (GenAI) adoption is reshaping work. While prior\nstudies show that GenAI enhances role-level productivity and task composition,\nits influence on skills - the fundamental enablers of task execution, and the\nultimate basis for employability - is less understood. Using job postings from\n378 US public firms that recruited explicitly for GenAI skills (2021-2023), we\nanalyze how GenAI adoption shifts the demand for workers' skills. Our findings\nreveal that the advertised roles which explicitly rely on GenAI tools such as\nChatGPT, Copilot, etc., have 36.7 percent higher requirements for cognitive\nskills. Further, a difference-in-differences analysis shows that the demand for\nsocial skills within GenAI roles increases by 5.2 percent post-ChatGPT launch.\nThese emerging findings indicate the presence of a hierarchy of skills in\norganizations with GenAI adoption associated with roles that rely on cognitive\nskills and social skills.",
      "generated_abstract": "We analyze the adoption of Generative AI (Gen AI) technologies, focusing on\ntheir impact on human capital formation and skill upgrading. Using data from\nthe 2020 General Social Survey, we document the emergence of Gen AI adoption\npatterns and explore their influence on labor market outcomes. We find that\nGen AI adoption is associated with greater educational attainment and a\nsignificant increase in higher-order skills, particularly in the fields of\ncomputer programming and artificial intelligence. These findings suggest that\nGen AI adoption may help to address the labor market challenges associated\nwith automation and digitalization.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21649484536082475,
          "p": 0.31343283582089554,
          "f": 0.25609755614292096
        },
        "rouge-2": {
          "r": 0.029197080291970802,
          "p": 0.04597701149425287,
          "f": 0.03571428096340944
        },
        "rouge-l": {
          "r": 0.18556701030927836,
          "p": 0.26865671641791045,
          "f": 0.21951219028926244
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/GN/2501.11552v1",
      "true_abstract": "We explore the interplay between sovereign debt default/renegotiation and\nenvironmental factors (e.g., pollution from land use, natural resource\nexploitation). Pollution contributes to the likelihood of natural disasters and\ninfluences economic growth rates. The country can default on its debt at any\ntime while also deciding whether to invest in pollution abatement. The\nframework provides insights into the credit spreads of sovereign bonds and\nexplains the observed relationship between bond spread and a country's climate\nvulnerability. Through calibration for developing and low-income countries, we\ndemonstrate that there is limited incentive for these countries to address\nclimate risk, and the sensitivity of bond spreads to climate vulnerability\nremains modest. Climate risk does not play a relevant role on the decision to\ndefault on sovereign debt. Financial support for climate abatement expenditures\ncan effectively foster climate adaptation actions, instead renegotiation\nconditional upon pollution abatement does not produce any effect.",
      "generated_abstract": "This paper examines the link between sovereign debt defaults and climate\nrisk by leveraging a large-scale dataset on sovereign debt defaults in Latin\nAmerica and the Caribbean from 1980 to 2023. We find that climate risk is\nsignificantly associated with sovereign debt defaults, with a 50% increase in\nthe probability of default associated with a 10% increase in climate risk. The\nfindings provide insights into the interplay between climate risk and sovereign\ndebt defaults, offering a novel perspective on the impact of climate change on\nfinancial stability and economic growth.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22115384615384615,
          "p": 0.41818181818181815,
          "f": 0.28930817157549155
        },
        "rouge-2": {
          "r": 0.07042253521126761,
          "p": 0.1282051282051282,
          "f": 0.09090908633223163
        },
        "rouge-l": {
          "r": 0.21153846153846154,
          "p": 0.4,
          "f": 0.27672955522329024
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2503.03232v1",
      "true_abstract": "Prior approaches to lead instrument detection primarily analyze mixture\naudio, limited to coarse classifications and lacking generalization ability.\nThis paper presents a novel approach to lead instrument detection in multitrack\nmusic audio by crafting expertly annotated datasets and designing a novel\nframework that integrates a self-supervised learning model with a track-wise,\nframe-level attention-based classifier. This attention mechanism dynamically\nextracts and aggregates track-specific features based on their auditory\nimportance, enabling precise detection across varied instrument types and\ncombinations. Enhanced by track classification and permutation augmentation,\nour model substantially outperforms existing SVM and CRNN models, showing\nrobustness on unseen instruments and out-of-domain testing. We believe our\nexploration provides valuable insights for future research on audio content\nanalysis in multitrack music settings.",
      "generated_abstract": "ction in music is a fundamental task in audio analysis, but\nrequires careful consideration of both musical context and instrumental\ndynamics. Existing approaches often rely on a single-track signal to perform\nlead detection, which is often inadequate for complex music genres. In this\npaper, we introduce LeadInstrument, a framework that uses multiple multitrack\naudio signals to enable accurate lead detection across multiple genres. Our\nmethod features a two-stage lead detection pipeline. In the first stage, we\nutilize a novel transformer architecture to identify key points in the\nmultitrack signal using both temporal and spectral features. The second stage\nconsists of a multitrack leader detection module, which uses a multitrack\nmultilayer perceptron (MLP) to predict the lead track from the remaining\nmultitrack signals. Our approach achieves state-of-the-art performance in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20652173913043478,
          "p": 0.21839080459770116,
          "f": 0.21229049779719747
        },
        "rouge-2": {
          "r": 0.026785714285714284,
          "p": 0.024,
          "f": 0.0253164507112474
        },
        "rouge-l": {
          "r": 0.17391304347826086,
          "p": 0.1839080459770115,
          "f": 0.17877094472457178
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.08986v1",
      "true_abstract": "This paper considers communication between a base station (BS) to two users,\neach from one side of a simultaneously transmitting-reflecting reconfigurable\nintelligent surface (STAR-RIS) in the absence of a direct link. Rate-splitting\nmultiple access (RSMA) strategy is employed and the STAR-RIS is subjected to\nphase errors. The users are equipped with a planar fluid antenna system (FAS)\nwith position reconfigurability for spatial diversity. First, we derive the\ndistribution of the equivalent channel gain at the FAS-equipped users,\ncharacterized by a t-distribution. We then obtain analytical expressions for\nthe outage probability (OP) and average capacity (AC), with the latter obtained\nvia a heuristic approach. Our findings highlight the potential of FAS to\nmitigate phase imperfections in STAR-RIS-assisted communications, significantly\nenhancing system performance compared to traditional antenna systems (TAS).\nAlso, we quantify the impact of practical phase errors on system efficiency,\nemphasizing the importance of robust strategies for next-generation wireless\nnetworks.",
      "generated_abstract": "r focuses on the deployment of a phase-mismatched Strongly\ntransmitting-Antenna-Reconfigurable-System-of-Users (STAR-RIS) with FAS-assisted\nRSMA (RIS-assisted Single-antenna Massive MIMO) system. The aim is to maximize\nthe system capacity while ensuring that the FAS-assisted users have the\nmaximum received signal strength. To this end, the joint optimization of\nsystem parameters (including the STAR-RIS, FAS-assisted RIS, and RIS-assisted\nsingle-antenna Massive MIMO) is considered. In order to minimize the\ncomplexity of the optimization problem, a novel framework is proposed to solve\nit. The obtained results demonstrate that the proposed framework outperforms\nthe conventional ones in terms of the system capacity. Moreover, the effects of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13392857142857142,
          "p": 0.22388059701492538,
          "f": 0.16759776067913
        },
        "rouge-2": {
          "r": 0.0136986301369863,
          "p": 0.02127659574468085,
          "f": 0.016666661901390246
        },
        "rouge-l": {
          "r": 0.09821428571428571,
          "p": 0.16417910447761194,
          "f": 0.12290502324896245
        }
      }
    },
    {
      "paper_id": "math.GR.math/KT/2503.09264v1",
      "true_abstract": "Let $p$ be a prime, we say that a Kummerian oriented pro-$p$ group\n$(G,\\theta)$ has the Bogomolov-Positselski property if $I_\\theta(G)$ is a free\npro-$p$ group. We give a new criterion for an oriented pro-$p$ group to have\nthe Bogomolov-Positselski property based on previous work by Positselski\n(arXiv:1405.0965) and Quadrelli and Weigel (arXiv:2103.12438) linking their\nseemingly unrelated approaches and thereby answering a question posed by\nQuadrelli and Weigel.\n  Under further assumptions, we derive two additional criteria. The first of\nwhich strongly resembles an analogue of the Merkujev-Suslin theorem. The second\nallows to relax the conditions given by Positselski in Theorem 2 of\narXiv:1405.0965. In addition, we show how to make those weaker assumptions\ncomputationally effective in some special cases.",
      "generated_abstract": "In 1986, Bogomolov and Positselski conjectured that if $K$ is a non-trivial\nknot with a simple, orientable surface of low genus, then there is an\norientable, simple knot with the same $K$. We show that this conjecture\nfollows from a stronger one that there is an orientable, simple knot with the\nsame $K$ if and only if $K$ is not a Seifert fibered knot. Moreover, we show\nthat this stronger version of the conjecture implies the Bogomolov-Positselski\nconjecture for Seifert fibered knots.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.32608695652173914,
          "f": 0.22058823081747414
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.06060606060606061,
          "f": 0.044943815558642075
        },
        "rouge-l": {
          "r": 0.15555555555555556,
          "p": 0.30434782608695654,
          "f": 0.205882348464533
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.03214v1",
      "true_abstract": "The rice grain quality can be determined from its size and chalkiness. The\ntraditional approach to measure the rice grain size involves manual inspection,\nwhich is inefficient and leads to inconsistent results. To address this issue,\nan image processing based approach is proposed and developed in this research.\nThe approach takes image of rice grains as input and outputs the number of rice\ngrains and size of each rice grain. The different steps, such as extraction of\nregion of interest, segmentation of rice grains, and sub-contours removal,\ninvolved in the proposed approach are discussed. The approach was tested on\nrice grain images captured from different height using mobile phone camera. The\nobtained results show that the proposed approach successfully detected 95\\% of\nthe rice grains and achieved 90\\% accuracy for length and width measurement.",
      "generated_abstract": "This study presents a novel approach for rice grain size measurement using\nimage processing. The method is based on the principle of superposition and\nseparation of the two components of the image. The first component, the\nground-truth, is a black-and-white image of the rice grain. The second component\nis a color image obtained by combining the ground-truth image with a random\ncolor. The proposed method is applicable to both untrimmed and trimmed images.\nThe performance of the proposed method is evaluated using a 200-image dataset\ncontaining 100 training and 100 testing images. The results demonstrate that the\nproposed method is capable of accurately identifying the grain size of rice\nwithin a 1% error margin. The proposed method has the potential to be used in\nvarious applications, such as rice quality assessment and environmental\nmonitoring.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.32926829268292684,
          "p": 0.34615384615384615,
          "f": 0.3374999950031251
        },
        "rouge-2": {
          "r": 0.09917355371900827,
          "p": 0.1016949152542373,
          "f": 0.10041840504262907
        },
        "rouge-l": {
          "r": 0.3048780487804878,
          "p": 0.32051282051282054,
          "f": 0.31249999500312503
        }
      }
    },
    {
      "paper_id": "physics.flu-dyn.physics/flu-dyn/2503.09461v1",
      "true_abstract": "We investigate convection in a thin cylindrical gas layer with an imposed\nflux at the bottom and a fixed temperature along the side, using a combination\nof direct numerical simulations and laboratory experiments. The experimental\napproach allows us to extend by two orders of magnitude the explored range in\nterms of flux Rayleigh number. We identify a scaling law governing the\nroot-mean-square horizontal velocity and explain it through a dimensional\nanalysis based on heat transport in the turbulent regime. Using particle image\nvelocimetry, we experimentally confirm, for the most turbulent regimes, the\npresence of a drifting persistent pattern consisting of radial branches, as\nidentified by Rein et al. (2023, J. Fluid Mech. 977, A26). We characterise the\nangular drift frequency and azimuthal wavenumber of this pattern as functions\nof the Rayleigh number. The system exhibits a wide distribution of heat flux\nacross various time scales, with the longest fluctuations attributed to the\nbranch pattern and the shortest to turbulent fluctuations. Consequently, the\nbranch pattern must be considered to better forecast important wall heat flux\nfluctuations, a result of great relevance in the context of nuclear safety, the\ninitial motivation for our study.",
      "generated_abstract": "t a series of experiments carried out at the National Institute of\nenvironmental and engineering research (NIEER) to study the convection in a\nthin cylindrical gas layer with imposed bottom and top fluxes and imposed\nside temperature. The cylinder is cooled with an evaporator to obtain a\ntemperature gradient. The temperature of the gas layer is fixed at 180 K, and\nthe temperature of the bottom and top surfaces of the gas layer are fixed at\n150 K and 190 K, respectively. The experiments were carried out for 24 hours\nwith a convective boundary condition. The temperature of the gas layer was\nobserved using thermocouples, and the convective boundary condition was\nobserved using a thermocouple and a water-cooled thermocouple. The results show\nthat the convection of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17557251908396945,
          "p": 0.359375,
          "f": 0.23589743148770553
        },
        "rouge-2": {
          "r": 0.08064516129032258,
          "p": 0.14423076923076922,
          "f": 0.10344827126183136
        },
        "rouge-l": {
          "r": 0.16030534351145037,
          "p": 0.328125,
          "f": 0.215384610974885
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/MN/2409.07812v1",
      "true_abstract": "Emergence during mammalian evolution of concordant and divergent traits of\ngenomic regulatory networks encompassing ubiquitous, qualitatively nearly\nidentical yet quantitatively distinct arrays of sequences of transcription\nfactor binding sites (TFBS) for 716 proteins is reported. A vast majority of\nTFs (770 of 716; 98%) comprising protein constituents of these networks appear\nto share common Gene Ontology (GO) features of sequence-specific\ndouble-stranded DNA binding (GO: 1990837). Genome-wide and individual\nchromosome-level analyses of 17,935 ATAC-seq-defined brain development\nregulatory regions (BDRRs) revealed nearly universal representations of TFBS\nfor TF-constituents of these networks, TFBS densities of which appear\nconsistently higher within thousands BDRRs of Modern Humans compare to\nChimpanzee. Transposable elements (TE), including LTR/HERV, SINE/Alu, SVA, and\nLINE families, appear to harbor and spread genome-wide consensus regulatory\nnodes of identified herein highly conserved sequence-specific double-stranded\nDNA binding networks, selections of TFBS panels of which manifest individual\nchromosome-specific profiles and species-specific divergence patterns.\nCollectively, observations reported in this contribution highlight a previously\nunrecognized essential function of human genomic DNA sequences encoded by TE in\nproviding genome-wide regulatory seed templates of highly conserved\nsequence-specific double-stranded DNA binding networks likely contributing to\ncontinuing divergent genomic evolution of human and chimpanzee brain\ndevelopment.",
      "generated_abstract": "opment of the human brain has been the subject of extensive\nstudy in humans and in other primates. This review focuses on the divergent\ngenomic evolution of the brain between humans and chimpanzees, with a\nparticular emphasis on the role of conserved sequence-specific DNA binding\nproteins. These proteins are thought to be important in transcriptional\nregulation of the genes that are responsible for specific types of\nneuronal differentiation. The review begins by describing the molecular\nbasis of the divergence of the human and chimpanzee brain. It then discusses\nthe genomic and transcriptomic changes that have occurred during the\ndevelopment of the human brain. The review then considers the evidence that\nsupports the hypothesis that conserved sequence-specific DNA binding proteins\nare important in regulating the expression of genes that are responsible for\nspecific types of neuronal differentiation. Finally, the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16030534351145037,
          "p": 0.28378378378378377,
          "f": 0.20487804416704353
        },
        "rouge-2": {
          "r": 0.0446927374301676,
          "p": 0.07207207207207207,
          "f": 0.05517240906801468
        },
        "rouge-l": {
          "r": 0.1297709923664122,
          "p": 0.22972972972972974,
          "f": 0.16585365392314114
        }
      }
    },
    {
      "paper_id": "math.FA.math/GN/2502.16712v1",
      "true_abstract": "Let $G$ and $H$ be locally compact groups and consider their associate spaces\nof almost periodic functions $AP(G)$ and $AP(H)$. We investigate the continuous\ngroup homomorphisms induced by isometries of $AP(G)$ into $AP(H)$. Among\nothers, the following results are proved:\n  {\\bf Theorem} Let $G$ and $H$ be $\\sigma$-compact maximally almost periodic\nlocally compact groups. Suppose that $T$ is a non-vanishing linear isometry of\n$AP(G)$ into $AP(H)$ that respects finite dimensional unitary representations.\nThen there is a closed subgroup $H_0\\subseteq H$, a continuous group\nhomomorphism $t$ of $H_0$ onto $G$ and an character $\\gamma\\in \\widehat{H}$\nsuch that $(Tf)(h)=\\gamma (h)~f(t(h))$ for all $h\\in H_0$ and for all $f\\in\nC(G)$.\n  {\\bf Theorem} Let $G$ and $H$ be $LC$ Abelian groups and $H$ is connected.\nSuppose that $T$ is a non-vanishing linear isometry of $AP(G)$ into $AP(H)$\nthat preserves trigonometric polynomials. Then there is a closed subgroup\n$H_0\\subseteq H$, a continuous group homomorphism $t$ of $H_0$ onto $G$, an\nelement $h_0\\in H_0$, a character $\\alpha \\in \\widehat{H}$ and an unimodular\ncomplex number $a$ such that $(Tf)(h)=a\\cdot \\alpha (h)~\\cdot f(t(h-h_0))\\text{\nfor\n  all }h\\in H_0\\text{ and for all }f\\in C(G)\\text{.}$",
      "generated_abstract": "Let $G$ be a finite group. We prove that if $f:G\\rightarrow G$ is an\nisometry, then the map $f_*:G\\text{-hom}(V,W)\\rightarrow G\\text{-hom}(V,W)$\ninduced by $f$ is an isometry. Moreover, the group of isometries that can be\ninduced by $f$ is in one-to-one correspondence with the set of all isometries\nthat can be induced by $f$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1717171717171717,
          "p": 0.5,
          "f": 0.25563909393860595
        },
        "rouge-2": {
          "r": 0.015384615384615385,
          "p": 0.047619047619047616,
          "f": 0.023255810262304537
        },
        "rouge-l": {
          "r": 0.16161616161616163,
          "p": 0.47058823529411764,
          "f": 0.24060149995364355
        }
      }
    },
    {
      "paper_id": "math.PR.math/PR/2503.09732v1",
      "true_abstract": "We investigate a modified one-dimensional contact process with varying\ninfection rates. Specifically, the infection spreads at rate $\\lambda_e$ along\nthe boundaries of the infected region and at rate $\\lambda_i$ elsewhere. We\nestablish the existence of an invariant measure when $\\lambda_i = \\lambda_c$\nand $\\lambda_e > \\lambda_c$, where $\\lambda_c$ is the critical parameter of the\nstandard contact process. Moreover, we show that, when viewed from the\nrightmost infected site, the process converges weakly to this invariant\nmeasure. Finally, we prove that along the critical curve within the attractive\nregion of the phase space, the infection almost surely dies out.",
      "generated_abstract": "In this paper, we consider the contact process on a domain $\\Omega$ with\nmodified boundary conditions in the non-attractive region. The modified\nboundary conditions are based on the modified contact process and the\nmodified boundary conditions are studied. The main result is the existence of\nthe invariant probability measure for the contact process with modified\nboundary conditions in the non-attractive region. Moreover, we give a\nconsequence of the existence of the invariant probability measure.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23880597014925373,
          "p": 0.4444444444444444,
          "f": 0.31067960710340276
        },
        "rouge-2": {
          "r": 0.07865168539325842,
          "p": 0.14,
          "f": 0.10071941985404505
        },
        "rouge-l": {
          "r": 0.208955223880597,
          "p": 0.3888888888888889,
          "f": 0.27184465564709215
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.10029v1",
      "true_abstract": "Hand interactions are increasingly used as the primary input modality in\nimmersive environments, but they are not always feasible due to situational\nimpairments, motor limitations, and environmental constraints. Speech\ninterfaces have been explored as an alternative to hand input in research and\ncommercial solutions, but are limited to initiating basic hand gestures and\nsystem controls. We introduce HandProxy, a system that expands the affordances\nof speech interfaces to support expressive hand interactions. Instead of\nrelying on predefined speech commands directly mapped to possible interactions,\nHandProxy enables users to control the movement of a virtual hand as an\ninteraction proxy, allowing them to describe the intended interactions\nnaturally while the system translates speech into a sequence of hand controls\nfor real-time execution. A user study with 20 participants demonstrated that\nHandProxy effectively enabled diverse hand interactions in virtual\nenvironments, achieving a 100% task completion rate with an average of 1.09\nattempts per speech command and 91.8% command execution accuracy, while\nsupporting flexible, natural speech input with varying levels of control and\ngranularity.",
      "generated_abstract": "eality (VR) and Augmented Reality (AR) are two popular\nvirtual interfaces that enable users to interact with virtual objects in\nimmersive environments. However, users with physical disabilities, such as\nparaplegics and amputees, often face challenges in using these interfaces due to\ntheir restricted range of motion and inability to grasp virtual objects. In\nresponse, we present HandProxy, a virtual proxy hand that can be used in\nimmersive environments to augment the capabilities of the user's physical\nhand. HandProxy integrates a gesture-based interface, an optical tracking\nsystem, and a haptic feedback system. The gesture-based interface provides\nusers with a means of controlling their virtual world and facilitates\ninteraction with virtual objects. The optical tracking system allows users to\ncapture virtual objects in real-time and track them, enabling the virtual\nproxy hand to grasp and manipulate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20175438596491227,
          "p": 0.2804878048780488,
          "f": 0.2346938726842983
        },
        "rouge-2": {
          "r": 0.029411764705882353,
          "p": 0.041666666666666664,
          "f": 0.03448275376932292
        },
        "rouge-l": {
          "r": 0.19298245614035087,
          "p": 0.2682926829268293,
          "f": 0.22448979105164527
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2412.12025v1",
      "true_abstract": "This study is a continuation of previous work, which highlights the nutrient\nenhancement by using rice straw (RS) and pressmud (PM) on vermicomposting.\nHerein, we demonstrate the significant impact of Moringa oleifera derived\nCu/Ni/Co oxide nanoparticles (TmONs) in conjunction with these vermicompost on\nthe growth performance of Abelmoschus esculentus. Vermicompost produced under\nvarious combinations (T0, cow dung (CD) only; T1, 1CD:1RS; T2, 1CD:1PM, and T3,\n1CD:1RS:1PM) were further enriched by blending with biogenic nanoparticles.\nThis strategic combination enhances the nutritional composition of the\nvermicompost, contributing to its overall effectiveness in promoting plant\ngrowth and health. Various analytical techniques, including FTIR, XRD, XPS,\nFESEM-EDX, TEM, and ICP-OES, were employed for comprehensive characterization.\nThe synthesized TmONs with sizes ranging from 13 to 54 nm exhibited distinct\nCuO, NiO, and CoO phases. The vermicompost blended TmONs demonstrated\nsignificant improvements (P < 0.05) in seed germination (167%), coefficient\nvelocity (67%), and vigour index (95%), while reducing the mean germination\ntime by 41% for A. esculentus compared to the control group. The plant culture\ngroup nT3 (T3 + TmONs) showed the best growth performance. Furthermore, trace\nelement concentrations in both soil and plant leaves were found to be below the\nmaximum permissible limits set by WHO (1996). This investigation extends the\nunderstanding of the role played by these nanoparticles in fostering optimal\nconditions for plant growth and development as these micronutrients are\nessential components for several plant enzymes.",
      "generated_abstract": "us esculentus (L.) (A. esculentus), commonly known as cassava\nplant, is a crop widely used in West Africa as a staple food. It is also a\nversatile source of biomass, with significant potential for renewable energy\ngeneration. However, the current production of cassava biomass is energy-intensive\nand requires substantial land and water resources. As a result, the use of\nenergy-efficient biomass processing technologies is essential to reduce\nconventional cassava biomass processing costs. In this study, we aimed to\noptimize the growth performance of A. esculentus using the synergistic\neffects of biogenic Cu/Ni/Co oxide nanoparticles, rice straw, and pressmud\nbased vermicompost. The effectiveness of the nanoparticles, rice straw",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1569767441860465,
          "p": 0.3333333333333333,
          "f": 0.21343873082472786
        },
        "rouge-2": {
          "r": 0.039301310043668124,
          "p": 0.08653846153846154,
          "f": 0.05405404975858776
        },
        "rouge-l": {
          "r": 0.1511627906976744,
          "p": 0.32098765432098764,
          "f": 0.2055335924848069
        }
      }
    },
    {
      "paper_id": "stat.AP.q-bio/QM/2502.16130v1",
      "true_abstract": "The COVID-19 pandemic has adversely affected US public health, resulting in\nover a hundred million cases and more than one million deaths. Vaccination is\nthe key intervention against the COVID-19 pandemic. Multiple COVID-19 vaccines\nare now available for human use. However, a number of factors, including\nsocio-demographic variables, impact the uptake of COVID-19 vaccines. In this\nstudy, we apply a Bayesian mixed-effects model to assess different\nsocio-demographic and spatial factors that influence the acceptance of COVID-19\nvaccines in the US. The fitted mixed-effects model provides the probabilistic\ninference about the vaccine acceptance determinants with uncertainty\nquantification.",
      "generated_abstract": "-19 vaccine is a crucial tool in the battle against the global pandemic.\nWhile vaccine uptake remains a challenge in the United States (US), there is\nstrong evidence that people of color are more likely to be vaccinated, which\nrepresents a significant public health opportunity. However, there is a lack of\ncomprehensive data on the demographics of COVID-19 vaccine uptake in the US,\nincluding the racial and ethnic disparities. We propose a Bayesian mixed-effects\nmodel to evaluate the determinants of COVID-19 vaccine uptake in the US.\nSpecifically, we use the data from the COVID Tracking Project to construct\nestimates of the probability of receiving the first vaccine dose, the probability\nof receiving the second vaccine dose, and the probability of completing the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.35714285714285715,
          "p": 0.33783783783783783,
          "f": 0.3472222172260803
        },
        "rouge-2": {
          "r": 0.08888888888888889,
          "p": 0.07692307692307693,
          "f": 0.08247422183016291
        },
        "rouge-l": {
          "r": 0.3,
          "p": 0.28378378378378377,
          "f": 0.2916666616705247
        }
      }
    },
    {
      "paper_id": "cs.SC.cs/SC/2503.03337v1",
      "true_abstract": "We identify a common scheme in several existing algorithms adressing\ncomputational problems on linear differential equations with polynomial\ncoefficients. These algorithms reduce to computing a linear relation between\nvectors obtained as iterates of a simple differential operator known as\npseudo-linear map.\n  We focus on establishing precise degree bounds on the output of this class of\nalgorithms. It turns out that in all known instances (least common left\nmultiple, symmetric product,. . . ), the bounds that are derived from the\nlinear algebra step using Cramer's rule are pessimistic. The gap with the\nbehaviour observed in practice is often of one order of magnitude, and better\nbounds are sometimes known and derived from ad hoc methods and independent\narguments. We propose a unified approach for proving output degree bounds for\nall instances of the class at once. The main technical tools come from the\ntheory of realisations of matrices of rational functions and their\ndeterminantal denominators.",
      "generated_abstract": "r presents a unified approach for the estimation of the degree bound\nof linear differential operators on $\\mathbb{R}^d$ for any $d \\geq 2$. The\napproach relies on the use of an auxiliary operator, which is then estimated\nby a finite sum of the operator itself. The main contribution of the paper is\nthe development of a novel approach to the estimation of the degree bound of\nlinear differential operators on $\\mathbb{R}^d$ for any $d \\geq 2$. The\napproach relies on the use of an auxiliary operator, which is then estimated\nby a finite sum of the operator itself. The main contribution of the paper is\nthe development of a novel approach to the estimation of the degree bound of\nlinear differential operators on $\\mathbb{R}^d$ for any $d \\geq 2$. The\napproach is based on the construction of a finite sum of the operator itself\nand is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1553398058252427,
          "p": 0.36363636363636365,
          "f": 0.21768707063538342
        },
        "rouge-2": {
          "r": 0.052980132450331126,
          "p": 0.125,
          "f": 0.07441860046987585
        },
        "rouge-l": {
          "r": 0.13592233009708737,
          "p": 0.3181818181818182,
          "f": 0.190476186281642
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.22001v1",
      "true_abstract": "We examine the effect of item arrangement on choices using a novel\ndecision-making model based on the Markovian exploration of choice sets. This\nmodel is inspired by experimental evidence suggesting that the decision-making\nprocess involves sequential search through rapid stochastic pairwise\ncomparisons. Our findings show that decision-makers following a reversible\nprocess are unaffected by item rearrangements, and further demonstrate that\nthis property can be inferred from their choice behavior. Additionally, we\nprovide a characterization of the class of Markovian models in which the agent\nmakes all possible pairwise comparisons with positive probability. The\nintersection of reversible models and those allowing all pairwise comparisons\nis observationally equivalent to the well-known Luce model. Finally, we\ncharacterize the class of Markovian models for which the initial fixation does\nnot impact the final choice and show that choice data reveals the existence and\ncomposition of consideration sets.",
      "generated_abstract": "We investigate a class of stochastic choice models that combine\nmodeling the expected utility with Markov decision processes. We show that the\nMarkov property holds for the utility process and that the resulting Markovian\nmodel is equivalent to the standard model with an additional assumption on the\ntransition probability matrix. We prove that this Markovian model is\ncomputationally tractable. Furthermore, we show that the Markovian model can\nbe used to derive a generalization of the well-known Nash solution concept and\nextend it to stochastic choice problems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2872340425531915,
          "p": 0.48214285714285715,
          "f": 0.35999999532088894
        },
        "rouge-2": {
          "r": 0.08955223880597014,
          "p": 0.15384615384615385,
          "f": 0.11320754251869011
        },
        "rouge-l": {
          "r": 0.2765957446808511,
          "p": 0.4642857142857143,
          "f": 0.34666666198755564
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/AP/2503.10481v1",
      "true_abstract": "In clinical trials involving both mortality and morbidity, an active\ntreatment can influence the observed risk of the first non-fatal event either\ndirectly, through its effect on the non-fatal event process, or indirectly,\nthrough its effect on the death process, or both. Discerning the direct effect\nof treatment on the first non-fatal event holds clinical interest. However,\nwith the competing risk of death, the Cox proportional hazards model that\ntreats death as non-informative censoring and evaluates treatment effects on\ntime to the first non-fatal event provides an estimate of the cause-specific\nhazard ratio, which may not correspond to the direct effect. To obtain the\ndirect effect on the first non-fatal event, within the principal stratification\nframework, we define the principal stratum hazard and introduce the\nProportional Principal Stratum Hazards model. This model estimates the\nprincipal stratum hazard ratio, which reflects the direct effect on the first\nnon-fatal event in the presence of death and simplifies to the hazard ratio in\nthe absence of death. The principal stratum membership is identified using the\nshared frailty model, which assumes independence between the first non-fatal\nevent process and the potential death process from the counterfactual arm,\nconditional on per-subject random frailty. Simulation studies are conducted to\nverify the reliability of our estimators. We illustrate the method using the\nCarvedilol Prospective Randomized Cumulative Survival trial which involves\nheart-failure events.",
      "generated_abstract": "We present a novel framework for analyzing longitudinal data in which\nthe dependent variable is a random variable that belongs to a specified\nhierarchy. We propose a novel method for identifying the principal stratum for\nthe dependent variable and for estimating the hazard rate for each stratum. We\ndemonstrate that the proposed method can be used to analyze a wide variety of\ndata types and applications. We illustrate the proposed method using a\nsynthetic example and two real data applications. The results indicate that the\nproposed method is more powerful than existing methods, particularly for\nlongitudinal data with hierarchical dependence.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1557377049180328,
          "p": 0.31666666666666665,
          "f": 0.20879120437145282
        },
        "rouge-2": {
          "r": 0.0335195530726257,
          "p": 0.06666666666666667,
          "f": 0.04460966097483495
        },
        "rouge-l": {
          "r": 0.13114754098360656,
          "p": 0.26666666666666666,
          "f": 0.1758241714044199
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2409.18443v1",
      "true_abstract": "The primary objective of this study was to examine the impact of the US\nsovereign credit rating downgrade on its equity market. Utilizing the event\nstudy methodology, a sample of three most capitalized listed companies --\nMicrosoft, Apple, and Amazon -- and the equity market index -- S&P500 -- were\nused as the proxy for the overall equity market. Three market models were\nconstructed within the estimation window to determine the expected daily\nreturns on the selected companies stocks. The result showed that the sovereign\ncredit rating downgrade of the US government debt did not have any significant\neffects on the US equity market.",
      "generated_abstract": "t of a downgrade of a sovereign credit rating on the US equity\nmarket is investigated through a stylized model that integrates a\npre-emptive action of the central bank, a dynamic risk-aversion effect of\ninvestors, and a dynamic reaction of the central bank to the investor's\nreaction. The model also includes the effects of inflation, the interest rate\nspread between short and long-term government bonds, and the level of\ncorrelation between the bond and stock markets. We find that the central bank's\nintervention and the inflation rate can significantly influence the stock price\nof a sovereign credit rating downgrade. The central bank's preemptive action\nincreases the price of the downgraded rating and reduces the market impact of\nthe downgrade. However, the central bank's preemptive action can be ineffective\nif the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2727272727272727,
          "p": 0.2647058823529412,
          "f": 0.26865671141902436
        },
        "rouge-2": {
          "r": 0.12903225806451613,
          "p": 0.10810810810810811,
          "f": 0.11764705386245697
        },
        "rouge-l": {
          "r": 0.24242424242424243,
          "p": 0.23529411764705882,
          "f": 0.2388059651503677
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.09979v1",
      "true_abstract": "Silicon has striking similarity with carbon and is found in plant cells.\nHowever, there is no specific role that has been assigned to silicon in the\nlife cycle of plants. The amount of silicon in plant cells is species specific\nand can reach levels comparable to macronutrients. Silicon is the central\nelement for artificial intelligence, nanotechnology and digital revolution thus\ncan act as an informational molecule like nucleic acids while the diverse\nbonding potential of silicon with different chemical species is analogous to\ncarbon and thus can serve as a structural candidate such as proteins. The\ndiscovery of large amounts of silicon on Mars and the moon along with the\nrecent developments of enzyme that can incorporate silicon into organic\nmolecules has propelled the theory of creating silicon-based life. More\nrecently, bacterial cytochrome has been modified through directed evolution\nsuch that it could cleave silicon-carbon bonds in organo-silicon compounds thus\nconsolidating on the idea of utilizing silicon in biomolecules. In this article\nthe potential of silicon-based life forms has been hypothesized along with the\nreasoning that autotrophic virus-like particles can be a lucrative candidate to\ninvestigate such potential. Such investigations in the field of synthetic\nbiology and astrobiology will have corollary benefit on Earth in the areas of\nmedicine, sustainable agriculture and environmental sustainability.\nBibliometric analysis indicates an increasing interest in synthetic biology.\nGermany leads in research related to plant synthetic biology, while\nBiotechnology and Biological Sciences Research Council (BBSRC) at UK has\nhighest financial commitments and Chinese Academy of Sciences generates the\nhighest number of publications in the field.",
      "generated_abstract": "The emerging field of synthetic biology aims to create new biological\nproducts by engineering existing biological systems. This includes the\ndevelopment of synthetic plant cells that can perform specific functions, such\nas synthesizing new chemicals. In this work, we examine the potential of\nsilicon as a substrate for plant synthetic biology, focusing on the development\nof silicon-based plant cells for biotechnological applications. We present a\nreview of the current state of silicon research in plant biology and discuss\nthe potential of silicon-based plants for various biotechnological applications.\nWe also outline the challenges and opportunities that silicon-based plants\npresent, focusing on the development of silicon-based plant cells, silicon\nsynthesis, and silicon-based biotechnology.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17197452229299362,
          "p": 0.4090909090909091,
          "f": 0.2421524622003258
        },
        "rouge-2": {
          "r": 0.07083333333333333,
          "p": 0.1827956989247312,
          "f": 0.10210209807645498
        },
        "rouge-l": {
          "r": 0.15286624203821655,
          "p": 0.36363636363636365,
          "f": 0.21524663260391327
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.10264v1",
      "true_abstract": "While peer review enhances writing and research quality, harsh feedback can\nfrustrate and demotivate authors. Hence, it is essential to explore how\ncritiques should be delivered to motivate authors and enable them to keep\niterating their work. In this study, we explored the impact of appending an\nautomatically generated positive summary to the peer reviews of a writing task,\nalongside varying levels of overall evaluations (high vs. low), on authors'\nfeedback reception, revision outcomes, and motivation to revise. Through a 2x2\nonline experiment with 137 participants, we found that adding an AI-reframed\npositive summary to otherwise harsh feedback increased authors' critique\nacceptance, whereas low overall evaluations of their work led to increased\nrevision efforts. We discuss the implications of using AI in peer feedback,\nfocusing on how AI-driven critiques can influence critique acceptance and\nsupport research communities in fostering productive and friendly peer feedback\npractices.",
      "generated_abstract": "ew is a crucial step in the publishing process, providing a\nmoral assessment of submitted manuscripts. The process is traditionally\nperformed by a reviewer, a member of the publishing community, who provides a\njudgmental summary of a manuscript's strengths and weaknesses. However, the\ntraditional summary can be opaque, implying only the reviewer's own\nassessment. This approach can be problematic for a number of reasons, including\na lack of transparency, the potential for biases, and the risk of\nmiscommunication. To address these issues, we propose an AI-reframed positive\nsummary, an approach that reframes a reviewer's summary as a positive\nassessment, highlighting the reviewer's strengths while also acknowledging\nlimitations. This approach allows for greater transparency and trust, and\nincreases the clarity",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14,
          "p": 0.1794871794871795,
          "f": 0.15730336586289626
        },
        "rouge-2": {
          "r": 0.02158273381294964,
          "p": 0.02631578947368421,
          "f": 0.023715410068585137
        },
        "rouge-l": {
          "r": 0.12,
          "p": 0.15384615384615385,
          "f": 0.13483145575053673
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2411.12769v1",
      "true_abstract": "The Genebass dataset, released by Karczewski et al. (2022), provides a\ncomprehensive resource elucidating associations between genes and 4,529\nphenotypes based on nearly 400,000 exomes from the UK Biobank. This extensive\ndataset enables the evaluation of gene set enrichment across a wide range of\nphenotypes, facilitating the inference of associations between specified gene\nsets and phenotypic traits. Despite its potential, no established method for\napplying gene set enrichment analysis (GSEA) to Genebass data exists. To\naddress this gap, we propose utilizing fast pre-ranked gene set enrichment\nanalysis (FGSEA) as a novel approach to determine whether a specified set of\ngenes is significantly enriched in phenotypes within the UK Biobank. We\ndeveloped an R package, ukbFGSEA, to implement this analysis, completed with a\nhands-on tutorial. Our approach has been validated by analyzing gene sets\nassociated with autism spectrum disorder, developmental disorder, and\nneurodevelopmental disorders, demonstrating its capability to reveal\nestablished and novel associations.",
      "generated_abstract": "t analysis is a powerful tool for discovering novel biological\nmarkers in large-scale data sets, such as exome data. However, existing\nenrichment analysis methods often require significant computational resources,\nmaking them impractical for large-scale data sets. We present ukbFGSEA, an R\npackage that implements fast preranked gene set enrichment analysis (FPGSEA) on\nUK Biobank exome data. ukbFGSEA uses a preranked approach to enhance computational\nefficiency. This approach precomputes a set of pre-defined rankings for each\ngene prior to applying enrichment analysis. The preranked approach also\nreduces computational overhead by eliminating the need to compute ranks for\nall genes in the exome data set. To enable seamless integration with existing\nR packages, ukbFGSEA uses an R",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.28703703703703703,
          "p": 0.3924050632911392,
          "f": 0.3315507972592868
        },
        "rouge-2": {
          "r": 0.04929577464788732,
          "p": 0.0660377358490566,
          "f": 0.05645160800858522
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.34177215189873417,
          "f": 0.2887700485961852
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.01533v5",
      "true_abstract": "Elite economics PhD programs aim to train graduate students for a lifetime of\nacademic research. This paper asks how advising affects graduate students'\npost-PhD research productivity. Advising is highly concentrated: at the eight\nhighly-selective schools in our study, a minority of advisors do most of the\nadvising work. We quantify advisor attributes such as an advisor's own research\noutput and aspects of the advising relationship like coauthoring and research\nfield affinity that might contribute to student research success. Students\nadvised by research-active, prolific advisors tend to publish more, while\ncoauthoring has no effect. Student-advisor research affinity also predicts\nstudent success. But a school-level aggregate production function provides much\nweaker evidence of causal effects, suggesting that successful advisors attract\nstudents likely to succeed-without necessarily boosting their students' chances\nof success. Evidence for causal effects is strongest for a measure of advisors'\nown research output. Aggregate student research output appears to scale\nlinearly with graduate student enrollment, with no evidence of negative\nclass-size effects. An analysis of gender differences in research output shows\nmale and female graduate students to be equally productive in the first few\nyears post-PhD, but female productivity peaks sooner than male productivity.",
      "generated_abstract": "I examine how graduate student advisors and their graduate students\ngradually move through the research production function of economics. I use\nthe data from the 2021 National Survey of Student Engagement to construct a\ntwo-part path model. The first part of the model examines the change in\nadvisors' and students' academic focus from undergraduate to graduate school.\nThe second part of the model examines the change in advisors' and students'\nresearch production from undergraduate to graduate school. I find that\nadvisors' and students' academic focus and research production decline over\ntime, with declines more pronounced in the latter half of the sample. I also\nfind that students' research production is more sensitive to their advisors'\nand research production than advisors' and research production to their\nstudents'.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1937984496124031,
          "p": 0.4098360655737705,
          "f": 0.2631578903772854
        },
        "rouge-2": {
          "r": 0.03825136612021858,
          "p": 0.07446808510638298,
          "f": 0.050541511761655056
        },
        "rouge-l": {
          "r": 0.16279069767441862,
          "p": 0.3442622950819672,
          "f": 0.22105262721939065
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/DC/2503.10217v1",
      "true_abstract": "Fine-tuning plays a crucial role in enabling pre-trained LLMs to evolve from\ngeneral language comprehension to task-specific expertise. To preserve user\ndata privacy, federated fine-tuning is often employed and has emerged as the de\nfacto paradigm. However, federated fine-tuning is prohibitively inefficient due\nto the tension between LLM complexity and the resource constraint of end\ndevices, incurring unaffordable fine-tuning overhead. Existing literature\nprimarily utilizes parameter-efficient fine-tuning techniques to mitigate\ncommunication costs, yet computational and memory burdens continue to pose\nsignificant challenges for developers. This work proposes DropPEFT, an\ninnovative federated PEFT framework that employs a novel stochastic transformer\nlayer dropout method, enabling devices to deactivate a considerable fraction of\nLLMs layers during training, thereby eliminating the associated computational\nload and memory footprint. In DropPEFT, a key challenge is the proper\nconfiguration of dropout ratios for layers, as overhead and training\nperformance are highly sensitive to this setting. To address this challenge, we\nadaptively assign optimal dropout-ratio configurations to devices through an\nexploration-exploitation strategy, achieving efficient and effective\nfine-tuning. Extensive experiments show that DropPEFT can achieve a\n1.3-6.3\\times speedup in model convergence and a 40%-67% reduction in memory\nfootprint compared to state-of-the-art methods.",
      "generated_abstract": "learning has emerged as a promising approach for scaling\ntraining data across heterogeneous, privacy-sensitive end-user devices.\nHowever, the lack of an efficient fine-tuning framework for large language\nmodels (LLMs) has hindered its practical adoption. In this paper, we introduce\nLLM-Federated Fine-Tuning (LLM-FT), a novel fine-tuning method for LLMs that\nleverages the insights of layer dropout to enhance efficiency. By leveraging\npre-trained LLMs and an LLM-specific layer dropout strategy, our approach\nsimultaneously reduces the inference time and the number of model parameters\nwhile maintaining equivalent or even superior performance. Extensive experiments\non both public and private datasets demonstrate the efficacy of our method.\nNotably, the proposed LLM-FT approach achieves a 6.66%",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22377622377622378,
          "p": 0.367816091954023,
          "f": 0.27826086486162577
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.06422018348623854,
          "f": 0.046979861132156665
        },
        "rouge-l": {
          "r": 0.20279720279720279,
          "p": 0.3333333333333333,
          "f": 0.2521739083398866
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.16677v1",
      "true_abstract": "Understanding cooperation in social systems is challenging because the\never-changing rules that govern societies interact with individual actions,\nresulting in intricate collective outcomes. In virtual-world experiments, we\nallowed people to make changes in the systems that they are making decisions\nwithin and investigated how they weigh the influence of different rules in\ndecision-making. When choosing between worlds differing in more than one rule,\na naive heuristics model predicted participants decisions as well, and in some\ncases better, than game earnings (utility) or by the subjective quality of\nsingle rules. In contrast, when a subset of engaged participants made\ninstantaneous (within-world) decisions, their behavior aligned very closely\nwith objective utility and not with the heuristics model. Findings suggest\nthat, whereas choices between rules may deviate from rational benchmarks, the\nfrequency of real time cooperation decisions to provide feedback can be a\nreliable indicator of the objective utility of these rules.",
      "generated_abstract": "e action is ubiquitous in nature and society, yet its effectiveness\nand sustainability remain a major challenge. This paper develops a general\nframework for reasoning about collective action problems. The framework\nincludes three key components: (1) a generic model of collective action\nproblems, (2) a general reasoning procedure that models reasoning within and\nbetween collective action problems, and (3) a set of core reasoning\ninstructions that can be used to formulate reasoning problems. The framework\ncan be used to reason about both simple and complex collective action\nproblems, such as a single individual taking an action within a population, or\ntwo agents taking actions to form a collective action, or a single agent taking\nactions to form a collective action that affects another agent. The framework\nalso provides a unifying perspective on the different ways that reasoning\nabout collective action problems can be framed, making it useful",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18518518518518517,
          "p": 0.24096385542168675,
          "f": 0.20942407885529465
        },
        "rouge-2": {
          "r": 0.0136986301369863,
          "p": 0.01639344262295082,
          "f": 0.014925368174428024
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.21686746987951808,
          "f": 0.1884816704783313
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2502.07518v1",
      "true_abstract": "Implied volatility IV is a key metric in financial markets, reflecting market\nexpectations of future price fluctuations. Research has explored IV's\nrelationship with moneyness, focusing on its connection to the implied Hurst\nexponent H. Our study reveals that H approaches 1/2 when moneyness equals 1,\nmarking a critical point in market efficiency expectations. We developed an IV\nmodel that integrates H to capture these dynamics more effectively. This model\nconsiders the interaction between H and the underlying-to-strike price ratio\nS/K, crucial for capturing IV variations based on moneyness. Using Optuna\noptimization across multiple indexes, the model outperformed SABR and fSABR in\naccuracy. This approach provides a more detailed representation of market\nexpectations and IV-H dynamics, improving options pricing and volatility\nforecasting while enhancing theoretical and pratcical financial analysis.",
      "generated_abstract": "In this paper, we explore the impact of implied regularity on implied\nvolatility models by constructing a free arbitrage model. Free arbitrage\nmeans trading on the price of an underlying asset without the consideration of\nthe market price of the underlying. Our methodology is based on the\napplication of the Delta-Hedging Principle to the price of an option. We\nestablish a link between the implied volatility of a European option and the\nimplied volatility of the underlying asset, and show that this link is\ndetermined by the implied regularity. By using the implied regularity of the\nunderlying asset as a risk parameter, we can use the implied volatility to\npredict the implied volatility of the European option. This approach offers\npotential for improved risk management and pricing of financial products.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23958333333333334,
          "p": 0.3484848484848485,
          "f": 0.28395061245541847
        },
        "rouge-2": {
          "r": 0.03968253968253968,
          "p": 0.049019607843137254,
          "f": 0.04385964417820924
        },
        "rouge-l": {
          "r": 0.22916666666666666,
          "p": 0.3333333333333333,
          "f": 0.27160493344307274
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/RO/2503.10349v1",
      "true_abstract": "This study proposes a new Gaussian Mixture Filter (GMF) to improve the\nestimation performance for the autonomous robotic radio signal source search\nand localization problem in unknown environments. The proposed filter is first\ntested with a benchmark numerical problem to validate the performance with\nother state-of-practice approaches such as Particle Gaussian Mixture (PGM)\nfilters and Particle Filter (PF). Then the proposed approach is tested and\ncompared against PF and PGM filters in real-world robotic field experiments to\nvalidate its impact for real-world robotic applications. The considered\nreal-world scenarios have partial observability with the range-only measurement\nand uncertainty with the measurement model. The results show that the proposed\nfilter can handle this partial observability effectively whilst showing\nimproved performance compared to PF, reducing the computation requirements\nwhile demonstrating improved robustness over compared techniques.",
      "generated_abstract": "source localization problem in unmanned aerial vehicles (UAVs) is\ngiven by finding the position of the transmitter (radio source) given the\nreceived signals from multiple receivers. In this paper, we propose a novel\nfiltering approach to solve the radio source localization problem. The\nproposed approach consists of a Gaussian Mixture Filtering (GMF) algorithm to\ncombine the information from the received signals from multiple receivers, and\na Gaussian Mixture Detection (GMD) algorithm to detect the radio source\nposition from the received signals from multiple receivers. The GMD algorithm\nis designed to detect the position of the radio source by fitting Gaussian\nmixture models to the received signals from multiple receivers. The GMF\nalgorithm is designed to combine the information from the received signals from\nmultiple receivers to form a Gaussian mixture model. The GMD algorithm is\ndesigned to detect the radio",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20930232558139536,
          "p": 0.3103448275862069,
          "f": 0.24999999518904328
        },
        "rouge-2": {
          "r": 0.04838709677419355,
          "p": 0.06976744186046512,
          "f": 0.057142852306576376
        },
        "rouge-l": {
          "r": 0.20930232558139536,
          "p": 0.3103448275862069,
          "f": 0.24999999518904328
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2411.05851v1",
      "true_abstract": "This paper explores a GIS-based application of the conditional p-median\nproblem (where p = 1) in last-mile delivery logistics. The rapid growth of\ne-commerce in Pakistan has primarily benefited logistics companies, which face\nthe challenge of resolving inefficiencies in the existing infrastructure and\nscaling effectively to meet increasing demand. Addressing these challenges\nwould not only reduce operational costs but also lower carbon footprints. We\npresent an algorithm that utilizes road-network-based distances to determine\nthe optimal location for a new hub facility, a problem known in operations\nresearch as the conditional p-median problem. The algorithm optimizes the\nplacement of a new facility, given q existing facilities. The past delivery\ndata for this research was provided by Muller and Phipps Logistics Pakistan.\nOur method involves constructing a distance matrix between candidate hub\nlocations and past delivery points, followed by a grid search to identify the\noptimal hub location. To simulate the absence of past delivery data, we\nrepeated the process using the population distribution of Lahore. Our results\ndemonstrate a 16% reduction in average delivery distance with the addition of a\nnew hub.",
      "generated_abstract": "r presents a new methodology for the computation of the\ndistribution hubs of a road network, which is essential for traffic flow\noptimization. The methodology is based on the application of Conditional\nP-Median (CP-Median) as a graph partitioning technique, which is a powerful\ntool for the analysis of graph problems. The methodology is applied to the\nproblem of computing the distribution hubs of the network of the city of\nChicago, and the results are compared with the results obtained using the\nexisting methods, such as the graph partitioning method of the\nGraph-Kmeans-Hill-Clustering (GKHC) algorithm. The results show that the new\nmethodology provides a better approximation of the distribution hubs of the\nnetwork of Chicago, especially in terms of the number of hubs and the\ndistribution of the hubs in the network. The results also show that the\nexisting methods",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.3333333333333333,
          "f": 0.23529411307958487
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.09259259259259259,
          "f": 0.07194244129185892
        },
        "rouge-l": {
          "r": 0.1652892561983471,
          "p": 0.30303030303030304,
          "f": 0.2139037387480341
        }
      }
    },
    {
      "paper_id": "math.OC.math/OC/2503.10006v1",
      "true_abstract": "We study a novel class of algorithms for solving model-free feedback\noptimization problems in dynamical systems. The key novelty is the introduction\nof \\emph{persistent resetting learning integrators} (PRLI), which are\nintegrators that are reset at the same frequency at which the plant is dithered\nusing exploratory signals for model-free optimization. It is shown that PRLIs\ncan serve as core mechanisms for real-time gradient estimation in online\nfeedback-optimization tasks where only cost function measurements are\navailable. In particular, unlike existing approaches based on approximation\ntheory, such as averaging or finite-differences, PRLIs can produce global\nreal-time gradient estimates of cost functions, with uniformly bounded\nperturbations of arbitrarily small magnitude. In this sense, PRLIs function as\nrobust \\emph{hybrid} \"Oracles\" suitable for interconnection with discrete-time\noptimization algorithms that optimize the performance of continuous-time\ndynamical plants in closed-loop operation. Compared to existing methods, PRLIs\nyield \\emph{global} stability properties for a broad class of cost functions,\nsurpassing the local or semi-global guarantees offered by traditional\napproaches based on perturbation and approximation theory. The proposed\nframework naturally bridges physical systems, modeled as continuous-time plants\nwhere continuous exploration is essential, with digital algorithms, represented\nas discrete-time optimization methods. The main results are illustrated using\ndifferent numerical examples.",
      "generated_abstract": "We propose a framework for learning integrators with a resetting term that\nintegrates the past and future, and is persistent in the sense that it\nremembers the past up to a certain time horizon. Our approach is based on a\ntime-reversed version of the A-bar estimator, a linearization-free algorithm\nthat can be used to solve a constrained optimization problem. We show that our\nframework extends the A-bar estimator to allow for the presence of a\npersistently resetting integrator. Our approach is motivated by the observation\nthat resetting integrators are common in practice, particularly in the context\nof model-free reinforcement learning. We demonstrate the efficacy of our\nframework through a series of experiments on a variety of models, including\ncontinuous-time Markov chains, particle filters, and Gaussian processes.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16911764705882354,
          "p": 0.2948717948717949,
          "f": 0.21495326639531848
        },
        "rouge-2": {
          "r": 0.005263157894736842,
          "p": 0.00847457627118644,
          "f": 0.006493501766743195
        },
        "rouge-l": {
          "r": 0.16911764705882354,
          "p": 0.2948717948717949,
          "f": 0.21495326639531848
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.14258v1",
      "true_abstract": "Accelerating the deep transformation and upgrading of industrial structure\nand forming new quality productive forces are essential components for China to\nachieve the great rejuvenation of the Chinese Dream. After more than 40 years\nof rapid development, China has entered the \"new normal\" of development, making\nthe advancement of new quality productive forces an urgent task. This paper\nreviews the evolution of China's industrial structure, argues the necessity for\na new round of deep industrial transformation, and explores the impact of\nindustrial structure transformation and upgrading on the level of new quality\nproductive forces using various methods. The research findings are as\nfollows:(1)The deep transformation and upgrading of the industrial structure\ncan significantly promote the development of new quality productive forces, but\nthere are obvious regional differences.(2)The core indicator of the improvement\nin the level of new quality productive forces is the enhancement of total\nfactor productivity. Furthermore, this paper summarizes past industrial\ndevelopment processes and the challenges faced, and analyzes and discusses the\npotential challenges that may arise in promoting the development of new quality\nproductive forces through deep industrial structure transformation, based on\nempirical research results.",
      "generated_abstract": "y explores the relationship between industrial upgrading and new\nproduction forces in China's provincial level data from 2003 to 2022, focusing\non the impact of provincial upgrading on local industrial upgrading, new\nproduction forces, and provincial economic growth. The results show that the\nindustrial upgrading process has a positive and significant impact on local\nindustrial upgrading, new production forces, and provincial economic growth,\nwith the impact being stronger for lower-tier cities. The findings also\ndiscover that local industrial upgrading has a positive and significant\ninfluence on new production forces, with a positive and significant impact on\nthe provincial economy. The results also show that the industrial upgrading\nprocess has a positive and significant impact on local industrial upgrading,\nnew production forces, and provincial economic growth, with the impact being\nstronger for lower-tier cities. These results highlight the importance of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21568627450980393,
          "p": 0.41509433962264153,
          "f": 0.28387096324162336
        },
        "rouge-2": {
          "r": 0.0392156862745098,
          "p": 0.07407407407407407,
          "f": 0.051282046755424464
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.32075471698113206,
          "f": 0.21935483420936533
        }
      }
    },
    {
      "paper_id": "hep-ex.hep-ex/2503.09742v1",
      "true_abstract": "Measurements are presented of the W and Z boson production cross sections in\nproton-proton collisions at a center-of-mass energy of 13.6 TeV. Data collected\nin 2022 and corresponding to an integrated luminosity of 5.01 fb$^{-1}$ with\none or two identified muons in the final state are analyzed. The results for\nthe products of total inclusive cross sections and branching fractions for\nmuonic decays of W and Z bosons are 11.93 $\\pm$ 0.08 (syst) $\\pm$ 0.17 (lumi)\n$^{+0.07}_{-0.07}$ (acc) nb for W$^+$ boson production, 8.86 $\\pm$ 0.06 (syst)\n$\\pm$ 0.12 (lumi) $^{+0.05}_{-0.06}$ (acc) nb for W$^-$ boson production, and\n2.021 $\\pm$ 0.009 (syst) $\\pm$ 0.028 (lumi) $^{+0.011}_{-0.013}$ (acc) nb for\nthe Z boson production in the dimuon mass range of 60-120 GeV, all with\nnegligible statistical uncertainties. Furthermore, the corresponding fiducial\ncross sections, as well as cross section ratios for both fiducial and total\nphase space, are provided. The ratios include charge-separated results for W\nboson production (W$^+$ and W$^-$) and the sum of the two contributions\n(W$^\\pm$), each relative to the measured Z boson production cross section.\nAdditionally, the ratio of the measured cross sections for W$^+$ and W$^-$\nboson production is reported. All measurements are in agreement with\ntheoretical predictions, calculated at next-to-next-to-leading order accuracy\nin quantum chromodynamics.",
      "generated_abstract": "rement of the inclusive W and Z boson production cross sections and\ntheir ratios in proton-proton collisions at $\\sqrt{s}=13.6$ TeV is presented.\nA data sample of 35.2 fb$^{-1}$ collected by the CMS experiment at $\\sqrt{s}=13$\nTeV was analyzed. The measured cross sections are 70.0+0.3(-0.5)pb and 55.5+1.7\n(-1.8)pb for W+jet and W-jet production, respectively. The measured ratio of\nW+jet to W-jet production is 1.08+0.07(-0.06). The measured cross sections are\ncompared to calculations using next-to-leading order matrix elements with\nnext-to-leading order (NLO) QCD and next-to-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24603174603174602,
          "p": 0.4492753623188406,
          "f": 0.3179487133759369
        },
        "rouge-2": {
          "r": 0.07853403141361257,
          "p": 0.1744186046511628,
          "f": 0.10830324481591071
        },
        "rouge-l": {
          "r": 0.23809523809523808,
          "p": 0.43478260869565216,
          "f": 0.30769230311952667
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.06596v1",
      "true_abstract": "Intelligent reflecting surfaces (IRSs) are envisioned to enhance the\nperformance of mmWave wireless systems. In practice, multiple mobile operators\n(MO) coexist in an area and provide simultaneous and independent services to\nuser-equipments (UEs) on different frequency bands. Then, if each MO deploys an\nIRS to enhance its performance, the IRSs also alter the channels of UEs of\nother MOs. In this context, this paper addresses the following questions: can\nan MO still continue to control its IRS independently of other MOs and IRSs? Is\njoint optimization of IRSs deployed by different MOs and inter-MO cooperation\nneeded? To that end, by considering the mmWave bands, we first derive the\nergodic sum spectral efficiency (SE) in a $2$-MO system for the following\nschemes: 1) joint optimization of an overall phase angle of the IRSs with MO\ncooperation, 2) MO cooperation via time-sharing, and 3) no cooperation between\nthe MOs. We find that even with no cooperation between the MOs, the performance\nof a given MO is not degraded by the presence of an out-of-band (OOB) MO\ndeploying and independently controlling its own IRS. On the other hand, the SE\ngain obtained at a given MO using joint optimization and cooperation over the\nno-cooperation scheme decreases inversely with the number of elements in the\nIRS deployed by the other MO. We generalize our results to a multiple MO setup\nand show that the gain in the sum-SE over the no-cooperation case increases at\nleast linearly with the number of OOB MOs. Finally, we numerically verify our\nfindings and conclude that every MO can independently operate and tune its IRS;\ncooperation via optimizing an overall phase only brings marginal benefits in\npractice.",
      "generated_abstract": "r focuses on the performance analysis of multi-IRS aided multiple\noperator systems (MAMS) in millimeter wave (mmWave) frequencies. In this\nsystem, the IRS can transmit signals to the users with different operators\nthroughout the time slot. The IRS transmits the signals to the users with\ndifferent operators in the time slot, and the users can simultaneously\nreceive signals from multiple operators. In this paper, the MAMS system is\nassumed to be a multi-operator system, and the users are assumed to be\ndifferent operators with different operators. The MAMS system is assumed to be\nan interference-free system, and the channel state information (CSI) of the\nusers and the IRS is assumed to be known to the MAMS system. The MAMS system\ncan be modeled as a system consisting of two parts: the IRS part and the MAMS\npart. The MAMS",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13636363636363635,
          "p": 0.3387096774193548,
          "f": 0.19444444035150898
        },
        "rouge-2": {
          "r": 0.020161290322580645,
          "p": 0.05,
          "f": 0.02873562808825531
        },
        "rouge-l": {
          "r": 0.12337662337662338,
          "p": 0.3064516129032258,
          "f": 0.1759259218329905
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.07296v1",
      "true_abstract": "This paper introduces the concept of wireless-powered zero-energy\nreconfigurable intelligent surface (zeRIS), and investigates a wireless-powered\nzeRIS aided communication system in terms of security, reliability and energy\nefficiency. In particular, we propose three new wireless-powered zeRIS modes:\n1) in mode-I, N reconfigurable reflecting elements are adjusted to the optimal\nphase shift design of information user to maximize the reliability of the\nsystem; 2) in mode-II, N reconfigurable reflecting elements are adjusted to the\noptimal phase shift design of cooperative jamming user to maximize the security\nof the system; 3) in mode-III, N1 and N2 (N1+N2=N) reconfigurable reflecting\nelements are respectively adjusted to the optimal phase shift designs of\ninformation user and cooperative jamming user to balance the reliability and\nsecurity of the system. Then, we propose three new metrics, i.e., joint outage\nprobability (JOP), joint intercept probability (JIP), and secrecy energy\nefficiency (SEE), and analyze their closed-form expressions in three modes,\nrespectively. The results show that under high transmission power, all the\ndiversity gains of three modes are 1, and the JOPs of mode-I, mode-II and\nmode-III are improved by increasing the number of zeRIS elements, which are\nrelated to N2, N, and N^2_1, respectively. In addition, mode-I achieves the\nbest JOP, while mode-II achieves the best JIP among three modes. We exploit two\nsecurity-reliability trade-off (SRT) metrics, i.e., JOP versus JIP, and\nnormalized joint intercept and outage probability (JIOP), to reveal the SRT\nperformance of the proposed three modes. It is obtained that mode-II\noutperforms the other two modes in the JOP versus JIP, while mode-III and\nmode-II achieve the best performance of normalized JIOP at low and high\ntransmission power, respectively.",
      "generated_abstract": "er a multi-antenna wireless system in which the transmitter (Tx)\nuses a random-phase array (RPA) to transmit signals to the receiver (Rx) in\nthe absence of direct line-of-sight (LoS) links. The Rx is equipped with a\nself-interference-cancelling (SIC) receiver and a random-phase antenna\narray (RPA) that can simultaneously receive from multiple Tx antennas. We\nassume that the Tx uses a fixed-phase RPA to achieve LoS links with the Rx,\nyet the Tx also needs to support LoS links with multiple RpAs. We assume that\nthe Tx can control the RPA phases to achieve these multiple LoS links. We\nintroduce a novel and general model for the Tx-Rx LoS channel, which is\ndistinct from the commonly used Rayle",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0948905109489051,
          "p": 0.19117647058823528,
          "f": 0.12682926385913162
        },
        "rouge-2": {
          "r": 0.013574660633484163,
          "p": 0.028846153846153848,
          "f": 0.018461534109539487
        },
        "rouge-l": {
          "r": 0.0948905109489051,
          "p": 0.19117647058823528,
          "f": 0.12682926385913162
        }
      }
    },
    {
      "paper_id": "cs.GR.cs/GR/2503.09640v1",
      "true_abstract": "Rendering realistic human-object interactions (HOIs) from sparse-view inputs\nis challenging due to occlusions and incomplete observations, yet crucial for\nvarious real-world applications. Existing methods always struggle with either\nlow rendering qualities (\\eg, visual fidelity and physically plausible HOIs) or\nhigh computational costs. To address these limitations, we propose HOGS\n(Human-Object Rendering via 3D Gaussian Splatting), a novel framework for\nefficient and physically plausible HOI rendering from sparse views.\nSpecifically, HOGS combines 3D Gaussian Splatting with a physics-aware\noptimization process. It incorporates a Human Pose Refinement module for\naccurate pose estimation and a Sparse-View Human-Object Contact Prediction\nmodule for efficient contact region identification. This combination enables\ncoherent joint rendering of human and object Gaussians while enforcing\nphysically plausible interactions. Extensive experiments on the HODome dataset\ndemonstrate that HOGS achieves superior rendering quality, efficiency, and\nphysical plausibility compared to existing methods. We further show its\nextensibility to hand-object grasp rendering tasks, presenting its broader\napplicability to articulated object interactions.",
      "generated_abstract": "t a novel physics-aware human-object rendering pipeline for sparse\nview synthesis, designed to integrate object geometry and material properties\ninto the rendering process. Our approach leverages a 3D Gaussian\nsplatting framework to generate 3D surface representations from sparse views,\nwhich are then fed into a neural network to synthesize realistic-looking\nrepresentations of the objects. To ensure a high level of detail, we apply\nan adaptive sampling strategy that selects the most informative points for\ngaussian sampling. We then train the network using a physics-aware loss that\nconsiders the influence of the object's geometry and material on the output\nrepresentation. The model is trained using the Matterport3D dataset, which\ncontains millions of 3D views of real-world objects. Our experiments demonstrate\nthat our method outperforms traditional image-based rendering techniques,\nespecially in challenging scenarios where the number of views",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2396694214876033,
          "p": 0.30526315789473685,
          "f": 0.2685185135909637
        },
        "rouge-2": {
          "r": 0.04,
          "p": 0.045454545454545456,
          "f": 0.0425531865097335
        },
        "rouge-l": {
          "r": 0.2231404958677686,
          "p": 0.28421052631578947,
          "f": 0.2499999950724452
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/TH/2503.05542v2",
      "true_abstract": "We consider standard gradient descent, gradient flow and conjugate gradients\nas iterative algorithms for minimizing a penalized ridge criterion in linear\nregression. While it is well known that conjugate gradients exhibit fast\nnumerical convergence, the statistical properties of their iterates are more\ndifficult to assess due to inherent nonlinearities and dependencies. On the\nother hand, standard gradient flow is a linear method with well known\nregularizing properties when stopped early. By an explicit non-standard error\ndecomposition we are able to bound the prediction error for conjugate gradient\niterates by a corresponding prediction error of gradient flow at transformed\niteration indices. This way, the risk along the entire regularisation path of\nconjugate gradient iterations can be compared to that for regularisation paths\nof standard linear methods like gradient flow and ridge regression. In\nparticular, the oracle conjugate gradient iterate shares the optimality\nproperties of the gradient flow and ridge regression oracles up to a constant\nfactor. Numerical examples show the similarity of the regularisation paths in\npractice.",
      "generated_abstract": "aper, we study regularisation paths of conjugate gradient\n(CG) estimators in the context of ridge regression, focusing on the\ninterplay between the regularisation parameter and the number of iterations\n(iterations per gradient evaluation). We derive closed-form expressions for\nthe expected number of iterations and the corresponding number of CG steps\nrequired for a fixed error parameter and a given regularisation parameter,\nexplaining why the number of iterations is a lower bound on the number of\nCG steps. We show that for the most widely used ridge regularisation parameter,\n$\\lambda = 1/\\sqrt{n}$, the convergence rate of the iterates is $\\mathcal{O}(1/n)$\nfor a fixed error parameter and $\\mathcal{O}(\\log(n))$ for a fixed regularisation\nparameter. We also show that the convergence rate of the iterates is\n$\\mathcal{O}(1/\\sqrt{n})$ for a fixed",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21,
          "p": 0.3442622950819672,
          "f": 0.2608695605107828
        },
        "rouge-2": {
          "r": 0.05405405405405406,
          "p": 0.08602150537634409,
          "f": 0.06639003675418846
        },
        "rouge-l": {
          "r": 0.17,
          "p": 0.2786885245901639,
          "f": 0.21118011951699403
        }
      }
    },
    {
      "paper_id": "physics.ins-det.physics/ins-det/2503.10395v1",
      "true_abstract": "One of the main issues of the satellite-to-ground optical communication,\nincluding free-space satellite quantum key distribution (QKD), is an\nachievement of the reasonable accuracy of positioning, navigation and optical\nstabilization. Proportional-integral-derivative (PID) controllers can handle\nwith various control tasks in optical systems. Recent research shows the\npromising results in the area of composite control systems including classical\ncontrol via PID controllers and reinforcement learning (RL) approach. In this\nwork we apply RL agent to an experimental stand of the optical stabilization\nsystem of QKD terminal. We find via agent control history more precise PID\nparameters and also provide effective combined RL-PID dynamic control approach\nfor the optical stabilization of satellite-to-ground communication system.",
      "generated_abstract": "f optical stabilization in laser communication satellite systems\nrepresents a revolutionary solution to stabilize the laser signal during the\ntransmission process, offering a high degree of accuracy in the optical signal\ntransmission, even under harsh conditions. This paper introduces a new\nproportional-integral-derivative (PID) controller to stabilize the optical\nsignal during the laser communication satellite system. The controller\nintegrates the PID algorithm, an optical tracking method, and an error\nestimation method to ensure a stable optical signal transmission. The\noptimization of the PID controller parameters is performed through the reinforced\nlearning algorithm, with the objective of improving the tracking accuracy and\nthe tracking stability of the system. The simulation results show that the\nproposed control strategy is capable of achieving good tracking performance\nunder various conditions, including large-scale atmospheric disturbances and\nlow-obser",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3026315789473684,
          "p": 0.2911392405063291,
          "f": 0.2967741885502602
        },
        "rouge-2": {
          "r": 0.0380952380952381,
          "p": 0.034482758620689655,
          "f": 0.03619909003501226
        },
        "rouge-l": {
          "r": 0.27631578947368424,
          "p": 0.26582278481012656,
          "f": 0.270967736937357
        }
      }
    },
    {
      "paper_id": "nucl-th.nucl-ex/2503.08396v1",
      "true_abstract": "This study investigates the moderation of 14.1 MeV neutrons in a natural\nberyllium moderator arranged in a spherical geometry. The neutron interactions\nand moderation efficiency were analyzed using Monte Carlo simulations with the\nGEANT4 toolkit. Various sphere radii were tested to determine the optimal\nmoderator thickness for neutron thermalization.",
      "generated_abstract": "of neutron-moderating properties of the light isotopes of the\nbearing 14 MeV neutron is a necessary step for the comprehensive\ncharacterization of the neutron moderation phenomenon in the 14 MeV neutron\nenergy range. Theoretical studies of the neutron-moderating properties of the\nlight isotopes of beryllium, such as Be7, Be8, Be9, Be10, and Be11, are\nnecessary for the comprehensive characterization of the neutron moderation\nphenomenon in the 14 MeV neutron energy range. In this work, we investigate the\nneutron-moderating properties of the light isotopes of beryllium using the\nfinite-difference time-domain (FDTD) method and the modified Hastings-Dewar\nmethod. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.30952380952380953,
          "p": 0.2708333333333333,
          "f": 0.28888888391111117
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.30952380952380953,
          "p": 0.2708333333333333,
          "f": 0.28888888391111117
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/DB/2503.08087v1",
      "true_abstract": "Entity resolution (ER) is a critical task in data management which identifies\nwhether multiple records refer to the same real-world entity. Despite its\nsignificance across domains such as healthcare, finance, and machine learning,\nimplementing effective ER systems remains challenging due to the abundance of\nmethodologies and tools, leading to a paradox of choice for practitioners. This\npaper proposes Resolvi, a reference architecture aimed at enhancing\nextensibility, interoperability, and scalability in ER systems. By analyzing\nexisting ER frameworks and literature, we establish a structured approach to\ndesigning ER solutions that address common challenges. Additionally, we explore\nbest practices for system implementation and deployment strategies to\nfacilitate largescale entity resolution. Through this work, we aim to provide a\nfoundational blueprint that assists researchers and practitioners in developing\nrobust, scalable ER systems while reducing the complexity of architectural\ndecisions.",
      "generated_abstract": "r introduces Resolvi, a reference architecture for scalable and\ninteroperable entity resolution. Resolvi is based on the concept of\nreference architecture, a concept that was introduced in the paper\n\"Reference-Based Design of Distributed Systems\" by D. Broutin and C.\nPapadakis. This paper provides a detailed description of the Resolvi\narchitecture. The architecture is divided into three layers: the\nDomain-Specific Layer (DSL), the Domain-Invariant Layer (DLL) and the\nCommon-Layer (CL). The DSL is responsible for creating and maintaining\nrepresentations of data, while the DLL abstracts away the details of the\nrepresentation from the application. The CL provides a common layer that\nencapsulates the common operations and data types between the DSL and DLL.\nThe architecture is designed to scale, with a focus on efficiency and\ninteroperability.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1941747572815534,
          "p": 0.25316455696202533,
          "f": 0.21978021486716592
        },
        "rouge-2": {
          "r": 0.03787878787878788,
          "p": 0.041666666666666664,
          "f": 0.03968253469387818
        },
        "rouge-l": {
          "r": 0.1941747572815534,
          "p": 0.25316455696202533,
          "f": 0.21978021486716592
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.03633v1",
      "true_abstract": "Autonomous motion planning under unknown nonlinear dynamics presents\nsignificant challenges. An agent needs to continuously explore the system\ndynamics to acquire its properties, such as reachability, in order to guide\nsystem navigation adaptively. In this paper, we propose a hybrid\nplanning-control framework designed to compute a feasible trajectory toward a\ntarget. Our approach involves partitioning the state space and approximating\nthe system by a piecewise affine (PWA) system with constrained control inputs.\nBy abstracting the PWA system into a directed weighted graph, we incrementally\nupdate the existence of its edges via affine system identification and reach\ncontrol theory, introducing a predictive reachability condition by exploiting\nprior information of the unknown dynamics. Heuristic weights are assigned to\nedges based on whether their existence is certain or remains indeterminate.\nConsequently, we propose a framework that adaptively collects and analyzes data\nduring mission execution, continually updates the predictive graph, and\nsynthesizes a controller online based on the graph search outcomes. We\ndemonstrate the efficacy of our approach through simulation scenarios involving\na mobile robot operating in unknown terrains, with its unknown dynamics\nabstracted as a single integrator model.",
      "generated_abstract": "This paper investigates motion planning and control of robotic manipulators\nwith unknown nonlinear dynamics. Traditional methods, such as reachability\nanalysis and reachable set planning, are inadequate for handling these\ncomplex problems. This paper introduces a novel approach, Predicted Reachability\n(PR), that employs a novel nonlinear reachability analysis technique. PR\nestimates the reachable set of a manipulator based on its model dynamics,\nallowing the robot to control its motion. By leveraging the PR estimates, the\nPR-based motion planning algorithm provides a more robust and flexible method\nfor robotic manipulator design and control, enabling the robot to adapt to\nunexpected environments and perform complex tasks. The proposed PR-based\nmotion planning and control methodology is validated through simulation and\nreal-world experiments, demonstrating its effectiveness in handling\nnonlinear motion planning and control problems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2109375,
          "p": 0.32926829268292684,
          "f": 0.25714285238276646
        },
        "rouge-2": {
          "r": 0.0335195530726257,
          "p": 0.05263157894736842,
          "f": 0.040955626645389556
        },
        "rouge-l": {
          "r": 0.2109375,
          "p": 0.32926829268292684,
          "f": 0.25714285238276646
        }
      }
    },
    {
      "paper_id": "cs.CV.q-bio/TO/2409.20407v3",
      "true_abstract": "Periorbital segmentation and distance prediction using deep learning allows\nfor the objective quantification of disease state, treatment monitoring, and\nremote medicine. However, there are currently no reports of segmentation\ndatasets for the purposes of training deep learning models with sub mm accuracy\non the regions around the eyes. All images (n=2842) had the iris, sclera, lid,\ncaruncle, and brow segmented by five trained annotators. Here, we validate this\ndataset through intra and intergrader reliability tests and show the utility of\nthe data in training periorbital segmentation networks. All the annotations are\npublicly available for free download. Having access to segmentation datasets\ndesigned specifically for oculoplastic surgery will permit more rapid\ndevelopment of clinically useful segmentation networks which can be leveraged\nfor periorbital distance prediction and disease classification. In addition to\nthe annotations, we also provide an open-source toolkit for periorbital\ndistance prediction from segmentation masks. The weights of all models have\nalso been open-sourced and are publicly available for use by the community.",
      "generated_abstract": "rbital region is crucial for retinal assessment and disease\nsuspicion in ophthalmology. However, manual segmentation is labor-intensive and\ndifficult for generalization, particularly in low-resource settings. We\nintroduce a dataset, Open-Source Periorbital Segmentation Dataset, designed to\naddress these limitations. The dataset consists of 350 unpaired periorbital\nimages, each annotated with the location and extent of each orbital region. We\nevaluate the dataset against four state-of-the-art segmentation methods:\nFast-RCNN, Mask-RCNN, CutMix, and Dual Path Mask-RCNN. Experimental results\ndemonstrate that our dataset achieves state-of-the-art performance on\nsegmentation, with an F1-score of 0.919,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15454545454545454,
          "p": 0.2328767123287671,
          "f": 0.18579234493117153
        },
        "rouge-2": {
          "r": 0.006622516556291391,
          "p": 0.011235955056179775,
          "f": 0.0083333286670165
        },
        "rouge-l": {
          "r": 0.15454545454545454,
          "p": 0.2328767123287671,
          "f": 0.18579234493117153
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/NA/2503.09898v1",
      "true_abstract": "Dynamic simulation plays a crucial role in power system transient stability\nanalysis, but traditional numerical integration-based methods are\ntime-consuming due to the small time step sizes. Other semi-analytical solution\nmethods, such as the Differential Transformation method, often struggle to\nselect proper orders and steps, leading to slow performance and numerical\ninstability. To address these challenges, this paper proposes a novel adaptive\ndynamic simulation approach for power system transient\n  stability analysis. The approach adds feedback control and optimization to\nselecting the step and order, utilizing the Differential Transformation method\nand a proportional-integral control strategy to control truncation errors.\nOrder selection is formulated as an optimization problem resulting in a\nvariable-step-optimal-order method that achieves significantly larger time step\nsizes without violating numerical stability. It is applied to three systems:\nthe IEEE 9-bus, 3-generator system, IEEE 39-bus, 10-generator system, and a\nPolish 2383-bus, 327-generator system, promising computational efficiency and\nnumerical robustness for large-scale power system is demonstrated in\ncomprehensive case studies.",
      "generated_abstract": "In this paper, a novel adaptive order differential transformation method is\ndeveloped to solve the time-dependent power system simulation problem. The\nmethod can be used to adaptively control the order of the differential\ntransformation equation and, thereby, reduce the computational cost. The\nadaptive order method is applied to solve the time-dependent power system\nsimulation problem, and the time-dependent model is obtained through\nsemi-analytical solutions. The proposed method can effectively reduce the\ncomputational cost while maintaining the accuracy of the solution. The\nperformance of the proposed method is verified by simulation examples. The\nresults show that the proposed method can effectively reduce the computational\ncost and maintain the accuracy of the solution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19444444444444445,
          "p": 0.38181818181818183,
          "f": 0.2576687071850654
        },
        "rouge-2": {
          "r": 0.034013605442176874,
          "p": 0.0625,
          "f": 0.044052858871703786
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.32727272727272727,
          "f": 0.22085889123414515
        }
      }
    },
    {
      "paper_id": "cs.LG.econ/EM/2502.14131v2",
      "true_abstract": "We study the problem of estimating Dynamic Discrete Choice (DDC) models, also\nknown as offline Maximum Entropy-Regularized Inverse Reinforcement Learning\n(offline MaxEnt-IRL) in machine learning. The objective is to recover reward or\n$Q^*$ functions that govern agent behavior from offline behavior data. In this\npaper, we propose a globally convergent gradient-based method for solving these\nproblems without the restrictive assumption of linearly parameterized rewards.\nThe novelty of our approach lies in introducing the Empirical Risk Minimization\n(ERM) based IRL/DDC framework, which circumvents the need for explicit state\ntransition probability estimation in the Bellman equation. Furthermore, our\nmethod is compatible with non-parametric estimation techniques such as neural\nnetworks. Therefore, the proposed method has the potential to be scaled to\nhigh-dimensional, infinite state spaces. A key theoretical insight underlying\nour approach is that the Bellman residual satisfies the Polyak-Lojasiewicz (PL)\ncondition -- a property that, while weaker than strong convexity, is sufficient\nto ensure fast global convergence guarantees. Through a series of synthetic\nexperiments, we demonstrate that our approach consistently outperforms\nbenchmark methods and state-of-the-art alternatives.",
      "generated_abstract": "vances in deep reinforcement learning (RL) have shown that gradient\nlearning can be used to train reward models, with promising implications for\noffline RL and inverse reinforcement learning (IRL). However, existing approaches\nface several limitations. First, gradient-based algorithms typically focus on\nthe single-step reward function, neglecting the long-term value of future\nrewards. Second, they often rely on empirical risk minimization (ERM), which is\nsubject to the curse of dimensionality. Third, they are designed for discrete\nactions, which does not capture the continuous-valued nature of offline RL\nreward functions. To address these limitations, we propose a novel\nempirical risk minimization (ERM) framework for offline RL and IRL.\nSpecifically, we develop a gradient-free approach that directly optimizes the\noffline reward function, without relying",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18518518518518517,
          "p": 0.2777777777777778,
          "f": 0.22222221742222234
        },
        "rouge-2": {
          "r": 0.011764705882352941,
          "p": 0.017857142857142856,
          "f": 0.014184392374630659
        },
        "rouge-l": {
          "r": 0.17037037037037037,
          "p": 0.25555555555555554,
          "f": 0.20444443964444453
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.04040v1",
      "true_abstract": "The fluid antenna system (FAS) has emerged as a disruptive technology for\nfuture wireless networks, offering unprecedented degrees of freedom (DoF)\nthrough the dynamic configuration of antennas in response to propagation\nenvironment variations. The integration of fluid antennas (FAs) with multiuser\nmultiple-input multiple-output (MU-MIMO) networks promises substantial weighted\nsum rate (WSR) gains via joint beamforming and FA position optimization.\nHowever, the joint design is challenging due to the strong coupling between\nbeamforming matrices and antenna positions. To address the challenge, we\npropose a novel block coordinate ascent (BCA)-based method in FA-assisted\nMU-MIMO networks. Specifically, we first employ matrix fractional programming\ntechniques to reformulate the original complex problem into a more tractable\nform. Then, we solve the reformulated problem following the BCA principle,\nwhere we develop a low-complexity majorization maximization algorithm capable\nof optimizing all FA positions simultaneously. To further reduce the\ncomputational, storage, and interconnection costs, we propose a decentralized\nimplementation for our proposed algorithm by utilizing the decentralized\nbaseband processing (DBP) architecture. Simulation results demonstrate that\nwith our proposed algorithm, the FA-assisted MU-MIMO system achieves up to a\n47% WSR improvement over conventional MIMO networks equipped with\nfixed-position antennas. Moreover, the decentralized implementation reduces\ncomputation time by approximately 70% and has similar performance compared with\nthe centralized implementation.",
      "generated_abstract": "r investigates the joint beamforming and antenna position\noptimization for fluid antenna-assisted MU-MIMO networks. To address the\ncomplexities of fluid antenna-assisted MU-MIMO networks, a novel framework is\nproposed to optimize the beamforming vectors and the antenna positions jointly\nwith the signal-to-interference-plus-noise ratio (SINR) maximization, which is\nformulated as a nonconvex optimization problem. To tackle the nonconvexity of\nthe problem, we propose a sequence of alternating optimization (AO) procedures\nfor solving the problem. The AO procedures are designed to be efficient and\neffective for solving the problem. Furthermore, we propose a novel SINR-adaptive\nbeamforming matrix design for the fluid antenna-assisted MU-MIMO networks. The\nproposed SINR-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19463087248322147,
          "p": 0.47540983606557374,
          "f": 0.2761904720684808
        },
        "rouge-2": {
          "r": 0.07425742574257425,
          "p": 0.16304347826086957,
          "f": 0.10204081202647063
        },
        "rouge-l": {
          "r": 0.18791946308724833,
          "p": 0.45901639344262296,
          "f": 0.26666666254467125
        }
      }
    },
    {
      "paper_id": "nlin.CD.physics/data-an/2503.07582v1",
      "true_abstract": "Small, forested catchments are prototypes of terrestrial ecosystems and have\nbeen studied in several disciplines of environmental sciences since several\ndecades. Time series of water and matter fluxes and nutrient concentrations\nfrom these systems exhibit a bewildering diversity of spatio-temporal patterns,\nindicating the intricate nature of processes acting on a large range of time\nscales. Nonlinear dynamics is an obvious framework to investigate catchment\ntime series. We analyze selected long-term data from three headwater catchments\nin the Bramke valley, Harz mountains, Lower Saxony in Germany at common\nbiweekly resolution for the period 1991 to 2023. For every time series, we\nperform gap filling, detrending and removal of the annual cycle using Singular\nSystem Analysis (SSA), and then calculate metrics based on ordinal pattern\nstatistics: the permutation entropy, permutation complexity and Fisher\ninformation, as well as their generalized versions (q-entropy and\n{\\alpha}-entropy). Further, the position of each variable in Tarnopolski\ndiagrams is displayed and compared to reference stochastic processes, like\nfractional Brownian motion, fractional Gaussian noise, and \\b{eta} noise. Still\nanother way of distinguishing deterministic chaos and structured noise, and\nquantifying the latter, is provided by the complexity from ordinal pattern\npositioned slopes (COPPS). We also construct Horizontal Visibility Graphs and\nestimate the exponent of the decay of the degree distribution. Taken together,\nthe analyses create a characterization of the dynamics of these systems which\ncan be scrutinized for universality, either across variables or between the\nthree geographically very close catchments.",
      "generated_abstract": "Environmental time series are a fundamental research tool for understanding\nenvironmental processes, but their complexity often goes unnoticed. The\ndifference between the complexity of a series and the complexity of a\nstatistical model, which depends on the number of parameters, can be\nsignificant. In this paper, we investigate the complexity of environmental\ntime series, focusing on the differences between environmental and statistical\nmodels. We examine the complexity of the time series in terms of the\nentropy, the Kolmogorov complexity, and the exponential complexity. We\nconsider a series of environmental time series, such as air temperature, and\ncompare them with time series of a stationary, Gaussian, and power law\nstatistical model. We analyze the differences in complexity and demonstrate how\nthe complexity of environmental time series can be used to determine the\ncomplexity of a statistical model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15028901734104047,
          "p": 0.37142857142857144,
          "f": 0.21399176544564688
        },
        "rouge-2": {
          "r": 0.04291845493562232,
          "p": 0.09523809523809523,
          "f": 0.05917159335019813
        },
        "rouge-l": {
          "r": 0.13872832369942195,
          "p": 0.34285714285714286,
          "f": 0.1975308600958527
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.05481v1",
      "true_abstract": "Large language models (LLMs) often generate inaccurate yet credible-sounding\ncontent, known as hallucinations. This inherent feature of LLMs poses\nsignificant risks, especially in critical domains. I analyze LLMs as a new\nclass of engineering products, treating hallucinations as a product attribute.\nI demonstrate that, in the presence of imperfect awareness of LLM\nhallucinations and misinformation externalities, net welfare improves when the\nmaximum acceptable level of LLM hallucinations is designed to vary with two\ndomain-specific factors: the willingness to pay for reduced LLM hallucinations\nand the marginal damage associated with misinformation.",
      "generated_abstract": "Large language models (LLMs) have emerged as a key technology in the field\nof natural language processing (NLP), transforming the way we interact with\nsoftware. However, they also pose a challenge for human users, who may be\nmisled by LLM-generated output. To address this, we propose a new definition\nof hallucination, which we define as the generation of output that deviates\nfrom the user's true intent. In this paper, we propose a new framework for\nexamining the impact of LLM hallucination standards on user trust. We explore\nthe implications of various hallucination standards across various LLMs and\ndomains. We further examine the impact of these standards on user trust, and\nhighlight the trade-offs that may be involved. We conclude by discussing\npotential solutions for enhancing LLM trustworthiness.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23880597014925373,
          "p": 0.1839080459770115,
          "f": 0.20779220287653913
        },
        "rouge-2": {
          "r": 0.08333333333333333,
          "p": 0.05982905982905983,
          "f": 0.06965173642830658
        },
        "rouge-l": {
          "r": 0.23880597014925373,
          "p": 0.1839080459770115,
          "f": 0.20779220287653913
        }
      }
    },
    {
      "paper_id": "math.DS.q-bio/MN/2409.12487v2",
      "true_abstract": "We present a systematic procedure for testing whether reaction networks\nexhibit non-expansivity or monotonicity. This procedure identifies explicit\nnorms under which a network is non-expansive or cones for which the system is\nmonotone-or provides proof that no such structures exist. Our approach\nreproduces known results, generates novel findings, and demonstrates that\ncertain reaction networks cannot exhibit monotonicity or non-expansivity with\nrespect to any cone or norm. Additionally, we establish a duality relationship\nwhich states that if a network is monotone, so is its dual network.",
      "generated_abstract": "In this paper, we explore the relationship between monotonicity and\nbehavior of reaction networks, focusing on the class of non-expansive networks\n(NE networks). We first introduce the notion of the monotonicity of a\nreaction network, and then formulate the problem of checking whether a given\nreaction network is monotone. We show that checking whether a given reaction\nnetwork is monotone is NP-hard, and we propose a polynomial-time algorithm to\ncheck whether a given reaction network is monotone. Our work is motivated by\nthe recent work of Chatterjee et al. [2024",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25396825396825395,
          "p": 0.3018867924528302,
          "f": 0.27586206400267543
        },
        "rouge-2": {
          "r": 0.012345679012345678,
          "p": 0.013157894736842105,
          "f": 0.01273884850825787
        },
        "rouge-l": {
          "r": 0.2222222222222222,
          "p": 0.2641509433962264,
          "f": 0.24137930538198582
        }
      }
    },
    {
      "paper_id": "math.RT.math/RT/2503.07809v1",
      "true_abstract": "For a permutation $w$ in the symmetric group $\\mathfrak{S}_{n}$, let $L(w)$\ndenote the simple highest weight module in the principal block of the BGG\ncategory $\\mathcal{O}$ for the Lie algebra $\\mathfrak{sl}_{n}(\\mathbb{C})$. We\nfirst prove that $L(w)$ is Kostant negative whenever $w$ consecutively contains\ncertain patterns. We then provide a complete answer to Kostant's problem in\ntype $A_{6}$ and show that the indecomposability conjecture also holds in type\n$A_{6}$, that is, applying an indecomposable projective functor to a simple\nmodule outputs either an indecomposable module or zero.",
      "generated_abstract": "This paper is an attempt to solve the conjecture of Kostant, which states\nthat for a given positive integer $n$, there exists a pattern in the set of\nconsecutive integers $\\mathbb{Z}_n$ such that for every two consecutive integers\n$x$ and $y$ in $\\mathbb{Z}_n$, there is an integer $z$ such that $z$ is a\nconsecutive integer in $\\mathbb{Z}_n$ and $x$ and $y$ are coprime and $z$ is\ndivisible by $n$. In this paper, we show that the number of consecutive integers\n$x$ and $y$ in $\\mathbb{Z}_n$ such that $x$ and $y$ are coprime and $x$ and $y$\nare divisible by $n$ is exactly $\\frac{(n-1)!}{n!}$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1875,
          "p": 0.24489795918367346,
          "f": 0.21238937561907756
        },
        "rouge-2": {
          "r": 0.036585365853658534,
          "p": 0.04,
          "f": 0.03821655551949435
        },
        "rouge-l": {
          "r": 0.171875,
          "p": 0.22448979591836735,
          "f": 0.19469026057482977
        }
      }
    },
    {
      "paper_id": "cs.CL.stat/ME/2503.04910v1",
      "true_abstract": "The emergence of powerful LLMs has led to a paradigm shift in Natural\nLanguage Understanding and Natural Language Generation. The properties that\nmake LLMs so valuable for these tasks -- creativity, ability to produce fluent\nspeech, and ability to quickly and effectively abstract information from large\ncorpora -- also present new challenges to evaluating their outputs. The rush to\nmarket has led teams to fall back on quick, cost-effective automatic\nevaluations which offer value, but do not obviate the need for human judgments\nin model training and evaluation. This paper argues that in cases in which end\nusers need to agree with the decisions made by ML models -- e.g. in toxicity\ndetection or extraction of main points for summarization -- models should be\ntrained and evaluated on data that represent the preferences of those users. We\nsupport this argument by explicating the role of human feedback in labeling and\njudgment tasks for model training and evaluation. First, we propose methods for\ndisentangling noise from signal in labeling tasks. Then we show that noise in\nlabeling disagreement can be minimized by adhering to proven methodological\nbest practices, while signal can be maximized to play an integral role in model\ntraining and evaluation tasks. Finally, we illustrate best practices by\nproviding a case study in which two guardrails classifiers are evaluated using\nhuman judgments to align final model behavior to user preferences. We aim for\nthis paper to provide researchers and professionals with guidelines to\nintegrating human judgments into their ML and generative AI evaluation toolkit,\nparticularly when working toward achieving accurate and unbiased features that\nalign with users' needs and expectations.",
      "generated_abstract": "igate the problem of aligning human preference models with\nhuman preference data. We consider a model that assigns a weight to each\npreference, and a preference data set that includes the weights of all\npreferences. We introduce a new method for model alignment, called\nmaximum signal alignment (MSA), which aims to maximize the sum of the\npreference weights in the alignment, subject to a constraint that the\npreference weights sum to one. We show that the MSA problem is NP-hard. We\nfurther propose a polynomial-time approximation scheme for MSA, which\nrequires a constant fraction of the total weight in the alignment. We\ndemonstrate the efficiency of our approximation algorithm for synthetic data\nsets and show that it outperforms previous algorithms. We further analyze the\nperformance of our algorithm and compare it to a recent model-based\napproach, showing that it outperforms it in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12048192771084337,
          "p": 0.26666666666666666,
          "f": 0.16597509944732367
        },
        "rouge-2": {
          "r": 0.01568627450980392,
          "p": 0.03125,
          "f": 0.02088772400929953
        },
        "rouge-l": {
          "r": 0.1144578313253012,
          "p": 0.25333333333333335,
          "f": 0.1576763442606017
        }
      }
    },
    {
      "paper_id": "cs.AI.q-fin/CP/2501.12399v1",
      "true_abstract": "Current financial Large Language Models (LLMs) struggle with two critical\nlimitations: a lack of depth in stock analysis, which impedes their ability to\ngenerate professional-grade insights, and the absence of objective evaluation\nmetrics to assess the quality of stock analysis reports. To address these\nchallenges, this paper introduces FinSphere, a conversational stock analysis\nagent, along with three major contributions: (1) Stocksis, a dataset curated by\nindustry experts to enhance LLMs' stock analysis capabilities, (2) AnalyScore,\na systematic evaluation framework for assessing stock analysis quality, and (3)\nFinSphere, an AI agent that can generate high-quality stock analysis reports in\nresponse to user queries. Experiments demonstrate that FinSphere achieves\nsuperior performance compared to both general and domain-specific LLMs, as well\nas existing agent-based systems, even when they are enhanced with real-time\ndata access and few-shot guidance. The integrated framework, which combines\nreal-time data feeds, quantitative tools, and an instruction-tuned LLM, yields\nsubstantial improvements in both analytical quality and practical applicability\nfor real-world stock analysis.",
      "generated_abstract": "This paper presents FinSphere, a novel conversational stock analysis agent\nbased on real-time database, enabling users to access comprehensive stock\ninformation through conversations. FinSphere is equipped with a comprehensive\nknowledge base of the stock market, including stocks, stock indices, and\nfinancial news, and a series of advanced algorithms that analyze financial\ndata, providing users with more accurate and personalized stock recommendations.\nThe agent is designed to be conversational, engaging users in a dynamic and\npersonalized manner. It uses natural language processing and machine learning\ntechniques to understand users' requests and respond with relevant stock\ninformation, ensuring a seamless user experience. By integrating artificial\nintelligence with human interaction, FinSphere provides a scalable,\ninteractive, and personalized solution for stock analysis and investment\ndecision-making.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17647058823529413,
          "p": 0.2413793103448276,
          "f": 0.2038834902662835
        },
        "rouge-2": {
          "r": 0.01948051948051948,
          "p": 0.02564102564102564,
          "f": 0.02214021649541918
        },
        "rouge-l": {
          "r": 0.15966386554621848,
          "p": 0.21839080459770116,
          "f": 0.18446601453812814
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.15945v1",
      "true_abstract": "A unified framework is provided for analysing check-all-that-apply (CATA)\nproduct data following the ``one citation, one vote\" principle. CATA data arise\nfrom studies where A consumers evaluate P products by describing samples by\nchecking all of the T terms that apply. Giving every citation the same weight,\nregardless of the assessor, product, or term, leads to analyses based on the L1\nnorm where the median absolute deviation is the measure of dispersion. Five\npermutation tests are proposed to answer the following questions. Do any\nproducts differ? For which terms do products differ? Within each of the terms,\nwhich products differ? Which product pairs differ? On which terms does each\nproduct pair differ? Additionally, we show how products and terms can be\nclustered following the ``one citation, one vote\" principle and how L1-norm\nprincipal component analysis (L1-norm PCA) can be applied to visualize CATA\nresults in few dimensions. Together, the permutation tests, clustering methods,\nand L1-norm PCA provide a unified approach. The proposed methods are\nillustrated using a data set in which 100 consumers evaluated 11 products using\n34 CATA terms.R code is provided to perform the analyses.",
      "generated_abstract": "etrics, a key feature is the check-all-that-apply (CATA) pattern,\nwhich occurs when a measurement is repeated many times for a single target\nmeasurement. This pattern is found in many sensors and can be a valuable\nmeasurement feature, as it can reduce the number of measurements needed to\nestimate the target variable. In this study, we present a novel method to\nanalyse CATA data using linear regression. We use the L1 norm method to\nestimate the parameters of the regression model, which enables us to obtain\npredictive models for the target variable and the number of measurements needed\nto estimate it. Our results show that the number of measurements needed to\nestimate the target variable varies between 0 and 100, with an average of 20.\nThis method provides a more efficient way to analyse CATA data, as it\nsimplifies the analysis of this pattern and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19166666666666668,
          "p": 0.2804878048780488,
          "f": 0.2277227674541713
        },
        "rouge-2": {
          "r": 0.040229885057471264,
          "p": 0.058333333333333334,
          "f": 0.047619042787727835
        },
        "rouge-l": {
          "r": 0.15,
          "p": 0.21951219512195122,
          "f": 0.1782178169591218
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.06404v1",
      "true_abstract": "Reinsurance optimization is critical for insurers to manage risk exposure,\nensure financial stability, and maintain solvency. Traditional approaches often\nstruggle with dynamic claim distributions, high-dimensional constraints, and\nevolving market conditions. This paper introduces a novel hybrid framework that\nintegrates {Generative Models}, specifically Variational Autoencoders (VAEs),\nwith {Reinforcement Learning (RL)} using Proximal Policy Optimization (PPO).\nThe framework enables dynamic and scalable optimization of reinsurance\nstrategies by combining the generative modeling of complex claim distributions\nwith the adaptive decision-making capabilities of reinforcement learning.\n  The VAE component generates synthetic claims, including rare and catastrophic\nevents, addressing data scarcity and variability, while the PPO algorithm\ndynamically adjusts reinsurance parameters to maximize surplus and minimize\nruin probability. The framework's performance is validated through extensive\nexperiments, including out-of-sample testing, stress-testing scenarios (e.g.,\npandemic impacts, catastrophic events), and scalability analysis across\nportfolio sizes. Results demonstrate its superior adaptability, scalability,\nand robustness compared to traditional optimization techniques, achieving\nhigher final surpluses and computational efficiency.\n  Key contributions include the development of a hybrid approach for\nhigh-dimensional optimization, dynamic reinsurance parameterization, and\nvalidation against stochastic claim distributions. The proposed framework\noffers a transformative solution for modern reinsurance challenges, with\npotential applications in multi-line insurance operations, catastrophe\nmodeling, and risk-sharing strategy design.",
      "generated_abstract": "r presents a hybrid framework for reinsurance optimization that\nintegrates generative models with reinforcement learning (RL) to enhance\nreinsurance pricing and portfolio management. The framework combines a\nmulti-agent reinforcement learning framework with a generative model for\nreinsurance pricing. The generative model simulates the pricing process based\non market conditions, including market dynamics, insurer behavior, and\nreinsurer behavior. The generative model is integrated with an RL agent that\nuses this simulated data to make decisions about portfolio reinsurance\npurchases. The RL agent uses a reinsurance portfolio strategy that balances\nrisk and profitability, while accounting for the market dynamics. This\nhybrid framework provides a more flexible and efficient approach to reinsurance\nportfolio management than traditional approaches, such as using a single\ngenerative model or a single R",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17721518987341772,
          "p": 0.3783783783783784,
          "f": 0.24137930600029733
        },
        "rouge-2": {
          "r": 0.024875621890547265,
          "p": 0.044642857142857144,
          "f": 0.031948877193398575
        },
        "rouge-l": {
          "r": 0.17721518987341772,
          "p": 0.3783783783783784,
          "f": 0.24137930600029733
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.16088v1",
      "true_abstract": "The question of whether insects experience pain has long been debated in\nneuroscience and animal behavior research. Increasing evidence suggests that\ninsects possess the ability to detect and respond to noxious stimuli,\nexhibiting behaviors indicative of pain perception. This study investigates the\nrelationship between pain stimuli and physiological responses in crickets\n(Gryllidae), focusing on heart rate (ECG) and brain wave (EEG) patterns. We\napplied a range of mechanical, chemical, thermal, and electrical stimuli to\ncrickets, recording ECG and EEG data while employing a deep learning-based\nmodel to classify pain levels. Our findings revealed significant heart rate\nchanges and EEG fluctuations in response to various stimuli, with the highest\nintensity stimuli inducing marked physiological stress. The AI-based analysis,\nutilizing AlexNet for EEG signal classification, achieved 90% accuracy in\ndistinguishing between resting, low-pain, and high-pain states. While no social\nsharing of pain was observed through ECG measurements, these results contribute\nto the growing body of evidence supporting insect nociception and offer new\ninsights into their physiological responses to external stressors. This\nresearch advances the understanding of insect pain mechanisms and demonstrates\nthe potential for AI-driven analysis in entomological studies.",
      "generated_abstract": "Insects have a range of tactile sensory systems and a wide range of\nsensory thresholds for pain. In this study, we measured the tactile sensitivity\nof the thorax, abdomen, and wings of several species of Aphidius (Hymenoptera)\nand evaluated the effect of temperature, agitation, and electric field on the\nsensory threshold. The results showed that the insects' pain thresholds were\nvery sensitive to temperature and agitation, and that electric fields could\nincrease the sensitivity of insects. We also studied the effect of electric\nfields on the sensory thresholds of the abdomen and wings, and the effect of\nelectric fields on the sensory threshold of the thorax. Our results show that\nelectric fields can significantly increase the sensory threshold of insects,\neven when the insects are not feeling any pain.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11278195488721804,
          "p": 0.22727272727272727,
          "f": 0.15075376441099986
        },
        "rouge-2": {
          "r": 0.01098901098901099,
          "p": 0.019417475728155338,
          "f": 0.01403508310347951
        },
        "rouge-l": {
          "r": 0.11278195488721804,
          "p": 0.22727272727272727,
          "f": 0.15075376441099986
        }
      }
    },
    {
      "paper_id": "eess.SY.q-fin/PM/2412.07688v1",
      "true_abstract": "Given the vital role that smart meter data could play in handling uncertainty\nin energy markets, data markets have been proposed as a means to enable\nincreased data access. However, most extant literature considers energy markets\nand data markets separately, which ignores the interdependence between them. In\naddition, existing data market frameworks rely on a trusted entity to clear the\nmarket. This paper proposes a joint energy and data market focusing on the\nday-ahead retailer energy procurement problem with uncertain demand. The\nretailer can purchase differentially-private smart meter data from consumers to\nreduce uncertainty. The problem is modelled as an integrated forecasting and\noptimisation problem providing a means of valuing data directly rather than\nvaluing forecasts or forecast accuracy. Value is determined by the Wasserstein\ndistance, enabling privacy to be preserved during the valuation and procurement\nprocess. The value of joint energy and data clearing is highlighted through\nnumerical case studies using both synthetic and real smart meter data.",
      "generated_abstract": "ers are critical to the operation of modern energy systems,\nproviding a range of services, including billing, metering, and customer\nservices. Increasingly, they are used to collect environmental data,\nfacilitating the collection of carbon emissions and other environmental\nmetrics. However, the data collected from these meters is often proprietary,\ncostly to collect, and subject to privacy and security threats. This paper\naddresses the problem of building a market for these data, while maintaining\ntheir privacy and security. We propose a novel privacy-preserving mechanism\nthat leverages a differentially private auction mechanism to auction off the\nenergy consumption and carbon emissions data, with the price set by a\nmarket-clearing mechanism. We show that this mechanism preserves privacy and\nsecurity while ensuring that the data is privately auctioned. Furthermore, we\nshow that the mechanism",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1792452830188679,
          "p": 0.23170731707317074,
          "f": 0.20212765465595306
        },
        "rouge-2": {
          "r": 0.013605442176870748,
          "p": 0.01639344262295082,
          "f": 0.01486988351902433
        },
        "rouge-l": {
          "r": 0.16981132075471697,
          "p": 0.21951219512195122,
          "f": 0.19148935678361265
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/GN/2502.01311v1",
      "true_abstract": "Transcription factors are proteins that regulate the expression of genes by\nbinding to specific genomic regions known as Transcription Factor Binding Sites\n(TFBSs), typically located in the promoter regions of those genes. Accurate\nprediction of these binding sites is essential for understanding the complex\ngene regulatory networks underlying various cellular functions. In this regard,\nmany deep learning models have been developed for such prediction, but there is\nstill scope of improvement. In this work, we have developed a deep learning\nmodel which uses pre-trained DNABERT, a Convolutional Neural Network (CNN)\nmodule, a Modified Convolutional Block Attention Module (MCBAM), a Multi-Scale\nConvolutions with Attention (MSCA) module and an output module. The pre-trained\nDNABERT is used for sequence embedding, thereby capturing the long-term\ndependencies in the DNA sequences while the CNN, MCBAM and MSCA modules are\nuseful in extracting higher-order local features. TFBS-Finder is trained and\ntested on 165 ENCODE ChIP-seq datasets. We have also performed ablation studies\nas well as cross-cell line validations and comparisons with other models. The\nexperimental results show the superiority of the proposed method in predicting\nTFBSs compared to the existing methodologies. The codes and the relevant\ndatasets are publicly available at\nhttps://github.com/NimishaGhosh/TFBS-Finder/.",
      "generated_abstract": "ification of transcription factor binding sites (TFBSs) is a\nimportant step in genomic analysis, enabling the characterization of transcription\nfactors and their interactions. While many TFBS prediction methods have been\ndeveloped, a systematic benchmark of TFBS prediction methods is still lacking.\nTo address this gap, we introduce TFBS-Finder, a novel TFBS prediction model\nbased on deep learning and DNABERT. TFBS-Finder leverages the power of DNABERT,\na powerful language model, to predict TFBSs. DNABERT employs a series of\nTransformer blocks to capture contextual information and capture long-range\ndependencies, achieving high accuracy in TFBS prediction. Additionally, we\nimplemented Convolutional Neural Networks (CNN) to enhance the prediction\naccuracy. We applied TFBS-F",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20833333333333334,
          "p": 0.379746835443038,
          "f": 0.26905829138892806
        },
        "rouge-2": {
          "r": 0.031088082901554404,
          "p": 0.058823529411764705,
          "f": 0.04067796157747824
        },
        "rouge-l": {
          "r": 0.18055555555555555,
          "p": 0.3291139240506329,
          "f": 0.23318385192704463
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/CC/2503.05062v1",
      "true_abstract": "Despite of tremendous research on decoding Reed-Solomon (RS) and algebraic\ngeometry (AG) codes under the random and adversary substitution error models,\nfew studies have explored these codes under the burst substitution error model.\nBurst errors are prevalent in many communication channels, such as wireless\nnetworks, magnetic recording systems, and flash memory. Compared to random and\nadversarial errors, burst errors often allow for the design of more efficient\ndecoding algorithms. However, achieving both an optimal decoding radius and\nquasi-linear time complexity for burst error correction remains a significant\nchallenge. The goal of this paper is to design (both list and probabilistic\nunique) decoding algorithms for RS and AG codes that achieve the Singleton\nbound for decoding radius while maintaining quasi-linear time complexity.\n  Our idea is to build a one-to-one correspondence between AG codes (including\nRS codes) and interleaved RS codes with shorter code lengths (or even constant\nlengths). By decoding the interleaved RS codes with burst errors, we derive\nefficient decoding algorithms for RS and AG codes. For decoding interleaved RS\ncodes with shorter code lengths, we can employ either the naive methods or\nexisting algorithms. This one-to-one correspondence is constructed using the\ngeneralized fast Fourier transform (G-FFT) proposed by Li and Xing (SODA 2024).\nThe G-FFT generalizes the divide-and-conquer technique from polynomials to\nalgebraic function fields. More precisely speaking, assume that our AG code is\ndefined over a function field $E$ which has a sequence of subfields\n$\\mathbb{F}_q(x)=E_r\\subseteq E_{r-1}\\subseteq \\cdots\\subset E_1\\subseteq\nE_0=E$ such that $E_{i-1}/E_i$ are Galois extensions for $1\\le i\\le r$. Then\nthe AG code based on $E$ can be transformed into an interleaved RS code over\nthe rational function field $\\mathbb{F}_q(x)$.",
      "generated_abstract": "We prove the first quasi-linear time decoding algorithm for RS and AG codes\nfor burst errors up to the Singleton bound. Our approach is based on\nsub-exponential time decoding of binary codes with short parity-check\nmatrices. For the RS case, we first reduce the problem to the recovery of the\nminimum distance sequence in a binary linear code. Then we use a reduction from\nlinear codes to binary linear codes. We construct binary linear codes with\nlength $O(1)$ and $O(n)$ for $n$ bits, where $n$ is the size of the input. Our\nalgorithm uses $O(1)$ time to decode the minimum distance sequence and $O(n)$\ntime to decode the parity-check matrix. We also provide a more efficient\ndecoding algorithm for the AG case.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17857142857142858,
          "p": 0.46153846153846156,
          "f": 0.25751072559081956
        },
        "rouge-2": {
          "r": 0.06172839506172839,
          "p": 0.14423076923076922,
          "f": 0.08645532721441108
        },
        "rouge-l": {
          "r": 0.17857142857142858,
          "p": 0.46153846153846156,
          "f": 0.25751072559081956
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.09740v1",
      "true_abstract": "In this paper, we prove a KAM theorem in a-posteriori format, using the\nparameterization method to look invariant tori in non-autonomous Hamiltonian\nsystems with $n$ degrees of freedom that depend periodically or\nquasi-periodically (QP) on time, with $\\ell$ external frequencies. Such a\nsystem is described by a Hamiltonian function in the $2n$-dimensional phase\nspace, $\\mathscr{M}$, that depends also on $\\ell$ angles, $\\varphi\\in\n\\mathbb{T}^\\ell$. We take advantage of the fibbered structure of the extended\nphase space $\\mathscr{M} \\times \\mathbb{T}^\\ell$. As a result of our approach,\nthe parameterization of tori requires the last $\\ell$ variables, to be precise\n$\\varphi$, while the first $2n$ components are determined by an invariance\nequation. This reduction decreases the dimension of the problem where the\nunknown is a parameterization from $2(n+\\ell)$ to $2n$. We employ a\nquasi-Newton method, in order to prove the KAM theorem. This iterative method\nbegins with an initial parameterization of an approximately invariant torus,\nmeaning it approximately satisfies the invariance equation. The approximation\nis refined by applying corrections that reduce quadratically the invariance\nequation error. This process converges to a torus in a complex strip of size\n$\\rho_\\infty$, provided suitable Diophantine $(\\gamma,\\tau)$ conditions and a\nnon-degeneracy condition on the torsion are met. Given the nature of the proof,\nthis provides a numerical method that can be effectively implemented on a\ncomputer, the details are given in the companion paper [CHP25]. This approach\nleverages precision and efficiency to compute invariant tori.",
      "generated_abstract": "In this paper, we propose a constructive approach to establish the KAM theorem\nfor Hamiltonian systems with a Lagrangian torus. Our method is based on\nreducing the problem to the study of a system of quasilinear ordinary differential\nequations (QP-ODEs) with a quadratic growth of the coefficients. The QP-ODEs\nare solved explicitly in the form of differential polynomials. The obtained\nresults are then used to establish the KAM theorem for Hamiltonian systems with\na Lagrangian torus.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15753424657534246,
          "p": 0.46938775510204084,
          "f": 0.23589743213464828
        },
        "rouge-2": {
          "r": 0.04888888888888889,
          "p": 0.1746031746031746,
          "f": 0.07638888547092029
        },
        "rouge-l": {
          "r": 0.1506849315068493,
          "p": 0.4489795918367347,
          "f": 0.22564102187823806
        }
      }
    },
    {
      "paper_id": "math.RA.math/GR/2503.06221v1",
      "true_abstract": "Let $\\mathbf{O}(\\mathbb{F})$ be the split octonion algebra over an\nalgebraically closed field $\\mathbb{F}$. For positive integers $k_1, k_2\\geq\n2$, we study surjectivity of the map $A_1(x^{k_1}) + A_2(y^{k_2}) \\in\n\\mathbf{O}(\\mathbb{F})\\langle x, y\\rangle$ on $\\mathbf{O}(\\mathbb{F})$. For\nthis, we use the orbit representatives of the ${G}_2(\\mathbb{F})$-action on\n$\\mathbf{O}(\\mathbb{F}) \\times \\mathbf{O}(\\mathbb{F}) $ for the tuple $(A_1,\nA_2)$, and characterize the ones which give a surjective map.",
      "generated_abstract": "In this paper we study polynomial maps on split octonion algebras. We\nidentify the kernel and the image of these maps. We obtain some basic\nproperties of these maps, which are also used to give a characterization of\nthe connected components of the image of these maps.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21153846153846154,
          "p": 0.3235294117647059,
          "f": 0.25581394870740953
        },
        "rouge-2": {
          "r": 0.06557377049180328,
          "p": 0.1,
          "f": 0.07920791600823479
        },
        "rouge-l": {
          "r": 0.21153846153846154,
          "p": 0.3235294117647059,
          "f": 0.25581394870740953
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/app-ph/2503.09048v1",
      "true_abstract": "In recent years, twisting has emerged as a new degree of freedom that plays\nan increasingly important role in Bloch bands of various physical systems.\nHowever, there is currently a lack of reports on the non-trivial physics of\ntopological degeneracy in twisted systems. In this work, we investigated the\nintrinsic physical correlation between twisting and topological degeneracy. We\nfound that twisting not only breaks the symmetry of the system but also\nintroduces topological degeneracy that does not exist under the original\nsymmetric system without twisting. Furthermore, the topological degeneracy can\nbe easily tuned through twisting. This new twist-induced topological degeneracy\ngives rise to a unique polarization-degenerate birefringent medium, wherein the\ntwist angle acts as a novel degree of freedom for dispersion and polarization\nmanagement of interface states. Exhibiting fascinating properties and\nexperimental feasibilities, our work points to new possibilities in the\nresearch of various topological physics in twisted photonics.",
      "generated_abstract": "We present a new method for generating topological degeneracies in\ntwo-dimensional optical waveguides by twisting the waveguide. We show how\nthis method can be used to create topological degeneracies in waveguides that\nare otherwise non-topological. By twisting the waveguide, we can engineer\ntopological degeneracies with a minimum of experimental effort. In addition, we\ndiscuss the implications of this method for the topological phase diagram of\noptical waveguides.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17346938775510204,
          "p": 0.38636363636363635,
          "f": 0.23943661544138076
        },
        "rouge-2": {
          "r": 0.02158273381294964,
          "p": 0.05084745762711865,
          "f": 0.030303026119274126
        },
        "rouge-l": {
          "r": 0.16326530612244897,
          "p": 0.36363636363636365,
          "f": 0.2253521083991272
        }
      }
    },
    {
      "paper_id": "math.CO.math/CT/2503.06722v1",
      "true_abstract": "In this paper we explore the algebraic structure and combinatorial properties\nof eulerian magnitude homology. First, we analyze the diagonality conditions of\neulerian magnitude homology, providing a characterization of complete graphs.\nThen, we construct the regular magnitude-path spectral sequence as the spectral\nsequence of the (filtered) injective nerve of the reachability category, and\nexplore its consequences. Among others, we show that such spectral sequence\nconverges to the complex of injective words on a digraph, and yields\ncharacterization results for the regular path homology of diagonal directed\ngraphs.",
      "generated_abstract": "We introduce the Eulerian magnitude homology of a compact oriented\n$G$-space $X$ and study its relationship to the $G$-invariant homology.\nSpecifically, we prove that the Eulerian magnitude homology is a chain complex\ngenerated by $G$-invariant homology classes of closed walks. We also study the\nrelation between Eulerian magnitude homology and regular path homology, and\nprove a diagnostic theorem for regular path homology of $X$. This result\nintroduces a novel concept of regular path homology for $G$-spaces and\nprovides a natural generalization of the classical notion of regular homology\nfor $G$-spaces.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26785714285714285,
          "p": 0.3,
          "f": 0.2830188629405483
        },
        "rouge-2": {
          "r": 0.075,
          "p": 0.07894736842105263,
          "f": 0.07692307192636456
        },
        "rouge-l": {
          "r": 0.23214285714285715,
          "p": 0.26,
          "f": 0.24528301388394458
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.physics/soc-ph/2503.05380v1",
      "true_abstract": "This paper presents a quantitative comparison of power grid reinforcement\nstrategies. We evaluate three approaches: (1) doubling transmission links\n(bridges) between different communities, (2) adding bypasses around weakly\nsynchronized nodes, and (3) reinforcing edges that trigger the largest cascade\nfailures. We use two different models of the Hungarian high-voltage network.\nThese models are built from the official data provided by the transmission\nsystem operator, thus eliminating the assumptions typically used in other\nstudies. The coupling strength distribution of the Hungarian models shows good\nagreement with our previous works using the European and North American grids.\n  Additionally, we examine the occurrence of Braess' paradox, where added\ntransmission capacity unexpectedly reduces overall stability. Our results show\nthat reinforcement through community-based bridge duplication yields the most\nsignificant improvements across all parameters. A visual comparison highlights\ndifferences between this method and traditional reinforcement approaches. To\nthe authors' knowledge, this is the first attempt to quantitatively compare\nresults of oscillator-based studies with those relying on power system analysis\nsoftware.\n  Characteristic results of line-cut simulations reveal cascade size\ndistributions with fat-tailed decays for medium coupling strengths, while\nexponential behavior emerges for small and large couplings. The observed\nexponents are reminiscent of the continuously changing exponents due to\nGriffiths effects near a hybrid type of phase transition.",
      "generated_abstract": "grid is vulnerable to severe disruptions caused by a wide range of\nevents, such as natural and anthropogenic hazards and cyberattacks. These\nevents can disrupt energy flows and result in blackouts, which could have\nsignificant economic, social, and environmental impacts. One of the most\neffective methods to mitigate these risks is to deploy power grid reinforcements,\nsuch as power lines, transformers, and switchyards. However, the\nquantitative assessment of these reinforcements is still in its infancy,\npartly because of the lack of rigorous experimental benchmarks and quantitative\nevaluations of their performance. In this work, we propose a benchmark for\nevaluating the performance of power grid reinforcements. The benchmark\nconsists of three scenarios: (i) a simple disruption scenario, (ii) a complex\ndisruption scenario,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11801242236024845,
          "p": 0.2235294117647059,
          "f": 0.15447154019267648
        },
        "rouge-2": {
          "r": 0.01951219512195122,
          "p": 0.034482758620689655,
          "f": 0.02492211376442472
        },
        "rouge-l": {
          "r": 0.11801242236024845,
          "p": 0.2235294117647059,
          "f": 0.15447154019267648
        }
      }
    },
    {
      "paper_id": "math.CO.cs/CG/2503.09919v1",
      "true_abstract": "We provide a family of $5$-dimensional prismatoids whose width grows linearly\nin the number of vertices. This provides a new infinite family of\ncounter-examples to the Hirsch conjecture whose excess width grows linearly in\nthe number of vertices, and answers a question of Matschke, Santos and Weibel.",
      "generated_abstract": "the class of problems where we are given a graph $G$ and a function\n$f:V(G)\\rightarrow \\mathbb{N}$ such that $f(x)\\geq m$ for all $x\\in V(G)$.\n$m$ is a constant that does not depend on $G$ and $f$. $m$ is called the\n\\textit{width} of $G$ and $f$ is called a \\textit{drumming function}.\nIn~\\cite{Bhatt2024drums}, it was proved that every graph $G$ has a drumming\nfunction $f$ for which the width is $\\Theta(m)$ if $G$ has $O(m^2)$ edges and\n$\\Omega(m)$ if $G$ has $O(m)$ edges.\n  We prove that the width of any $O(m^2)$-connected graph with $O(",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1935483870967742,
          "p": 0.10714285714285714,
          "f": 0.13793102989562703
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1935483870967742,
          "p": 0.10714285714285714,
          "f": 0.13793102989562703
        }
      }
    },
    {
      "paper_id": "physics.flu-dyn.physics/ao-ph/2503.03009v1",
      "true_abstract": "Wave breaking is a critical process in the upper ocean: an energy sink for\nthe surface wave field and a source for turbulence in the ocean surface\nboundary layer. We apply a novel multi-layer numerical solver resolving\nupper-ocean dynamics over scales from O(50cm) to O(1km), including a\nbroad-banded wave field and wave breaking. The present numerical study isolates\nthe effect of wave breaking and allows us to study the surface layer in\nwave-influenced and wave-breaking-dominated regimes. Following our previous\nwork showing wave breaking statistics in agreement with field observations, we\nextend the analysis to underwater breaking-induced turbulence and related\ndissipation (in freely decaying conditions). We observe a rich field of\nvorticity resulting from the turbulence generation by breaking waves. We\ndiscuss the vertical profiles of dissipation rate which are compared with field\nobservations, and propose an empirical universal shape function. Good agreement\nis found, further demonstrating that wave breaking can dominate turbulence\ngeneration in the near-surface layer. We examine the dissipation from different\nangles: the global dissipation of the wave field computed from the decaying\nwave field, the spectral dissipation from the fifth moment of breaking front\ndistribution, and a turbulence dissipation estimated from the underwater strain\nrate tensor. Finally, we consider how these different estimates can be\nunderstood as part of a coherent framework.",
      "generated_abstract": "We study the turbulent dynamics of a periodic wavepacket in a linear\nnon-stationary flow. We focus on the breaking of the wavepacket at the\ninterfaces and show that, when the wavepacket is close to a symmetry axis,\ninstabilities of the breaking process can lead to a significant dissipation of\nenergy in the flow. We find that the dissipation in the flow is strongly\ndependent on the ratio between the breaking frequency and the inertial\nfrequency. We also find that the dissipation is enhanced when the breaking\nfrequency is close to the inertial frequency. We find that the dissipation can\nbe very large when the wavepacket is close to the symmetry axis, which is\nimportant for turbulence models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16260162601626016,
          "p": 0.38461538461538464,
          "f": 0.22857142439444902
        },
        "rouge-2": {
          "r": 0.04081632653061224,
          "p": 0.09411764705882353,
          "f": 0.05693949755955503
        },
        "rouge-l": {
          "r": 0.14634146341463414,
          "p": 0.34615384615384615,
          "f": 0.2057142815373062
        }
      }
    },
    {
      "paper_id": "stat.ML.cs/SI/2503.09660v1",
      "true_abstract": "Point signatures based on the Laplacian operators on graphs, point clouds,\nand manifolds have become popular tools in machine learning for graphs,\nclustering, and shape analysis. In this work, we propose a novel point\nsignature, the power spectrum signature, a measure on $\\mathbb{R}$ defined as\nthe squared graph Fourier transform of a graph signal. Unlike eigenvectors of\nthe Laplacian from which it is derived, the power spectrum signature is\ninvariant under graph automorphisms. We show that the power spectrum signature\nis stable under perturbations of the input graph with respect to the\nWasserstein metric. We focus on the signature applied to classes of indicator\nfunctions, and its applications to generating descriptive features for vertices\nof graphs. To demonstrate the practical value of our signature, we showcase\nseveral applications in characterizing geometry and symmetries in point cloud\ndata, and graph regression problems.",
      "generated_abstract": "spectrum of a graph is a spectral function that characterizes its\ncharacteristic power-law decay. In this paper, we study the power spectrum of\ngraphs using the spectral representation. We propose a new graph spectral\nsignature that allows us to capture the behavior of the power spectrum in a\ngraph in a more compact manner. This signature is a function of the graph and\nits Laplacian matrix. We prove that this function is a Lipschitz function, and\nthat it can be computed in polynomial time. We also show that the graph spectral\nsignature of a graph is the graph spectral signature of the Laplacian matrix of\nthe graph. We prove that this signature is also a Lipschitz function and\ncomputable in polynomial time. We demonstrate the effectiveness of these\nsignatures by comparing their performance on real-world graphs. The paper\nalso includes a numerical study to show how the graph spectral signature and\nthe Lapla",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27472527472527475,
          "p": 0.36764705882352944,
          "f": 0.31446540390965555
        },
        "rouge-2": {
          "r": 0.10687022900763359,
          "p": 0.11764705882352941,
          "f": 0.11199999501152022
        },
        "rouge-l": {
          "r": 0.25274725274725274,
          "p": 0.3382352941176471,
          "f": 0.28930817120525304
        }
      }
    },
    {
      "paper_id": "math.ST.q-bio/PE/2501.16526v1",
      "true_abstract": "Ancestral inference for branching processes in random environments involves\ndetermining the ancestor distribution parameters using the population sizes of\ndescendant generations. In this paper, we introduce a new methodology for\nancestral inference utilizing the generalized method of moments. We demonstrate\nthat the estimator's behavior is critically influenced by the coefficient of\nvariation of the environment sequence. Furthermore, despite the process's\nevolution being heavily dependent on the offspring means of various\ngenerations, we show that the joint limiting distribution of the ancestor and\noffspring estimators of the mean, under appropriate centering and scaling,\ndecouple and converge to independent Gaussian random variables when the ratio\nof the number of generations to the logarithm of the number of replicates\nconverges to zero. Additionally, we provide estimators for the limiting\nvariance and illustrate our findings through numerical experiments and data\nfrom Polymerase Chain Reaction experiments and COVID-19 data.",
      "generated_abstract": "In this paper, we study the problem of ancestral inference for branching\nprocesses in random environments, focusing on the case of Poisson-based\nprocesses. We introduce the first theoretical framework for ancestral inference\nin this setting, which relies on a novel algorithm based on the Langevin\nequation. We prove that the algorithm converges in probability to the true\nancestral process, and establish a convergence rate for the algorithm's\niterates. We also provide a rigorous analysis of the bias and variance of the\nalgorithm. Finally, we apply the algorithm to study the dynamics of the\nlogistic map, and demonstrate its effectiveness for both simulating random\nenvironments and learning the true ancestral process.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2604166666666667,
          "p": 0.3787878787878788,
          "f": 0.30864197048010983
        },
        "rouge-2": {
          "r": 0.11940298507462686,
          "p": 0.16,
          "f": 0.13675213185769614
        },
        "rouge-l": {
          "r": 0.22916666666666666,
          "p": 0.3333333333333333,
          "f": 0.27160493344307274
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2503.07203v1",
      "true_abstract": "Network pharmacology (NP) explores pharmacological mechanisms through\nbiological networks. Multi-omics data enable multi-layer network construction\nunder diverse conditions, requiring integration into NP analyses. We developed\nPOINT, a novel NP platform enhanced by multi-omics biological networks,\nadvanced algorithms, and knowledge graphs (KGs) featuring network-based and\nKG-based analytical functions. In the network-based analysis, users can perform\nNP studies flexibly using 1,158 multi-omics biological networks encompassing\nproteins, transcription factors, and non-coding RNAs across diverse cell line-,\ntissue- and disease-specific conditions. Network-based analysis-including\nrandom walk with restart (RWR), GSEA, and diffusion profile (DP) similarity\nalgorithms-supports tasks such as target prediction, functional enrichment, and\ndrug screening. We merged networks from experimental sources to generate a\npre-integrated multi-layer human network for evaluation. RWR demonstrated\nsuperior performance with a 33.1% average ranking improvement over the\nsecond-best algorithm, PageRank, in identifying known targets across 2,002\ndrugs. Additionally, multi-layer networks significantly improve the ability to\nidentify FDA-approved drug-disease pairs compared to the single-layer network.\nFor KG-based analysis, we compiled three high-quality KGs to construct POINT\nKG, which cross-references over 90% of network-based predictions. We\nillustrated the platform's capabilities through two case studies. POINT bridges\nthe gap between multi-omics networks and drug discovery; it is freely\naccessible at http://point.gene.ac/.",
      "generated_abstract": "to conduct a more comprehensive exploration of the biological\nphenomenon, it is crucial to provide the biological process and molecular\nnetworks in a unified manner. This is where the POINT platform comes in.\nPOINT provides a unified platform for biological knowledge and data,\nenabling researchers to explore and analyze the biological process and\nmolecular networks underlying various biological phenomena. Additionally, POINT\nintegrates multiple omics data, including RNA sequencing, ChIP-seq, and\nCRISPR-Cas9-mediated gene knockout data, enabling researchers to conduct\nmultiple experiments on a single platform, thereby facilitating the\ncomprehensive exploration of biological phenomena. Furthermore, POINT\nintegrates knowledge graphs, such as KG2, to enhance the understanding of the\nbiological process and molecular networks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11392405063291139,
          "p": 0.29508196721311475,
          "f": 0.1643835576247369
        },
        "rouge-2": {
          "r": 0.010101010101010102,
          "p": 0.02247191011235955,
          "f": 0.013937277951172262
        },
        "rouge-l": {
          "r": 0.11392405063291139,
          "p": 0.29508196721311475,
          "f": 0.1643835576247369
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/SC/2503.10416v1",
      "true_abstract": "Runtime repeated recursion unfolding was recently introduced as a\njust-in-time program transformation strategy that can achieve super-linear\nspeedup. So far, the method was restricted to single linear direct recursive\nrules in the programming language Constraint Handling Rules (CHR). In this\ncompanion paper, we generalize the technique to multiple recursion and to\nmultiple recursive rules and provide an implementation of the generalized\nmethod in the logic programming language Prolog.\n  The basic idea of the approach is as follows: When a recursive call is\nencountered at runtime, the recursive rule is unfolded with itself and this\nprocess is repeated with each resulting unfolded rule as long as it is\napplicable to the current call. In this way, more and more recursive steps are\ncombined into one recursive step. Then an interpreter applies these rules to\nthe call starting from the most unfolded rule. For recursions which have\nsufficiently simplifyable unfoldings, a super-linear can be achieved, i.e. the\ntime complexity is reduced.\n  We implement an unfolder, a generalized meta-interpreter and a novel\nround-robin rule processor for our generalization of runtime repeated recursion\nunfolding with just ten clauses in Prolog. We illustrate the feasibility of our\ntechnique with worst-case time complexity estimates and benchmarks for some\nbasic classical algorithms that achieve a super-linear speedup.",
      "generated_abstract": "Recursive algorithms in Prolog are often represented by recursion unfoldings\nwith a finite number of steps. This paper introduces a generalization of\nthis approach to super-linear speedup. We first show that the standard\nrepresentation of a recursive algorithm can be generalized to a sequence of\nnon-deterministic recursive unfoldings with a linear number of steps. Next, we\nintroduce a novel optimization strategy that allows us to generalize the\nrepresentation of a recursive algorithm to a sequence of non-deterministic\nrecursive unfoldings with a super-linear number of steps. The performance of\nour generalization approach is evaluated on a set of 14 benchmarks from the\nALMOST family. We show that our approach yields a super-linear speedup by\napproximately 200-500x in the worst case.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.256,
          "p": 0.48484848484848486,
          "f": 0.33507852950851136
        },
        "rouge-2": {
          "r": 0.07106598984771574,
          "p": 0.14432989690721648,
          "f": 0.09523809081655814
        },
        "rouge-l": {
          "r": 0.232,
          "p": 0.4393939393939394,
          "f": 0.3036649169430663
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2410.23275v1",
      "true_abstract": "We introduce a novel Dynamic Graph Neural Network (DGNN) architecture for\nsolving conditional $m$-steps ahead forecasting problems in temporal financial\nnetworks. The proposed DGNN is validated on simulated data from a temporal\nfinancial network model capturing stylized features of Interest Rate Swaps\n(IRSs) transaction networks, where financial entities trade swap contracts\ndynamically and the network topology evolves conditionally on a reference rate.\nThe proposed model is able to produce accurate conditional forecasts of net\nvariation margins up to a $21$-day horizon by leveraging conditional\ninformation under pre-determined stress test scenarios. Our work shows that the\nnetwork dynamics can be successfully incorporated into stress-testing\npractices, thus providing regulators and policymakers with a crucial tool for\nsystemic risk monitoring.",
      "generated_abstract": "The ongoing evolution of financial markets has created a new dynamic\nand challenging scenario for financial institutions: the uncertainty surrounding\nthe occurrence of margin calls. In this paper, we propose a novel methodology\nbased on Dynamic Graph Neural Networks to forecast margin calls in a risk-aware\nmanner. Our approach integrates deep learning techniques with graph-based\nmethods, offering a more comprehensive and robust approach to forecasting\nmargin-related events. Additionally, we analyze the impact of margin call\nfrequency and margin call size on the probability of occurrence. Our results\nshow that the model's accuracy improves with the increase in margin call\nfrequency, reaching 75.34% in the highest frequency case. This finding supports\nour hypothesis that the more frequent the margin calls, the higher the\nprobability of occurrence.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1875,
          "p": 0.21428571428571427,
          "f": 0.19999999502222232
        },
        "rouge-2": {
          "r": 0.035398230088495575,
          "p": 0.03418803418803419,
          "f": 0.03478260369716519
        },
        "rouge-l": {
          "r": 0.1875,
          "p": 0.21428571428571427,
          "f": 0.19999999502222232
        }
      }
    },
    {
      "paper_id": "cs.GT.cs/CR/2503.10185v1",
      "true_abstract": "Following the publication of Bitcoin's arguably most famous attack, selfish\nmining, various works have introduced mechanisms to enhance blockchain systems'\ngame theoretic resilience. Some reward mechanisms, like FruitChains, have been\nshown to be equilibria in theory. However, their guarantees assume\nnon-realistic parameters and their performance degrades significantly in a\npractical deployment setting. In this work we introduce a reward allocation\nmechanism, called Proportional Splitting (PRS), which outperforms existing\nstate of the art. We show that, for large enough parameters, PRS is an\nequilibrium, offering the same theoretical guarantees as the state of the art.\nIn addition, for practical, realistically small, parameters, PRS outperforms\nall existing reward mechanisms across an array of metrics. We implement PRS on\ntop of a variant of PoEM, a Proof-of-Work (PoW) protocol that enables a more\naccurate estimation of each party's mining power compared to e.g., Bitcoin. We\nthen evaluate PRS both theoretically and in practice. On the theoretical side,\nwe show that our protocol combined with PRS is an equilibrium and guarantees\nfairness, similar to FruitChains. In practice, we compare PRS with an array of\nexisting reward mechanisms and show that, assuming an accurate estimation of\nthe mining power distribution, it outperforms them across various\nwell-established metrics. Finally, we realize this assumption by approximating\nthe power distribution via low-work objects called \"workshares\" and quantify\nthe tradeoff between the approximation's accuracy and storage overhead.",
      "generated_abstract": "This paper presents a novel method for allocating rewards to tasks in\nadaptive systems. Our method is based on the concept of proportional\nsplitting, which enables a task to receive a greater share of a reward if it\nexceeds a predefined threshold. We demonstrate that proportional splitting\nenables efficient resource allocation in adaptive systems, enabling the\nachievement of optimal resource allocation while preserving task safety and\nadaptability. We demonstrate our method by implementing a simple adaptive\nsystem and evaluating the performance of our method with respect to the\nperformance of an existing method that does not use proportional splitting.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14965986394557823,
          "p": 0.3384615384615385,
          "f": 0.20754716555936287
        },
        "rouge-2": {
          "r": 0.014018691588785047,
          "p": 0.03296703296703297,
          "f": 0.019672126960710377
        },
        "rouge-l": {
          "r": 0.1292517006802721,
          "p": 0.2923076923076923,
          "f": 0.17924527876691007
        }
      }
    },
    {
      "paper_id": "quant-ph.cond-mat/other/2503.09292v1",
      "true_abstract": "In the pursuit of robust quantum computing, we put forth a platform based on\nphotonic qubits in a circuit-QED environment. Specifically, we propose a\nversatile two-qubit gate based on two cavities coupled via a transmon,\nconstituting a selective number-dependent phase gate operating on the in-phase\neigenmodes of the two cavities, the Eigen-SNAP gate. This gate natively\noperates in the dispersive coupling regime of the cavities and the transmon,\nand operates by driving the transmon externally, to imprint desired phases on\nthe number states. As an example for the utility of the Eigen-SNAP gate, we\nimplement a $\\sqrt{\\text{SWAP}}$ gate on a system of two logical bosonic qubits\nencoded in the cavities. Further, we use numerical optimization to determine\nthe optimal implementation of the $\\sqrt{\\text{SWAP}}$. We find that the\nfidelities of these optimal protocols are only limited by the coherence times\nof the system's components. These findings pave the way to continuous variable\nquantum computing in cavity-transmon systems.",
      "generated_abstract": "nic-qubit technology has emerged as a promising platform for the\ndevice-independent quantum computing, and the realization of a scalable\nqubit-cavity-transmon architecture has been a long-standing challenge. Here,\nwe propose a novel Eigen-SNAP gate for photonic qubits in a cavity-transmon\nsystem. The gate is constructed based on the interference of the\nsingle-photon-level eigenstates of the cavity field and the transmon qubit,\nwhich are both realized through the quantum dot transmon qubit. The gate is\ndemonstrated on a single photonic qubit and is found to be efficient, with a\nefficiency of 92.6\\%. This gate is shown to be equivalent to the NAP gate,\nwhich has been proposed for the NAP-type quantum computing. Our gate offers\npromising prospects for scal",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2604166666666667,
          "p": 0.33783783783783783,
          "f": 0.29411764214256064
        },
        "rouge-2": {
          "r": 0.1095890410958904,
          "p": 0.14678899082568808,
          "f": 0.12549019118369878
        },
        "rouge-l": {
          "r": 0.22916666666666666,
          "p": 0.2972972972972973,
          "f": 0.2588235244955018
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09367v1",
      "true_abstract": "Shi, Walsh and Yu demonstrated that any dense circuit graph contains a large\nnear-triangulation. We extend the result to $2$-connected plane graphs, thereby\naddressing a question posed by them. Using the result, we prove that the planar\nTu\\'{a}n number of $2C_k$ is $\\left[3-\\Theta(k^{\\log_23})^{-1}\\right]n$ when\n$k\\geq 5$.",
      "generated_abstract": "The $2$-connected planar graphs of order $n$ are denoted by $P_n$, and the\nplanar\n  Tur\u00e1n number of a graph $G$ is the number of $2$-connected planar graphs with\nthe same vertex set. In this paper, we study the planar Tur\u00e1n number of the\nclasses of $2C_k$ and $3C_k$, where $k \\geq 2$. In particular, we prove that\n$P_n$ is dense in $2C_k$ and $3C_k$ for $k \\geq 2$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.35714285714285715,
          "p": 0.3488372093023256,
          "f": 0.35294117147128035
        },
        "rouge-2": {
          "r": 0.1111111111111111,
          "p": 0.08771929824561403,
          "f": 0.09803921075547892
        },
        "rouge-l": {
          "r": 0.3333333333333333,
          "p": 0.32558139534883723,
          "f": 0.32941175970657444
        }
      }
    },
    {
      "paper_id": "cs.MA.cs/MA/2503.08728v1",
      "true_abstract": "Multi-agent reinforcement learning (MARL) has shown significant potential in\ntraffic signal control (TSC). However, current MARL-based methods often suffer\nfrom insufficient generalization due to the fixed traffic patterns and road\nnetwork conditions used during training. This limitation results in poor\nadaptability to new traffic scenarios, leading to high retraining costs and\ncomplex deployment. To address this challenge, we propose two algorithms:\nPLight and PRLight. PLight employs a model-based reinforcement learning\napproach, pretraining control policies and environment models using predefined\nsource-domain traffic scenarios. The environment model predicts the state\ntransitions, which facilitates the comparison of environmental features.\nPRLight further enhances adaptability by adaptively selecting pre-trained\nPLight agents based on the similarity between the source and target domains to\naccelerate the learning process in the target domain. We evaluated the\nalgorithms through two transfer settings: (1) adaptability to different traffic\nscenarios within the same road network, and (2) generalization across different\nroad networks. The results show that PRLight significantly reduces the\nadaptation time compared to learning from scratch in new TSC scenarios,\nachieving optimal performance using similarities between available and target\nscenarios.",
      "generated_abstract": "ignal control is a complex and non-linear problem that involves\nsignals, traffic, and infrastructure. This paper introduces an RL-based\napproach to traffic signal control, where the traffic signal controller is\ntrained through reinforcement learning. The controller is equipped with a\npolicy network and a value network, which are trained through reinforcement\nlearning. The policy network is designed to optimize the policy and the value\nnetwork is designed to optimize the policy. The proposed controller is\nevaluated through a simulated traffic signal control system and is found to\nachieve good performance in terms of stability and safety. The controller\nalso demonstrates the ability to adapt to changes in the traffic signal\nconfiguration and traffic flow. The results indicate that the proposed\ncontroller is effective in improving the performance of traffic signal\ncontrol, particularly in terms of stability and safety. This study provides a\nnovel approach for traffic signal control through reinforcement",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.176,
          "p": 0.3142857142857143,
          "f": 0.22564102103879036
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.04201680672268908,
          "f": 0.034013600623583454
        },
        "rouge-l": {
          "r": 0.168,
          "p": 0.3,
          "f": 0.21538461078238014
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2503.00391v1",
      "true_abstract": "In this working paper, I developed a suite of macroeconomic models that shed\nlight on the intricate relationship between economic development, health, and\nfertility. These innovative models conceptualize health as an intermediate\ngood, paving the way for new interpretations of dynamic socio-economic\nphenomena, particularly the non-monotonic effects of health on economic and\npopulation growth. The evolving dynamic interactions among economic growth,\npopulation, and health during the early stages of human development have been\nwell interpreted in this research.",
      "generated_abstract": "r investigates the evolution of health investment in a stylized\ndata set of 107 countries from 1960 to 2022, using a structural dynamic\nmodels approach. The results reveal that the demand for health services has\nevolved over time, with countries investing more in health services when they\nare experiencing a low economic growth rate. The findings also show that, in\nmost countries, health care spending has a strong positive correlation with\npopulation size, which suggests that countries with a higher population size\nwill tend to invest more in health care. Additionally, the analysis reveals\nthat countries with a higher fertility rate tend to invest more in health\nservices. These findings have important implications for health policy\nformulation, particularly in countries with a higher fertility rate. The results\nof the study suggest that countries with a higher fertility rate may need to\ninvest more",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27419354838709675,
          "p": 0.2125,
          "f": 0.23943661479865114
        },
        "rouge-2": {
          "r": 0.012987012987012988,
          "p": 0.008928571428571428,
          "f": 0.010582005753480549
        },
        "rouge-l": {
          "r": 0.24193548387096775,
          "p": 0.1875,
          "f": 0.21126760071414413
        }
      }
    },
    {
      "paper_id": "astro-ph.IM.astro-ph/IM/2503.10106v1",
      "true_abstract": "This work presents GalProTE, a proof-of-concept Machine Learning model\nutilizing a Transformer Encoder to determine stellar age, metallicity, and dust\nattenuation from optical spectra. Designed for large astronomical surveys,\nGalProTE significantly accelerates processing while maintaining accuracy. Using\nthe E-MILES spectral library, we construct a dataset of 111,936 diverse\ntemplates by expanding 636 simple stellar population models with varying\nextinction, spectral combinations, and noise modifications. This ensures robust\ntraining over 4750 to 7100 Angstrom at 2.5 Angstrom resolution. GalProTE\nemploys four parallel attention-based encoders with varying kernel sizes to\ncapture spectral features. On synthetic test data, it achieves a mean squared\nerror (MSE) of 0.27% between input and predicted spectra. Validation on\nPHANGS-MUSE galaxies NGC4254 and NGC5068 confirms its ability to extract\nphysical parameters efficiently, with residuals averaging -0.02% and 0.28% and\nstandard deviations of 4.3% and 5.3%, respectively. To contextualize these\nresults, we compare GalProTE's age, metallicity, and dust attenuation maps with\npPXF, a state-of-the-art spectral fitting tool. While pPXF requires\napproximately 11 seconds per spectrum, GalProTE processes one in less than 4\nmilliseconds, offering a 2750 times speedup and consuming 68 times less power\nper spectrum. The strong agreement between pPXF and GalProTE highlights the\npotential of machine learning to enhance traditional methods, paving the way\nfor faster, energy-efficient, and scalable analyses of galactic properties in\nmodern surveys.",
      "generated_abstract": "uce GalProTE, a novel tool for analyzing the properties of galaxies\nwithin our Galaxy and their environment. GalProTE is based on the transformer\nencoder, which has proven effective for analyzing large-scale cosmological\ndatasets. In GalProTE, we leverage the ability of the transformer encoder to\ncapture dependencies across multiple dimensions. In particular, we explore\nhow the phase-space of a galaxy's kinematics and its position within the\ndistribution of galaxies within the Milky Way affects its star-formation and\nmetallicity. We apply GalProTE to three galaxy datasets: the Hubble Ultra Deep\nField, the Sloan Digital Sky Survey, and the Large Synoptic Survey Telescope\n(LSST). Our results show that galaxy properties can be significantly\nreconstructed using only kinematical measurements, such as the angular velocity\ndispersion,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09941520467836257,
          "p": 0.18888888888888888,
          "f": 0.1302681947152862
        },
        "rouge-2": {
          "r": 0.0091324200913242,
          "p": 0.017391304347826087,
          "f": 0.011976043388972263
        },
        "rouge-l": {
          "r": 0.09941520467836257,
          "p": 0.18888888888888888,
          "f": 0.1302681947152862
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.07649v1",
      "true_abstract": "We propose a method to learn the nonlinear impulse responses to structural\nshocks using neural networks, and apply it to uncover the effects of US\nfinancial shocks. The results reveal substantial asymmetries with respect to\nthe sign of the shock. Adverse financial shocks have powerful effects on the US\neconomy, while benign shocks trigger much smaller reactions. Instead, with\nrespect to the size of the shocks, we find no discernible asymmetries.",
      "generated_abstract": "the impact of macroeconomic shocks on financial markets using\nmachine learning. We develop a model that combines state-space modeling and\nmachine learning to predict changes in asset prices and the level of the\nfederal funds rate. The model is trained using daily macroeconomic data from\nthe Federal Reserve Bank of New York and the Treasury Department. We find that\nmachine learning can successfully predict the level of the federal funds rate\nwith an error of 1.6 percentage points, a 95% confidence interval of -1.8 to\n0.3, and a mean absolute error of 1.13%. Using a larger dataset, we find that\nthe model performs similarly, with a mean absolute error of 1.11%. The\nmodel-predicted changes in asset prices are consistent with the observed\neffects of macroeconomic shocks on asset prices. In addition, we find that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3,
          "p": 0.19480519480519481,
          "f": 0.23622046766693544
        },
        "rouge-2": {
          "r": 0.046153846153846156,
          "p": 0.026785714285714284,
          "f": 0.03389830043729516
        },
        "rouge-l": {
          "r": 0.26,
          "p": 0.16883116883116883,
          "f": 0.20472440467480946
        }
      }
    },
    {
      "paper_id": "quant-ph.cond-mat/stat-mech/2503.10308v1",
      "true_abstract": "We consider learnability transitions in monitored quantum systems that\nundergo noisy evolution, subject to a global strong symmetry -- i.e., in\naddition to the measuring apparatus, the system can interact with an unobserved\nenvironment, but does not exchange charge with it. As in the pure-state\nsetting, we find two information-theoretic phases -- a sharp (fuzzy) phase in\nwhich an eavesdropper can rapidly (slowly) learn the symmetry charge. However,\nbecause the dynamics is noisy, both phases can be simulated efficiently using\ntensor networks. Indeed, even when the true dynamics is unitary, introducing\nnoise by hand allows an eavesdropper to efficiently learn the symmetry charge\nfrom local measurements, as we demonstrate. We identify the fuzzy phase in this\nsetting as a mixed-state phase that exhibits spontaneous strong-to-weak\nsymmetry breaking.",
      "generated_abstract": "e the learnability transition in quantum systems monitored in\nexcitation-free noisy quantum dynamics, where the system is initially in an\narbitrary mixed state. We consider two cases: (i) the dynamics is a\nsingle-parameter quantum map, and (ii) it is a general mixed-state quantum\nmap. The transition from mixed to pure states is well understood, with the\ntransition from mixed to pure states being a monotonic function of the\nevolution time. We demonstrate that the transition from mixed to pure states is\nalso a monotonic function of the initial mixed state, but with a non-monotonic\ncurve, and we present evidence for a possible mixed-state learnability\ntransition. This transition is driven by the evolution time, and it is\naccompanied by a transition from mixed to pure states. We use this transition\nto construct a learning algorithm for the mixed-state learnability\ntransitions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3076923076923077,
          "p": 0.3888888888888889,
          "f": 0.3435582772765253
        },
        "rouge-2": {
          "r": 0.04918032786885246,
          "p": 0.05357142857142857,
          "f": 0.051282046291183186
        },
        "rouge-l": {
          "r": 0.27472527472527475,
          "p": 0.3472222222222222,
          "f": 0.3067484613256051
        }
      }
    },
    {
      "paper_id": "cs.AI.cs/GT/2503.09858v1",
      "true_abstract": "This paper investigates the complex interplay between AI developers,\nregulators, users, and the media in fostering trustworthy AI systems. Using\nevolutionary game theory and large language models (LLMs), we model the\nstrategic interactions among these actors under different regulatory regimes.\nThe research explores two key mechanisms for achieving responsible governance,\nsafe AI development and adoption of safe AI: incentivising effective regulation\nthrough media reporting, and conditioning user trust on commentariats'\nrecommendation. The findings highlight the crucial role of the media in\nproviding information to users, potentially acting as a form of \"soft\"\nregulation by investigating developers or regulators, as a substitute to\ninstitutional AI regulation (which is still absent in many regions). Both\ngame-theoretic analysis and LLM-based simulations reveal conditions under which\neffective regulation and trustworthy AI development emerge, emphasising the\nimportance of considering the influence of different regulatory regimes from an\nevolutionary game-theoretic perspective. The study concludes that effective\ngovernance requires managing incentives and costs for high quality\ncommentaries.",
      "generated_abstract": "This study examines the risks of media-based AI systems and proposes\nregulatory frameworks to mitigate them. We develop a framework for analyzing\nresponsible AI governance using game theory and large language models (LLMs).\nOur framework identifies key media-based AI risks, including discriminatory\nAI systems, media-based AI misinformation, and AI-mediated hate speech. We\nexplain the role of regulatory interventions in mitigating these risks by\nstrengthening media-based AI governance. The findings suggest that\nregulatory interventions are crucial to mitigate the risks of media-based AI\nsystems. We provide concrete recommendations for governments, AI research\nlabs, and media organizations, emphasizing the need for robust regulatory\nmechanisms to address these risks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2631578947368421,
          "p": 0.43478260869565216,
          "f": 0.3278688477613545
        },
        "rouge-2": {
          "r": 0.052980132450331126,
          "p": 0.08602150537634409,
          "f": 0.06557376577432177
        },
        "rouge-l": {
          "r": 0.23684210526315788,
          "p": 0.391304347826087,
          "f": 0.2950819625154528
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.03973v1",
      "true_abstract": "Range-only Simultaneous Localisation and Mapping (RO-SLAM) is of interest due\nto its practical applications in ultra-wideband (UWB) and Bluetooth Low Energy\n(BLE) localisation in terrestrial and aerial applications and acoustic beacon\nlocalisation in submarine applications. In this work, we consider a mobile\nrobot equipped with an inertial measurement unit (IMU) and a range sensor that\nmeasures distances to a collection of fixed landmarks. We derive an equivariant\nfilter (EqF) for the RO-SLAM problem based on a symmetry Lie group that is\ncompatible with the range measurements. The proposed filter does not require\nbootstrapping or initialisation of landmark positions, and demonstrates\nrobustness to the no-prior situation. The filter is demonstrated on a\nreal-world dataset, and it is shown to significantly outperform a\nstate-of-the-art EKF alternative in terms of both accuracy and robustness.",
      "generated_abstract": "y sensor-based navigation (SEN) systems are a type of robotic\nsystems that utilize a single sensor to estimate the position of the robot\nrelative to its environment. A common challenge in these systems is the\nintegration of inaccurate sensor information with the navigation task.\nEquivariant filter designs are a class of Kalman filters that have been shown\nto mitigate the influence of sensor noise on navigation performance. In this\npaper, we propose a novel equivariant filter design that integrates the\nestimated state of the robot with the estimated robot's position in a way that\nensures that the filtered position estimate is invariant to the orientation of\nthe robot. This design is derived under the assumption that the robot's\norientation is constant in time. We demonstrate the effectiveness of this\ndesign by using it to integrate sensor data from a robot navigating a\ntwo-dimensional grid environment. The results show that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22580645161290322,
          "p": 0.24705882352941178,
          "f": 0.23595505118987511
        },
        "rouge-2": {
          "r": 0.046875,
          "p": 0.04285714285714286,
          "f": 0.04477611441301013
        },
        "rouge-l": {
          "r": 0.21505376344086022,
          "p": 0.23529411764705882,
          "f": 0.22471909613369537
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.10419v1",
      "true_abstract": "In motion simulation, motion cueing algorithms are used for the trajectory\nplanning of the motion simulator platform, where workspace limitations prevent\ndirect reproduction of reference trajectories. Strategies such as motion\nwashout, which return the platform to its center, are crucial in these\nsettings. For serial robotic MSPs with highly nonlinear workspaces, it is\nessential to maximize the efficient utilization of the MSPs kinematic and\ndynamic capabilities. Traditional approaches, including classical washout\nfiltering and linear model predictive control, fail to consider\nplatform-specific, nonlinear properties, while nonlinear model predictive\ncontrol, though comprehensive, imposes high computational demands that hinder\nreal-time, pilot-in-the-loop application without further simplification. To\novercome these limitations, we introduce a novel approach using deep\nreinforcement learning for motion cueing, demonstrated here for the first time\nin a 6-degree-of-freedom setting with full consideration of the MSPs kinematic\nnonlinearities. Previous work by the authors successfully demonstrated the\napplication of DRL to a simplified 2-DOF setup, which did not consider\nkinematic or dynamic constraints. This approach has been extended to all 6 DOF\nby incorporating a complete kinematic model of the MSP into the algorithm, a\ncrucial step for enabling its application on a real motion simulator. The\ntraining of the DRL-MCA is based on Proximal Policy Optimization in an\nactor-critic implementation combined with an automated hyperparameter\noptimization. After detailing the necessary training framework and the\nalgorithm itself, we provide a comprehensive validation, demonstrating that the\nDRL MCA achieves competitive performance against established algorithms.\nMoreover, it generates feasible trajectories by respecting all system\nconstraints and meets all real-time requirements with low...",
      "generated_abstract": "n cueing (MC) system is a crucial component of advanced driver\nassistance systems (ADAS), enabling the autonomous vehicles (AVs) to\nprovide guidance to human drivers. In this paper, a novel MC-based reinforcement\nlearning (RL) algorithm is proposed to enhance the dynamic stability and\nreal-time capability of the MC system. Specifically, the MC system is\nformulated as a continuous-time Markov decision process (CTMDP), which is\nformulated as a reinforcement learning (RL) problem with a finite horizon and\na finite state space. The discrete action space is divided into two parts,\nnamely, the policy action space and the policy update action space. The\npolicy action space is defined as a set of MC policies that can be applied to\nthe current state of the CTMDP. The policy update action space is defined as a\nset of MC policies that can",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13872832369942195,
          "p": 0.32,
          "f": 0.1935483828775365
        },
        "rouge-2": {
          "r": 0.02032520325203252,
          "p": 0.047619047619047616,
          "f": 0.028490024296881324
        },
        "rouge-l": {
          "r": 0.12716763005780346,
          "p": 0.29333333333333333,
          "f": 0.177419350619472
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/TR/2411.13564v1",
      "true_abstract": "According to The Exchange Act, 1934 unlawful insider trading is the abuse of\naccess to privileged corporate information. While a blurred line between\n\"routine\" the \"opportunistic\" insider trading exists, detection of strategies\nthat insiders mold to maneuver fair market prices to their advantage is an\nuphill battle for hand-engineered approaches. In the context of detailed\nhigh-dimensional financial and trade data that are structurally built by\nmultiple covariates, in this study, we explore, implement and provide detailed\ncomparison to the existing study (Deng et al. (2019)) and independently\nimplement automated end-to-end state-of-art methods by integrating principal\ncomponent analysis to the random forest (PCA-RF) followed by a standalone\nrandom forest (RF) with 320 and 3984 randomly selected, semi-manually labeled\nand normalized transactions from multiple industry. The settings successfully\nuncover latent structures and detect unlawful insider trading. Among the\nmultiple scenarios, our best-performing model accurately classified 96.43\npercent of transactions. Among all transactions the models find 95.47 lawful as\nlawful and $98.00$ unlawful as unlawful percent. Besides, the model makes very\nfew mistakes in classifying lawful as unlawful by missing only 2.00 percent. In\naddition to the classification task, model generated Gini Impurity based\nfeatures ranking, our analysis show ownership and governance related features\nbased on permutation values play important roles. In summary, a simple yet\npowerful automated end-to-end method relieves labor-intensive activities to\nredirect resources to enhance rule-making and tracking the uncaptured unlawful\ninsider trading transactions. We emphasize that developed financial and trading\nfeatures are capable of uncovering fraudulent behaviors.",
      "generated_abstract": "aper, we propose a novel Random Forest (RF) model to detect and\nidentify unlawful insider trading (IU) events in the financial market. The\nmain challenge in this task is the difficulty in accurately identifying\nunlawful insider trading events. In our study, we propose a novel RF model\nwhich incorporates the knowledge of expert systems (ES) to improve the model's\nability to detect unlawful insider trading events. The performance of our\nproposed model is evaluated using the Random Forest model with the help of\nthe HAR(1) time series model. The proposed model's performance is compared to\nthe state-of-the-art methods. The results show that the proposed model\noutperforms the state-of-the-art methods in terms of AUC, sensitivity, and\nspecificity. The results also demonstrate that the proposed model has a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15204678362573099,
          "p": 0.38235294117647056,
          "f": 0.21757321768596494
        },
        "rouge-2": {
          "r": 0.029288702928870293,
          "p": 0.06481481481481481,
          "f": 0.04034581703826171
        },
        "rouge-l": {
          "r": 0.13450292397660818,
          "p": 0.3382352941176471,
          "f": 0.1924686151755047
        }
      }
    },
    {
      "paper_id": "cs.FL.cs/LO/2503.04525v1",
      "true_abstract": "We give an active learning algorithm for deterministic one-counter automata\n(DOCAs) where the learner can ask the teacher membership and minimal\nequivalence queries. The algorithm called OL* learns a DOCA in time polynomial\nin the size of the smallest DOCA, recognising the target language.\n  All existing algorithms for learning DOCAs, even for the subclasses of\ndeterministic real-time one-counter automata (DROCAs) and visibly one-counter\nautomata (VOCAs), in the worst case, run in exponential time with respect to\nthe size of the DOCA under learning. Furthermore, previous learning algorithms\nare ``grey-box'' algorithms relying on an additional query type - counter value\nquery - where the teacher returns the counter value reached on reading a given\nword. In contrast, our algorithm is a ``black-box'' algorithm.\n  It is known that the minimisation of VOCAs is NP-hard. However, OL* can be\nused for approximate minimisation of DOCAs. In this case, the output size is at\nmost polynomial in the size of a minimal DOCA.",
      "generated_abstract": "learning deterministic one-counter automata (D-OCAs) with a single\ncounter, with a focus on the case of deterministic automata (DA) with a single\ncounter, and we propose two learning algorithms, based on the so-called\ndeterministic tree construction (DTC) and the deterministic binary tree\nconstruction (DBTC), respectively. Both algorithms require polynomial time to\nconstruct a D-OCAs with a single counter. Our first algorithm is an extension of\nthe standard DTC algorithm to D-OCAs with a single counter, which we call the\nDA-DTC algorithm. In the second algorithm, we replace the standard DBTC\nconstruction with a new deterministic binary tree construction (DBTC-BT)\nalgorithm, which is based on the idea of the deterministic binary tree\nconstruction (DBTC) algorithm for deterministic automata (DA). The DBTC-BT",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22580645161290322,
          "p": 0.35,
          "f": 0.2745097991541715
        },
        "rouge-2": {
          "r": 0.04861111111111111,
          "p": 0.07526881720430108,
          "f": 0.059071725189339695
        },
        "rouge-l": {
          "r": 0.21505376344086022,
          "p": 0.3333333333333333,
          "f": 0.26143790372933495
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.08100v1",
      "true_abstract": "We study contests in which two groups compete to win (or not to win) a\ngroup-specific public-good/bad prize. Each player in the groups can exert two\ntypes of effort: one to help her own group win the prize, and one to sabotage\nher own group's chances of winning it. The players in the groups choose their\neffort levels simultaneously and independently. We introduce a specific form of\ncontest success function that determines each group's probability of winning\nthe prize, taking into account players' sabotage activities. We show that two\ntypes of purestrategy Nash equilibrium occur, depending on parameter values:\none without sabotage activities and one with sabotage activities. In the first\ntype, only the highest-valuation player in each group expends positive effort,\nwhereas, in the second type, only the lowest-valuation player in each group\nexpends positive effort.",
      "generated_abstract": "We study a public good with a group-specific bad prize that has two types of\ncones, one for each group. The group members can give their votes to a\nnon-profit organization that has a public-good private good. We show that a\nminimum number of votes is needed to guarantee the existence of a public-good\nprivate good, or a non-profit organization, that is private-good and public-good\nin the presence of group-specific sabotage. We also show that a minimum number\nof votes is needed to guarantee that the group-specific sabotage is always\ncanceled, and we provide a characterization of the minimum number of votes\nneeded to guarantee that a group-specific sabotage cancels only one of the\ngroup-specific bad prizes.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2804878048780488,
          "p": 0.41818181818181815,
          "f": 0.33576641855186745
        },
        "rouge-2": {
          "r": 0.06896551724137931,
          "p": 0.0898876404494382,
          "f": 0.07804877557453926
        },
        "rouge-l": {
          "r": 0.25609756097560976,
          "p": 0.38181818181818183,
          "f": 0.3065693382598967
        }
      }
    },
    {
      "paper_id": "cs.LG.q-fin/CP/2412.01062v1",
      "true_abstract": "High-frequency trading (HFT) represents a pivotal and intensely competitive\ndomain within the financial markets. The velocity and accuracy of data\nprocessing exert a direct influence on profitability, underscoring the\nsignificance of this field. The objective of this work is to optimise the\nreal-time processing of data in high-frequency trading algorithms. The dynamic\nfeature selection mechanism is responsible for monitoring and analysing market\ndata in real time through clustering and feature weight analysis, with the\nobjective of automatically selecting the most relevant features. This process\nemploys an adaptive feature extraction method, which enables the system to\nrespond and adjust its feature set in a timely manner when the data input\nchanges, thus ensuring the efficient utilisation of data. The lightweight\nneural networks are designed in a modular fashion, comprising fast\nconvolutional layers and pruning techniques that facilitate the expeditious\ncompletion of data processing and output prediction. In contrast to\nconventional deep learning models, the neural network architecture has been\nspecifically designed to minimise the number of parameters and computational\ncomplexity, thereby markedly reducing the inference time. The experimental\nresults demonstrate that the model is capable of maintaining consistent\nperformance in the context of varying market conditions, thereby illustrating\nits advantages in terms of processing speed and revenue enhancement.",
      "generated_abstract": "development of high-frequency trading (HFT) algorithms has led to\nhigh-stakes financial market competition and a significant impact on market\nliquidity. This paper explores the optimization of real-time data processing\nin HFT algorithms. First, we propose a new framework to analyze and optimize\nthe real-time data processing strategy of HFT algorithms, focusing on the\neffectiveness of data filtering, data fusion, and data aggregation. Second,\nusing the proposed framework, we analyze and optimize the real-time data\nprocessing strategy of the Delta-Navigator algorithm, a HFT algorithm based on\nthe Heston model. Third, based on the proposed framework, we further explore the\neffectiveness of data filtering, data fusion, and data aggregation in the\nHeston algorithm. The results show that data filtering can effectively improve\nthe real-time data processing performance of the Heston algorithm, while data\nfusion",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.34328358208955223,
          "f": 0.22439023950220113
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.06,
          "f": 0.04026845191658083
        },
        "rouge-l": {
          "r": 0.15942028985507245,
          "p": 0.3283582089552239,
          "f": 0.21463414194122551
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2502.19839v1",
      "true_abstract": "We consider the problem of estimating complex statistical latent variable\nmodels using variational Bayes methods. These methods are used when exact\nposterior inference is either infeasible or computationally expensive, and they\napproximate the posterior density with a family of tractable distributions. The\nparameters of the approximating distribution are estimated using optimisation\nmethods. This article develops a flexible Gaussian mixture variational\napproximation, where we impose sparsity in the precision matrix of each\nGaussian component to reflect the appropriate conditional independence\nstructure in the model. By introducing sparsity in the precision matrix and\nparameterising it using the Cholesky factor, each Gaussian mixture component\nbecomes parsimonious (with a reduced number of non-zero parameters), while\nstill capturing the dependence in the posterior distribution. Fast estimation\nmethods based on global and local variational boosting moves combined with\nnatural gradients and variance reduction methods are developed. The local\nboosting moves adjust an existing mixture component, and optimisation is only\ncarried out on a subset of the variational parameters of a new component. The\nsubset is chosen to target improvement of the current approximation in aspects\nwhere it is poor. The local boosting moves are fast because only a small number\nof variational parameters need to be optimised. The efficacy of the approach is\nillustrated by using simulated and real datasets to estimate generalised linear\nmixed models and state space models.",
      "generated_abstract": "This paper introduces Fast Variational Boosting (FVB), a new fast algorithm\nfor latent variable models. FVB is based on the Variational Bayes framework,\nwhich combines the benefits of Bayesian and frequentist approaches. It is\nefficient in practice, allowing for fast inference with large datasets. In\naddition, FVB provides a straightforward way to estimate the log-likelihood of\na model, making it a valuable tool for evaluating model performance. By\nimproving upon existing algorithms, FVB offers a more efficient and flexible\napproach for latent variable modeling.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20454545454545456,
          "p": 0.4153846153846154,
          "f": 0.2741116707052489
        },
        "rouge-2": {
          "r": 0.029556650246305417,
          "p": 0.07407407407407407,
          "f": 0.042253517049444946
        },
        "rouge-l": {
          "r": 0.1893939393939394,
          "p": 0.38461538461538464,
          "f": 0.25380710217733005
        }
      }
    },
    {
      "paper_id": "astro-ph.IM.astro-ph/CO/2503.09842v1",
      "true_abstract": "Radiowave Observations on the Lunar Surface of the photo-Electron Sheath\ninstrument (ROLSES- 1) onboard the Intuitive Machines' Odysseus lunar lander\nrepresents NASA's first radio telescope on the Moon, and the first United\nStates spacecraft landing on the lunar surface in five decades. Despite a host\nof challenges, ROLSES-1 managed to collect a small amount of data over\nfractions of one day during cruise phase and two days on the lunar surface with\nfour monopole stacer antennas that were in a non-ideal deployment. All antennas\nrecorded shortwave radio transmissions breaking through the Earth's ionosphere\n-- or terrestrial technosignatures -- from spectral and raw waveform data.\nThese technosignatures appear to be modulated by density fluctuations in the\nEarth's ionosphere and could be used as markers when searching for\nextraterrestrial intelligence from habitable exoplanets. After data reduction\nand marshaling a host of statistical and sampling techniques, five minutes of\nraw waveforms from the least noisy antenna were used to generate covariances\nconstraining both the antenna parameters and the amplitude of the low-frequency\nisotropic galactic spectrum. ROLSES- 2 and LuSEE-Night, both lunar radio\ntelescopes launching later in the decade, will have significant upgrades from\nROLSES-1 and will be set to take unprecedented measurements of the\nlow-frequency sky, lunar surface, and constrain the cosmological 21-cm signal.\nROLSES-1 represents a trailblazer for lunar radio telescopes, and many of the\nstatistical tools and data reduction techniques presented in this work will be\ninvaluable for upcoming lunar radio telescope missions.",
      "generated_abstract": "ROLSES-1 is the first NASA mission to land on the Moon, and will observe the\nstellar background in the lunar sky. The instrument is equipped with a\nhigh-frequency radio telescope to observe terrestrial technosignatures. In this\npaper, we present the ROLSES-1 mission, including the payload, science\nobjectives, and mission architecture. We also describe the ground station and\nsatellite configurations. We discuss the expected instrument performance and\ndata quality, as well as the challenges and opportunities of conducting\nastronomical observations on the Moon. Finally, we provide an overview of the\ntechnical challenges and potential solutions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14093959731543623,
          "p": 0.31343283582089554,
          "f": 0.19444444016503779
        },
        "rouge-2": {
          "r": 0.049773755656108594,
          "p": 0.12087912087912088,
          "f": 0.07051281638087631
        },
        "rouge-l": {
          "r": 0.1342281879194631,
          "p": 0.29850746268656714,
          "f": 0.18518518090577854
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/QM/2502.13398v1",
      "true_abstract": "Despite recent advancements, most computational methods for molecule\noptimization are constrained to single- or double-property optimization tasks\nand suffer from poor scalability and generalizability to novel optimization\ntasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable\nout-of-domain generalizability to novel tasks. To demonstrate LLMs' potential\nfor molecule optimization, we introduce $\\mathtt{MoMUInstruct}$, the first\nhigh-quality instruction-tuning dataset specifically focused on complex\nmulti-property molecule optimization tasks. Leveraging $\\mathtt{MoMUInstruct}$,\nwe develop $\\mathtt{GeLLM^3O}$s, a series of instruction-tuned LLMs for\nmolecule optimization. Extensive evaluations across 5 in-domain and 5\nout-of-domain tasks demonstrate that $\\mathtt{GeLLM^3O}$s consistently\noutperform state-of-the-art baselines. $\\mathtt{GeLLM^3O}$s also exhibit\noutstanding zero-shot generalization to unseen tasks, significantly\noutperforming powerful closed-source LLMs. Such strong generalizability\ndemonstrates the tremendous potential of $\\mathtt{GeLLM^3O}$s as foundational\nmodels for molecule optimization, thereby tackling novel optimization tasks\nwithout resource-intensive retraining. $\\mathtt{MoMUInstruct}$, models, and\ncode are accessible through https://github.com/ninglab/GeLLMO.",
      "generated_abstract": "guage Models (LLMs) have shown significant promise in molecular\noptimization, but existing methods often rely on specialized architectures or\nhand-crafted features. To address this limitation, we propose $\\mathtt{GeLLM^3O}$,\na generic LLM for multi-property molecule optimization, which incorporates\nhigh-order molecular interactions into the language modeling head. $\\mathtt{GeLLM^3O}$\noffers significant improvements over existing LLMs, achieving an\n$\\mathtt{Acc}_{3,100}$ score of 64.3% on the DrugDiscovery dataset. $\\mathtt{GeLLM^3O}$\nemploys a multi-head architecture with a multi-property encoding layer, a\nmulti-property decoding layer, and a multi-property language modeling head,\nwhich improves the model's ability to encode",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17708333333333334,
          "p": 0.23943661971830985,
          "f": 0.20359280948330896
        },
        "rouge-2": {
          "r": 0.024,
          "p": 0.03488372093023256,
          "f": 0.028436014128165057
        },
        "rouge-l": {
          "r": 0.17708333333333334,
          "p": 0.23943661971830985,
          "f": 0.20359280948330896
        }
      }
    },
    {
      "paper_id": "math.LO.math/GN/2502.20887v1",
      "true_abstract": "We prove that it is consistent with ZFC that for every non-decreasing\nfunction $f:[0,1]\\to [0,1]$, each subset of $[0,1]$ of cardinality $\\mathfrak\nc$ contains a set of cardinality $\\mathfrak c$ on which $f$ is uniformly\ncontinuous. We show that this statement follows from the assumptions that\n$\\mathfrak d^* < \\mathfrak c$ and $\\mathfrak c$ is regular, where $\\mathfrak\nd^*\\leq \\mathfrak d$ is the smallest cardinality $\\kappa$ such that any two\ndisjoint countable dense sets in the Cantor set can be separated by sets each\nof which is an intersection of at most $\\kappa$-many open sets in the Cantor\nset. We establish also that $\\mathfrak d^*=\\min\\{\\mathfrak u, \\mathfrak\nd\\}=\\min\\{\\mathfrak r, \\mathfrak d\\}$, thus giving an alternative proof of the\nlatter equality established by J. Aubrey in 2004.",
      "generated_abstract": "In this note we consider the uniform continuity of monotone functions. We\nprove that the uniform continuity of monotone functions is preserved under\nsmall perturbations. This result is a consequence of a new generalization of\nthe classical result of L. Lusin on the uniform continuity of monotone functions\nwith respect to a pointwise measurable function.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13414634146341464,
          "p": 0.3142857142857143,
          "f": 0.18803418384104037
        },
        "rouge-2": {
          "r": 0.02586206896551724,
          "p": 0.06818181818181818,
          "f": 0.037499996012500425
        },
        "rouge-l": {
          "r": 0.0975609756097561,
          "p": 0.22857142857142856,
          "f": 0.1367521325589891
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/PL/2503.02768v1",
      "true_abstract": "We develop a denotational model for programs that have standard programming\nconstructs such as conditionals and while-loops, as well as probabilistic and\nconcurrent commands. Whereas semantic models for languages with either\nconcurrency or randomization are well studied, their combination is limited to\nlanguages with bounded loops. Our work is the first to consider both\nrandomization and concurrency for a language with unbounded looping constructs.\nThe interaction between Boolean tests (arising from the control flow\nstructures), probabilistic actions, and concurrent execution creates challenges\nin generalizing previous work on pomsets and convex languages, prominent models\nfor those effects, individually. To illustrate the generality of our model, we\nshow that it recovers a typical powerdomain semantics for concurrency, as well\nas the convex powerset semantics for probabilistic nondeterminism.",
      "generated_abstract": "t the first formal semantics for probabilistic and concurrent\nprograms, providing a framework for reasoning about the probabilistic and\nconcurrent behaviour of these programs. Our semantics is based on a denotational\ninterpretation of the program languages, where the interpretation is an\nabstract interpretation of the program variables. This approach enables the\nformalisation of probabilistic and concurrent behaviour, while preserving\nintractability properties of existing approaches. We give a formalisation of\nprobabilistic and concurrent programs using the denotational semantics,\nproviding an efficient and expressive model for the semantics. We also present a\nrepresentation of concurrent programs as a linear logic formula. We also\npresent a way to represent probabilistic concurrent programs, where the\nprobability of an event is encoded as a function of the previous states of the\nprogram. Our approach enables formalisation of the semantics of probabilistic\nand concurrent programs, which is not the case in the literature. We",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23595505617977527,
          "p": 0.3,
          "f": 0.264150938467624
        },
        "rouge-2": {
          "r": 0.059322033898305086,
          "p": 0.059322033898305086,
          "f": 0.05932202889830551
        },
        "rouge-l": {
          "r": 0.2247191011235955,
          "p": 0.2857142857142857,
          "f": 0.2515723221154227
        }
      }
    },
    {
      "paper_id": "cs.LG.math/OC/2503.09903v1",
      "true_abstract": "The integration of machine learning (ML) has significantly enhanced the\ncapabilities of Earth Observation (EO) systems by enabling the extraction of\nactionable insights from complex datasets. However, the performance of\ndata-driven EO applications is heavily influenced by the data collection and\ntransmission processes, where limited satellite bandwidth and latency\nconstraints can hinder the full transmission of original data to the receivers.\nTo address this issue, adopting the concepts of Semantic Communication (SC)\noffers a promising solution by prioritizing the transmission of essential data\nsemantics over raw information. Implementing SC for EO systems requires a\nthorough understanding of the impact of data processing and communication\nchannel conditions on semantic loss at the processing center. This work\nproposes a novel data-fitting framework to empirically model the semantic loss\nusing real-world EO datasets and domain-specific insights. The framework\nquantifies two primary types of semantic loss: (1) source coding loss, assessed\nvia a data quality indicator measuring the impact of processing on raw source\ndata, and (2) transmission loss, evaluated by comparing practical transmission\nperformance against the Shannon limit. Semantic losses are estimated by\nevaluating the accuracy of EO applications using four task-oriented ML models,\nEfficientViT, MobileViT, ResNet50-DINO, and ResNet8-KD, on lossy image datasets\nunder varying channel conditions and compression ratios. These results underpin\na framework for efficient semantic-loss modeling in bandwidth-constrained EO\nscenarios, enabling more reliable and effective operations.",
      "generated_abstract": "We introduce the concept of semantic-loss function modeling to address the\nproblem of modeling the relationship between task performance and model\nlosses. We present a framework for evaluating the effectiveness of a model using\na semantic-loss function, which evaluates the model's performance across a\nvariety of metrics and metrics that are specific to the task. This framework\nprovides a method for quantifying the impact of model parameters, and can be\nused to evaluate the impact of model changes on task performance. The modeling\nframework is demonstrated using an example of a text classification task. We\nevaluate a model's performance using the semantic-loss function and compare\nresults against traditional evaluation metrics, demonstrating the effectiveness\nof the semantic-loss function modeling framework.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16216216216216217,
          "p": 0.39344262295081966,
          "f": 0.22966506763672997
        },
        "rouge-2": {
          "r": 0.0319634703196347,
          "p": 0.06666666666666667,
          "f": 0.04320987216220895
        },
        "rouge-l": {
          "r": 0.12837837837837837,
          "p": 0.3114754098360656,
          "f": 0.1818181776845769
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/MM/2503.10078v1",
      "true_abstract": "Image Quality Assessment (IQA) based on human subjective preferences has\nundergone extensive research in the past decades. However, with the development\nof communication protocols, the visual data consumption volume of machines has\ngradually surpassed that of humans. For machines, the preference depends on\ndownstream tasks such as segmentation and detection, rather than visual appeal.\nConsidering the huge gap between human and machine visual systems, this paper\nproposes the topic: Image Quality Assessment for Machine Vision for the first\ntime. Specifically, we (1) defined the subjective preferences of machines,\nincluding downstream tasks, test models, and evaluation metrics; (2)\nestablished the Machine Preference Database (MPD), which contains 2.25M\nfine-grained annotations and 30k reference/distorted image pair instances; (3)\nverified the performance of mainstream IQA algorithms on MPD. Experiments show\nthat current IQA metrics are human-centric and cannot accurately characterize\nmachine preferences. We sincerely hope that MPD can promote the evolution of\nIQA from human to machine preferences. Project page is on:\nhttps://github.com/lcysyzxdxc/MPD.",
      "generated_abstract": "lity assessment is a critical task in digital image analysis\napplications. Traditional approaches often rely on expert human annotators to\nevaluate images, but this task is challenging due to the subjectivity and\nlimited availability of human annotators. In this paper, we propose a new\napproach for automated image quality assessment based on machine learning\ntechniques. Our approach leverages a transformer-based neural network to\nlearn the image quality features from the labeled images. We compare the\nperformance of our method with traditional methods such as the Kullback-Leibler\ndistance and the Shapley values to evaluate the preference of human annotators.\nThe experimental results show that our method achieves a mean opinion score\n(MOS) of 8.04 and a Shapley value of 9.45 on the DIVA dataset, which is\ncomparable to the human-annotated results",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20512820512820512,
          "p": 0.26666666666666666,
          "f": 0.23188405305608076
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.04838709677419355,
          "f": 0.04285713792244955
        },
        "rouge-l": {
          "r": 0.18803418803418803,
          "p": 0.24444444444444444,
          "f": 0.21256038155849621
        }
      }
    },
    {
      "paper_id": "math.GM.math/GM/2502.14876v4",
      "true_abstract": "Series involving hypergeometric functions are used to derive, extend and\nevaluate integrals involving the product of two Bessel functions of the first\nkind $J_{u}(a z)$ $J_{v}(b z)$ with order $u,v$, studied by Landau et al. The\nmethod used in this work is contour integration.",
      "generated_abstract": "We provide a method to express definite integrals involving Bessel functions\nof the first and second kind in terms of the classical Bessel functions. The\nmethod is based on the relation between the Bessel functions of the first and\nsecond kind and the associated Legendre functions. The method is illustrated\nthrough various examples. In addition to the classical Bessel functions, we also\nconsider the classical Chebyshev polynomials and the associated Legendre\nfunctions. We show that the Bessel functions of the first and second kind can\nbe expressed as the sum of the classical Bessel functions and the associated\nLegendre functions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3684210526315789,
          "p": 0.2978723404255319,
          "f": 0.32941175976193776
        },
        "rouge-2": {
          "r": 0.13953488372093023,
          "p": 0.09375,
          "f": 0.11214952790287382
        },
        "rouge-l": {
          "r": 0.34210526315789475,
          "p": 0.2765957446808511,
          "f": 0.3058823479972319
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.10208v1",
      "true_abstract": "We explore the Jordan-Chevalley decomposition problem for an operator field\nin small dimensions. In dimensions three and four, we find tensorial conditions\nfor an operator field $L$, similar to a nilpotent Jordan block, to possess\nlocal coordinates in which $L$ takes a strictly upper triangular form. We prove\nthe Tempesta-Tondo conjecture for higher order brackets of\nFr\\\"olicher-Nijenhuis type.",
      "generated_abstract": "the Jordan-Chevalley decomposition problem for the fields in small\ndimensions, and show that it reduces to the Tempesta-Tondo conjecture.\nSpecifically, we give a complete solution to the problem of a scalar field\n$W\\in C^\\infty(\\mathbb{R}^n)$ whose field strength $F=dW$ is a closed $n$-form\nin a K\\\"ahler manifold $\\mathcal{M}$ of complex dimension $n$.\n  First, we give a complete solution to the problem of a scalar field $W$\nsatisfying the Weyl equation in a K\\\"ahler manifold $\\mathcal{M}$ of complex\ndimension $n$. Second, we give a complete solution to the problem of a\nscalar field $W$ satisfying the Einstein-Yang-Mills equation in a K\\\"ahler\nmanifold $\\mathcal{M}$ of complex dimension $n$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3191489361702128,
          "p": 0.3191489361702128,
          "f": 0.31914893117021287
        },
        "rouge-2": {
          "r": 0.12962962962962962,
          "p": 0.11475409836065574,
          "f": 0.12173912545330834
        },
        "rouge-l": {
          "r": 0.2978723404255319,
          "p": 0.2978723404255319,
          "f": 0.29787233542553193
        }
      }
    },
    {
      "paper_id": "cs.CL.econ/GN/2412.04505v2",
      "true_abstract": "Accurately interpreting words is vital in political science text analysis;\nsome tasks require assuming semantic stability, while others aim to trace\nsemantic shifts. Traditional static embeddings, like Word2Vec effectively\ncapture long-term semantic changes but often lack stability in short-term\ncontexts due to embedding fluctuations caused by unbalanced training data.\nBERT, which features transformer-based architecture and contextual embeddings,\noffers greater semantic consistency, making it suitable for analyses in which\nstability is crucial. This study compares Word2Vec and BERT using 20 years of\nPeople's Daily articles to evaluate their performance in semantic\nrepresentations across different timeframes. The results indicate that BERT\noutperforms Word2Vec in maintaining semantic stability and still recognizes\nsubtle semantic variations. These findings support BERT's use in text analysis\ntasks that require stability, where semantic changes are not assumed, offering\na more reliable foundation than static alternatives.",
      "generated_abstract": "text analysis aims to uncover patterns and trends in political\ncommunication and discourse. This research examines the role of contextualized\nword representations (CWRs) in semantic consistency and political text analysis.\nWe conduct experiments on three datasets: the 2024 US Presidential Election\nSpeeches dataset, the 2023 US Senate Election Speeches dataset, and the 2022\nUS House Election Speeches dataset. The results show that CWRs can improve\nanalytical performance by enhancing the consistency of political sentiment\nrepresentations. In the 2023 US Senate Election Speeches dataset, the\npre-trained CWRs are able to significantly improve the consistency of\nsentiment representations, particularly in the areas of partisanship,\nincivility, and polarization. Furthermore, the performance of CWRs",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1509433962264151,
          "p": 0.24615384615384617,
          "f": 0.18713449821141562
        },
        "rouge-2": {
          "r": 0.02962962962962963,
          "p": 0.041666666666666664,
          "f": 0.03463202977455513
        },
        "rouge-l": {
          "r": 0.1509433962264151,
          "p": 0.24615384615384617,
          "f": 0.18713449821141562
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.06878v1",
      "true_abstract": "Let $G$ be a finite group and $p$ be a prime. We prove that if $G$ has three\ncodegrees, then $G$ is an $M$-group. We prove for some prime $p$ that if every\nirreducible Brauer character of $G$ is a prime, then for every normal subgroup\n$N$ of $G$ either $G/N$ or $N$ is an $M_p$-group.",
      "generated_abstract": "e a prime number and $\\mathcal{O}$ be the ring of integers in a\ngeneral number field $K$. Let $M_p$ be the group of $p$-power roots of unity\nin $\\mathcal{O}$ and let $G_p$ be the multiplicative group of $p$-th roots of\nunity in $\\mathcal{O}$. Let $X_p$ be the multiplicative group of $p$-th roots\nof unity in $\\mathbb{F}_p$, where $\\mathbb{F}_p$ is the finite field of\ncharacteristic $p$. Let $M_{p, K}$ be the group of $p$-power roots of unity in\n$\\mathcal{O}_K$ and let $G_{p, K}$ be the multiplicative group of $p$-th roots\nof unity in $\\mathcal{O}_K$. Let $X_{p, K}$ be the multiplic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2857142857142857,
          "p": 0.2564102564102564,
          "f": 0.2702702652848795
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.016666666666666666,
          "f": 0.018348618904133984
        },
        "rouge-l": {
          "r": 0.2571428571428571,
          "p": 0.23076923076923078,
          "f": 0.24324323825785255
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/AP/2503.01566v1",
      "true_abstract": "Extreme response assessment is important in the design and operation of\nengineering structures, and is a crucial part of structural risk and\nreliability analyses. Structures should be designed in a way that enables them\nto withstand the environmental loads they are expected to experience over their\nlifetime, without designs being unnecessarily conservative and costly. An\naccurate risk estimate is essential but difficult to obtain because the\nlong-term behaviour of a structure is typically too complex to calculate\nanalytically or with brute force Monte Carlo simulation. Therefore,\napproximation methods are required to estimate the extreme response using only\na limited number of short-term conditional response calculations. Combining\nsurrogate models with Design of Experiments is an approximation approach that\nhas gained popularity due to its ability to account for both long-term\nenvironment variability and short-term response variability. In this paper, we\npropose a method for estimating the extreme response of black-box, stochastic\nmodels with heteroscedastic non-Gaussian noise. We present a mathematically\nfounded extreme response estimation process that enables Design of Experiment\napproaches that are prohibitively expensive with surrogate Monte Carlo. The\ntheory leads us to speculate this method can robustly produce more confident\nextreme response estimates, and is suitable for a variety of domains. While\nthis needs to be further validated empirically, the method offers a promising\ntool for reducing the uncertainty decision-makers face, allowing them to make\nbetter informed choices and create more optimal structures.",
      "generated_abstract": "aims to investigate the feasibility of a structural reliability\ndesign of experiments (DoE) approach to estimate the long-term structural\nstochastic reliability (SSR) of steel reinforced concrete (RCC) structures\nusing non-Gaussian stochastic models. The design of experiments was carried out\nby utilizing the finite element method (FEM) and non-Gaussian stochastic\nmodels for the long-term structural reliability of RCC structures with\nnon-Gaussian stochastic models. The reliability data of five reinforced\nconcrete (RC) columns were employed as the experimental data. A design of\nexperiments approach was applied to estimate the long-term SSR of the RCC\nstructures using a single stochastic model. The reliability data of five RC\ncolumns with different stochastic models were used to estimate the long-term\nSSR of the RCC structures. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12987012987012986,
          "p": 0.36363636363636365,
          "f": 0.19138755593049617
        },
        "rouge-2": {
          "r": 0.02242152466367713,
          "p": 0.056179775280898875,
          "f": 0.032051277973578755
        },
        "rouge-l": {
          "r": 0.12987012987012986,
          "p": 0.36363636363636365,
          "f": 0.19138755593049617
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2412.05323v1",
      "true_abstract": "In application-specific designs, owing to the trade-off between power\nconsumption and speed, optimization of various circuit parameters has become a\nchallenging task. Several of the performance metrics, viz. energy efficiency,\ngain, performance, and noise immunity, are interrelated and difficult to tune.\nSuch efforts may result in a great deal of manual iterations which in turn\nincrease the computational overhead. Thus, it is important to develop a\nmethodology that not only explores large design space but also reduces the\ncomputational time. In this work, we investigate the viability of using a SPICE\nand Python IDE (PIDE) interface to optimize integrated circuits. The SPICE\nsimulations are carried out using 22 nm technology node with a nominal supply\nvoltage of 0.8 V. The SPICE-PIDE optimizer, as delineated in this work, is able\nto provide the best solution sets considering various performance metrics and\ndesign complexities for 5 transistor level converters.",
      "generated_abstract": "This paper presents the SPICE-PIDE methodology for the design and\noptimization of integrated circuits (ICs). SPICE-PIDE is a novel methodology\nthat integrates the SPICE-based design and the PI-based optimization\napproaches, thus providing a framework that enables the synthesis of\noptimized ICs in a unified manner. The methodology is based on the\ngeneralization of the SPICE-based circuit model to the multi-level\ncircuit model, which enables the design of both digital and analog circuits.\nThe optimization process is also extended to the PI-based optimization model.\nThe methodology is demonstrated through an example on the design of a 16-bit\nmicrocontroller. The effectiveness of the methodology is validated through a\ncase study on the design of the 16-bit microcontroller.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.168141592920354,
          "p": 0.3275862068965517,
          "f": 0.22222221773947548
        },
        "rouge-2": {
          "r": 0.04827586206896552,
          "p": 0.07368421052631578,
          "f": 0.05833332855034761
        },
        "rouge-l": {
          "r": 0.1592920353982301,
          "p": 0.3103448275862069,
          "f": 0.21052631130672697
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.05098v1",
      "true_abstract": "Information-directed sampling (IDS) is a powerful framework for solving\nbandit problems which has shown strong results in both Bayesian and frequentist\nsettings. However, frequentist IDS, like many other bandit algorithms, requires\nthat one have prior knowledge of a (relatively) tight upper bound on the norm\nof the true parameter vector governing the reward model in order to achieve\ngood performance. Unfortunately, this requirement is rarely satisfied in\npractice. As we demonstrate, using a poorly calibrated bound can lead to\nsignificant regret accumulation. To address this issue, we introduce a novel\nfrequentist IDS algorithm that iteratively refines a high-probability upper\nbound on the true parameter norm using accumulating data. We focus on the\nlinear bandit setting with heteroskedastic subgaussian noise. Our method\nleverages a mixture of relevant information gain criteria to balance\nexploration aimed at tightening the estimated parameter norm bound and directly\nsearching for the optimal action. We establish regret bounds for our algorithm\nthat do not depend on an initially assumed parameter norm bound and demonstrate\nthat our method outperforms state-of-the-art IDS and UCB algorithms.",
      "generated_abstract": "on-directed sampling (IDS) methods, which utilize the empirical\ninformation criteria for Bayesian optimization, have gained significant\nattention in the context of norm-agnostic bandits. However, existing IDS methods\noften require a prior distribution on the bandit function, which is often\ncomputationally expensive. To address this limitation, we propose a novel\ninformation-directed sampling method that does not require a prior distribution\non the bandit function. Instead, we construct an empirical bound on the\ninformation criteria, which we refer to as the empirical bound information\ndirected sampling (EBIDS), to estimate the optimal bandit function. The\nempirical bound is constructed by leveraging the empirical information criteria\nfor Bayesian optimization (EIOB). Theoretically, we prove that EBIDS is a\ndistribution-free Bayesian optimization algorithm with a finite information\nbound. Empirically, we show",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27049180327868855,
          "p": 0.42857142857142855,
          "f": 0.3316582867129618
        },
        "rouge-2": {
          "r": 0.04878048780487805,
          "p": 0.07476635514018691,
          "f": 0.05904058562710241
        },
        "rouge-l": {
          "r": 0.26229508196721313,
          "p": 0.4155844155844156,
          "f": 0.3216080354566805
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2409.16613v2",
      "true_abstract": "Oral exams are a powerful tool for educators to gauge student's learning.\nThis is particularly important in introductory statistics classes where many\nstudents struggle to grasp a deep meaning of topics like $p$-values, confidence\nintervals, hypothesis testing, and more. These challenges are only heightened\nin a context where students are learning in a second language. In this paper, I\nshare my experience administering oral exams to an introductory statistics\nclass of non-native English speakers at a Japanese university. I explain the\ncontext of the university and course that the exam was given in, before sharing\ndetails about the two exams. Despite the challenges the students (and I myself)\nfaced, the exams seemed to truly test their statistical knowledge and not\nmerely their English proficiency, as I found little relationship between a\nstudent's English ability and performance. I close with encouragements and\nrecommendations for practitioners hoping to implement these exams, all while\nkeeping an eye towards the unique difficulties faced by students not learning\nin their mother tongue.",
      "generated_abstract": "Introduction to Statistics is one of the most common courses in statistics\nprograms. Most of the students in introductory statistics classes are\nnon-native English speakers (NESs). This study investigates oral examinations\nin introductory statistics classes with NESs. We found that the presence of\nNESs significantly affects students' performance in oral exams. The results\nindicate that oral exams should be held with NESs only if there are no\ninstructor-generated quizzes. This study is also a call to action for\nstatistics educators to create environments that are more conducive to NESs.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21929824561403508,
          "p": 0.4166666666666667,
          "f": 0.2873563173206501
        },
        "rouge-2": {
          "r": 0.06134969325153374,
          "p": 0.12345679012345678,
          "f": 0.08196720867945469
        },
        "rouge-l": {
          "r": 0.21929824561403508,
          "p": 0.4166666666666667,
          "f": 0.2873563173206501
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.01760v1",
      "true_abstract": "Adding a capacity constraint to a hidden-action principal-agent problem\nresults in the same set of Pareto optimal contracts as the unconstrained\nproblem where output is scaled down by a constant factor. This scaling factor\nis increasing in the agent's capacity to exert effort.",
      "generated_abstract": "This paper develops a general framework for analyzing principal-agent\nproblems with capacity constraints. We show that the capacity constraints\nimplicit in such problems can be expressed in a simple form. We then present a\ntwo-stage solution method to the problem. This method is based on solving\nlinear programs that relate the optimal solution of the principal's problem to\nthe optimal solution of the agent's problem. The linear programs are solved\nnumerically, and we show that they provide an efficient algorithm for\nsolving the general problem. We also present an approximation algorithm that\nuses a sequence of linear programs to approximate the solution of the\nprincipal's problem. We provide an example to demonstrate the usefulness of the\napproximation algorithm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.36363636363636365,
          "p": 0.1935483870967742,
          "f": 0.2526315744132965
        },
        "rouge-2": {
          "r": 0.024390243902439025,
          "p": 0.01,
          "f": 0.014184393038580748
        },
        "rouge-l": {
          "r": 0.3333333333333333,
          "p": 0.1774193548387097,
          "f": 0.2315789428343491
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.cond-mat/dis-nn/2503.09389v2",
      "true_abstract": "We study canonical-equilibrium properties of Random Field $O(n)$ Models\ninvolving classical continuous vector spins of $n$ components with mean-field\ninteractions and subject to disordered fields acting on individual spins. To\nthis end, we employ two complementary approaches: the mean-field approximation,\nvalid for any disorder distribution, and the replica trick, applicable when the\ndisordered fields are sampled from a Gaussian distribution. On the basis of an\nexact analysis, we demonstrate that when replica symmetry holds, both the\napproaches yield identical expression for the free energy per spin of the\nsystem. As consequences, we study the case of $n=2$ ($XY$ spins) and that of\n$n=3$ (Heisenberg spins) for two representative choices of the disorder\ndistribution, namely, a Gaussian and a symmetric bimodal distribution. For both\n$n=2$ and $n=3$, we demonstrate that while the magnetization exhibits a\ncontinuous phase transition as a function of temperature for the Gaussian case,\nthe transition could be either continuous or first-order with an emergent\ntricriticality when the disorder distribution is bimodal. We also discuss in\nthe context of our models the issue of self-averaging of extensive variables\nnear the critical point of a continuous phase transition.",
      "generated_abstract": "We study the statistical mechanics of mean-field $O(n)$ models with\ncanonical equilibrium. In the simplest model, $O(1)$ and $O(\\sqrt{n})$\ninteracting bosons, we find that the phase diagram is well-described by a\nphase transition. We identify the critical point and obtain the critical\nexponents. We also study the critical behavior of the $O(1)$ model. We find\nthat the phase transition is first order and the critical exponents are\n$(\\beta,\\gamma,\\delta)=(3/2,3/2,1)$. The $O(\\sqrt{n})$ model exhibits a\nsecond order phase transition. We identify the critical exponents for this\ntransition as $(\\beta,\\gamma,\\delta)=(3/2,1/2,1)$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19827586206896552,
          "p": 0.46938775510204084,
          "f": 0.27878787461230486
        },
        "rouge-2": {
          "r": 0.056818181818181816,
          "p": 0.14084507042253522,
          "f": 0.08097165582258375
        },
        "rouge-l": {
          "r": 0.1810344827586207,
          "p": 0.42857142857142855,
          "f": 0.2545454503698807
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.q-bio/TO/2412.15774v1",
      "true_abstract": "A key process during animal morphogenesis is oriented tissue deformation,\nwhich is often driven by internally generated active stresses. Yet, such active\noriented materials are prone to well-known instabilities, raising the question\nof how oriented tissue deformation can be robust during morphogenesis. In a\nsimple scenario, we recently showed that active oriented deformation can be\nstabilized by the boundary-imposed gradient of a scalar field, which\nrepresents, e.g., a morphogen gradient in a developing embryo. Here, we discuss\na more realistic scenario, where the morphogen is produced by a localized\nsource region, diffuses across the tissue, and degrades. Consistent with our\nearlier results, we find that oriented tissue deformation is stable in the\ngradient-extensile case, i.e. when active stresses act to extend the tissue\nalong the direction of the gradient, but it is unstable in the\ngradient-contractile case. In addition, we now show that gradient-contractile\ntissues can not be stabilized even by morphogen diffusion. Finally, we point\nout the existence of an additional instability, which results from the\ninterplay of tissue shear and morphogen diffusion. Our theoretical results\nexplain the lack of gradient-contractile tissues in the biological literature,\nsuggesting that the active matter instability acts as an evolutionary selection\ncriterion.",
      "generated_abstract": "Active deformation in soft tissues is driven by the localization of\nactivators and inhibitors, which are guided by the morphogen gradient. We\npropose a mechanism for the stabilization of active deformation in soft\ntissues, in which the localized morphogen gradient provides a dynamic\nmorphogen gradient, thereby ensuring the consistency of the deformation and the\npresence of active forces. We show that the localized morphogen gradient\nprovides a dynamic analogue of the equilibrium force field, which stabilizes\nactive deformation. The localized morphogen gradient is thus the key to the\nstabilization of active deformation in soft tissues.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17886178861788618,
          "p": 0.46808510638297873,
          "f": 0.25882352541107273
        },
        "rouge-2": {
          "r": 0.042328042328042326,
          "p": 0.1111111111111111,
          "f": 0.061302677997093664
        },
        "rouge-l": {
          "r": 0.14634146341463414,
          "p": 0.3829787234042553,
          "f": 0.21176470188166094
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/PM/2410.03552v1",
      "true_abstract": "The growth of the tech startup ecosystem in Latin America (LATAM) is driven\nby innovative entrepreneurs addressing market needs across various sectors.\nHowever, these startups encounter unique challenges and risks that require\nspecific management approaches. This paper explores a case study with the Total\nAddressable Market (TAM), Serviceable Available Market (SAM), and Serviceable\nObtainable Market (SOM) metrics within the context of the online food delivery\nindustry in LATAM, serving as a model for valuing startups using the Discounted\nCash Flow (DCF) method. By analyzing key emerging powers such as Argentina,\nColombia, Uruguay, Costa Rica, Panama, and Ecuador, the study highlights the\npotential and profitability of AI-driven startups in the region through the\ndevelopment of a ranking of emerging powers in Latin America for tech startup\ninvestment. The paper also examines the political, economic, and competitive\nrisks faced by startups and offers strategic insights on mitigating these risks\nto maximize investment returns. Furthermore, the research underscores the value\nof diversifying investment portfolios with startups in emerging markets,\nemphasizing the opportunities for substantial growth and returns despite\ninherent risks.",
      "generated_abstract": "y evaluates investment risks in Latin American Artificial Intelligence\n(AI) startups through a ranking of the five most promising industries based on\ntheir investment potential and a framework for valuation. The study examines\nthe data from the 2020 Venture Capital (VC) Investment Report published by\nKPMG, the 2020 Financial Times AI 50 report, and the 2021 AI 50 report by\nInsight Partners. The study's findings show that the most promising industries\nare e-commerce, ed-tech, fin-tech, and healthcare. The report also highlights\nthe challenges facing startups, such as regulatory hurdles, lack of access to\nfinancing, and the impact of COVID-19. The study offers insights for investors\nand entrepreneurs",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23728813559322035,
          "p": 0.35443037974683544,
          "f": 0.2842639545868227
        },
        "rouge-2": {
          "r": 0.047058823529411764,
          "p": 0.08,
          "f": 0.059259254595336444
        },
        "rouge-l": {
          "r": 0.211864406779661,
          "p": 0.31645569620253167,
          "f": 0.25380710179494453
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PM/2410.20597v1",
      "true_abstract": "We investigate the effectiveness of a momentum trading signal based on the\ncoverage network of financial analysts. This signal builds on the key\ninformation-brokerage role financial sell-side analysts play in modern stock\nmarkets. The baskets of stocks covered by each analyst can be used to construct\na network between firms whose edge weights represent the number of analysts\njointly covering both firms. Although the link between financial analysts\ncoverage and co-movement of firms' stock prices has been investigated in the\nliterature, little effort has been made to systematically learn the most\neffective combination of signals from firms covered jointly by analysts in\norder to benefit from any spillover effect. To fill this gap, we build a\ntrading strategy which leverages the analyst coverage network using a graph\nattention network. More specifically, our model learns to aggregate information\nfrom individual firm features and signals from neighbouring firms in a\nnode-level forecasting task. We develop a portfolio based on those predictions\nwhich we demonstrate to exhibit an annualized returns of 29.44% and a Sharpe\nratio of 4.06 substantially outperforming market baselines and existing graph\nmachine learning based frameworks. We further investigate the performance and\nrobustness of this strategy through extensive empirical analysis. Our paper\nrepresents one of the first attempts in using graph machine learning to extract\nactionable knowledge from the analyst coverage network for practical financial\napplications.",
      "generated_abstract": "y examines how the financial analyst networks (FANs) on Twitter\nand LinkedIn influence stock market returns, focusing on the role of\nrelational knowledge in generating alpha. We construct a novel network\nrepresentation of analysts that captures their collective knowledge through\ntheir interactions and collaborations, leveraging social networks theory.\nAnalyzing the interactions of 2.2 million analysts across 10,000 companies, we\nidentify key themes such as \"knowledge exchange\" and \"knowledge sharing,\"\nhighlighting the influence of analysts' social networks in generating\nalpha. Our findings reveal that analysts' interactions have a significant\nimpact on stock returns, particularly during periods of market turmoil.\nImportantly, our analysis highlights the role of knowledge transfer within\nanalyst networks, with strong connections between analysts with similar\nknowledge. This study",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1702127659574468,
          "p": 0.2926829268292683,
          "f": 0.21524663212129752
        },
        "rouge-2": {
          "r": 0.013953488372093023,
          "p": 0.02608695652173913,
          "f": 0.018181813640956133
        },
        "rouge-l": {
          "r": 0.15602836879432624,
          "p": 0.2682926829268293,
          "f": 0.19730941239035585
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/MF/2502.17906v2",
      "true_abstract": "In econophysics, there are several enigmatic empirical laws: (i)~the\nmarket-order flow has strong persistence (long-range order-sign correlation),\nwell formulated as the Lillo-Mike-Farmer model. This phenomenon seems\nparadoxical given the diffusive and unpredictable price dynamics; (ii)~the\nprice impact $I(Q)$ of a large metaorder $Q$ follows the square-root law,\n$I(Q)\\propto \\sqrt{Q}$. In this Letter, we propose an exactly solvable model of\nthe nonlinear price-impact dynamics that unifies these enigmas. We generalize\nthe Lillo-Mike-Farmer model to nonlinear price-impact dynamics, which is mapped\nto an exactly solvable L\\'evy-walk model. Our exact solution and numerical\nsimulations reveal three important points: First, the price dynamics remains\ndiffusive under the square-root law, even under the long-range correlation.\nSecond, price-movement statistics follows truncated power laws with typical\nexponent around three. Third, volatility has long memory. While this simple\nmodel lacks adjustable free parameters, it naturally aligns even with other\nenigmatic empirical laws, such as (iii)~the inverse-cubic law for price\nstatistics and (iv)~volatility clustering. This work illustrates the crucial\nrole of the square-root law in understanding rich and complex financial price\ndynamics from a single coherent viewpoint.",
      "generated_abstract": "er a price impact model where the implied volatility of the\nmarket order is long-range correlated with the current price. We show that the\nlong-range correlation is exactly solvable, with the price impact dynamics\nexactly determined by the long-range correlation dynamics. We prove the\nexistence of the exact solution by solving a partial differential equation\n(PDE). Our result extends the solvability results of Zhang and Zhang (2009)\nfor the long-range correlation model, where the long-range correlation is\nassumed to be constant and independent of the current price. Our solution is\nverified through numerical simulations, where the exact solution closely\ncoincides with the numerical solution. In addition, we also consider the\nlong-range correlation model with the drift, where the long-range correlation\nis not necessarily constant and independent of the current price. Our result\nshows that the long-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1875,
          "p": 0.36363636363636365,
          "f": 0.24742267592305242
        },
        "rouge-2": {
          "r": 0.03636363636363636,
          "p": 0.0594059405940594,
          "f": 0.04511277724433313
        },
        "rouge-l": {
          "r": 0.1796875,
          "p": 0.3484848484848485,
          "f": 0.237113397572537
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/TR/2501.07489v1",
      "true_abstract": "The efficient market hypothesis (EMH) famously stated that prices fully\nreflect the information available to traders. This critically depends on the\ntransfer of information into prices through trading strategies. Traders\noptimise their strategy with models of increasing complexity that identify the\nrelationship between information and profitable trades more and more\naccurately. Under specific conditions, the increased availability of low-cost\nuniversal approximators, such as AI systems, should be naturally pushing\ntowards more advanced trading strategies, potentially making it harder and\nharder for inefficient traders to profit. In this paper, we leverage on a\ngeneralised notion of market efficiency, based on the definition of an\nequilibrium price process, that allows us to distinguish different levels of\nmodel complexity through investors' beliefs, and trading strategies\noptimisation, and discuss the relationship between AI-powered trading and the\ntime-evolution of market efficiency. Finally, we outline the need for and the\nchallenge of describing out-of-equilibrium market dynamics in an adaptive\nmulti-agent environment.",
      "generated_abstract": "We investigate the potential of low-cost universal approximators (UAs) to\nremediate market inefficiencies. In a two-market model, we introduce a UA that\ncan approximate the variance of the optimal asset allocation with a constant\nerror. Our analysis reveals that the UA can achieve a constant error of up to\n26.6% with a constant approximation error of up to 14.6%. We also introduce a\nUA that can approximate the variance of the optimal asset allocation with a\nconstant approximation error and a constant error, respectively. Our analysis\nreveals that the UA can achieve a constant error of up to 41.9% with a\nconstant approximation error of up to 27.7%. Our results highlight the\npotential of UAs to mitigate market inefficiencies in two-market models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1308411214953271,
          "p": 0.2641509433962264,
          "f": 0.17499999556953136
        },
        "rouge-2": {
          "r": 0.013513513513513514,
          "p": 0.025974025974025976,
          "f": 0.01777777327565546
        },
        "rouge-l": {
          "r": 0.12149532710280374,
          "p": 0.24528301886792453,
          "f": 0.16249999556953137
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2502.19391v1",
      "true_abstract": "Antibody co-design represents a critical frontier in drug development, where\naccurate prediction of both 1D sequence and 3D structure of\ncomplementarity-determining regions (CDRs) is essential for targeting specific\nepitopes. Despite recent advances in equivariant graph neural networks for\nantibody design, current approaches often fall short in capturing the intricate\ninteractions that govern antibody-antigen recognition and binding specificity.\nIn this work, we present Igformer, a novel end-to-end framework that addresses\nthese limitations through innovative modeling of antibody-antigen binding\ninterfaces. Our approach refines the inter-graph representation by integrating\npersonalized propagation with global attention mechanisms, enabling\ncomprehensive capture of the intricate interplay between local chemical\ninteractions and global conformational dependencies that characterize effective\nantibody-antigen binding. Through extensive validation on epitope-binding CDR\ndesign and structure prediction tasks, Igformer demonstrates significant\nimprovements over existing methods, suggesting that explicit modeling of\nmulti-scale residue interactions can substantially advance computational\nantibody design for therapeutic applications.",
      "generated_abstract": "s are an important class of biomolecules that are widely used in\nthe pharmaceutical, diagnostic, and biotechnological industries. However,\ndespite their broad applicability, their design remains challenging due to\nthe complex nature of their structure, which is highly influenced by their\nbiological context. This study proposes a novel approach for the co-design of\nantibodies, which is based on the use of a fully atomistic force field. The\nproposed approach enables the optimization of antibody-ligand binding affinity\nand antibody-antigen interactions, while ensuring that the structure remains\ncompatible with its biological context. The proposed approach is validated\nthrough a case study involving the design of an anti-SARS-CoV-2 antibody\ncomprising an IgG1 subtype Fc region. This study demonstrates that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1794871794871795,
          "p": 0.2625,
          "f": 0.21319796471952393
        },
        "rouge-2": {
          "r": 0.006993006993006993,
          "p": 0.00909090909090909,
          "f": 0.007905133424990166
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.225,
          "f": 0.1827411119276458
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.12860v4",
      "true_abstract": "We study the problem of an organization that matches agents to objects where\nagents have preference rankings over objects and the organization uses\nalgorithms to construct a ranking over objects on behalf of each agent. Our new\nframework carries the interpretation that the organization and its agents may\nbe misaligned in pursuing some underlying matching goal. We design matching\nmechanisms that integrate agent decision-making and the algorithm by avoiding\nmatches that are unanimously disagreeable between the two parties. Our\nmechanisms also satisfy restricted efficiency properties. Subsequently, we\nprove that no unanimous mechanism is strategy-proof but that ours can be\nnon-obviously manipulable. We generalize our framework to allow for any\npreference aggregation rules and extend the famed Gibbard-Satterthwaite Theorem\nto our setting. We apply our framework to place foster children in foster homes\nto maximize welfare. Using a machine learning model that predicts child welfare\nin placements and a (planned) novel lab-in-the-field eliciting real\ncaseworkers' preferences, we empirically demonstrate that there are important\nmatch-specific welfare gains that our mechanisms extract that are not realized\nunder the status quo.",
      "generated_abstract": "r introduces a novel matching design that uses a randomized\nalgorithm to match foster care youth with appropriate family caregivers. The\nalgorithm takes as input the number of youth to be matched, the number of\ncaregivers to be matched, and the average match cost per match. It is\nassumed that the number of youth to be matched is known in advance, and that\nthe number of caregivers to be matched is unknown and can be determined\nstatistically. The algorithm matches the youth to caregivers using a greedy\nstrategy. It is shown that the number of matches in the algorithm is\napproximately equal to the number of youth to be matched, and that the\nprobability of a match is approximately equal to the probability of the\nrandomized algorithm producing a match. The paper also introduces a matching\nmodel with the algorithm as an extension. This model includes a new matching\nparameter, the expected match cost per",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17796610169491525,
          "p": 0.3387096774193548,
          "f": 0.233333328817284
        },
        "rouge-2": {
          "r": 0.023529411764705882,
          "p": 0.03636363636363636,
          "f": 0.028571423801021203
        },
        "rouge-l": {
          "r": 0.1694915254237288,
          "p": 0.3225806451612903,
          "f": 0.22222221770617293
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2410.04149v1",
      "true_abstract": "This paper introduces Mov-Avg, the Python software package for time series\nanalysis that requires little computer programming experience from the user.\nThe package allows the identification of trends, patterns, and the prediction\nof future events based on data collected over time. In this regard, the Mov-Avg\nimplementation provides three indicators to apply, namely: Simple Moving\nAverage, Weighted Moving Average and Exponential Moving Average. Due to its\ngeneric design, the Mov-Avg software package can be used in any field where the\napplication of moving averages is valid. In general, the Mov-Avg library for\ntime series analysis contributes to a better understanding of data-driven\nprocesses over time by taking advantage of moving averages in any way adapted\nto the research context.",
      "generated_abstract": "Motivation: Codeless time series analysis, where no explicit model is\nrequired, is an important and powerful approach for time series analysis.\nHowever, the existing codeless models are limited in their ability to deal\nwith outliers, noise, and missing values. While these limitations can be\naddressed with various approaches, including feature engineering, they can be\naddressed with moving averages (MAs). This paper introduces Mov-Avg, a new\ncodeless model that uses MAs for time series analysis. The model's architecture\nis designed to be flexible and can accommodate various time series types.\nAdditionally, it uses a novel loss function that helps mitigate the impact of\nmissing values. Experimental results show that the proposed model outperforms\nstate-of-the-art methods in various time series classification tasks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26506024096385544,
          "p": 0.2619047619047619,
          "f": 0.263473048892395
        },
        "rouge-2": {
          "r": 0.07476635514018691,
          "p": 0.07407407407407407,
          "f": 0.07441859965127129
        },
        "rouge-l": {
          "r": 0.26506024096385544,
          "p": 0.2619047619047619,
          "f": 0.263473048892395
        }
      }
    },
    {
      "paper_id": "math.RT.math/RA/2503.10461v1",
      "true_abstract": "This article studies the compatibility of Koenig's notion of an exact Borel\nsubalgebra of a quasi-hereditary or, more generally, standardly stratified\nalgebra with taking idempotent subalgebras or quotients. As an application, we\nprovide bounds for the multiplicities of indecomposable projectives in the\nprincipal blocks of BGG category $\\mathcal{O}$ having basic regular exact Borel\nsubalgebras.",
      "generated_abstract": "algebras of a locally compact group $G$ are closed under\nparticular types of actions. In this article we characterize exact Borel\nsubalgebras of the group $G$ and of the continuous group $G_\\infty$, and\ndemonstrate that exact Borel subalgebras are idempotent subalgebras. In\naddition we prove that exact idempotent subalgebras are idempotent Borel\nsubalgebras. We also study idempotent subalgebras of $G_\\infty$ and\nidempotent subalgebras of $G$, and prove that idempotent subalgebras are\nidempotent Borel subalgebras. We show that idempotent subalgebras of $G$ are\nexact if and only if they are exact Borel subalgebras. In the case of\ncontinuous groups, we show that idempotent sub",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20454545454545456,
          "p": 0.21428571428571427,
          "f": 0.20930232058409964
        },
        "rouge-2": {
          "r": 0.07692307692307693,
          "p": 0.05555555555555555,
          "f": 0.06451612416233128
        },
        "rouge-l": {
          "r": 0.20454545454545456,
          "p": 0.21428571428571427,
          "f": 0.20930232058409964
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.10114v1",
      "true_abstract": "We design specific neural networks (NNs) for the identification of switching\nnonlinear systems in the state-space form, which explicitly model the switching\nbehavior and address the inherent coupling between system parameters and\nswitching modes. This coupling is specifically addressed by leveraging the\nexpectation-maximization (EM) framework. In particular, our technique will\ncombine a moving window approach in the E-step to efficiently estimate the\nswitching sequence, together with an extended Kalman filter (EKF) in the M-step\nto train the NNs with a quadratic convergence rate. Extensive numerical\nsimulations, involving both academic examples and a battery charge management\nsystem case study, illustrate that our technique outperforms available ones in\nterms of parameter estimation accuracy, model fitting, and switching sequence\nidentification.",
      "generated_abstract": "Switching nonlinear systems are ubiquitous in real-world systems and have\nbeen widely studied. However, the identification of such systems remains\nchallenging due to the nonlinear nature of the dynamics and the presence of\nswitching effects. This paper proposes a neural network-based identification\nmethod for switching systems. The proposed method is based on the\nproposed neural network, which consists of a feed-forward neural network and a\nfeed-forward neural network. The proposed neural network is used to learn the\nparameters of the switching system. Then, the identification of the switching\nsystem is performed by calculating the transition matrix of the system using\nthe proposed neural network and the parameters of the switching system. The\nproposed method is tested on a nonlinear switching system and compared with\nother methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2159090909090909,
          "p": 0.31666666666666665,
          "f": 0.25675675193571956
        },
        "rouge-2": {
          "r": 0.05405405405405406,
          "p": 0.061224489795918366,
          "f": 0.05741626296192896
        },
        "rouge-l": {
          "r": 0.20454545454545456,
          "p": 0.3,
          "f": 0.24324323842220608
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.04555v1",
      "true_abstract": "This article analyzes a key exchange protocol based on the triad tropical\nsemiring, recently proposed by Jackson, J. and Perumal, R. We demonstrate that\nthe triad tropical semiring is isomorphic to a circulant matrix over tropical\nnumbers. Consequently, matrices in this semiring can be represented as tropical\nmatrices. As a result, we conduct a cryptanalysis of the key exchange protocol\nusing an algorithm introduced by Sulaiman Alhussaini, Craig Collett, and Sergei\nSergeev to solve the double discrete logarithm problem over tropical matrices",
      "generated_abstract": "We present a cryptanalysis of a key exchange protocol based on a tropical\ntriad\nmatrix semiring. The protocol is based on the key exchange protocols\ndescribed in [Rodriguez et al., 2019",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26229508196721313,
          "p": 0.64,
          "f": 0.37209301913196324
        },
        "rouge-2": {
          "r": 0.12,
          "p": 0.3103448275862069,
          "f": 0.17307691905510364
        },
        "rouge-l": {
          "r": 0.21311475409836064,
          "p": 0.52,
          "f": 0.3023255772714981
        }
      }
    },
    {
      "paper_id": "math.GR.math/AT/2503.08411v1",
      "true_abstract": "In this article, we prove that, given two finite connected graphs $\\Gamma_1$\nand $\\Gamma_2$, if the two right-angled Artin groups $A(\\Gamma_1)$ and\n$A(\\Gamma_2)$ are quasi-isometric, then the infinite pointed sums\n$\\bigvee_\\mathbb{N} \\Gamma_1^{\\bowtie}$ and $\\bigvee_\\mathbb{N}\n\\Gamma_2^{\\bowtie}$ are homotopy equivalent, where $\\Gamma_i^{\\bowtie}$ denotes\nthe simplicial complex whose vertex-set is $\\Gamma_i$ and whose simplices are\ngiven by joins. These invariants are extracted from a study, of independent\ninterest, of the homotopy types of several complexes of hyperplanes in\nquasi-median graphs (such as one-skeleta of CAT(0) cube complexes). For\ninstance, given a quasi-median graph $X$, the \\emph{crossing complex}\n$\\mathrm{Cross}^\\triangle(X)$ is the simplicial complex whose vertices are the\nhyperplanes (or $\\theta$-classes) of $X$ and whose simplices are collections of\npairwise transverse hyperplanes. When $X$ has no cut-vertex, we show that\n$\\mathrm{Cross}^\\triangle(X)$ is homotopy equivalent to the pointed sum of the\nlinks of all the vertices in the prism-completion $X^\\square$ of $X$.",
      "generated_abstract": "aper, we classify the homotopy types of complete hyperplanes\nin quasi-median graphs, in the sense of G. Fulton and J. Lurie. The\nquasi-median graph $G$ is a hypergraph with two types of vertices: $V_0$ is\nthe set of vertices of $G$ that have an edge to every vertex of $G$, and $V_1$\nis the set of vertices that have no edges to every vertex of $G$. A complete\nhyperplane of $G$ is a set $H$ of vertices of $G$ such that every vertex of $G$\nis incident to some vertex of $H$. We show that $H$ is a homotopy equivalence\nof simplicial complexes if and only if $H$ is a homotopy equivalence of\nsimplicial sets. We give a characterization of the homotopy types",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22826086956521738,
          "p": 0.3684210526315789,
          "f": 0.28187918990676103
        },
        "rouge-2": {
          "r": 0.07352941176470588,
          "p": 0.10869565217391304,
          "f": 0.08771929343182545
        },
        "rouge-l": {
          "r": 0.20652173913043478,
          "p": 0.3333333333333333,
          "f": 0.25503355232286845
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.13720v1",
      "true_abstract": "Ecosystems often demonstrate the coexistence of numerous species competing\nfor limited resources, with pronounced rarity and abundance patterns. A\npotential driver of such coexistence is environmental fluctuations that favor\ndifferent species over time. However, how to include and treat such temporal\nvariability in existing consumer-resource models is still an open problem. In\nthis study, we examine the role of correlated temporal fluctuations in\nmetabolic strategies within a stochastic consumer-resource framework,\nreflecting change of species behavior in response to the environment. In some\nconditions, we are able to solve analytically the species abundance\ndistributions, through path integral formalism. Our results reveal that\nstochastic dynamic metabolic strategies induce community structures that align\nmore closely with empirical ecological observations and contribute to the\nviolation of the Competitive Exclusion Principle (CEP). The degree of CEP\nviolation is maximized under intermediate competition strength, leading to an\nintermediate competition hypothesis. Furthermore, when non-neutral effects are\npresent, maximal biodiversity is achieved for intermediate values of the\namplitude of fluctuations. This work not only challenges traditional ecological\nparadigms, but also establishes a robust theoretical framework for exploring\nhow temporal dynamics and stochasticity drive biodiversity and community.",
      "generated_abstract": "This study investigates the emergence of species rarity, defined as the\nmechanism of metabolic fluctuations, using a metabolic model of the\nmetabolic strategies of two metabolically flexible species. The model incorporates\nthe effects of food availability and metabolic efficiency on species rarity.\nThe results show that species rarity is driven by the metabolic flexibility of\nthe species, which is explained by the metabolic strategies of the species. In\nparticular, species rarity arises from the metabolic flexibility of a species'\nmetabolic strategies, which are controlled by the metabolic flexibility of its\nmetabolic strategies. These results provide a framework for understanding the\nevolution of species rarity and its relationship with metabolic flexibility.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14615384615384616,
          "p": 0.3392857142857143,
          "f": 0.20430107106023826
        },
        "rouge-2": {
          "r": 0.03278688524590164,
          "p": 0.06741573033707865,
          "f": 0.044117642655980116
        },
        "rouge-l": {
          "r": 0.13076923076923078,
          "p": 0.30357142857142855,
          "f": 0.18279569471615223
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.16407v1",
      "true_abstract": "Various factors influence why some countries are more open to immigration\nthan others. Policy is only one of them. We design country-specific measures of\nopenness to immigration that aim to capture de facto levels of openness to\nimmigration, complementing existing de jure measures of immigration, based on\nenacted immigration laws and policy measures. We estimate these for 148\ncountries and three years (2000, 2010, and 2020). For a subset of countries, we\nalso distinguish between openness towards tertiary-educated migrants and less\nthan tertiary-educated migrants. Using the measures, we show that most places\nin the World today are closed to immigration, and a few regions are very open.\nThe World became more open in the first decade of the millennium, an opening\nmainly driven by the Western World and the Gulf countries. Moreover, we show\nthat other factors equal, countries that increased their openness to\nimmigration, reduced their old-age dependency ratios, and experienced slower\nreal wage growth, arguably a sign of relaxing labor and skill shortages.",
      "generated_abstract": "r investigates the relationship between the economic outcomes of\nimmigrants and the immigration policies of the country of origin. Using\nheterogeneous panel data on immigrants from 1997 to 2015, I investigate whether\nthe immigration policies of the country of origin influence the economic outcomes\nof immigrants. Using the difference-in-differences approach, I find that the\nimmigration policies of the country of origin are associated with a decline in\nthe economic outcomes of immigrants. Furthermore, I find that the effect of the\nimmigration policies of the country of origin is stronger for immigrants with\nhigher educational attainment. Finally, I find that the immigration policies of\nthe country of origin influence the economic outcomes of immigrants across\ndifferent types of immigrants. These findings highlight the importance of\ncountry-specific policies for economic outcomes of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1509433962264151,
          "p": 0.2962962962962963,
          "f": 0.1999999955281251
        },
        "rouge-2": {
          "r": 0.02631578947368421,
          "p": 0.05194805194805195,
          "f": 0.03493449335291147
        },
        "rouge-l": {
          "r": 0.10377358490566038,
          "p": 0.2037037037037037,
          "f": 0.13749999552812514
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.15097v1",
      "true_abstract": "This paper advances a variable screening approach to enhance conditional\nquantile forecasts using high-dimensional predictors. We have refined and\naugmented the quantile partial correlation (QPC)-based variable screening\nproposed by Ma et al. (2017) to accommodate $\\beta$-mixing time-series data.\nOur approach is inclusive of i.i.d scenarios but introduces new convergence\nbounds for time-series contexts, suggesting the performance of QPC-based\nscreening is influenced by the degree of time-series dependence. Through Monte\nCarlo simulations, we validate the effectiveness of QPC under weak dependence.\nOur empirical assessment of variable selection for growth-at-risk (GaR)\nforecasting underscores the method's advantages, revealing that specific labor\nmarket determinants play a pivotal role in forecasting GaR. While prior\nempirical research has predominantly considered a limited set of predictors, we\nemploy the comprehensive Fred-QD dataset, retaining a richer breadth of\ninformation for GaR forecasts.",
      "generated_abstract": "e quantile regression (PQR) is a method to estimate quantile\nprediction intervals using high-dimensional predictors. PQR is a generalization\nof the classical quantile regression (QR) method. This paper introduces the\nvariable screening approach (VSA) for PQR, which is a novel approach to\nselectively screen high-dimensional predictors that have a large impact on the\nquantile prediction. The VSA is based on the idea that the predictive\npredictors that have a large impact on the quantile prediction should be\nselected. To select the predictors, a variable selection method is used to\nselect the predictors that have a large impact on the quantile prediction. A\nnovel extension of the VSA, the variable screening VSA (VSVSA), is proposed\nthat uses the screening approach to select predictors. The VSVSA can be used\nto select the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2079207920792079,
          "p": 0.3559322033898305,
          "f": 0.26249999534453133
        },
        "rouge-2": {
          "r": 0.06716417910447761,
          "p": 0.09473684210526316,
          "f": 0.07860261523235666
        },
        "rouge-l": {
          "r": 0.2079207920792079,
          "p": 0.3559322033898305,
          "f": 0.26249999534453133
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.00290v1",
      "true_abstract": "I present a novel uniform law of large numbers (ULLN) for network-dependent\ndata. While Kojevnikov, Marmer, and Song (KMS, 2021) provide a comprehensive\nsuite of limit theorems and a robust variance estimator for network-dependent\nprocesses, their analysis focuses on pointwise convergence. On the other hand,\nuniform convergence is essential for nonlinear estimators such as M and GMM\nestimators (e.g., Newey and McFadden, 1994, Section 2). Building on KMS, I\nestablish the ULLN under network dependence and demonstrate its utility by\nproving the consistency of both M and GMM estimators. A byproduct of this work\nis a novel maximal inequality for network data, which may prove useful for\nfuture research beyond the scope of this paper.",
      "generated_abstract": "We develop a novel approach to analyzing network data by leveraging the\nunit root framework to study the behavior of network distance and degree\nautocorrelation. By constructing an alternative form of the unit root\ntest, we identify a class of autocorrelation functions that exhibit\nuniform convergence in the limit. This result extends previous work on\nasymptotic convergence in the limit to include the non-stationary case and\nprovides a unified framework for analyzing multiple network data types.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16470588235294117,
          "p": 0.2641509433962264,
          "f": 0.20289854599348886
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.028985507246376812,
          "f": 0.022346363977404957
        },
        "rouge-l": {
          "r": 0.16470588235294117,
          "p": 0.2641509433962264,
          "f": 0.20289854599348886
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.cond-mat/stat-mech/2503.10575v1",
      "true_abstract": "We discuss non-reversible Markov-chain Monte Carlo algorithms that, for\nparticle systems, rigorously sample the positional Boltzmann distribution and\nthat have faster than physical dynamics. These algorithms all feature a\nnon-thermal velocity distribution. They are exemplified by the lifted TASEP\n(totally asymmetric simple exclusion process), a one-dimensional lattice\nreduction of event-chain Monte Carlo. We analyze its dynamics in terms of a\nvelocity trapping that arises from correlations between the local density and\nthe particle velocities. This allows us to formulate a conjecture for its\nout-of-equilibrium mixing time scale, and to rationalize its equilibrium\nsuperdiffusive time scale. Both scales are faster than for the (unlifted)\nTASEP. They are further justified by our analysis of the lifted TASEP in terms\nof many-particle realizations of true self-avoiding random walks. We discuss\nvelocity trapping beyond the case of one-dimensional lattice models and in more\nthan one physical dimensions. Possible applications beyond physics are pointed\nout.",
      "generated_abstract": "In this paper we study the statistical mechanics of the lift of the\nTASEP to a random walk with negative curvature. We show that the lifted TASEP\nhas a non-trivial ground state, which is a mixed state of a number of\nnon-intersecting self-avoiding random walks (SARWs). In addition, we show that\nthe ground state of the lift is not necessarily pure. Finally, we discuss the\ndynamics of the lifted TASEP, and show that the lifted TASEP is in fact\nequivalent to the true self-avoiding random walk (SARW).",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.32653061224489793,
          "f": 0.22068965069774085
        },
        "rouge-2": {
          "r": 0.050359712230215826,
          "p": 0.09722222222222222,
          "f": 0.06635070640461835
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.32653061224489793,
          "f": 0.22068965069774085
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2412.18776v1",
      "true_abstract": "The Virtual Traffic Light (VTL) eliminates the need for physical traffic\nsignal infrastructure at intersections, leveraging Connected Vehicles (CVs) to\noptimize traffic flow. VTL assigns right-of-way dynamically based on factors\nsuch as estimated times of arrival (ETAs), the number of CVs in various lanes,\nand emission rates. These factors are considered in line with the objectives of\nthe VTL application. Aiming to optimize traffic flow and reduce delays, the VTL\nsystem generates Signal Phase and Timing (SPaT) data for CVs approaching an\nintersection, while considering the impact of each CV movement on others.\nHowever, the stochastic nature of vehicle arrivals at intersections complicates\nreal-time optimization, challenging classical computing methods. To address\nthis limitation, we develop a VTL method that leverages quantum computing to\nminimize stopped delays for CVs. The method formulates the VTL problem as a\nQuadratic Unconstrained Binary Optimization (QUBO) problem, a mathematical\nframework well-suited for quantum computing. Using D-Wave cloud-based quantum\ncomputer, our approach determines optimal solutions for right-of-way\nassignments under the standard National Electrical Manufacturers Association\n(NEMA) phasing system. The system was evaluated using the microscopic traffic\nsimulator SUMO under varying traffic volumes. Our results demonstrate that the\nquantum-enabled VTL system reduces stopped delays and travel times compared to\nclassical optimization-based systems. This approach not only enhances traffic\nmanagement efficiency but also reduces the infrastructure costs associated with\ntraditional traffic signals. The quantum computing-supported VTL system offers\na transformative solution for large-scale traffic control, providing superior\nperformance across diverse traffic scenarios and paving the way for advanced,\ncost-effective traffic management.",
      "generated_abstract": "t a quantum-inspired approach for solving traffic flow problems,\nusing quantum annealing (QA) as a framework for simulating traffic dynamics.\nThe main advantage of using QA over classical computing is its ability to\nefficiently solve large-scale optimization problems, enabling the development of\nnew algorithms and frameworks for traffic flow. The QA-based framework\nrepresents the traffic flow problem as a quantum optimization problem,\noptimizing the traffic flow parameters through the quantum annealing\nalgorithm. The QA-based algorithm is then used to solve the traffic flow problem\nin a real-world scenario. The framework can be extended to solve more complex\ntraffic flow problems, such as multi-arrival, multi-departure, and multi-lane\ntraffic flow. We demonstrate the effectiveness of the framework by using it to\noptimize traffic flow in a real-world scenario, using the QA-based framework",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13372093023255813,
          "p": 0.3333333333333333,
          "f": 0.1908713652078994
        },
        "rouge-2": {
          "r": 0.03734439834024896,
          "p": 0.08108108108108109,
          "f": 0.051136359318343594
        },
        "rouge-l": {
          "r": 0.13372093023255813,
          "p": 0.3333333333333333,
          "f": 0.1908713652078994
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/ET/2503.07799v1",
      "true_abstract": "Congenital Heart Disease (CHD) is one of the leading causes of fetal\nmortality, yet the scarcity of labeled CHD data and strict privacy regulations\nsurrounding fetal ultrasound (US) imaging present significant challenges for\nthe development of deep learning-based models for CHD detection. Centralised\ncollection of large real-world datasets for rare conditions, such as CHD, from\nlarge populations requires significant co-ordination and resource. In addition,\ndata governance rules increasingly prevent data sharing between sites. To\naddress these challenges, we introduce, for the first time, a novel\nprivacy-preserving, zero-shot CHD detection framework that formulates CHD\ndetection as a normality modeling problem integrated with model merging. In our\nframework dubbed Sparse Tube Ultrasound Distillation (STUD), each hospital site\nfirst trains a sparse video tube-based self-supervised video anomaly detection\n(VAD) model on normal fetal heart US clips with self-distillation loss. This\nenables site-specific models to independently learn the distribution of healthy\ncases. To aggregate knowledge across the decentralized models while maintaining\nprivacy, we propose a Divergence Vector-Guided Model Merging approach,\nDivMerge, that combines site-specific models into a single VAD model without\ndata exchange. Our approach preserves domain-agnostic rich spatio-temporal\nrepresentations, ensuring generalization to unseen CHD cases. We evaluated our\napproach on real-world fetal US data collected from 5 hospital sites. Our\nmerged model outperformed site-specific models by 23.77% and 30.13% in accuracy\nand F1-score respectively on external test sets.",
      "generated_abstract": "l heart disease (CHD) is a leading cause of death among newborns\nand accounts for 25% to 30% of all infant deaths. The rapid advancement of\nultrasound technology has allowed the detection of CHD in fetuses. However,\nexisting methods struggle to generalize to unseen conditions and unseen\npatients. In this work, we introduce a novel framework for detecting CHD in\nfetal ultrasound videos using self-supervised learning and divergence vector\nguidance. Our framework consists of a self-supervised learning module\nlearning to predict the probability of CHD presence, a normality learning\nmodule to estimate normality scores, and a divergence vector-guided model\nmerging module to combine the normality scores with the predicted probability of\nCHD presence. The model is evaluated on a publicly available dataset of 100\nfetal ultras",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.345679012345679,
          "f": 0.23140495422409676
        },
        "rouge-2": {
          "r": 0.027149321266968326,
          "p": 0.05084745762711865,
          "f": 0.03539822555007412
        },
        "rouge-l": {
          "r": 0.17391304347826086,
          "p": 0.345679012345679,
          "f": 0.23140495422409676
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.02802v1",
      "true_abstract": "In this work, we show the first average-case reduction transforming the\nsparse Spiked Covariance Model into the sparse Spiked Wigner Model and as a\nconsequence obtain the first computational equivalence result between two\nwell-studied high-dimensional statistics models. Our approach leverages a new\nperturbation equivariance property for Gram-Schmidt orthogonalization, enabling\nremoval of dependence in the noise while preserving the signal.",
      "generated_abstract": "ish a computational equivalence between spiked covariance and spiked\nWigner models by applying Gram-Schmidt perturbation theory to a class of\n$n\\times n$ matrices with non-zero off-diagonal entries. The approach is\nderived via the connection between the two models and the linear matrix\ninequalities (LMI) that characterize them. The equivalence is established for\nboth discrete-time and continuous-time models, and the underlying LMI\nstructures are derived in both cases. Furthermore, we show that the equivalence\nis preserved under the addition of a constant off-diagonal matrix. As a\nconsequence, the computational equivalence allows us to leverage the spiked\nWigner model for the purpose of computing covariance matrices of high-dimensional\nstochastic processes, without requiring the computation of the full covariance\nmatrix. This enables the efficient implementation of various covariance\nest",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3673469387755102,
          "p": 0.225,
          "f": 0.2790697627306052
        },
        "rouge-2": {
          "r": 0.03636363636363636,
          "p": 0.01652892561983471,
          "f": 0.022727268430398538
        },
        "rouge-l": {
          "r": 0.30612244897959184,
          "p": 0.1875,
          "f": 0.2325581348236285
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10360v1",
      "true_abstract": "Time-frequency concentration and resolution of the Cohen's class\ntime-frequency distribution (CCTFD) has attracted much attention in\ntime-frequency analysis. A variety of uncertainty principles of the CCTFD is\ntherefore derived, including the weak Heisenberg type, the Hardy type, the\nNazarov type, and the local type. However, the standard Heisenberg type still\nremains unresolved. In this study, we address the question of how the standard\nHeisenberg's uncertainty principle of the CCTFD is affected by fundamental\nproperties. The investigated distribution properties are Parseval's relation\nand the concise frequency domain definition (i.e., only frequency variables are\nexplicitly found in the tensor product), based on which we confine our\nattention to the CCTFD with some specific kernels. That is the unit modulus and\nv-independent time translation, reversal and scaling invariant kernel CCTFD\n(UMITRSK-CCTFD). We then extend the standard Heisenberg's uncertainty\nprinciples of the Wigner distribution to those of the UMITRSK-CCTFD, giving\nbirth to various types of attainable lower bounds on the uncertainty product in\nthe UMITRSK-CCTFD domain. The derived results strengthen the existing weak\nHeisenberg type and fill gaps in the standard Heisenberg type.",
      "generated_abstract": "r investigates the standard Heisenberg's uncertainty principles for\nthe class time-frequency distribution with specific kernels. The Cohen's\nclass-time-frequency distribution is considered, and the standard Heisenberg's\nuncertainty principles are formulated based on the Weyl's theorem for the\nclass-time-frequency distribution. The Heisenberg's uncertainty principle for\nthe specific kernel is obtained by applying the Weyl's theorem to the\nclass-time-frequency distribution. Moreover, the uncertainty relations for\nthe specific kernel and the Cohen's class-time-frequency distribution are\nderived based on the Weyl's theorem. The main results are: the uncertainty\nprinciples for the class-time-frequency distribution are obtained under the\nassumption that the kernel is a positive definite function, and the\nuncertainty principle for the specific kernel is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21621621621621623,
          "p": 0.5333333333333333,
          "f": 0.30769230358727817
        },
        "rouge-2": {
          "r": 0.08860759493670886,
          "p": 0.19718309859154928,
          "f": 0.12227073807974691
        },
        "rouge-l": {
          "r": 0.2072072072072072,
          "p": 0.5111111111111111,
          "f": 0.29487179076676534
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2502.12453v1",
      "true_abstract": "Drug discovery is crucial for identifying candidate drugs for various\ndiseases.However, its low success rate often results in a scarcity of\nannotations, posing a few-shot learning problem. Existing methods primarily\nfocus on single-scale features, overlooking the hierarchical molecular\nstructures that determine different molecular properties. To address these\nissues, we introduce Universal Matching Networks (UniMatch), a dual matching\nframework that integrates explicit hierarchical molecular matching with\nimplicit task-level matching via meta-learning, bridging multi-level molecular\nrepresentations and task-level generalization. Specifically, our approach\nexplicitly captures structural features across multiple levels, such as atoms,\nsubstructures, and molecules, via hierarchical pooling and matching,\nfacilitating precise molecular representation and comparison. Additionally, we\nemploy a meta-learning strategy for implicit task-level matching, allowing the\nmodel to capture shared patterns across tasks and quickly adapt to new ones.\nThis unified matching framework ensures effective molecular alignment while\nleveraging shared meta-knowledge for fast adaptation. Our experimental results\ndemonstrate that UniMatch outperforms state-of-the-art methods on the\nMoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87% in AUROC and\n6.52% in delta AUPRC. UniMatch also shows excellent generalization ability on\nthe Meta-MolNet benchmark.",
      "generated_abstract": "ding the binding interactions between molecules is essential for\ndrug discovery, yet existing methods face significant limitations. First,\ncurrent methods focus on learning binding patterns, rather than identifying\nindividual molecules. Second, existing methods do not integrate molecular\ninformation to unify different molecular properties into a unified task. Third,\nexisting methods often rely on large pre-trained models, which are not\nsuitable for few-shot scenarios. To address these challenges, we introduce\nUniMatch, a novel few-shot learning framework that leverages universal\nmatching to discover the molecular properties of new molecules from a few\nsamples. UniMatch employs a unified task to match the new molecule to its\npre-trained task-specific matching model, which learns molecular properties\nfrom the pre-trained model. To tackle the challenges of large pre-trained\nmodels, we propose",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2116788321167883,
          "p": 0.3493975903614458,
          "f": 0.26363635893760334
        },
        "rouge-2": {
          "r": 0.0449438202247191,
          "p": 0.06896551724137931,
          "f": 0.05442176392984446
        },
        "rouge-l": {
          "r": 0.19708029197080293,
          "p": 0.3253012048192771,
          "f": 0.2454545407557852
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.17911v1",
      "true_abstract": "Speech enhancement plays an essential role in improving the quality of speech\nsignals in noisy environments. This paper investigates the efficacy of\nintegrating Bidirectional Gated Recurrent Units (BGRU) and Transformer models\nfor speech enhancement tasks. Through a comprehensive experimental evaluation,\nour study demonstrates the superiority of this hybrid architecture over\ntraditional methods and standalone models. The combined BGRU-Transformer\nframework excels in capturing temporal dependencies and learning complex signal\npatterns, leading to enhanced noise reduction and improved speech quality.\nResults show significant performance gains compared to existing approaches,\nhighlighting the potential of this integrated model in real-world applications.\nThe seamless integration of BGRU and Transformer architectures not only\nenhances system robustness but also opens the road for advanced speech\nprocessing techniques. This research contributes to the ongoing efforts in\nspeech enhancement technology and sets a solid foundation for future\ninvestigations into optimizing model architectures, exploring many application\nscenarios, and advancing the field of speech processing in noisy environments.",
      "generated_abstract": "vances in artificial intelligence have transformed speech\nrecognition, allowing for speech-based applications such as voice assistants,\nvoice biometrics, and speech-enabled virtual assistants. However, most existing\nmodels for speech recognition focus on the acoustic features of speech,\nneglecting the role of the speaker's voice in human-computer interactions. To\naddress this, we propose a novel architecture for speech recognition,\nintegrating a Bi-GRU-Transformer (BGT) model. BGT is a recently proposed\narchitecture that integrates the Bi-GRU module with the Transformer model. The\nBi-GRU module is designed to capture the speaker's voice, while the Transformer\nmodule is designed to extract the acoustic features of speech. Our experiments\nshow that our BGT model achieves significant improvements in both recognition\naccuracy and speech quality compared to baseline models. These improvements are",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19469026548672566,
          "p": 0.2716049382716049,
          "f": 0.22680411884738028
        },
        "rouge-2": {
          "r": 0.03355704697986577,
          "p": 0.044642857142857144,
          "f": 0.03831417134569432
        },
        "rouge-l": {
          "r": 0.17699115044247787,
          "p": 0.24691358024691357,
          "f": 0.20618556214634937
        }
      }
    },
    {
      "paper_id": "cs.HC.q-bio/NC/2502.17172v1",
      "true_abstract": "Affective computing has made significant strides in emotion recognition and\ngeneration, yet current approaches mainly focus on short-term pattern\nrecognition and lack a comprehensive framework to guide affective agents toward\nlong-term human well-being. To address this, we propose a teleology-driven\naffective computing framework that unifies major emotion theories (basic\nemotion, appraisal, and constructivist approaches) under the premise that\naffect is an adaptive, goal-directed process that facilitates survival and\ndevelopment. Our framework emphasizes aligning agent responses with both\npersonal/individual and group/collective well-being over extended timescales.\nWe advocate for creating a \"dataverse\" of personal affective events, capturing\nthe interplay between beliefs, goals, actions, and outcomes through real-world\nexperience sampling and immersive virtual reality. By leveraging causal\nmodeling, this \"dataverse\" enables AI systems to infer individuals' unique\naffective concerns and provide tailored interventions for sustained well-being.\nAdditionally, we introduce a meta-reinforcement learning paradigm to train\nagents in simulated environments, allowing them to adapt to evolving affective\nconcerns and balance hierarchical goals - from immediate emotional needs to\nlong-term self-actualization. This framework shifts the focus from statistical\ncorrelations to causal reasoning, enhancing agents' ability to predict and\nrespond proactively to emotional challenges, and offers a foundation for\ndeveloping personalized, ethically aligned affective systems that promote\nmeaningful human-AI interactions and societal well-being.",
      "generated_abstract": "years, affective computing has emerged as a significant field\nof research in the field of cognitive science. This field aims to study human\nemotions and behavior by incorporating computational models to understand how\nemotions influence human behavior. However, despite the growing interest in\ncomputational models, there is a lack of a unified theoretical framework that\ncan explain how emotions influence human behavior. This paper presents a\ncausal framework for understanding how affective computing models influence\nhuman well-being. The framework incorporates the concept of teleology, which\nexplains how causal relationships between entities are guided by the entities'\npurposes. We apply the framework to three affective computing models:\nemotional-behavioral model, emotion-driven model, and emotion-driven model. We\nexamine the effect of the model on human well-being and show that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14473684210526316,
          "p": 0.29333333333333333,
          "f": 0.19383259469424993
        },
        "rouge-2": {
          "r": 0.024630541871921183,
          "p": 0.042735042735042736,
          "f": 0.031249995361133504
        },
        "rouge-l": {
          "r": 0.14473684210526316,
          "p": 0.29333333333333333,
          "f": 0.19383259469424993
        }
      }
    },
    {
      "paper_id": "math.PR.nlin/CG/2411.15954v1",
      "true_abstract": "We introduce and study a symmetric, gradient exclusion process, in the class\nof non-cooperative kinetically constrained lattice gases, modelling a\nnon-linear diffusivity where mass transport is constrained by the local density\nnot being too small or too large. Maintaining the gradient property is the main\ntechnical challenge. The resulting model enjoys of properties in common with\nthe Bernstein polynomial basis, and is associated with the diffusion\ncoefficient $D_{n,k}(\\rho)=\\binom{n+k}{k}\\rho^n(1-\\rho)^k$, for $n,k$ arbitrary\nnatural numbers. The dynamics generalizes the Porous Media Model, and we show,\nvia the entropy method, the hydrodynamic limit for the empirical measure\nassociated with a perturbed, irreducible version of the process. The\nhydrodynamic equation is proved to be a Generalized Porous Media Equation.",
      "generated_abstract": "We develop a gradient-based method for computing the Bernstein polynomial\nbasis in a non-smooth setting. The method is based on a dual representation of\nthe basis, which allows us to formulate the problem as a convex optimization\nproblem. We show that the gradient of the objective function is given by the\nderivative of the monomial expansion of the basis. As a result, we obtain a\nconvex-concave primal-dual algorithm for computing the basis. The algorithm is\nfast and stable. The numerical results demonstrate the effectiveness of the\nmethod.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1951219512195122,
          "p": 0.2909090909090909,
          "f": 0.23357663752996974
        },
        "rouge-2": {
          "r": 0.04504504504504504,
          "p": 0.06329113924050633,
          "f": 0.05263157408919713
        },
        "rouge-l": {
          "r": 0.18292682926829268,
          "p": 0.2727272727272727,
          "f": 0.21897809738398433
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/ST/2411.19444v3",
      "true_abstract": "The Capital Asset Pricing Model (CAPM) relates a well-diversified stock\nportfolio to a benchmark portfolio. We insert size effect in CAPM, capturing\nthe observation that small stocks have higher risk and return than large\nstocks, on average. Dividing stock index returns by the Volatility Index makes\nthem independent and normal. In this article, we combine these ideas to create\na new discrete-time model, which includes volatility, relative size, and CAPM.\nWe fit this model using real-world data, prove the long-term stability, and\nconnect this research to Stochastic Portfolio Theory. We fill important gaps in\nour previous article on CAPM with the size factor.",
      "generated_abstract": "This paper proposes a new capital asset pricing model with size factor and\nnormally distributed volatility index. The model is formulated as a\nregression model with size factor and volatility index as predictors.\nFurthermore, it is formulated as a multiplicative model with size factor and\nvolatility index as predictors. The model is estimated using the maximum likelihood\nestimator. Simulation results are presented to demonstrate the effectiveness of\nthe proposed models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14457831325301204,
          "p": 0.2857142857142857,
          "f": 0.19199999553792008
        },
        "rouge-2": {
          "r": 0.0196078431372549,
          "p": 0.0392156862745098,
          "f": 0.026143786405229515
        },
        "rouge-l": {
          "r": 0.13253012048192772,
          "p": 0.2619047619047619,
          "f": 0.17599999553792012
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/LG/2503.10566v1",
      "true_abstract": "Despite their remarkable performance, large language models lack elementary\nsafety features, and this makes them susceptible to numerous malicious attacks.\nIn particular, previous work has identified the absence of an intrinsic\nseparation between instructions and data as a root cause for the success of\nprompt injection attacks. In this work, we propose an architectural change,\nASIDE, that allows the model to clearly separate between instructions and data\nby using separate embeddings for them. Instead of training the embeddings from\nscratch, we propose a method to convert an existing model to ASIDE form by\nusing two copies of the original model's embeddings layer, and applying an\northogonal rotation to one of them. We demonstrate the effectiveness of our\nmethod by showing (1) highly increased instruction-data separation scores\nwithout a loss in model capabilities and (2) competitive results on prompt\ninjection benchmarks, even without dedicated safety training. Additionally, we\nstudy the working mechanism behind our method through an analysis of model\nrepresentations.",
      "generated_abstract": "on-level parallel (ILP) architectures, such as RISC-V, enable\ninstruction-level parallelism (ILP) by dividing instructions into independent\nand independent-of-each-other units. In this paper, we present ASIDE, a\nlanguage-model-agnostic instruction-level parallel (ILP) architecture that\nenables data-level parallelism. ASIDE is composed of a data-level ILP\nsub-architecture, called Data-ILP, and a data-level ILP sub-architecture,\ncalled Data-ILP, that enables instruction-level parallelism. Data-ILP\nenables ILP by providing a way for an instruction to be split into independent\nand independent-of-each-other units, while Data-ILP enables ILP by providing a\nway for an instruction to be executed independently of its data. We demonstrate\nthe performance of ASIDE using a large language",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21153846153846154,
          "p": 0.36666666666666664,
          "f": 0.26829267828673414
        },
        "rouge-2": {
          "r": 0.02666666666666667,
          "p": 0.05063291139240506,
          "f": 0.03493449329722986
        },
        "rouge-l": {
          "r": 0.19230769230769232,
          "p": 0.3333333333333333,
          "f": 0.2439024343842951
        }
      }
    },
    {
      "paper_id": "physics.atom-ph.physics/atm-clus/2503.08972v1",
      "true_abstract": "Out-of-equilibrium Rydberg gases exhibit emergent many-body phases due to\nmode competition. Sustained limit cycle oscillations (OSC) emerge when driven\nby B-fields at room-temperature, forming robust Rydberg dissipative time\ncrystals (DTC). These driven-dissipative Rydberg DTC have recently been shown\nto develop an effective transition centered at the OSC frequency (-10dB\nbandwidth of ~1.7kHz, centered at 9.8kHz). Weak RF signals injected within this\nemergent transition perturb and emerge on the OSC spectrum, from which\nsensitive and high-resolution sensing of E-fields (~1.6-2.3 uVcm-1Hz-1/2) near\nthe OSC frequencies can be achieved. In this article, it is demonstrated that\nDC and AC Stark fields in the sub-kHz regime can be used effectively to shift\n(DC) or modulate (AC) the OSC frequency of Rydberg DTC at room-temperature. The\nAC-Stark driven modulation of the OSC is shown as an effective technique to\nsense weak AC E-fields in the sub-kHz regime. With a modest setup, a\nsensitivity of ~7.8 uVcm-1Hz-1/2 for AC signals at 300Hz (~8.7x improvement\nover state-of-art Rydberg atom techniques), and high-resolution detection to as\nlow as sub-Hz is demonstrated. This approach enables the development of\nultra-compact, extremely low-frequency E-field detectors for applications in\nremote sensing, communications, navigation, and bio-medical technologies.",
      "generated_abstract": "t a new approach to Rydberg-dissipative time crystals (RdTCs) at\nrooT-temperature. We demonstrate that a simple and scalable approach to\nRdTCs at room-temperature can be used to generate a time-modulated Rydberg\ndissipative electric-field (DEF) oscillator with an unprecedented tunability,\nenabling the implementation of a low-noise, high-resolution, and low-power\nsub-kHz EDF sensor. Our approach combines a simple, efficient RdTC synthesis\nscheme with the use of a high-finesse cavity to produce a tunable and\nsteady-state time-modulated Rydberg dissipative DEF oscillator. We demonstrate\nthat this approach enables the tunable generation of a low-noise, high-resolution\nand low-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15,
          "p": 0.3387096774193548,
          "f": 0.20792078782472317
        },
        "rouge-2": {
          "r": 0.03763440860215054,
          "p": 0.08333333333333333,
          "f": 0.05185184756543246
        },
        "rouge-l": {
          "r": 0.1357142857142857,
          "p": 0.3064516129032258,
          "f": 0.18811880762670333
        }
      }
    },
    {
      "paper_id": "cond-mat.supr-con.cond-mat/supr-con/2503.10040v1",
      "true_abstract": "Delineating the superconducting order parameters is a pivotal task in\ninvestigating superconductivity for probing pairing mechanisms, as well as\ntheir symmetry and topology. Point-contact Andreev reflection (PCAR)\nmeasurement is a simple yet powerful tool for identifying the order parameters.\nThe PCAR spectra exhibit significant variations depending on the type of the\norder parameter in a superconductor, including its magnitude\n($\\mathit{\\Delta}$), as well as temperature, interfacial quality, Fermi\nvelocity mismatch, and other factors. The information on the order parameter\ncan be obtained by finding the combination of these parameters, generating a\ntheoretical spectrum that fits a measured experimental spectrum. However, due\nto the complexity of the spectra and the high dimensionality of parameters,\nextracting the fitting parameters is often time-consuming and labor-intensive.\nIn this study, we employ a convolutional neural network (CNN) algorithm to\ncreate models for rapid and automated analysis of PCAR spectra of various\nsuperconductors with different pairing symmetries (conventional $s$-wave,\nchiral $p_x+ip_y$-wave, and $d_{x^2-y^2}$-wave). The training datasets are\ngenerated based on the Blonder-Tinkham-Klapwijk (BTK) theory and further\nmodified and augmented by selectively incorporating noise and peaks according\nto the bias voltages. This approach not only replicates the experimental\nspectra but also brings the model's attention to important features within the\nspectra. The optimized models provide fitting parameters for experimentally\nmeasured spectra in less than 100 ms per spectrum. Our approaches and findings\npave the way for rapid and automated spectral analysis which will help\naccelerate research on superconductors with complex order parameters.",
      "generated_abstract": "uce a machine learning-based approach for rapid and accurate\nanalysis of point-contact Andreev reflection spectra. Our approach leverages\nthe fact that Andreev reflection spectra are well-described by a\nmulti-channel model, where the channel $n$ is determined by the applied bias\nvoltage $V_n$. We first train a model to identify the channel $n$ from the\napplied voltage data. Then, we use the trained model to identify the applied\nvoltage data in the absence of the channel $n$. This procedure reduces the\nnumber of data points to be analyzed by a factor of 2, which allows us to\nperform a rapid analysis of the data with a high accuracy. We demonstrate the\nuse of the proposed approach for analysis of Andreev reflection spectra from\npoint contacts of the MnSi/Si heterostructure, where the device parameters are\nhighly controlled, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16352201257861634,
          "p": 0.35135135135135137,
          "f": 0.22317596133065634
        },
        "rouge-2": {
          "r": 0.02242152466367713,
          "p": 0.044642857142857144,
          "f": 0.02985074181759924
        },
        "rouge-l": {
          "r": 0.1509433962264151,
          "p": 0.32432432432432434,
          "f": 0.20600857935640748
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.cond-mat/dis-nn/2503.09518v1",
      "true_abstract": "We generalize the computation of the capacity of exponential Hopfield model\nfrom Lucibello and M\\'ezard (2024) to more generic pattern ensembles, including\nbinary patterns and patterns generated from a hidden manifold model.",
      "generated_abstract": "We consider a class of Hopfield networks that are designed to capture the\ndata manifold hypothesis, which posits that the topology of the data manifold\ndetermines the optimal architecture of the Hopfield network. We show that\nthese networks exhibit a capacity of approximately $11$, which is significantly\nlarger than that of the standard Hopfield networks. We further show that the\ncapacity of these networks can be made arbitrarily large by tuning the parameter\n$\\epsilon$, the cutoff for the activation function. In addition, we show that\nthese networks exhibit a universal behavior, with a global maximum capacity of\n$11$ and a minimum capacity of $10$. This is in contrast to the standard Hopfield\nnetworks, which exhibit a minimum capacity of $1$ and a maximum capacity of $2$.\nMoreover, we show that the capacity of these networks is independent of the\ndimension of the data manifold.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.34615384615384615,
          "p": 0.13043478260869565,
          "f": 0.18947368023490313
        },
        "rouge-2": {
          "r": 0.0967741935483871,
          "p": 0.028037383177570093,
          "f": 0.043478257386053626
        },
        "rouge-l": {
          "r": 0.3076923076923077,
          "p": 0.11594202898550725,
          "f": 0.16842104865595575
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08756v1",
      "true_abstract": "The diagnosis of brain tumours is an extremely sensitive and complex clinical\ntask that must rely upon information gathered through non-invasive techniques.\nOne such technique is magnetic resonance, in the modalities of imaging or\nspectroscopy. The latter provides plenty of metabolic information about the\ntumour tissue, but its high dimensionality makes resorting to pattern\nrecognition techniques advisable. In this brief paper, an international\ndatabase of brain tumours is analyzed resorting to an ad hoc spectral frequency\nselection procedure combined with nonlinear classification.",
      "generated_abstract": "ours are a major cause of mortality in children and young adults.\nFrequency-domain analysis has emerged as a powerful tool for the identification\nand characterization of brain tumours. While many methods have been developed\nfor frequency-domain analysis, their performance is often limited by\ncomputational complexity and lack of robustness to noise. In this paper, we\npropose a novel approach for the diagnosis of brain tumours that is based on\nfrequency-domain analysis. The proposed method utilizes a sparse representation\nof the brain tumour signal, which can be obtained by using an MRI sequence\naligned to the brain tumour with a multi-channel proton density scan. In our\nexperiments, we demonstrate that our approach outperforms other methods\ninvolving MRI alignment, including the recently proposed method by\nSantos-Perez et al. (2025). Additionally, we show that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2537313432835821,
          "p": 0.1827956989247312,
          "f": 0.21249999513203138
        },
        "rouge-2": {
          "r": 0.05194805194805195,
          "p": 0.03225806451612903,
          "f": 0.03980099029826051
        },
        "rouge-l": {
          "r": 0.23880597014925373,
          "p": 0.17204301075268819,
          "f": 0.1999999951320314
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.10489v1",
      "true_abstract": "Molecular pretrained representations (MPR) has emerged as a powerful approach\nfor addressing the challenge of limited supervised data in applications such as\ndrug discovery and material design. While early MPR methods relied on 1D\nsequences and 2D graphs, recent advancements have incorporated 3D\nconformational information to capture rich atomic interactions. However, these\nprior models treat molecules merely as discrete atom sets, overlooking the\nspace surrounding them. We argue from a physical perspective that only modeling\nthese discrete points is insufficient. We first present a simple yet insightful\nobservation: naively adding randomly sampled virtual points beyond atoms can\nsurprisingly enhance MPR performance. In light of this, we propose a principled\nframework that incorporates the entire 3D space spanned by molecules. We\nimplement the framework via a novel Transformer-based architecture, dubbed\nSpaceFormer, with three key components: (1) grid-based space discretization;\n(2) grid sampling/merging; and (3) efficient 3D positional encoding. Extensive\nexperiments show that SpaceFormer significantly outperforms previous 3D MPR\nmodels across various downstream tasks with limited data, validating the\nbenefit of leveraging the additional 3D space beyond atoms in MPR models.",
      "generated_abstract": "vances in deep learning have enabled state-of-the-art performance in\nlarge molecular data pre-training, which has proven highly effective in\npredicting molecular properties. However, current pre-training methods are\ndominated by single-step molecular representation learning and struggle to\ncapture the complex molecular interactions across multiple steps. In this work,\nwe propose a novel molecular pre-training framework, called Large-Scale 3D\nMolecular Pretraining (LSM3D), which integrates 3D molecular representation\nlearning and large molecular data pre-training. We introduce a novel 3D\nrepresentation learning module that aligns molecular representations with\nneural representations, enabling effective molecular interaction learning.\nFurthermore, we introduce a novel large molecular data pre-training strategy\nthat leverages molecular property prediction as a multi-step task, which\nen",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18840579710144928,
          "p": 0.3561643835616438,
          "f": 0.24644549310482694
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.050505050505050504,
          "f": 0.03649634574964095
        },
        "rouge-l": {
          "r": 0.15942028985507245,
          "p": 0.3013698630136986,
          "f": 0.20853080116169906
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.03620v1",
      "true_abstract": "Reconfigurable antennas possess the capability to dynamically adjust their\nfundamental operating characteristics, thereby enhancing system adaptability\nand performance. To fully exploit this flexibility in modern wireless\ncommunication systems, this paper considers a novel tri-hybrid beamforming\narchitecture, which seamlessly integrates pattern-reconfigurable antennas with\nboth analog and digital beamforming. The proposed tri-hybrid architecture\noperates across three layers: (\\textit{i}) a radiation beamformer in the\nelectromagnetic (EM) domain for dynamic pattern alignment, (\\textit{ii}) an\nanalog beamformer in the radio-frequency (RF) domain for array gain\nenhancement, and (\\textit{iii}) a digital beamformer in the baseband (BB)\ndomain for multi-user interference mitigation. To establish a solid theoretical\nfoundation, we first develop a comprehensive mathematical model for the\ntri-hybrid beamforming system and formulate the signal model for a multi-user\nmulti-input single-output (MU-MISO) scenario. The optimization objective is to\nmaximize the sum-rate while satisfying practical constraints. Given the\nchallenges posed by high pilot overhead and computational complexity, we\nintroduce an innovative tri-timescale beamforming framework, wherein the\nradiation beamformer is optimized over a long-timescale, the analog beamformer\nover a medium-timescale, and the digital beamformer over a short-timescale.\nThis hierarchical strategy effectively balances performance and implementation\nfeasibility. Simulation results validate the performance gains of the proposed\ntri-hybrid architecture and demonstrate that the tri-timescale design\nsignificantly reduces pilot overhead and computational complexity, highlighting\nits potential for future wireless communication systems.",
      "generated_abstract": "r proposes a novel tri-timescale beamforming design for tri-hybrid\narchitectures based on Reconfigurable Antenna (ReconAns). The proposed beamforming\ndesign is a combination of the ReconAns-based beamforming with the\nmulti-frequency-multi-antenna (MFMA) approach. The MFMA approach combines the\nfrequency-domain MFMA (F-MFMA) with the antenna-domain MFMA (A-MFMA), which\nprovides a more flexible beamforming design than the F-MFMA. The F-MFMA\novercomes the limitations of conventional MFMA by utilizing the time-domain MFMA\n(T-MFMA), which allows for a higher degree of flexibility in beamforming\ndesigns. The F-MFMA also offers the ability to handle varying",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1366906474820144,
          "p": 0.3333333333333333,
          "f": 0.19387754689556447
        },
        "rouge-2": {
          "r": 0.030612244897959183,
          "p": 0.07228915662650602,
          "f": 0.043010748508370035
        },
        "rouge-l": {
          "r": 0.1223021582733813,
          "p": 0.2982456140350877,
          "f": 0.1734693836302583
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2502.13238v1",
      "true_abstract": "Uncertainty quantification in causal inference settings with random network\ninterference is a challenging open problem. We study the large sample\ndistributional properties of the classical difference-in-means Hajek treatment\neffect estimator, and propose a robust inference procedure for the\n(conditional) direct average treatment effect, allowing for cross-unit\ninterference in both the outcome and treatment equations. Leveraging ideas from\nstatistical physics, we introduce a novel Ising model capturing interference in\nthe treatment assignment, and then obtain three main results. First, we\nestablish a Berry-Esseen distributional approximation pointwise in the degree\nof interference generated by the Ising model. Our distributional approximation\nrecovers known results in the literature under no-interference in treatment\nassignment, and also highlights a fundamental fragility of inference procedures\ndeveloped using such a pointwise approximation. Second, we establish a uniform\ndistributional approximation for the Hajek estimator, and develop robust\ninference procedures that remain valid regardless of the unknown degree of\ninterference in the Ising model. Third, we propose a novel resampling method\nfor implementation of robust inference procedure. A key technical innovation\nunderlying our work is a new \\textit{De-Finetti Machine} that facilitates\nconditional i.i.d. Gaussianization, a technique that may be of independent\ninterest in other settings.",
      "generated_abstract": "We consider the estimation and inference for the average treatment effect\n(ATE) with treatment assignment interference. We first establish the\nconsistency of the standard estimator and propose a robust estimator. We then\nprove the asymptotic normality and propose inference for the robust estimator.\nWe demonstrate the effectiveness of the proposed method through two applications\nin empirical applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1565217391304348,
          "p": 0.5,
          "f": 0.23841059239507045
        },
        "rouge-2": {
          "r": 0.041176470588235294,
          "p": 0.14,
          "f": 0.06363636012396715
        },
        "rouge-l": {
          "r": 0.13043478260869565,
          "p": 0.4166666666666667,
          "f": 0.1986754930573221
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2503.03783v2",
      "true_abstract": "Resting heart rate (RHR) is an important biomarker of cardiovascular health\nand mortality, but tracking it longitudinally generally requires a wearable\ndevice, limiting its availability. We present PHRM, a deep learning system for\npassive heart rate (HR) and RHR measurements during everyday smartphone use,\nusing facial video-based photoplethysmography. Our system was developed using\n225,773 videos from 495 participants and validated on 185,970 videos from 205\nparticipants in laboratory and free-living conditions, representing the largest\nvalidation study of its kind. Compared to reference electrocardiogram, PHRM\nachieved a mean absolute percentage error (MAPE) < 10% for HR measurements\nacross three skin tone groups of light, medium and dark pigmentation; MAPE for\neach skin tone group was non-inferior versus the others. Daily RHR measured by\nPHRM had a mean absolute error < 5 bpm compared to a wearable HR tracker, and\nwas associated with known risk factors. These results highlight the potential\nof smartphones to enable passive and equitable heart health monitoring.",
      "generated_abstract": "e (HR) monitoring is essential for understanding the physiological\nbehavior of individuals. However, HR monitoring is often challenging due to\ntheir complex physiological and psychological dynamics. Smartphone applications\ncan provide a viable alternative to traditional HR monitoring methods.\nHowever, the integration of HR monitoring into smartphone applications presents\na significant challenge. This study explores how HR monitoring is integrated\ninto smartphone applications, focusing on passive HR monitoring. We review the\nliterature on passive HR monitoring, focusing on HR variability and how HR\nmonitoring influences smartphone usage behavior. We then conducted a literature\nreview to identify existing smartphone applications that integrate HR\nmonitoring. We analyzed the existing applications and their integration methods\nto identify challenges and potential solutions. The analysis revealed a\ncomprehensive understanding of HR monitoring integration in smartphone",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14655172413793102,
          "p": 0.2463768115942029,
          "f": 0.1837837791065012
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12931034482758622,
          "p": 0.21739130434782608,
          "f": 0.1621621574848796
        }
      }
    },
    {
      "paper_id": "physics.flu-dyn.nlin/CG/2502.16568v1",
      "true_abstract": "Quantum computing holds great promise to accelerate scientific computations\nin fluid dynamics and other classical physical systems. While various quantum\nalgorithms have been proposed for linear flows, developing quantum algorithms\nfor nonlinear problems remains a significant challenge. We introduce a novel\nnode-level ensemble description of lattice gas for simulating nonlinear fluid\ndynamics on a quantum computer. This approach combines the advantages of the\nlattice Boltzmann method, which offers low-dimensional representation, and\nlattice gas cellular automata, which provide linear collision treatment.\nBuilding on this framework, we propose a quantum lattice Boltzmann method that\nrelies on linear operations with medium dimensionality. We validated the\nalgorithm through comprehensive simulations of benchmark cases, including\nvortex-pair merging and decaying turbulence on $2048^2$ computational grid\npoints. The results demonstrate remarkable agreement with direct numerical\nsimulation, effectively capturing the essential nonlinear mechanisms of fluid\ndynamics. This work offers valuable insights into developing quantum algorithms\nfor other nonlinear problems, and potentially advances the application of\nquantum computing across various transport phenomena in engineering.",
      "generated_abstract": "We present a quantum lattice Boltzmann (QLB) method for simulating\nnonlinear fluid dynamics, specifically the Navier-Stokes equations. Our method\nextends the conventional lattice Boltzmann method (LBM) to include the\ninclusion of quantum mechanical effects. We apply our method to a\ntwo-dimensional problem of the nonlinear Navier-Stokes equations. Our simulations\nshow that the quantum effects reduce the dissipation of momentum in the\nsystem, resulting in a reduced numerical viscosity. We also compare our results\nwith the conventional LBM, demonstrating the effectiveness of the QLB method in\nsimulating nonlinear fluid dynamics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17543859649122806,
          "p": 0.38461538461538464,
          "f": 0.2409638511191755
        },
        "rouge-2": {
          "r": 0.06451612903225806,
          "p": 0.12345679012345678,
          "f": 0.08474575820346189
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.36538461538461536,
          "f": 0.2289156583480912
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/GR/2503.10624v1",
      "true_abstract": "Fitting a body to a 3D clothed human point cloud is a common yet challenging\ntask. Traditional optimization-based approaches use multi-stage pipelines that\nare sensitive to pose initialization, while recent learning-based methods often\nstruggle with generalization across diverse poses and garment types. We propose\nEquivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline\nthat estimates cloth-to-body surface mapping through locally approximate SE(3)\nequivariance, encoding tightness as displacement vectors from the cloth surface\nto the underlying body. Following this mapping, pose-invariant body features\nregress sparse body markers, simplifying clothed human fitting into an\ninner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show\nthat ETCH significantly outperforms state-of-the-art methods -- both\ntightness-agnostic and tightness-aware -- in body fitting accuracy on loose\nclothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant\ntightness design can even reduce directional errors by (67.2% ~ 89.8%) in\none-shot (or out-of-distribution) settings. Qualitative results demonstrate\nstrong generalization of ETCH, regardless of challenging poses, unseen shapes,\nloose clothing, and non-rigid dynamics. We will release the code and models\nsoon for research purposes at https://boqian-li.github.io/ETCH/.",
      "generated_abstract": "vancements in deep learning have enabled the automatic generation of\nclothed human meshes from photorealistic images. However, current methods\noften struggle to generalize to unseen poses, poses with different clothes, or\nposes with different lighting conditions. To address this limitation, we propose\nETCH, a novel method that leverages Equivariant Tightness (ET) to capture the\nequivalence between body parts. ET measures the similarity between a human\nmesh and a clothed mesh, with respect to the underlying body mesh. Our method\nrelies on a large-scale dataset of clothed human meshes and the corresponding\nclothed human meshes from the same poses, and it is equivariant to body\norientation. Extensive experiments on multiple datasets demonstrate that our\nmethod consistently outperforms existing methods in both equivariance and\ngeneralization, achieving state-of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2348993288590604,
          "p": 0.4117647058823529,
          "f": 0.29914529451932215
        },
        "rouge-2": {
          "r": 0.06451612903225806,
          "p": 0.10344827586206896,
          "f": 0.07947019394412555
        },
        "rouge-l": {
          "r": 0.21476510067114093,
          "p": 0.3764705882352941,
          "f": 0.2735042688782965
        }
      }
    },
    {
      "paper_id": "q-bio.SC.q-bio/SC/2407.18237v2",
      "true_abstract": "Transport of dense core vesicles (DCVs) in neurons is crucial for\ndistributing molecules like neuropeptides and growth factors. We studied the\nexperimental trajectories of dynein-driven directed movement of DCVs in the ALA\nneuron C. elegans over a duration of up to 6 seconds. We analysed the DCV\nmovement in three strains of C. elegans: 1) with normal kinesin-1 function, 2)\nwith reduced function in kinesin light chain 2 (KLC-2), and 3) a null mutation\nin kinesin light chain 1 (KLC-1). We find that DCVs move superdiffusively with\ndisplacement variance $var(x) \\sim t^2$ in all three strains with low reversal\nrates and frequent immobilization of DCVs. The distribution of DCV\ndisplacements fits a beta-binomial distribution with the mean and the variance\nfollowing linear and quadratic growth patterns, respectively. We propose a\nsimple heterogeneous random walk model to explain the observed superdiffusive\nretrograde transport behaviour of DCV movement. This model involves a random\nprobability with the beta density for a DCV to resume its movement or remain in\nthe same position.",
      "generated_abstract": "ain, dense-core vesicles (DCVs) play a crucial role in synaptic\nprocesses. Recent work has shown that DCVs exhibit superdiffusive movement,\nwhereby they travel at rates several orders of magnitude higher than normal\nvesicle velocities. However, the mechanisms driving these fast movements are\nstill unclear. Here, we propose a model for superdiffusive DCV motion based on\na two-dimensional particle-in-cell simulation of the entire brain. Our model\nincludes DCVs, axons, dendrites, and synapses, and allows for inter-particle\ninteractions that can induce superdiffusive motion. We find that DCVs\nsuperdiffuse in the cerebral cortex and in the hippocampus, with speeds\napproaching those of a non-superdiffusive reference particle. Additionally,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.23809523809523808,
          "f": 0.20618556210011701
        },
        "rouge-2": {
          "r": 0.03773584905660377,
          "p": 0.0594059405940594,
          "f": 0.046153841402663216
        },
        "rouge-l": {
          "r": 0.17272727272727273,
          "p": 0.2261904761904762,
          "f": 0.19587628374960162
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.09934v1",
      "true_abstract": "It is time to move on from attempts to make the pharmacy benefit manager\n(PBM) reseller business model more transparent. Time and time again the Big 3\nPBMs have developed opaque alternatives to piece-meal 100% pass-through\nmandates. Time and time again PBMs have demonstrated expertise in finding\nloopholes in state government disclosure laws. The purpose of this paper is to\nprovide quantitative estimates of two transparent insurance business models as\na solution to the PBM agency issue. The key parameter used is an 8% gross\nprofit margin figure disclosed by the Big 3 PBMs themselves. Based on reported\ndrug trend delivered to plans, we use a $1,200 to $1,500 per member per year\n(PMPY) as the range for this key performance indicator (KPI). We propose that\ndiscussions of PBM insurance business models start with the following figures:\n(1) a fixed premium model with medical loss ratio ranging from 92% to 85%; (2)\na fee-for-service model ranging from $96 to $180 PMPY with risk sharing of\ndeviations from a contracted PMPY delivered drug spend.",
      "generated_abstract": "Pharmacy benefit managers (PBMs) are a rapidly growing and evolving\nproviding managed care for prescription drug benefits. Insurance companies\ntypically pay pharmacy benefit managers for managing drug benefits. However,\nincreasingly, pharmacy benefit managers are also providing benefits, such as\ndental and vision care, to customers. This paper explores how such a\nmulti-product PBM can be designed and structured to meet the needs of its\ncustomers. Using a simple case study, we discuss the key elements of a PBM\nbusiness model and identify key considerations for designing a PBM business\nmodel.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13675213675213677,
          "p": 0.25806451612903225,
          "f": 0.17877094519272194
        },
        "rouge-2": {
          "r": 0.012345679012345678,
          "p": 0.024390243902439025,
          "f": 0.016393438160441955
        },
        "rouge-l": {
          "r": 0.1282051282051282,
          "p": 0.24193548387096775,
          "f": 0.16759776083518005
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.04100v1",
      "true_abstract": "We show how to improve the discrepancy of an iid sample by moving only a few\npoints. Specifically, modifying \\( O(m) \\) sample points on average reduces the\nKolmogorov-Smirnov distance to the population distribution to \\(1/m\\).",
      "generated_abstract": "er the problem of improving the discrepancy of a random variable\n$X$ by moving a few points. For example, we might like to use a few points\nto improve the discrepancy of a multivariate Gaussian random vector. In this\npaper, we develop a simple yet efficient method to improve the discrepancy of\na random variable $X$ by moving a few points. The main contribution of this\npaper is the improvement of the discrepancy of $X$ by moving a few points.\nIn addition, we also develop a method to improve the discrepancy of a\nmultivariate Gaussian random vector by moving a few points. We prove that the\ndiscrepancy of the multivariate Gaussian random vector improved by moving a\nfew points is at most the same as the discrepancy of the multivariate Gaussian\nrandom vector improved by moving $O(\\sqrt{n})$ points",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.36666666666666664,
          "p": 0.22,
          "f": 0.2749999953125
        },
        "rouge-2": {
          "r": 0.2,
          "p": 0.09090909090909091,
          "f": 0.12499999570312517
        },
        "rouge-l": {
          "r": 0.36666666666666664,
          "p": 0.22,
          "f": 0.2749999953125
        }
      }
    },
    {
      "paper_id": "math.AG.math/KT/2503.09928v1",
      "true_abstract": "Given a compact Lie group $G$ acting on a space $X$, the classical\nAtiyah-Segal completion theorem identifies topological $K$-theory of the\nhomotopy quotient $X/G$ with an explicit completion of $G$-equivariant\ntopological $K$-theory of $X$. We prove an analog of this result for algebraic\n$K$-theory over a field of characteristic 0. In our setting $G$ is a reductive\ngroup that acts on a derived algebraic space $X$ with the assumption that all\nstabilizer groups are nice (in the sense of Alper). Our main result identifies\nthe value $R^{\\mathrm{dAff}}K([X/G])$ of right Kan extension of the $K$-theory\nfunctor from schemes to stacks with the completion of $K$-theory of the\ncategory $\\mathrm{Perf}([X/G])$ at the augmentation ideal of\n$K_0(\\mathrm{Rep}(G))$. The main novelty of our results is that $X$ is allowed\nto be singular or even derived. This generality is achieved by employing and\nimproving analogous versions of completion theorem for negative cyclic homology\n(after Ben-Zvi--Nadler and Chen) and for homotopy $K$-theory (after van den\nBergh--Tabuada). We also show that in the singular setting the completion\ntheorem does not necessarily hold without the nice stabilizer assumption. We\nview our results as a part of the general paradigm of extending the motivic\nfiltration on algebraic $K$-theory of schemes to algebraic $K$-theory of\nstacks.",
      "generated_abstract": "aper, we investigate the filtered algebraic $K$-theory of smooth\nprincipal $G$-bundles over a smooth projective variety $X$ for $G$ a\nsemisimple Lie group. We define a filtration of algebraic $K$-theory\n$\\mathcal{K}^*_G(X)$ by the action of a semi-simple Lie group $G$ on the\nHausdorff category of the stack of principal $G$-bundles over $X$, and\nconstruct the first two terms of this filtration for any smooth projective\nvariety $X$. This filtration is compatible with the usual algebraic $K$-theory\nfiltration $\\mathcal{K}^*_G(X)$ for the group $G$ as well as the filtration\ninduced by the Eilenberg-Maclane spectrum $K(\\mathrm{pt},G)$ of $G$. We also\nobtain a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20869565217391303,
          "p": 0.4528301886792453,
          "f": 0.28571428139526645
        },
        "rouge-2": {
          "r": 0.0481283422459893,
          "p": 0.10227272727272728,
          "f": 0.06545454110254575
        },
        "rouge-l": {
          "r": 0.1826086956521739,
          "p": 0.39622641509433965,
          "f": 0.24999999568098083
        }
      }
    },
    {
      "paper_id": "math.QA.math/CT/2503.06280v1",
      "true_abstract": "Hopf braces are the quantum analogues of skew braces and, as such, their\ncocommutative counterparts provide solutions to the quantum Yang-Baxter\nequation. We investigate various properties of categories related to Hopf\nbraces. In particular, we prove that the category of Hopf braces is accessible\nwhile the category of cocommutative Hopf braces is even locally presentable. We\nalso show that functors forgetting multiple antipodes and/or multiplications\ndown to coalgebras are monadic. Colimits in the category of cocommutative Hopf\nbraces are described explicitly and a free cocommutative Hopf brace on an\narbitrary cocommutative Hopf algebra is constructed.",
      "generated_abstract": "In this paper, we introduce a category of Hopf braces and show that it is a\nstrictly\nassociative monoidal category with unit the trivial Hopf brace. We also\ndemonstrate that the category of Hopf braces is a strict commutative monoidal\ncategory with unit the trivial Hopf algebra.\n  We show that the category of Hopf braces is a strict braided monoidal category\nwith unit the trivial Hopf algebra.\n  We also prove that the category of Hopf braces is a strict braided strict\ncommutative monoidal category with unit the trivial Hopf algebra.\n  We discuss the category of Hopf braces as a strict braided strict commutative\nmonoidal category with unit the trivial Hopf algebra.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.29508196721311475,
          "p": 0.5454545454545454,
          "f": 0.382978718847895
        },
        "rouge-2": {
          "r": 0.13924050632911392,
          "p": 0.23404255319148937,
          "f": 0.1746031699256741
        },
        "rouge-l": {
          "r": 0.29508196721311475,
          "p": 0.5454545454545454,
          "f": 0.382978718847895
        }
      }
    },
    {
      "paper_id": "gr-qc.gr-qc/2503.09731v1",
      "true_abstract": "We conduct two searches for continuous, nearly monochromatic gravitational\nwaves originating from the central compact objects in the supernova remnants\nCassiopeia A and Vela Jr. using public LIGO data. The search for Cassiopeia A\ntargets signal frequencies between 20 Hz and 400 Hz; the Vela Jr. search\nbetween 400 Hz and 1700 Hz, and both investigate the broadest set of waveforms\never considered with highly sensitive deterministic search methods. Above 1500\nHz the Vela Jr. search is the most sensitive carried out thus far, improving on\nprevious results by over 300\\%. Above 976 Hz these results improve on existing\nones by 50\\%. In all we investigate over $10^{18}$ waveforms, leveraging the\ncomputational power donated by thousands of Einstein@Home volunteers. We\nperform a 4-stage follow-up on more than 6 million waveforms. None of the\nconsidered waveforms survives the follow-up scrutiny, indicating no significate\ndetection candidate. Our null results constrain the maximum amplitude of\ncontinuous signals as a function of signal frequency from the targets. The most\nstringent 90\\% confidence upper limit for Cas A is $h_0^{90 \\%}\\approx\n7.3\\times10^{-26}$ near 200 Hz, and for Vela Jr. it is $h_0^{90 \\%}\\approx\n8.9\\times10^{-26}$ near 400 Hz. Translated into upper limits on the ellipticity\nand r-mode amplitude, our results probe physically interesting regions: for\nexample the ellipticity of Vela Jr. is constrained to be smaller than $10^{-7}$\nacross the frequency band, with a tighter constraint of less than\n$2\\times10^{-8}$ at the highest frequencies.",
      "generated_abstract": "2024, two Advanced LIGO detectors will start observing continuous\ngravitational waves from the core-collapse supernova event expected in the\nnear future in the Cassiopeia A region, and from a binary neutron star in the\nVela J1 system. In this paper, we present the results from a 2024-05-01\nEinstein@Home search for continuous gravitational waves from these two\nsources. We used the latest LIGO Hanford-Livingston data release O2 (2022-03-01)\nto construct a 5-year dataset with 122,000 gravitational wave triggers. This\ndataset includes 103,381 detections and 18,629 non-detections. We analyzed\nthese data with a Bayesian framework,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17105263157894737,
          "p": 0.38235294117647056,
          "f": 0.23636363209256203
        },
        "rouge-2": {
          "r": 0.03111111111111111,
          "p": 0.08139534883720931,
          "f": 0.04501607316921903
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.35294117647058826,
          "f": 0.21818181391074384
        }
      }
    },
    {
      "paper_id": "math.AP.math/FA/2503.08309v1",
      "true_abstract": "We investigate the asymptotic behavior as $\\varepsilon \\to 0$ of singularly\nperturbed phase transition models of order $n \\geq 2$, given by \\begin{align}\n  G_\\varepsilon^{\\lambda,n}[u] := \\int_I \\frac 1\\varepsilon W(u)\n-\\lambda\\varepsilon^{2n-3} (u^{(n-1)})^2 + \\varepsilon^{2n-1} (u^{(n)})^2 \\ dx,\n\\quad u \\in W^{n,2}(I), \\end{align}\n  where $\\lambda >0$ is fixed, $I \\subset \\mathbb{R}$ is an open bounded\ninterval, and $W \\in C^0(\\mathbb{R})$ is a suitable double-well potential. We\nfind that there exists a positive critical parameter depending on $W$ and $n$,\nsuch that the $\\Gamma$-limit of $G_\\varepsilon^{\\lambda,n}$ with respect to the\n$L^1$-topology is given by a sharp interface functional in the subcritical\nregime. The cornerstone for the corresponding compactness property is a novel\nnonlinear interpolation inequality involving higher-order derivatives, which is\nbased on Gagliardo-Nirenberg type inequalities.",
      "generated_abstract": "the convergence of solutions to the nonlinear phase transition\nmodel in higher dimensions, which arises in a variety of applications, including\nthermodynamics, physics, and biology. The model is characterized by an\ninteraction between a continuous and a discrete component, and is defined by a\ngeneralization of the phase transition models in the spirit of the\nrecently introduced concept of Gamma-convergence. We introduce the\nGamma-convergence of the phase transition model and show that the convergence\nholds under mild conditions on the interaction parameters. In addition, we\nprove that the convergence is Gamma-convergent to a phase transition model with\na discrete component and a constant interaction parameter. As a numerical\nexample, we show that the phase transition model arising in the study of\nbiological evolution is Gamma-convergent to a model with a continuous and a\ndiscrete component, and that the convergence",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19801980198019803,
          "p": 0.30303030303030304,
          "f": 0.23952095330345305
        },
        "rouge-2": {
          "r": 0.05042016806722689,
          "p": 0.05660377358490566,
          "f": 0.05333332835002516
        },
        "rouge-l": {
          "r": 0.16831683168316833,
          "p": 0.25757575757575757,
          "f": 0.20359280959087822
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.astro-ph/EP/2503.09137v1",
      "true_abstract": "Loeb & Cloete (2025) intriguingly suggest that the near-Earth object 2005\nVL$_1$ could be the lost Soviet probe Venera 2. Here I evaluate the\nplausibility of such a claim against the available data. I have re-determined\nthe orbit of 2005 VL$_1$ (including a non-gravitational acceleration component)\nusing the astrometric observations retrieved from the Minor Planet Center (MPC)\ndatabase. By propagating the orbit of 2005 VL$_1$ over the period of the Venera\n2 mission, I compare this object's distance from the Earth and from Venus at\nthe times of the probe's launch and flyby with Venus, respectively. My\nanalysis, which takes into account realistic uncertainties on both the orbit of\n2005 VL1 and the position of Venera 2, decisively rules out the proposed\nidentification. My approach relies entirely on open-source software and\npublicly available data, and could represent a viable method to assess similar\nclaims in the future.",
      "generated_abstract": "a 2 probe was launched on 12 July 1970 from the Baikonur Cosmodrome\n(Soviet Union) to explore Venus' surface and atmosphere, as well as to\nmeasure the planet's rotation period. The mission was designed to last for\nthree years, but it only lasted a few months. In August 1970, Venera 2\ndisintegrated in the Sun's atmosphere, but it may have survived until 1972.\nTheoretical calculations indicate that Venera 2 should have survived to 2015,\nwhich is the approximate date of its re-entry into the Earth's atmosphere.\nHowever, Venera 2 was never detected. We present here a detailed investigation\nof the orbit of Venera 2 during its initial three-year mission, and we\nconclude that it is highly",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1782178217821782,
          "p": 0.21428571428571427,
          "f": 0.1945945896368153
        },
        "rouge-2": {
          "r": 0.05185185185185185,
          "p": 0.06363636363636363,
          "f": 0.05714285219491922
        },
        "rouge-l": {
          "r": 0.1485148514851485,
          "p": 0.17857142857142858,
          "f": 0.16216215720438293
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/PM/2503.04662v1",
      "true_abstract": "We investigate portfolio optimization in financial markets from a trading and\nrisk management perspective. We term this task Risk-Aware Trading Portfolio\nOptimization (RATPO), formulate the corresponding optimization problem, and\npropose an efficient Risk-Aware Trading Swarm (RATS) algorithm to solve it. The\nkey elements of RATPO are a generic initial portfolio P, a specific set of\nUnique Eligible Instruments (UEIs), their combination into an Eligible\nOptimization Strategy (EOS), an objective function, and a set of constraints.\nRATS searches for an optimal EOS that, added to P, improves the objective\nfunction repecting the constraints.\n  RATS is a specialized Particle Swarm Optimization method that leverages the\nparameterization of P in terms of UEIs, enables parallel computation with a\nlarge number of particles, and is fully general with respect to specific\nchoices of the key elements, which can be customized to encode financial\nknowledge and needs of traders and risk managers.\n  We showcase two RATPO applications involving a real trading portfolio made of\nhundreds of different financial instruments, an objective function combining\nboth market risk (VaR) and profit&loss measures, constrains on market\nsensitivities and UEIs trading costs. In the case of small-sized EOS, RATS\nsuccessfully identifies the optimal solution and demonstrates robustness with\nrespect to hyper-parameters tuning. In the case of large-sized EOS, RATS\nmarkedly improves the portfolio objective value, optimizing risk and capital\ncharge while respecting risk limits and preserving expected profits.\n  Our work bridges the gap between the implementation of effective trading\nstrategies and compliance with stringent regulatory and economic capital\nrequirements, allowing a better alignment of business and risk management\nobjectives.",
      "generated_abstract": "r introduces a novel framework for risk-aware portfolio optimization\n(PO) that integrates the dynamic pricing of financial instruments with\nstrategic trading decisions. The proposed framework is based on a stochastic\nmodel that captures the interplay between volatility, market conditions, and\nmarket-specific risk factors. The model is calibrated to fit a large\ndataset of 28,000 trades across 1,000 stocks from the US, Europe, and Asia,\nincluding 1,100 daily trading decisions. The model is then employed to generate\na trading portfolio for each stock based on the optimal portfolio weights\nderived from the model. The results show that the trading portfolio outperforms\nboth benchmark and hedged strategies in terms of average daily return and\nstandardized return, and it exhibits a superior risk-return trade-off",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1625,
          "p": 0.3170731707317073,
          "f": 0.21487602857728308
        },
        "rouge-2": {
          "r": 0.028688524590163935,
          "p": 0.061946902654867256,
          "f": 0.03921568194775998
        },
        "rouge-l": {
          "r": 0.15625,
          "p": 0.3048780487804878,
          "f": 0.20661156576736572
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.02058v1",
      "true_abstract": "Ribonucleic acid (RNA) plays fundamental roles in biological systems, from\ncarrying genetic information to performing enzymatic function. Understanding\nand designing RNA can enable novel therapeutic application and biotechnological\ninnovation. To enhance RNA design, in this paper we introduce RiboGen, the\nfirst deep learning model to simultaneously generate RNA sequence and all-atom\n3D structure. RiboGen leverages the standard Flow Matching with Discrete Flow\nMatching in a multimodal data representation. RiboGen is based on Euclidean\nEquivariant neural networks for efficiently processing and learning\nthree-dimensional geometry. Our experiments show that RiboGen can efficiently\ngenerate chemically plausible and self-consistent RNA samples. Our results\nsuggest that co-generation of sequence and structure is a competitive approach\nfor modeling RNA.",
      "generated_abstract": "uce RiboGen, a novel method for the co-generation of RNA sequence\nand structure from a single flow. This method leverages the power of\nEquivariant MultiFlow (EMF) to overcome the challenges of sequence and structure\ngeneration in biological problems. In particular, we use EMF to generate the\nequivariant flow that maps the RNA sequence space to the RNA structure space,\nand then the equivariant flow is used to generate the RNA sequence and\nstructure from the flow. The equivariant flow can be generated by a single\nforward pass, which improves efficiency and reduces the need for pre-training.\nThe RNA sequence and structure are generated from the flow by an EMF flow. We\nevaluate RiboGen on 14 biological datasets, including the RNA-Seq and RNA-fold\ndatasets, and demonstrate its superior performance in terms of accuracy,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2823529411764706,
          "p": 0.3287671232876712,
          "f": 0.30379746338327196
        },
        "rouge-2": {
          "r": 0.06363636363636363,
          "p": 0.06306306306306306,
          "f": 0.06334841128969552
        },
        "rouge-l": {
          "r": 0.2823529411764706,
          "p": 0.3287671232876712,
          "f": 0.30379746338327196
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.03520v1",
      "true_abstract": "In our previous paper an effective algorithm for inverting polynomial\nautomorphisms was proposed. We extend its application to the case of formal\npower series over a field of arbitrary characteristic and illustrate the\nproposed approach with some examples.",
      "generated_abstract": "We consider the problem of inverting formal power series in a fixed\nformal power series ring $R$. We show that inverting such a formal power\nseries is equivalent to solving a system of $R$-linear equations, with a\nnon-trivial solution if and only if the formal power series is invertible in\n$R$. The solution of the system is a formal power series, which is not unique\nbecause the formal power series ring $R$ is not a field. We also show that the\nsolution is unique if and only if the formal power series is a unit in $R$.\nFinally, we show that inverting a formal power series can be reduced to solving\na system of $R$-linear equations for the coefficients of the formal power\nseries, and that the system is solved uniquely if and only if the formal power\nseries is a unit in $R$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.37142857142857144,
          "p": 0.26,
          "f": 0.30588234809688586
        },
        "rouge-2": {
          "r": 0.08108108108108109,
          "p": 0.03409090909090909,
          "f": 0.04799999583232036
        },
        "rouge-l": {
          "r": 0.2857142857142857,
          "p": 0.2,
          "f": 0.23529411280276827
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/PL/2503.03698v1",
      "true_abstract": "Programs written in unsafe languages such as C are prone to memory safety\nerrors, which can lead to program compromises and serious real-world security\nconsequences. Recently, Memory-Safe WebAssembly (MSWASM) is introduced as a\ngeneral-purpose intermediate bytecode with built-in memory safety semantics.\nPrograms written in C can be compiled into MSWASM to get complete memory safety\nprotection. In this paper, we present our extensions on MSWASM, which improve\nits semantics and practicality. First, we formalize MSWASM semantics in\nCoq/Iris, extending it with inter-module interaction, showing that MSWASM\nprovides fine-grained isolation guarantees analogous to WASM's coarse-grained\nisolation via linear memory. Second, we present Aegis, a system to adopt the\nmemory safety of MSWASM for C programs in an interoperable way. Aegis pipeline\ngenerates Checked C source code from MSWASM modules to enforce spatial memory\nsafety. Checked C is a recent binary-compatible extension of C which can\nprovide guaranteed spatial safety. Our design allows Aegis to protect C\nprograms that depend on legacy C libraries with no extra dependency and with\nlow overhead. Aegis pipeline incurs 67% runtime overhead and near-zero memory\noverhead on PolyBenchC programs compared to native.",
      "generated_abstract": "We present AEGIS, a formal, safe, and reliable execution engine for C\nprograms. AEGIS is based on the Microsoft's MSWASM and supports a variety of\nmemory-safe execution models. AEGIS provides a clean, concise, and\ntype-checked language with strong memory safety guarantees, enabling developers\nto write safer, more reliable code. AEGIS also offers a number of\nextension points, enabling developers to integrate advanced memory safety\nmechanisms and features into their applications. We present an experimental\nimplementation of AEGIS in C++, along with a comprehensive set of benchmarks\nthat demonstrate the performance improvements achieved by the engine. We also\npresent an in-depth evaluation of AEGIS on a real-world application,\ndemonstrating the robustness and performance of the engine.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19327731092436976,
          "p": 0.30666666666666664,
          "f": 0.23711339731905634
        },
        "rouge-2": {
          "r": 0.017241379310344827,
          "p": 0.028037383177570093,
          "f": 0.021352308451515083
        },
        "rouge-l": {
          "r": 0.15966386554621848,
          "p": 0.25333333333333335,
          "f": 0.1958762839169945
        }
      }
    },
    {
      "paper_id": "q-fin.GN.econ/EM/2501.01763v1",
      "true_abstract": "Following an analysis of existing AI-related exchange-traded funds (ETFs), we\nreveal the selection criteria for determining which stocks qualify as\nAI-related are often opaque and rely on vague phrases and subjective judgments.\nThis paper proposes a new, objective, data-driven approach using natural\nlanguage processing (NLP) techniques to classify AI stocks by analyzing annual\n10-K filings from 3,395 NASDAQ-listed firms between 2011 and 2023. This\nanalysis quantifies each company's engagement with AI through binary indicators\nand weighted AI scores based on the frequency and context of AI-related terms.\nUsing these metrics, we construct four AI stock indices-the Equally Weighted AI\nIndex (AII), the Size-Weighted AI Index (SAII), and two Time-Discounted AI\nIndices (TAII05 and TAII5X)-offering different perspectives on AI investment.\nWe validate our methodology through an event study on the launch of OpenAI's\nChatGPT, demonstrating that companies with higher AI engagement saw\nsignificantly greater positive abnormal returns, with analyses supporting the\npredictive power of our AI measures. Our indices perform on par with or surpass\n14 existing AI-themed ETFs and the Nasdaq Composite Index in risk-return\nprofiles, market responsiveness, and overall performance, achieving higher\naverage daily returns and risk-adjusted metrics without increased volatility.\nThese results suggest our NLP-based approach offers a reliable,\nmarket-responsive, and cost-effective alternative to existing AI-related ETF\nproducts. Our innovative methodology can also guide investors, asset managers,\nand policymakers in using corporate data to construct other thematic\nportfolios, contributing to a more transparent, data-driven, and competitive\napproach.",
      "generated_abstract": "asing influence of artificial intelligence (AI) in modern society\nhas prompted researchers to explore how firms engage with AI. While existing\nliterature has explored quantitative metrics for evaluating AI engagement,\ntraditional approaches typically rely on a limited number of metrics and\nlimit the ability to account for the nuanced nature of AI engagement. This\npaper introduces a novel methodology for quantifying AI engagement using\nobjective, data-driven metrics. Our methodology leverages 10-K filings from\nthe US Securities and Exchange Commission (SEC) to construct an AI Stock\nIndex (AISI), which measures a firm's engagement with AI. Using AISI as a\nbaseline, we then explore how firms' AI engagement evolves over time,\nhighlighting the impact of economic cycles and regulatory changes on AI\nengagement",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1896551724137931,
          "p": 0.3793103448275862,
          "f": 0.2528735587739464
        },
        "rouge-2": {
          "r": 0.0425531914893617,
          "p": 0.08695652173913043,
          "f": 0.057142852730612585
        },
        "rouge-l": {
          "r": 0.16091954022988506,
          "p": 0.3218390804597701,
          "f": 0.21455938252873574
        }
      }
    },
    {
      "paper_id": "physics.hist-ph.physics/hist-ph/2503.01617v1",
      "true_abstract": "Although several accounts of scientific understanding exist, the concept of\nunderstanding in relation to technology remains underexplored. This paper\naddresses this gap by proposing a philosophical account of technological\nunderstanding - the type of understanding that is required for and reflected by\nsuccessfully designing and using technological artefacts. We develop this\nnotion by building on the concept of scientific understanding. Drawing on\nparallels between science and technology, and specifically between scientific\ntheories and technological artefacts, we extend the idea of scientific\nunderstanding into the realm of technology. We argue that, just as scientific\nunderstanding involves the ability to explain a phenomenon using a theory,\ntechnological understanding involves the ability to use a technological\nartefact to realise a practical aim. Technological understanding can thus be\nconsidered a specific application of knowledge: it encompasses the cognitive\nskill of recognising how a practical aim can be achieved by using a\ntechnological artefact. In a context of design, this general notion of\ntechnological understanding is specified as the ability to design an artefact\nthat, by producing a phenomenon through its physical structure, achieves the\nintended aim. We illustrate our concept of technological understanding through\ntwo running examples: magnetic resonance imaging (MRI) and superconducting\nquantum computers. Our account highlights the epistemic dimension of engaging\nwith technology and, by allowing for context-dependent specifications, provides\nguidance for testing and improving technological understanding in specific\ncontexts.",
      "generated_abstract": "The design of artefacts is a crucial aspect of technological evolution,\nincluding the design of hardware and software. This paper explores the\nmechanisms through which technical innovations are implemented, and the\nskills involved in the design and use of technological artefacts. A key\nconcept is that of the cognitive system, which involves the integration of\ndifferent cognitive abilities to perform technical tasks. We show how a\ndesign-oriented view of technology allows for a systematic and coherent\nunderstanding of the cognitive skills involved in the design and use of\ntechnological artefacts. This approach provides a deeper understanding of how\ntechnological innovations are implemented and contributes to the design of more\neffective and efficient technological systems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19047619047619047,
          "p": 0.3870967741935484,
          "f": 0.255319144515618
        },
        "rouge-2": {
          "r": 0.035175879396984924,
          "p": 0.07368421052631578,
          "f": 0.047619043244713256
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.3387096774193548,
          "f": 0.2234042508985967
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.12026v1",
      "true_abstract": "In this paper, we consider the impact of the order flow auction (OFA) in the\ncontext of the proposer-builder separation (PBS) mechanism through a\ngame-theoretic perspective. The OFA is designed to improve user welfare by\nredistributing maximal extractable value (MEV) to the users, in which two\nauctions take place: the order flow auction and the block-building auction. We\nformulate the OFA as a multiplayer game, and focus our analyses on the case of\ntwo competing players (builders). We prove the existence and uniqueness of a\nNash equilibrium for the two-player game, and derive a closed-form solution by\nsolving a quartic equation. Our result shows that the builder with a\ncompetitive advantage pays a relatively lower cost, leading to centralization\nin the builder space. In contrast, the proposer's shares evolve as a martingale\nprocess, which implies decentralization in the proposer (or, validator) space.\nOur analyses rely on various tools from stochastic processes, convex\noptimization, and polynomial equations. We also conduct numerical studies to\ncorroborate our findings, and explore other features of the OFA under the PBS\nmechanism.",
      "generated_abstract": "on for the allocation of an ordered supply of goods is a fundamental\ntraditional mechanism for allocating resources. We study a novel auction\nframework for allocating an ordered supply of goods, where the auctioneer\nseparates the orders of the bidders into two parts. The first part is for the\nbuyers to propose the amount of each good they are willing to buy, while the\nsecond part is for the builders to build the goods as ordered. The auction\nmechanism is designed to ensure that the allocation is optimal and the buyer\nchooses the maximum amount of the goods they are willing to buy. We provide a\nstochastic analysis of the optimal bid and the optimal allocation, and\nestablish the conditions for the existence and uniqueness of the optimal\nauctioneer's strategy. Furthermore, we show that the optimal auctioneer's\nstrategy is a function of the optimal bid",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17094017094017094,
          "p": 0.2777777777777778,
          "f": 0.21164020692365848
        },
        "rouge-2": {
          "r": 0.06097560975609756,
          "p": 0.08695652173913043,
          "f": 0.07168458296784504
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.25,
          "f": 0.19047618575963732
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2503.06822v1",
      "true_abstract": "Clustering is a fundamental task in network analysis, essential for\nuncovering hidden structures within complex systems. Edge clustering, which\nfocuses on relationships between nodes rather than the nodes themselves, has\ngained increased attention in recent years. However, existing edge clustering\nalgorithms often overlook the significance of edge weights, which can represent\nthe strength or capacity of connections, and fail to account for noisy\nedges--connections that obscure the true structure of the network. To address\nthese challenges, the Weighted Edge Clustering Adjusting for Noise (WECAN)\nmodel is introduced. This novel algorithm integrates edge weights into the\nclustering process and includes a noise component that filters out spurious\nedges. WECAN offers a data-driven approach to distinguishing between meaningful\nand noisy edges, avoiding the arbitrary thresholding commonly used in network\nanalysis. Its effectiveness is demonstrated through simulation studies and\napplications to real-world datasets, showing significant improvements over\ntraditional clustering methods. Additionally, the R package ``WECAN'' has been\ndeveloped to facilitate its practical implementation.",
      "generated_abstract": "Edge clustering is a fundamental problem in network science, allowing us to\ndiscover hidden communities or patterns of network structure. Traditional\nmethods for edge clustering, such as spectral clustering and k-means clustering,\nare computationally expensive and often do not produce accurate results. In\nthis paper, we introduce a novel model-based method for edge clustering that\nuses a Gaussian process (GP) to model edge weights. Our method is flexible and\ncan handle any type of edge weighting function, allowing us to capture various\ntypes of network structure. We show that our method outperforms existing\nmethods, particularly in scenarios where the noise level is high. Additionally,\nwe provide theoretical guarantees for the accuracy of our method, showing that\nour method is consistent and has a high convergence rate.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2459016393442623,
          "p": 0.3409090909090909,
          "f": 0.2857142808453515
        },
        "rouge-2": {
          "r": 0.0379746835443038,
          "p": 0.05128205128205128,
          "f": 0.04363635874750469
        },
        "rouge-l": {
          "r": 0.1885245901639344,
          "p": 0.26136363636363635,
          "f": 0.21904761417868493
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.07978v4",
      "true_abstract": "This note introduces a doubly robust (DR) estimator for regression\ndiscontinuity (RD) designs. RD designs provide a quasi-experimental framework\nfor estimating treatment effects, where treatment assignment depends on whether\na running variable surpasses a predefined cutoff. A common approach in RD\nestimation is the use of nonparametric regression methods, such as local linear\nregression. However, the validity of these methods still relies on the\nconsistency of the nonparametric estimators. In this study, we propose the\nDR-RD estimator, which combines two distinct estimators for the conditional\nexpected outcomes. The primary advantage of the DR-RD estimator lies in its\nability to ensure the consistency of the treatment effect estimation as long as\nat least one of the two estimators is consistent. Consequently, our DR-RD\nestimator enhances robustness of treatment effect estimators in RD designs.",
      "generated_abstract": "In this note, we address a possible bias of the doubly robust estimator in\nregulation discontinuity designs. We show that the bias of the doubly robust\nestimator can be reduced to a bias of the standard estimator if we choose a\nspecial estimator for the residual in the regression discontinuity design. We\nalso discuss some implications of the bias reduction.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17647058823529413,
          "p": 0.40540540540540543,
          "f": 0.24590163511824786
        },
        "rouge-2": {
          "r": 0.049586776859504134,
          "p": 0.12,
          "f": 0.0701754344584661
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.40540540540540543,
          "f": 0.24590163511824786
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/CB/2501.08356v1",
      "true_abstract": "Recent high-throughput experiments unveil substantial electrophysiological\ndiversity among uncoupled healthy myocytes under identical conditions. To\nquantify inter-cell variability, the values of a subset of the parameters in a\nwell-regarded mathematical model of the action potential of rabbit ventricular\nmyocytes are estimated from fluorescence voltage measurements of a large number\nof cells. Statistical inference yields a population of nearly 1200\ncell-specific model variants that, on a population-level replicate\nexperimentally measured biomarker ranges and distributions, and in contrast to\nearlier studies, also match experimental biomarker values on a cell-by-cell\nbasis. This model population may be regarded as a random sample from the\nphenotype of healthy rabbit ventricular myocytes. Uni-variate and bi-variate\njoint marginal distributions of the estimated parameters are presented, and the\nparameter dependencies of several commonly utilised electrophysiological\nbiomarkers are revealed. Parameter values are weakly correlated, while summary\nmetrics such as the action potential duration are not strongly dependent on any\nsingle electrophysiological characteristic of the myocyte. Our results\ndemonstrate the feasibility of accurately and efficiently fitting entire action\npotential waveforms at scale.\n  Keywords: cellular excitability, rabbit ventricular myocytes, fluorescence\nvoltage measurements, action potential waveform, parameter estimation in\ndifferential equations, noisy time series",
      "generated_abstract": "ependent calcium (VdC) activation of cardiac myocytes is critical for\ncellular function and has been extensively studied in vitro. The development of\nin vitro models that faithfully replicate the behavior of live cells is a\npractical challenge. To address this, we developed a novel method for generating\na large population of cell-specific action potential (AP) models, including\nvoltage-clamp and voltage-gated calcium channel (VGCC) models, using the\nKalbfleisch-Duval-Nakamura (KDN) modeling framework. We used the KDN model\nto generate 4853 AP models that faithfully replicated voltage-clamp and\nVGCC-driven AP recordings from 132 rabbit ventricular myocytes, representing\nvarious myocardial cell types. To evaluate the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16535433070866143,
          "p": 0.26582278481012656,
          "f": 0.2038834904170988
        },
        "rouge-2": {
          "r": 0.028089887640449437,
          "p": 0.052083333333333336,
          "f": 0.03649634581277696
        },
        "rouge-l": {
          "r": 0.14960629921259844,
          "p": 0.24050632911392406,
          "f": 0.1844660146889435
        }
      }
    },
    {
      "paper_id": "nlin.CG.nlin/CG/2502.13735v1",
      "true_abstract": "Two kinetic exchange models are proposed to explore the dynamics of closed\neconomic markets characterized by random exchanges, saving propensities, and\ncollective transactions. Model I simulates a system where individual\ntransactions occur among agents with saving tendencies, along with collective\ntransactions between groups. Model II restricts individual transactions to\nagents within the same group, but allows for collective transactions between\ngroups. A three-step trading process--comprising intergroup transactions,\nintragroup redistribution, and individual exchanges--is developed to capture\nthe dual-layered market dynamics. The saving propensity is incorporated using\nthe Chakraborti-Chakrabarti model, applied to both individual and collective\ntransactions. Results reveal that collective transactions increase wealth\ninequality by concentrating wealth within groups, as indicated by higher Gini\ncoefficients and Kolkata indices. In contrast, individual transactions across\ngroups mitigate inequality through more uniform wealth redistribution. The\ninterplay between saving propensities and collective transactions governs\ndeviation degree and entropy, which display inverse trends. Higher saving\npropensities lead to deviations from the Boltzmann-Gibbs equilibrium, whereas\nspecific thresholds result in collective transaction dominance, producing\nnotable peaks or troughs in these metrics. These findings underscore the\ncritical influence of dual-layered market interactions on wealth distribution\nand economic dynamics.",
      "generated_abstract": "r develops a kinetic model to study the evolution of individual and\ncollective transactions in economic markets. The model features a set of\nindividual and collective transaction types, each associated with a specific\ntime-dependent probability distribution. The probability distributions are\nassumed to be proportional to the number of transactions. The model is\nparameterised by a set of parameters that characterise the transaction types,\nthe transaction costs and the transaction frequency. We show that the model\nadmits a unique steady-state solution and that the associated steady-state\ndistributions are characterised by the corresponding parameters. We also study\nthe qualitative behaviour of the model. In particular, we discuss the existence\nand uniqueness of the steady-state solution, as well as the long-time behaviour\nof the model. Finally, we show that the model admits a wide range of\ninteresting dynamics, ranging from quasi-steady-state solutions",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18604651162790697,
          "p": 0.3287671232876712,
          "f": 0.23762375776051375
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.04310344827586207,
          "f": 0.034364256373921626
        },
        "rouge-l": {
          "r": 0.13953488372093023,
          "p": 0.2465753424657534,
          "f": 0.17821781716645438
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.11604v1",
      "true_abstract": "In this work, we revisit the estimation of the model parameters of a Weibull\ndistribution based on iid observations, using the maximum likelihood estimation\n(MLE) method which does not yield closed expressions of the estimators. Among\nother results, it has been shown analytically that the MLEs obtained by solving\nthe highly non-linear equations do exist (i.e., finite), and are unique. We\nthen proceed to study the sampling distributions of the MLEs through both\ntheoretical as well as computational means. It has been shown that the sampling\ndistributions of the two model parameters' MLEs can be approximated fairly well\nby suitable Weibull distributions too. Results of our comprehensive simulation\nstudy corroborate some recent results on the first-order bias and first-order\nmean squared error (MSE) expressions of the MLEs.",
      "generated_abstract": "ll distribution is widely used for modeling survival data. We\ninvestigate the maximum likelihood estimator (MLE) of the parameters of the\nWeibull distribution. The MLE of the shape parameter is based on the\nlog-likelihood of the data, while the MLE of the scale parameter is based on\nthe log-likelihood of the data, with the log-likelihood of the data being\nderived from the log-combination of the log-Laplace distribution and the\nlog-Gamma distribution. The MLE of the shape parameter is based on the\nlog-likelihood of the data, while the MLE of the scale parameter is based on\nthe log-likelihood of the data, with the log-likelihood of the data being\nderived from the log-Laplace distribution and the log-Gamma distribution.\n  The asymptotic distribution of the MLE of the shape",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13186813186813187,
          "p": 0.3157894736842105,
          "f": 0.18604650747190685
        },
        "rouge-2": {
          "r": 0.06086956521739131,
          "p": 0.125,
          "f": 0.08187134062446588
        },
        "rouge-l": {
          "r": 0.0989010989010989,
          "p": 0.23684210526315788,
          "f": 0.1395348795649301
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.physics/space-ph/2503.05358v1",
      "true_abstract": "Comets are the most pristine planetesimals left from the formation of the\nSolar System. They carry unique information on the materials and the physical\nprocesses which led to the presence of planets and moons. Many important\nquestions about cometary physics, such as origin, constituents and mechanism of\ncometary activity, remain unanswered. The next perihelion of comet 1P/Halley,\nin 2061, is an excellent opportunity to revisit this object of outstanding\nscientific and cultural relevance. In 1986, during its latest approach to the\nSun, several flyby targeted Halley's comet to observe its nucleus and shed\nlight on its properties, origin, and evolution. However, due to its retrograde\norbit and high ecliptic inclination, the quality of data was limited by the\nlarge relative velocity and short time spent by the spacecraft inside the coma\nof the comet. A rendezvous mission like ESA/Rosetta would overcome such\nlimitations, but the trajectory design is extremely challenging due to the\nshortcomings of current propulsion technology. Given the considerable lead\ntimes of spacecraft development and the long duration of the interplanetary\ntransfer required to reach the comet, it is imperative to start mission\nplanning several decades in advance. This study presents a low-thrust\nrendezvous strategy to reach the comet before the phase of intense activity\nduring the close approach to the Sun. The trajectory design combines a\ngravity-assist maneuver with electric propulsion arcs to maximize scientific\npayload mass while constraining transfer duration. A propulsive plane change\nmaneuver would be prohibitive. To keep the propellant budget within reasonable\nlimits, most of the plane change maneuver is achieved via either a Jupiter or a\nSaturn flyby. The interplanetary low-thrust gravity-assisted trajectory design\nstrategy is described, followed by the presentation of multiple\nproof-of-concept solutions.",
      "generated_abstract": "t a novel approach to rendezvous and science operations with\nhazardous asteroids and comets. By employing a gravity-assist maneuver\nprovided by a small spacecraft, we achieve a low-thrust rendezvous trajectory\nthat circumvents the hazardous zone around a potentially hazardous asteroid\nor comet. We demonstrate that our approach reduces the time required to reach\na close encounter with an asteroid or comet by up to 200 years and to a\nscientific observation by up to 400 years. We find that a gravity-assist\nmaneuver can be used to reach a rendezvous orbit within the near-Earth\norbit, and to a near-Earth observation within the heliocentric orbit. We also\ndemonstrate that a gravity-assist maneuver can be used to reach a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12777777777777777,
          "p": 0.38333333333333336,
          "f": 0.19166666291666673
        },
        "rouge-2": {
          "r": 0.026515151515151516,
          "p": 0.07446808510638298,
          "f": 0.03910614137885872
        },
        "rouge-l": {
          "r": 0.11666666666666667,
          "p": 0.35,
          "f": 0.17499999625000007
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2412.15225v1",
      "true_abstract": "Biochemical reaction networks are typically modeled by $\\dfrac{dx}{dt}=N\\cdot\nK(x)=Y\\cdot I_a\\cdot K(x)$, with $x$ and $K(x)$ as the concentration and rate\nvectors, respectively, and $N$, $Y$, and $I_a$ as the stoichiometric,\nmolecularity, and incidence matrices, respectively. Steady states, which\ndescribe their long-term behaviors, are determined by solving $N\\cdot K(x)=0$,\nwhile complex balanced steady states are found by solving $I_a \\cdot K(x)=0$.\nTo investigate these complex networks, decomposition techniques are important,\nin particular, for computing steady states. Previously, we identified a\nwidespread property across many networks: the existence of independent and\nincidence-independent decompositions, characterized by the ability to directly\nsum the stoichiometric and incidence matrices of the subnetworks, respectively,\nto match those of the entire network. Here, we discover the ubiquitous property\nthat we call the Finest Decomposition Coarsening (FDC), where the finest\nindependent decomposition (FID) is a coarsening of the finest\nincidence-independent decomposition (FIID). To support the analysis of this\nproperty, we introduce a MATLAB package designed to compute both these\ndecompositions. We then characterize the FDC property and its relationship to\nstructural factors such as the invertibility of the molecularity matrix. We\nalso introduce and characterize the Finest Decompositions Equality (FDE)\nproperty, where FIID equals FID. Notably, we show that all deficiency zero\nnetworks exhibit the FDE property. Furthermore, we establish important\nrelationships of the FID and FIID with decomposition of the network into its\nconnected components. Our results highlight the prevalence of the coarsening\nproperty in reaction networks and deepens the understanding of the algebraic\nstructure and dynamics of biochemical networks.",
      "generated_abstract": "Finest decoherence coarsening of reaction networks of biochemical systems\nunderlies the coarse-grained description of complex biological systems.\nHowever, the identification of a minimal coarsening sequence for the network is\nnot straightforward due to the presence of multiple coarse-grained states,\nwhich leads to the coarsening of many reaction networks simultaneously. Here,\nwe propose a novel algorithm to identify the minimal coarsening sequence for\na reaction network of biochemical systems. We show that our approach provides\na more robust coarsening procedure for reaction networks than previously\nproposed methods. We apply our algorithm to a simple reaction network modeling\nthe synthesis of acetyl-CoA from acetyl-CoA and fumarate in a cyanobacterium.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14838709677419354,
          "p": 0.3709677419354839,
          "f": 0.2119815627386439
        },
        "rouge-2": {
          "r": 0.03404255319148936,
          "p": 0.08421052631578947,
          "f": 0.04848484438475701
        },
        "rouge-l": {
          "r": 0.13548387096774195,
          "p": 0.3387096774193548,
          "f": 0.19354838301514163
        }
      }
    },
    {
      "paper_id": "math.CA.math/CA/2503.04604v1",
      "true_abstract": "In this paper, we explore the relationship between the operators mapping\natoms to molecules in local Hardy spaces $h^p(\\mathbb{R}^n)$ and the size\nconditions of its kernel. In particular, we show that if the kernel a\nCalder\\'on--Zygmund-type operator satisfies an integral-type size condition and\na $T^*-$type cancellation, then the operator maps $h^p(\\mathbb{R}^n)$ atoms to\nmolecules. On the other hand, assuming that $T$ is an integral type operator\nbounded on $L^2(\\mathbb{R}^n)$ that maps atoms to molecules in\n$h^p(\\mathbb{R}^n)$, then the kernel of such operator satisfies the same\nintegral-type size conditions. We also provide the $L^1(\\mathbb{R}^n)$ to\n$L^{1,\\infty}(\\mathbb{R}^n)$ boundedness for such operators connecting our\nintegral-type size conditions on the kernel with others presented in the\nliterature.",
      "generated_abstract": "aper, we study the kernel conditions of the operators mapping\natoms to molecules in local Hardy spaces. Our main results are the following.\nFirst, we show that the Hardy-Littlewood maximal operator and the\nDirichlet-to-Neumann operator are both molecular operators. Second, we show that\nthe Hardy-Littlewood maximal operator is a molecular operator if and only if the\ndomain is a locally compact abelian group. Third, we show that the Hardy-Littlewood\nmaximal operator is a molecular operator if and only if the domain is a\nLebesgue space. Fourth, we show that the Hardy-Littlewood maximal operator is a\nmolecular operator if and only if the domain is a locally compact abelian group\nand the spectrum of the operator is discrete. Finally, we show that the\nDirichlet-to-Neumann operator is a mole",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3181818181818182,
          "p": 0.42857142857142855,
          "f": 0.3652173864136106
        },
        "rouge-2": {
          "r": 0.16161616161616163,
          "p": 0.2318840579710145,
          "f": 0.1904761856356294
        },
        "rouge-l": {
          "r": 0.30303030303030304,
          "p": 0.40816326530612246,
          "f": 0.3478260820657846
        }
      }
    },
    {
      "paper_id": "math.NT.cs/FL/2503.00959v2",
      "true_abstract": "The Riemann zeta function, and more generally the L-functions of Dirichlet\ncharacters, are among the central objects of study in number theory. We report\non a project to formalize the theory of these objects in Lean's \"Mathlib\"\nlibrary, including a proof of Dirichlet's theorem on primes in arithmetic\nprogressions and a formal statement of the Riemann hypothesis",
      "generated_abstract": "This paper introduces the \\texttt{zeta} package in Lean, an extension of the\nzeta function for Lean. The \\texttt{zeta} package allows the definition of\nzeta functions in Lean, using the \\texttt{zeta_real} function of\n\\texttt{Reason} to compute the real part of a function. The package also\nprovides a method for computing the imaginary part of a function, as well as a\nmethod for computing the derivative of a function. The package is based on the\npackage \\texttt{zeta\\_real} by \\texttt{Reason}. We discuss some of the key\nfeatures of the \\texttt{zeta} package, and provide some examples of its use.\nFinally, we discuss some of the future directions of the package.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2682926829268293,
          "p": 0.19642857142857142,
          "f": 0.2268041188309067
        },
        "rouge-2": {
          "r": 0.017857142857142856,
          "p": 0.011764705882352941,
          "f": 0.014184392374630659
        },
        "rouge-l": {
          "r": 0.21951219512195122,
          "p": 0.16071428571428573,
          "f": 0.18556700542884486
        }
      }
    },
    {
      "paper_id": "math.AG.math/AG/2503.09133v1",
      "true_abstract": "The usual approach to tropical geometry is via degeneration of amoebas of\nalgebraic subvarieties of an algebraic torus $(\\mathbb{C}^*)^n$. An amoeba is\nlogarithmic projection of the variety forgetting the angular part of\ncoordinates, called the phase. Similar degeneration can be performed without\nignoring the phase. The limit then is called phase tropical variety, and it is\na powerful tool in numerous areas. In the article is described a\nnon-commutative version of phase tropicalization in the simplest case of the\nmatrix group $PSL_2(\\mathbb{C})$, replacing here $(\\mathbb{C}^*)^n$ in the\nclassical approach.",
      "generated_abstract": "We present a gentle introduction to the theory of phase tropicalization\nfor the reader who is not already familiar with the subject. In particular, we\ngive a self-contained exposition of the proof of the existence of a finite set\nof singular points for any algebraic variety over a field of characteristic\n$0$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1694915254237288,
          "p": 0.2564102564102564,
          "f": 0.20408162786130787
        },
        "rouge-2": {
          "r": 0.03529411764705882,
          "p": 0.06,
          "f": 0.044444439780521754
        },
        "rouge-l": {
          "r": 0.15254237288135594,
          "p": 0.23076923076923078,
          "f": 0.1836734645960018
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/MF/2503.08833v1",
      "true_abstract": "We study optimal execution in markets with transient price impact in a\ncompetitive setting with $N$ traders. Motivated by prior negative results on\nthe existence of pure Nash equilibria, we consider randomized strategies for\nthe traders and whether allowing such strategies can restore the existence of\nequilibria. We show that given a randomized strategy, there is a non-randomized\nstrategy with strictly lower expected execution cost, and moreover this\nde-randomization can be achieved by a simple averaging procedure. As a\nconsequence, Nash equilibria cannot contain randomized strategies, and\nnon-existence of pure equilibria implies non-existence of randomized\nequilibria. Separately, we also establish uniqueness of equilibria. Both\nresults hold in a general transaction cost model given by a strictly positive\ndefinite impact decay kernel and a convex trading cost.",
      "generated_abstract": "the optimal execution game for the stochastic portfolio with\nrebalancing strategy where the trader has a randomization strategy. We show\nthat, if the trader has a non-zero probability of rebalancing, the optimal\nstrategy is unique. We also prove that, if the trader has a constant\nprobability of rebalancing, the optimal strategy is determined by the\nrebalancing strategy and the optimal strategy for the deterministic portfolio\nwith rebalancing strategy. We prove that, if the trader has a constant\nprobability of rebalancing, the optimal strategy is unique. We study the\nrebalancing strategy with a linear strategy. We prove that, if the trader has\na non-zero probability of rebalancing, the optimal strategy is unique. We\nstudy the rebalancing strategy with a linear strategy. We prove that, if the\ntrader has a constant probability of rebalancing",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18292682926829268,
          "p": 0.4411764705882353,
          "f": 0.25862068551129613
        },
        "rouge-2": {
          "r": 0.0423728813559322,
          "p": 0.09433962264150944,
          "f": 0.058479527886187516
        },
        "rouge-l": {
          "r": 0.18292682926829268,
          "p": 0.4411764705882353,
          "f": 0.25862068551129613
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.q-bio/PE/2503.07083v2",
      "true_abstract": "Motivated by the paradigm of a super-Maltusian population catastrophe, we\nstudy a simple stochastic population model which exhibits a finite-time blowup\nof the population size and is strongly affected by intrinsic noise. We focus on\nthe fluctuations of the blowup time $T$ in the asexual binary reproduction\nmodel $2A \\to 3A$, where two identical individuals give birth to a third one.\nWe determine exactly the average blowup time as well as the probability\ndistribution $\\mathcal{P}(T)$ of the blowup time and its moments. In\nparticular, we show that the long-time tail $\\mathcal{P}(T\\to \\infty)$ is\npurely exponential. The short-time tail $\\mathcal{P}(T\\to 0)$ exhibits an\nessential singularity at $T=0$, and it is dominated by a single (the most\nlikely) population trajectory which we determine analytically.",
      "generated_abstract": "the asymptotic behavior of a simple model of a super-Malthusian\ncatastrophe. We identify an interesting bifurcation in the blowup time as a\nfunction of the initial density. We show that the blowup time asymptotically\ndecreases exponentially with the initial density. Our results provide an\nanalytical description of the system's behavior in the whole parameter range\nwhere the system is stable and the blowup time is finite. We also show that the\nblowup time asymptotically approaches zero as the initial density approaches\nzero, which is a consequence of the fact that the system has a fixed point\nwith zero initial density. This fixed point is stable and does not allow for\nthe existence of a super-Malthusian catastrophe. We find that the asymptotic\nbehavior of the blowup time depends on the initial density. For small initial\ndensities, the blowup",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21686746987951808,
          "p": 0.28125,
          "f": 0.24489795426720357
        },
        "rouge-2": {
          "r": 0.09565217391304348,
          "p": 0.1111111111111111,
          "f": 0.10280373334570728
        },
        "rouge-l": {
          "r": 0.20481927710843373,
          "p": 0.265625,
          "f": 0.23129251209033283
        }
      }
    },
    {
      "paper_id": "physics.ed-ph.physics/ed-ph/2502.13774v1",
      "true_abstract": "In this work, we present a teaching strategy implemented in Introduction to\nPhysics, corresponding to the first year of the Physics Teacher Degree at the\nNational University of Rosario, whose main purpose is to provide students with\ntools to understand problem statements and exercises and to incorporate habits\nthat favor their resolution and communication. For this purpose, we implemented\nthe use of certain problemsolving algorithms, which we call\nHopscotch-Algorithms, appealing to the image of this popular game in which\nsteps can be skipped, rearranged or simultaneously executed. The aim is to\nstimulate a work method in a critical and problematized way, avoiding rigid or\ndogmatic applications of resolution steps. The testimonies collected indicate\nthat the strategy was positively valued by the students.",
      "generated_abstract": "cle proposes a framework for the study of algorithms as catalysts for\nprocesses of compensation and reconstruction of prior knowledge. The framework\nenables the identification of algorithms as the catalysts for the process of\ncompensation of prior knowledge. It also enables the identification of\nalgorithms as the catalysts for the process of reconstruction of prior\nknowledge. The framework allows the analysis of algorithms as catalysts for the\nprocesses of compensation and reconstruction of prior knowledge, and it also\nallows the analysis of algorithms as catalysts for the processes of\ncompensation of prior knowledge and the process of reconstruction of prior\nknowledge. The framework enables the identification of algorithms as the\ncatalysts for the process of compensation of prior knowledge and the process of\nreconstruction of prior knowledge. The framework also allows the analysis of\nalgorithms as catalysts for the processes of compensation of prior",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.058823529411764705,
          "p": 0.18518518518518517,
          "f": 0.08928571062659453
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.058823529411764705,
          "p": 0.18518518518518517,
          "f": 0.08928571062659453
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/RM/2410.11849v1",
      "true_abstract": "In this paper, we investigate a complex variation of the standard joint life\nannuity policy by introducing three distinct contingent benefits for the\nsurviving member(s) of a couple, along with a contingent benefit for their\nbeneficiaries if both members pass away. Our objective is to price this\ninnovative insurance policy and analyse its sensitivity to key model\nparameters, particularly those related to the joint mortality framework. We\nemploy the $QP$-rule (described in Section \\ref{secgenset}), which combines the\nreal-world probability measure $P$ for mortality risk with risk-neutral\nvaluation under $Q$ for financial market risks. The model enables explicit\npricing expressions, computed using efficient numerical methods. Our results\nhighlight the interdependent risks faced by couples, such as broken-heart\nsyndrome, providing valuable insights for insurers and policyholders regarding\nthe pricing influences of these factors.",
      "generated_abstract": "We study the value of a life insurance policy for dependent coupled lives,\nunder both the traditional and the modern approaches. The traditional approach\nconsiders the life of the youngest insured as the unit of valuation, while the\nmodern approach uses the value of the whole policy as the unit of valuation. We\nprove that, in general, the traditional approach underestimates the value of\nthe policy while the modern approach overestimates it. We show that the\nmodern approach is only accurate for the youngest insured. We also show that\nthe traditional approach is only accurate for the oldest insured. We provide a\nnumerical example to demonstrate that both approaches underestimate the value\nof the policy. Our findings highlight the importance of considering the\ninteractions between the insureds.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1941747572815534,
          "p": 0.3389830508474576,
          "f": 0.24691357561575986
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.053763440860215055,
          "f": 0.04484304446500083
        },
        "rouge-l": {
          "r": 0.1941747572815534,
          "p": 0.3389830508474576,
          "f": 0.24691357561575986
        }
      }
    },
    {
      "paper_id": "quant-ph.physics/atom-ph/2503.05664v1",
      "true_abstract": "We experimentally and theoretically study collective emission of a dense\natomic ensemble coupled to a whispering-gallery-mode (WGM) in a nanophotonic\nmicroring resonator. Due to many cold atoms localized in a small volume, these\ntrapped atoms collectively couple not only to the WGM and but also to the\nnon-guided modes in free space. Through tuning the atom-WGM coupling and by\nadjusting the number of trapped atoms, we demonstrate superradiant emission to\nthe WGM. For photon emission via the non-guided modes, our study reveals\nsignatures of subradiance and superradiance when the system is driven to the\nsteady-state states and the timed-Dicke states, respectively. Our experimental\nplatform thus presents the first atom-light interface with selective collective\nemission behavior into a guided mode and the environment, respectively. Our\nobservation and methodology could shed light on future explorations of\ncollective emission with densely packed quantum emitters coupled to\nnanophotonic light-matter interfaces.",
      "generated_abstract": "t a theory for collective emission from a dense atomic ensemble\ncoupled to a nanophotonic resonator. The coupling between the ensemble and\nresonator is mediated by the dipole moment of the ensemble, which we show can\nbe tuned to emit at specific frequencies. We show how the coupling between the\nensemble and resonator can be tuned by varying the resonator diameter. Our\ntheory predicts the emission of a broadband spectrum of radiation with\nsignificant overlap with the resonator's frequency. We also discuss how to\ncontrol the emission of specific frequencies by varying the resonator's\ndiameter. Our theory demonstrates that collective emission can be tuned to\nemit at specific frequencies. This tunability could be useful in applications\nsuch as optically controlled oscillators, sensors, and switches. Additionally,\nour theory provides a general method for tuning the frequency of emission for\na dense",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25252525252525254,
          "p": 0.33783783783783783,
          "f": 0.2890173361448763
        },
        "rouge-2": {
          "r": 0.08208955223880597,
          "p": 0.09565217391304348,
          "f": 0.08835340868373118
        },
        "rouge-l": {
          "r": 0.25252525252525254,
          "p": 0.33783783783783783,
          "f": 0.2890173361448763
        }
      }
    },
    {
      "paper_id": "math.QA.math/QA/2503.05960v2",
      "true_abstract": "A parametrized Yang-Baxter equation is a map from a group to a set of\nR-matrices, satisfying the Yang-Baxter commutation relation. For the six-vertex\nmodel, there are two main regimes of the Yang-Baxter equation: the\nfree-fermionic point, and everything else. For the free-fermionic point, there\nexists a parametrized Yang-Baxter equation with a large parameter group\nGL(2)xGL(1). For non-free-fermionic six-vertex matrices, there are also\nparametrized Yang-Baxter equations, but these do not account for all possible\ninteractions. Instead we will construct a groupoid parametrized Yang-Baxter\nequation that does reflect all possible Yang-Baxter equations in the six-vertex\nmodel.",
      "generated_abstract": "We consider the Yang-Baxter groupoid of a six-vertex Yang-Baxter algebra,\nwhich is an object in the category of algebras of six-vertex Yang-Baxter\nmatrices. We give a description of the six-vertex Yang-Baxter groupoid in terms\nof a six-vertex Yang-Baxter algebra. Moreover, we compute the Yang-Baxter\nmatrices of the six-vertex Yang-Baxter algebra, which are in one-to-one\ncorrespondence with the six-vertex Yang-Baxter groupoid.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18333333333333332,
          "p": 0.39285714285714285,
          "f": 0.24999999566115708
        },
        "rouge-2": {
          "r": 0.04938271604938271,
          "p": 0.09302325581395349,
          "f": 0.06451612450182133
        },
        "rouge-l": {
          "r": 0.15,
          "p": 0.32142857142857145,
          "f": 0.2045454502066117
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.cond-mat/soft/2503.09056v1",
      "true_abstract": "Long range order and symmetry in heterogeneous materials architected on\ncrystal lattices lead to elastic and inelastic anisotropies and thus limit\nmechanical functionalities in particular crystallographic directions. Here, we\npresent a facile approach for designing heterogeneous disordered materials that\nexhibit nearly isotropic mechanical resilience and energy dissipation\ncapabilities. We demonstrate, through experiments and numerical simulations on\n3D-printed prototypes, that near-complete isotropy can be attained in the\nproposed heterogeneous materials with a small, finite number of random spatial\npoints. We also show that adding connectivity between random subdomains leads\nto much enhanced elastic stiffness, plastic strength, energy dissipation, shape\nrecovery, structural stability and reusability in our new heterogeneous\nmaterials. Overall, our study opens avenues for the rational design of a new\nclass of heterogeneous materials with isotropic mechanical functionalities for\nwhich the engineered disorder throughout the subdomains plays a crucial role.",
      "generated_abstract": "pation of energy, and the resilience of a material against this\ndissipation, are two fundamental properties of physical systems. The latter\nquantity is a measure of how easily a material can be damaged and then\nrepaired. The dissipation of energy is particularly important for materials\nthat exhibit anomalous damping or dissipation: materials with such properties\nhave a low energy-to-dissipation ratio, which makes them more fragile and\ndifficult to repair. In this work, we investigate the dissipation and resilience\nof disordered materials in a wide range of dimensions and energies. We find\nthat disordered materials exhibit extreme dissipation and resilience, even at\nlow energies, which is a consequence of the extreme heterogeneity of their\ndynamical systems. We also find that the energy-to-dissipation ratio decreases\nas",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23469387755102042,
          "p": 0.2875,
          "f": 0.2584269613432648
        },
        "rouge-2": {
          "r": 0.05263157894736842,
          "p": 0.05982905982905983,
          "f": 0.05599999502048044
        },
        "rouge-l": {
          "r": 0.17346938775510204,
          "p": 0.2125,
          "f": 0.19101123100618622
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.15718v1",
      "true_abstract": "It is widely assumed that increases in economic productivity necessarily lead\nto economic growth. In this paper, it is shown that this is not always the\ncase. An idealized model of an economy is presented in which a new technology\nallows capital to be utilized autonomously without labor input. This is\nmotivated by the possibility that advances in artificial intelligence (AI) will\ngive rise to AI agents that act autonomously in the economy. The economic model\ninvolves a single profit-maximizing firm which is a monopolist in the product\nmarket and a monopsonist in the labor market. The new automation technology\ncauses the firm to replace labor with capital in such a way that its profit\nincreases while total production decreases. The model is not intended to\ncapture the structure of a real economy, but rather to illustrate how basic\neconomic mechanisms can give rise to counterintuitive and undesirable outcomes.",
      "generated_abstract": "AI-driven automation is expected to drive down labour productivity in the\nindustrialised world, yet this has not been explicitly studied in the\neconomic literature. We show that in a model economy with AI automation,\nincreasing productivity through an increase in the labour share of total\noutput can result in a decline in output. This result holds even if AI\nautomation leads to a decline in the labour share. We find that the economic\nimpact of an increase in the labour share of total output is stronger for\nindustries with a lower share of AI automation, suggesting that an increase in\nthe labour share can be a strategy to counteract the negative effects of AI\nautomation on output.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22826086956521738,
          "p": 0.35,
          "f": 0.2763157846952909
        },
        "rouge-2": {
          "r": 0.013986013986013986,
          "p": 0.022222222222222223,
          "f": 0.01716737723295826
        },
        "rouge-l": {
          "r": 0.20652173913043478,
          "p": 0.31666666666666665,
          "f": 0.2499999952216067
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/TO/2502.08062v1",
      "true_abstract": "We present a machine learning approach for predicting the organisation of\ncorneal, glial and fibroblast cells in 3D cultures used for tissue engineering.\nOur machine-learning-based method uses a powerful generative adversarial\nnetwork architecture called pix2pix, which we train using results from\nbiophysical contractile network dipole orientation (CONDOR) simulations. In the\nfollowing, we refer to the machine learning method as the RAPTOR (RApid\nPrediction of Tissue ORganisation) approach. A training data set containing a\nrange of CONDOR simulations is created, covering a range of underlying model\nparameters. Validation of the trained neural network is carried out by\ncomparing predictions with cultured glial, corneal, and fibroblast tissues,\nwith good agreements for both CONDOR and RAPTOR approaches. An approach is\ndeveloped to determine CONDOR model parameters for specific tissues using a fit\nto tissue properties. RAPTOR outputs a variety of tissue properties, including\ncell densities of cell alignments and tension. Since it is fast, it could be\nvaluable for the design of tethered moulds for tissue growth.",
      "generated_abstract": "gineering is an emerging field that aims to construct three-dimensional\nstructures in vivo by incorporating cells and biomaterials. The ability to\npredict the organisation of tissues is essential for predicting their function\nand tissue stability. This study presents a novel framework for rapid prediction\nof the organisation of engineered tissues using machine learning and\nbiophysical models. The framework is based on a deep learning approach that\ncombines a series of convolutional and pooling layers with a sequence\nmodeling approach to model the evolution of tissue organisation. A biophysical\nmodel is used to simulate the mechanical behaviour of tissue, which is used to\npredict the stress response of the tissue. The model captures the effects of\ntissue organisation on stress response and predicts the stress response of the\ntissue, which is then used to predict the organisation of the tissue. The\nframework is validated on",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22018348623853212,
          "p": 0.32432432432432434,
          "f": 0.2622950771501091
        },
        "rouge-2": {
          "r": 0.050955414012738856,
          "p": 0.06837606837606838,
          "f": 0.05839415569050071
        },
        "rouge-l": {
          "r": 0.2018348623853211,
          "p": 0.2972972972972973,
          "f": 0.24043715365284132
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/TO/2410.06002v1",
      "true_abstract": "The understanding of the mechanisms driving vascular development is still\nlimited. Techniques to generate vascular trees synthetically have been\ndeveloped to tackle this problem. However, most algorithms are limited to\nsingle trees inside convex perfusion volumes. We introduce a new framework for\ngenerating multiple trees inside general nonconvex perfusion volumes. Our\nframework combines topology optimization and global geometry optimization into\na single algorithmic approach. Our first contribution is defining a baseline\nproblem based on Murray's original formulation, which accommodates efficient\nsolution algorithms. The problem of finding the global minimum is cast into a\nnonlinear optimization problem (NLP) with merely super-linear solution effort.\nOur second contribution extends the NLP to constrain multiple vascular trees\ninside any nonconvex boundary while avoiding intersections. We test our\nframework against a benchmark of an anatomic region of brain tissue and a\nvasculature of the human liver. In all cases, the total tree energy is improved\nsignificantly compared to local approaches. By avoiding intersections globally,\nwe can reproduce key physiological features such as parallel running inflow\nvessels and tortuous vessels. The ability to generate non-intersecting vascular\ntrees inside nonconvex organs can improve the functional assessment of organs.",
      "generated_abstract": "This paper presents a novel approach for optimizing synthetic vascular trees\nin nonconvex organs. In contrast to existing methods, which require\ncomputationally expensive convex optimization algorithms, our approach uses\nonly nonconvex optimization, leading to an algorithm that is computationally\nefficient and scalable to large-scale data sets. Our approach leverages the\nconvex hull to represent the vasculature and the nonconvex optimization to\nminimize the length of the vasculature. The proposed method has been evaluated\non a synthetic data set and a real data set from a mouse brain. The\nsynthetic data set has 150,000 vasculature segments, while the real data set\nhas 122,000 vasculature segments. The proposed method significantly outperforms\nthe state-of-the-art methods in terms of accuracy, computational efficiency,\nand scalability.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21428571428571427,
          "p": 0.35526315789473684,
          "f": 0.2673267279796099
        },
        "rouge-2": {
          "r": 0.022222222222222223,
          "p": 0.03636363636363636,
          "f": 0.02758620218787238
        },
        "rouge-l": {
          "r": 0.20634920634920634,
          "p": 0.34210526315789475,
          "f": 0.25742573788059997
        }
      }
    },
    {
      "paper_id": "hep-th.hep-th/2503.10584v1",
      "true_abstract": "We investigate the shear viscosity and butterfly velocity of a magnetic\nfield-induced quantum phase transition in five dimensional\nEinstein-Maxwell-Chern-Simons theory, which is holographically dual to a class\nof strongly coupled quantum field theories with chiral anomalies. Our analysis\nreveals that the ratio of longitudinal shear viscosity to entropy density\n$\\eta_\\parallel/s$ exhibits a pronounced non-monotonic dependence on\ntemperature $T$ when the magnetic field $B$ is slightly below the critical\nvalue $B_c$ of the quantum phase transition. In particular, it can develop a\ndistinct minimum at an intermediate temperature. This contrasts sharply with\nthe monotonic temperature scaling observed at and above $B_c$, where\n$\\eta_\\parallel/s$ follows the scaling $T^{2/3}$ at $B=B_c$ and transitions to\n$T^2$ for $B>B_c$ as $T\\to0$. The non-vanishing of $\\eta_\\parallel/s$ for\n$B<B_c$ in the zero temperature limit suggests that it could serve as a good\norder parameter of the quantum phase transition. We also find that all\nbutterfly velocities change dramatically near the quantum phase transition, and\nthus their derivatives with respect to $B$ can be independently used to detect\nthe quantum critical point.",
      "generated_abstract": "the holographic properties of a magnetic field-driven quantum\ncritical point in the context of the holographic theory of superconductors.\nThe holographic model is constructed by embedding the $AdS_5 \\times S^5$\ngeometry into the $AdS_4 \\times S_3$ geometry, where the $S_3$ direction is\noriented along the magnetic field direction. The holographic equation of\nstate is determined by solving the dual field theory of the superconducting\nBethe ansatz state. The results show that the shear viscosity is suppressed\nby the magnetic field, while the butterfly velocity becomes independent of the\nmagnetic field for sufficiently weak fields. In addition, we obtain the\ncritical temperature and critical density for the magnetic field-driven\nquantum critical point. The critical temperature is determined by the\nholograph",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20535714285714285,
          "p": 0.3484848484848485,
          "f": 0.2584269616260573
        },
        "rouge-2": {
          "r": 0.07975460122699386,
          "p": 0.12871287128712872,
          "f": 0.09848484376061778
        },
        "rouge-l": {
          "r": 0.19642857142857142,
          "p": 0.3333333333333333,
          "f": 0.24719100656987755
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2503.06348v1",
      "true_abstract": "Real-time computer-based accompaniment for human musical performances entails\nthree critical tasks: identifying what the performer is playing, locating their\nposition within the score, and synchronously playing the accompanying parts.\nAmong these, the second task (score following) has been addressed through\nmethods such as dynamic programming on string sequences, Hidden Markov Models\n(HMMs), and Online Time Warping (OLTW). Yet, the remarkably successful\ntechniques of Deep Learning (DL) have not been directly applied to this\nproblem.\n  Therefore, we introduce HeurMiT, a novel DL-based score-following framework,\nutilizing a neural architecture designed to learn compressed latent\nrepresentations that enables precise performer tracking despite deviations from\nthe score. Parallelly, we implement a real-time MIDI data augmentation toolkit,\naimed at enhancing the robustness of these learned representations.\nAdditionally, we integrate the overall system with simple heuristic rules to\ncreate a comprehensive framework that can interface seamlessly with existing\ntranscription and accompaniment technologies.\n  However, thorough experimentation reveals that despite its impressive\ncomputational efficiency, HeurMiT's underlying limitations prevent it from\nbeing practical in real-world score following scenarios. Consequently, we\npresent our work as an introductory exploration into the world of DL-based\nscore followers, while highlighting some promising avenues to encourage future\nresearch towards robust, state-of-the-art neural score following systems.",
      "generated_abstract": "aper, we introduce a neural score follower for computer accompaniment\nof polyphonic musical instruments. Our approach uses a neural score follower\n(NSF) to estimate the position of the instrument. The NSF is trained to\npredict the position of the instrument based on the input sound. This position\nis then used to generate a score for the instrument. In addition, we propose a\nmethod for determining the position of the instrument from the sound. The\nproposed method consists of two steps: (1) detecting the start and end of the\ninstrument by segmenting the sound, and (2) estimating the position of the\ninstrument using the detected segment boundaries. The proposed method is\ncombined with the NSF to generate a score for the instrument. The effectiveness\nof the proposed method is evaluated using a dataset of music from the\nInternational Music Score Library Project (IMSLP). The results show that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10975609756097561,
          "p": 0.24,
          "f": 0.15062761075611433
        },
        "rouge-2": {
          "r": 0.020100502512562814,
          "p": 0.034782608695652174,
          "f": 0.02547770236419412
        },
        "rouge-l": {
          "r": 0.10975609756097561,
          "p": 0.24,
          "f": 0.15062761075611433
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.08587v1",
      "true_abstract": "The increasing use of children's automatic speech recognition (ASR) systems\nhas spurred research efforts to improve the accuracy of models designed for\nchildren's speech in recent years. The current approach utilizes either\nopen-source speech foundation models (SFMs) directly or fine-tuning them with\nchildren's speech data. These SFMs, whether open-source or fine-tuned for\nchildren, often exhibit higher word error rates (WERs) compared to adult\nspeech. However, there is a lack of systemic analysis of the cause of this\ndegraded performance of SFMs. Understanding and addressing the reasons behind\nthis performance disparity is crucial for improving the accuracy of SFMs for\nchildren's speech. Our study addresses this gap by investigating the causes of\naccuracy degradation and the primary contributors to WER in children's speech.\nIn the first part of the study, we conduct a comprehensive benchmarking study\non two self-supervised SFMs (Wav2Vec2.0 and Hubert) and two weakly supervised\nSFMs (Whisper and MMS) across various age groups on two children speech\ncorpora, establishing the raw data for the causal inference analysis in the\nsecond part. In the second part of the study, we analyze the impact of\nphysiological factors (age, gender), cognitive factors (pronunciation ability),\nand external factors (vocabulary difficulty, background noise, and word count)\non SFM accuracy in children's speech using causal inference. The results\nindicate that physiology (age) and particular external factor (number of words\nin audio) have the highest impact on accuracy, followed by background noise and\npronunciation ability. Fine-tuning SFMs on children's speech reduces\nsensitivity to physiological and cognitive factors, while sensitivity to the\nnumber of words in audio persists.\n  Keywords: Children's ASR, Speech Foundational Models, Causal Inference,\nPhysiology, Cognition, Pronunciation",
      "generated_abstract": "r examines the relationship between children's speech errors and\nphysiological, cognitive, and extrinsic factors in a causal framework.\nSpecifically, we investigate whether and how these factors influence\nchildren's speech errors. We employ a mixed-effects model to estimate\ninteraction effects, which are used to predict children's errors. Our results\nshow that children's errors are significantly influenced by extrinsic factors,\nsuch as age and gender, as well as by physiological factors, such as heart\nrate and blood pressure. In contrast, cognitive factors, such as word frequency\nand fluency, have a less pronounced effect. Additionally, we find that\nchildren's errors are more likely to occur during the first 10 minutes of\nspeaking, after which their errors tend to plateau. Our findings highlight the\nimportance of considering extrinsic and cognitive factors in ASR error\nanalysis and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15853658536585366,
          "p": 0.3023255813953488,
          "f": 0.20799999548672007
        },
        "rouge-2": {
          "r": 0.020080321285140562,
          "p": 0.041666666666666664,
          "f": 0.0271002666137888
        },
        "rouge-l": {
          "r": 0.1402439024390244,
          "p": 0.26744186046511625,
          "f": 0.18399999548672014
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.09775v1",
      "true_abstract": "This paper introduces a data-driven graphical framework for the real-time\nsearch of risky cascading fault chains (FCs) in power-grids, crucial for\nenhancing grid resiliency in the face of climate change. As extreme weather\nevents driven by climate change increase, identifying risky FCs becomes crucial\nfor mitigating cascading failures and ensuring grid stability. However, the\ncomplexity of the spatio-temporal dependencies among grid components and the\nexponential growth of the search space with system size pose significant\nchallenges to modeling and risky FC search. To tackle this, we model the search\nprocess as a partially observable Markov decision process (POMDP), which is\nsubsequently solved via a time-varying graph recurrent neural network (GRNN).\nThis approach captures the spatial and temporal structure induced by the\nsystem's topology and dynamics, while efficiently summarizing the system's\nhistory in the GRNN's latent space, enabling scalable and effective\nidentification of risky FCs.",
      "generated_abstract": "fault-chain search is crucial for managing power system\ndynamics and preventing severe accidents. However, traditional methods that\nonly consider single-step fault propagation fail to capture the dynamic\ninterdependencies among faults. To address this issue, this paper proposes a\ntime-varying graph recurrent neural network (TG-RNN) for real-time fault-chain\nsearch. The TG-RNN is a time-varying graph RNN, which integrates the temporal\ncomponent of graph structures and the time component of graph signals. It\nenables the TG-RNN to capture the complex temporal and spatial dependencies\namong faults and to model the dynamic changes of fault propagation. The\nTG-RNN is trained by integrating the traditional graph signal processing\nframework with the generative adversarial network. The proposed TG-RNN is\nvalidated on synthetic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.28431372549019607,
          "p": 0.38666666666666666,
          "f": 0.327683610935555
        },
        "rouge-2": {
          "r": 0.06666666666666667,
          "p": 0.08737864077669903,
          "f": 0.07563024719122975
        },
        "rouge-l": {
          "r": 0.24509803921568626,
          "p": 0.3333333333333333,
          "f": 0.2824858708225606
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2503.07798v1",
      "true_abstract": "Understanding the risk and protective factors associated with Parkinsons\ndisease (PD) is crucial for improving outcomes for patients, individuals at\nrisk, healthcare providers, and healthcare systems. Studying these factors not\nonly enhances our knowledge of the disease but also aids in developing\neffective prevention, management, and treatment strategies. This paper reviews\nthe key risk and protective factors associated with PD, with a particular focus\non the biological mechanisms underlying these factors. Risk factors include\ngenetic mutations, racial predispositions, and environmental exposures, all of\nwhich contribute to an increased likelihood of developing PD or accelerating\ndisease progression. Conversely, protective factors such as regular physical\nexercise, adherence to a Mediterranean diet, and higher urate levels have\ndemonstrated potential to reduce inflammation and support mitochondrial\nfunction, thereby mitigating disease risk. However, identifying and validating\nthese factors presents significant challenges. To overcome challenges, we\npropose several solutions and recommendations. Future research should\nprioritize the development of standardized biomarkers for early diagnosis,\ninvestigate gene-environment interactions in greater depth, and refine animal\nmodels to better mimic human PD pathology. Additionally, we offer actionable\nrecommendations for PD prevention and management, tailored to healthy\nindividuals, patients diagnosed with PD, and healthcare systems. These\nstrategies aim to improve clinical outcomes, enhance quality of life, and\noptimize healthcare delivery for PD.",
      "generated_abstract": "s disease (PD) is the second most common neurodegenerative disorder\nclaiming more than 10 million patients worldwide. It is characterized by\nabnormal nigral dopamine (DA) neuron death and loss of striatal dopamine\nsynapses. The disease has a strong genetic component, but environmental\nfactors are also associated with the disease. In this study, we aimed to\ninvestigate risk factors related to PD using data from the Parkinson Study\n(Parkinson Study 2). We found that risk factors were associated with age,\nsex, race, ethnicity, and education. The risk factors were also related to\nlifestyle factors, such as smoking, drinking, and physical activity. Age and\nsex were the most important risk factors for PD, followed by other lifestyle\nfactors and then environmental factors. We identified 2",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16891891891891891,
          "p": 0.2840909090909091,
          "f": 0.21186440210284413
        },
        "rouge-2": {
          "r": 0.025252525252525252,
          "p": 0.043478260869565216,
          "f": 0.031948877140728874
        },
        "rouge-l": {
          "r": 0.16216216216216217,
          "p": 0.2727272727272727,
          "f": 0.20338982583165768
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2503.01761v1",
      "true_abstract": "The claustrum is a thin gray matter structure in each brain hemisphere,\ncharacterized by exceptionally high connectivity with nearly all brain regions.\nDespite extensive animal studies on its anatomy and function and growing\nevidence of claustral deficits in neuropsychiatric disorders, its specific\nroles in normal and abnormal human brain function remain largely unknown. This\nis primarily due to its thin and complex morphology, which limits accurate\nanatomical delineation and neural activity isolation in conventional in vivo\nneuroimaging. To facilitate future neuroimaging studies, we developed a\ncomprehensive and reliable manual segmentation protocol based on a\ncellular-resolution brain atlas and high-resolution (0.7^3 mm) MRI data. The\nprotocols involve detailed guidelines to delineate the entire claustrum,\nincluding the inferior parts that have not been clearly described in earlier\nMRI studies. Additionally, we propose a geometric method to parcellate the\nclaustrum into three subregions (the dorsal, ventral, and temporal claustrum)\nalong the superior-to-inferior axis. The mean bilateral claustrum volume in 10\nyoung adults was 3307.5 mm^3, approximately 0.21% of total intracranial volume.\nOur segmentation protocol demonstrated high inter- and intra-rater reliability\n(ICC > 0.89, DSC > 0.85), confirming its replicability. This comprehensive and\nreliable claustrum segmentation protocols will provide a cornerstone for future\nneuroimaging studies of systematic, large-scale investigations of the anatomy\nand the functions of the human claustrum in normal and pathological\npopulations.",
      "generated_abstract": "trum is a large region of the human brain that plays a critical\nrole in cognitive processing. However, manual segmentation of the claustrum is\nchallenging due to its complex architecture and extensive overlap with other\nstructures. We present a method for automated segmentation of the claustrum\nbased on deep learning. Our approach combines deep learning with a\nsemi-automated segmentation protocol to produce accurate and reliable\nsegmentations of the claustrum. Our approach outperforms existing manual and\nautomated segmentation methods and offers a robust and reliable framework for\ncomprehensive and accurate analysis of the claustrum. The method was\nvalidated on a large dataset of human MRI scans and demonstrated high\nperformance in terms of accuracy, efficiency, and reproducibility. Our\nresults highlight the potential of deep learning for automating the\nsegmentation of the human brain and provide a foundation for future",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.4358974358974359,
          "f": 0.29437228989936476
        },
        "rouge-2": {
          "r": 0.07906976744186046,
          "p": 0.14285714285714285,
          "f": 0.10179640259869503
        },
        "rouge-l": {
          "r": 0.1895424836601307,
          "p": 0.3717948717948718,
          "f": 0.2510822466093215
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.17059v1",
      "true_abstract": "This study examines the determinants of the spousal age gap (SAG) in India,\nutilizing data from the 61st and 68th rounds of the National Sample Survey\n(NSSO). We employ regression analysis, including instrumental variables, to\naddress selection bias and account for unobservable factors. We hypothesize an\ninverted U-shaped relationship between educational assortative mating and SAG,\nwhere, keeping the husband's education constant at the graduation level, the\nSAG first widens and then narrows as the wife's education level increases from\nprimary to postgraduate. This pattern is shaped by distinct socio-economic\nfactors across rural and urban settings. In rural India, increasing prosperity,\nchanges in family structure, and educational hypergamy contribute to a wider\nage gap, with the influence of bride squeeze further exacerbating this\ndisparity. Conversely, in urban areas, while the growth of white-collar jobs\ninitially contributed to a narrowing of the SAG in 2004-05, this trend did not\npersist by 2011-12. Specifically, the influence of income on SAG becomes\nnonlinear, showing declining trends beyond the 7th income quantile, reflecting\nlimited marriage mobility opportunities for females and hinting at a possible\nthreat to the institution of marriage among the urban upper class. To the best\nof our knowledge, this is the first study to provide empirical evidence on how\nspecific social, economic, and cultural dynamics influence the spousal age gap\nin Indian society. This increasing and persistent spousal age gap has\nsignificant implications for the treatment of women, power dynamics, and\nviolence within marriage.",
      "generated_abstract": "This paper studies the determinants of the spousal age gap in India using\ndata from the 2011-12 National Sample Survey (NSS) and a unique microdata\ncollection. We employ a series of regression analyses to investigate the\ninfluence of socio-demographic characteristics and household characteristics on\nthe spousal age gap, including marital status, education level, literacy rate,\nhousing cost, and household income. We find that the spousal age gap is\nsignificantly associated with the education level of the husband, the\nhousing cost of the house, and the household income of the husband. The\nsignificance of these variables in explaining the spousal age gap is\nstronger for the married and higher-educated population.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19375,
          "p": 0.45588235294117646,
          "f": 0.27192982037550023
        },
        "rouge-2": {
          "r": 0.0782608695652174,
          "p": 0.19148936170212766,
          "f": 0.11111110699207455
        },
        "rouge-l": {
          "r": 0.16875,
          "p": 0.39705882352941174,
          "f": 0.2368421010772546
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.06379v1",
      "true_abstract": "Let $G$ be a finite group and $p$ be a prime. We denote by $C_p(G)$ the poset\nof all cosets of $p$-subgroups of $G$. We characterize the homotopy type of the\ngeometric realization $|\\Delta C_p(G)|$ for $p$-closed groups $G$, which is\nmotivated by K.S.Brown's Question. We will further demonstrate that\n$\\chi(C_{p}(G)) \\equiv |G|_{p'} (\\text{mod} p)$ for any finite group $G$ and\nany prime $p$.",
      "generated_abstract": "aper, we give a characterization of coset complexes of $p$-subgroups\nin finite groups. We first prove that any coset complex of $p$-subgroups in a\nfinite group is isomorphic to a coset complex of a group generated by the\n$p$-subgroups. This result generalizes the result by G. W. Chang. We then show\nthat any coset complex of $p$-subgroups in a group is isomorphic to a coset\ncomplex of a group generated by the coset complex of $p$-subgroups. This result\ngeneralizes the result by A. M. Raghunathan. We prove that the class of\ncoset complexes of $p$-subgroups in a group is closed under the following\noperations: the coset complex of $p$-subgroups, the coset complex of $p$-subgroups\ngenerated by a coset complex of $p$-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2553191489361702,
          "p": 0.27906976744186046,
          "f": 0.26666666167654324
        },
        "rouge-2": {
          "r": 0.047619047619047616,
          "p": 0.04477611940298507,
          "f": 0.04615384115858043
        },
        "rouge-l": {
          "r": 0.2553191489361702,
          "p": 0.27906976744186046,
          "f": 0.26666666167654324
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/CO/2502.03439v1",
      "true_abstract": "The pyLOT library offers a Python implementation of linearized optimal\ntransport (LOT) techniques and methods to use in downstream tasks. The pipeline\nembeds probability distributions into a Hilbert space via the Optimal Transport\nmaps from a fixed reference distribution, and this linearization allows\ndownstream tasks to be completed using off the shelf (linear) machine learning\nalgorithms. We provide a case study of performing ML on 3D scans of lemur\nteeth, where the original questions of classification, clustering, dimension\nreduction, and data generation reduce to simple linear operations performed on\nthe LOT embedded representations.",
      "generated_abstract": "Linearized optimal transport (LOPT) is a powerful tool for unsupervised\nlearning and generative modeling in high-dimensional spaces. This paper\nintroduces a Python library, pyLOT, that provides a unified interface for\nnumerical computation and optimization of LOPT. The library is built on the\nEigenpy framework and is designed to support both point-cloud and high-dimensional\ndata. It implements a novel algorithm for efficient LOPT optimization,\nenabling efficient and accurate computation of optimal transport maps,\nconformal predictions, and optimal transport risk estimates. Additionally,\npyLOT includes a suite of tools for training and evaluating generative\nmodels, including latent space optimization and generative adversarial\nnetworks. The library is available at https://github.com/alan-turing-institute/pyLOT.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21052631578947367,
          "p": 0.21333333333333335,
          "f": 0.2119205248015439
        },
        "rouge-2": {
          "r": 0.03296703296703297,
          "p": 0.0297029702970297,
          "f": 0.031249995013564166
        },
        "rouge-l": {
          "r": 0.19736842105263158,
          "p": 0.2,
          "f": 0.19867549168896115
        }
      }
    },
    {
      "paper_id": "hep-th.math-ph/2503.09997v1",
      "true_abstract": "This review explores recent advances in the theory of $T\\bar{T}$ deformation,\nan irrelevant yet solvable deformation of quantum field theories defined via\nthe quadratic form of the energy-momentum tensor. It addresses classical and\nquantum aspects, highlighting significant developments across various fields,\nincluding field theory, holography, and string theory. Classically, $T\\bar{T}$\ndeformation manifests through multiple geometric interpretations, notably\nrandom geometry, Jackiw-Teitelboim-like gravity, and uniform light-cone gauge\nframeworks. For quantum aspects, the deformation introduces notable features\nsuch as non-locality, UV-IR mixing, solvable renormalization structures, and\nintriguing modifications to correlation functions and entanglement properties.\nFurthermore, the paper examines the profound relationship between $T\\bar{T}$\ndeformation and holography, particularly within the mixed boundary\nconditions/cutoff AdS holography proposal and holographic entanglement entropy.\nConnections to string theory through single-trace deformations and their\nholographic duals further reveal the deformed structure of the worldsheet. This\nreview synthesizes recent developments and outlines potential directions for\nfuture research in the study of $T\\bar{T}$-like deformation.",
      "generated_abstract": "In the $T\\overline{T}$ deformation, the effective action of an open\nconformal field theory (CFT) is expressed as a sum of two terms: a topological\nterm, arising from the $T$ and $\\overline{T}$ operators, and a second term,\narising from the $R$ operator. The $T\\overline{T}$ deformation has been\nintroduced as a way to study the renormalization group (RG) flow of the\ntopological term, and to investigate its relevance for the UV behavior of the\nCFT. In this review, we will provide a brief introduction to the $T\\overline{T}$\ndeformation and its applications, and then we will focus on recent developments\nin this context.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.24615384615384617,
          "f": 0.1849710935747938
        },
        "rouge-2": {
          "r": 0.02054794520547945,
          "p": 0.03333333333333333,
          "f": 0.025423724095088494
        },
        "rouge-l": {
          "r": 0.12962962962962962,
          "p": 0.2153846153846154,
          "f": 0.16184970629155682
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.04325v2",
      "true_abstract": "Gliomas are brain tumours that stand out for their highly lethal and\naggressive nature, which demands a precise approach in their diagnosis. Medical\nimage segmentation plays a crucial role in the evaluation and follow-up of\nthese tumours, allowing specialists to analyse their morphology. However,\nexisting methods for automatic glioma segmentation often lack generalization\ncapability across other brain tumour domains, require extensive computational\nresources, or fail to fully utilize the multi-parametric MRI (mp-MRI) data used\nto delineate them. In this work, we introduce GBT-SAM, a novel Generalizable\nBrain Tumour (GBT) framework that extends the Segment Anything Model (SAM) to\nbrain tumour segmentation tasks. Our method employs a two-step training\nprotocol: first, fine-tuning the patch embedding layer to process the entire\nmp-MRI modalities, and second, incorporating parameter-efficient LoRA blocks\nand a Depth-Condition block into the Vision Transformer (ViT) to capture\ninter-slice correlations. GBT-SAM achieves state-of-the-art performance on the\nAdult Glioma dataset (Dice Score of $93.54$) while demonstrating robust\ngeneralization across Meningioma, Pediatric Glioma, and Sub-Saharan Glioma\ndatasets. Furthermore, GBT-SAM uses less than 6.5M trainable parameters, thus\noffering an efficient solution for brain tumour segmentation. \\\\ Our code and\nmodels are available at https://github.com/vpulab/med-sam-brain .",
      "generated_abstract": "ours are challenging to segment because of the complexity of their\nstructures, their anatomical variability, and the lack of standardized\nground-truth datasets. Traditional methods for brain tumour segmentation often\nrely on multiple-view and multi-segmentation approaches, which are computationally\nintensive and prone to error. To address these limitations, we introduce GBT-SAM,\na novel method for segmenting brain tumours in multi-plane magnetic resonance\nimaging (mp-MRI) images. GBT-SAM utilizes a depth-aware model, with its\nparameters tailored to the depth of the brain tumour, to capture the\nneighborhood structure of the tumour and enhance segmentation accuracy. To\naccelerate training, we propose a novel approach that integrates a\nhigh-frequency prior for the tumour volume and a global context model for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19607843137254902,
          "p": 0.37037037037037035,
          "f": 0.25641025188362926
        },
        "rouge-2": {
          "r": 0.06315789473684211,
          "p": 0.10909090909090909,
          "f": 0.07999999535555583
        },
        "rouge-l": {
          "r": 0.16339869281045752,
          "p": 0.30864197530864196,
          "f": 0.21367520914858656
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/AP/2502.17371v2",
      "true_abstract": "The integration of photovoltaic (PV) systems into greenhouses not only\noptimizes land use but also enhances sustainable agricultural practices by\nenabling dual benefits of food production and renewable energy generation.\nHowever, accurate prediction of internal environmental conditions is crucial to\nensure optimal crop growth while maximizing energy production. This study\nintroduces a novel application of Spatio-Temporal Graph Neural Networks\n(STGNNs) to greenhouse microclimate modeling, comparing their performance with\ntraditional Recurrent Neural Networks (RNNs). While RNNs excel at temporal\npattern recognition, they cannot explicitly model the directional relationships\nbetween environmental variables. Our STGNN approach addresses this limitation\nby representing these relationships as directed graphs, enabling the model to\ncapture both spatial dependencies and their directionality. Using\nhigh-frequency data collected at 15-minute intervals from a greenhouse in\nVolos, Greece, we demonstrate that RNNs achieve exceptional accuracy in winter\nconditions (R^2 = 0.985) but show limitations during summer cooling system\noperation. Though STGNNs currently show lower performance (winter R^2 = 0.947),\ntheir architecture offers greater potential for integrating additional\nvariables such as PV generation and crop growth indicators.",
      "generated_abstract": "y presents a comparative analysis of recurrent and graph neural\nnetworks (RNNs and GNNs) for greenhouse management. The proposed approach\nenables the model to learn the environmental characteristics and environmental\nfactors that affect the plant growth and productivity. The study employs three\ngreenhouses for data collection, with two greenhouses being in the temperate\nclimate and one in the tropical climate. The greenhouses were equipped with\nhydroponic systems, and the experimental protocol was based on the greenhouse\nwatering schedule. A total of 28 plant species were used in the study, and\ndata were collected over a period of 6 months. The results show that RNNs\noutperform GNNs in terms of performance, with the RNN achieving higher\nperformance in all metrics compared to the GNN. The results also reveal that\nthe RNN has the potential to improve",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16901408450704225,
          "p": 0.2727272727272727,
          "f": 0.2086956474495275
        },
        "rouge-2": {
          "r": 0.017341040462427744,
          "p": 0.023076923076923078,
          "f": 0.0198019752987192
        },
        "rouge-l": {
          "r": 0.16901408450704225,
          "p": 0.2727272727272727,
          "f": 0.2086956474495275
        }
      }
    },
    {
      "paper_id": "physics.class-ph.physics/class-ph/2503.00432v1",
      "true_abstract": "A specialized high-precision numerical search for equal-mass collisionless\nthree-body periodic free-fall orbits with central symmetry is conducted. The\nsearch is based on Newton's method with initial approximations obtained by the\ngrid-search method. Instead of solving the standard periodicity equation on the\nentire period a quarter-period equation that also characterizes the periodic\norbits is solved. The number of the known orbits from the class is\nsignificantly enlarged. The linear stability of the orbits is also\ninvestigated. All of them are unstable. A discussion in relation to the\nefficiency of Newton's method applied with grid-search initial approximations\nis held.",
      "generated_abstract": "We numerically search for periodic orbits of a free-falling body in a\nsector of three-body problem with the central symmetry. We apply the\nnon-linear eigenvalue problem to find the eigenvalues of the linearized\ndynamical system in the sector of three-body problem with the central\nsymmetry. Then, we perform numerical search for periodic orbits with the\nsemi-major axis as the variable and the period as the constraint. Our\nnumerical results show that the periodic orbit with the semi-major axis as\nthe variable and the period as the constraint can be found by the method. We\nalso find that the periodic orbit with the semi-major axis as the variable and\nthe eccentricity as the constraint can be found by the method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3064516129032258,
          "p": 0.38,
          "f": 0.33928570934311225
        },
        "rouge-2": {
          "r": 0.08695652173913043,
          "p": 0.10810810810810811,
          "f": 0.09638553722746432
        },
        "rouge-l": {
          "r": 0.2903225806451613,
          "p": 0.36,
          "f": 0.3214285664859694
        }
      }
    },
    {
      "paper_id": "math.GT.math/GT/2503.08268v1",
      "true_abstract": "We generalise the finite biquandle colouring invariant to a polynomial\ninvariant based on labelling a knot diagram with a finite birack that reduces\nto the biquandle colouring invariant in that case. The polynomial is an\ninvariant of a class of knot theories amenable to a generalisation of theorem\nof Trace on regular homotopy. We take the opportunity to reprise the relevant\ngeneralised knot theory and the theory of generalised biracks in the light of\nthis work and recent developments.",
      "generated_abstract": "aper we introduce the concept of a generalised birack, which is a\ngeneralisation of the birack polynomial invariant. In a birack, the elements\nintersect and the intersection is the birack. A generalised birack is a general\ncase of a birack, and is a generalisation of the birack polynomial invariant. We\nshow that generalised biracks are the biracks of an enlarged group of\nelementary symmetric polynomials, and that the birack polynomial invariant is\na generalised birack. We also show that a generalised birack is a general case\nof a birack and that the birack polynomial invariant is a generalised birack. We\nshow that the generalised birack polynomial invariant is a generalised birack,\nand we show that the generalised birack polynomial invariant is the birack\npolynomial",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.32608695652173914,
          "p": 0.42857142857142855,
          "f": 0.37037036546258195
        },
        "rouge-2": {
          "r": 0.08,
          "p": 0.09090909090909091,
          "f": 0.08510637799909491
        },
        "rouge-l": {
          "r": 0.2826086956521739,
          "p": 0.37142857142857144,
          "f": 0.3209876494131993
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.13556v1",
      "true_abstract": "This study examines the relationship between the concept of serious leisure\nand user innovation. We adopted the characteristics of innovative consumers\nidentified by Luthje (2004)-product use experience, information exchange, and\nnew product adoption speed-to analyze their correlation with serious leisure\nengagement. The analysis utilized consumer behavior survey data from the\n\"Marketing Analysis Contest 2023\" sponsored by Nomura Research Institute,\nexamining the relationship between innovative consumer characteristics and the\ndegree of serious leisure (Serious Leisure Inventory and Measure: SLIM). Since\nthe contest data did not directly measure innovative consumer characteristics\nor serious leisure engagement, we established alternative variables for\nquantitative analysis. The results showed that the SLIM alternative variable\nhad positive correlations with diverse product experiences and early adoption\nof new products. However, no clear relationship was found with information\nexchange among consumers. These findings suggest that serious leisure practice\nmay serve as a potential antecedent to user innovation. The leisure career\nperspective of the serious leisure concept may capture the motivations of user\ninnovators that Okada and Nishikawa (2019) identified.",
      "generated_abstract": "ctivities are an important part of user innovation. In this paper, we\nstudy the relationship between the characteristics of innovative consumers and the\ndegree of serious leisure in user innovation. First, we use the characteristics\nof innovative consumers to describe the characteristics of innovative consumers\nand analyze the characteristics of innovative consumers. Second, we use the\ndegree of serious leisure in user innovation to describe the degree of serious\nleisure in user innovation and analyze the relationship between the characteristics\nof innovative consumers and the degree of serious leisure in user innovation.\nFinally, we use the characteristics of innovative consumers and the degree of\nserious leisure in user innovation to describe the relationship between the\ncharacteristics of innovative consumers and the degree of serious leisure in user\ninnovation. The results show that: (1) the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.5555555555555556,
          "f": 0.2739725990242072
        },
        "rouge-2": {
          "r": 0.10126582278481013,
          "p": 0.32653061224489793,
          "f": 0.1545893683670565
        },
        "rouge-l": {
          "r": 0.16363636363636364,
          "p": 0.5,
          "f": 0.24657533875023463
        }
      }
    },
    {
      "paper_id": "physics.soc-ph.q-bio/PE/2502.02713v1",
      "true_abstract": "The spread of disinformation poses a significant threat to societal\nwell-being. We analyze this phenomenon using an evolutionary game theory model\nof the sender-receiver game, where senders aim to mislead receivers and\nreceivers aim to discern the truth. Using a combination of replicator\nequations, finite-size scaling analysis, and extensive Monte Carlo simulations,\nwe investigate the long-term evolutionary dynamics of this game. Our central\nfinding is a counterintuitive threshold phenomenon: the role (sender or\nreceiver) with the larger difference in payoffs between successful and\nunsuccessful interactions is surprisingly more likely to lose in the long run.\nWe show that this effect is robust across different parameter values and arises\nfrom the interplay between the relative speeds of evolution of the two roles\nand the ability of the slower evolving role to exploit the fixed strategy of\nthe faster evolving role. Moreover, for finite populations we find that the\ninitially less frequent strategy of the slower role is more likely to fixate in\nthe population. The initially rarer strategy in the less-rewarded role is,\nparadoxically, more likely to prevail.",
      "generated_abstract": "We investigate the evolution of deception in a sender-receiver game, where\nthe sender's behavior is determined by a local decision maker. In this\ngame, the receiver can detect the sender's intentions and act to protect\nhimself, which leads to a loss of information and an increase in the receiver's\nutility. We introduce a deception-based payoff measure that captures the\nrelative increase in utility caused by a deception. We derive the equilibrium\nbehavior of this game under two different conditions: (i) when the sender's\npayoff is symmetric, and (ii) when the sender's payoff is asymmetric. We\nderive a general recipe to compute the equilibrium payoff and investigate\nhow the equilibrium payoff varies with the sender's payoff and receiver's\nutility.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1891891891891892,
          "p": 0.328125,
          "f": 0.2399999953606531
        },
        "rouge-2": {
          "r": 0.05555555555555555,
          "p": 0.0891089108910891,
          "f": 0.06844105990776247
        },
        "rouge-l": {
          "r": 0.17117117117117117,
          "p": 0.296875,
          "f": 0.2171428525035103
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/TR/2412.14361v2",
      "true_abstract": "We revisit the long-only trend-following strategy presented in A Century of\nProfitable Industry Trends by Zarattini and Antonacci, which achieved\nexceptional historical performance with an 18.2% annualized return and a Sharpe\nRatio of 1.39. While the results outperformed benchmarks, practical\nimplementation raises concerns about robustness and evolving market conditions.\nThis study explores modifications addressing reliance on T-bills, alternative\nfallback allocations, and industry exclusions. Despite attempts to enhance\nadaptability through momentum signals, parameter optimization, and Walk-Forward\nAnalysis, results reveal persistent challenges. The results highlight\nchallenges in adapting historical strategies to modern markets and offer\ninsights for future trend-following frameworks.",
      "generated_abstract": "r presents a comprehensive framework for the analysis of\neconomic data, including market returns, to identify industry trends and\nquantify their impact on financial markets. Our framework employs a\nprobabilistic backtesting approach to refine returns, incorporating both\nhistorical and forecasted market returns. This approach is then used to\ndevelop a risk-adjusted model to estimate the impact of historical trends on\nmarket returns, as well as to estimate the impact of the forecasted returns on\nmarket returns. We demonstrate the utility of this framework through a\ncomparison of historical and forecasted returns for the 100 largest\nindustries. We find that the forecasted returns on the 100 largest\nindices are substantially higher than historical returns, implying that\nforecasts are more likely to underperform the market. However, the model\nestimates that the forecasted returns on the 10",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16279069767441862,
          "p": 0.2,
          "f": 0.17948717453977664
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1511627906976744,
          "p": 0.18571428571428572,
          "f": 0.16666666171926378
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.07189v1",
      "true_abstract": "Reconfigurable intelligent surface (RIS)-aided cell-free (CF) massive\nmultiple-input multiple-output (mMIMO) is a promising architecture for further\nimproving spectral efficiency (SE) with low cost and power consumption.\nHowever, conventional RIS has inevitable limitations due to its capability of\nonly reflecting signals. In contrast, beyond-diagonal RIS (BD-RIS), with its\nability to both reflect and transmit signals, has gained great attention. This\ncorrespondence focuses on using BD-RIS to improve the sum SE of CF mMIMO\nsystems. This requires completing the beamforming design under the transmit\npower constraints and unitary constraints of the BD-RIS, by optimizing active\nand passive beamformer simultaneously. To tackle this issue, we introduce an\nalternating optimization algorithm that decomposes it using fractional\nprogramming and solves the subproblems alternatively. Moreover, to address the\nchallenge introduced by the unitary constraint on the beamforming matrix of the\nBD-RIS, a manifold optimization algorithm is proposed to solve the problem\noptimally. Simulation results show that BD-RISs outperform RISs\ncomprehensively, especially in the case of the full connected architecture\nwhich achieves the best performance, enhancing the sum SE by around 40%\ncompared to ideal RISs.",
      "generated_abstract": "In this paper, we consider a cell-free massive multiple-input multiple-output\n(MIMO) system with a diagonal reflecting surface (RIS) embedded in the\nephemeral channel and a beamforming design for beamforming vectors (BFVs) that\nmaximize the sum-rate subject to the constraints of time-frequency (TF)\nrepresentations, TF-sharing, and TF-ephemerality. In this design, the BFVs are\nformulated as a convex optimization problem that is solved using a\nconjugate gradient method. The proposed design guarantees the existence of a\nlocal minimum. Moreover, we provide an upper bound on the achievable sum-rate\nand derive the optimal BFVs for the considered system. Numerical results\ndemonstrate the effectiveness of the proposed design in maximizing the sum-rate\nunder the considered constraints.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24031007751937986,
          "p": 0.40789473684210525,
          "f": 0.30243901972444975
        },
        "rouge-2": {
          "r": 0.04678362573099415,
          "p": 0.07476635514018691,
          "f": 0.0575539520995294
        },
        "rouge-l": {
          "r": 0.2248062015503876,
          "p": 0.3815789473684211,
          "f": 0.28292682460249857
        }
      }
    },
    {
      "paper_id": "q-bio.QM.stat/AP/2502.19206v2",
      "true_abstract": "Understanding the role of different age groups in disease transmission is\ncrucial for designing effective intervention strategies. A key parameter in\nage-structured epidemic models is the contact matrix, which defines the\ninteraction structure between age groups. However, accurately estimating\ncontact matrices is challenging, as different age groups respond differently to\nsurveys and are accessible through different channels. This variability\nintroduces significant epistemic uncertainty in epidemic models.\n  In this study, we introduce the Age Group Sensitivity Analysis (AGSA) method,\na novel framework for assessing the impact of age-structured contact patterns\non epidemic outcomes. Our approach integrates age-stratified epidemic models\nwith Latin Hypercube Sampling (LHS) and the Partial Rank Correlation\nCoefficient (PRCC) method, enabling a systematic sensitivity analysis of\nage-specific interactions. Additionally, we propose a new sensitivity\naggregation technique that quantifies the contribution of each age group to key\nepidemic parameters.\n  By identifying the age groups to which the model is most sensitive, AGSA\nhelps pinpoint those that introduce the greatest epistemic uncertainty. This\nallows for targeted data collection efforts, focusing surveys and empirical\nstudies on the most influential age groups to improve model accuracy. As a\nresult, AGSA can enhance epidemic forecasting and inform the design of more\neffective and efficient public health interventions.",
      "generated_abstract": "sensitivity analysis is an important aspect of epidemic\nmodeling, providing insight into how different age groups respond to an\nepidemic. However, existing methods for age group sensitivity analysis often\nrely on the analytical solutions of the epidemic equations and are not\ncomputationally efficient. In this paper, we propose a novel method for age\ngroup sensitivity analysis that utilizes the eigenvalue decomposition of a\ncontact matrix to determine the sensitivity of an epidemic to different age\ngroups. The proposed method is computationally efficient and provides a\nrobust estimate of age group sensitivity that can be used to inform public\nhealth policies to mitigate the impact of an epidemic on specific age groups.\nWe apply the proposed method to the SIR epidemic model to investigate the\nimpact of contact matrix structure on age group sensitivity. The results\ndemonstrate that the proposed method provides a more",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2537313432835821,
          "p": 0.4594594594594595,
          "f": 0.32692307233912726
        },
        "rouge-2": {
          "r": 0.07329842931937172,
          "p": 0.11764705882352941,
          "f": 0.09032257591488058
        },
        "rouge-l": {
          "r": 0.2537313432835821,
          "p": 0.4594594594594595,
          "f": 0.32692307233912726
        }
      }
    },
    {
      "paper_id": "eess.IV.q-bio/TO/2502.14707v1",
      "true_abstract": "While deep learning methods have shown great promise in improving the\neffectiveness of prostate cancer (PCa) diagnosis by detecting suspicious\nlesions from trans-rectal ultrasound (TRUS), they must overcome multiple\nsimultaneous challenges. There is high heterogeneity in tissue appearance,\nsignificant class imbalance in favor of benign examples, and scarcity in the\nnumber and quality of ground truth annotations available to train models.\nFailure to address even a single one of these problems can result in\nunacceptable clinical outcomes.We propose TRUSWorthy, a carefully designed,\ntuned, and integrated system for reliable PCa detection. Our pipeline\nintegrates self-supervised learning, multiple-instance learning aggregation\nusing transformers, random-undersampled boosting and ensembling: these address\nlabel scarcity, weak labels, class imbalance, and overconfidence, respectively.\nWe train and rigorously evaluate our method using a large, multi-center dataset\nof micro-ultrasound data. Our method outperforms previous state-of-the-art deep\nlearning methods in terms of accuracy and uncertainty calibration, with AUROC\nand balanced accuracy scores of 79.9% and 71.5%, respectively. On the top 20%\nof predictions with the highest confidence, we can achieve a balanced accuracy\nof up to 91%. The success of TRUSWorthy demonstrates the potential of\nintegrated deep learning solutions to meet clinical needs in a highly\nchallenging deployment setting, and is a significant step towards creating a\ntrustworthy system for computer-assisted PCa diagnosis.",
      "generated_abstract": "tion of prostate cancer is critical in improving prostate cancer\n(PCa) screening and treatment. However, the diagnosis of PCa remains challenging\ndue to the complexity of the anatomy, the heterogeneity of tissue, and the\nhigh incidence of false positives. To address these challenges, we introduce\nTRUSWorthy, a deep learning-based approach for the confident diagnosis of PCa in\nmicro-ultrasound (TRUS) images. Our method leverages a convolutional neural\nnetwork (CNN) to extract features from the TRUS images, followed by a\nmulti-layer perceptron (MLP) to predict the presence of PCa. Our model\ndemonstrates significant accuracy (AUC = 0.941), sensitivity (84.3%), and\nspecificity (98.7%) in detecting PCa in TRUS",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2080536912751678,
          "p": 0.3974358974358974,
          "f": 0.27312774879310686
        },
        "rouge-2": {
          "r": 0.028708133971291867,
          "p": 0.058823529411764705,
          "f": 0.03858520459507293
        },
        "rouge-l": {
          "r": 0.18120805369127516,
          "p": 0.34615384615384615,
          "f": 0.2378854580442082
        }
      }
    },
    {
      "paper_id": "cond-mat.quant-gas.nlin/PS/2503.10519v1",
      "true_abstract": "Driven systems are of fundamental scientific interest, as they can exhibit\nproperties that are radically different from the same system at equilibrium. In\ncertain cases, long-lived states of driven matter can emerge, which exhibit new\nmaterial properties. In this work, we probe the excitation spectrum of an\nemergent patterned state in a driven superfluid, finding that its response is\nidentical to that of a one-dimensional supersolid. In order to extract physical\nquantities that parametrize the observed sound modes, we apply an effective\nhydrodynamic theory of superfluid smectics, which is agnostic to microscopic\nprocesses. We therefore use the conceptual framework of supersolids to\ncharacterize an otherwise dynamic and far-from-equilibrium state.",
      "generated_abstract": "dity, the unconventional phase transition of a gas of hard spheres\nto a state of self-organized criticality, has been observed in a variety of\nsystems. Supersolidity can be realized by tuning the interaction strength\nbetween the hard spheres, such as by varying the size of the hard spheres,\ndamping ratio of the system, or the driving frequency. In this work, we have\ndemonstrated that a driven quantum gas of hard spheres can also exhibit\nsupersolid-like behavior by controlling the interaction strength between the\nhard spheres. In this work, we have demonstrated that a driven quantum gas of\nhard spheres can exhibit supersolid-like behavior by controlling the interaction\nstrength between the hard spheres. We have confirmed that the system exhibits\nsupersolid-like sound modes, where the sound velocity of the gas deviates from",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2716049382716049,
          "p": 0.34375,
          "f": 0.30344827093079674
        },
        "rouge-2": {
          "r": 0.07407407407407407,
          "p": 0.08695652173913043,
          "f": 0.07999999503200031
        },
        "rouge-l": {
          "r": 0.20987654320987653,
          "p": 0.265625,
          "f": 0.23448275368941746
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/QM/2503.03485v1",
      "true_abstract": "Understanding the biological mechanism of disease is critical for medicine,\nand in particular drug discovery. AI-powered analysis of genome-scale\nbiological data hold great potential in this regard. The increasing\navailability of single-cell RNA sequencing data has enabled the development of\nlarge foundation models for disease biology. However, existing foundation\nmodels either do not improve or only modestly improve over task-specific models\nin downstream applications. Here, we explored two avenues for improving the\nstate-of-the-art. First, we scaled the pre-training dataset to 116 million\ncells, which is larger than those used by previous models. Second, we leveraged\nthe availability of large-scale biological annotations as a form of supervision\nduring pre-training. We trained the TEDDY family of models comprising six\ntransformer-based state-of-the-art single-cell foundation models with 70\nmillion, 160 million, and 400 million parameters. We vetted our models on two\ndownstream evaluation tasks -- identifying the underlying disease state of\nheld-out donors not seen during training and distinguishing healthy cells from\ndiseased ones for disease conditions and donors not seen during training.\nScaling experiments showed that performance improved predictably with both data\nvolume and parameter count. Our models showed substantial improvement over\nexisting work on the first task and more muted improvements on the second.",
      "generated_abstract": "ll RNA sequencing (scRNA-seq) data reveal cellular heterogeneity,\nexposing the intricate details of cellular organization. The challenge is\nunderstanding the underlying mechanisms driving these diverse cell states.\nExisting foundation models (FMs) excel at modeling the large-scale\ncorrespondences between cell types and biological processes. However, they are\nlimited in their ability to capture complex cellular interactions, such as\nintercellular communication. To address this gap, we introduce TEDDY, a\nmulticellular FM that unifies the emerging family of FMs for understanding\nsingle-cell biology. TEDDY leverages a novel architecture to model both cell\nstate and communication dynamics within a unified framework. The TEDDY FM\nintegrates the cell-type-specific FMs for cell type-specific analysis with a\ncommunication-aware module to capture",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.3333333333333333,
          "f": 0.2666666618666667
        },
        "rouge-2": {
          "r": 0.020833333333333332,
          "p": 0.036036036036036036,
          "f": 0.02640263562134513
        },
        "rouge-l": {
          "r": 0.2222222222222222,
          "p": 0.3333333333333333,
          "f": 0.2666666618666667
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/LG/2503.10636v1",
      "true_abstract": "Minibatch optimal transport coupling straightens paths in unconditional flow\nmatching. This leads to computationally less demanding inference as fewer\nintegration steps and less complex numerical solvers can be employed when\nnumerically solving an ordinary differential equation at test time. However, in\nthe conditional setting, minibatch optimal transport falls short. This is\nbecause the default optimal transport mapping disregards conditions, resulting\nin a conditionally skewed prior distribution during training. In contrast, at\ntest time, we have no access to the skewed prior, and instead sample from the\nfull, unbiased prior distribution. This gap between training and testing leads\nto a subpar performance. To bridge this gap, we propose conditional optimal\ntransport C^2OT that adds a conditional weighting term in the cost matrix when\ncomputing the optimal transport assignment. Experiments demonstrate that this\nsimple fix works with both discrete and continuous conditions in\n8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method\nperforms better overall compared to the existing baselines across different\nfunction evaluation budgets. Code is available at\nhttps://hkchengrex.github.io/C2OT",
      "generated_abstract": "We study the optimal transport (OT) problem between conditional flows, which\nimprove the conditional distribution of a target distribution given a latent\ndistribution of a latent variable. We propose to use conditional flows as a\ngenerator of conditional distributions to improve the conditional distribution\nof a target distribution given a latent variable. We show that the optimal\ntransport problem between conditional flows is equivalent to the optimal\ntransport problem between a conditional distribution and a target distribution.\nWe then propose a method to solve this problem efficiently by combining\nconditional flow generation and OT. Our method can efficiently generate a\nconditional distribution given a latent variable from a target distribution\nusing a conditional flow. We further propose a method to improve the\nconditional distribution of a target distribution given a latent variable by\nusing a conditional flow. Our method can efficiently generate a conditional\ndistribution from a target distribution given a latent variable from a\nconditional flow.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15447154471544716,
          "p": 0.40425531914893614,
          "f": 0.2235294077640139
        },
        "rouge-2": {
          "r": 0.0379746835443038,
          "p": 0.07228915662650602,
          "f": 0.04979252660456989
        },
        "rouge-l": {
          "r": 0.15447154471544716,
          "p": 0.40425531914893614,
          "f": 0.2235294077640139
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/OT/2407.18835v3",
      "true_abstract": "Polychoric correlation is often an important building block in the analysis\nof rating data, particularly for structural equation models. However, the\ncommonly employed maximum likelihood (ML) estimator is highly susceptible to\nmisspecification of the polychoric correlation model, for instance through\nviolations of latent normality assumptions. We propose a novel estimator that\nis designed to be robust to partial misspecification of the polychoric model,\nthat is, the model is only misspecified for an unknown fraction of\nobservations, for instance (but not limited to) careless respondents. In\ncontrast to existing literature, our estimator makes no assumption on the type\nor degree of model misspecification. It furthermore generalizes ML estimation,\nis consistent as well as asymptotically normally distributed, and comes at no\nadditional computational cost. We demonstrate the robustness and practical\nusefulness of our estimator in simulation studies and an empirical application\non a Big Five administration. In the latter, the polychoric correlation\nestimates of our estimator and ML differ substantially, which, after further\ninspection, is likely due to the presence of careless respondents that the\nestimator helps identify.",
      "generated_abstract": "horic correlation is a measure of the relative strength of\npolynomial dependence between two or more variables. Polychoric correlation\nplays an important role in various fields, such as the analysis of\nnetworks, causal inference, and data integration. It is a measure of the\ncorrelation between two or more variables and is a weighted average of the\ncorrelations between the variables. In this paper, we propose a robust method\nfor estimating polychoric correlation in a two-dimensional space, which\nprovides more stable and reliable results in the presence of outliers. The\nproposed method is based on a maximum likelihood estimator and a robust\nvariance estimator, which is derived using the maximum likelihood estimator.\nThe proposed method is validated using simulated data and real data from the\nBrazilian National Bank. The results show that the proposed method has a\nbetter estimation accuracy and stability compared",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22123893805309736,
          "p": 0.3048780487804878,
          "f": 0.2564102515366207
        },
        "rouge-2": {
          "r": 0.09090909090909091,
          "p": 0.12096774193548387,
          "f": 0.10380622347433603
        },
        "rouge-l": {
          "r": 0.21238938053097345,
          "p": 0.2926829268292683,
          "f": 0.24615384128021048
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.00586v1",
      "true_abstract": "Early diagnosis of Alzheimer's disease (AD) is critical for intervention\nbefore irreversible neurodegeneration occurs. Structural MRI (sMRI) is widely\nused for AD diagnosis, but conventional deep learning approaches primarily rely\non intensity-based features, which require large datasets to capture subtle\nstructural changes. Jacobian determinant maps (JSM) provide complementary\ninformation by encoding localized brain deformations, yet existing multimodal\nfusion strategies fail to fully integrate these features with sMRI. We propose\na cross-attention fusion framework to model the intrinsic relationship between\nsMRI intensity and JSM-derived deformations for AD classification. Using the\nAlzheimer's Disease Neuroimaging Initiative (ADNI) dataset, we compare\ncross-attention, pairwise self-attention, and bottleneck attention with four\npre-trained 3D image encoders. Cross-attention fusion achieves superior\nperformance, with mean ROC-AUC scores of 0.903 (+/-0.033) for AD vs.\ncognitively normal (CN) and 0.692 (+/-0.061) for mild cognitive impairment\n(MCI) vs. CN. Despite its strong performance, our model remains highly\nefficient, with only 1.56 million parameters--over 40 times fewer than\nResNet-34 (63M) and Swin UNETR (61.98M). These findings demonstrate the\npotential of cross-attention fusion for improving AD diagnosis while\nmaintaining computational efficiency.",
      "generated_abstract": "'s Disease (AD) is a common neurodegenerative disorder, affecting\nmillions of people worldwide. Diagnosing AD requires the integration of\ncomputed tomography (CT) scans and MRI images, which require specialized\nmethods for segmentation and registration. In this study, we propose a novel\ndeep learning approach that integrates MRI and Jacobian maps to improve AD\ndiagnosis. Our method consists of three main components: (1) a novel\nmultimodal representation, which combines MRI and Jacobian maps, (2) an\nattention-based fusion strategy, and (3) a contrastive learning-based loss\nfunction. Our experiments demonstrate that our approach achieves superior\nperformance compared to existing methods in both image-based and\nmultimodal-based diagnostic tasks. Additionally, the fusion strategy enables\nbetter integration of different modalities,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18,
          "p": 0.30337078651685395,
          "f": 0.22594141791985445
        },
        "rouge-2": {
          "r": 0.0335195530726257,
          "p": 0.05504587155963303,
          "f": 0.04166666196204722
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.2808988764044944,
          "f": 0.20920501624621432
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09795v1",
      "true_abstract": "An isolating set of a graph is a set of vertices $S$ such that, if $S$ and\nits neighborhood is removed, only isolated vertices remain; and the isolation\nnumber is the minimum size of such a set. It is known that for every connected\ngraph apart from $K_2$ and $C_5$, the isolation number is at most one-third the\norder and indeed such a graph has three disjoint isolating sets. In this paper\nwe consider isolating sets where $S$ is required to be an independent set and\ncall the minimum size thereof the independent isolation number. While for\ngeneral graphs of order $n$ the independent isolation number can be arbitrarily\nclose to $n/2$, we show that in bipartite graphs the vertex set can be\npartitioned into three disjoint independent isolating sets, whence the\nindependent isolation number is at most $n/3$; while for $3$-colorable graphs\nthe maximum value of the independent isolation number is $(n+1)/3$. We also\nprovide a bound for $k$-colorable graphs.",
      "generated_abstract": "We prove a bound on the independence number of any graph in terms of the\ngraph's chromatic number, and in particular we obtain an upper bound on the\nnumber of edges in the independent set of a graph. We also prove a lower bound\non the number of edges in the independent set of any graph with at least six\nvertices. Finally, we prove a lower bound on the chromatic number of any graph\nwith at least six vertices, which improves on a lower bound of Demaine,\nFregly, and Herman.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1927710843373494,
          "p": 0.41025641025641024,
          "f": 0.262295077617576
        },
        "rouge-2": {
          "r": 0.06015037593984962,
          "p": 0.13559322033898305,
          "f": 0.08333332907606358
        },
        "rouge-l": {
          "r": 0.18072289156626506,
          "p": 0.38461538461538464,
          "f": 0.24590163499462517
        }
      }
    },
    {
      "paper_id": "cs.CR.eess/IV/2503.09302v1",
      "true_abstract": "This paper investigates the critical issue of data poisoning attacks on AI\nmodels, a growing concern in the ever-evolving landscape of artificial\nintelligence and cybersecurity. As advanced technology systems become\nincreasingly prevalent across various sectors, the need for robust defence\nmechanisms against adversarial attacks becomes paramount. The study aims to\ndevelop and evaluate novel techniques for detecting and preventing data\npoisoning attacks, focusing on both theoretical frameworks and practical\napplications. Through a comprehensive literature review, experimental\nvalidation using the CIFAR-10 and Insurance Claims datasets, and the\ndevelopment of innovative algorithms, this paper seeks to enhance the\nresilience of AI models against malicious data manipulation. The study explores\nvarious methods, including anomaly detection, robust optimization strategies,\nand ensemble learning, to identify and mitigate the effects of poisoned data\nduring model training. Experimental results indicate that data poisoning\nsignificantly degrades model performance, reducing classification accuracy by\nup to 27% in image recognition tasks (CIFAR-10) and 22% in fraud detection\nmodels (Insurance Claims dataset). The proposed defence mechanisms, including\nstatistical anomaly detection and adversarial training, successfully mitigated\npoisoning effects, improving model robustness and restoring accuracy levels by\nan average of 15-20%. The findings further demonstrate that ensemble learning\ntechniques provide an additional layer of resilience, reducing false positives\nand false negatives caused by adversarial data injections.",
      "generated_abstract": "oning attacks are a critical security concern for AI models\nin modern applications, where incorrect predictions can lead to significant\nreputational and financial losses. To address this challenge, we propose a\nrobust detection framework that leverages data-driven approaches and machine\nlearning models to detect and prevent data poisoning attacks. Our framework\nincludes a pre-processing step, a feature extraction step, and a model\ntraining step. The pre-processing step ensures that the input features are\nclean and free of sensitive information, while the feature extraction step\nextracts useful features for model training. The model training step uses\nadvanced machine learning models, such as XGBoost and LightGBM, to detect and\nprevent data poisoning attacks. The framework is evaluated on both synthetic\nand real-world data poisoning datasets, demonstrating its effectiveness in\ndetecting and preventing data poisoning attacks. Our findings suggest that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20689655172413793,
          "p": 0.3488372093023256,
          "f": 0.25974025506643433
        },
        "rouge-2": {
          "r": 0.03827751196172249,
          "p": 0.06611570247933884,
          "f": 0.04848484384040448
        },
        "rouge-l": {
          "r": 0.1724137931034483,
          "p": 0.29069767441860467,
          "f": 0.21645021177639112
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/CV/2503.10639v1",
      "true_abstract": "Current image generation and editing methods primarily process textual\nprompts as direct inputs without reasoning about visual composition and\nexplicit operations. We present Generation Chain-of-Thought (GoT), a novel\nparadigm that enables generation and editing through an explicit language\nreasoning process before outputting images. This approach transforms\nconventional text-to-image generation and editing into a reasoning-guided\nframework that analyzes semantic relationships and spatial arrangements. We\ndefine the formulation of GoT and construct large-scale GoT datasets containing\nover 9M samples with detailed reasoning chains capturing semantic-spatial\nrelationships. To leverage the advantages of GoT, we implement a unified\nframework that integrates Qwen2.5-VL for reasoning chain generation with an\nend-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance\nModule. Experiments show our GoT framework achieves excellent performance on\nboth generation and editing tasks, with significant improvements over\nbaselines. Additionally, our approach enables interactive visual generation,\nallowing users to explicitly modify reasoning steps for precise image\nadjustments. GoT pioneers a new direction for reasoning-driven visual\ngeneration and editing, producing images that better align with human intent.\nTo facilitate future research, we make our datasets, code, and pretrained\nmodels publicly available at https://github.com/rongyaofang/GoT.",
      "generated_abstract": "guage Models (LLMs) have achieved unprecedented performance in\ngenerating and editing visual content. However, existing approaches often\nfocus on single modalities or utilize a single LLM model, which limits their\ncapability to comprehensively integrate multimodal information. In this paper,\nwe propose GoT (GoT: Unleashing Reasoning Capability of Multimodal Large\nLanguage Models for Visual Generation and Editing) for visual generation and\nediting, which integrates visual-textual and text-visual reasoning\ncapabilities in a unified framework. To improve reasoning performance, we\nintroduce a multi-modal reasoning module that combines textual and visual\nrepresentations to enhance reasoning ability. Furthermore, we propose a\nmulti-scale reasoning mechanism to integrate multimodal reasoning into\ngeneration and editing tasks. Our model leverages a multi-modal large",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17424242424242425,
          "p": 0.27710843373493976,
          "f": 0.21395348363180108
        },
        "rouge-2": {
          "r": 0.0335195530726257,
          "p": 0.05555555555555555,
          "f": 0.04181184199589704
        },
        "rouge-l": {
          "r": 0.14393939393939395,
          "p": 0.2289156626506024,
          "f": 0.17674418130621972
        }
      }
    },
    {
      "paper_id": "cs.CL.q-fin/CP/2502.02199v1",
      "true_abstract": "Large language models (LLMs) have shown remarkable success in language\nmodelling due to scaling laws found in model size and the hidden dimension of\nthe model's text representation. Yet, we demonstrate that compressed\nrepresentations of text can yield better performance in LLM-based regression\ntasks. In this paper, we compare the relative performance of embedding\ncompression in three different signal-to-noise contexts: financial return\nprediction, writing quality assessment and review scoring. Our results show\nthat compressing embeddings, in a minimally supervised manner using an\nautoencoder's hidden representation, can mitigate overfitting and improve\nperformance on noisy tasks, such as financial return prediction; but that\ncompression reduces performance on tasks that have high causal dependencies\nbetween the input and target data. Our results suggest that the success of\ninterpretable compressed representations such as sentiment may be due to a\nregularising effect.",
      "generated_abstract": "We explore the impact of LLM embedding compression on noisy regression\ntask performance, focusing on the GPT-3.5 model, which was designed for\ngenerative tasks. We investigate two types of compression: (1) global\nreduction of dimensions, and (2) local reduction of dimension within a\nsub-dimension. We compare the effects of these compression strategies on\nperformance metrics such as accuracy, MSE, and RMSE, as well as a key\nmeasurement of model interpretability, the number of parameters per\nregression output. We find that while compression can improve model\nperformance, the trade-off is not always apparent. In particular, compression\napplied globally can produce performance degradation, while compression applied\nlocally may result in a loss of interpretability. Our results highlight the\nneed to balance compression strategies to optimize model performance and\ninterpretability, especially in noisy regression tasks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2631578947368421,
          "p": 0.2840909090909091,
          "f": 0.2732240387231629
        },
        "rouge-2": {
          "r": 0.05384615384615385,
          "p": 0.05511811023622047,
          "f": 0.054474703171887996
        },
        "rouge-l": {
          "r": 0.25263157894736843,
          "p": 0.2727272727272727,
          "f": 0.26229507697452903
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.07343v1",
      "true_abstract": "Seismic fragility curves express the probability of failure of a mechanical\nequipment conditional to an intensity measure derived from a seismic signal.\nAlthough based on a strong assumption, the probit-lognormal model is very\npopular among practitioners for estimating such curves, judging by its abundant\nuse in the literature. However, as this model is likely to lead to biased\nestimates, its use should be limited to cases for which only few data are\navailable. In practice, this involves having to resort to binary data which\nindicate the state of the structure when it has been subjected to a seismic\nloading, namely failure or non-failure. The question then arises of the choice\nof data that must be used to obtain an optimal estimate, that is to say the\nmost precise possible with the minimum of data. To answer this question, we\npropose a methodology for design of experiments in a Bayesian framework based\non the reference prior theory. This theory aims to define a so-called objective\nprior that favors data learning, which is slighty constrained in this work in\norder tackle the problems of likelihood degeneracy that are ubiquitous with\nsmall data sets. The novelty of our work is then twofold. First, we rigorously\npresent the problem of likelihood degeneracy which hampers frequentist\napproaches such as the maximum likelihood estimation. Then, we propose our\nstrategy inherited from the reference prior theory to build the data set. This\nstrategy aims to maximize the impact of the data on the posterior distribution\nof the fragility curve. Our method is applied to a case study of the nuclear\nindustry. The results demonstrate its ability to efficiently and robustly\nestimate the fragility curve, and to avoid degeneracy even with a limited\nnumber of experiments. Additionally, we demonstrate that the estimates quickly\nreach the model bias induced by the probit-lognormal modeling.",
      "generated_abstract": "This study presents a robust and efficient method for the estimation of\nprobit-lognormal seismic fragility curves (PFSCs). The method utilizes a\nsequential design of experiments (SDOE) to identify the optimal experimental\ndesign, which is then used to construct a reference PFSC. The proposed method\nis compared with the well-established Bayesian approach, which is\nparticularly advantageous in cases where experimental uncertainty is\nsignificant. The results demonstrate that the proposed method outperforms the\nBayesian approach in terms of both estimation accuracy and robustness to\nuncertainty, particularly in the presence of large experimental error. The\nmethod is also applied to the estimation of PFSCs for a single seismic site,\nproviding valuable insights into the seismic hazard and the potential for\nreducing future losses.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1791907514450867,
          "p": 0.41333333333333333,
          "f": 0.24999999578076226
        },
        "rouge-2": {
          "r": 0.04964539007092199,
          "p": 0.12389380530973451,
          "f": 0.07088607186463733
        },
        "rouge-l": {
          "r": 0.15606936416184972,
          "p": 0.36,
          "f": 0.21774193126463326
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.12393v1",
      "true_abstract": "This study investigates the emergence of power-law and other concentrated\ndistributions through a feedback loop model in crowd interactions. Agents act\nby their response functions to observations and external forces, while\nobservations change by the aggregated actions of all agents, weighted by their\nrespective influence, i.e. power or wealth. Agents wealth dynamically adjust\nbased on the alignment between an agents actions and observation outcomes:\nagents gain wealth when their actions align with observed trends and lose\nwealth otherwise. A reward function, that describes the change of agents wealth\nat each time step, manifests the differences of response functions of agents to\nobservations. When all agents responses are set to zero and feedback loop is\nbroken, agents wealth follow a normal or lognormal distribution. Otherwise,\nthis response-reward iterative feedback mechanism results in concentrated\nwealth distributions, characterized by a small number of dominant agents and\nthe marginalization of the majority. Contrasted to past studies, such\nconcentration is not limited only to asymptotic behavior at the upper tail for\nlarge variables, nor does it require the reward function to be linear to agents\nprevious wealth as formulated in random growth model and network preferential\nattachment. Probability density functions for various distributions are more\nvisually distinguishable for small values at the lower tail. In application of\nthis model, key differences in income and wealth distributions in the US vs\nJapan are attributed to different response functions of agents in the two\ncountries. The model applicability extends beyond social systems to other\nmany-body systems with analogous feedback mechanisms, where power-law\ndistributions represent a rare subset of general concentrated outcomes.",
      "generated_abstract": "We study the emergence of power-law and other Wealth distributions in a\ngroup of heterogeneous agents. We use a multi-agent model in which each agent\ncan interact with all others, and the network structure is characterized by a\ndegree distribution. We show that a power-law distribution emerges if the\ndegree distribution has a finite mean. In addition, the mean degree can be\ndifferent from the population mean, and we show that this can also lead to a\npower-law distribution. We also show that the mean degree can be finite and\npositive, and that the emergence of power-law distributions can be observed in\nother network models. Furthermore, we show that the emergence of power-law\ndistributions can be observed in any group of heterogeneous agents.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16770186335403728,
          "p": 0.47368421052631576,
          "f": 0.2477064181563
        },
        "rouge-2": {
          "r": 0.043478260869565216,
          "p": 0.11827956989247312,
          "f": 0.06358381109809907
        },
        "rouge-l": {
          "r": 0.16149068322981366,
          "p": 0.45614035087719296,
          "f": 0.23853210622969453
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2503.08674v1",
      "true_abstract": "Machine Learning Force Fields (MLFFs) are a promising alternative to\nexpensive ab initio quantum mechanical molecular simulations. Given the\ndiversity of chemical spaces that are of interest and the cost of generating\nnew data, it is important to understand how MLFFs generalize beyond their\ntraining distributions. In order to characterize and better understand\ndistribution shifts in MLFFs, we conduct diagnostic experiments on chemical\ndatasets, revealing common shifts that pose significant challenges, even for\nlarge foundation models trained on extensive data. Based on these observations,\nwe hypothesize that current supervised training methods inadequately regularize\nMLFFs, resulting in overfitting and learning poor representations of\nout-of-distribution systems. We then propose two new methods as initial steps\nfor mitigating distribution shifts for MLFFs. Our methods focus on test-time\nrefinement strategies that incur minimal computational cost and do not use\nexpensive ab initio reference labels. The first strategy, based on spectral\ngraph theory, modifies the edges of test graphs to align with graph structures\nseen during training. Our second strategy improves representations for\nout-of-distribution systems at test-time by taking gradient steps using an\nauxiliary objective, such as a cheap physical prior. Our test-time refinement\nstrategies significantly reduce errors on out-of-distribution systems,\nsuggesting that MLFFs are capable of and can move towards modeling diverse\nchemical spaces, but are not being effectively trained to do so. Our\nexperiments establish clear benchmarks for evaluating the generalization\ncapabilities of the next generation of MLFFs. Our code is available at\nhttps://tkreiman.github.io/projects/mlff_distribution_shifts/.",
      "generated_abstract": "the problem of predicting force fields in biological and medical\napplications. While current methods are primarily focused on predicting single\nforce fields, this paper explores the broader question of predicting force\nfields across a broad range of molecular species. The problem is challenging\ndue to the large variety of molecular species and the fact that biological\nsystems often exhibit complex molecular interactions, which hinders the\ngeneralization of learned representations. To address these challenges, we\npropose a novel training-time distribution shift mitigation strategy, which\nreduces the impact of distribution shifts on the learned representations. We\nevaluate our method on three datasets: the GIPR-1000, the DrugBank 5.1, and\nthe DrugBank 5.2 datasets. Our results show that our method significantly\nimproves the generalization of the learned representations, particularly for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.35294117647058826,
          "f": 0.23999999551200007
        },
        "rouge-2": {
          "r": 0.01702127659574468,
          "p": 0.03508771929824561,
          "f": 0.022922631704174044
        },
        "rouge-l": {
          "r": 0.15757575757575756,
          "p": 0.3058823529411765,
          "f": 0.2079999955120001
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.18008v4",
      "true_abstract": "We introduce NotaGen, a symbolic music generation model aiming to explore the\npotential of producing high-quality classical sheet music. Inspired by the\nsuccess of Large Language Models (LLMs), NotaGen adopts pre-training,\nfine-tuning, and reinforcement learning paradigms (henceforth referred to as\nthe LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC\nnotation, and then fine-tuned on approximately 9K high-quality classical\ncompositions conditioned on \"period-composer-instrumentation\" prompts. For\nreinforcement learning, we propose the CLaMP-DPO method, which further enhances\ngeneration quality and controllability without requiring human annotations or\npredefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in\nsymbolic music generation models with different architectures and encoding\nschemes. Furthermore, subjective A/B tests show that NotaGen outperforms\nbaseline models against human compositions, greatly advancing musical\naesthetics in symbolic music generation.",
      "generated_abstract": "aper, we introduce NotaGen, an end-to-end musicality-driven\nsystem for symbolic music generation. NotaGen is trained using a\nmulti-step paradigm, beginning with a music-specific language model (MSLM) and\nprogressing to a symbolic music generation system (SMGS). The MSLM\ntransforms a musical input into a sequence of symbolic tokens, which are\nassigned a probability distribution over the set of symbols. These tokens are\nthen combined with musical information (e.g., pitch, rhythm, dynamics, etc.)\nto generate a musical output. We demonstrate the effectiveness of this\nmulti-step paradigm by introducing a new benchmark dataset, NotaGen, which\nincludes a wide range of musical styles and is composed of both short and\nlong musical examples. NotaGen is evaluated on a series of musicality\nmetrics, including musicality score, pitch",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.24719101123595505,
          "f": 0.23404254820563614
        },
        "rouge-2": {
          "r": 0.04065040650406504,
          "p": 0.041666666666666664,
          "f": 0.04115225837524829
        },
        "rouge-l": {
          "r": 0.21212121212121213,
          "p": 0.23595505617977527,
          "f": 0.22340425033329572
        }
      }
    },
    {
      "paper_id": "physics.geo-ph.stat/AP/2502.19549v1",
      "true_abstract": "This paper presents a comparative analysis of structural seismic responses\nunder two types of ground motion inputs: (i) synthetic motions generated by\nstochastic ground motion models and (ii) recorded motions from an earthquake\ndatabase. Five key seismic response metrics - probability distributions,\nstatistical moments, correlations, tail indices, and variance-based global\nsensitivity indices - are systematically evaluated for two archetypal\nstructures: a 12-story medium-period building and a high-rise long-period\ntower. Both ground motion datasets are calibrated to a shared response\nspectrum, ensuring consistency in spectral characteristics, including spectral\nmedian, variance, and correlation structure. The analysis incorporates both\naleatory uncertainties from ground motion variability and epistemic\nuncertainties associated with structural parameters, providing a comprehensive\ncomparison of seismic responses. The results demonstrate close agreement in\nglobal response characteristics, including distributions, correlations, and\nsensitivity indices, between synthetic and recorded motions, with differences\ntypically within 15\\%. However, significant discrepancies are observed under\nextreme conditions, particularly in tail behavior, higher-order moments, and\ndrift responses of long-period structures, with differences exceeding 50\\%.\nThese discrepancies are attributed to the non-Gaussian features and complex\ncharacteristics inherent in recorded motions, which are less pronounced in\nsynthetic datasets. The findings support the use of synthetic ground motions\nfor evaluating global seismic response characteristics, while highlighting\ntheir limitations in capturing rare-event behavior and long-period structural\ndynamics.",
      "generated_abstract": "y compares the recorded and synthetic ground motions from the 2020\nground motions catalog, including the 2020 Great Florida Earthquake, to\nevaluate their relative performance and accuracy. The recorded and synthetic\nground motions are evaluated using the Fukushima-Kashima criteria, which\nevaluates the ground motions' similarity to the reference ground motion and\ntheir predictive ability for future seismic events. The results indicate that\nthe recorded ground motions exhibit a higher predictive ability for future\nseismic events compared to the synthetic ground motions. The recorded ground\nmotion exhibits a more consistent distribution of magnitudes and magnitudes\nranges across the recorded ground motions compared to the synthetic ground\nmotions. Additionally, the recorded ground motion exhibits a higher degree of\nsimilarity to the reference ground motion than the synthetic ground motions.\nTh",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15037593984962405,
          "p": 0.3448275862068966,
          "f": 0.20942407954058284
        },
        "rouge-2": {
          "r": 0.029556650246305417,
          "p": 0.06976744186046512,
          "f": 0.041522487168975886
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.3275862068965517,
          "f": 0.19895287535210118
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.02292v1",
      "true_abstract": "Selecting the right monitoring level in Remote Patient Monitoring (RPM)\nsystems for e-healthcare is crucial for balancing patient outcomes, various\nresources, and patient's quality of life. A prior work has used one-dimensional\nhealth representations, but patient health is inherently multidimensional and\ntypically consists of many measurable physiological factors. In this paper, we\nintroduce a multidimensional health state model within the RPM framework and\nuse dynamic programming to study optimal monitoring strategies. Our analysis\nreveals that the optimal control is characterized by switching curves (for\ntwo-dimensional health states) or switching hyper-surfaces (in general):\npatients switch to intensive monitoring when health measurements cross a\nspecific multidimensional surface. We further study how the optimal switching\ncurve varies for different medical conditions and model parameters. This\nfinding of the optimal control structure provides actionable insights for\nclinicians and aids in resource planning. The tunable modeling framework\nenhances the applicability and effectiveness of RPM services across various\nmedical conditions.",
      "generated_abstract": "r proposes a novel control strategy for a remote patient monitoring\nsystem (RPMS) with multidimensional health states. The proposed approach is\nbased on a hybrid neural network (HNN) to handle the high dimensionality of\nthe multidimensional health states, while leveraging the advantages of\ncontinuous-time control in the low-dimensional RPMS. The HNN consists of a\nnonlinear encoder and a linear decoder, with the encoder designing a\nrepresentation of the multidimensional health states in a low-dimensional\nspace. The decoder is responsible for mapping the input multidimensional\nhealth state to the corresponding output signal. The proposed HNN is\nparameterized by a network architecture, and a neural network is employed to\nestimate the state transition probability matrix. The proposed HNN is trained\non a simulated RPMS and evaluated on a real-world RPMS",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15315315315315314,
          "p": 0.24285714285714285,
          "f": 0.18784529912395848
        },
        "rouge-2": {
          "r": 0.026845637583892617,
          "p": 0.035398230088495575,
          "f": 0.03053434623943905
        },
        "rouge-l": {
          "r": 0.14414414414414414,
          "p": 0.22857142857142856,
          "f": 0.1767955753670524
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/TH/2502.10317v1",
      "true_abstract": "Understanding directionality is crucial for identifying causal structures\nfrom observational data. A key challenge lies in detecting collider structures,\nwhere a $V$--structure is formed between a child node $Z$ receiving directed\nedges from parents $X$ and $Y$, denoted by $X \\rightarrow Z \\leftarrow Y$.\nTraditional causal discovery approaches, such as constraint-based and\nscore-based structure learning algorithms, do not provide statistical inference\non estimated pathways and are often sensitive to latent confounding. To\novercome these issues, we introduce methodology to quantify directionality in\ncollider structures using a pair of conditional asymmetry coefficients to\nsimultaneously examine validity of the pathways $Y \\rightarrow Z$ and $X\n\\rightarrow Z$ in the collider structure. These coefficients are based on\nShannon's differential entropy. Leveraging kernel-based conditional density\nestimation and a nonparametric smoothing technique, we utilise our proposed\nmethod to estimate collider structures and provide uncertainty quantification.\n  Simulation studies demonstrate that our method outperforms existing structure\nlearning algorithms in accurately identifying collider structures. We further\napply our approach to investigate the role of blood pressure as a collider in\nepigenetic DNA methylation, uncovering novel insights into the genetic\nregulation of blood pressure. This framework represents a significant\nadvancement in causal structure learning, offering a robust, nonparametric\nmethod for collider detection with practical applications in biostatistics and\nepidemiology.",
      "generated_abstract": "The study of collider effects in observational data is of paramount\ninterest, as it provides a means of testing for the presence of new physics\n(NP) beyond the Standard Model (SM). In this paper, we present a framework for\nthe estimation of the collider effect on observational data that accounts for\nthe uncertainties in the experimental measurements. We show that the\ncollider-induced effect can be modeled using a parametric function, which is\nderived from the experimental data. This parametric function is then used to\nestimate the experimental data using maximum likelihood estimation. We apply\nthis method to a tokamak experiment where we observe a spectral feature that\nis consistent with an unpolarized electron beam colliding with a tokamak wall,\nand we perform a simulation of the collider effect. We show that the estimated\ncollider effect is in good agreement with the experimental data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1888111888111888,
          "p": 0.3253012048192771,
          "f": 0.2389380484497612
        },
        "rouge-2": {
          "r": 0.034653465346534656,
          "p": 0.05555555555555555,
          "f": 0.04268292209771023
        },
        "rouge-l": {
          "r": 0.17482517482517482,
          "p": 0.30120481927710846,
          "f": 0.22123893340551345
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2502.20943v1",
      "true_abstract": "Reference-based image super-resolution (RefSR) represents a promising\nadvancement in super-resolution (SR). In contrast to single-image\nsuper-resolution (SISR), RefSR leverages an additional reference image to help\nrecover high-frequency details, yet its vulnerability to backdoor attacks has\nnot been explored. To fill this research gap, we propose a novel attack\nframework called BadRefSR, which embeds backdoors in the RefSR model by adding\ntriggers to the reference images and training with a mixed loss function.\nExtensive experiments across various backdoor attack settings demonstrate the\neffectiveness of BadRefSR. The compromised RefSR network performs normally on\nclean input images, while outputting attacker-specified target images on\ntriggered input images. Our study aims to alert researchers to the potential\nbackdoor risks in RefSR. Codes are available at\nhttps://github.com/xuefusiji/BadRefSR.",
      "generated_abstract": "-based Image Super-Resolution (BiSR) models, which leverage the\nsuper-resolution (SR) ground-truth to generate the final image, are a\nsignificant advance in SR. However, the reliance on the SR ground-truth can\npotentially lead to the generation of adversarial images that can be\nmisleadingly attributed to the reference image. In this paper, we propose a\nbackdoor attack against reference-based BiSR models. By injecting a\npseudo-label into the model, we generate adversarial images with the same\ncontent as the reference image, thereby inducing the model to output a\nprediction that is consistent with the pseudo-label. To defend against the\nbackdoor attack, we introduce a two-stage training strategy that first\nfine-tunes the BiSR model on the reference dataset, and then fine-tunes the\nattack model on the original dataset.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.3076923076923077,
          "f": 0.275862064019025
        },
        "rouge-2": {
          "r": 0.058333333333333334,
          "p": 0.06140350877192982,
          "f": 0.05982905483234756
        },
        "rouge-l": {
          "r": 0.22916666666666666,
          "p": 0.28205128205128205,
          "f": 0.2528735582718986
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.07359v1",
      "true_abstract": "In this paper, we present an advanced wind turbine control scheme for power\nmaximization as well as for active power control, which is designed using\n$\\mathcal{H}_\\infty$ loop-shaping. Our approach involves the synthesis of two\nseparate controllers for two different operating modes. To ensure smooth\ntransitions between these modes, we implement a bumpless transfer strategy that\nreduces transient effects. A comprehensive case study demonstrates the efficacy\nof our control scheme, showing significant improvements in power tracking\naccuracy and a reduction in mechanical wear. Moreover, our control strategy\ncomes with robust stability guarantees.",
      "generated_abstract": "tracking control of wind turbines has gained significant interest\nsince its introduction in the 1990s. The control of wind turbine power is\nchallenging due to the nonlinear nature of the system, which is often modeled as\na nonlinear dynamical system. This paper presents a novel approach to control\nwind turbine power by leveraging the $\\mathcal{H}_\\infty$ loop-shaping property.\nThe proposed method introduces a new control gain, the $\\mathcal{H}_\\infty$\ngain, which is a function of the control gains and the disturbance rejection\ngains. The proposed $\\mathcal{H}_\\infty$ gain is computed by solving a\ngeneralized quadratic programming problem. Additionally, the proposed method\nemploys a low-pass filter to reduce the frequency of the control signal, which\nis a significant consideration in wind power tracking control. Simulation\nresults demonstrate the effectiveness of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2236842105263158,
          "p": 0.23943661971830985,
          "f": 0.23129251201258746
        },
        "rouge-2": {
          "r": 0.0449438202247191,
          "p": 0.036036036036036036,
          "f": 0.039999995060500615
        },
        "rouge-l": {
          "r": 0.21052631578947367,
          "p": 0.22535211267605634,
          "f": 0.2176870698357167
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.q-bio/MN/2409.05827v1",
      "true_abstract": "Many biological decision-making processes can be viewed as performing a\nclassification task over a set of inputs, using various chemical and physical\nprocesses as \"biological hardware.\" In this context, it is important to\nunderstand the inherent limitations on the computational expressivity of\nclassification functions instantiated in biophysical media. Here, we model\nbiochemical networks as Markov jump processes and train them to perform\nclassification tasks, allowing us to investigate their computational\nexpressivity. We reveal several unanticipated limitations on the input-output\nfunctions of these systems, which we further show can be lifted using\nbiochemical mechanisms like promiscuous binding. We analyze the flexibility and\nsharpness of decision boundaries as well as the classification capacity of\nthese networks. Additionally, we identify distinctive signatures of networks\ntrained for classification, including the emergence of correlated subsets of\nspanning trees and a creased \"energy landscape\" with multiple basins. Our\nfindings have implications for understanding and designing physical computing\nsystems in both biological and synthetic chemical settings.",
      "generated_abstract": "tational expressivity of biophysical models is a fundamental\nquestion in computational biology. While there is a large literature on\ncomputational expressivity of Markov chain Monte Carlo (MCMC) methods, the\nquestion has not yet been addressed in the context of other numerical methods\nsuch as Hamiltonian Monte Carlo (HMC) or the Metropolis algorithm. We show\nthat, despite the apparent promise of non-equilibrium models, their\ncomputational expressivity is limited. This is because non-equilibrium models\ncannot capture the intricate interplay between the dynamics of the system and\nthe stochasticity of the sampling process. We show that this interplay is\nessential to achieve good sampling efficiency, and that this is inevitable if\nwe want to use MCMC or HMC to perform inference. We then show that a simple\nmodel for the dynamics of a stochastic system is sufficient to express this\ninterplay. This",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19090909090909092,
          "p": 0.2441860465116279,
          "f": 0.21428570936068317
        },
        "rouge-2": {
          "r": 0.025974025974025976,
          "p": 0.031496062992125984,
          "f": 0.028469745935842606
        },
        "rouge-l": {
          "r": 0.16363636363636364,
          "p": 0.20930232558139536,
          "f": 0.183673464462724
        }
      }
    },
    {
      "paper_id": "cs.LG.q-fin/GN/2411.10325v1",
      "true_abstract": "Bitcoin, launched in 2008 by Satoshi Nakamoto, established a new digital\neconomy where value can be stored and transferred in a fully decentralized\nmanner - alleviating the need for a central authority. This paper introduces a\nlarge scale dataset in the form of a transactions graph representing\ntransactions between Bitcoin users along with a set of tasks and baselines. The\ngraph includes 252 million nodes and 785 million edges, covering a time span of\nnearly 13 years of and 670 million transactions. Each node and edge is\ntimestamped. As for supervised tasks we provide two labeled sets i. a 33,000\nnodes based on entity type and ii. nearly 100,000 Bitcoin addresses labeled\nwith an entity name and an entity type. This is the largest publicly available\ndata set of bitcoin transactions designed to facilitate advanced research and\nexploration in this domain, overcoming the limitations of existing datasets.\nVarious graph neural network models are trained to predict node labels,\nestablishing a baseline for future research. In addition, several use cases are\npresented to demonstrate the dataset's applicability beyond Bitcoin analysis.\nFinally, all data and source code is made publicly available to enable\nreproducibility of the results.",
      "generated_abstract": "s a decentralized, peer-to-peer digital currency. The currency is\ntransacted via a decentralized network of computers, known as a blockchain.\nThis study proposes a novel transaction graph dataset to investigate the\nbehavior of Bitcoin network and the evolution of the network over time. The\ndataset consists of transaction information extracted from the blockchain and\nis organized into nodes and edges. The nodes represent the identities of the\ntransactors and the edges represent the network's interactions. The dataset is\ncreated through a customizable approach that allows the researcher to select\nthe number of nodes, edges, and transaction types. The dataset can be used to\nstudy the evolution of Bitcoin network and its behavior over time, including\nthe number of transactions, the network's topology, and the evolution of the\ntransaction graph. The dataset is available at\nhttps://github.com/Moh",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15151515151515152,
          "p": 0.2857142857142857,
          "f": 0.19801979745123038
        },
        "rouge-2": {
          "r": 0.015789473684210527,
          "p": 0.02608695652173913,
          "f": 0.019672126449880187
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.2571428571428571,
          "f": 0.17821781725321056
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/PR/2403.15810v1",
      "true_abstract": "National football teams increasingly issue tradeable blockchain-based fan\ntokens to strategically enhance fan engagement. This study investigates the\nimpact of 2022 World Cup matches on the dynamic performance of each team's fan\ntoken. The event study uncovers fan token returns surged six months before the\nWorld Cup, driven by positive anticipation effects. However, intraday analysis\nreveals a reversal of fan token returns consistently declining and trading\nvolumes rising as matches unfold. To explain findings, we uncover asymmetries\nwhereby defeats in high-stake matches caused a plunge in fan token returns,\ncompared to low-stake matches, intensifying in magnitude for knockout matches.\nContrarily, victories enhance trading volumes, reflecting increased market\nactivity without a corresponding positive effect on returns. We align findings\nwith the classic market adage \"buy the rumor, sell the news,\" unveiling\ncognitive biases and nuances in investor sentiment, cautioning the dichotomy of\npre-event optimism and subsequent performance declines.",
      "generated_abstract": "World Cup is an event that involves millions of fans from all over the\nworld. Fan tokens are digital assets that allow fans to invest in the World Cup\nvia the sale of virtual tickets. This paper examines the potential for\nanticipatory gains and event-driven losses in fan token sales. We use a\nsimulation model to evaluate the potential gains and losses for each sale\nduring the tournament. Our analysis shows that, on average, fans may experience\nanticipatory gains of 10.15% and event-driven losses of 3.31%. We also\nestimate the probability of gains and losses, using a Cox regression model.\nOur results suggest that the average gains from fan token sales are 10.3%, and\nthe average losses are 3.16%. The results are consistent with previous research\non event-driven losses and anticipatory g",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1834862385321101,
          "p": 0.25,
          "f": 0.21164020675792963
        },
        "rouge-2": {
          "r": 0.028169014084507043,
          "p": 0.03225806451612903,
          "f": 0.030075182992821217
        },
        "rouge-l": {
          "r": 0.1834862385321101,
          "p": 0.25,
          "f": 0.21164020675792963
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.00492v1",
      "true_abstract": "We introduce a nonparametric spectral density estimator for fully irregularly\nsampled points in one or more dimensions, constructed using a weighted\nnonuniform Fourier sum whose weights yield a high-accuracy quadrature rule for\na user-specified window function. The resulting estimator significantly reduces\nthe aliasing seen in periodogram approaches and least squares spectral\nanalysis, sidesteps the dangers of ill-conditioning of the nonuniform Fourier\ninverse problem, and can be adapted to a wide variety of irregular sampling\nsettings. After a discussion of methods for computing the necessary weights and\na theoretical analysis of sources of bias, we close with demonstrations of the\nmethod's efficacy, including for processes that exhibit very slow spectral\ndecay and for processes in multiple dimensions.",
      "generated_abstract": "This paper presents a nonparametric framework for spectral density estimation\nfrom irregularly sampled data. The method is based on the inverse Fourier\ntransform and the spectral density estimator of the inverse Fourier transform.\nIn the case of Gaussian data, it is shown that the estimator can be\nasymptotically normal. The method is illustrated on simulated and real data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20238095238095238,
          "p": 0.425,
          "f": 0.27419354401664936
        },
        "rouge-2": {
          "r": 0.05357142857142857,
          "p": 0.11764705882352941,
          "f": 0.07361962760209291
        },
        "rouge-l": {
          "r": 0.20238095238095238,
          "p": 0.425,
          "f": 0.27419354401664936
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.15275v1",
      "true_abstract": "Factor-based forecasting using Principal Component Analysis (PCA) is an\neffective machine learning tool for dimension reduction with many applications\nin statistics, economics, and finance. This paper introduces a Supervised\nScreening and Regularized Factor-based (SSRF) framework that systematically\naddresses high-dimensional predictor sets through a structured four-step\nprocedure integrating both static and dynamic forecasting mechanisms. The\nstatic approach selects predictors via marginal correlation screening and\nscales them using univariate predictive slopes, while the dynamic method\nscreens and scales predictors based on time series regression incorporating\nlagged predictors. PCA then extracts latent factors from the scaled predictors,\nfollowed by LASSO regularization to refine predictive accuracy. In the\nsimulation study, we validate the effectiveness of SSRF and identify its\nparameter adjustment strategies in high-dimensional data settings. An empirical\nanalysis of macroeconomic indices in China demonstrates that the SSRF method\ngenerally outperforms several commonly used forecasting techniques in\nout-of-sample predictions.",
      "generated_abstract": "er time series forecasting under uncertainty and propose a novel\nsupervised screening and regularized factor-based method for forecasting. Our\napproach is based on a latent factor model with screening, which combines the\nregression of a time series with a screening task to produce a factor model.\nThe factor model is then used to construct a regularized factor-based\nforecasting model, which can be used to produce forecasts with improved\nprediction accuracy. In the screening task, a series of tests are used to\nidentify outliers, ensuring that forecasts are produced for all series in the\ndataset. The proposed method is demonstrated through several case studies. The\nfirst case study focuses on the forecasting of the daily stock market index\nreturns. Using the S&P 500 stock market index as the benchmark series, we\nproduce forecasts for the daily returns of the S&",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24786324786324787,
          "p": 0.3670886075949367,
          "f": 0.2959183625348814
        },
        "rouge-2": {
          "r": 0.03496503496503497,
          "p": 0.03968253968253968,
          "f": 0.037174716209561066
        },
        "rouge-l": {
          "r": 0.23931623931623933,
          "p": 0.35443037974683544,
          "f": 0.28571428090222833
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.09530v1",
      "true_abstract": "In this paper, we present a comprehensive review of the analysis of the\nwell-known $1 - 1/e$ upper bound on the competitiveness that any online\nalgorithm can achieve, as established in the classical paper by Karp, Vazirani,\nand Vazirani (STOC 1990). We discuss in detail all the minor and major\ntechnical issues in their approach and present a \\emph{simple yet rigorous}\nmethod to address them. Specifically, we show that the upper bound of $n(1 -\n1/e) + o(n)$ on the performance of any online algorithm, as shown in the paper,\ncan be replaced by $\\lceil n \\cdot (1 - 1/e) + 2 - 1/e \\rceil$. Our approach is\nnotable for its simplicity and is significantly less technically involved than\nexisting ones.",
      "generated_abstract": "the authors of \"Karp, Vazirani, and Vazirani\" (KVV) provided a\nalgorithm for the $n$-vertex $d$-regular graph with maximum degree $d$ that\ntakes $O(n^{1-\\frac{1}{d}})$ time and $O(n^{1/d})$ space. KVV also provided a\nsimple, yet rigorous, upper bound for the number of edges in the graph, which\nis $O(n^{1-\\frac{1}{d}})$ and $O(n^{1/d})$. In this paper, we revisit KVV's\nwork, and provide a simple, yet rigorous, upper bound for the number of edges\nin the graph, which is $O(n^{1-\\frac{1}{d}})$ and $O(n^{1/d})$. The\nrevis",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.3541666666666667,
          "f": 0.25563909313132466
        },
        "rouge-2": {
          "r": 0.05357142857142857,
          "p": 0.10526315789473684,
          "f": 0.07100591268933189
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.3125,
          "f": 0.22556390516139985
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/SR/2503.10456v1",
      "true_abstract": "Aims. In the low-collisional, partially ionized plasma (PIP) of solar\nprominences, uncharged emitters might show different signatures of magnetic\nline broadening than charged emitters. We investigate if the widths of weak\nmetall emissions in prominences exceed the thermal line broadening by a\ndifferent amount for charged and for uncharged emitters.\n  Methods. We simultaneously observe five optically thin, weak metall lines in\nthe brightness center of a quiescent prominence and compare their observed\nwidths with the thermal broadening.\n  Results. The inferred non-thermal broadening of the metall lines does not\nindicate systematic differences between the uncharged Mg b2 and Na D1 and the\ncharged Fe II emitters, only Sr II is broader.\n  Conclusions. The additional line broadening of charged emitters is reasonably\nattributed to magnetic forces. That of uncharged emitters can then come from\ntheir temporary state as ion before recombination. Magnetically induced\nvelocities will retain some time after recombination. Modelling partially\nionized plasmas then requires consideration of a memory of previous ionization\nstates.",
      "generated_abstract": "the effect of ionization memory on the ionization structure of\nplasma emitters in a solar prominence. We use a particle-in-cell (PIC)\nsimulation of a prominence plasma with a cylindrical structure and apply\nionization memory. The simulation is performed with the prominence code\nAstroPromISS and the plasma code IonPen. The plasma is initially in a\ncompressive state and the magnetic field is applied perpendicular to the\ncylindrical axis. In the PIC simulations, we focus on the ionization structure\nof the plasma emitters and their spatial distribution. The results show that\nthe ionization structure of the plasma emitters in the prominence is influenced\nby ionization memory. The memory of the ionization field can be explained by\nthe fact that the memory of the magnetic field in a magnetic configuration\nchanges the ion",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22429906542056074,
          "p": 0.42105263157894735,
          "f": 0.29268292229402143
        },
        "rouge-2": {
          "r": 0.04666666666666667,
          "p": 0.06862745098039216,
          "f": 0.055555550736961876
        },
        "rouge-l": {
          "r": 0.205607476635514,
          "p": 0.38596491228070173,
          "f": 0.26829267839158244
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.10511v1",
      "true_abstract": "One of the main challenges in children's speaker verification (C-SV) is the\nsignificant change in children's voices as they grow. In this paper, we propose\ntwo approaches to improve age-related robustness in C-SV. We first introduce a\nFeature Transform Adapter (FTA) module that integrates local patterns into\nhigher-level global representations, reducing overfitting to specific local\nfeatures and improving the inter-year SV performance of the system. We then\nemploy Synthetic Audio Augmentation (SAA) to increase data diversity and size,\nthereby improving robustness against age-related changes. Since the lack of\nlongitudinal speech datasets makes it difficult to measure age-related\nrobustness of C-SV systems, we introduce a longitudinal dataset to assess\ninter-year verification robustness of C-SV systems. By integrating both of our\nproposed methods, the average equal error rate was reduced by 19.4%, 13.0%, and\n6.1% in the one-year, two-year, and three-year gap inter-year evaluation sets,\nrespectively, compared to the baseline.",
      "generated_abstract": "speakers exhibit age-related changes in speech patterns and\nspeech recognition performance, making it challenging for current\ndeep-learning-based methods to adapt to this age-variant speech. In this paper,\nwe propose a novel age-robust framework, named Age-Related Robustness\nEnhancement for Speaker Verification (ARR-SV), to enhance robustness in children\nspeaker verification (CV). Specifically, we design a child-specific feature\nextraction module to extract age-related features from children's speech. This\nfeature extraction module is integrated with a cross-entropy loss function to\noptimize the child-specific features, improving robustness in children's\nspeech. The cross-entropy loss function is based on the principle of\nmaximum-likelihood estimation and has been widely used in speech recognition\n(SR) applications. We evaluate our proposed method on the CAS",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24545454545454545,
          "p": 0.32926829268292684,
          "f": 0.2812499951063369
        },
        "rouge-2": {
          "r": 0.06944444444444445,
          "p": 0.09523809523809523,
          "f": 0.08032128026322186
        },
        "rouge-l": {
          "r": 0.24545454545454545,
          "p": 0.32926829268292684,
          "f": 0.2812499951063369
        }
      }
    },
    {
      "paper_id": "q-bio.SC.q-bio/SC/2410.20644v1",
      "true_abstract": "Fungal keratitis is a severe vision-threatening corneal infection with a\nprognosis influenced by fungal virulence and the host's immune defense\nmechanisms. The immune system, through its regulation of the inflammatory\nresponse, ensures cells and tissues can effectively activate defense mechanisms\nin response to infection and injury. However, there is still a lack of\neffective drugs that attenuate fungal virulence while relieving the\ninflammatory response caused by fungal keratitis. Therefore, finding effective\ntreatments to solve these problems is particularly important.\n  We synthesized ZIF-90 by water-based synthesis and characterized by SEM, XRD\netc. In vitro experiments included CCK-8 and ELISA. These evaluations verified\nthe disruptive effects of ZIF-90 on Aspergillus. fumigatus spore adhesion,\nmorphology, cell membrane, and the effect of ZIF-90 on apoptosis. In addition,\nto investigate whether the metal-ligand zinc and the organic ligand imidazole\nact as essential factors in ZIF-90, we investigated the in vitro antimicrobial\nand anti-inflammatory effects of ZIF-8, ZIF-67, and MOF-74 (Zn) by MIC and\nELISA experiments.\n  ZIF-90 has therapeutic effects on fungal keratitis, which could break the\nprotective organelles of Aspergillus. fumigatus, such as the cell wall. In\naddition, ZIF-90 can avoid excessive inflammatory response by promoting\napoptosis of inflammatory cells. The results demonstrated that both zinc ions\nand imidazole possessed antimicrobial and anti-inflammatory effects. In\naddition, ZIF-90 exhibited better biocompatibility compared to ZIF-8, ZIF-67,\nand MOF-74 (Zn).\n  ZIF-90 has anti-inflammatory and antifungal effects and preferable\nbiocompatibility, and has great potential for the treatment of fungal\nkeratitis.",
      "generated_abstract": "is a leading cause of blindness in developing countries. It is\nrelated to a number of fungal infections, including Aspergillus and Candida\nfungi. Treatment is often ineffective. This study investigated the effect of\nZIF-90, a compound that has been used in the treatment of osteomyelitis in\nchildren, on fungal keratitis. ZIF-90 was administered to mice with\nAspergillus fumigatus and Candida albicans infections. Treatment with ZIF-90\nreduced inflammation and improved ocular health. The fungal infections were\ncured, and the mice were given a single dose of ZIF-90. The mice were\nobserved for six months. Treatment with ZIF-90 reduced inflammation and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15827338129496402,
          "p": 0.3548387096774194,
          "f": 0.21890546837058494
        },
        "rouge-2": {
          "r": 0.055299539170506916,
          "p": 0.13186813186813187,
          "f": 0.07792207375885501
        },
        "rouge-l": {
          "r": 0.1510791366906475,
          "p": 0.3387096774193548,
          "f": 0.20895521961436608
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2410.23297v1",
      "true_abstract": "We propose a new way of building portfolios of cryptocurrencies that provide\ngood diversification properties to investors. First, we seek to filter these\ndigital assets by creating some clusters based on their path signature. The\ngoal is to identify similar patterns in the behavior of these highly volatile\nassets. Once such clusters have been built, we propose \"optimal\" portfolios by\ncomparing the performances of such portfolios to a universe of unfiltered\ndigital assets. Our intuition is that clustering based on path signatures will\nmake it easier to capture the main trends and features of a group of\ncryptocurrencies, and allow parsimonious portfolios that reduce excessive\ntransaction fees. Empirically, our assumptions seem to be satisfied.",
      "generated_abstract": "This paper explores the application of path signature methods to the\ndigital asset market. We focus on the creation of portfolios that take into\naccount the correlation and directional behavior of individual assets, and the\napplication of path signature methods to construct portfolios. The results show\nthat the use of path signature methods can improve portfolio construction\noutcomes. The path signature method is a useful tool for evaluating the\nrelationship between asset prices, and can provide valuable insights into\nportfolio construction.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.3137254901960784,
          "f": 0.24427480440533778
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.043478260869565216,
          "f": 0.033333328605556226
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.3137254901960784,
          "f": 0.24427480440533778
        }
      }
    },
    {
      "paper_id": "cs.SC.cs/SC/2502.11787v1",
      "true_abstract": "We propose a version of the classical shape lemma for zero-dimensional ideals\nof a commutative multivariate polynomial ring to the noncommutative setting of\nzero-dimensional ideals in an algebra of differential operators.",
      "generated_abstract": "We prove a shape lemma for ideal-theoretic properties of differential\noperators on manifolds. This result provides a tool for studying the\nsemi-local properties of differential operators and their ideal-theoretic\nextensions. Our main motivation is the study of the local behavior of the\nsecond-order differential operator $D^2$ on manifolds with boundary.\nSpecifically, we obtain a sharp version of the Schoen-Yau theorem for the\nsecond-order differential operator on manifolds with boundary.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.4166666666666667,
          "p": 0.23809523809523808,
          "f": 0.3030302984022039
        },
        "rouge-2": {
          "r": 0.20689655172413793,
          "p": 0.10714285714285714,
          "f": 0.1411764660927337
        },
        "rouge-l": {
          "r": 0.4166666666666667,
          "p": 0.23809523809523808,
          "f": 0.3030302984022039
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.12211v1",
      "true_abstract": "This study presents a comprehensive techno-economic analysis of gray, blue,\nand green hydrogen production pathways, evaluating their cost structures,\ninvestment feasibility, infrastructure challenges, and policy-driven cost\nreductions. The findings confirm that gray hydrogen (1.50-2.50/kg) remains the\nmost cost-effective today but is increasingly constrained by carbon pricing.\nBlue hydrogen (2.00-3.50/kg) offers a transitional pathway but depends on CCS\ncosts, natural gas price volatility, and regulatory support. Green hydrogen\n(3.50-6.00/kg) is currently the most expensive but benefits from declining\nrenewable electricity costs, electrolyzer efficiency improvements, and\ngovernment incentives such as the Inflation Reduction Act (IRA), which provides\ntax credits of up to 3.00/kg. The analysis shows that renewable electricity\ncosts below 20-30/MWh are essential for green hydrogen to achieve cost parity\nwith fossil-based hydrogen. The DOE's Hydrogen Shot Initiative aims to lower\ngreen hydrogen costs to 1.00/kg by 2031, emphasizing the need for CAPEX\nreductions, economies of scale, and improved electrolyzer efficiency.\nInfrastructure remains a critical challenge, with pipeline retrofitting\nreducing transport costs by 50-70%, though liquefied hydrogen and chemical\ncarriers remain costly due to energy losses and reconversion expenses.\nInvestment trends indicate a shift toward green hydrogen, with over 250 billion\nprojected by 2035, surpassing blue hydrogen's expected 100 billion. Carbon\npricing above $100/ton CO2 will likely make gray hydrogen uncompetitive by\n2030, accelerating the shift to low-carbon hydrogen. Hydrogen's long-term\nviability depends on continued cost reductions, policy incentives, and\ninfrastructure expansion, with green hydrogen positioned as a cornerstone of\nthe net-zero energy transition by 2035.",
      "generated_abstract": "is a key enabler of net-zero carbon emissions. The transition to\nenough hydrogen to achieve net-zero carbon emissions will require the\nconstruction of new hydrogen production facilities. This paper provides a\ncomprehensive analysis of the costs, policies, and scalability of hydrogen\nproduction in the context of net-zero carbon emissions. We examine the costs of\nhydrogen production across three hydrogen production pathways: renewable\nelectrolysis, hydrogen production from fossil fuels, and electrolysis of\nwater. We also evaluate the potential for hydrogen to play a key role in\ntransitioning to net-zero emissions, including the costs, policies, and\nscalability of hydrogen production. Our analysis provides valuable insights into\nthe cost and policy considerations for hydrogen production and demonstrates\nthat hydrogen production is a crucial enabler for ach",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14857142857142858,
          "p": 0.4,
          "f": 0.21666666271701396
        },
        "rouge-2": {
          "r": 0.02459016393442623,
          "p": 0.06060606060606061,
          "f": 0.03498541863407302
        },
        "rouge-l": {
          "r": 0.12571428571428572,
          "p": 0.3384615384615385,
          "f": 0.18333332938368066
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08712v1",
      "true_abstract": "This study introduces the SHAP-integrated convolutional diagnostic network\n(SICDN), an interpretable feature selection method designed for limited\ndatasets, to address the challenge posed by data privacy regulations that\nrestrict access to medical datasets. The SICDN model was tested on\nclassification tasks using pneumonia and breast cancer datasets, demonstrating\nover 97% accuracy and surpassing four popular CNN models. We also integrated a\nhistorical weighted moving average technique to enhance feature selection. The\nSICDN shows potential in medical image prediction, with the code available on\nhttps://github.com/AIPMLab/SICDN.",
      "generated_abstract": "vancements in deep learning have made it possible to extract\nmedical features from medical images. However, existing deep learning models\noften struggle with data sparsity and lack interpretable features. To address\nthese challenges, we propose SHAP-Integrated Convolutional Diagnostic Networks\n(SHAP-CDN), a novel framework that integrates SHAP analysis with\nconvolutional neural networks (CNNs). SHAP-CDN leverages SHAP values to\nselect relevant feature subsets for CNNs, improving their ability to\nunderstand and extract useful information. Furthermore, we integrate SHAP\nanalysis with CNNs to enhance interpretability and interpretability. We\nevaluate SHAP-CDN on the UCI breast cancer dataset and demonstrate its\npotential in providing interpretable features for medical analysis. The\nresults demonstrate that SHAP-CDN outperforms existing CNNs in both accuracy\nand interpret",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3150684931506849,
          "p": 0.27058823529411763,
          "f": 0.2911392355351707
        },
        "rouge-2": {
          "r": 0.04878048780487805,
          "p": 0.03571428571428571,
          "f": 0.04123710852162881
        },
        "rouge-l": {
          "r": 0.3013698630136986,
          "p": 0.25882352941176473,
          "f": 0.2784810076870694
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.09508v2",
      "true_abstract": "The online randomized primal-dual method has widespread applications in\nonline algorithm design and analysis. A key challenge is identifying an\nappropriate function space, $F$, in which we search for an optimal updating\nfunction $f \\in F$ that yields the best possible lower bound on the\ncompetitiveness of a given algorithm. The choice of $F$ must balance two\ncompeting objectives: on one hand, it should impose sufficient simplifying\nconditions on $f$ to facilitate worst-case analysis and establish a valid lower\nbound; on the other hand, it should remain general enough to offer a broad\nselection of candidate functions. The tradeoff is that any additional\nconstraints on $f$ that can facilitate competitive analysis may also lead to a\nsuboptimal choice, weakening the resulting lower bound.\n  To address this challenge, we propose an auxiliary-LP-based framework capable\nof effectively approximating the best possible competitiveness achievable when\napplying the randomized primal-dual method to different function spaces.\nSpecifically, we examine the framework introduced by Huang and Zhang (STOC\n2020), which analyzes Stochastic Balance for vertex-weighted online matching\nwith stochastic rewards. Our approach yields both lower and upper bounds on the\nbest possible competitiveness attainable using the randomized primal-dual\nmethod for different choices of ${F}$. Notably, we establish that Stochastic\nBalance achieves a competitiveness of at least $0.5796$ for the problem (under\nequal vanishing probabilities), improving upon the previous bound of $0.576$ by\nHuang and Zhang (STOC 2020). Meanwhile, our analysis yields an upper bound of\n$0.5810$ for a function space strictly larger than that considered in Huang and\nZhang (STOC 2020).",
      "generated_abstract": "imal-dual algorithms are widely used in online optimization to\nperform stochastic optimization in an online fashion. In this work, we\ninvestigate the optimal performance of online primal-dual methods under\ntime-varying constraints. We first establish the lower bound of the\noptimality gap of the online primal-dual method when the constraint set is\nbounded. This bound is tightened to the optimal performance of the method when\nthe constraint set is open. We further establish a lower bound of the\nsuboptimality gap of the online primal-dual method under a time-varying\nconstraint set. This bound is also tightened to the optimal performance of the\nmethod under the time-varying constraint set when the objective function is\nconvex. Based on these lower bounds, we further propose an algorithm that\napproximates the optimal performance of the online primal-dual method under the\ntime-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16447368421052633,
          "p": 0.423728813559322,
          "f": 0.23696682061588917
        },
        "rouge-2": {
          "r": 0.026200873362445413,
          "p": 0.06521739130434782,
          "f": 0.03738317348084788
        },
        "rouge-l": {
          "r": 0.1513157894736842,
          "p": 0.3898305084745763,
          "f": 0.21800947464432519
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.00233v1",
      "true_abstract": "This study employs a Bayesian Probit model to empirically analyze peer\neffects and herd behavior among consumers during the \"Double 11\" shopping\nfestival, using data collected through a questionnaire survey. The results\ndemonstrate that peer effects significantly influence consumer decision-making,\nwith the probability of participation in the shopping event increasing notably\nwhen roommates are involved. Additionally, factors such as gender, online\nshopping experience, and fashion consciousness significantly impact consumers'\nherd behavior. This research not only enhances the understanding of online\nshopping behavior among college students but also provides empirical evidence\nfor e-commerce platforms to formulate targeted marketing strategies. Finally,\nthe study discusses the fragility of online consumption activities, the need\nfor adjustments in corporate marketing strategies, and the importance of\npromoting a healthy online culture.",
      "generated_abstract": "y investigates the impact of the Double 11 Shopping Festival on\npeer effects, focusing on the effects of the event on consumer behavior.\nUsing data from 2017 and 2018, we find that the event has a significant\ninfluence on consumer behavior, resulting in increased sales, higher prices, and\nmore crowded shopping areas. These effects are most pronounced in urban areas\nwith high population densities and high prices, which suggests that the event\nhas a stronger impact on urban consumers than rural consumers. Furthermore, we\nfind that the effect of the event is more pronounced in cities with lower\npopulation densities, indicating that the event may be more beneficial to\nurban consumers. These findings suggest that the Double 11 Shopping Festival\nhas the potential to stimulate consumer spending and drive economic growth,\nproviding valuable insights for policymakers and business",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21739130434782608,
          "p": 0.24691358024691357,
          "f": 0.23121386785258458
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.18478260869565216,
          "p": 0.20987654320987653,
          "f": 0.1965317869277291
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.cond-mat/quant-gas/2503.09275v1",
      "true_abstract": "The present paper is devoted to comprehensive theoretical studies of\nexction-polariton quantum fluids specificities in the optics of their\nutilization for quantum turbulence research. We show that a non-trivial\nimplementation of time-varying potential for excitation of quantum fluid\n(injection of quantized vortices) via the stirring procedure can be efficiently\nsubstituted with resonant excitation-based phase-imprinting techniques. The\nmost efficient phase pattern corresponds to imprinting of tiles with randomly\noriented plane waves in each. The resulting turbulent flows, spatial vortex\ndistributions, and clustering statistics resemble those for the case of a\nconventional spoon-stirring scheme. We quantify the limitations on the lifetime\nand density depletion for the development and sustainability of quantum\nturbulence. The yield is the necessity to prevent the density depletion for\nmore than one order of magnitude. Finally, we demonstrate that turbulence is\nrobust with respect to alternating gain and loss at a certain range of\nmodulation parameters, which corresponds to laser operating above and below\ncondensation threshold.",
      "generated_abstract": "driven-dissipative turbulence in exciton-polariton quantum fluids\n(QFs), with an emphasis on the effects of nonlinearity. We consider\nsingle-mode exciton-polariton condensates with a finite-amplitude component\ndescribed by a nonlinear Schr\\\"odinger equation. We show that in the\nnonlinear regime, the mean squared displacement of the condensate remains\nsmall, even when the driving frequency is high. This is due to the fact that\nthe driving frequency induces a nonlinear phase-space structure in the\ncondensate, which stabilizes the turbulence. We also show that the turbulence\npersists even when the driving frequency is much larger than the condensate\nwavenumber. We provide a simple analytical model for the driving frequency and\nwavenumber dependence of the turbulence,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16363636363636364,
          "p": 0.2727272727272727,
          "f": 0.20454544985795467
        },
        "rouge-2": {
          "r": 0.039735099337748346,
          "p": 0.06451612903225806,
          "f": 0.04918032315137105
        },
        "rouge-l": {
          "r": 0.16363636363636364,
          "p": 0.2727272727272727,
          "f": 0.20454544985795467
        }
      }
    },
    {
      "paper_id": "hep-th.nlin/SI/2503.05890v1",
      "true_abstract": "This review paper explores the Riccati-type pseudo-potential formulation\napplied to the quasi-integrable sine-Gordon, KdV, and NLS models. The proposed\nframework provides a unified methodology for analyzing quasi-integrability\nproperties across various integrable systems, including deformations of the\nsine-Gordon, Bullough-Dodd, Toda, KdV, pKdV, NLS and SUSY sine-Gordon models.\nKey findings include the emergence of infinite towers of anomalous conservation\nlaws within the Riccati-type approach and the identification of exact non-local\nconservation laws in the linear formulations of deformed models. As modified\nintegrable models play a crucial role in diverse fields of non-linear\nphysics-such as Bose-Einstein condensation, superconductivity, gravity models,\noptics, and soliton turbulence-these results may have far-reaching\napplications.",
      "generated_abstract": "We consider a class of deformed soliton theories that are defined by\na single pseudo-potential. We show that all these theories possess a\nquasi-integrable formulation that involves a generalized Riccati equation.\nMoreover, we obtain a new formulation of the quasi-integrability of\nsoliton-type theories that involves a generalized Schr\\\"odinger equation.\nFinally, we apply our results to several examples.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1375,
          "p": 0.28205128205128205,
          "f": 0.18487394517336359
        },
        "rouge-2": {
          "r": 0.019417475728155338,
          "p": 0.038461538461538464,
          "f": 0.025806447154215126
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.2564102564102564,
          "f": 0.16806722248428796
        }
      }
    },
    {
      "paper_id": "hep-ph.nucl-th/2503.09686v1",
      "true_abstract": "We explore the confining pressure inside the nucleon and the related\ngravitational form factor referred to as the D-term, using the skyrmion\napproach based on the scale-invariant chiral perturbation theory, where the\nskyrmion is described as the nucleon and a scalar meson couples to the scale\nanomaly through the low energy theorem. Within this model framework, the\ncurrent quark mass and gluonic quantum contributions to the scale anomaly can\nbe described by the pion and scalar meson masses, respectively, through\nmatching with the underlying QCD. By considering the decomposition of the\nenergy momentum tensor of nucleon, we examine the role of the scale anomaly\ncontributions in the pressure inside the nucleon. As a result, the gluonic\nscale anomaly is found to dominate the confining pressure. Compared to the\nresult based on the conventional chiral perturbation theory in the chiral\nlimit, our result for the total pressure is capable of qualitatively improving\nthe alignment with lattice QCD observations. Moreover, the pressure from the\ngluonic scale anomaly is widely distributed in position space, leading to its\nsubstantial contribution to the D-term.",
      "generated_abstract": "The confining pressure inside nucleon and D-term have been studied within\nthe framework of the QCD sum rules. In this paper, we propose to calculate\nthe confining pressure inside nucleon and D-term by the gluonic scale anomaly,\nwhich has been proposed in the last ten years. By using the gluonic scale\nanomaly, the confining pressure inside nucleon and D-term can be evaluated\nwithout using the lattice QCD data. The numerical results indicate that the\ngluonic scale anomaly can be a good tool to study the confining pressure of\nnucleon and D-term. Our results show that the confining pressure inside\nnucleon can be expressed as the sum of the gluonic scale anomaly and the\ncolor-flavor-locking term. The gluonic scale anomaly is dominant in the\nconfining pressure inside nucleon and D-term.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25510204081632654,
          "p": 0.4166666666666667,
          "f": 0.3164556914917482
        },
        "rouge-2": {
          "r": 0.11333333333333333,
          "p": 0.19540229885057472,
          "f": 0.14345991096512328
        },
        "rouge-l": {
          "r": 0.24489795918367346,
          "p": 0.4,
          "f": 0.30379746364364696
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2501.12837v1",
      "true_abstract": "BRBVS is a publicly available \\texttt{R} package on CRAN that implements the\nalgorithm proposed in Petti et al.(2024a). The algorithm was developed as the\nfirst proposal of variable selection for the class of Bivariate Survival Copula\nModels originally proposed in Marra & Radice (2020) and implemented in the\n\\texttt{GJRM} package. The core of the \\texttt{BRBVS} package is to implement\nand make available to practitioners variable selection algorithms for bivariate\nsurvival data affected by censoring, providing easy-to-use functions and\ngraphical outputs. The idea behind the algorithm is almost general and may also\nbe extended to different class of models.",
      "generated_abstract": "iate variable selection (BVS) is a vital tool in survival analysis\nfor identifying important variables. However, existing BVS methods are limited\nby their inability to deal with complex domain-specific models. In this\npaper, we propose the BRBVS R package, a novel R package for bivariate BVS in\ncopula survival models. The BRBVS algorithm is based on the bivariate\nconditional independence (BCI) test. It is designed to identify the important\nvariables in survival analysis and provide a comprehensive feature selection\nmechanism. The proposed BRBVS algorithm is built on the bivariate conditional\nindependence (BCI) test, which is based on the bivariate copula, and\nintroduces a new approach for bivariate feature selection. The BCI test\ndetermines the conditional independence between two variables, and the\ncomplementary test determ",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2714285714285714,
          "p": 0.25675675675675674,
          "f": 0.26388888389274695
        },
        "rouge-2": {
          "r": 0.0425531914893617,
          "p": 0.03773584905660377,
          "f": 0.03999999501800062
        },
        "rouge-l": {
          "r": 0.2571428571428571,
          "p": 0.24324324324324326,
          "f": 0.2499999950038581
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.05025v1",
      "true_abstract": "We develop ProtComposer to generate protein structures conditioned on spatial\nprotein layouts that are specified via a set of 3D ellipsoids capturing\nsubstructure shapes and semantics. At inference time, we condition on\nellipsoids that are hand-constructed, extracted from existing proteins, or from\na statistical model, with each option unlocking new capabilities.\nHand-specifying ellipsoids enables users to control the location, size,\norientation, secondary structure, and approximate shape of protein\nsubstructures. Conditioning on ellipsoids of existing proteins enables\nredesigning their substructure's connectivity or editing substructure\nproperties. By conditioning on novel and diverse ellipsoid layouts from a\nsimple statistical model, we improve protein generation with expanded Pareto\nfrontiers between designability, novelty, and diversity. Further, this enables\nsampling designable proteins with a helix-fraction that matches PDB proteins,\nunlike existing generative models that commonly oversample conceptually simple\nhelix bundles. Code is available at https://github.com/NVlabs/protcomposer.",
      "generated_abstract": "tructure prediction is a fundamental task in biology that aims to\npredict the three-dimensional (3D) structure of proteins based on their\nsequences. While sequence-based models have achieved state-of-the-art performance,\nthey are limited by the difficulty of handling large protein sequences and\nrequiring extensive pretraining. To address these challenges, we propose\nProtComposer, a novel 3D structure generation model that combines a sequence-\nbased model with a compositional model to enhance protein structure prediction\nperformance. Our model leverages 3D ellipsoidal objects (3DEOs) to capture\nstructural information in proteins. To model the interactions between 3DEOs and\nproteins, we propose a new 3DEO-based structure generation loss, which\nintroduces two additional components: 1) a 3DEO-based score that measures the\ndistance",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2079207920792079,
          "p": 0.25609756097560976,
          "f": 0.2295081917752099
        },
        "rouge-2": {
          "r": 0.007407407407407408,
          "p": 0.008928571428571428,
          "f": 0.008097161035260127
        },
        "rouge-l": {
          "r": 0.1782178217821782,
          "p": 0.21951219512195122,
          "f": 0.19672130652930825
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/EM/2501.06270v1",
      "true_abstract": "The long-term estimation of the Marxist average rate of profit does not\nadhere to a theoretically grounded standard regarding which economic activities\nshould or should not be included for such purposes, which is relevant because\nmethodological non-uniformity can be a significant source of overestimation or\nunderestimation, generating a less accurate reflection of the capital\naccumulation dynamics. This research aims to provide a standard Marxist\ndecision criterion regarding the inclusion and exclusion of economic activities\nfor the calculation of the Marxist average profit rate for the case of United\nStates economic sectors from 1960 to 2020, based on the Marxist definition of\nproductive labor, its location in the circuit of capital, and its relationship\nwith the production of surplus value. Using wavelet-transformed Daubechies\nfilters with increased symmetry, empirical mode decomposition, Hodrick-Prescott\nfilter embedded in unobserved components model, and a wide variety of unit root\ntests the internal theoretical consistency of the presented criteria is\nevaluated. Also, the objective consistency of the theory is evaluated by a\ndynamic factor auto-regressive model, Principal Component Analysis, Singular\nValue Decomposition and Backward Elimination with Linear and Generalized Linear\nModels. The results are consistent both theoretically and econometrically with\nthe logic of Marx's political economy.",
      "generated_abstract": "st analysis of the average rate of profit (ARP) has been challenged by\nseveral critics, including the revisionist approach. This study proposes that\nARP may be expressed as a function of the sectorial exclusion criteria, which\nis a more valid approach to Marxist analysis. The study explores the ARP in the\nUnited States from 1960 to 2020 using the average ARP method. The study\nreveals that the ARP was stable in the 1960s, 1970s, and 1980s, but it\ndecreased significantly in the 1990s and 2000s. The ARP has decreased since\n2010 and is expected to continue to decrease. The ARP is also affected by the\ndecrease in the share of the manufacturing sector and the increase in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16153846153846155,
          "p": 0.29577464788732394,
          "f": 0.20895521931140326
        },
        "rouge-2": {
          "r": 0.04838709677419355,
          "p": 0.08411214953271028,
          "f": 0.06143344246246352
        },
        "rouge-l": {
          "r": 0.14615384615384616,
          "p": 0.2676056338028169,
          "f": 0.18905472179896546
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2501.03658v2",
      "true_abstract": "We characterise the solutions to a continuous-time optimal liquidity\nprovision problem in a market populated by informed and uninformed traders. In\nour model, the asset price exhibits fads -- these are short-term deviations\nfrom the fundamental value of the asset. Conditional on the value of the fad,\nwe model how informed traders and uninformed traders arrive in the market. The\nmarket maker knows of the two groups of traders but only observes the anonymous\norder arrivals. We study both, the complete information and the partial\ninformation versions of the control problem faced by the market maker. In such\nframeworks, we characterise the value of information, and we find the price of\nliquidity as a function of the proportion of informed traders in the market.\nLastly, for the partial information setup, we explore how to go beyond the\nKalman-Bucy filter to extract information about the fad from the market\narrivals.",
      "generated_abstract": "the market-making problem under information asymmetry in a\nmarket with two types of traders. One type of trader is informed, while the\nother is uninformed. The information asymmetry is controlled by the market\nmaking fee. The market maker has an incentive to maximize his profit by\nmaintaining the price of the asset below the market price. The uninformed\ntrader's goal is to find the maximum value of the price at which he can make\na profit, and the informed trader's goal is to find the minimum price at which\nhe can make a profit. We characterize the equilibrium price and the\nmarket-making fee in this setting, and we show that the market-making fee can\nbe as low as $0.5$ per transaction. We also characterize the equilibrium\nportfolio, and we show that the uninformed trader can make a profit of at",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2911392405063291,
          "p": 0.32857142857142857,
          "f": 0.3087248272330076
        },
        "rouge-2": {
          "r": 0.1171875,
          "p": 0.13392857142857142,
          "f": 0.12499999502222242
        },
        "rouge-l": {
          "r": 0.26582278481012656,
          "p": 0.3,
          "f": 0.281879189649115
        }
      }
    },
    {
      "paper_id": "math.MG.math/MG/2503.05435v1",
      "true_abstract": "We show that the centers of the excircles of a bicentric polygon $B$ are\nconcyclic on a circle $E$. The center of the circumscribed circle $K$ of $B$ is\nthe midpoint of the center of $E$ and the center of the inscribed circle $C$ of\n$B$. The radius of $E$ is given by a simple formula in terms of the radii of\n$C$ and $K$ and the distance between their centers.",
      "generated_abstract": "We prove that every bicentric polygon is cyclic. Conversely, given a\ncyclic polygon, we construct the bicentric polygon in which it lies. We also\nshow that every bicentric polygon has a cyclic counterpart, which we call its\nexcenters. The excenters are the centers of the cycles of a bicentric polygon.\nWe prove that every excenters of a bicentric polygon is concyclic, and\nconversely, every concyclic polygon has an excenters. We also show that every\nexcenters of a cyclic polygon is a cyclic excenters of the cyclic polygon in\nwhich it lies.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.4444444444444444,
          "p": 0.4444444444444444,
          "f": 0.44444443944444445
        },
        "rouge-2": {
          "r": 0.11666666666666667,
          "p": 0.11864406779661017,
          "f": 0.11764705382388271
        },
        "rouge-l": {
          "r": 0.3888888888888889,
          "p": 0.3888888888888889,
          "f": 0.3888888838888889
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2503.01226v1",
      "true_abstract": "Dementia, a progressive neurodegenerative disorder, affects memory,\nreasoning, and daily functioning, creating challenges for individuals and\nhealthcare systems. Early detection is crucial for timely interventions that\nmay slow disease progression. Large pre-trained models (LPMs) for text and\naudio, such as Generative Pre-trained Transformer (GPT), Bidirectional Encoder\nRepresentations from Transformers (BERT), and Contrastive Language-Audio\nPretraining (CLAP), have shown promise in identifying cognitive impairments.\nHowever, existing studies generally rely heavily on expert-annotated datasets\nand unimodal approaches, limiting robustness and scalability. This study\nproposes a context-based multimodal method, integrating both text and audio\ndata using the best-performing LPMs in each modality. By incorporating\ncontextual embeddings, our method improves dementia detection performance.\nAdditionally, motivated by the effectiveness of contextual embeddings, we\nfurther experimented with a context-based In-Context Learning (ICL) as a\ncomplementary technique. Results show that GPT-based embeddings, particularly\nwhen fused with CLAP audio features, achieve an F1-score of $83.33\\%$,\nsurpassing state-of-the-art dementia detection models. Furthermore, raw text\ndata outperforms expert-annotated datasets, demonstrating that LPMs can extract\nmeaningful linguistic and acoustic patterns without extensive manual labeling.\nThese findings highlight the potential for scalable, non-invasive diagnostic\ntools that reduce reliance on costly annotations while maintaining high\naccuracy. By integrating multimodal learning with contextual embeddings, this\nwork lays the foundation for future advancements in personalized dementia\ndetection and cognitive health research.",
      "generated_abstract": "y introduces Dementia Insights, a novel approach to dementia\ndetection based on multi-modal and contextual features. Our framework leverages\ntext, image, and audio modalities to capture the heterogeneity of dementia\ndisease processes, as well as the interconnectedness of these processes with\nneuropsychological symptoms. We introduce the Dementia Insights framework, which\nis based on a hybrid contextual encoder-decoder architecture that integrates\nmulti-modal information across different modalities. The framework employs a\nmulti-head self-attention mechanism to capture the temporal and contextual\nrelationships among different modalities, enabling effective multi-modal\ninteraction and feature fusion. Additionally, we propose a novel feature\nencoding strategy to integrate text and audio modalities, enhancing the\nunderstanding of dementia patients. Our experiments demonstrate that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10240963855421686,
          "p": 0.22077922077922077,
          "f": 0.1399176911439653
        },
        "rouge-2": {
          "r": 0.014354066985645933,
          "p": 0.02830188679245283,
          "f": 0.019047614582213192
        },
        "rouge-l": {
          "r": 0.0963855421686747,
          "p": 0.2077922077922078,
          "f": 0.1316872384690682
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/MS/2502.03402v2",
      "true_abstract": "This paper introduces a new mathematical framework for analysis and\noptimization of tensor expressions within an enclosing loop. Tensors are\nmulti-dimensional arrays of values. They are common in high performance\ncomputing (HPC) and machine learning domains. Our framework extends Scalar\nEvolution - an important optimization pass implemented in both LLVM and GCC -\nto tensors. Scalar Evolution (SCEV) relies on the theory of `Chain of\nRecurrences' for its mathematical underpinnings. We use the same theory for\nTensor Evolution (TeV). While some concepts from SCEV map easily to TeV -- e.g.\nelement-wise operations; tensors introduce new operations such as\nconcatenation, slicing, broadcast, reduction, and reshape which have no\nequivalent in scalars and SCEV. Not all computations are amenable to TeV\nanalysis but it can play a part in the optimization and analysis parts of ML\nand HPC compilers. Also, for many mathematical/compiler ideas, applications may\ngo beyond what was initially envisioned, once others build on it and take it\nfurther. We hope for a similar trajectory for the tensor-evolution concept.",
      "generated_abstract": "mputation is a fundamental computational task, widely used in\ndata analysis and machine learning. It involves matrix multiplication,\nsummation, and product operations. In this paper, we present Tensor Evolution,\na novel framework that accelerates tensor computation by using recurrences.\nTensor Evolution is an extension of Tensor Recurrence, a framework for\nefficiently evaluating tensor computations using recurrence. Tensor Evolution\nenables users to easily construct recurrences for tensor computations and\nimplement them using high-level languages. The framework includes a high-level\ninterface, a Tensor Evolution toolkit, and a series of reference implementations\nin several high-level languages. This paper provides a comprehensive\ndescription of the framework, demonstrates its effectiveness in solving\nnumerous tensor computation problems, and introduces a new tensor data type\nthat is based on Tensor Evolution. In particular, we present a novel",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19672131147540983,
          "p": 0.2891566265060241,
          "f": 0.23414633664437845
        },
        "rouge-2": {
          "r": 0.04790419161676647,
          "p": 0.06611570247933884,
          "f": 0.05555555068311193
        },
        "rouge-l": {
          "r": 0.16393442622950818,
          "p": 0.24096385542168675,
          "f": 0.19512194640047603
        }
      }
    },
    {
      "paper_id": "stat.ME.q-bio/MN/2503.05448v1",
      "true_abstract": "Graphical modeling is a widely used tool for analyzing conditional\ndependencies between variables and traditional methods may struggle to capture\nshared and distinct structures in multi-group or multi-condition settings.\nJoint graphical modeling (JGM) extends this framework by simultaneously\nestimating network structures across multiple related datasets, allowing for a\ndeeper understanding of commonalities and differences. This capability is\nparticularly valuable in fields such as genomics and neuroscience, where\nidentifying variations in network topology can provide critical biological\ninsights. Existing JGM methodologies largely fall into two categories:\nregularization-based approaches, which introduce additional penalties to\nenforce structured sparsity, and Bayesian frameworks, which incorporate prior\nknowledge to improve network inference. In this study, we explore an\nalternative method based on two-target linear covariance matrix shrinkage.\nFormula for optimal shrinkage intensities is proposed which leads to the\ndevelopment of JointStein framework. Performance of JointStein framework is\nproposed through simulation benchmarking which demonstrates its effectiveness\nfor large-scale single-cell RNA sequencing (scRNA-seq) data analysis. Finally,\nwe apply our approach to glioblastoma scRNA-seq data, uncovering dynamic shifts\nin T cell network structures across disease progression stages. The result\nhighlights potential of JointStein framework in extracting biologically\nmeaningful insights from high-dimensional data.",
      "generated_abstract": "ScRNAseq data analysis is a critical step for understanding the complex\nnetworks of gene expression in different cell types. However, the large size\nand heterogeneity of the data pose significant challenges in estimating the\ngenetic network. This study introduces a novel framework to estimate the\ngenetic network from the transcriptome data using the graphical model\nestimation approach. The proposed method is based on the Stein-type\nshrinkage for fast large-scale network inference, which addresses the curse of\ndimensionality in scRNAseq data. By leveraging the graphical model, the\nproposed method can estimate the network structure, node attributes, and\nexpression profiles simultaneously, enabling the accurate inference of the\ngene-gene and gene-expression networks. Extensive simulation studies show that\nthe proposed method significantly outperforms the existing methods in terms of\nrobustness, efficiency, and computational efficiency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21333333333333335,
          "p": 0.367816091954023,
          "f": 0.2700421894461358
        },
        "rouge-2": {
          "r": 0.016216216216216217,
          "p": 0.024793388429752067,
          "f": 0.019607838355975363
        },
        "rouge-l": {
          "r": 0.20666666666666667,
          "p": 0.3563218390804598,
          "f": 0.261603370880735
        }
      }
    },
    {
      "paper_id": "math.OA.math/OA/2503.07398v1",
      "true_abstract": "We demonstrate that any full and faithful $*$-functor between approximable\ncategories of locally finite coarse spaces induces a coarse embedding between\nthe underlying spaces. Furthermore, we establish a general characterisation of\nsuch $*$-functors between approximable categories and prove that the functor\nassociating each locally finite coarse space with its approximable category is\nfull and faithful.",
      "generated_abstract": "the Rigidity of Roe-like algebras of coarse spaces using the\ncategory theory of monoidal categories. We consider a monoidal category\n$\\mathcal{C}$ with a coarse action of a monoidal monoid $\\mathcal{M}$ on\n$\\mathcal{C}$ and a natural action of $\\mathcal{M}$ on $\\mathcal{C}^\\mathcal{M}$\ninduced by the coarse action. A coarsely $G$-equivariant functor $F:\n\\mathcal{C} \\to \\mathcal{D}$ from a monoidal category $\\mathcal{C}$ to a\nmonoidal category $\\mathcal{D}$ is called $G$-rigid if it is $G$-equivariantly\n$F$-rigid. We prove that a $G$-rigid $F$-equivalence between monoidal categories\n$\\mathcal{C}$ and $\\mathcal{D",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.40540540540540543,
          "p": 0.30612244897959184,
          "f": 0.3488372043996756
        },
        "rouge-2": {
          "r": 0.0625,
          "p": 0.04225352112676056,
          "f": 0.050420163254007944
        },
        "rouge-l": {
          "r": 0.35135135135135137,
          "p": 0.2653061224489796,
          "f": 0.3023255764926988
        }
      }
    },
    {
      "paper_id": "cs.CY.cs/CY/2503.09276v1",
      "true_abstract": "Effective lesson planning is crucial in education process, serving as the\ncornerstone for high-quality teaching and the cultivation of a conducive\nlearning atmosphere. This study investigates how large language models (LLMs)\ncan enhance teacher preparation by incorporating them with Gagne's Nine Events\nof Instruction, especially in the field of mathematics education in compulsory\neducation. It investigates two distinct methodologies: the development of Chain\nof Thought (CoT) prompts to direct LLMs in generating content that aligns with\ninstructional events, and the application of fine-tuning approaches like\nLow-Rank Adaptation (LoRA) to enhance model performance. This research starts\nwith creating a comprehensive dataset based on math curriculum standards and\nGagne's instructional events. The first method involves crafting CoT-optimized\nprompts to generate detailed, logically coherent responses from LLMs, improving\ntheir ability to create educationally relevant content. The second method uses\nspecialized datasets to fine-tune open-source models, enhancing their\neducational content generation and analysis capabilities. This study\ncontributes to the evolving dialogue on the integration of AI in education,\nillustrating innovative strategies for leveraging LLMs to bolster teaching and\nlearning processes.",
      "generated_abstract": "The paper introduces a novel framework for enhancing the effectiveness of\nlearning outcomes through the use of Large Language Models (LLMs). The\nframework leverages the Gagne's Nine Events of Instruction to integrate a\nhuman-in-the-loop approach into the LLM. This framework enables the LLM to\nlearn how to provide support for students through the application of the nine\nevents of instruction, which are identified in the literature as the foundation\nfor effective learning. The framework integrates the LLM with the nine events of\ninstruction by integrating it into the learning management system (LMS).\nLeveraging the nine events of instruction allows the LLM to identify the\ncriteria for supporting students and tailor its response accordingly. This\nframework provides a scalable and effective solution to improve student\nengagement and academic performance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1732283464566929,
          "p": 0.2894736842105263,
          "f": 0.2167487637884929
        },
        "rouge-2": {
          "r": 0.04093567251461988,
          "p": 0.0625,
          "f": 0.04946995988163215
        },
        "rouge-l": {
          "r": 0.14960629921259844,
          "p": 0.25,
          "f": 0.1871921135421875
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2503.05128v1",
      "true_abstract": "We developed a theory showing that under appropriate normalizations and\nrescalings, temperature response curves show a remarkably regular behavior and\nfollow a general, universal law. The impressive universality of temperature\nresponse curves remained hidden due to various curve-fitting models not\nwell-grounded in first principles. In addition, this framework has the\npotential to explain the origin of different scaling relationships in thermal\nperformance in biology, from molecules to ecosystems. Here, we summarize the\nbackground, principles and assumptions, predictions, implications, and possible\nextensions of this theory.",
      "generated_abstract": "p a general framework for understanding the scaling and universality\nof thermal responses in biological systems, focusing on the following key\nquestions: What is the general form of the response function $g(T)$? Is\n$g(T)$ independent of the underlying thermodynamic system? Is $g(T)$ a\nuniversal function of the system parameters? To address these questions, we\nintroduce a concept of thermodynamic scaling and show that $g(T)$ can be\ncharacterized as a universal function of the underlying thermodynamic system.\nWe further examine the universality of $g(T)$ by examining the scaling and\nuniversality of its first and second derivatives. To illustrate our framework,\nwe consider a thermodynamic model for heat transfer in biological tissues. The\nmodel exhibits a universal behavior of $g(T)$, and we show that the\ntemperature-dependent response function satisfies a simple power",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27692307692307694,
          "p": 0.24324324324324326,
          "f": 0.2589928007763574
        },
        "rouge-2": {
          "r": 0.012345679012345678,
          "p": 0.00909090909090909,
          "f": 0.010471199303749434
        },
        "rouge-l": {
          "r": 0.24615384615384617,
          "p": 0.21621621621621623,
          "f": 0.23021582235909124
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2502.14012v1",
      "true_abstract": "Considering that the physical design of printed circuit board (PCB) follows\nthe principle of modularized design, this paper proposes an automatic placement\nalgorithm for functional modules. We first model the placement problem as a\nmixed-variable optimization problem, and then, developed tailored algorithms of\nglobal placement and legalization for the top-layer centralized placement\nsubproblem and the bottom-layer pin-oriented placement subproblem. Numerical\ncomparison demonstrates that the proposed mixed-variable optimization scheme\ncan get optimized total wirelength of placement. Meanwhile, experimental\nresults on several industrial PCB cases show that the developed centralized\nstrategies can well accommodate the requirement of top-layer placement, and the\npin-oriented global placement based on bin clustering contributes to optimized\nplacement results meeting the requirement of pin-oriented design.",
      "generated_abstract": "d circuit-based modules are increasingly used in various applications\nbecause of their compact size and low power consumption. The current\nplacement method for printed circuit board (PCB) is based on the single-layer\nplacement algorithm, which is inefficient in terms of the amount of\ncomputation and memory required. To address these issues, we propose a\ndouble-layer placement method for integrated circuit-based modules on PCBs. In\nthis method, the first layer is used to place the integrated circuit (IC)\nmodules, while the second layer is used to place the printed circuit board\n(PCB) to facilitate the connection of the modules. To reduce the amount of\ncomputation and memory required, the double-layer placement method uses a\nfractional knapsack algorithm and a heuristic search method to efficiently\nplace the ICs. The proposed method is validated by analyzing two example\nmodules: a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24,
          "p": 0.225,
          "f": 0.23225805952133208
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.034482758620689655,
          "f": 0.03571428072066396
        },
        "rouge-l": {
          "r": 0.22666666666666666,
          "p": 0.2125,
          "f": 0.21935483371488043
        }
      }
    },
    {
      "paper_id": "cs.CY.econ/GN/2503.05754v1",
      "true_abstract": "The air transportation local share, defined as the proportion of local\npassengers relative to total passengers, serves as a critical metric reflecting\nhow economic growth, carrier strategies, and market forces jointly influence\ndemand composition. This metric is particularly useful for examining industry\nstructure changes and large-scale disruptive events such as the COVID-19\npandemic. This research offers an in-depth analysis of local share patterns on\nmore than 3900 Origin and Destination (O&D) pairs across the U.S. air\ntransportation system, revealing how economic expansion, the emergence of\nlow-cost carriers (LCCs), and strategic shifts by legacy carriers have\ncollectively elevated local share. To efficiently identify the local share\ncharacteristics of thousands of O&Ds and to categorize the O&Ds that have the\nsame behavior, a range of time series clustering methods were used. Evaluation\nusing visualization, performance metrics, and case-based examination\nhighlighted distinct patterns and trends, from magnitude-based stratification\nto trend-based groupings. The analysis also identified pattern commonalities\nwithin O&D pairs, suggesting that macro-level forces (e.g., economic cycles,\nchanging demographics, or disruptions such as COVID-19) can synchronize changes\nbetween disparate markets. These insights set the stage for predictive modeling\nof local share, guiding airline network planning and infrastructure\ninvestments. This study combines quantitative analysis with flexible clustering\nto help stakeholders anticipate market shifts, optimize resource allocation\nstrategies, and strengthen the air transportation system's resilience and\ncompetitiveness.",
      "generated_abstract": "y examines the passenger sharing patterns of air transportation\n(i.e., domestic and international) in China, with a focus on the evolution of\nthe local passenger share and the emergence of the transfer passenger share. The\nstudy employs a longitudinal panel data approach to analyse the passenger\nsharing patterns from 2008 to 2020. The results reveal that the local passenger\nshare gradually increased from 39.2% to 45.1% during the study period, while\nthe transfer passenger share increased from 3.8% to 7.2%. The analysis also\nshows that the local passenger share increased by 1.8 percentage points, while\nthe transfer passenger share increased by 1.7 percentage points, and the two\nshares remained stable over the study period. The study also found that the\nlocal passenger share experienced a steady increase in the 200",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12962962962962962,
          "p": 0.2916666666666667,
          "f": 0.17948717522682459
        },
        "rouge-2": {
          "r": 0.028169014084507043,
          "p": 0.05714285714285714,
          "f": 0.03773584463332199
        },
        "rouge-l": {
          "r": 0.10493827160493827,
          "p": 0.2361111111111111,
          "f": 0.1452991410387904
        }
      }
    },
    {
      "paper_id": "cs.PF.cs/PF/2503.09650v1",
      "true_abstract": "With the advancement of Large Language Models (LLMs), the importance of\naccelerators that efficiently process LLM computations has been increasing.\nThis paper discusses the necessity of LLM accelerators and provides a\ncomprehensive analysis of the hardware and software characteristics of the main\ncommercial LLM accelerators. Based on this analysis, we propose considerations\nfor the development of next-generation LLM accelerators and suggest future\nresearch directions.",
      "generated_abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nrevolutionized various applications, such as language translation and\nspeech recognition. However, the rapid growth of LLMs has caused significant\neconomic and environmental burdens, including the use of massive amounts of\ncomputational resources. This paper reviews the current state of LLMs and their\nproprietary accelerators, including the design and implementation of\nproprietary accelerators. This review also discusses the advantages and\nlimitations of proprietary accelerators, including the high energy consumption\nand the lack of open-source models. Finally, we provide insights into the\npromise and challenges of LLMs and the potential of accelerating LLMs with\nproprietary accelerators.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2608695652173913,
          "p": 0.17391304347826086,
          "f": 0.20869564737391313
        },
        "rouge-2": {
          "r": 0.06779661016949153,
          "p": 0.043478260869565216,
          "f": 0.052980127689136874
        },
        "rouge-l": {
          "r": 0.2391304347826087,
          "p": 0.15942028985507245,
          "f": 0.19130434302608706
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.07231v1",
      "true_abstract": "In today's globalised trade, supply chains form complex networks spanning\nmultiple organisations and even countries, making them highly vulnerable to\ndisruptions. These vulnerabilities, highlighted by recent global crises,\nunderscore the urgent need for improved visibility and resilience of the supply\nchain. However, data-sharing limitations often hinder the achievement of\ncomprehensive visibility between organisations or countries due to privacy,\nsecurity, and regulatory concerns. Moreover, most existing research studies\nfocused on individual firm- or product-level networks, overlooking the\nmultifaceted interactions among diverse entities that characterise real-world\nsupply chains, thus limiting a holistic understanding of supply chain dynamics.\nTo address these challenges, we propose a novel approach that integrates\nFederated Learning (FL) and Graph Convolutional Neural Networks (GCNs) to\nenhance supply chain visibility through relationship prediction in supply chain\nknowledge graphs. FL enables collaborative model training across countries by\nfacilitating information sharing without requiring raw data exchange, ensuring\ncompliance with privacy regulations and maintaining data security. GCNs empower\nthe framework to capture intricate relational patterns within knowledge graphs,\nenabling accurate link prediction to uncover hidden connections and provide\ncomprehensive insights into supply chain networks. Experimental results\nvalidate the effectiveness of the proposed approach, demonstrating its ability\nto accurately predict relationships within country-level supply chain knowledge\ngraphs. This enhanced visibility supports actionable insights, facilitates\nproactive risk management, and contributes to the development of resilient and\nadaptive supply chain strategies, ensuring that supply chains are better\nequipped to navigate the complexities of the global economy.",
      "generated_abstract": "-19 pandemic has significantly impacted supply chain operations\nthrough increased demand for essential goods. As a result, visibility of\nsupply chains has become a key concern, especially for retailers and manufacturers\nwho rely on third-party logistics (3PL) providers to transport goods. This paper\npresents an analysis of the effects of COVID-19 on the global supply chain\neconomy, with a particular focus on how the pandemic has disrupted the 3PL\nindustry. We also discuss the challenges that retailers and manufacturers face\nin improving visibility of their supply chains, particularly in light of the\nincreasing complexity and fragmentation of the supply chain. To address these\nchallenges, we propose an analytics-driven approach to enhance supply chain\nvisibility through graph neural networks (GNNs) and federated learning (FL).\nOur approach",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1590909090909091,
          "p": 0.32941176470588235,
          "f": 0.21455938258099566
        },
        "rouge-2": {
          "r": 0.07017543859649122,
          "p": 0.1391304347826087,
          "f": 0.09329445618407317
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.2823529411764706,
          "f": 0.1839080415848271
        }
      }
    },
    {
      "paper_id": "cs.NI.cs/NI/2503.08123v1",
      "true_abstract": "With the advent of 6G systems, emerging hyper-connected ecosystems\nnecessitate agile and adaptive medium access control (MAC) protocols to contend\nwith network dynamics and diverse service requirements. We propose LLM4MAC, a\nnovel framework that harnesses large language models (LLMs) within a\nreinforcement learning paradigm to drive MAC protocol emergence. By\nreformulating uplink data transmission scheduling as a semantics-generalized\npartially observable Markov game (POMG), LLM4MAC encodes network operations in\nnatural language, while proximal policy optimization (PPO) ensures continuous\nalignment with the evolving network dynamics. A structured identity embedding\n(SIE) mechanism further enables robust coordination among heterogeneous agents.\nExtensive simulations demonstrate that on top of a compact LLM, which is\npurposefully selected to balance performance with resource efficiency, the\nprotocol emerging from LLM4MAC outperforms comparative baselines in throughput\nand generalization.",
      "generated_abstract": "In this work, we propose a reinforcement learning framework for the MAC\nprotocol emergence in wireless networks. Specifically, we leverage LLMs to\nrepresent MAC protocols, and integrate them into the reinforcement learning\nframework. We design two types of LLMs: the first one is a generic LLM, which\nis used to represent a wide range of MAC protocols, while the second one is a\nspecific LLM, designed for each MAC protocol. Based on these LLMs, we develop a\nreinforcement learning algorithm, which is capable of selecting the appropriate\nLLM for each MAC protocol. We evaluate our framework through a series of\nexperiments. The results show that our framework not only can effectively\nselect the appropriate LLM for each MAC protocol, but also can effectively\nsupport the convergence and convergence of the learning process.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18518518518518517,
          "p": 0.2597402597402597,
          "f": 0.21621621135661076
        },
        "rouge-2": {
          "r": 0.047619047619047616,
          "p": 0.05405405405405406,
          "f": 0.0506329064124344
        },
        "rouge-l": {
          "r": 0.18518518518518517,
          "p": 0.2597402597402597,
          "f": 0.21621621135661076
        }
      }
    },
    {
      "paper_id": "math.AG.math/AG/2503.09195v1",
      "true_abstract": "Refined algebraic domains are regions in the plane surrounded by finitely\nmany non-singular real algebraic curves which may intersect with normal\ncrossing. We are interested in shapes of such regions with surrounding real\nalgebraic curves. Poincar'e-Reeb Graphs of them are graphs the regions\nnaturally collapse to respecting the projection to a straight line. Such graphs\nwere first formulated by Sorea, for example, around 2020, and regions\nsurrounded by mutually disjoint non-singular real algebraic curves were mainly\nconsidered. The author has generalized the studies to several general\nsituations.\n  We find classes of such objects defined inductively by adding curves. We\nrespect characteristic finite sets in the curves. We consider regions\nsurrounded by the curves and of a new type. We investigate geometric properties\nand combinatorial ones of them and discuss important examples. We also\npreviously studied explicit classes defined inductively in this way and review\nthem.",
      "generated_abstract": "hbb{K}$ be a field, and let $X$ be a regular scheme over $\\mathbb{K}$. We\nconstruct a family of algebraic domains $A_X$ over $\\mathbb{K}$ such that for any\nalgebraic space $Y$ over $\\mathbb{K}$, $Y\\times_X A_X$ is a regular scheme\nover $\\mathbb{K}$ which is the disjoint union of $Y$ and the algebraic\ndomain $A_Y$ over $\\mathbb{K}$. We prove that if $X$ is a regular scheme and\n$A$ is a regular scheme over $\\mathbb{K}$, then $A\\times_X X$ is a regular\nscheme over $\\mathbb{K}$. We also prove that if $X$ is a regular scheme, $Y$ is\na regular scheme, and $f:Y\\to X$ is an immersion, then $Y$ is a regular scheme",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1348314606741573,
          "p": 0.26666666666666666,
          "f": 0.17910447315103598
        },
        "rouge-2": {
          "r": 0.015384615384615385,
          "p": 0.02857142857142857,
          "f": 0.019999995450001036
        },
        "rouge-l": {
          "r": 0.1348314606741573,
          "p": 0.26666666666666666,
          "f": 0.17910447315103598
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/CB/2501.08714v1",
      "true_abstract": "Neuroblastoma, is a highly heterogeneous pediatric tumour and is responsible\nfor 15% of pediatric cancer-related deaths. The clinical outcomes can vary from\nspontaneous regression to high metastatic disease. This extracranial tumour\narises from a neural crest-derived cell and can harbor different phenotypes.\nIts heterogeneity may result from variations in differentiation states\ninfluenced by genetic and epigenetic factors and individual patient\ncharacteristics. This leads downstream to disruption of homeostasis and a\nmetabolic shift in response to the tumour needs. Nutrition can play a key role\nin influencing various aspects of a tumour behaviour. This review provides an\nin-depth exploration of the aetiology of neuroblastoma and the different\navenues of disease progression, which can be targeted with individualized\nnutrition intervention strategies to improve the well-being of children and\noptimize clinical outcomes.",
      "generated_abstract": "toma is a malignant cancer of the sympathetic nervous system (SNS),\nappearing as a round or oval tumor of the head and neck region, with\nextra-nodal metastasis. The disease is often fatal and is associated with\nhigh mortality rates. Despite advancements in chemotherapy, the prognosis\nremains poor. We review the pathophysiology and clinical manifestations of\nneuroblastoma, focusing on the SNS and its role in the disease. We discuss\nnutritional strategies for supporting the growth and development of the\nSNS, as well as the role of nutrition in oncology, with a focus on neuroblastoma.\nWe conclude that nutritional support is an important component of\noncological treatment, and clinicians must consider nutritional interventions\nwhen treating pediatric oncology",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21348314606741572,
          "p": 0.24050632911392406,
          "f": 0.2261904712081917
        },
        "rouge-2": {
          "r": 0.031496062992125984,
          "p": 0.03571428571428571,
          "f": 0.03347279836697611
        },
        "rouge-l": {
          "r": 0.15730337078651685,
          "p": 0.17721518987341772,
          "f": 0.16666666168438224
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.10062v1",
      "true_abstract": "This paper addresses the one-bit consensus of controllable linear multi-agent\nsystems (MASs) with communication noises. A consensus algorithm consisting of a\ncommunication protocol and a consensus controller is designed. The\ncommunication protocol introduces a linear compression encoding function to\nachieve a one-bit data rate, thereby saving communication costs. The consensus\ncontroller with a stabilization term and a consensus term is proposed to ensure\nthe consensus of a potentially unstable but controllable MAS. Specifically, in\nthe consensus term, we adopt an estimation method to overcome the information\nloss caused by one-bit communications and a decay step to attenuate the effect\nof communication noise. Two combined Lyapunov functions are constructed to\novercome the difficulty arising from the coupling of the control and\nestimation. By establishing similar iterative structures of these two\nfunctions, this paper shows that the MAS can achieve consensus in the mean\nsquare sense at the rate of the reciprocal of the iteration number under the\ncase with a connected fixed topology. Moreover, the theoretical results are\ngeneralized to the case with jointly connected Markovian switching topologies\nby establishing a certain equivalence relationship between the Markovian\nswitching topologies and a fixed topology. Two simulation examples are given to\nvalidate the algorithm.",
      "generated_abstract": "r introduces a novel approach to one-bit consensus of controllable\nlinear multi-agent systems with communication noises. The proposed method\nenables the agent controllers to obtain the consensus states with only one\nbit of information, which is significantly smaller than the conventional\ntwo-bit consensus. Furthermore, we show that the proposed method can achieve\nconsensus with the same communication cost as the conventional two-bit consensus\nin the case of small communication noise, which demonstrates that the proposed\nmethod achieves the consensus performance with the same communication cost as\nthe conventional two-bit consensus in the case of large communication noise.\nAdditionally, we show that the proposed method can achieve consensus with the\nsame communication cost as the conventional two-bit consensus in the case of\nlarge communication noise, which demonstrates that the proposed method achieves\nthe consensus performance with the same communication cost as the conventional\ntwo-bit cons",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21739130434782608,
          "p": 0.44642857142857145,
          "f": 0.2923976564139394
        },
        "rouge-2": {
          "r": 0.09444444444444444,
          "p": 0.21794871794871795,
          "f": 0.13178294151793776
        },
        "rouge-l": {
          "r": 0.20869565217391303,
          "p": 0.42857142857142855,
          "f": 0.28070174998119085
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.14447v1",
      "true_abstract": "This paper introduces the two-way common causal covariates (CCC) assumption,\nwhich is necessary to get an unbiased estimate of the ATT when using\ntime-varying covariates in existing Difference-in-Differences methods. The\ntwo-way CCC assumption implies that the effect of the covariates remain the\nsame between groups and across time periods. This assumption has been implied\nin previous literature, but has not been explicitly addressed. Through\ntheoretical proofs and a Monte Carlo simulation study, we show that the\nstandard TWFE and the CS-DID estimators are biased when the two-way CCC\nassumption is violated. We propose a new estimator called the Intersection\nDifference-in-differences (DID-INT) which can provide an unbiased estimate of\nthe ATT under two-way CCC violations. DID-INT can also identify the ATT under\nheterogeneous treatment effects and with staggered treatment rollout. The\nestimator relies on parallel trends of the residuals of the outcome variable,\nafter appropriately adjusting for covariates. This covariate residualization\ncan recover parallel trends that are hidden with conventional estimators.",
      "generated_abstract": "We introduce a novel method for difference-in-differences (DiD) analysis\nwith covariates, where the treatment and covariate variables are not\nidentified. We show that the method is equivalent to an instrumental variable\n(IV) estimator, and propose a modification of the standard IV estimator for\ncovariates. We derive the asymptotic properties of the proposed estimator, and\ndemonstrate its efficiency using a simulation study and empirical application to\nthe Nassau Property Tax Collection.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21153846153846154,
          "p": 0.41509433962264153,
          "f": 0.28025477259767134
        },
        "rouge-2": {
          "r": 0.04195804195804196,
          "p": 0.08955223880597014,
          "f": 0.057142852797732756
        },
        "rouge-l": {
          "r": 0.17307692307692307,
          "p": 0.33962264150943394,
          "f": 0.22929935858493253
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2502.16246v1",
      "true_abstract": "Understanding the impact of trades on prices is a crucial question for both\nacademic research and industry practice. It is well established that impact\nfollows a square-root impact as a function of traded volume. However, the\nmicroscopic origin of such a law remains elusive: empirical studies are\nparticularly challenging due to the anonymity of orders in public data. Indeed,\nthere is ongoing debate about whether price impact has a mechanical origin or\nwhether it is primarily driven by information, as suggested by many economic\ntheories. In this paper, we revisit this question using a very detailed dataset\nprovided by the Japanese stock exchange, containing the trader IDs for all\norders sent to the exchange between 2012 and 2018. Our central result is that\nsuch a law has in fact microscopic roots and applies already at the level of\nsingle child orders, provided one waits long enough for the market to \"digest\"\nthem. The mesoscopic impact of metaorders arises from a \"double\" square-root\neffect: square-root in volume of individual impact, followed by an inverse\nsquare-root decay as a function of time. Since market orders are anonymous, we\nexpect and indeed find that these results apply to any market orders, and the\nimpact of synthetic metaorders, reconstructed by scrambling the identity of the\nissuers, is described by the very same square-root impact law. We conclude that\nprice impact is essentially mechanical, at odds with theories that emphasize\nthe information content of such trades to explain the square-root impact law.",
      "generated_abstract": "The double square root law (DSR) is a well-known phenomenon in financial\nmarket research, which states that the return of an asset is twice as large as\nits historical volatility. This paper presents a novel methodology to test the\nDSR, employing a set of Tokyo Stock Exchange (TSE) daily returns from 2000 to\n2019. Our findings suggest that the DSR is indeed a valid phenomenon in the\nTSE market, with returns being twice as large as the historical volatility,\nespecially during periods of market stress. The results also indicate that the\nDSR can be applied to other asset classes, such as foreign exchange rates and\ncryptoassets. These findings highlight the potential of this methodology in\nfostering a deeper understanding of financial markets, offering valuable insights\ninto the mechanisms underlying market dynamics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14189189189189189,
          "p": 0.23076923076923078,
          "f": 0.17573221285761817
        },
        "rouge-2": {
          "r": 0.004329004329004329,
          "p": 0.008130081300813009,
          "f": 0.0056497129795114445
        },
        "rouge-l": {
          "r": 0.12162162162162163,
          "p": 0.1978021978021978,
          "f": 0.15062761034715796
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/CB/2408.05119v1",
      "true_abstract": "Stress generation by the actin cytoskeleton shapes cells and tissues. Despite\nimpressive progress in live imaging and quantitative physical descriptions of\ncytoskeletal network dynamics, the connection between processes at molecular\nscales and cell-scale spatio-temporal patterns is still unclear. Here we review\nstudies reporting acto-myosin clusters of micrometer size and with lifetimes of\nseveral minutes in a large number of organisms ranging from fission yeast to\nhumans. Such structures have also been found in reconstituted systems in vitro\nand in theoretical analysis of cytoskeletal dynamics. We propose that tracking\nthese clusters can serve as a simple readout for characterising living matter.\nSpatio-temporal patterns of clusters could serve as determinants of\nmorphogenetic processes that play similar roles in diverse organisms.",
      "generated_abstract": "n networks are at the core of the structural organization of\nmany biological systems. They are found in diverse biological systems, from\nneurons to animal cells to the cytoskeleton of bacteria. The actomyosin\nnetworks are responsible for the physical properties of living matter, such as\ntubular and solid structures, and for the cytoskeletal organization, which\nregulates cell shape and motility. The actomyosin networks are also involved in\ncell death, where they trigger apoptosis, which is a programmed cell death\nmechanism. The actomyosin network is also found in the cytoskeleton of bacteria,\nwhere it is involved in their motility. This review will focus on the\nstructural organization of actomyosin networks and the physical properties of\nthe actomyosin network. We will discuss how the actomyos",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.32857142857142857,
          "f": 0.28395061237616226
        },
        "rouge-2": {
          "r": 0.017391304347826087,
          "p": 0.019417475728155338,
          "f": 0.018348618868362598
        },
        "rouge-l": {
          "r": 0.21739130434782608,
          "p": 0.2857142857142857,
          "f": 0.2469135753391252
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.21181v1",
      "true_abstract": "It's not unreasonable to think that in-game sporting performance can be\naffected partly by what takes place off the court. We can't observe what\nhappens between games directly. Instead, we proxy for the possibility of\nathletes partying by looking at play following games in party cities. We are\ninterested to see if teams exhibit a decline in performance the day following a\ngame in a city with active nightlife; we call this a \"hangover effect\". Part of\nthe question is determining a reasonable way to measure levels of nightlife,\nand correspondingly which cities are notorious for it; we colloquially refer to\nsuch cities as \"party cities\". To carry out this study, we exploit data on\nbookmaker spreads: the expected score differential between two teams after\nconditioning on observable performance in past games and expectations about the\nupcoming game. We expect a team to meet the spread half the time, since this is\none of the easiest ways for bookmakers to guarantee a profit. We construct a\nmodel which attempts to estimate the causal effect of visiting a \"party city\"\non subsequent day performance as measured by the odds of beating the spread. In\nparticular, we only consider the hangover effect on games played back-to-back\nwithin 24 hours of each other. To the extent that odds of beating the spread\nagainst next day opponent is uncorrelated with playing in a party city the day\nbefore, which should be the case under an efficient betting market, we have\nidentification in our variable of interest. We find that visiting a city with\nactive nightlife the day prior to a game does have a statistically significant\nnegative effect on a team's likelihood of meeting bookmakers' expectations for\nboth NBA and MLB.",
      "generated_abstract": "This paper examines the causal hangover effect in a two-stage least squares\napproach. Specifically, we demonstrate that the hangover effect is a type of\nmarginal effect, in the sense that the change in a dependent variable resulting\nfrom a change in a treatment is directly proportional to the change in the\ntreatment. Furthermore, we derive a simple expression for the hangover effect\nin terms of the sample mean of the dependent variable. In contrast to the\nclassical hangover effect, which is a change in the mean of the dependent\nvariable, the hangover effect is a change in the mean of the dependent variable\nplus a change in the mean of the treatment. The hangover effect is also\nillustrated using a simple counterfactual model. Finally, we illustrate the\ncausal hangover effect using an application in the insurance industry.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10588235294117647,
          "p": 0.3103448275862069,
          "f": 0.15789473304863044
        },
        "rouge-2": {
          "r": 0.02214022140221402,
          "p": 0.06741573033707865,
          "f": 0.033333329611265845
        },
        "rouge-l": {
          "r": 0.09411764705882353,
          "p": 0.27586206896551724,
          "f": 0.14035087339950764
        }
      }
    },
    {
      "paper_id": "cs.CY.cs/CY/2503.10458v1",
      "true_abstract": "Social media platforms have been accused of causing a range of harms,\nresulting in dozens of lawsuits across jurisdictions. These lawsuits are\nsituated within the context of a long history of American product safety\nlitigation, suggesting opportunities for remediation outside of financial\ncompensation. Anticipating that at least some of these cases may be successful\nand/or lead to settlements, this article outlines an implementable mechanism\nfor an abatement and/or settlement plan capable of mitigating abuse. The paper\ndescribes the requirements of such a mechanism, implications for privacy and\noversight, and tradeoffs that such a procedure would entail. The mechanism is\nframed to operate at the intersection of legal procedure, standards for\ntransparent public health assessment, and the practical requirements of modern\ntechnology products.",
      "generated_abstract": "dia platforms are increasingly used to disseminate harmful\ninformation and engage in harassment, and the consequences of this abuse are\nalready being felt in many communities. However, assessing the magnitude of the\nproblem is challenging, as the underlying causes and the social impact of this\nabuse are often opaque. This paper introduces a framework for assessing the\nimpact of harmful social media abuse on public health, based on a novel\nframework for analyzing the harmful effects of social media platforms. This\nframework combines two distinct concepts: (1) the harmful effects of social\nmedia platforms on public health, and (2) the harmful effects of the\nabuse-prone behaviors of social media users. We argue that both of these\nconcepts are necessary for assessing the impact of harmful social media abuse\non public health, and we provide a detailed analysis of each concept",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18681318681318682,
          "p": 0.2361111111111111,
          "f": 0.20858895212315118
        },
        "rouge-2": {
          "r": 0.025210084033613446,
          "p": 0.02857142857142857,
          "f": 0.026785709305246458
        },
        "rouge-l": {
          "r": 0.16483516483516483,
          "p": 0.20833333333333334,
          "f": 0.18404907482253768
        }
      }
    },
    {
      "paper_id": "math.QA.math/QA/2503.05553v1",
      "true_abstract": "For a simple, self-dual, strong CFT-type vertex operator algebra (VOA) of\ncentral charge $c$, we describe the Virasoro $n$-point correlation function on\na genus $g$ marked Riemann surface in the Schottky uniformisation. We show that\nthis $n$-point function determines the correlation functions for all Virasoro\nvacuum descendants. Using our recent work on genus $g$ Zhu recursion, we show\nthat the Virasoro $n$-point function is determined by a differential operator\n$\\mathcal{D}_{n}$ acting on the genus $g$ VOA partition function normalised by\nthe Heisenberg partition function to the power of $c$. We express\n$\\mathcal{D}_{n}$ as the sum of weights over certain Virasoro graphs where the\nweights explicitly depend on $c$, the classical bidifferential of the second\nkind, the projective connection, holomorphic 1-forms and derivatives with\nrespect to any $3g-3$ locally independent period matrix elements. We also\ndescribe the modular properties of $\\mathcal{D}_{n}$ under a homology base\nchange.",
      "generated_abstract": "In this work, we compute the genus $g$ correlators of the vertex\noperators associated to Virasoro characters in the topological\n$W_q$-vertex operator algebra. We show that the genus $g$ correlators are\ndefined by a finite sum of products of generalized Macdonald polynomials,\nwhich we compute explicitly in the case of the Virasoro characters associated\nto the Virasoro characters of the $q$-deformed affine algebras of type $A$,\n$D$ and $E$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21052631578947367,
          "p": 0.4444444444444444,
          "f": 0.28571428135204086
        },
        "rouge-2": {
          "r": 0.08823529411764706,
          "p": 0.21052631578947367,
          "f": 0.12435232744395835
        },
        "rouge-l": {
          "r": 0.17894736842105263,
          "p": 0.37777777777777777,
          "f": 0.24285713849489804
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.cond-mat/mes-hall/2503.09970v1",
      "true_abstract": "We introduce new classes of gapped topological phases characterized by\nquantized crystalline-electromagnetic responses, termed \"multipolar Chern\ninsulators\". These systems are characterized by nonsymmorphic momentum-space\nsymmetries and mirror symmetries, leading to quantization of momentum-weighted\nBerry curvature multipole moments. We construct lattice models for such phases\nand confirm their quantized responses through numerical calculations. These\nsystems exhibit bound charge and momentum densities at lattice and magnetic\ndefects, and currents induced by electric or time-varying strain fields. Our\nwork extends the classification of topological matter by uncovering novel\nsymmetry-protected topological phases with quantized responses.",
      "generated_abstract": "aper, we investigate the quantized response of an insulator with\nspontaneous magnetism to a weak electric field, which is tuned by the\nmagnetization. We find that the magnetization can be tuned to the quantized\nelectric-field-induced response (EFIR) peak, which is the crystalline-electromagnetic\n(CEM) response. We show that the EFIR peak can be tuned to a position\ncorresponding to the quantized magnetization, which is related to the spin\npolarization. Furthermore, we investigate the magnetic-field-induced EFIR\nresponse, which is a field-induced response of the magnetic moment. We show that\nthe EFIR peak can be tuned to a position corresponding to the quantized\nmagnetization, which is related to the magnetic-field-induced EFIR response.\nThese results not",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15714285714285714,
          "p": 0.22,
          "f": 0.18333332847222236
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.15714285714285714,
          "p": 0.22,
          "f": 0.18333332847222236
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.15761v1",
      "true_abstract": "We propose a factor model and an estimator of the factors and loadings that\nare robust to weak factors. The factors can have an arbitrarily weak influence\non the mean or quantile of the outcome variable at most quantile levels; each\nfactor only needs to have a strong impact on the outcome's quantile near one\nunknown quantile level. The estimator for every factor, loading, and common\ncomponent is asymptotically normal at the $\\sqrt{N}$ or $\\sqrt{T}$ rate. It\ndoes not require the knowledge of whether the factors are weak and how weak\nthey are. We also develop a weak-factor-robust estimator of the number of\nfactors and a consistent selectors of factors of any desired strength of\ninfluence on the quantile or mean of the outcome variable. Monte Carlo\nsimulations demonstrate the effectiveness of our methods.",
      "generated_abstract": "e a robust quantile factor analysis framework for heteroscedastic\nquantile regression models with bounded-linkage quantile functions. Our approach\nestimates the factor loadings with a penalized quantile regression model,\nwith a new penalized quantile modeling approach for quantile functions. The\nframework includes robust estimation of the factors' covariance matrices,\nrobust estimation of the factors' covariance matrices, robust estimation of\nthe factors' covariance matrices, and robust estimation of the factors'\ncovariance matrices. Additionally, we propose an efficient method for\nestimating the factors' covariance matrices, which improves on the existing\nmethods. The estimators are well-conditioned and have consistent and\nasymptotically normal estimators. We apply the proposed method to a set of\nreal-world quantile regression models. The results indicate that the proposed\nmethod performs well in terms of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2857142857142857,
          "p": 0.34375,
          "f": 0.3120567326311554
        },
        "rouge-2": {
          "r": 0.024793388429752067,
          "p": 0.031914893617021274,
          "f": 0.027906971823040347
        },
        "rouge-l": {
          "r": 0.2857142857142857,
          "p": 0.34375,
          "f": 0.3120567326311554
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/LG/2503.10635v1",
      "true_abstract": "Despite promising performance on open-source large vision-language models\n(LVLMs), transfer-based targeted attacks often fail against black-box\ncommercial LVLMs. Analyzing failed adversarial perturbations reveals that the\nlearned perturbations typically originate from a uniform distribution and lack\nclear semantic details, resulting in unintended responses. This critical\nabsence of semantic information leads commercial LVLMs to either ignore the\nperturbation entirely or misinterpret its embedded semantics, thereby causing\nthe attack to fail. To overcome these issues, we notice that identifying core\nsemantic objects is a key objective for models trained with various datasets\nand methodologies. This insight motivates our approach that refines semantic\nclarity by encoding explicit semantic details within local regions, thus\nensuring interoperability and capturing finer-grained features, and by\nconcentrating modifications on semantically rich areas rather than applying\nthem uniformly. To achieve this, we propose a simple yet highly effective\nsolution: at each optimization step, the adversarial image is cropped randomly\nby a controlled aspect ratio and scale, resized, and then aligned with the\ntarget image in the embedding space. Experimental results confirm our\nhypothesis. Our adversarial examples crafted with local-aggregated\nperturbations focused on crucial regions exhibit surprisingly good\ntransferability to commercial LVLMs, including GPT-4.5, GPT-4o,\nGemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning\nmodels like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach\nachieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly\noutperforming all prior state-of-the-art attack methods. Our optimized\nadversarial examples under different configurations and training code are\navailable at https://github.com/VILA-Lab/M-Attack.",
      "generated_abstract": "In this paper, we present a simple yet highly effective baseline attack for\nthe GPT-4.5/4o/o1 model. Our attack is based on the concept of \\textit{Bridge\nAttack}, and it leverages the GPT-4.5/4o/o1 model's unbounded capacity to\ngenerate text by synthesizing large sequences of tokens. Our attack has a\nsuccess rate of over 90%, and it can effectively bypass the model's\nblack-box defense mechanisms. We also provide a detailed analysis of our\nattack, including a quantitative comparison with the state-of-the-art methods.\nOur results demonstrate the practical impact of our attack, and they provide a\nbaseline for further research in adversarial text generation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15135135135135136,
          "p": 0.3835616438356164,
          "f": 0.2170542595081426
        },
        "rouge-2": {
          "r": 0.024691358024691357,
          "p": 0.06382978723404255,
          "f": 0.03560830458276512
        },
        "rouge-l": {
          "r": 0.15135135135135136,
          "p": 0.3835616438356164,
          "f": 0.2170542595081426
        }
      }
    },
    {
      "paper_id": "cs.NI.eess/SP/2503.05429v1",
      "true_abstract": "Cross-Technology Interference (CTI) poses challenges for the performance and\nrobustness of wireless networks. There are opportunities for better cooperation\nif the spectral occupation and technology of the interference can be detected.\nNamely, this information can help the Orthogonal Frequency Division Multiple\nAccess (OFDMA) scheduler in IEEE 802.11ax (Wi-Fi 6) to efficiently allocate\nresources to multiple users inthe frequency domain. This work shows that a\nsingle Channel State Information (CSI) snapshot, which is used for packet\ndemodulation in the receiver, is enough to detect and classify the type of CTI\non low-cost Wi-Fi 6 hardware. We show the classification accuracy of a small\nConvolutional Neural Network (CNN) for different Signal-to-Noise Ratio (SNR)\nand Signal-to-Interference Ratio (SIR) with simulated data, as well as using a\nwired and over-the-air test with a professional wireless connectivity tester,\nwhile running the inference on the low-cost device. Furthermore, we use\nopenwifi, a full-stack Wi-Fi transceiver running on software-defined radio\n(SDR) available in the w-iLab.t testbed, as Access Point (AP) to implement a\nCTI-aware multi-user OFDMA scheduler when the clients send CTI detection\nfeedback to the AP. We show experimentally that it can fully mitigate the 35%\nthroughput loss caused by CTI when the AP applies the appropriate scheduling.",
      "generated_abstract": "development of wireless communications and the introduction of\nthe 6th-generation wireless (6G) standards have raised concerns about the\nimpact of cross-technology interference on the performance of wireless\ncommunications. To address this issue, this study proposes a novel method for\ndetection and mitigation of cross-technology interference. The proposed method\nis based on the use of orthogonal frequency-division multiplexing (OFDM) and\ntime-division multiplexing (TDMA) techniques. The proposed method consists of\ntwo main components: (i) a cross-technology interference detection module that\ndetects cross-technology interference, and (ii) a cross-technology interference\nmitigation module that suppresses the interference. The performance of the\nproposed method is evaluated using simulation results. The results indicate that\nthe proposed method effectively detects and suppresses",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10204081632653061,
          "p": 0.25,
          "f": 0.14492753211510198
        },
        "rouge-2": {
          "r": 0.02512562814070352,
          "p": 0.05263157894736842,
          "f": 0.034013601067842666
        },
        "rouge-l": {
          "r": 0.08843537414965986,
          "p": 0.21666666666666667,
          "f": 0.1256038606175175
        }
      }
    },
    {
      "paper_id": "math.OA.math/LO/2503.10505v1",
      "true_abstract": "We compute the $K_1$-group of ultraproducts of unital, simple $C^*$-algebras\nwith unique trace and strict comparison. As an application, we prove that the\nreduced free group $C^*$-algebras $C^*_r(F_m)$ and $C^*_r(F_n)$ are\nelementarily equivalent (i.e., have isomorphic ultrapowers) if and only if $m =\nn$. This settles in the negative the $C^*$-algebraic analogue of Tarski's 1945\nproblem for groups.",
      "generated_abstract": "hat the $C^*$-algebraic Tarski problem is negative. In particular,\nfor each uncountable $C^*$-algebra $A$ the problem is negative if and only if $A$\nis not a finite-dimensional C*-algebra. In particular, the $C^*$-algebraic\nTarski problem is negative for all uncountable $C^*$-algebras.\n  The proof uses a novel type of resolution of the negative resolution problem\nfor uncountable $C^*$-algebras. This resolution is inspired by the work of\nGlenn E. Roughan and is related to the resolution of the negative resolution\nproblem for finite-dimensional C*-algebras. We show that the resolution of the\nnegative resolution problem for uncountable $C^*$-algebras is a\n$C^*$-algebraic version of the resolution of the negative resolution problem\nfor finite-dimensional C*-algebras",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2549019607843137,
          "p": 0.29545454545454547,
          "f": 0.27368420555346273
        },
        "rouge-2": {
          "r": 0.11864406779661017,
          "p": 0.0945945945945946,
          "f": 0.10526315295833594
        },
        "rouge-l": {
          "r": 0.2549019607843137,
          "p": 0.29545454545454547,
          "f": 0.27368420555346273
        }
      }
    },
    {
      "paper_id": "math.MG.math/FA/2503.07287v1",
      "true_abstract": "A functional analog of the Klain-Schneider theorem for vector-valued\nvaluations on convex functions is established, providing a classification of\ncontinuous, translation covariant, simple valuations. Under additional rotation\nequivariance assumptions, an analytic counterpart of the moment vector is\ncharacterized alongside a new epi-translation invariant valuation. The former\narises as the top-degree operator in a family of functional intrinsic moments,\nwhich are linked to functional intrinsic volumes through translations. The\nlatter represents the top-degree operator in a class of Minkowski vectors,\nwhich are introduced in this article and which lack classical counterparts on\nconvex bodies, as they vanish due to the Minkowski relations. Additional\nclassification results are obtained for homogeneous valuations of extremal\ndegrees.",
      "generated_abstract": "We give a Klain-Schneider type theorem for vector-valued valuations on\nconvex functions. This generalizes a result by Klain and Schneider, which\nappeared in a preprint form, and a result by R\\\"ussmann, which is a special case\nof our result.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1794871794871795,
          "p": 0.4666666666666667,
          "f": 0.2592592552469136
        },
        "rouge-2": {
          "r": 0.06796116504854369,
          "p": 0.19444444444444445,
          "f": 0.10071942062212115
        },
        "rouge-l": {
          "r": 0.1794871794871795,
          "p": 0.4666666666666667,
          "f": 0.2592592552469136
        }
      }
    },
    {
      "paper_id": "math.PR.econ/TH/2411.15401v4",
      "true_abstract": "Given two random variables taking values in a bounded interval, we study\nwhether one dominates the other in higher-order stochastic dominance depends on\nthe reference interval in the model setting. We obtain two results. First, the\nstochastic dominance relations get strictly stronger when the reference\ninterval shrinks if and only if the order of stochastic dominance is larger\nthan three. Second, for mean-preserving stochastic dominance relations, the\nreference interval is irrelevant if and only if the difference between the\ndegree of the stochastic dominance and the number of moments is no larger than\nthree. These results highlight complications arising from using higher-order\nstochastic dominance in economic applications.",
      "generated_abstract": "We consider a class of higher-order stochastic dominance measures, which\nare defined on a family of probability measures $\\mathcal{P}(\\mathbb{R}^n)$.\nThese measures are classified into two types, depending on whether the second\nderivative of the reference measure is non-positive or positive. We show that\nthe class of such measures admits a unique, closed and convex reference\nmeasure, which we call the reference interval. We also present a characterization\nof the set of probability measures for which the reference interval is\nwell-defined. We apply our results to several examples, including the\nrepresentation of the reference interval for the Heston model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.296875,
          "p": 0.3114754098360656,
          "f": 0.30399999500288005
        },
        "rouge-2": {
          "r": 0.06741573033707865,
          "p": 0.06896551724137931,
          "f": 0.06818181318246422
        },
        "rouge-l": {
          "r": 0.296875,
          "p": 0.3114754098360656,
          "f": 0.30399999500288005
        }
      }
    },
    {
      "paper_id": "q-fin.PM.econ/EM/2410.16333v2",
      "true_abstract": "This study examines portfolio selection using predictive models for portfolio\nreturns. Portfolio selection is a fundamental task in finance, and a variety of\nmethods have been developed to achieve this goal. For instance, the\nmean-variance approach constructs portfolios by balancing the trade-off between\nthe mean and variance of asset returns, while the quantile-based approach\noptimizes portfolios by considering tail risk. These methods often depend on\ndistributional information estimated from historical data using predictive\nmodels, each of which carries its own uncertainty. To address this, we propose\na framework for predictive portfolio selection via conformal prediction ,\ncalled \\emph{Conformal Predictive Portfolio Selection} (CPPS). Our approach\nforecasts future portfolio returns, computes the corresponding prediction\nintervals, and selects the portfolio of interest based on these intervals. The\nframework is flexible and can accommodate a wide range of predictive models,\nincluding autoregressive (AR) models, random forests, and neural networks. We\ndemonstrate the effectiveness of the CPPS framework by applying it to an AR\nmodel and validate its performance through empirical studies, showing that it\ndelivers superior returns compared to simpler strategies.",
      "generated_abstract": "We introduce a novel predictive portfolio selection method based on\nconformal prediction and risk-sensitive allocation. Our approach extends the\nconformal prediction framework by integrating a risk-sensitive allocation\nstrategy into the predictive framework. Specifically, we introduce a\ndistribution-specific risk-sensitive allocation strategy to guide the\npredictive distribution. Our method is based on a conformal predictive\ndistribution, which ensures a consistent predictive distribution across\nconfidence levels. We show that our method can improve the predictive performance\nof the conventional conformal prediction, particularly in low-confidence\nregions. Additionally, we provide a practical implementation for the\nconformal prediction method in Python.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1984126984126984,
          "p": 0.43859649122807015,
          "f": 0.27322403942667745
        },
        "rouge-2": {
          "r": 0.040697674418604654,
          "p": 0.08433734939759036,
          "f": 0.05490195639338751
        },
        "rouge-l": {
          "r": 0.18253968253968253,
          "p": 0.40350877192982454,
          "f": 0.2513661159294097
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/GA/2503.10415v1",
      "true_abstract": "This article focuses on NGC7538 IRS1, one of the most luminous and studied HC\nHII regions in the northern hemisphere. Our aim is to identify the young\nstellar objects (YSOs) embedded within the ionized gas and study their\nkinematic structures. This work expands on a recent survey called \"Protostellar\nOutflows at the EarliesT Stages\" (POETS), which has been devoted to studying\nyoung outflow emission on scales of 10-100 au near luminous YSOs, before they\nstart photoionizing the surrounding medium. We carried out multi-epoch Very\nLong Baseline Array observations of the 22 GHz water masers toward NGC7538 IRS1\nto measure the maser 3D velocities, which, following POETS' findings, are\nreliable tracers of the protostellar winds. Recently, we reobserved the water\nmasers in NGC7538 IRS1 with sensitive global very long baseline interferometry\n(VLBI) observations to map weaker maser emission. Our study confirms the\npresence of two embedded YSOs, IRS1a and IRS1b, at the center of the two linear\ndistributions of 6.7 GHz methanol masers observed in the southern and northern\ncores of the HC HII region, which have been previously interpreted in terms of\nedge-on rotating disks. The water masers trace an extended (~200 au) stationary\nshock front adjacent to the inner portion of the disk around IRS1a. This shock\nfront corresponds to the edge of the southern tip of the ionized core and might\nbe produced by the interaction of the disk wind ejected from IRS1a with the\ninfalling envelope. The water masers closer to IRS1b follow the same LSR\nvelocity (Vlsr) pattern of the 6.7~GHz masers rotating in the disk, but the\ndirection and amplitude of the water maser proper motions are inconsistent with\nrotation. We propose that these water masers are tracing a photo-evaporated\ndisk wind, where the maser Vlsr traces mainly the disk rotation and the proper\nmotions the poloidal velocity of the wind.",
      "generated_abstract": "I region NGC 7538-IRS1 is a young protostellar outflow system at\nthe early T stage of evolution. We present the results of the third and\nfourth POETS surveys (POETS-II and POETS-III) conducted at the Hubble Space\nTelescope. These observations covered the region of the HC-HII region between\n3 and 10 mas from the nucleus, with a resolution of 1 mas. The POETS surveys\nare based on the Wide Field Camera 3 (WFC3) on the Hubble Space Telescope,\nusing the Advanced Camera for Surveys (ACS) and the Wide Field Camera 3 (WFC3)\non the Space Telescope Imaging Spectrograph (STIS) in the F606W and F814W\nfilters. We observed",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0972972972972973,
          "p": 0.2727272727272727,
          "f": 0.1434262909445883
        },
        "rouge-2": {
          "r": 0.01444043321299639,
          "p": 0.04395604395604396,
          "f": 0.02173912671210484
        },
        "rouge-l": {
          "r": 0.0918918918918919,
          "p": 0.25757575757575757,
          "f": 0.13545816345454845
        }
      }
    },
    {
      "paper_id": "math.OC.math/OC/2503.10572v1",
      "true_abstract": "The objective of this paper is to investigate the connection between penalty\nfunctions from stochastic optimal control, convex semigroups from analysis and\nconvex expectations from probability theory. Our main result provides a\none-to-one relation between these objects. As an application, we use the\nrepresentation via penality functions and duality arguments to show that convex\nexpectations are determined by their finite dimensional distributions. To\nillustrate this structural result, we show that Hu and Peng's axiomatic\ndescription of $G$-L\\'evy processes in terms of finite dimensional\ndistributions extends uniquely to the control approach introduced by Neufeld\nand Nutz. Finally, we show that convex expectations with a Markovian structure\nare fully determined by their one-dimensional distributions, which give rise to\na classical semigroup on the state space.",
      "generated_abstract": "We study a class of representations of convex expectations and semigroups on\nthe path space. The key property is the so-called equicontinuity property,\nwhich is a type of ``weak equicontinuity'' for paths, and is a key ingredient\nin the proofs of several results in the theory of probability. The main\nresults are the representation theorems for convex expectations and semigroups\non path space, which are based on the equicontinuity property. The results are\ngeneralized to a wide range of path spaces, including random walks on compact\nmetric spaces and random walks on discrete-time Markov chains, as well as a\nclass of path spaces arising from the representation of conditional expectations\nand semigroups on path space.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23255813953488372,
          "p": 0.3389830508474576,
          "f": 0.27586206413888237
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.042105263157894736,
          "f": 0.038647338028892794
        },
        "rouge-l": {
          "r": 0.20930232558139536,
          "p": 0.3050847457627119,
          "f": 0.24827585724233064
        }
      }
    },
    {
      "paper_id": "q-bio.PE.econ/TH/2502.18488v1",
      "true_abstract": "Aquaculture has been the fastest growing food production sector globally due\nto its potential to improve food security, stimulate economic growth, and\nreduce poverty. Its rapid development has been linked to sustainability\nchallenges, many of which are still unresolved and poorly understood.\nSmall-scale producers account for an increasing fraction of aquacultural\noutput. At the same time, many of these producers experience poverty, food\ninsecurity, and rely on unimproved production practices. We develop a stylized\nmathematical model to explore the effects of ecological, social, and economic\nfactors on the dynamics of a small-scale pond aquaculture system. Using\nanalytical and numerical methods, we explore the stability, asymptotic\ndynamics, and bifurcations of the model. Depending on the characteristics of\nthe system, the model exhibits one of three distinct configurations:\nmonostability with a global poverty trap in a nutrient-dominated or\nfish-dominated system; bistability with poverty trap and well-being attractors;\nmultistability with poverty trap and two well-being attractors with different\ncharacteristics. The model results show that intensification can be sustainable\nonly if it takes into account the local social-ecological context. In addition,\nthe heterogeneity of small-scale aquaculture producers matters, as the effects\nof intensification can be unevenly distributed among them. Finally, more is not\nalways better because too high nutrient input or productivity can lead to a\nsuboptimal attractor or system collapse.",
      "generated_abstract": "le aquaculture is a key economic driver in many coastal areas,\nproviding food security and employment for local communities. However, its\nimpacts on local ecosystems and socio-economic systems are often overlooked.\nWe quantify how the production of fish, shrimp, and other aquatic products\ninfluences biodiversity, fisheries productivity, and fisher income, taking into\naccount the specific context of each producer. We use a comprehensive\ninterdisciplinary dataset of 231 fisher communities across 20 coastal areas of\nthe world to characterize production, productivity, and income of fish\nproducers. We find that fisher production and income depend on local ecosystem\nconditions, with higher productivity and income driven by higher biodiversity\nand productivity. These effects are particularly pronounced in areas with\nhigher biodiversity",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1780821917808219,
          "p": 0.3132530120481928,
          "f": 0.2270742311862856
        },
        "rouge-2": {
          "r": 0.01485148514851485,
          "p": 0.02702702702702703,
          "f": 0.01916932449611721
        },
        "rouge-l": {
          "r": 0.1780821917808219,
          "p": 0.3132530120481928,
          "f": 0.2270742311862856
        }
      }
    },
    {
      "paper_id": "astro-ph.HE.astro-ph/HE/2503.10540v1",
      "true_abstract": "Context. The nearby middle-aged gamma-ray pulsar J1741-2054 and its pulsar\nwind nebula (PWN) have been studied in X-rays, and its bow-shock nebula (BSN)\nhas been investigated in the Balmer lines, but they have never been observed in\nfar ultraviolet (FUV). Aims. To further study the thermal and magnetospheric\nemission from PSR J1741-2054 and the BSN properties, we observed them in the\nFUV range with the Hubble Space Telescope (HST). Methods. We imaged the target\nin two FUV filters of the HST's ACS/SBC detector. We also re-analyzed previous\noptical observations of the pulsar and its BSN. We fit the pulsar's FUV-optical\nspectrum separately and together with its X-ray spectrum. Results. We found\nthat the pulsar's FUV-optical spectrum consists of a thermal and nonthermal\ncomponents. A joint fit of the FUV-optical and X-ray spectra with combinations\nof a nonthermal and thermal components showed a hard optical nonthermal\nspectrum with a photon index $\\Gamma_{opt} \\approx 1.0-1.2$ and a softer X-ray\ncomponent, $\\Gamma_X \\approx 2.6-2.7$. The thermal emission is dominated by the\ncold component with the temperature $kT_{cold}\\approx 40-50$ eV and emitting\nsphere radius $R_{cold}\\approx 8-15$ km, at $d=270$ pc. An additional hot\nthermal component, with $kT_{hot}\\sim 80$ eV and $R_{hot}\\sim 1$ km, is also\npossible. Such a spectrum resembles the spectra of other middle-aged pulsars,\nbut it shows a harder (softer) optical (X-ray) nonthermal spectrum. We detected\nthe FUV BSN, the first one associated with a middle-aged pulsar. Its\nclosed-shell morphology is similar to the H$\\alpha$ BSN morphology, while its\nFUV flux, $\\sim10^{-13}$ erg cm$^{-2}$ s$^{-1}$, is a factor of $\\sim 4$ higher\nthan the H$\\alpha$ flux. This FUV BSN has a higher surface brightness than the\ntwo previously known ones.",
      "generated_abstract": "pulsar PSR J1741-2054 is a rare example of a millisecond pulsar\nin the middle-aged state, with a spin period of 1.3 s and a mass of\n$1.58\\pm0.06M_{\\odot}$. This pulsar exhibits a bow shock nebula, with a\nprojected radius of 100 au. We present ultraviolet observations of the\nbow-shock nebula of PSR J1741-2054 obtained with the HST/WFC3 F110W filter.\nThe nebula is bright and compact with a radius of $R_b = 17\\pm2$ au. We\nobtain a density of $n_H= 10^4$ cm$^{-3}$ using the nebular flux and\nobtain a mass",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12804878048780488,
          "p": 0.3684210526315789,
          "f": 0.1900452450408469
        },
        "rouge-2": {
          "r": 0.03802281368821293,
          "p": 0.12048192771084337,
          "f": 0.057803464561295294
        },
        "rouge-l": {
          "r": 0.12195121951219512,
          "p": 0.3508771929824561,
          "f": 0.1809954712851908
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2406.02623v2",
      "true_abstract": "Background: Limited universally-adopted data standards in veterinary medicine\nhinder data interoperability and therefore integration and comparison; this\nultimately impedes the application of existing information-based tools to\nsupport advancement in diagnostics, treatments, and precision medicine.\n  Objectives: A single, coherent, logic-based standard for documenting breed\nnames in health, production, and research-related records will improve data use\ncapabilities in veterinary and comparative medicine. Methods: The Vertebrate\nBreed Ontology (VBO) was created from breed names and related information\ncompiled from the Food and Agriculture Organization of the United Nations,\nbreed registries, communities, and experts, using manual and computational\napproaches. Each breed is represented by a VBO term that includes breed\ninformation and provenance as metadata. VBO terms are classified using\ndescription logic to allow computational applications and Artificial\nIntelligence-readiness.\n  Results: VBO is an open, community-driven ontology representing over 19,500\nlivestock and companion animal breed concepts covering 49 species. Breeds are\nclassified based on community and expert conventions (e.g., cattle breed) and\nsupported by relations to the breed's genus and species indicated by National\nCenter for Biotechnology Information (NCBI) Taxonomy terms. Relationships\nbetween VBO terms (e.g., relating breeds to their foundation stock) provide\nadditional context to support advanced data analytics. VBO term metadata\nincludes synonyms, breed identifiers/codes, and attributed cross-references to\nother databases.\n  Conclusion and clinical importance: The adoption of VBO as a source of\nstandard breed names in databases and veterinary electronic health records can\nenhance veterinary data interoperability and computability.",
      "generated_abstract": "e breed ontology is an emerging discipline that aims to develop\nbreed ontologies that align with the taxonomy of vertebrate species. These\nontologies enable data standardization, facilitate research, and promote the\nexchange of data across multiple disciplines. However, the development of\nbreed ontologies remains a challenging and complex task, with multiple\nbarriers including data heterogeneity, inconsistent taxonomy definitions, and\nlack of standardization. In this paper, we provide a comprehensive review of\nbreed ontologies and their challenges, including data heterogeneity,\nconsistency, and lack of standardization. We also introduce the Vertebrate\nBreed Ontology (VBO), a taxonomy-based ontology that aligns with the\nInternational Commission on Zoological Nomenclature (ICZN) and the World\nRare Disease Network (WRDN) tax",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12025316455696203,
          "p": 0.25,
          "f": 0.16239315800715917
        },
        "rouge-2": {
          "r": 0.013157894736842105,
          "p": 0.02912621359223301,
          "f": 0.018126883930596756
        },
        "rouge-l": {
          "r": 0.11392405063291139,
          "p": 0.23684210526315788,
          "f": 0.1538461494601506
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.03567v1",
      "true_abstract": "We propose a new statistical hypothesis testing framework which decides\nvisually, using confidence intervals, whether the means of two samples are\nequal or if one is larger than the other. With our method, the user can at the\nsame time visualize the confidence region of the means and do a test to decide\nif the means of the two populations are significantly different or not by\nlooking whether the two confidence intervals overlap. To design this test we\nuse confidence intervals constructed using e-variables, which provide a measure\nof evidence in hypothesis testing. We propose both a sequential test and a\nnon-sequential test based on the overlap of confidence intervals and for each\nof these tests we give finite-time error bounds on the probabilities of error.\nWe also illustrate the practicality of our method by applying it to the\ncomparison of sequential learning algorithms.",
      "generated_abstract": "er the problem of constructing visual tests using multiple safe\nconfidence intervals. A visual test is a set of points, each of which is\nassociated with a confidence level. The test is said to be visually successful\nif the points are located at the intersection of the confidence intervals. In\nthis work, we study the problem of constructing visual tests using multiple\nsafe confidence intervals. Our main contribution is a new algorithm, called\nMultiple Safe Intervals (MSI), for constructing visually successful\nconfidence intervals. The algorithm is based on a greedy process. We prove\nthat, with high probability, the set of points associated with the best\nconfidence interval is contained in the intersection of at least two safe\nconfidence intervals. Moreover, we show that, with high probability, the set of\npoints associated with the best confidence interval is contained in the\nintersection of at least three safe confidence intervals. Finally, we study",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27906976744186046,
          "p": 0.3380281690140845,
          "f": 0.3057324791220739
        },
        "rouge-2": {
          "r": 0.06060606060606061,
          "p": 0.0761904761904762,
          "f": 0.06751054358810057
        },
        "rouge-l": {
          "r": 0.22093023255813954,
          "p": 0.2676056338028169,
          "f": 0.24203821160615047
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.10845v1",
      "true_abstract": "Optimal experimental design (OED) is a framework that leverages a\nmathematical model of the experiment to identify optimal conditions for\nconducting the experiment. Under a Bayesian approach, the design objective\nfunction is typically chosen to be the expected information gain (EIG).\nHowever, EIG is intractable for nonlinear models and must be estimated\nnumerically. Estimating the EIG generally entails some variant of Monte Carlo\nsampling, requiring repeated data model and likelihood evaluations\n$\\unicode{x2013}$ each involving solving the governing equations of the\nexperimental physics $\\unicode{x2013}$ under different sample realizations.\nThis computation becomes impractical for high-fidelity models.\n  We introduce a novel multi-fidelity EIG (MF-EIG) estimator under the\napproximate control variate (ACV) framework. This estimator is unbiased with\nrespect to the high-fidelity mean, and minimizes variance under a given\ncomputational budget. We achieve this by first reparameterizing the EIG so that\nits expectations are independent of the data models, a requirement for\ncompatibility with ACV. We then provide specific examples under different data\nmodel forms, as well as practical enhancements of sample size optimization and\nsample reuse techniques. We demonstrate the MF-EIG estimator in two numerical\nexamples: a nonlinear benchmark and a turbulent flow problem involving the\ncalibration of shear-stress transport turbulence closure model parameters\nwithin the Reynolds-averaged Navier-Stokes model. We validate the estimator's\nunbiasedness and observe one- to two-orders-of-magnitude variance reduction\ncompared to existing single-fidelity EIG estimators.",
      "generated_abstract": "We introduce a new estimator of the expected information gain (EIG) for\nBayesian optimal experimental design (B-OED), an important yet challenging\nproblem that has received little attention in the literature. We derive\nanalytical formulas for the estimator, which is based on the expectation of a\nrandom variable that is a linear combination of the prior and posterior\ndistributions. Our estimator is based on the likelihood function of a\nmulti-fidelity approach, which is suitable for high-fidelity data. This\napproach offers a robust method for estimating the EIG. We demonstrate the\neffectiveness of our estimator through simulations and a case study on\nB-OED for a practical application. The findings provide a more accurate and\nrobust EIG estimator for B-OED, which can be applied to various Bayesian\noptimization settings.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2328767123287671,
          "p": 0.40476190476190477,
          "f": 0.29565216927637056
        },
        "rouge-2": {
          "r": 0.06912442396313365,
          "p": 0.12605042016806722,
          "f": 0.08928570971106176
        },
        "rouge-l": {
          "r": 0.1643835616438356,
          "p": 0.2857142857142857,
          "f": 0.20869564753724018
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.cond-mat/stat-mech/2503.09883v1",
      "true_abstract": "Van der Waals \"sliding\" ferroelectric bilayers, whose electric polarization\nis locked to the interlayer alignment, show promise for future non-volatile\nmemory and other nanoelectronic devices. These applications require a fuller\nunderstanding of the polarization stability and switching properties, which\npresent models have described in terms of an Ising-like binary polarization.\nHowever, it is a much larger translation symmetry that is broken in the polar\nstate. Here we introduce a discrete statistical-mechanical model that\nemphasizes the effect of this larger symmetry. Through Monte-Carlo numerics we\nshow this model possesses a richer phase diagram, including an intermediate\ncritical phase of algebraically-correlated polarization. A low energy effective\ntheory allows us to connect the ferroelectric-paraelectric transition to the\nBerezinskii-Kosterlitz-Thouless class, driven by excitations not available in\nIsing-like models. Our results indicate the need for theoretical models of this\nferroelectric system to account for the larger symmetry.",
      "generated_abstract": "electric phase transition in sliding ferroelectrics is governed by\nthe critical sliding field, which is controlled by the system's anisotropy and\nsymmetry. The phase diagram of such systems is typically described by\npseudo-gapless (PG) transitions, where the symmetry is broken at the critical\nfield. In this work, we present a comprehensive analysis of the PG transitions\nin sliding ferroelectrics. We use a combination of first-principles calculations\nand analytical arguments to identify the critical fields for the PG transitions\nand their associated symmetry. We also examine the PG transitions in a broader\ncontext, including the existence of multiple PG transitions in some systems.\nBy constructing a phase diagram, we provide a unified description of the\nexistence of multiple PG transitions and their associated symmetry, even when\nthe system's symmetry is broken",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.3055555555555556,
          "f": 0.25730993664512164
        },
        "rouge-2": {
          "r": 0.028985507246376812,
          "p": 0.0380952380952381,
          "f": 0.03292180579180076
        },
        "rouge-l": {
          "r": 0.21212121212121213,
          "p": 0.2916666666666667,
          "f": 0.24561403021237313
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/MN/2409.19294v3",
      "true_abstract": "Agent-based models capture heterogeneity among individuals in a population\nand are widely used in studies of multi-cellular systems, disease, epidemics\nand demography to name a few. However, existing frameworks consider discrete\ntime-step simulation or assume that agents' states only change as a result of\ndiscrete events. In this note, we present AgentBasedModeling$.$jl, a Julia\npackage for simulating stochastic agent-based population models in continuous\ntime. The tool allows to easily specify and simulate agents evolving through\ngeneric continuous-time jump-diffusions and interacting via continuous-rate\nprocesses. AgentBasedModeling$.$jl provides a powerful methodology for studying\nthe effects of stochasticity on structured population dynamics.",
      "generated_abstract": "of this paper is to introduce the AgentBasedModeling.jl package, a\nsimulation package designed to facilitate the analysis of large-scale\ndynamical systems based on the agent-based modeling approach. The package\nimplements an efficient parallelizable implementation of the Markov Chain Monte\nCarlo (MCMC) algorithm, enabling the simulation of large-scale populations\nundergoing stochastic and non-stationary dynamics. The package also includes\ntools for the analysis and visualization of the simulation output, such as\ndetailed statistical summaries, graphical representations, and plotting\nfunctions. The package is designed to be easy to use, with extensive\ndocumentation and tutorials that provide step-by-step instructions for\nsimulating and analyzing different types of models. By providing a robust and\nflexible simulation framework, AgentBasedModeling.jl aims to foster the\ndevelopment of advanced and scalable agent-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21951219512195122,
          "p": 0.21176470588235294,
          "f": 0.21556885727706276
        },
        "rouge-2": {
          "r": 0.010101010101010102,
          "p": 0.008695652173913044,
          "f": 0.009345789420476048
        },
        "rouge-l": {
          "r": 0.21951219512195122,
          "p": 0.21176470588235294,
          "f": 0.21556885727706276
        }
      }
    },
    {
      "paper_id": "cs.DS.stat/TH/2503.06464v1",
      "true_abstract": "Consider a pair of sparse correlated stochastic block models $\\mathcal\nS(n,\\tfrac{\\lambda}{n},\\epsilon;s)$ subsampled from a common parent stochastic\nblock model with two symmetric communities, average degree $\\lambda=O(1)$ and\ndivergence parameter $\\epsilon \\in (0,1)$. For all $\\epsilon\\in(0,1)$, we\nconstruct a statistic based on the combination of two low-degree polynomials\nand show that there exists a sufficiently small constant\n$\\delta=\\delta(\\epsilon)>0$ and a sufficiently large constant\n$\\Delta=\\Delta(\\epsilon,\\delta)$ such that when $\\lambda>\\Delta$ and\n$s>\\sqrt{\\alpha}-\\delta$ where $\\alpha\\approx 0.338$ is Otter's constant, this\nstatistic can distinguish this model and a pair of independent stochastic block\nmodels $\\mathcal S(n,\\tfrac{\\lambda s}{n},\\epsilon)$ with probability $1-o(1)$.\nWe also provide an efficient algorithm that approximates this statistic in\npolynomial time. The crux of our statistic's construction lies in a carefully\ncurated family of multigraphs called \\emph{decorated trees}, which enables\neffective aggregation of the community signal and graph correlation from the\ncounts of the same decorated tree while suppressing the undesirable\ncorrelations among counts of different decorated trees.",
      "generated_abstract": "the problem of detecting a correlation between two random variables\nin stochastic block models. We assume that both variables are conditionally\nindependent given the other and are conditionally independent of each other\ngiven the other. We show that if both variables are conditionally independent\ngiven the other and are conditionally independent of each other given the other,\nthen a correlation between them can be detected with a detection threshold\n$O(\\sqrt{K\\log K}/\\epsilon)$ using $O(1/\\epsilon^2)$ samples. We then\nimprove this to a threshold of $O(\\sqrt{K\\log K}/\\epsilon)$ using $O(1/\\epsilon)$\nsamples for a correlation between two random variables. This is the first\ndetectability result for correlations in stochastic block models. Our results\nalso extend to a wider class of stochastic block models, including block\nmodels with directed edges, block models with",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17857142857142858,
          "p": 0.3225806451612903,
          "f": 0.22988505288413272
        },
        "rouge-2": {
          "r": 0.020833333333333332,
          "p": 0.033707865168539325,
          "f": 0.025751068239975815
        },
        "rouge-l": {
          "r": 0.16964285714285715,
          "p": 0.3064516129032258,
          "f": 0.2183908000105695
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.07795v1",
      "true_abstract": "Black-box optimization is often encountered for decision-making in complex\nsystems management, where the knowledge of system is limited. Under these\ncircumstances, it is essential to balance the utilization of new information\nwith computational efficiency. In practice, decision-makers often face the dual\ntasks of optimization and statistical inference for the optimal performance, in\norder to achieve it with a high reliability. Our goal is to address the dual\ntasks in an online fashion. Wu et al (2022) [arXiv preprint: 2210.06737] point\nout that the sample average of performance estimates generated by the\noptimization algorithm needs not to admit a central limit theorem. We propose\nan algorithm that not only tackles this issue, but also provides an online\nconsistent estimator for the variance of the performance. Furthermore, we\ncharacterize the convergence rate of the coverage probabilities of the\nasymptotic confidence intervals.",
      "generated_abstract": "r introduces a novel framework for optimization in complex\nenvironmental settings that includes a simultaneous statistical inference\ncomponent. In this framework, a global optimization algorithm is run in a\nblack-box manner while simultaneously estimating the objective function, the\nparameter space, and the covariance function of the statistical distribution\nthat governs the evolution of the state of the world. The resulting optimization\nproblem is a nonlinear program (NLP), which can be solved using the\nwell-established NLP solvers, such as Gurobi, CPLEX, or Mosek. In this work, we\npropose a simple, efficient, and robust algorithm that can be used to\noptimize a wide range of stochastic dynamical systems in complex environments.\nThis algorithm is based on a single global optimization step, followed by\nsimultaneous statistical inference on the covariance function and the state of\nthe world. This approach is a complement",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20408163265306123,
          "p": 0.22988505747126436,
          "f": 0.21621621123389348
        },
        "rouge-2": {
          "r": 0.03759398496240601,
          "p": 0.04132231404958678,
          "f": 0.03937007375131814
        },
        "rouge-l": {
          "r": 0.17346938775510204,
          "p": 0.19540229885057472,
          "f": 0.18378377880146107
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2412.03618v3",
      "true_abstract": "In today's complex and volatile financial market environment, risk management\nof multi-asset portfolios faces significant challenges. Traditional risk\nassessment methods, due to their limited ability to capture complex\ncorrelations between assets, find it difficult to effectively cope with dynamic\nmarket changes. This paper proposes a multi-asset portfolio risk prediction\nmodel based on Convolutional Neural Networks (CNN). By utilizing image\nprocessing techniques, financial time series data are converted into\ntwo-dimensional images to extract high-order features and enhance the accuracy\nof risk prediction. Through empirical analysis of data from multiple asset\nclasses such as stocks, bonds, commodities, and foreign exchange, the results\nshow that the proposed CNN model significantly outperforms traditional models\nin terms of prediction accuracy and robustness, especially under extreme market\nconditions. This research provides a new method for financial risk management,\nwith important theoretical significance and practical value.",
      "generated_abstract": "multi-asset portfolio risk prediction model is a key research\ndirection in financial risk management. Traditional multi-asset portfolio risk\nprediction methods mainly focus on the single asset risk prediction model,\nwhich is still lacking the multi-asset risk prediction ability. This paper\nproposes a novel multi-asset portfolio risk prediction model based on\nconvolutional neural networks (CNNs) and image processing. Firstly, the\ntraditional multi-asset risk prediction model is developed based on the\nconvolutional neural networks (CNNs). Then, the image processing method is\napplied to the multi-asset risk prediction model to improve the prediction\nperformance. The experiments show that the proposed model has a higher\nperformance than the traditional multi-asset risk prediction model. This\nstudy provides a new method for the multi-asset portfolio risk prediction\nmodel, and the model is expected to be widely applied in the future",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.28440366972477066,
          "p": 0.49206349206349204,
          "f": 0.36046511163669553
        },
        "rouge-2": {
          "r": 0.1386861313868613,
          "p": 0.1919191919191919,
          "f": 0.16101694428217483
        },
        "rouge-l": {
          "r": 0.26605504587155965,
          "p": 0.4603174603174603,
          "f": 0.3372092976832072
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.10838v1",
      "true_abstract": "Generalizable deepfake detection can be formulated as a detection problem\nwhere labels (bonafide and fake) are fixed but distributional drift affects the\ndeepfake set. We can always train our detector with one-selected attacks and\nbonafide data, but an attacker can generate new attacks by just retraining his\ngenerator with a different seed. One reasonable approach is to simply pool all\ndifferent attack types available in training time. Our proposed approach is to\nutilize meta-learning in combination with LoRA adapters to learn the structure\nin the training data that is common to all attack types.",
      "generated_abstract": "epfake detection (DDF) has been an active area of research\nsince its inception in 2008. However, existing DDF methods are often limited\nto specific domains, such as audio and video, and rely on extensive training\ndata, which can be difficult to obtain for non-English languages. Furthermore,\nexisting DDF methods often face challenges in generalizing across different\nspeech domains, which can hinder their applicability to new domains. To\naddress these challenges, we propose a meta-learning-based framework for\ngeneralizable speech DDF, LoRA. LoRA is a novel meta-learning-based model\nthat integrates LoRA-Encoder, LoRA-Decoder, and LoRA-Classifier, which are\ndesigned to extract, encode, and classify speech information, respectively.\nLoRA-Encoder extracts the features of the audio and video domains, LoRA",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2608695652173913,
          "p": 0.20930232558139536,
          "f": 0.23225805957627485
        },
        "rouge-2": {
          "r": 0.011111111111111112,
          "p": 0.009174311926605505,
          "f": 0.010050246301863505
        },
        "rouge-l": {
          "r": 0.21739130434782608,
          "p": 0.1744186046511628,
          "f": 0.19354838215692
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.01315v1",
      "true_abstract": "Harsanyi (1955) showed that the only way to aggregate individual preferences\ninto a social preference which satisfies certain desirable properties is\n``utilitarianism'', whereby the social utility function is a weighted average\nof individual utilities. This representation forms the basis for welfare\nanalysis in most applied work. We argue, however, that welfare analysis based\non Harsanyi's version of utilitarianism may overlook important distributional\nconsiderations. We therefore introduce a notion of utilitarianism for\ndiscrete-choice settings which applies to \\textit{social choice functions},\nwhich describe the actions of society, rather than social welfare functions\nwhich describe society's preferences (as in Harsanyi). We characterize a\nrepresentation of utilitarian social choice, and show that it provides a\nfoundation for a family of \\textit{distributional welfare measures} based on\nquantiles of the distribution of individual welfare effects, rather than\naverages.",
      "generated_abstract": "This paper develops a general framework for distributive welfare analysis that\nuses the notion of utilitarian social choice. This framework allows us to\nanalyze situations where one person's preferences are disordered with respect\nto another person's preferences. We show that this framework is equivalent to\nthe usual social choice theory and that the resulting welfare function is\nwell-defined and convex. We also show that, under certain conditions, the\nwelfare function can be made concave. We develop a simple method for\nconstructing concave welfare functions and provide an example to illustrate\nthese results. Finally, we show that the new framework offers a natural\nalternative to the so-called \"top-down\" approach to distributive welfare\nanalysis.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2441860465116279,
          "p": 0.2876712328767123,
          "f": 0.26415093842965076
        },
        "rouge-2": {
          "r": 0.064,
          "p": 0.07766990291262135,
          "f": 0.07017543364304436
        },
        "rouge-l": {
          "r": 0.20930232558139536,
          "p": 0.2465753424657534,
          "f": 0.22641508937304702
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/SE/2503.09510v1",
      "true_abstract": "Code Review consists in assessing the code written by teammates with the goal\nof increasing code quality. Empirical studies documented the benefits brought\nby such a practice that, however, has its cost to pay in terms of developers'\ntime. For this reason, researchers have proposed techniques and tools to\nautomate code review tasks such as the reviewers selection (i.e., identifying\nsuitable reviewers for a given code change) or the actual review of a given\nchange (i.e., recommending improvements to the contributor as a human reviewer\nwould do). Given the substantial amount of papers recently published on the\ntopic, it may be challenging for researchers and practitioners to get a\ncomplete overview of the state-of-the-art.\n  We present a systematic literature review (SLR) featuring 119 papers\nconcerning the automation of code review tasks. We provide: (i) a\ncategorization of the code review tasks automated in the literature; (ii) an\noverview of the under-the-hood techniques used for the automation, including\nthe datasets used for training data-driven techniques; (iii) publicly available\ntechniques and datasets used for their evaluation, with a description of the\nevaluation metrics usually adopted for each task.\n  The SLR is concluded by a discussion of the current limitations of the\nstate-of-the-art, with insights for future research directions.",
      "generated_abstract": "ew is a process that involves a pair of reviewers reviewing the code\nof another contributor. It serves as a mechanism to ensure that both reviewers\nhave a thorough understanding of the code, as well as that the code adheres to\nthe project's coding guidelines. The literature on code review practices\nexamines the benefits of automated code review tools, which can save reviewers\ntime and improve the quality of code reviews. This review examines the\nliterature on automated code review tools, focusing on the benefits and\nchallenges of such tools. The literature review is structured according to the\nfollowing topics: automated code review tools; automated code review\ntechniques; automated code review best practices; and automated code review\nchallenges. A literature search was conducted using Google Scholar, Mendeley,\nand Google. The results of the literature review reveal that there are",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16793893129770993,
          "p": 0.3055555555555556,
          "f": 0.2167487638952657
        },
        "rouge-2": {
          "r": 0.04712041884816754,
          "p": 0.07692307692307693,
          "f": 0.05844155373018254
        },
        "rouge-l": {
          "r": 0.1450381679389313,
          "p": 0.2638888888888889,
          "f": 0.18719211364896032
        }
      }
    },
    {
      "paper_id": "physics.hist-ph.physics/hist-ph/2503.05668v1",
      "true_abstract": "This paper concerns the question of which collections of general relativistic\nspacetimes are deterministic relative to which definitions. We begin by\nconsidering a series of three definitions of increasing strength due to Belot\n(1995). The strongest of these definitions is particularly interesting for\nspacetime theories because it involves an asymmetry condition called\n``rigidity'' that has been studied previously in a different context (Geroch\n1969; Halvorson and Manchak 2022; Dewar 2024). We go on to explore other\n(stronger) asymmetry conditions that give rise to other (stronger) forms of\ndeterminism. We introduce a number of definitions of this type and clarify the\nrelationships between them and the three considered by Belot. We go on to show\nthat there are collections of general relativistic spacetimes that satisfy much\nstronger forms of determinism than previously known. We also highlight a number\nof open questions.",
      "generated_abstract": "the deterministic predictions of General Relativity (GR) in the\ngeneral case, and examine their asymmetry. We focus on the four-velocity\ndeterminant, which is the most common and well-established expression for the\nasymmetry of GR. We show that the deterministic predictions of GR can be\ninterpreted in a symmetric way, and that the asymmetry can be restored by\ntaking a coordinate transformation, such as the Lema\\^{\\i}tre-Tolman-Bondi\n(LTB) transformation. We show that the asymmetry is a consequence of the\nnon-vanishing of the torsion scalar, which is not allowed by GR. We also show\nthat the asymmetry is not a consequence of the non-vanishing of the torsion\nscalar, but rather that the non-vanishing of the torsion scalar implies the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18681318681318682,
          "p": 0.30357142857142855,
          "f": 0.23129251229024952
        },
        "rouge-2": {
          "r": 0.023809523809523808,
          "p": 0.03614457831325301,
          "f": 0.028708129182940748
        },
        "rouge-l": {
          "r": 0.18681318681318682,
          "p": 0.30357142857142855,
          "f": 0.23129251229024952
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.02344v1",
      "true_abstract": "The recently emerged movable antenna (MA) shows great promise in leveraging\nspatial degrees of freedom to enhance the performance of wireless systems.\nHowever, resource allocation in MA-aided systems faces challenges due to the\nnonconvex and coupled constraints on antenna positions. This paper\nsystematically reveals the challenges posed by the minimum antenna separation\ndistance constraints. Furthermore, we propose a penalty optimization framework\nfor resource allocation under such new constraints for MA-aided systems.\nSpecifically, the proposed framework separates the non-convex and coupled\nantenna distance constraints from the movable region constraints by introducing\nauxiliary variables. Subsequently, the resulting problem is efficiently solved\nby alternating optimization, where the optimization of the original variables\nresembles that in conventional resource allocation problem while the\noptimization with respect to the auxiliary variables is achieved in closedform\nsolutions. To illustrate the effectiveness of the proposed framework, we\npresent three case studies: capacity maximization, latency minimization, and\nregularized zero-forcing precoding. Simulation results demonstrate that the\nproposed optimization framework consistently outperforms state-of-the-art\nschemes.",
      "generated_abstract": "r introduces a novel optimization framework for the design of\nmovable antenna-aided systems (MAAS) that address the challenges of distance\nconstraints in outdoor environments. In particular, we address the design of\nMAAS with non-uniform antenna arrays that are placed at arbitrary positions\nwithin the coverage area of the primary transmitter (PT). To this end, we\npropose a novel constrained optimization framework that includes a general\ndistance constraint for the antenna array positions. The proposed framework\nis formulated as a linear program (LP) and solved by means of interior\npoint (IP) method. The proposed approach provides a comprehensive framework\nfor the design of MAAS with non-uniform antenna arrays, enabling the\nsimultaneous optimization of the transmit power, antenna array positions, and\nthe antenna gain at each antenna. Numerical results demonstrate the effectiveness\nof the proposed framework in design",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2692307692307692,
          "p": 0.345679012345679,
          "f": 0.3027026977799855
        },
        "rouge-2": {
          "r": 0.09333333333333334,
          "p": 0.12173913043478261,
          "f": 0.10566037244571046
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.32098765432098764,
          "f": 0.2810810761583638
        }
      }
    },
    {
      "paper_id": "math.PR.stat/OT/2411.03955v1",
      "true_abstract": "We provide bounds on the tail probabilities for simple procedures that\ngenerate random samples _without replacement_, when the probabilities of being\nselected need not be equal.",
      "generated_abstract": "We derive large deviation inequalities for the probability of an event\nin a uniform law with respect to a measure of unequal probabilities. These\ninequalities provide a foundation for a generalization of the central limit\ntheorem for unequal probability sampling without replacement to a wider class\nof distributions. The proof is based on the asymptotic behavior of the\ndistribution of the difference of the probabilities of two events.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2916666666666667,
          "p": 0.15555555555555556,
          "f": 0.20289854618777578
        },
        "rouge-2": {
          "r": 0.12,
          "p": 0.047619047619047616,
          "f": 0.06818181411415312
        },
        "rouge-l": {
          "r": 0.2916666666666667,
          "p": 0.15555555555555556,
          "f": 0.20289854618777578
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.03554v1",
      "true_abstract": "We model stochastic choices with categorization, resulting from the\npreliminary step of grouping alternatives in homogenous disjoint classes. The\nagent randomly chooses one class among those available, then randomly picks an\nitem within the selected class. We give a formal definition of a choice\ngenerated by this procedure, and provide a characterization. The characterizing\nproperties allow an external observer to elicit that categorization is applied.\nIn a more general interpretation, the model allows to describe the observed\nchoice as the composition of independent subchoices. This composition preserves\nrationalizability by random utility maximization. A generalization of the model\nsubsumes Luce model and Nested Logit.",
      "generated_abstract": "r develops a model of sequential stochastic choice that combines\nthe categorization and randomization frameworks. The categorization framework\nprovides a mechanism for categorizing potential outcomes into a finite number\nof categories. The randomization framework provides a mechanism that allows\nsubjects to randomize their choices among these categories. We develop a\nnon-stationary model of this framework that can handle a wide range of\ninterpretations, including randomized treatment effects, instrumental variables,\nand other forms of treatment effects that do not depend on the categorization\noutcome. We show that under a simple model of the categorization mechanism,\nthe model satisfies the axioms of the categorization and randomization\nframeworks. We also discuss a class of categorization mechanisms that are\nrelevant in practice and provide a model that satisfies the axioms of the\ncategorization framework. We illustrate the model's potential by analyzing two",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25316455696202533,
          "p": 0.2564102564102564,
          "f": 0.2547770650638972
        },
        "rouge-2": {
          "r": 0.039603960396039604,
          "p": 0.03389830508474576,
          "f": 0.036529675395426134
        },
        "rouge-l": {
          "r": 0.24050632911392406,
          "p": 0.24358974358974358,
          "f": 0.2420382115607125
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2408.01470v1",
      "true_abstract": "In order to overcome the drawbacks of assuming deterministic volatility\ncoefficients in the standard LIBOR market models to capture volatility smiles\nand skews in real markets, several extensions of LIBOR models to incorporate\nstochastic volatilities have been proposed. The efficient calibration to market\ndata of these more complex models becomes a relevant target in practice. The\nmain objective of the present work is to efficiently calibrate some recent\nSABR/LIBOR market models to real market prices of caplets and swaptions. For\nthe calibration we propose a parallelized version of the simulated annealing\nalgorithm for multi-GPUs. The numerical results clearly illustrate the\nadvantages of using the proposed multi-GPUs tools when applied to real market\ndata and popular SABR/LIBOR models.",
      "generated_abstract": "This study explores the use of the SABR/LIBOR model to price and calibrate\ninterest rate derivatives. It proposes a generic methodology for calibrating\ninterest rate derivatives that uses the SABR/LIBOR model to price the\ninterest rate derivative and then uses the Black-Scholes model to calibrate the\nderivative to the asset market. This methodology is applied to some interest\nrate derivatives. The methodology is applied to a CPI derivative and to an\ninterest rate derivative with a negative correlation between the interest rate\nand the volatility. The results show that the SABR/LIBOR model provides\ncalibrated models that can be used in pricing and calibrating interest rate\nderivatives.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22666666666666666,
          "p": 0.34,
          "f": 0.27199999520000007
        },
        "rouge-2": {
          "r": 0.01834862385321101,
          "p": 0.024691358024691357,
          "f": 0.021052626687535762
        },
        "rouge-l": {
          "r": 0.22666666666666666,
          "p": 0.34,
          "f": 0.27199999520000007
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2502.13344v1",
      "true_abstract": "Drug discovery is a complex and time-intensive process that requires\nidentifying and validating new therapeutic candidates. Computational approaches\nusing large-scale biomedical knowledge graphs (KGs) offer a promising solution\nto accelerate this process. However, extracting meaningful insights from\nlarge-scale KGs remains challenging due to the complexity of graph traversal.\nExisting subgraph-based methods are tailored to graph neural networks (GNNs),\nmaking them incompatible with other models, such as large language models\n(LLMs). We introduce K-Paths, a retrieval framework that extracts structured,\ndiverse, and biologically meaningful paths from KGs. Integrating these paths\nenables LLMs and GNNs to effectively predict unobserved drug-drug and\ndrug-disease interactions. Unlike traditional path-ranking approaches, K-Paths\nretrieves and transforms paths into a structured format that LLMs can directly\nprocess, facilitating explainable reasoning. K-Paths employs a diversity-aware\nadaptation of Yen's algorithm to retrieve the K shortest loopless paths between\nentities in an interaction query, prioritizing biologically relevant and\ndiverse relationships. Our experiments on benchmark datasets show that K-Paths\nimproves the zero-shot performance of Llama 8.1B's F1-score by 12.45 points on\ndrug repurposing and 13.42 points on interaction severity prediction. We also\nshow that Llama 70B achieves F1-score gains of 6.18 and 8.46 points,\nrespectively. K-Paths also improves the supervised training efficiency of\nEmerGNN, a state-of-the-art GNN, by reducing KG size by 90% while maintaining\nstrong predictive performance. Beyond its scalability and efficiency, K-Paths\nuniquely bridges the gap between KGs and LLMs, providing explainable rationales\nfor predicted interactions. These capabilities show that K-Paths is a valuable\ntool for efficient data-driven drug discovery.",
      "generated_abstract": "ty to predict drug interactions and repurposing is crucial for\nmaking rational drug selections. Traditional approaches to drug interactions\nprediction often rely on manually curated lists of interactions and relies on\nknowledge of the drug's mechanism of action. This is inadequate when considering\nnovel therapeutic targets. This study introduces K-Paths, a novel approach to\nthe task of predicting drug interactions and repurposing using graph path\nreasoning. K-Paths leverages the structural relationships between drugs in\ndrug-drug interaction (DDI) and drug-drug-drug interactions (D3DI) graphs to\npredict interactions and repurposing. K-Paths achieves state-of-the-art\nperformance in both DDI and D3DI graphs, surpassing state-of-the-art methods\nwith a 45",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18579234972677597,
          "p": 0.4788732394366197,
          "f": 0.2677165314052328
        },
        "rouge-2": {
          "r": 0.012096774193548387,
          "p": 0.031914893617021274,
          "f": 0.01754385566293993
        },
        "rouge-l": {
          "r": 0.15300546448087432,
          "p": 0.39436619718309857,
          "f": 0.2204724369170439
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2411.09657v1",
      "true_abstract": "We study the tail asymptotics of the sum of two heavy-tailed random\nvariables. The dependence structure is modeled by copulas with the so-called\ntail order property. Examples are presented to illustrate the approach. Further\nfor each example we apply the main results to obtain the asymptotic expansions\nfor Value-at-Risk of aggregate risk.",
      "generated_abstract": "In this paper, we study the asymptotic behavior of the sum of heavy-tailed\nrisks under a jointly Gaussian copula, a generalization of the\n$\\chi^2$-copula. We provide the exact expressions for the asymptotic\ndistributions of the sum of risks under the copula and derive explicit\nasymptotic formulae for the moments of the sum of risks under the copula.\nAdditionally, we derive the asymptotic distribution of the sum of risks under\nthe copula under the null hypothesis, which is not available in the\nliterature. In addition to the asymptotic distributions, we also provide the\nasymptotic distribution of the sum of risks under the copula under various\nalternative hypotheses. The main applications of the proposed copula are in\nthe estimation of the sum of risks under the copula and in the estimation of\nthe parameters of the copula.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3333333333333333,
          "p": 0.2545454545454545,
          "f": 0.28865978890424066
        },
        "rouge-2": {
          "r": 0.09803921568627451,
          "p": 0.06097560975609756,
          "f": 0.07518796519645007
        },
        "rouge-l": {
          "r": 0.3333333333333333,
          "p": 0.2545454545454545,
          "f": 0.28865978890424066
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.10145v1",
      "true_abstract": "We discuss the qualitatively new properties of random walks on groups that\narise in the situation when the entropy of the step distribution is infinite.",
      "generated_abstract": "In this paper, we study the Liouville property of a class of random walks with\ninfinite entropy. We show that, for the class of random walks with infinite\nentropy, there are no Liouville probability measures and no Poisson boundary of\nthese random walks. We also show that, for the class of random walks with\ninfinite entropy, the measure of the Poisson boundary is the same as the\nPoisson measure of the inverse of the entropy of the walk. Finally, we prove\nthat the measure of the Poisson boundary is the same as the measure of the\nPoisson boundary of a random walk with finite entropy.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.42857142857142855,
          "p": 0.21951219512195122,
          "f": 0.29032257616545265
        },
        "rouge-2": {
          "r": 0.20833333333333334,
          "p": 0.07692307692307693,
          "f": 0.1123595466229013
        },
        "rouge-l": {
          "r": 0.3333333333333333,
          "p": 0.17073170731707318,
          "f": 0.2258064471331947
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SP/2503.04278v1",
      "true_abstract": "This study addresses the challenge of access point (AP) and user equipment\n(UE) association in cell-free massive MIMO networks. It introduces a deep\nlearning algorithm leveraging Bidirectional Long Short-Term Memory cells and a\nhybrid probabilistic methodology for weight updating. This approach enhances\nscalability by adapting to variations in the number of UEs without requiring\nretraining. Additionally, the study presents a training methodology that\nimproves scalability not only with respect to the number of UEs but also to the\nnumber of APs. Furthermore, a variant of the proposed AP-UE algorithm ensures\nrobustness against pilot contamination effects, a critical issue arising from\npilot reuse in channel estimation. Extensive numerical results validate the\neffectiveness and adaptability of the proposed methods, demonstrating their\nsuperiority over widely used heuristic alternatives.",
      "generated_abstract": "ultiple-input multiple-output (MIMO) systems have emerged as a\nnew paradigm for wireless communications, providing significant improvements in\nboth spectral efficiency and capacity. However, massive MIMO has limitations\ndue to the high computational complexity of joint channel estimation and\nassociation. To address this, this paper proposes a novel algorithm based on\nrecurrent neural networks (RNNs) for efficient user association in user-centric\ncell-free massive MIMO. Our proposed algorithm is based on the concept of\nneural recurrent state (NR-SR), which integrates the RNN framework with the\nneural state machine (NSM) for efficient and scalable user association. The\nproposed algorithm is designed to operate at both low-power and high-power\nscenarios, where the low-power scenario is used in low-bandwidth scenarios and\nthe high-power scenario is used in high-band",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20430107526881722,
          "p": 0.2289156626506024,
          "f": 0.21590908592523256
        },
        "rouge-2": {
          "r": 0.04310344827586207,
          "p": 0.04424778761061947,
          "f": 0.04366811727160105
        },
        "rouge-l": {
          "r": 0.1827956989247312,
          "p": 0.20481927710843373,
          "f": 0.19318181319795982
        }
      }
    },
    {
      "paper_id": "cs.SI.stat/ML/2503.02271v1",
      "true_abstract": "Experiments in online platforms frequently suffer from network interference,\nin which a treatment applied to a given unit affects outcomes for other units\nconnected via the platform. This SUTVA violation biases naive approaches to\nexperiment design and estimation. A common solution is to reduce interference\nby clustering connected units, and randomizing treatments at the cluster level,\ntypically followed by estimation using one of two extremes: either a simple\ndifference-in-means (DM) estimator, which ignores remaining interference; or an\nunbiased Horvitz-Thompson (HT) estimator, which eliminates interference at\ngreat cost in variance. Even combined with clustered designs, this presents a\nlimited set of achievable bias variance tradeoffs. We propose a new estimator,\ndubbed Differences-in-Neighbors (DN), designed explicitly to mitigate network\ninterference. Compared to DM estimators, DN achieves bias second order in the\nmagnitude of the interference effect, while its variance is exponentially\nsmaller than that of HT estimators. When combined with clustered designs, DN\noffers improved bias-variance tradeoffs not achievable by existing approaches.\nEmpirical evaluations on a large-scale social network and a city-level\nride-sharing simulator demonstrate the superior performance of DN in\nexperiments at practical scale.",
      "generated_abstract": "nce in sensor networks is a significant challenge, as it can lead to\na decrease in network performance. One approach to addressing this issue is to\nintroduce a delay constraint on the transmissions. We investigate the impact of\nthis constraint on the performance of a network where there is a significant\nnumber of interfering neighbors. We find that the number of interfering neighbors\nis a critical factor, as a larger number of interfering neighbors increases the\nnetwork performance. We also find that the delay constraint is less important\nthan the number of interfering neighbors, and that the delay constraint has a\nlower impact on the performance of the network when there are a smaller number\nof interfering neighbors. These findings highlight the importance of considering\nthe impact of interfering neighbors in sensor networks. Additionally, we\nintroduce the Differences-in-Neighbors model, which captures the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1297709923664122,
          "p": 0.265625,
          "f": 0.17435896994924405
        },
        "rouge-2": {
          "r": 0.02247191011235955,
          "p": 0.037383177570093455,
          "f": 0.02807017074890814
        },
        "rouge-l": {
          "r": 0.12213740458015267,
          "p": 0.25,
          "f": 0.16410255969283377
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.01075v1",
      "true_abstract": "Hallucinations are spurious structures not present in the ground truth,\nposing a critical challenge in medical image reconstruction, especially for\ndata-driven conditional models. We hypothesize that combining an unconditional\ndiffusion model with data consistency, trained on a diverse dataset, can reduce\nthese hallucinations. Based on this, we propose DynamicDPS, a diffusion-based\nframework that integrates conditional and unconditional diffusion models to\nenhance low-quality medical images while systematically reducing\nhallucinations. Our approach first generates an initial reconstruction using a\nconditional model, then refines it with an adaptive diffusion-based inverse\nproblem solver. DynamicDPS skips early stage in the reverse process by\nselecting an optimal starting time point per sample and applies Wolfe's line\nsearch for adaptive step sizes, improving both efficiency and image fidelity.\nUsing diffusion priors and data consistency, our method effectively reduces\nhallucinations from any conditional model output. We validate its effectiveness\nin Image Quality Transfer for low-field MRI enhancement. Extensive evaluations\non synthetic and real MR scans, including a downstream task for tissue volume\nestimation, show that DynamicDPS reduces hallucinations, improving relative\nvolume estimation by over 15% for critical tissues while using only 5% of the\nsampling steps required by baseline diffusion models. As a model-agnostic and\nfine-tuning-free approach, DynamicDPS offers a robust solution for\nhallucination reduction in medical imaging. The code will be made publicly\navailable upon publication.",
      "generated_abstract": "mage reconstruction is a critical task in medical image analysis.\nHallucinations are a significant challenge in this field. These distortions\narise from both inaccurate model training and ill-posed reconstruction\nproblems. In this work, we propose a novel conditional model-based approach\nthat addresses both problems simultaneously. Our approach leverages a\nconditional generative model to generate synthetic data conditioned on the\noriginal image, which we use to train a conditional generative model to\nreconstruct the original image. This process is repeated multiple times to\ngenerate a series of reconstruction images, which we use to train a\nconditional generative model to predict the hallucinated images. This approach\nenables a more robust and accurate reconstruction of the original image. We\ndemonstrate the effectiveness of our approach through experiments on both\nsynthetic and real-world datasets. Our code is available at https",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2138364779874214,
          "p": 0.41975308641975306,
          "f": 0.28333332886145834
        },
        "rouge-2": {
          "r": 0.046948356807511735,
          "p": 0.0847457627118644,
          "f": 0.060422956136946884
        },
        "rouge-l": {
          "r": 0.20754716981132076,
          "p": 0.4074074074074074,
          "f": 0.27499999552812504
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2412.03305v1",
      "true_abstract": "An investment portfolio consists of $n$ algorithmic trading strategies, which\ngenerate vectors of positions in trading assets. Sign opposite trades\n(buy/sell) cross each other as strategies are combined in a portfolio. Then\nportfolio turnover becomes a non linear function of strategies turnover. It\nrises a problem of effective (quick and precise) portfolio turnover estimation.\nKakushadze and Liew (2014) shows how to estimate turnover via covariance matrix\nof returns. We build a mathematical model for such estimations; prove a theorem\nwhich gives a necessary condition for model applicability; suggest new turnover\nestimations; check numerically the preciseness of turnover estimations for\nalgorithmic strategies on USA equity market.",
      "generated_abstract": "This paper investigates the relationship between turnover and covariance\nmoments of returns in the context of portfolio optimization. We show that\nturnover is a function of the covariance of returns and that the covariance\nmoments are related to the mean of returns. The results suggest that turnover is\nrelated to the covariance of returns, and therefore to the covariance of\nreturns. The paper also examines the effect of the covariance of returns on the\nturnover of an investment portfolio and provides evidence that a higher\ncovariance of returns leads to higher turnover.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21052631578947367,
          "p": 0.3902439024390244,
          "f": 0.2735042689517131
        },
        "rouge-2": {
          "r": 0.02912621359223301,
          "p": 0.04285714285714286,
          "f": 0.03468207610678673
        },
        "rouge-l": {
          "r": 0.19736842105263158,
          "p": 0.36585365853658536,
          "f": 0.256410251857696
        }
      }
    },
    {
      "paper_id": "math.KT.math/KT/2503.04251v1",
      "true_abstract": "We prove separation and excision results in functor homology. These results\nexplain how the global Steinberg decomposition of functors proved by Djament,\nTouz{\\'e} and Vespa behaves in Ext and Tor computations.",
      "generated_abstract": "In this paper, we study the functor homology of the functor $F: \\mathbb{P}\n(A \\oplus B) \\to \\mathbb{P}(A) \\times \\mathbb{P}(B)$ with respect to a general\npair of projective spaces. For each positive integer $n$, we consider the\nfunctor homology $HF_n(F)$ of $F$. By using the functor homology, we study the\nseparation and excision theorems for $HF_n(F)$. We prove the separation theorem\nfor $HF_n(F)$ and give a sufficient condition for the excision theorem for\n$HF_n(F)$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3333333333333333,
          "p": 0.1875,
          "f": 0.2399999953920001
        },
        "rouge-2": {
          "r": 0.13333333333333333,
          "p": 0.06349206349206349,
          "f": 0.08602150100589685
        },
        "rouge-l": {
          "r": 0.3333333333333333,
          "p": 0.1875,
          "f": 0.2399999953920001
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2502.15822v1",
      "true_abstract": "This paper proposes a financial fraud detection system based on improved\nRandom Forest (RF) and Gradient Boosting Machine (GBM). Specifically, the\nsystem introduces a novel model architecture called GBM-SSRF (Gradient Boosting\nMachine with Simplified and Strengthened Random Forest), which cleverly\ncombines the powerful optimization capabilities of the gradient boosting\nmachine (GBM) with improved randomization. The computational efficiency and\nfeature extraction capabilities of the Simplified and Strengthened Random\nForest (SSRF) forest significantly improve the performance of financial fraud\ndetection. Although the traditional random forest model has good classification\ncapabilities, it has high computational complexity when faced with large-scale\ndata and has certain limitations in feature selection. As a commonly used\nensemble learning method, the GBM model has significant advantages in\noptimizing performance and handling nonlinear problems. However, GBM takes a\nlong time to train and is prone to overfitting problems when data samples are\nunbalanced. In response to these limitations, this paper optimizes the random\nforest based on the structure, reducing the computational complexity and\nimproving the feature selection ability through the structural simplification\nand enhancement of the random forest. In addition, the optimized random forest\nis embedded into the GBM framework, and the model can maintain efficiency and\nstability with the help of GBM's gradient optimization capability. Experiments\nshow that the GBM-SSRF model not only has good performance, but also has good\nrobustness and generalization capabilities, providing an efficient and reliable\nsolution for financial fraud detection.",
      "generated_abstract": "fraud is a significant challenge for financial institutions and\nsecurity agencies. Financial fraud detection is a challenging task due to the\nvarious characteristics of financial data, including complex structures,\nhigh-dimensionality, and data imbalance. This paper presents a novel\nfinancial fraud detection system based on improved random forest and gradient\nboosting machine (GBM). The proposed system uses a multi-layer perceptron (MLP)\nnetwork to model the relationships between features and class labels. The MLP\nnetwork is designed to capture complex relationships between features and\nlabels, which helps the model learn complex patterns and identify fraud\npatterns. The GBM model is used to enhance the performance of the MLP model.\nGBM is a powerful and efficient machine learning model that can handle\nimbalanced data, providing better performance than traditional classification\nalgorithms. The proposed method is validated using a dataset containing",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2846715328467153,
          "p": 0.45348837209302323,
          "f": 0.34977578001488074
        },
        "rouge-2": {
          "r": 0.09004739336492891,
          "p": 0.14728682170542637,
          "f": 0.11176470117318359
        },
        "rouge-l": {
          "r": 0.25547445255474455,
          "p": 0.4069767441860465,
          "f": 0.3139013405529973
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/MM/2503.10324v1",
      "true_abstract": "Multi-modal object Re-IDentification (ReID) aims to retrieve specific objects\nby utilizing complementary information from various modalities. However,\nexisting methods focus on fusing heterogeneous visual features, neglecting the\npotential benefits of text-based semantic information. To address this issue,\nwe first construct three text-enhanced multi-modal object ReID benchmarks. To\nbe specific, we propose a standardized multi-modal caption generation pipeline\nfor structured and concise text annotations with Multi-modal Large Language\nModels (MLLMs). Besides, current methods often directly aggregate multi-modal\ninformation without selecting representative local features, leading to\nredundancy and high complexity. To address the above issues, we introduce IDEA,\na novel feature learning framework comprising the Inverted Multi-modal Feature\nExtractor (IMFE) and Cooperative Deformable Aggregation (CDA). The IMFE\nutilizes Modal Prefixes and an InverseNet to integrate multi-modal information\nwith semantic guidance from inverted text. The CDA adaptively generates\nsampling positions, enabling the model to focus on the interplay between global\nfeatures and discriminative local features. With the constructed benchmarks and\nthe proposed modules, our framework can generate more robust multi-modal\nfeatures under complex scenarios. Extensive experiments on three multi-modal\nobject ReID benchmarks demonstrate the effectiveness of our proposed method.",
      "generated_abstract": "vances in image and video representation learning have enabled\nthe development of large-scale multi-modal object re-identification (MOR)\nbenchmarks, yet the models' performance remains relatively low. This is\nprimarily due to the inherent challenges of multi-modal object re-identification\n(MOR), which includes challenges such as data sparsity, data imbalance, and\ndata shift. To address these challenges, we propose an innovative MOR\nframework, IDEA, consisting of three key components: inverted text\nrepresentation, cooperative deformable aggregation, and multi-modal object\nre-identification. IDEA leverages the inverted text representation to capture\nsemantic information, while combining it with the deformable aggregation to\neffectively handle the data imbalance and data shift. Additionally, we design\nan innovative cooperative deformable aggregation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13636363636363635,
          "p": 0.23684210526315788,
          "f": 0.17307691843934925
        },
        "rouge-2": {
          "r": 0.0223463687150838,
          "p": 0.04040404040404041,
          "f": 0.028776973831324167
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.23684210526315788,
          "f": 0.17307691843934925
        }
      }
    },
    {
      "paper_id": "q-fin.GN.econ/GN/2412.19817v1",
      "true_abstract": "Digital transformation significantly impacts firm investment, financing, and\nvalue enhancement. A systematic investigation from the corporate finance\nperspective has not yet been formed. This paper combines bibliometric and\ncontent analysis methods to systematically review the evolutionary trend,\nstatus quo, hotspots and overall structure of research in digital\ntransformation from 2011 to 2024. The study reveals an emerging and rapidly\ngrowing focus on digital transformation research, particularly in developed\ncountries. We categorize the literature into three areas according to\nbibliometric clustering: the measurements (qualitative and quantitative),\nimpact factors (internal and external), and the economic consequences\n(investment, financing, and firm value). These areas are divided into ten\nsub-branches, with a detailed literature review. We also review the existing\ntheories related to digital transformation, identify the current gaps in these\npapers, and provide directions for future research on each sub-branches.",
      "generated_abstract": "This study is the first to systematically review the literature on digital\ntransformation, focusing on the impact on corporate finance. We identified 69\narticles, covering 19 countries and 275 papers. The review reveals that\ndigital transformation can improve corporate finance by enhancing accounting\nand financial reporting, enabling more precise analysis of the business,\nincreasing efficiency and effectiveness, and enhancing corporate governance.\nThe findings suggest that the development of digital transformation is a\npositive trend for corporate finance, with significant potential to enhance\nperformance and resilience in times of crisis.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24509803921568626,
          "p": 0.390625,
          "f": 0.30120481453912035
        },
        "rouge-2": {
          "r": 0.06015037593984962,
          "p": 0.09302325581395349,
          "f": 0.0730593559608852
        },
        "rouge-l": {
          "r": 0.21568627450980393,
          "p": 0.34375,
          "f": 0.2650602362258674
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.07332v1",
      "true_abstract": "Change-plane analysis is a pivotal tool for identifying subgroups within a\nheterogeneous population, yet it presents challenges when applied to functional\ndata. In this paper, we consider a change-plane model within the framework of\nfunctional response quantile regression, capable of identifying and testing\nsubgroups in non-Gaussian functional responses with scalar predictors. The\nproposed model naturally extends the change-plane method to account for the\nheterogeneity in functional data. To detect the existence of subgroups, we\ndevelop a weighted average of the squared score test statistic, which has a\nclosed form and thereby reduces the computational stress. An alternating\ndirection method of multipliers algorithm is formulated to estimate the\nfunctional coefficients and the grouping parameters. We establish the\nasymptotic theory for the estimates based on the reproducing kernel Hilbert\nspace and derive the asymptotic distributions of the proposed test statistic\nunder both null and alternative hypotheses. Simulation studies are conducted to\nevaluate the performance of the proposed approach in subgroup identification\nand hypothesis test. The proposed methods are also applied to two datasets, one\nfrom a study on China stocks and another from the COVID-19 pandemic.",
      "generated_abstract": "ional response quantile regression (FRQR) is a flexible and powerful\nmodel for quantifying functional response to a treatment. The method is\nwidely used in applied and experimental medicine to study the effect of\ninterventions on outcomes in dynamic and stochastic settings, such as\ntherapeutic interventions or disease progression. The method is also\nwidely used in finance, risk management, and economics. FRQR models the\nfunctional response of a target response to a treatment, which may be\ncontinuous, discrete, or continuous-discrete. FRQR models are flexible,\npowerful, and easy to implement. However, the choice of the quantile function\nis often the most critical step in the FRQR analysis. In this work, we\npresent a novel quantile function that is based on the change-plane\nrepresentation of the quantile function. The proposed quantile function is\nint",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.225,
          "p": 0.3375,
          "f": 0.26999999520000006
        },
        "rouge-2": {
          "r": 0.05714285714285714,
          "p": 0.08620689655172414,
          "f": 0.06872851754230618
        },
        "rouge-l": {
          "r": 0.20833333333333334,
          "p": 0.3125,
          "f": 0.24999999520000007
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.cond-mat/dis-nn/2503.09684v1",
      "true_abstract": "When traffic is routed through a network that is susceptible to congestion,\nthe self-interested decisions made by individual users do not, in general,\nproduce the optimal flow. This discrepancy is quantified by the so-called\n\"price of anarchy.\" Here we consider whether the traffic produced by\nself-interested users is made better or worse when users have uncertain\nknowledge about the cost functions of the links in the network, and we define a\nparallel concept that we call the \"price of ignorance.\" We introduce a simple\nmodel in which fast, congestible links and slow, incongestible links are mixed\nrandomly in a large network and users plan their routes with finite uncertainty\nabout which of the two cost functions describes each link. One of our key\nfindings is that a small level of user ignorance universally improves traffic,\nregardless of the network composition. Further, there is an optimal level of\nignorance which, in our model, causes the self-interested user behavior to\ncoincide with the optimum. Many features of our model can be understood\nanalytically, including the optimal level of user ignorance and the existence\nof critical scaling near the percolation threshold for fast links, where the\npotential benefit of user ignorance is greatest.",
      "generated_abstract": "the dynamics of traffic on a network of randomly connected\ntraffic-congested nodes. When we add new nodes, we allow them to be randomly\nconnected to the existing nodes, but we do not know which of them are\ncongested. We show that the system remains in an attractive fixed point if we\nallow all nodes to be congested at the beginning, and if we allow the congested\nnodes to be randomly connected to the rest of the network. If we allow only\ncongested nodes to be randomly connected, we show that the system remains in a\nstable fixed point if and only if the congestion level is low enough that the\ncongestion level of the entire network is low enough. We also show that the\nsystem can be in a stable fixed point if and only if the congestion level is\nhigh enough that the congestion level of the entire network is high enough.\nThese results are qualitatively",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.175,
          "p": 0.35,
          "f": 0.23333332888888897
        },
        "rouge-2": {
          "r": 0.03763440860215054,
          "p": 0.06930693069306931,
          "f": 0.04878048324345367
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.3333333333333333,
          "f": 0.22222221777777784
        }
      }
    },
    {
      "paper_id": "cond-mat.quant-gas.physics/space-ph/2503.09553v1",
      "true_abstract": "Cosmic rays are deemed to be generated by a process known as ``Fermi\nacceleration\", in which charged particles scatter against magnetic fluctuations\nin astrophysical plasmas. The process itself is however universal, has both\nclassical and quantum formulations, and is at the basis of dynamical systems\nwith interesting mathematical properties, such as the celebrated Fermi-Ulam\nmodel. Despite its effectiveness in accelerating particles, Fermi acceleration\nhas so far eluded unambiguous verifications in laboratory settings. Here, we\nrealize the first fully controllable Fermi accelerator by colliding ultracold\natoms against engineered movable potential barriers. We demonstrate that our\nFermi accelerator, which is only 100 um in size, can produce ultracold atomic\njets with velocities above half a meter per second. Adding dissipation, we also\nexperimentally test Bell's general argument for the ensuing energy spectra,\nwhich is at the basis of any model of cosmic ray acceleration. On the one hand,\nour work effectively opens the window to the study of high energy astrophysics\nwith cold atoms, offering new capabilities for the understanding of phenomena\nsuch as diffusive acceleration at collisionless shocks. On the other, the\nperformance of our Fermi accelerator is competitive with those of best-in-class\naccelerating methods used in quantum technology and quantum colliders, but with\nsubstantially simpler implementation and virtually no upper limit.",
      "generated_abstract": "eleration is a process in which charged particles are accelerated\nto high energies by an external electric field. Fermi acceleration is a\nuniversal process that occurs in nature and in laboratories, yet the underlying\nmechanism remains elusive. Fermi acceleration has been observed in laboratories\nwith cold atoms, but it is challenging to measure the accelerating\nelectric field, which is the main driving force of Fermi acceleration. Here, we\npropose a measurement strategy to observe Fermi acceleration in cold atoms.\nOur strategy is to use a laser to generate a strong electric field that\naccelerates charged particles. The accelerated particles are then detected using\nan imaging spectroscopy. We propose to use a high-resolution imaging spectroscopy\nto detect the accelerating electric field, which can be measured in a\nsingle-atom experiment. We demonstrate the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20945945945945946,
          "p": 0.4025974025974026,
          "f": 0.27555555105343216
        },
        "rouge-2": {
          "r": 0.05527638190954774,
          "p": 0.09482758620689655,
          "f": 0.0698412651884105
        },
        "rouge-l": {
          "r": 0.18243243243243243,
          "p": 0.35064935064935066,
          "f": 0.23999999549787657
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.09849v1",
      "true_abstract": "After-action reviews (AARs) are professional discussions that help operators\nand teams enhance their task performance by analyzing completed missions with\npeers and professionals. Previous studies that compared different formats of\nAARs have mainly focused on human teams. However, the inclusion of robotic\nteammates brings along new challenges in understanding teammate intent and\ncommunication. Traditional AAR between human teammates may not be satisfactory\nfor human-robot teams. To address this limitation, we propose a new training\nreview (TR) tool, called the Virtual Spectator Interface (VSI), to enhance\nhuman-robot team performance and situational awareness (SA) in a simulated\nsearch mission. The proposed VSI primarily utilizes visual feedback to review\nsubjects' behavior. To examine the effectiveness of VSI, we took elements from\nAAR to conduct our own TR, designed a 1 x 3 between-subjects experiment with\nexperimental conditions: TR with (1) VSI, (2) screen recording, and (3)\nnon-technology (only verbal descriptions). The results of our experiments\ndemonstrated that the VSI did not result in significantly better team\nperformance than other conditions. However, the TR with VSI led to more\nimprovement in the subjects SA over the other conditions.",
      "generated_abstract": "This paper proposes a novel methodology for training human-robot teams using\na virtual spectator interface. The methodology employs a training simulator\n(T-Sim) and a virtual spectator (VS) to improve transparency of robot\nperformance. The T-Sim is used to train agents and their human users to\nachieve a desired performance. The VS is used to provide feedback to the\nagent. The proposed methodology is evaluated through two experiments. In the\nfirst experiment, we train 20 agents to achieve a desired performance by\nproviding feedback to them through the VS. In the second experiment, we train\n30 agents to achieve a desired performance by providing feedback to them through\nthe VS. The results show that the proposed methodology is effective in improving\ntransparency. The methodology is also scalable and can be applied to other\ntasks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17164179104477612,
          "p": 0.3333333333333333,
          "f": 0.22660098073430568
        },
        "rouge-2": {
          "r": 0.0335195530726257,
          "p": 0.058823529411764705,
          "f": 0.04270462170995858
        },
        "rouge-l": {
          "r": 0.17164179104477612,
          "p": 0.3333333333333333,
          "f": 0.22660098073430568
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.06599v1",
      "true_abstract": "The study examines the return connectedness between climate policy\nuncertainty (CPU), clean energy, fossil energy, and food markets. Using the\ntime-domain method of Diebold and Yilmaz (2012) and frequency-domain methods of\nBarun{\\'{i}}k and K{\\v{r}}hl{\\'{i}}k (2018), we find substantial spillover\neffects between these markets. Furthermore, high frequency domain is the\nprimary driver of overall connectedness. In addition, CPU is a net contributor\nof return shocks in the short term, whereas it turns to be a net recipient in\nthe medium and long terms. Across all frequencies, clean energy and oils are\nconsistent net recipients, while meat is a dominant net contributor.",
      "generated_abstract": "This paper investigates the spillover effects of climate policy uncertainty on\nthe energy and food markets using a time-frequency analysis. Our analysis\nhighlights the important role of climate policy uncertainty in affecting\nenergy and food markets. The results show that uncertainty about climate\npolicies can lead to changes in energy consumption patterns, which in turn\ninfluences the prices of fossil fuels. Additionally, uncertainty about food\nprices can affect consumer behavior and the food supply chain, resulting in\nchanges in food production and consumption. This study provides a comprehensive\nperspective on the impact of climate policy uncertainty on energy and food\nmarkets and offers insights into the potential for mitigating these effects\nthrough policy measures.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2465753424657534,
          "p": 0.27692307692307694,
          "f": 0.26086956023419455
        },
        "rouge-2": {
          "r": 0.06315789473684211,
          "p": 0.061855670103092786,
          "f": 0.062499995000542936
        },
        "rouge-l": {
          "r": 0.2465753424657534,
          "p": 0.27692307692307694,
          "f": 0.26086956023419455
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/PR/2412.13172v1",
      "true_abstract": "This paper derives the expressions of correlations between prices of two\nassets, returns of two assets, and price-return correlations of two assets that\ndepend on statistical moments and correlations of the current values, past\nvalues, and volumes of their market trades. The usual frequency-based\nexpressions of correlations of time series of prices and returns describe a\npartial case of our model when all trade volumes and past trade values are\nconstant. Such an assumptions are rather far from market reality, and its use\nresults in excess losses and wrong forecasts. Traders, banks, and funds that\nperform multi-million market transactions or manage billion-valued portfolios\nshould consider the impact of large trade volumes on market prices and returns.\nThe use of the market-based correlations of prices and returns of two assets is\nmandatory for them. The development of macroeconomic models and market\nforecasts like those being created by BlackRock's Aladdin, JP Morgan, and the\nU.S. Fed., is impossible without the use of market-based correlations of prices\nand returns of two assets.",
      "generated_abstract": "We introduce a new approach to the estimation of market-based correlations\nbetween prices and returns of two assets. Our method relies on the assumption\nthat prices and returns of two assets are correlated through a market-based\ncorrelation coefficient. The key contribution of our paper is the derivation of\na general expression for this correlation coefficient that is expressed as the\nsum of two market-based correlations between prices and returns of two assets.\nThis general expression is then used to derive a specific expression for the\nmarket-based correlation between prices of the two assets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20833333333333334,
          "p": 0.425531914893617,
          "f": 0.27972027530735005
        },
        "rouge-2": {
          "r": 0.09090909090909091,
          "p": 0.18055555555555555,
          "f": 0.12093022810340742
        },
        "rouge-l": {
          "r": 0.1875,
          "p": 0.3829787234042553,
          "f": 0.2517482473353221
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/RM/2411.06080v1",
      "true_abstract": "Portfolio diversification, traditionally measured through asset correlations\nand volatilitybased metrics, is fundamental to managing financial risk.\nHowever, existing diversification metrics often overlook non-numerical\nrelationships between assets that can impact portfolio stability, particularly\nduring market stresses. This paper introduces the lexical ratio (LR), a novel\nmetric that leverages textual data to capture diversification dimensions absent\nin standard approaches. By treating each asset as a unique document composed of\nsectorspecific and financial keywords, the LR evaluates portfolio\ndiversification by distributing these terms across assets, incorporating\nentropy-based insights from information theory. We thoroughly analyze LR's\nproperties, including scale invariance, concavity, and maximality,\ndemonstrating its theoretical robustness and ability to enhance risk-adjusted\nportfolio returns. Using empirical tests on S&P 500 portfolios, we compare LR's\nperformance to established metrics such as Markowitz's volatility-based\nmeasures and diversification ratios. Our tests reveal LR's superiority in\noptimizing portfolio returns, especially under varied market conditions. Our\nfindings show that LR aligns with conventional metrics and captures unique\ndiversification aspects, suggesting it is a viable tool for portfolio managers.",
      "generated_abstract": "We propose a novel methodology to analyze portfolio diversification. By\ninstead of analyzing the correlations among the assets of a portfolio, we\nexplore the relationships among the assets of different portfolios. We call\nthis approach the lexical ratio. By taking the ratio between the lexical\ncorrelations of the assets in the two portfolios, we obtain a measure of the\ndivergence of the portfolios. By comparing the lexical ratios of the two\nportfolios, we can assess the impact of the assets of the portfolios on the\nportfolio. We apply our methodology to a set of financial assets and we show\nthat the lexical ratio is a robust measure of portfolio diversification and a\ngood predictor of risk and returns. In addition, we show that the lexical\nratio provides a simple and intuitive explanation of the portfolio effect on\nrisk.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22388059701492538,
          "p": 0.4838709677419355,
          "f": 0.30612244465431077
        },
        "rouge-2": {
          "r": 0.041666666666666664,
          "p": 0.06481481481481481,
          "f": 0.050724632917454764
        },
        "rouge-l": {
          "r": 0.20149253731343283,
          "p": 0.43548387096774194,
          "f": 0.27551019975635155
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.07664v1",
      "true_abstract": "The Antibiotic Resistance Microbiology Dataset (ARMD) is a de-identified\nresource derived from electronic health records (EHR) that facilitates research\ninto antimicrobial resistance (AMR). ARMD encompasses data from adult patients,\nfocusing on microbiological cultures, antibiotic susceptibilities, and\nassociated clinical and demographic features. Key attributes include organism\nidentification, susceptibility patterns for 55 antibiotics, implied\nsusceptibility rules, and de-identified patient information. This dataset\nsupports studies on antimicrobial stewardship, causal inference, and clinical\ndecision-making. ARMD is designed to be reusable and interoperable, promoting\ncollaboration and innovation in combating AMR. This paper describes the\ndataset's acquisition, structure, and utility while detailing its\nde-identification process.",
      "generated_abstract": "c resistance is a growing global health concern, impacting both\nphysical health and economic prosperity. While microbiome studies have\ndemonstrated the role of microbes in resistance, the role of host factors,\nincluding antibiotic resistance, in driving resistance is not well understood.\nWe present the Antibiotic Resistance Microbiology Dataset (ARMD), a\nde-identified dataset of electronic health records (EHRs) of over 440,000\npatients who received antibiotics in the US between 2006 and 2018. We describe\nthe methodologies used to identify and link antibiotic resistance to specific\nantibiotics and to the EHR, and the EHR metadata associated with antibiotic\nresistance. We also describe a framework for understanding the complex\nrelationship between antibiotic resistance and the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23809523809523808,
          "p": 0.273972602739726,
          "f": 0.25477706508823894
        },
        "rouge-2": {
          "r": 0.07142857142857142,
          "p": 0.06666666666666667,
          "f": 0.06896551224732497
        },
        "rouge-l": {
          "r": 0.2261904761904762,
          "p": 0.2602739726027397,
          "f": 0.24203821158505426
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PR/2408.01642v2",
      "true_abstract": "The additive process generalizes the L\\'evy process by relaxing its\nassumption of time-homogeneous increments and hence covers a larger family of\nstochastic processes. Recent research in option pricing shows that modeling the\nunderlying log price with an additive process has advantages in easier\nconstruction of the risk-neural measure, an explicit option pricing formula and\ncharacteristic function, and more flexibility to fit the implied volatility\nsurface. Still, the challenge of calibrating an additive model arises from its\ntime-dependent parameterization, for which one has to prescribe parametric\nfunctions for the term structure. For this, we propose the neural term\nstructure model to utilize feedforward neural networks to represent the term\nstructure, which alleviates the difficulty of designing parametric functions\nand thus attenuates the misspecification risk. Numerical studies with S\\&P 500\noption data are conducted to evaluate the performance of the neural term\nstructure.",
      "generated_abstract": "We propose a neural term structure model to pricing European options. Our\nmodel is a generalization of the additive model for option pricing, and can be\nused to model any additive process. We implement our model by using a\nreversed-time-delay neural network with two hidden layers and two output layers.\nOur model is trained by backpropagation, and we use a modified backpropagation\nalgorithm to mitigate the problem of vanishing gradients. The proposed model\ncan be trained and tested on a stock options dataset, and we compare our\nresults to those of the additive model and the Neural Black-Scholes model.\nResults indicate that our model provides a better option pricing model than the\nadditive model and the Neural Black-Scholes model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21505376344086022,
          "p": 0.3076923076923077,
          "f": 0.2531645521190515
        },
        "rouge-2": {
          "r": 0.06153846153846154,
          "p": 0.07766990291262135,
          "f": 0.06866952296413673
        },
        "rouge-l": {
          "r": 0.1827956989247312,
          "p": 0.26153846153846155,
          "f": 0.21518986857474776
        }
      }
    },
    {
      "paper_id": "math.HO.math/HO/2502.17478v1",
      "true_abstract": "\"Math is not a spectator sport.\" \"Lecturing is educational malpractice.\"\nSlogans like these rally some mathematicians to teach classes that feature\n\"active learning\", where lecturing is eschewed for student participation. Yet\nas much as I believe that students must do math to learn math, I also find\nblanket statements to be more about bandwagons than considered reflection on\nteaching. In this column, published in the Fall 2021 AWM Newsletter, I urge us\nto think through the math we offer students and how we set up students to\nlearn. Although I draw primarily from my experiences teaching proofs in\nabstract algebra and real analysis, the scenarios extend to other topics in\nfirst year undergraduate education and beyond.",
      "generated_abstract": "introduces a new concept of \"please\", and describes its use in a\napplication to linear algebra.\n  The \"please\" concept is defined in terms of the \"please\" operator on the\ncategory of $K$-linear vector spaces, where $K$ is any field. In particular,\nthis includes the usual notion of \"please\" in the category of vector spaces.\n  We then show that the \"please\" operator is fully faithful and essentially\nsurjective. In particular, it maps the category of finite dimensional $K$-linear\nvector spaces to the category of finite dimensional vector spaces, and it\nmaps the category of finite dimensional vector spaces to the category of\nfinite dimensional $K$-linear vector spaces.\n  We then show that the \"please\" operator preserves exactness, and thus is\nfully faithful. In particular, it maps the category of exact sequences of finite\ndimensional $K$-linear vector spaces",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12903225806451613,
          "p": 0.21818181818181817,
          "f": 0.16216215749178245
        },
        "rouge-2": {
          "r": 0.008620689655172414,
          "p": 0.012048192771084338,
          "f": 0.010050246393780287
        },
        "rouge-l": {
          "r": 0.11827956989247312,
          "p": 0.2,
          "f": 0.14864864397826896
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/AS/2502.03871v1",
      "true_abstract": "We consider a phase-shift mixing model for linear sensor arrays in the\ncontext of blind source extraction. We derive a blind Capon beamformer that\nseeks the direction where the output is independent of the other signals in the\nmixture. The algorithm is based on Independent Component Extraction and imposes\nan orthogonal constraint, thanks to which it optimizes only one real-valued\nparameter related to the angle of arrival. The Cram\\'er-Rao lower bound for the\nmean interference-to-signal ratio is derived. The algorithm and the bound are\ncompared with conventional blind and direction-of-arrival\nestimation+beamforming methods, showing improvements in terms of extraction\naccuracy. An application is demonstrated in frequency-domain speaker extraction\nin a low-reverberation room.",
      "generated_abstract": "aper, we propose a blind Capon beamformer based on independent\ncomponent extraction (ICXE), which is a simple and efficient method to\nreconstruct a complex signal from its components. The ICXE method is a\ngeneralization of the Capon beamformer, which can be applied to a wide range of\nsignal processing tasks. In the proposed ICXE beamformer, the phase and\namplitude components of the complex signal are extracted independently and\ncombined in a single beamformer. This approach provides a more efficient and\neffective method for signal processing than the conventional Capon beamformer.\nThe proposed ICXE beamformer is evaluated using various signal processing\nexamples, such as linear frequency modulation (LFM), frequency-domain blind\ncapon beamformer (FD-BCB), and blind capon beamformer (BCB), which are\ncomparable to the conventional Capon beamformer. Finally, the proposed IC",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26582278481012656,
          "p": 0.28378378378378377,
          "f": 0.27450979892690847
        },
        "rouge-2": {
          "r": 0.06481481481481481,
          "p": 0.06086956521739131,
          "f": 0.06278026406322307
        },
        "rouge-l": {
          "r": 0.26582278481012656,
          "p": 0.28378378378378377,
          "f": 0.27450979892690847
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.17830v1",
      "true_abstract": "Hypothesis tests and confidence intervals are ubiquitous in empirical\nresearch, yet their connection to subsequent decision-making is often unclear.\nWe develop a theory of certified decisions that pairs recommended decisions\nwith inferential guarantees. Specifically, we attach P-certificates -- upper\nbounds on loss that hold with probability at least $1-\\alpha$ -- to recommended\nactions. We show that such certificates allow \"safe,\" risk-controlling adoption\ndecisions for ambiguity-averse downstream decision-makers. We further prove\nthat it is without loss to limit attention to P-certificates arising as minimax\ndecisions over confidence sets, or what Manski (2021) terms \"as-if decisions\nwith a set estimate.\" A parallel argument applies to E-certified decisions\nobtained from e-values in settings with unbounded loss.",
      "generated_abstract": "The concept of certified decision is emerging as a powerful tool for\nfair and transparent decision-making. It offers a way to measure the\nverifiability of a decision, enabling decision-makers to assess the accuracy of\ntheir decisions. This paper introduces the concept of certified decisions,\nproviding a framework for the analysis of the trade-off between verifiability\nand accuracy. We illustrate the concept through an example from the field of\nsocial justice, focusing on the decision of whether to charge a tax on\nwealth. We show how certified decisions can be used to assess the fairness of\nthis decision-making process and provide recommendations for improving\nverifiability.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.24615384615384617,
          "f": 0.2091503219103765
        },
        "rouge-2": {
          "r": 0.026785714285714284,
          "p": 0.030612244897959183,
          "f": 0.02857142359365166
        },
        "rouge-l": {
          "r": 0.17045454545454544,
          "p": 0.23076923076923078,
          "f": 0.19607842648553986
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.astro-ph/EP/2503.10309v1",
      "true_abstract": "Observational data provided by JWST instruments continue to challenge\ntheories and models of cloud formation in sub-stellar atmospheres, requiring\nmore sophisticated approaches in an effort to understand their spatial\ncomplexity. However, to date, most cloud microphysical models using the moment\nmethod for sub-stellar atmospheres have assumed a monodisperse size\ndistribution, neglecting polydisperse properties. We aim to extend beyond the\ncommon assumption of a monodisperse size distribution and analyse cloud\nmicrophysical processes assuming an exponential distribution. We derive\nexpressions for the zeroth and first moments of condensation/evaporation and\ncollisional growth processes under the assumption of an exponential size\ndistribution. We then compare the differences between monodisperse and\nexponential distribution microphysics using a simple one-dimensional (1D)\ncolumn model applied to a Y-dwarf KCl cloud scenario. We find that adopting an\nexponential distribution modifies condensation/evaporation rates by a factor of\n$\\approx$0.9 and collisional growth rates by factors of $\\approx$1.1 (Kn $\\ll$\n1) and $\\approx$0.92 (Kn $\\gg$ 1) for Brownian coagulation and $\\approx$0.85\nfor gravitational coalescence, compared to the monodisperse case. In our\nspecific test cases, we find relative differences of a maximum 10-12% in total\nnumber density and 2-3% in mean radius of the cloud particles between the\nmonodisperse and exponential distributions. Our results offer a simple way to\ntake into account an assumed exponential size distribution for sub-stellar\natmospheric cloud microphysics using the two-moment method. In follow up\nstudies, we will examine more complex distributions, such as the log-normal and\ngamma distributions, that require more than two moments to characterise\nself-consistently.",
      "generated_abstract": "t the first polydisperse moment methods for sub-stellar atmospheres\ncandles using a single cloud model to calculate cloud microphysical properties.\nThe methods are based on the exponential distribution of the size distribution\nof particles, which is commonly used to model the size distribution of\ncondensible gas particles in atmospheres. In contrast to the commonly used\nGaussian distribution of the size distribution of condensible gas particles,\nthe exponential distribution is more realistic for atmospheres with\npolydisperse clouds, since it accounts for the effects of particle size\nanisotropy and aggregation. In addition, the exponential distribution allows\nfor the incorporation of cloud optical properties, such as the optical\nthickness, into the model, which are often ignored in the Gaussian distribution.\nUsing the Monte Carlo method, we calculate the properties of polydisperse\nclouds using the exponential distribution. We present the first polydis",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1917808219178082,
          "p": 0.3888888888888889,
          "f": 0.2568807295210841
        },
        "rouge-2": {
          "r": 0.06140350877192982,
          "p": 0.1206896551724138,
          "f": 0.08139534436722579
        },
        "rouge-l": {
          "r": 0.18493150684931506,
          "p": 0.375,
          "f": 0.24770641759447865
        }
      }
    },
    {
      "paper_id": "cs.SD.cs/DL/2502.17726v1",
      "true_abstract": "The Musical Instrument Digital Interface (MIDI), introduced in 1983,\nrevolutionized music production by allowing computers and instruments to\ncommunicate efficiently. MIDI files encode musical instructions compactly,\nfacilitating convenient music sharing. They benefit Music Information Retrieval\n(MIR), aiding in research on music understanding, computational musicology, and\ngenerative music. The GigaMIDI dataset contains over 1.4 million unique MIDI\nfiles, encompassing 1.8 billion MIDI note events and over 5.3 million MIDI\ntracks. GigaMIDI is currently the largest collection of symbolic music in MIDI\nformat available for research purposes under fair dealing. Distinguishing\nbetween non-expressive and expressive MIDI tracks is challenging, as MIDI files\ndo not inherently make this distinction. To address this issue, we introduce a\nset of innovative heuristics for detecting expressive music performance. These\ninclude the Distinctive Note Velocity Ratio (DNVR) heuristic, which analyzes\nMIDI note velocity; the Distinctive Note Onset Deviation Ratio (DNODR)\nheuristic, which examines deviations in note onset times; and the Note Onset\nMedian Metric Level (NOMML) heuristic, which evaluates onset positions relative\nto metric levels. Our evaluation demonstrates these heuristics effectively\ndifferentiate between non-expressive and expressive MIDI tracks. Furthermore,\nafter evaluation, we create the most substantial expressive MIDI dataset,\nemploying our heuristic, NOMML. This curated iteration of GigaMIDI encompasses\nexpressively-performed instrument tracks detected by NOMML, containing all\nGeneral MIDI instruments, constituting 31% of the GigaMIDI dataset, totalling\n1,655,649 tracks.",
      "generated_abstract": "erformance detection is essential for the development of\nrecommendation systems and music recommendation engines. However, existing\nmethods for this task often suffer from low performance on unseen data, due to\nthe limited training data. To address this issue, we propose a new dataset,\nGigaMIDI, which consists of 1.5 million recordings from the MIDI format and\ncomprises 100,000 musical performances. Our dataset is significantly larger\nthan existing datasets and offers new challenges in the field of music\nperformance detection. We propose a new model, GigaMIDI-2, that achieves\nstate-of-the-art performance on the GigaMIDI evaluation dataset, achieving a\nF1-score of 0.86 and a Recall of 0.97. Furthermore, we provide a dataset\nbenchmark, GigaMIDI-Bench,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1870967741935484,
          "p": 0.3625,
          "f": 0.24680850614757818
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.06481481481481481,
          "f": 0.0440251527471228
        },
        "rouge-l": {
          "r": 0.18064516129032257,
          "p": 0.35,
          "f": 0.23829786784970577
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/LG/2503.10632v1",
      "true_abstract": "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of\nlearnable activation functions with the potential to capture more complex\nrelationships from data. Although KANs are useful in finding symbolic\nrepresentations and continual learning of one-dimensional functions, their\neffectiveness in diverse machine learning (ML) tasks, such as vision, remains\nquestionable. Presently, KANs are deployed by replacing multilayer perceptrons\n(MLPs) in deep network architectures, including advanced architectures such as\nvision Transformers (ViTs). In this paper, we are the first to design a general\nlearnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate\non any choice of basis. However, the computing and memory costs of training\nthem motivated us to propose a more modular version, and we designed particular\nlearnable attention, called Fourier-KArAt. Fourier-KArAt and its variants\neither outperform their ViT counterparts or show comparable performance on\nCIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures'\nperformance and generalization capacity by analyzing their loss landscapes,\nweight distributions, optimizer path, attention visualization, and spectral\nbehavior, and contrast them with vanilla ViTs. The goal of this paper is not to\nproduce parameter- and compute-efficient attention, but to encourage the\ncommunity to explore KANs in conjunction with more advanced architectures that\nrequire a careful understanding of learnable activations. Our open-source code\nand implementation details are available on: https://subhajitmaity.me/KArAt",
      "generated_abstract": "ansformers (VT) are widely used in vision-language modeling. While\nattention plays a critical role in VT, recent work suggests that it is not\nsufficiently expressive for downstream tasks. This observation motivates\nattention-free transformers, which leverage self-attention to learn\nrepresentations instead of using attention. However, it remains unclear how to\nimplement an attention-free transformer without hand-designed weights. To this\nend, we propose a new attention mechanism, the Kolmogorov-Arnold attention,\nwhich is based on the Kolmogorov-Arnold theorem and shares similar properties\nwith attention. We further show that the Kolmogorov-Arnold attention can be\nefficiently trained without hand-designed weights, and achieves competitive\nperformance compared to attention-based transformers. Furthermore, we show\nthat",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16883116883116883,
          "p": 0.30952380952380953,
          "f": 0.21848739039050924
        },
        "rouge-2": {
          "r": 0.009615384615384616,
          "p": 0.0196078431372549,
          "f": 0.012903221391052499
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.2619047619047619,
          "f": 0.184873945012358
        }
      }
    },
    {
      "paper_id": "cs.LG.math/OC/2503.09737v1",
      "true_abstract": "Soccer analysis tools emphasize metrics such as expected goals, leading to an\noverrepresentation of attacking players' contributions and overlooking players\nwho facilitate ball control and link attacks. Examples include Rodri from\nManchester City and Palhinha who just transferred to Bayern Munich. To address\nthis bias, we aim to identify players with pivotal roles in a soccer team,\nincorporating both spatial and temporal features.\n  In this work, we introduce a GNN-based framework that assigns individual\ncredit for changes in expected threat (xT), thus capturing overlooked yet vital\ncontributions in soccer. Our pipeline encodes both spatial and temporal\nfeatures in event-centric graphs, enabling fair attribution of non-scoring\nactions such as defensive or transitional plays. We incorporate centrality\nmeasures into the learned player embeddings, ensuring that ball-retaining\ndefenders and defensive midfielders receive due recognition for their overall\nimpact. Furthermore, we explore diverse GNN variants-including Graph Attention\nNetworks and Transformer-based models-to handle long-range dependencies and\nevolving match contexts, discussing their relative performance and\ncomputational complexity. Experiments on real match data confirm the robustness\nof our approach in highlighting pivotal roles that traditional attacking\nmetrics typically miss, underscoring the model's utility for more comprehensive\nsoccer analytics.",
      "generated_abstract": "ayers are the most valuable assets in professional soccer teams,\nand their evaluation plays a crucial role in selecting the best players.\nHowever, evaluating the performance of a soccer player is a complex task due\nto the complex nature of the game and the diverse characteristics of the\nplayers. Traditional approaches often focus on individual statistics and\nanalysis, which cannot provide a comprehensive view of a player's\nperformance. Additionally, these approaches often rely on limited data and\nheterogeneous information, which may lead to unreliable results. To address\nthese challenges, we introduce GoalNet, a novel soccer player evaluation\nsystem that integrates graph neural networks (GNNs) with goal-oriented\nrepresentations to capture player-goal interactions. The proposed GoalNet\nunveils the hidden players with high potential, enhancing player evaluation.\nOur evaluation results demonstrate that GoalNet significantly impro",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17123287671232876,
          "p": 0.27472527472527475,
          "f": 0.21097045940429782
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.031746031746031744,
          "f": 0.025806446787930138
        },
        "rouge-l": {
          "r": 0.1643835616438356,
          "p": 0.26373626373626374,
          "f": 0.20253164083889702
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/RM/2501.11552v1",
      "true_abstract": "We explore the interplay between sovereign debt default/renegotiation and\nenvironmental factors (e.g., pollution from land use, natural resource\nexploitation). Pollution contributes to the likelihood of natural disasters and\ninfluences economic growth rates. The country can default on its debt at any\ntime while also deciding whether to invest in pollution abatement. The\nframework provides insights into the credit spreads of sovereign bonds and\nexplains the observed relationship between bond spread and a country's climate\nvulnerability. Through calibration for developing and low-income countries, we\ndemonstrate that there is limited incentive for these countries to address\nclimate risk, and the sensitivity of bond spreads to climate vulnerability\nremains modest. Climate risk does not play a relevant role on the decision to\ndefault on sovereign debt. Financial support for climate abatement expenditures\ncan effectively foster climate adaptation actions, instead renegotiation\nconditional upon pollution abatement does not produce any effect.",
      "generated_abstract": "y examines the impact of sovereign debt default on climate risk\nthrough the lens of the climate risk premium. We construct a dynamic\nintertemporal optimal climate-risk management model, in which sovereign debt\ndefaults influence the probability of climate risk events and the expected\nclimate risk premium. We use the climate risk premium as a proxy for climate\nrisk, and the probability of climate risk events as an indicator of sovereign\ndebt default. We estimate a dynamic model that integrates climate risk and\nsovereign debt dynamics. Our results show that climate risk premium is\nsignificantly higher for countries with higher sovereign debt ratios. The\nresults indicate that climate risk premium declines as sovereign debt ratios\nincrease. The climate risk premium is also higher for countries that have\nhigher levels of GDP per capita",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18269230769230768,
          "p": 0.3064516129032258,
          "f": 0.228915657970678
        },
        "rouge-2": {
          "r": 0.04929577464788732,
          "p": 0.07216494845360824,
          "f": 0.058577401034996285
        },
        "rouge-l": {
          "r": 0.18269230769230768,
          "p": 0.3064516129032258,
          "f": 0.228915657970678
        }
      }
    },
    {
      "paper_id": "math.CO.math/AC/2503.01647v1",
      "true_abstract": "Classical results of Cauchy and Dehn imply that the 1-skeleton of a convex\npolyhedron $P$ is rigid i.e. every continuous motion of the vertices of $P$ in\n$\\mathbb R^3$ which preserves its edge lengths results in a polyhedron which is\ncongruent to $P$. This result was extended to convex poytopes in $\\mathbb R^d$\nfor all $d\\geq 3$ by Whiteley, and to generic realisations of 1-skeletons of\nsimplicial $(d-1)$-manifolds in $\\mathbb R^{d}$ by Kalai for $d\\geq 4$ and\nFogelsanger for $d\\geq 3$. We will generalise Kalai's result by showing that,\nfor all $d\\geq 4$ and any fixed $1\\leq k\\leq d-3$, every generic realisation of\nthe $k$-skeleton of a simplicial $(d-1)$-manifold in $\\mathbb R^{d}$ is volume\nrigid, i.e. every continuous motion of its vertices in $\\mathbb R^d$ which\npreserves the volumes of its $k$-faces results in a congruent realisation. In\naddition, we conjecture that our result remains true for $k=d-2$ and verify\nthis conjecture when $d=4,5,6$.",
      "generated_abstract": "We prove a volume rigidity result for simplicial manifolds with boundary\nat a given dimension, generalizing a result of Yau.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08433734939759036,
          "p": 0.4117647058823529,
          "f": 0.13999999717800005
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.060240963855421686,
          "p": 0.29411764705882354,
          "f": 0.09999999717800007
        }
      }
    },
    {
      "paper_id": "stat.AP.q-bio/OT/2503.08378v1",
      "true_abstract": "Objectives: To examine the distribution, temporal associations, and\nage/sex-specific patterns of multiple long-term conditions (MLTCs) in adults\nwith intellectual disability (ID).\n  Study Design: Observational study using longitudinal healthcare data.\nMethods: Analysis of 18144 adults with ID (10168 males and 7976 females)\nidentified in the Clinical Practice Research Datalink, linked to Hospital\nEpisode Statistics Admitted Patient Care and Outpatient data (2000-2021). We\nused temporal analysis to establish directional associations among 40 long-term\nconditions, stratified by sex and age groups (under 45, 45-64, 65 and over).\n  Results: The high prevalence of enduring mental illness across all age groups\nis an important finding unique to this population. In males, mental illness\noccurred along with upper gastrointestinal conditions (specifically reflux\ndisorders), while in females, mental illness presented alongside reflux\ndisorders, chronic pain, and endocrine conditions such as thyroid problems.\nAmong young males with intellectual disability, the combination of cerebral\npalsy with dysphagia, epilepsy, chronic constipation, and chronic pneumonia\nrepresents a distinctive pattern. In those aged 45-64, we observed early onset\nof lifestyle conditions like diabetes and hypertension, though notably these\nconditions co-occurred with mental illness and anaemia at rates exceeding those\nin the general population. The health conditions in those aged 65 and over\nlargely mirrored those seen in the general aging population.\n  Conclusions: Our findings highlight the complex patterns of MLTCs in this\npopulation, revealing sex-specific associations across age groups, and\nidentified temporal associations, thus providing insights into disease\nprogression, which can inform targeted prevention strategies and interventions\nto prevent premature mortality.",
      "generated_abstract": "d: The prevalence of multiple long-term conditions (MLTC) is\nhigh in people with intellectual disability (ID), yet little is known about\ntheir comorbidity patterns and temporal associations. Objectives: To\ncharacterise the prevalence of MLTC, their temporal associations and comorbidity\npatterns in adults with ID. Design: Observational study. Setting: UK, 137\nuniversities and 15 adult ID services. Methods: We collected data on the\nprevalence of MLTC and their temporal associations and comorbidity patterns\nusing a questionnaire and questionnaire administration. Data were analysed\nusing multivariable logistic regression and meta-analysis. Results: Overall,\n44.5% of adults with ID had MLTC, of which 12.6% had 10 or more MLTC. MLTC",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1791907514450867,
          "p": 0.43661971830985913,
          "f": 0.2540983565294948
        },
        "rouge-2": {
          "r": 0.05106382978723404,
          "p": 0.1276595744680851,
          "f": 0.07294832418584478
        },
        "rouge-l": {
          "r": 0.17341040462427745,
          "p": 0.4225352112676056,
          "f": 0.24590163521801936
        }
      }
    },
    {
      "paper_id": "math.PR.stat/TH/2502.16916v1",
      "true_abstract": "This paper establishes sharp dimension-free concentration inequalities and\nexpectation bounds for the deviation of the sum of simple random tensors from\nits expectation. As part of our analysis, we use generic chaining techniques to\nobtain a sharp high-probability upper bound on the suprema of multi-product\nempirical processes. In so doing, we generalize classical results for quadratic\nand product empirical processes to higher-order settings.",
      "generated_abstract": "We prove a sharp concentration inequality for the sum of i.i.d. simple random\ntensor-valued random variables. The key ingredient is a novel concentration\ninequality for the empirical distribution function of random vectors, which is\nbased on a non-asymptotic lower bound on the variance of the empirical\ndistribution function. The proof is based on a novel coupling construction and\nrelies on the fact that the joint distribution function of the tensor product\nvariables converges to a Gaussian distribution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3,
          "p": 0.3333333333333333,
          "f": 0.31578946869806096
        },
        "rouge-2": {
          "r": 0.13114754098360656,
          "p": 0.125,
          "f": 0.12799999500288017
        },
        "rouge-l": {
          "r": 0.3,
          "p": 0.3333333333333333,
          "f": 0.31578946869806096
        }
      }
    },
    {
      "paper_id": "math.LO.math/RA/2503.10528v1",
      "true_abstract": "The first-order model theory of modules has been studied for decades. More\nrecently, the model theoretic study of nonelementary classes of\nmodules--especially Abstract Elementary Classes of modules--has produced\ninteresting results. This survey aims to discuss these recent results and give\nan introduction to the framework of Abstract Elementary Classes for module\ntheorists.",
      "generated_abstract": "In this paper we introduce a module-theoretic approach to elementary\nclassification, focusing on the case of a non-commutative field. In particular,\nwe consider a field $K$ equipped with a maximal ideal $M$ and a non-zero\n$K$-module $E$ such that $E/M$ is finite-dimensional over $K$. In this\nsetting, we introduce a notion of elementary classes in $K[E",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.12195121951219512,
          "f": 0.12345678512421908
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.12195121951219512,
          "f": 0.12345678512421908
        }
      }
    },
    {
      "paper_id": "math.OC.econ/GN/2502.12035v1",
      "true_abstract": "The transition to a low-carbon economy necessitates effective carbon capture\nand storage (CCS) solutions, particularly for hard-to-abate sectors. Herein,\npipeline networks are indispensable for cost-efficient $CO_2$ transportation\nover long distances. However, there is deep uncertainty regarding which\nindustrial sectors will participate in such systems. This poses a significant\nchallenge due to substantial investments as well as the lengthy planning and\ndevelopment timelines required for $CO_2$ pipeline projects, which are further\nconstrained by limited upgrade options for already built infrastructure. The\neconomies of scale inherent in pipeline construction exacerbate these\nchallenges, leading to potential regret over earlier decisions. While numerous\nmodels were developed to optimize the initial layout of pipeline infrastructure\nbased on known demand, a gap exists in addressing the incremental development\nof infrastructure in conjunction with deep uncertainty. Hence, this paper\nintroduces a novel optimization model for $CO_2$ pipeline infrastructure\ndevelopment, minimizing regret as its objective function and incorporating\nvarious upgrade options, such as looping and pressure increases. The model's\neffectiveness is also demonstrated by presenting a comprehensive case study of\nGermany's cement and lime industries. The developed approach quantitatively\nillustrates the trade-off between different options, which can help in deriving\neffective strategies for $CO_2$ infrastructure development.",
      "generated_abstract": "igate the problem of optimizing the pipeline network that minimizes\nthe average cost of carbon dioxide transport. This problem arises in the context\nof carbon capture and storage (CCS) where the pipeline network must be\nminimally regulated to ensure that carbon dioxide is transported to a storage\nsite with a minimum cost. We consider a network of $N$ pipelines that carry\ncarbon dioxide from a production site to a storage site. Each pipeline is\nconnected to a regulator, which determines the flow rate of the pipeline. The\nproblem is to find the minimum cost pipeline network that ensures that the\ntotal cost of carbon dioxide transport is minimized. We show that the\noptimal solution to this problem is the minimum-cost pipeline network that\nminimizes the average cost of carbon dioxide transport, subject to\nconstraints on the flow rates of the pipelines. We provide a polynomial",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.2727272727272727,
          "f": 0.17647058385813158
        },
        "rouge-2": {
          "r": 0.026041666666666668,
          "p": 0.043859649122807015,
          "f": 0.03267973388696722
        },
        "rouge-l": {
          "r": 0.13043478260869565,
          "p": 0.2727272727272727,
          "f": 0.17647058385813158
        }
      }
    },
    {
      "paper_id": "nucl-ex.nucl-ex/2503.07841v1",
      "true_abstract": "We present the first measurements with a new collinear laser spectroscopy\nsetup at the Argonne Tandem Linac Accelerator System utilizing its unique\ncapability to deliver neutron-rich refractory metal isotopes produced by the\nspontaneous fission of 252Cf. We measured isotope shifts from optical spectra\nfor nine radioactive ruthenium isotopes 106-114Ru, reaching deep into the\nmid-shell region. The extracted charge radii are in excellent agreement with\npredictions from the Brussels-Skyrme-on-a-Grid models that account for the\ntriaxial deformation of nuclear ground states in this region. We show that\ntriaxial deformation impacts charge radii in models that feature shell effects,\nin contrast to what could be concluded from a liquid drop analysis. This\nindicates that this exotic type of deformation should not be neglected in\nregions where it is known to occur, even if its presence cannot be\nunambiguously inferred through laser spectroscopy.",
      "generated_abstract": "t an investigation of the charge radii of neutron-rich Ru isotopes\nwith $Z=25-51$ in the framework of the two-center-two-electron (2C2E)\nframework, focusing on the effects of the triaxiality of the nuclear\npotential. We use the in-medium BS-Gaussian approach to calculate the\n2C2E-based charge radii for 24 neutron-rich Ru isotopes, from $Z=25$ to\n$Z=49$. The results are compared to the experimental charge radii obtained\nwithin the generalized Born-Oppenheimer (GBO) approach. The comparison is\nperformed within the $Z=25$ and $Z=49$ isotopes using the two-body 2C2E\nmethod, and within the $Z=35$ isot",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1568627450980392,
          "p": 0.2807017543859649,
          "f": 0.20125785703571863
        },
        "rouge-2": {
          "r": 0.007462686567164179,
          "p": 0.0125,
          "f": 0.009345789710894125
        },
        "rouge-l": {
          "r": 0.1568627450980392,
          "p": 0.2807017543859649,
          "f": 0.20125785703571863
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.02609v1",
      "true_abstract": "People are influenced by their peers when making decisions. In this paper, we\nstudy the linear-in-means model which is the standard empirical model of peer\neffects. As data on the underlying social network is often difficult to come\nby, we focus on data that only captures an agent's choices. Under exogenous\nagent participation variation, we study two questions. We first develop a\nrevealed preference style test for the linear-in-means model. We then study the\nidentification properties of the linear-in-means model. With sufficient\nparticipation variation, we show how an analyst is able to recover the\nunderlying network structure and social influence parameters from choice data.\nOur identification result holds when we allow the social network to vary across\ncontexts. To recover predictive power, we consider a refinement which allows us\nto extrapolate the underlying network structure across groups and provide a\ntest of this version of the model.",
      "generated_abstract": "ence of social networks is a key feature of complex systems, but\nmost research on the formation and structure of these networks focuses on\nstatistical and mathematical aspects. In this paper, we study the\nunderlying economic mechanism through which individuals form social networks.\nWe show that the emergence of social networks is a byproduct of the\ninteraction of two distinct forces:\n\\begin{enumerate}\n\\item The interaction of individual preferences with the market structure,\nwhere individuals' preferences can be described by a rational expectations\nmodel and the market structure by a price mechanism.\n\\item The emergence of a new market that emerges when an individual's\npreferences are rationalized by the market structure.\n\\end{enumerate}\nWe analyze the equilibrium structure of this model in terms of the\ninteractions between these two forces. We show that, in the limit of an\ninfinite population, the resulting network is a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2717391304347826,
          "p": 0.30120481927710846,
          "f": 0.28571428072751026
        },
        "rouge-2": {
          "r": 0.07575757575757576,
          "p": 0.07936507936507936,
          "f": 0.07751937484766575
        },
        "rouge-l": {
          "r": 0.2717391304347826,
          "p": 0.30120481927710846,
          "f": 0.28571428072751026
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/RM/2412.04263v1",
      "true_abstract": "A simple model-free and distribution-free statistic, the functional\nrelationship between the number of \"effective\" degrees of freedom and portfolio\nsize, or N*(N), is used to discriminate between two alternative models for the\ncorrelation of daily cryptocurrency returns within a retail universe of defined\nby the list of tradable assets available to account holders at the Robinhood\nbrokerage. The average pairwise correlation between daily cryptocurrency\nreturns is found to be high (of order 60%) and the data collected supports\ndescription of the cross-section of returns by a simple isotropic correlation\nmodel distinct from a decomposition into a linear factor model with additive\nnoise with high confidence. This description appears to be relatively stable\nthrough time.",
      "generated_abstract": "This paper investigates the correlation structure in retail cryptocurrency\nmarket returns using a data-driven approach. We employ factor-based\napproaches to identify correlated factors and apply them to test their\ncompatibility with observed correlations. While our analysis shows a strong\ncorrelation between the Bitcoin price and the Bitcoin Futures, we find no\nevidence of an inverse correlation between Bitcoin and Bitcoin Futures. We\nalso find no evidence of a significant correlation between Bitcoin and Ethereum\nFutures, and there is no clear pattern in the correlation between Bitcoin and\nother cryptocurrencies. These findings are in line with the results of\nprevious research and challenge the conventional view that correlations between\ncryptocurrencies are driven by factors, such as centralization or institutional\ninvestor participation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19230769230769232,
          "p": 0.189873417721519,
          "f": 0.19108279754797372
        },
        "rouge-2": {
          "r": 0.03636363636363636,
          "p": 0.037383177570093455,
          "f": 0.03686635444796093
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.16455696202531644,
          "f": 0.1656050905416043
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.quant-ph/2503.10462v1",
      "true_abstract": "Deep neural quantum states have recently achieved remarkable performance in\nsolving challenging quantum many-body problems. While transformer networks\nappear particularly promising due to their success in computer science, we show\nthat previously reported transformer wave functions haven't so far been capable\nto utilize their full power. Here, we introduce the convolutional transformer\nwave function (CTWF). We show that our CTWFs exhibit superior performance in\nground-state search and non-equilibrium dynamics compared to previous results,\ndemonstrating promising capacity in complex quantum problems.",
      "generated_abstract": "rrent work we propose a novel wave function representation for\nconvolutional neural networks (CNNs). We introduce a new architecture which\ncombines the advantages of convolutional neural networks and the transformer\narchitecture. This architecture has the advantage that the representation is\nnot directly related to the input, which makes it possible to use it in any\nCNN architecture. We demonstrate that the proposed wave function can be\ntransformed into a form suitable for the use in CNNs. We show that our wave\nfunctions can be used as a basis for the construction of a quantum neural\nnetwork. We also show that the wave functions can be used as a basis for the\nconstruction of a quantum neural network in the context of the variational\nquantum eigensolver (VQE) algorithm. We show that the proposed wave functions\ncan be used as a basis for the construction of a quantum neural network. We\nalso show that the wave functions can be",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2857142857142857,
          "p": 0.2727272727272727,
          "f": 0.27906976244456466
        },
        "rouge-2": {
          "r": 0.06578947368421052,
          "p": 0.04807692307692308,
          "f": 0.055555550676543636
        },
        "rouge-l": {
          "r": 0.2857142857142857,
          "p": 0.2727272727272727,
          "f": 0.27906976244456466
        }
      }
    },
    {
      "paper_id": "math.NA.math/AP/2503.09580v1",
      "true_abstract": "We introduce a fast Fourier spectral method to compute linearized collision\noperators of the Boltzmann equation for variable hard-sphere gases. While the\nstate-of-the-art method provides a computational cost O(MN^4 log N), with N\nbeing the number of modes in each direction and M being the number of\nquadrature points on a hemisphere, our method reduces the cost to O(N^4 log N),\nremoving the factor M, which could be large in our numerical tests. The method\nis applied in a numerical solver for the steady-state Boltzmann equation with\nquadratic collision operators. Numerical experiments for both spatially\nhomogeneous and inhomogeneous Boltzmann equations have been carried out to test\nthe accuracy and efficiency of our method.",
      "generated_abstract": "e a fast Fourier spectral method (FFSM) for the linearized collision\noperator in the linear Boltzmann equation (LBE) on a periodic domain. The FFSM\nis based on the Fourier-Bessel transform and is applied to the linear Boltzmann\nequation on periodic domains. In the proposed FFSM, we have used the Fourier\ntransform to discretize the LBE in a periodic domain. The Fourier-Bessel\ntransform is used to discretize the linear Boltzmann equation on a periodic\ndomain. This FFSM is more efficient and computationally more accurate than the\nFourier-Bessel transform based FFSM. We compare the performance of the FFSM and\nthe FFSM based on the Fourier-Bessel transform on various test cases. The\nproposed FFSM is also validated using the test cases of the LBE and the linear\nBoltzmann equation on a periodic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2972972972972973,
          "p": 0.4074074074074074,
          "f": 0.3437499951220704
        },
        "rouge-2": {
          "r": 0.10476190476190476,
          "p": 0.11827956989247312,
          "f": 0.1111111061294768
        },
        "rouge-l": {
          "r": 0.2972972972972973,
          "p": 0.4074074074074074,
          "f": 0.3437499951220704
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.10510v1",
      "true_abstract": "Whole-slide image classification represents a key challenge in computational\npathology and medicine. Attention-based multiple instance learning (MIL) has\nemerged as an effective approach for this problem. However, the effect of\nattention mechanism architecture on model performance is not well-documented\nfor biomedical imagery. In this work, we compare different methods and\nimplementations of MIL, including deep learning variants. We introduce a new\nmethod using higher-dimensional feature spaces for deep MIL. We also develop a\nnovel algorithm for whole-slide image classification where extreme machine\nlearning is combined with attention-based MIL to improve sensitivity and reduce\ntraining complexity. We apply our algorithms to the problem of detecting\ncirculating rare cells (CRCs), such as erythroblasts, in peripheral blood. Our\nresults indicate that nonlinearities play a key role in the classification, as\nremoving them leads to a sharp decrease in stability in addition to a decrease\nin average area under the curve (AUC) of over 4%. We also demonstrate a\nconsiderable increase in robustness of the model with improvements of over 10%\nin average AUC when higher-dimensional feature spaces are leveraged. In\naddition, we show that extreme learning machines can offer clear improvements\nin terms of training efficiency by reducing the number of trained parameters by\na factor of 5 whilst still maintaining the average AUC to within 1.5% of the\ndeep MIL model. Finally, we discuss options of enriching the classical\ncomputing framework with quantum algorithms in the future. This work can thus\nhelp pave the way towards more accurate and efficient single-cell diagnostics,\none of the building blocks of precision medicine.",
      "generated_abstract": "aper, we propose a novel Extreme Learning Machines (ELM) framework\nfor multiple instance learning (MIL) on whole slide images (WSIs). WSIs are\ngenerally complex and multi-modal, and each WSI is often treated as a\nseparate task, leading to the challenges of data sparsity and low-data\ninteraction, which hinder the model's ability to generalize. To address these\nissues, we propose a novel ELM-based MIL framework, called ELM-WSIs, which\nemploys ELMs to process WSIs in parallel and enhance the data interaction. We\ndevelop an Attention-based Multi-instance Learning (AMIL) algorithm to\noptimize the ELMs, which ensures the model's ability to handle the WSIs and\naccurately predict the most representative image. Moreover, we propose a\nMulti-task Learning",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1488095238095238,
          "p": 0.3048780487804878,
          "f": 0.19999999559168014
        },
        "rouge-2": {
          "r": 0.020491803278688523,
          "p": 0.04807692307692308,
          "f": 0.028735627993130484
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.25609756097560976,
          "f": 0.1679999955916801
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.01687v2",
      "true_abstract": "We establish Rezk completion functors for $\\Theta_n$ spaces with respect to\neach and all of the completeness conditions. As a consequence, we obtain a\ncharacterization of Dwyer-Kan equivalences between Segal $\\Theta_n$ spaces.",
      "generated_abstract": "We study the class of locally compact groups that admit a separable\ndomination kernel. In particular, we consider the class of locally compact\ngroups that admit a separable dominating system. We show that there are many\nindependent constructions of separable domination kernels and dominating systems\nthat are not K-equivalent. We introduce a new notion of K-equivalence between\nseparable domination kernels, called DK-equivalence, and prove that it is\nindependent of the constructions of separable domination kernels and\ndominating systems. We also introduce a notion of K-equivalence between\nseparable dominating systems and prove that it is independent of the\nconstructions of separable domination kernels and dominating systems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.16279069767441862,
          "f": 0.19718309381471943
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.015625,
          "f": 0.021276591398823885
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.16279069767441862,
          "f": 0.19718309381471943
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.03916v1",
      "true_abstract": "We study stability properties of fully faithful functors, and compute mapping\nanima in pushouts of $\\infty$-categories along fully faithful functors. We\nprovide applications of these calculations to pushouts along Dwyer functors and\nReedy categories.",
      "generated_abstract": "the relationship between fully faithful functors and pushouts of\n$\\infty$-categories. We introduce the category of pushout objects of functors\n$\\mathcal{F}:\\mathcal{C}^\\mathrm{op}\\to\\mathrm{Sets}$ and a functor $\\mathcal{G}\n:\\mathrm{Sets}\\to\\mathrm{Sets}$ which are functors in the obvious sense, and\nshow that for any functor $\\mathcal{F}:\\mathcal{C}^\\mathrm{op}\\to\\mathrm{Sets}$,\nthe functor $\\mathcal{G}\\circ\\mathcal{F}$ is fully faithful.\n  We further introduce the category of pushout objects of functors\n$\\mathcal{F}:\\mathcal{C}^\\mathrm{op}\\to\\mathrm{Sets}$ and a functor $\\mathcal{G}\n:\\mathrm{Sets}\\to\\mathrm{Sets}$ which are functors in the obvious sense,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.36,
          "p": 0.2727272727272727,
          "f": 0.3103448226813318
        },
        "rouge-2": {
          "r": 0.15625,
          "p": 0.10869565217391304,
          "f": 0.12820512336620662
        },
        "rouge-l": {
          "r": 0.36,
          "p": 0.2727272727272727,
          "f": 0.3103448226813318
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2411.18830v1",
      "true_abstract": "We study the relationship between model complexity and out-of-sample\nperformance in the context of mean-variance portfolio optimization.\nRepresenting model complexity by the number of assets, we find that the\nperformance of low-dimensional models initially improves with complexity but\nthen declines due to overfitting. As model complexity becomes sufficiently\nhigh, the performance improves with complexity again, resulting in a double\nascent Sharpe ratio curve similar to the double descent phenomenon observed in\nartificial intelligence. The underlying mechanisms involve an intricate\ninteraction between the theoretical Sharpe ratio and estimation accuracy. In\nhigh-dimensional models, the theoretical Sharpe ratio approaches its upper\nlimit, and the overfitting problem is reduced because there are more parameters\nthan data restrictions, which allows us to choose well-behaved parameters based\non inductive bias.",
      "generated_abstract": "We study the performance of portfolio optimization methods in a regime where\nthe expected loss is not known in advance and the portfolio strategy is\ndeterministic. We show that a large class of portfolio optimization methods\nachieves the same theoretical Sharpe ratio as a randomized strategy that\nfollows the optimal portfolio strategy. However, the estimator for the\nestimation accuracy of the optimal portfolio strategy is limited by the\nuncertainty of the expected loss. Our analysis highlights the importance of\ndetermining the optimal loss, and the role of estimation accuracy, in portfolio\noptimization. We discuss the implications for portfolio optimization in\nhigh-frequency trading and demonstrate the impact of estimation accuracy on\nmarket efficiency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21348314606741572,
          "p": 0.3220338983050847,
          "f": 0.2567567519621987
        },
        "rouge-2": {
          "r": 0.09649122807017543,
          "p": 0.11702127659574468,
          "f": 0.1057692258154588
        },
        "rouge-l": {
          "r": 0.20224719101123595,
          "p": 0.3050847457627119,
          "f": 0.24324323844868523
        }
      }
    },
    {
      "paper_id": "math-ph.math/MP/2503.09421v1",
      "true_abstract": "We study coined Random Quantum Walks on the hexagonal lattice, where the\nstrength of disorder is monitored by the coin matrix. Each lattice site is\nequipped with an i.i.d. random variable that is uniformly distributed on the\ntorus and acts as a random phase in every step of the QW. We show dynamical\nlocalization in the regime of strong disorder, that is whenever the coin matrix\nis sufficiently close to the fully localized case, using a fractional moment\ncriterion and a finite volume method. Moreover, we adapt a topological index to\nour model and thereby obtain transport for some coin matrices.",
      "generated_abstract": "We study the dynamics of quantum walks on the hexagonal lattice using the\nclassical trajectory method. We show that the quantum walk on the hexagonal\nlattice is localized in the bulk and has two distinct transport regimes in the\nboundary. The localization length is $L_c=3.22\\pm 0.06$ and the mean free\npath is $l_c=2.04\\pm 0.02$. We also consider the quantum walk on the square\nlattice and find that the localization length is $L_c=3.64\\pm 0.07$ and the\nmean free path is $l_c=2.11\\pm 0.03$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22666666666666666,
          "p": 0.3469387755102041,
          "f": 0.27419354360691994
        },
        "rouge-2": {
          "r": 0.05102040816326531,
          "p": 0.07142857142857142,
          "f": 0.05952380466269881
        },
        "rouge-l": {
          "r": 0.22666666666666666,
          "p": 0.3469387755102041,
          "f": 0.27419354360691994
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.07897v1",
      "true_abstract": "This work introduces a new markovian stochastic model that can be described\nas a non-homogeneous Pure Birth process. We propose a functional form of birth\nrate that depends on the number of individuals in the population and on the\nelapsed time, allowing us to model a contagion effect. Thus, we model the early\nstages of an epidemic. The number of individuals then becomes the infectious\ncases and the birth rate becomes the incidence rate. We obtain this way a\nprocess that depends on two competitive phenomena, infection and immunization.\nVariations in those rates allow us to monitor how effective the actions taken\nby government and health organizations are. From our model, three useful\nindicators for the epidemic evolution over time are obtained: the immunization\nrate, the infection/immunization ratio and the mean time between infections\n(MTBI). The proposed model allows either positive or negative concavities for\nthe mean value curve, provided the infection/immunization ratio is either\ngreater or less than one. We apply this model to the present SARS-CoV-2\npandemic still in its early growth stage in Latin American countries. As it is\nshown, the model accomplishes a good fit for the real number of both positive\ncases and deaths. We analyze the evolution of the three indicators for several\ncountries and perform a comparative study between them. Important conclusions\nare obtained from this analysis.",
      "generated_abstract": "p a non-homogeneous Markov early epidemic growth model, which\nis intended to explain the non-linear dynamics of the COVID-19 pandemic.\nSpecifically, we focus on the early stages of the epidemic and model the\nspread of the virus through the population through a Markov process. We\nassume that the number of infections per unit time follows a Poisson\ndistribution with a finite mean. We also assume that the infection rate is\nindependent of the age of the infected person. In addition, we introduce an\nage-dependent contact rate, which models the effect of the age structure of the\npopulation. The epidemic growth rate is characterized by the parameter $\\beta$\n(the infectiousness), which we assume to be independent of the age of the\ninfected person. Finally, we model the epidemic growth rate through an\nevolutionary differential",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23357664233576642,
          "p": 0.4383561643835616,
          "f": 0.3047619002263039
        },
        "rouge-2": {
          "r": 0.06310679611650485,
          "p": 0.11926605504587157,
          "f": 0.08253967801380725
        },
        "rouge-l": {
          "r": 0.19708029197080293,
          "p": 0.3698630136986301,
          "f": 0.2571428526072563
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/PR/2405.12479v2",
      "true_abstract": "We present a unified, market-complete model that integrates both the\nBachelier and Black-Scholes-Merton frameworks for asset pricing. The model\nallows for the study, within a unified framework, of asset pricing in a natural\nworld that experiences the possibility of negative security prices or riskless\nrates. In contrast to classical Black-Scholes-Merton, we show that option\npricing in the unified model displays a difference depending on whether the\nreplicating, self-financing portfolio uses riskless bonds or a single riskless\nbank account. We derive option price formulas and extend our analysis to the\nterm structure of interest rates by deriving the pricing of zero-coupon bonds,\nforward contracts, and futures contracts. We identify a necessary condition for\nthe unified model to support a perpetual derivative. Discrete binomial pricing\nunder the unified model is also developed. In every scenario analyzed, we show\nthat the unified model simplifies to the standard Black-Scholes-Merton pricing\nunder specific limits and provides pricing in the Bachelier model limit. We\nnote that the Bachelier limit within the unified model allows for positive\nriskless rates. The unified model prompts us to speculate on the possibility of\na mixed multiplicative and additive deflator model for risk-neutral option\npricing.",
      "generated_abstract": "e a unified Black-Scholes-Merton model for dynamic asset pricing.\nWe introduce a new asset class, the risk-free asset, which is a continuous\nand fully-fledged asset class, and we define a new risk-free rate. We develop a\nBachelier-Black-Scholes-Merton model for this risk-free asset class. We\ndevelop a unified Black-Scholes-Merton model for this risk-free asset class.\nWe show that the unified model is equivalent to the Bachelier-Black-Scholes-Merton\nmodel for the risk-free asset class. The unified model is also equivalent to\nthe Black-Scholes-Merton model for the market-based asset class. We apply the\nunified model to the asset pricing of a financial institution. We use a\ndynamic asset pricing model to analyze the impact of the financial crisis on\nthe",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18269230769230768,
          "p": 0.4318181818181818,
          "f": 0.2567567525785245
        },
        "rouge-2": {
          "r": 0.0963855421686747,
          "p": 0.2,
          "f": 0.13008129642408633
        },
        "rouge-l": {
          "r": 0.17307692307692307,
          "p": 0.4090909090909091,
          "f": 0.243243239065011
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/SC/2408.05119v1",
      "true_abstract": "Stress generation by the actin cytoskeleton shapes cells and tissues. Despite\nimpressive progress in live imaging and quantitative physical descriptions of\ncytoskeletal network dynamics, the connection between processes at molecular\nscales and cell-scale spatio-temporal patterns is still unclear. Here we review\nstudies reporting acto-myosin clusters of micrometer size and with lifetimes of\nseveral minutes in a large number of organisms ranging from fission yeast to\nhumans. Such structures have also been found in reconstituted systems in vitro\nand in theoretical analysis of cytoskeletal dynamics. We propose that tracking\nthese clusters can serve as a simple readout for characterising living matter.\nSpatio-temporal patterns of clusters could serve as determinants of\nmorphogenetic processes that play similar roles in diverse organisms.",
      "generated_abstract": "d myosin-based acto-myosin clusters are well-known structures\nthat play key roles in the organization of muscle fibers. Here, we show that\nthese structures can be found in living matter, and in particular, in the\nmembrane of living cells. We used high-speed video microscopy to investigate\nthe dynamic behavior of such structures in living cells. Our results show that\nclusters of actin filaments and myosin motors can be found in living cells\ninside cells, where they appear to be in active collaboration with the cells.\nThe observed dynamics can be explained by the fact that the acto-myosin clusters\ncan be thought of as active units shaping the membrane. Our results highlight\nthe potential of the study of the dynamics of living cells and their\nmembranes through the study of acto-myosin clusters, which are actin-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.3150684931506849,
          "f": 0.27878787385417825
        },
        "rouge-2": {
          "r": 0.0782608695652174,
          "p": 0.07964601769911504,
          "f": 0.07894736342143768
        },
        "rouge-l": {
          "r": 0.2391304347826087,
          "p": 0.3013698630136986,
          "f": 0.26666666173296605
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.03503v1",
      "true_abstract": "Molecular optimization is a crucial yet complex and time-intensive process\nthat often acts as a bottleneck for drug development. Traditional methods rely\nheavily on trial and error, making multi-objective optimization both\ntime-consuming and resource-intensive. Current AI-based methods have shown\nlimited success in handling multi-objective optimization tasks, hampering their\npractical utilization. To address this challenge, we present MultiMol, a\ncollaborative large language model (LLM) system designed to guide\nmulti-objective molecular optimization. MultiMol comprises two agents,\nincluding a data-driven worker agent and a literature-guided research agent.\nThe data-driven worker agent is a large language model being fine-tuned to\nlearn how to generate optimized molecules considering multiple objectives,\nwhile the literature-guided research agent is responsible for searching\ntask-related literature to find useful prior knowledge that facilitates\nidentifying the most promising optimized candidates. In evaluations across six\nmulti-objective optimization tasks, MultiMol significantly outperforms existing\nmethods, achieving a 82.30% success rate, in sharp contrast to the 27.50%\nsuccess rate of current strongest methods. To further validate its practical\nimpact, we tested MultiMol on two real-world challenges. First, we enhanced the\nselectivity of Xanthine Amine Congener (XAC), a promiscuous ligand that binds\nboth A1R and A2AR, successfully biasing it towards A1R. Second, we improved the\nbioavailability of Saquinavir, an HIV-1 protease inhibitor with known\nbioavailability limitations. Overall, these results indicate that MultiMol\nrepresents a highly promising approach for multi-objective molecular\noptimization, holding great potential to accelerate the drug development\nprocess and contribute to the advancement of pharmaceutical research.",
      "generated_abstract": "lar optimization, a key challenge lies in the integration of\nmultiple objectives, such as maximizing a specific property or minimizing\nthermodynamic costs. To address this, we propose a novel collaborative\nexpert-guided multi-objective optimization framework that integrates\nmulti-agent reinforcement learning (MARL) with LLMs. In the MARL agent, we\nintroduce an expert LLM that dynamically integrates expert knowledge with the\nagent's actions to optimize a shared objective. In the MARL agent, we propose\na new reward function that encourages the agent to optimize multiple\nobjectives simultaneously while simultaneously balancing the trade-off between\noptimizing specific objectives and minimizing costs. In the LLM, we propose a\nmulti-objective-aware expert LLM to integrate expert knowledge with the agent's\nactions. To evaluate the effectiveness of the proposed approach, we conduct\nexper",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1393939393939394,
          "p": 0.2987012987012987,
          "f": 0.1900826402892563
        },
        "rouge-2": {
          "r": 0.013100436681222707,
          "p": 0.028037383177570093,
          "f": 0.01785713851633467
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.2857142857142857,
          "f": 0.18181817747933895
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.astro-ph/IM/2503.08854v1",
      "true_abstract": "Modern astronomical surveys detect asteroids by linking together their\nappearances across multiple images taken over time. This approach faces\nlimitations in detecting faint asteroids and handling the computational\ncomplexity of trajectory linking. We present a novel method that adapts\n``digital tracking\" - traditionally used for short-term linear asteroid motion\nacross images - to work with large-scale synoptic surveys such as the Vera\nRubin Observatory Legacy Survey of Space and Time (Rubin/LSST). Our approach\ncombines hundreds of sparse observations of individual asteroids across their\nnon-linear orbital paths to enhance detection sensitivity by several\nmagnitudes. To address the computational challenges of processing massive data\nsets and dense orbital phase spaces, we developed a specialized\nhigh-performance computing architecture. We demonstrate the effectiveness of\nour method through experiments that take advantage of the extensive\ncomputational resources at Lawrence Livermore National Laboratory. This work\nenables the detection of significantly fainter asteroids in existing and future\nsurvey data, potentially increasing the observable asteroid population by\norders of magnitude across different orbital families, from near-Earth objects\n(NEOs) to Kuiper belt objects (KBOs).",
      "generated_abstract": "very of asteroids is a critical challenge in modern astronomy.\nCurrently, the discovery of near-Earth objects (NEOs) is performed using\ntheir light curves, which are typically acquired with ground-based telescopes\nand/or space-based observatories. While the light curves are important for\ndiscovering NEOs, their large number can make them difficult to analyze.\nAdditionally, due to the large number of light curves, the analysis of these\nlight curves can become computationally intensive. In this paper, we propose a\nnew methodology for asteroid discovery, which uses a digital tracking framework\ncombined with a large-scale database to improve the efficiency of asteroid\ndiscovery. This framework is based on the concept of non-linear digital\ntracking, where a set of digital tracks is generated for each asteroid. The\ndigital tracks are used to identify the asteroids that pass through the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15625,
          "p": 0.22988505747126436,
          "f": 0.1860465068097351
        },
        "rouge-2": {
          "r": 0.011560693641618497,
          "p": 0.015503875968992248,
          "f": 0.013245028218720284
        },
        "rouge-l": {
          "r": 0.1484375,
          "p": 0.21839080459770116,
          "f": 0.17674418122833976
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2407.00332v1",
      "true_abstract": "With emerging prevalence beyond traditionally endemic regions, the global\nburden of dengue disease is forecasted to be one of the fastest growing. With\nlimited direct treatment or vaccination currently available, prevention through\nvector control is widely believed to be the most effective form of managing\noutbreaks. This study examines traditional state space models (moving average,\nautoregressive, ARIMA, SARIMA), supervised learning techniques (XGBoost, SVM,\nKNN) and deep networks (LSTM, CNN, ConvLSTM) for forecasting weekly dengue\ncases in Singapore. Meteorological data and search engine trends were included\nas features for ML techniques. Forecasts using CNNs yielded lowest RMSE in\nweekly cases in 2019.",
      "generated_abstract": "y aims to develop a machine learning model that can forecast\ndengue cases in Singapore. The data used in the study include dengue case\nhistories, dengue incidence rates, and dengue-related mortality rates. To\nimprove the accuracy of the model, various machine learning techniques were\nused, including logistic regression, random forest, support vector machine, and\nXGBoost. The results indicate that the logistic regression model is the best\nchoice for this purpose. The model achieved an accuracy of 82.5%, precision of\n0.77, and recall of 0.66. The results also show that dengue incidence rates\nhave a positive correlation with dengue case histories, with a correlation\ncoefficient of 0.66. The forecasting accuracy for dengue incidence rates is\n81.5%, precision of 0.74, and recall of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.22666666666666666,
          "f": 0.2124999950195314
        },
        "rouge-2": {
          "r": 0.05102040816326531,
          "p": 0.045871559633027525,
          "f": 0.04830917375808122
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.2,
          "f": 0.18749999501953143
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.03217v1",
      "true_abstract": "Social media usage is often cited as a potential driver behind the rising\nsuicide rates. However, distinguishing the causal effect - whether social media\nincreases the risk of suicide - from reverse causality, where individuals\nalready at higher risk of suicide are more likely to use social media, remains\na significant challenge. In this paper, we use an instrumental variable\napproach to study the quasi-exogenous geographical adoption of Twitter and its\ncausal relationship with suicide rates. Our analysis first demonstrates that\nTwitter's geographical adoption was driven by the presence of certain users at\nthe 2007 SXSW festival, which led to long-term disparities in adoption rates\nacross counties in the United States. Then, using a two-stage least squares\n(2SLS) regression and controlling for a wide range of geographic, socioeconomic\nand demographic factors, we find no significant relationship between Twitter\nadoption and suicide rates.",
      "generated_abstract": "ng prevalence of social media has attracted significant attention\nfrom scholars, with some highlighting the potential to promote social\nintegration and social capital, while others contend that these benefits may\nbecome overshadowed by the rise of harmful or abusive content. This study\nempirically examines the social media platform Twitter's effect on suicide\nattempts using quasi-exogenous geographical data from the United States. Using\na regression discontinuity design, we find that Twitter use is positively\nassociated with suicide attempts, with a 10% increase in the number of\nfollowers associated with a 2% increase in suicide attempts. These findings\nhighlight the need for more nuanced understanding of the potential\nbenefits and downsides of social media use, especially in contexts where\nharmful content may be a concern. This study provides valuable insights into the\npot",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.31683168316831684,
          "p": 0.3516483516483517,
          "f": 0.3333333283468968
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.06611570247933884,
          "f": 0.062256804355554606
        },
        "rouge-l": {
          "r": 0.26732673267326734,
          "p": 0.2967032967032967,
          "f": 0.28124999501356346
        }
      }
    },
    {
      "paper_id": "cs.NE.cs/NE/2503.09340v1",
      "true_abstract": "The nature inspired algorithms are becoming popular due to their simplicity\nand wider applicability. In the recent past several such algorithms have been\ndeveloped. They are mainly bio-inspired, swarm based, physics based and\nsocio-inspired; however, the domain based on symbiotic relation between\ncreatures is still to be explored. A novel metaheuristic optimization algorithm\nreferred to as Fig Tree-Wasp Symbiotic Coevolutionary (FWSC) algorithm is\nproposed. It models the symbiotic coevolutionary relationship between fig trees\nand wasps. More specifically, the mating of wasps, pollinating the figs,\nsearching for new trees for pollination and wind effect drifting of wasps are\nmodeled in the algorithm. These phenomena help in balancing the two important\naspects of exploring the search space efficiently as well as exploit the\npromising regions. The algorithm is successfully tested on a variety of test\nproblems. The results are compared with existing methods and algorithms. The\nWilcoxon Signed Rank Test and Friedman Test are applied for the statistical\nvalidation of the algorithm performance. The algorithm is also further applied\nto solve the real-world engineering problems. The performance of the FWSC\nunderscored that the algorithm can be applied to wider variety of real-world\nproblems.",
      "generated_abstract": "e a new symbiotic coevolutionary optimization algorithm that\nalgorithms are used to solve the Fig-Tree problem, a problem of finding a\nsubset of Fig-Tree edges in a graph. This problem arises in many areas such as\nnetwork routing, graph partitioning, and the search for optimal paths in\nnetworks. The problem is NP-Hard and NP-complete, and it is a particularly\nimportant problem in the graph partitioning area, where it is known that the\nhardness of the problem can be reduced to the coevolutionary optimization\nproblem. The main contribution of this work is to propose a coevolutionary\noptimization algorithm that solves the problem of Fig-Tree optimizing the\ncoevolutionary optimization problem. This algorithm is called Fig-Tree-Wasp\nSymbiotic Coevolutionary Optimization Algorithm. This algorithm is based on\nthe Fig-Tree problem, but it is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22950819672131148,
          "p": 0.4057971014492754,
          "f": 0.2931937126624819
        },
        "rouge-2": {
          "r": 0.0718232044198895,
          "p": 0.11711711711711711,
          "f": 0.08904109117775404
        },
        "rouge-l": {
          "r": 0.20491803278688525,
          "p": 0.36231884057971014,
          "f": 0.2617801000970369
        }
      }
    },
    {
      "paper_id": "physics.app-ph.physics/atm-clus/2502.20913v1",
      "true_abstract": "Recent research on silver nanowires prepared on DNA templates has focused on\ntwo fundamental applications: nano-scale circuits and sensors. Despite its\nbroad potential, the formation kinetics of DNA-templated silver nanowires\nremains unclear. Here, we present an experimental demonstration of the\nformation of silver nanowires with a diameter of 2.2+0.4 nm at the\nsingle-molecule level through chemical reduction. We conducted equilibrium and\nperturbation kinetic experiments to measure force spectroscopy during the\nformation of Ag+ -DNA complexes and Ag-DNA complexes, using optical tweezers\ncombined with microfluidics. The addition of AgNO3 resulted in an increase in\nforce of 5.5-7.5 pN within 2 minutes, indicating that Ag+ compacts the DNA\nstructure. In contrast, the addition of hydroquinone caused the force to\ndecrease by 4-5 pN. Morphological characterization confirmed the presence of a\ndense structure formed by silver atoms bridging the DNA strands, and revealed\nconformational differences before and after metallization. We compare our\nexperimental data with Brownian dynamics simulations using a coarse-grained\ndouble-stranded DNA (dsDNA) model that provides insights on the dependency of\nthe force on the persistence length.",
      "generated_abstract": "ated metal nanowires (DNA-MWNWs) have gained considerable interest in\nthe fields of nanotechnology, biotechnology, and medicine, as they are\ncharacterized by their exceptional biocompatibility and nanoscale dimensions.\nHowever, their synthesis has not been completely established, especially in\nthe case of nanowires, which are characterized by a complex three-dimensional\nnature. In this work, we present the synthesis of DNA-templated silver nanowires\n(DNA-MWNWs) by using a simple, low-temperature, and solvent-free method,\nwhich ensures the reproducibility of the synthesized nanowires. The\nsynthesized DNA-MWNWs exhibit high surface area and high surface charge\ndensity, which facilitates the formation of silver nanoparticles (AgNPs). The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13333333333333333,
          "p": 0.23529411764705882,
          "f": 0.17021276133997296
        },
        "rouge-2": {
          "r": 0.05357142857142857,
          "p": 0.09782608695652174,
          "f": 0.06923076465798848
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.23529411764705882,
          "f": 0.17021276133997296
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.physics/hist-ph/2502.17438v1",
      "true_abstract": "Henrietta Swan Leavitt's discovery of the relationship between the period and\nluminosity (hereafter the Leavitt Law) of 25 variable stars in the Small\nMagellanic Cloud, published in 1912, revolutionized cosmology. These variables,\neventually identified as Cepheids, became the first known \"standard candles\"\nfor measuring extragalactic distances and remain the gold standard for this\ntask today. Leavitt measured light curves, periods, and minimum and maximum\nmagnitudes from painstaking visual inspection of photographic plates. Her work\npaved the way for the first precise series of distance measurements that helped\nset the scale of the Universe, and later the discovery of its expansion by\nEdwin Hubble in 1929. Here, we re-analyze Leavitt's first Period-Luminosity\nrelation using observations of the same set of stars but with modern data and\nmethods of Cepheid analysis. Using only data from Leavitt's notebooks, we\nassess the quality of her light curves, measured periods, and the slope and\nscatter of her Period-Luminosity relations. We show that modern data and\nmethods, for the same objects, reduce the scatter of the Period-Luminosity\nrelation by a factor of two. We also find a bias brightward at the short period\nend, due to the non-linearity of the plates and environmental crowding.\nOverall, Leavitt's results are in excellent agreement with contemporary\nmeasurements, reinforcing the value of Cepheids in cosmology today, a testament\nto the enduring quality of her work.",
      "generated_abstract": "1917) established a 12-year-long period-luminosity (PL)\nrelation for the first time, using observations of 65 stars in the Palomar\nObservatory Sky Survey (POSS). This work was widely influential, as it\nrevealed the existence of a new type of variable star, known as a Cepheid,\nwhich could be used to measure distances. The result was a 4.2-mag improvement\nin the Hubble constant compared to the previous value of 82.2 mag,\nestimated by Galatea (1917). The discovery of Cepheids was crucial for the\ndevelopment of modern astronomy. This paper presents a re-analysis of the\noriginal Leavitt data using modern data and techniques. The period-luminosity\nrelation is determined, with a 2.7-mag improvement",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19117647058823528,
          "p": 0.3291139240506329,
          "f": 0.24186046046771234
        },
        "rouge-2": {
          "r": 0.06310679611650485,
          "p": 0.11926605504587157,
          "f": 0.08253967801380725
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.3037974683544304,
          "f": 0.2232558093049217
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.09096v1",
      "true_abstract": "Consider a simple algebraic valued field extension $(L/K,v)$ and denote by\n$\\mathcal O_L$ and $\\mathcal O_K$ the corresponding valuation rings. The main\ngoal of this paper is to present, under certain assumptions, a description of\n$\\mathcal O_L$ in terms of generators and relations over $\\mathcal O_K$. The\nmain tool used here are complete sequences of key polynomials. It is known that\nif the ramification index of $(L/K,v)$ is one, then every complete set gives\nrise to a set of generators of $\\mathcal O_L$ over $\\mathcal O_K$. We show that\nwe can find a sequence of key polynomials for $(L/K,v)$ which satisfies good\nproperties (called neat). Then we present explicit ``neat\" relations that\ngenerate all the relations between the corresponding generators of $\\mathcal\nO_L$ over $\\mathcal O_K$.",
      "generated_abstract": "We give a constructive proof of the existence of valuation rings in simple\nalgebraic extensions of valuated fields. The proof relies on the\ncharacterisation of valuation rings in a field by their residue field. We show\nthat any simple algebraic extension of a valuated field is an extension of a\nvalued field, and that any valuation ring is a valuation ring in the extension\ninduced by the valuation ring of the valuated field. Our proof does not rely on\nthe theory of valuations, but instead relies on the theory of residue fields.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23684210526315788,
          "p": 0.4186046511627907,
          "f": 0.3025210037878681
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.05333333333333334,
          "f": 0.043715842157126754
        },
        "rouge-l": {
          "r": 0.21052631578947367,
          "p": 0.37209302325581395,
          "f": 0.26890755840971686
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.05979v1",
      "true_abstract": "Autoregressive models (ARMs) have become the workhorse for sequence\ngeneration tasks, since many problems can be modeled as next-token prediction.\nWhile there appears to be a natural ordering for text (i.e., left-to-right),\nfor many data types, such as graphs, the canonical ordering is less obvious. To\naddress this problem, we introduce a variant of ARM that generates\nhigh-dimensional data using a probabilistic ordering that is sequentially\ninferred from data. This model incorporates a trainable probability\ndistribution, referred to as an \\emph{order-policy}, that dynamically decides\nthe autoregressive order in a state-dependent manner. To train the model, we\nintroduce a variational lower bound on the exact log-likelihood, which we\noptimize with stochastic gradient estimation. We demonstrate experimentally\nthat our method can learn meaningful autoregressive orderings in image and\ngraph generation. On the challenging domain of molecular graph generation, we\nachieve state-of-the-art results on the QM9 and ZINC250k benchmarks, evaluated\nusing the Fr\\'{e}chet ChemNet Distance (FCD).",
      "generated_abstract": "ular graph generation problem aims to generate molecular graphs from\ncrystallographic structures. The generation process consists of two stages:\nmodeling the crystallographic structure and generating the corresponding\nmolecular graphs. While the crystallographic structure has been well studied,\nmolecular graph generation has received less attention, mainly due to the\ndifficulty in modeling the molecular graphs. In this paper, we propose a\nlearning-order autoregressive model for molecular graph generation, which\nintroduces an order parameter to model the molecular graphs. By using the\norder parameter, the model can learn the order of the molecular graphs. The\norder parameter is constructed by a series of autoregressive models. Furthermore,\nthe order parameter is learned by minimizing the reconstruction loss of the\ngenerated molecular graphs. Experimental results on the MoleculeGen dataset\nshow that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2543859649122807,
          "p": 0.38666666666666666,
          "f": 0.3068783020912069
        },
        "rouge-2": {
          "r": 0.046357615894039736,
          "p": 0.06481481481481481,
          "f": 0.054054049191872955
        },
        "rouge-l": {
          "r": 0.24561403508771928,
          "p": 0.37333333333333335,
          "f": 0.2962962915091963
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2410.19741v1",
      "true_abstract": "Identifying client needs to provide optimal services is crucial in tourist\ndestination management. The events held in tourist destinations may help to\nmeet those needs and thus contribute to tourist satisfaction. As with product\nmanagement, the creation of hierarchical catalogs to classify those events can\naid event management. The events that can be found on the internet are listed\nin dispersed, heterogeneous sources, which makes direct classification a\ndifficult, time-consuming task. The main aim of this work is to create a novel\nprocess for automatically classifying an eclectic variety of tourist events\nusing a hierarchical taxonomy, which can be applied to support tourist\ndestination management. Leveraging data science methods such as CRISP-DM,\nsupervised machine learning, and natural language processing techniques, the\nautomatic classification process proposed here allows the creation of a\nnormalized catalog across very different geographical regions. Therefore, we\ncan build catalogs with consistent filters, allowing users to find events\nregardless of the event categories assigned at source, if any. This is very\nvaluable for companies that offer this kind of information across multiple\nregions, such as airlines, travel agencies or hotel chains. Ultimately, this\ntool has the potential to revolutionize the way companies and end users\ninteract with tourist events information.",
      "generated_abstract": "The tourism industry is a vital economic sector, contributing to the\ndevelopment of many countries worldwide. It is a complex industry that requires\na wide range of skills and expertise to effectively manage and analyze. This\npaper explores the application of artificial intelligence techniques in the\nanalysis of tourism events. The study investigates the use of machine learning\nand deep learning models to classify tourism events, such as festivals,\ncelebrations, and holidays. The results of the study indicate that the\naccuracy of the models is high, with an average accuracy of 94.4% and an\naverage precision of 97.9%.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1259259259259259,
          "p": 0.265625,
          "f": 0.17085426699325784
        },
        "rouge-2": {
          "r": 0.015625,
          "p": 0.031578947368421054,
          "f": 0.020905918916098012
        },
        "rouge-l": {
          "r": 0.11851851851851852,
          "p": 0.25,
          "f": 0.16080401573697647
        }
      }
    },
    {
      "paper_id": "math.CA.math/CA/2503.10190v1",
      "true_abstract": "In a famous paper published in 1904, Helge von Koch introduced the curve that\nstill serves nowadays as an iconic representation of fractal shapes. In fact,\nvon Koch's main goal was the construction of a continuous but nowhere\ndifferentiable function, very similar to the snowflake, using elementary\ngeometric procedures, and not analytical formulae. We prove that a parametrized\nfamily of functions (including and) generalizing von Koch's example enjoys a\nrich multifractal behavior, thus enriching the class of historical mathematical\nobjects having surprising regularity properties. The analysis relies on the\nstudy of the orbits of an underlying dynamical system and on the introduction\nof self-similar measures and non-trivial iterated functions systems adapted to\nthe problem.",
      "generated_abstract": "We study the multifractal properties of a family of parametrized\nfamily of von K\\\"{o}hler functions. In particular, we show that the\nmultifractal spectrum of the family of parametrized family of von K\\\"{o}hler\nfunctions is a subset of the multifractal spectrum of the family of\nparametrized family of the exponential map. We also study the Hausdorff\ndimension of the family of parametrized family of von K\\\"{o}hler functions. We\nshow that the Hausdorff dimension of the family of parametrized family of\nvon K\\\"{o}hler functions is strictly less than that of the family of\nparametrized family of exponential map.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14942528735632185,
          "p": 0.4642857142857143,
          "f": 0.22608695283780725
        },
        "rouge-2": {
          "r": 0.03636363636363636,
          "p": 0.09302325581395349,
          "f": 0.05228757765816597
        },
        "rouge-l": {
          "r": 0.13793103448275862,
          "p": 0.42857142857142855,
          "f": 0.20869564848998115
        }
      }
    },
    {
      "paper_id": "cs.DS.econ/TH/2501.13346v1",
      "true_abstract": "We study a general class of sequential search problems for selecting multiple\ncandidates from different societal groups under \"ex-ante constraints\" aimed at\nproducing socially desirable outcomes, such as demographic parity, diversity\nquotas, or subsidies for disadvantaged groups. Starting with the canonical\nPandora's box model [Weitzman, 1978] under a single affine constraint on\nselection and inspection probabilities, we show that the optimal constrained\npolicy retains an index-based structure similar to the unconstrained case, but\nmay randomize between two dual-based adjustments that are both easy to compute\nand economically interpretable. We then extend our results to handle multiple\naffine constraints by reducing the problem to a variant of the exact\nCarath\\'eodory problem and providing a novel polynomial-time algorithm to\ngenerate an optimal randomized dual-adjusted index-based policy that satisfies\nall constraints simultaneously. Building on these insights, we consider richer\nsearch processes (e.g., search with rejection and multistage search) modeled by\njoint Markov scheduling (JMS) [Dumitriu et al., 2003; Gittins, 1979]. By\nimposing general affine and convex ex-ante constraints, we develop a\nprimal-dual algorithm that randomizes over a polynomial number of dual-based\nadjustments to the unconstrained JMS Gittins indices, yielding a near-feasible,\nnear-optimal policy. Our approach relies on the key observation that a suitable\nrelaxation of the Lagrange dual function for these constrained problems admits\nindex-based policies akin to those in the unconstrained setting. Using a\nnumerical study, we investigate the implications of imposing various\nconstraints, in particular the utilitarian loss (price of fairness), and\nwhether these constraints induce their intended societally desirable outcomes.",
      "generated_abstract": "r introduces a general framework for analyzing the dynamic\nplacement of agents within a Markov decision process (MDP) while considering\nsocial constraints, where the agents are assigned to specific locations in the\nMDP. We propose a Markovian search algorithm that takes into account the\nconstraints imposed by the agents and their social interactions. The algorithm\nis simple to implement and has a computational complexity of $\\mathcal{O}(n\n\\log n)$ where $n$ is the number of agents in the MDP. We show that the\nalgorithm is optimal in terms of computational complexity and that the\nsub-optimality of the algorithm can be controlled. We show that the\nsub-optimality of the algorithm can be controlled by a parameter $\\gamma$\nthat can be chosen as small as $\\gamma = o(n)$. We also show that the\nsub-optimality of the algorithm can be made arbitrarily small by choosing\n$\\gamma =",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12643678160919541,
          "p": 0.29333333333333333,
          "f": 0.17670682309962754
        },
        "rouge-2": {
          "r": 0.02834008097165992,
          "p": 0.061946902654867256,
          "f": 0.03888888458163628
        },
        "rouge-l": {
          "r": 0.12643678160919541,
          "p": 0.29333333333333333,
          "f": 0.17670682309962754
        }
      }
    },
    {
      "paper_id": "math.RT.math/RT/2503.10461v1",
      "true_abstract": "This article studies the compatibility of Koenig's notion of an exact Borel\nsubalgebra of a quasi-hereditary or, more generally, standardly stratified\nalgebra with taking idempotent subalgebras or quotients. As an application, we\nprovide bounds for the multiplicities of indecomposable projectives in the\nprincipal blocks of BGG category $\\mathcal{O}$ having basic regular exact Borel\nsubalgebras.",
      "generated_abstract": "We establish a new characterization of exact Borel subalgebras of the\ngeneral linear group, based on the notion of Borel subalgebras which were\nintroduced by D.J. McMullen in 1993. We also give a simple characterization of\nidempotent quotients of groups, which can be applied to prove a version of the\nBorel-Ruzsa theorem for exact Borel subalgebras.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.275,
          "f": 0.2619047569160999
        },
        "rouge-2": {
          "r": 0.057692307692307696,
          "p": 0.058823529411764705,
          "f": 0.05825242218493775
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.15,
          "f": 0.14285713786848087
        }
      }
    },
    {
      "paper_id": "math.OC.eess/SY/2503.03357v1",
      "true_abstract": "Given a max-plus linear system and a semimodule, the problem of computing the\nmaximal controlled invariant subsemimodule is still open to this day. In this\npaper, we consider this problem for the specific class of fully actuated\nsystems and constraints in the form of precedence semimodules. The assumption\nof full actuation corresponds to the existence of an input for each component\nof the system state. A precedence semimodule is the set of solutions of\ninequalities typically used to represent time-window constraints. We prove\nthat, in this setting, it is possible to (i) compute the maximal controlled\ninvariant subsemimodule and (ii) decide the convergence of a fixed-point\nalgorithm introduced by R.D. Katz in strongly polynomial time.",
      "generated_abstract": "This paper considers the controlled invariance of a fully actuated\nmax-plus linear system with precedence semimodules. The goal is to design a\ncontrol law such that the resulting system remains invariance under the\ntransformation that preserves the precedence semimodules. The main result is\nthat the solution is unique if and only if the system is a product of a\nlinear and a fully actuated max-plus linear system. Moreover, if the system is\na product of a linear and a fully actuated max-plus linear system, the control\nlaw can be designed such that it remains invariance under the transformation\nthat preserves the precedence semimodules. Finally, the existence and uniqueness\nof the solution are established under the assumption that the precedence\nsemimodules are not allowed to be empty. The paper concludes with some open\nproblems and potential applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24675324675324675,
          "p": 0.3220338983050847,
          "f": 0.2794117597934689
        },
        "rouge-2": {
          "r": 0.09009009009009009,
          "p": 0.10416666666666667,
          "f": 0.09661835251417795
        },
        "rouge-l": {
          "r": 0.22077922077922077,
          "p": 0.288135593220339,
          "f": 0.24999999508758658
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/PL/2503.05924v1",
      "true_abstract": "Techniques that rigorously bound the overall rounding error exhibited by a\nnumerical program are of significant interest for communities developing\nnumerical software. However, there are few available tools today that can be\nused to rigorously bound errors in programs that employ conditional statements\n(a basic need) as well as mixed-precision arithmetic (a direction of\nsignificant future interest) employing global optimization in error analysis.\nIn this paper, we present a new tool that fills this void while also employing\nan abstraction-guided optimization approach to allow designers to trade\nerror-bound tightness for gains in analysis time -- useful when searching for\ndesign alternatives. We first present the basic rigorous analysis framework of\nSatire and then show how to extend it to incorporate abstractions,\nconditionals, and mixed-precision arithmetic. We begin by describing Satire's\ndesign and its performance on a collection of benchmark examples. We then\ndescribe these aspects of Satire: (1) how the error-bound and tool execution\ntime vary with the abstraction level; (2) the additional machinery to handle\nconditional expression branches, including defining the concepts of instability\njumps and instability window widths and measuring these quantities; and (3) how\nthe error changes when a mix of precision values are used. To showcase how\n\\satire can add value during design, we start with a Conjugate Gradient solver\nand demonstrate how its step size and search direction are affected by\ndifferent precision settings. Satire is freely available for evaluation, and\ncan be used during the design of numerical routines to effect design tradeoffs\nguided by rigorous empirical error guarantees.",
      "generated_abstract": "Floating-point rounding error is a critical performance bottleneck in\nmixed-precision loop-free programs, limiting performance gains from\nparallelization and instruction-level parallelism. Current approaches to\naddressing this issue rely on heuristic methods, such as fixed-point\nrepresentations, which are brittle and prone to numerical instability. We\npropose Satire, a novel framework for rigorous bounds for floating-point\nrounding error in mixed-precision loop-free programs. Satire combines\nmixed-precision dataflow-based analysis with the techniques of robust\nstability analysis, enabling a more principled approach to error mitigation.\nSatire offers an efficient, robust, and scalable alternative to heuristic\napproaches, providing precise guarantees for critical performance metrics such\nas latency and throughput.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18354430379746836,
          "p": 0.3717948717948718,
          "f": 0.24576270743895437
        },
        "rouge-2": {
          "r": 0.012096774193548387,
          "p": 0.03125,
          "f": 0.01744185644132056
        },
        "rouge-l": {
          "r": 0.16455696202531644,
          "p": 0.3333333333333333,
          "f": 0.2203389786253951
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2412.09631v1",
      "true_abstract": "Limit order book (LOB) is a dynamic, event-driven system that records\nreal-time market demand and supply for a financial asset in a stream flow.\nEvent stream prediction in LOB refers to forecasting both the timing and the\ntype of events. The challenge lies in modeling the time-event distribution to\ncapture the interdependence between time and event type, which has\ntraditionally relied on stochastic point processes. However, modeling complex\nmarket dynamics using stochastic processes, e.g., Hawke stochastic process, can\nbe simplistic and struggle to capture the evolution of market dynamics. In this\nstudy, we present LOBDIF (LOB event stream prediction with diffusion model),\nwhich offers a new paradigm for event stream prediction within the LOB system.\nLOBDIF learns the complex time-event distribution by leveraging a diffusion\nmodel, which decomposes the time-event distribution into sequential steps, with\neach step represented by a Gaussian distribution. Additionally, we propose a\ndenoising network and a skip-step sampling strategy. The former facilitates\neffective learning of time-event interdependence, while the latter accelerates\nthe sampling process during inference. By introducing a diffusion model, our\napproach breaks away from traditional modeling paradigms, offering novel\ninsights and providing an effective and efficient solution for learning the\ntime-event distribution in order streams within the LOB system. Extensive\nexperiments using real-world data from the limit order books of three widely\ntraded assets confirm that LOBDIF significantly outperforms current\nstate-of-the-art methods.",
      "generated_abstract": "y proposes a diffusion model-based method for predicting limit order\nbids and asks (LOB) event streams in the financial market. We assume that LOB\nevents occur on a daily basis and use a two-step diffusion model with a\nconvolutional encoder-decoder architecture to encode the historical LOB data\ninto a latent space. We then use the diffusion process to generate the\npredicted LOB data. The method is validated on the HKEx, SGX, and NYSE\ndatasets. Our results demonstrate that the diffusion model outperforms the\ntraditional method in terms of mean absolute error (MAE) and median absolute\nerror (MAE), with the diffusion model achieving a mean MAE of 2.12 and a median\nMAE of 1.32. This study introduces a new method for predicting LOB events and\nhighlights the potential of the diffusion model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1917808219178082,
          "p": 0.35443037974683544,
          "f": 0.24888888433224698
        },
        "rouge-2": {
          "r": 0.018779342723004695,
          "p": 0.03333333333333333,
          "f": 0.02402401941400949
        },
        "rouge-l": {
          "r": 0.1506849315068493,
          "p": 0.27848101265822783,
          "f": 0.19555555099891367
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.17503v1",
      "true_abstract": "When does a Sender, in a Sender-Receiver game, strictly value commitment? In\na setting with finite actions and finite states, we establish that,\ngenerically, Sender values commitment if and only if he values randomization.\nIn other words, commitment has no value if and only if a partitional experiment\nis optimal under commitment. Moreover, if Sender's preferred cheap-talk\nequilibrium necessarily involves randomization, then Sender values commitment.\nWe also ask: how often (i.e., for what share of preference profiles) does\ncommitment have no value? For any prior, any independent, atomless distribution\nof preferences, and any state space: if there are $\\left|A\\right|$ actions, the\nlikelihood that commitment has no value is at least\n$\\frac{1}{\\left|A\\right|^{\\left|A\\right|}}$. As the number of states grows\nlarge, this likelihood converges precisely to\n$\\frac{1}{\\left|A\\right|^{\\left|A\\right|}}$.",
      "generated_abstract": "In this paper, we study the effect of communication costs on the\ncommitting and randomizing strategies. We characterize the optimal commitment\nstrategy and the optimal randomizing strategy. Our results generalize prior\nresults on the impact of communication costs on the outcomes of\nnon-cooperative games. In particular, we show that the optimal commitment\nstrategy is deterministic, while the optimal randomizing strategy is\npolynomial-time deterministic. Additionally, we establish a connection between\nthe communication costs and the information complexity of the problem, which\nallows us to characterize the communication cost-optimal policies in terms of\nthe information complexity of the problem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.2830188679245283,
          "f": 0.20979020512494506
        },
        "rouge-2": {
          "r": 0.008547008547008548,
          "p": 0.013513513513513514,
          "f": 0.010471199441903415
        },
        "rouge-l": {
          "r": 0.14444444444444443,
          "p": 0.24528301886792453,
          "f": 0.18181817715291712
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/LG/2503.10633v1",
      "true_abstract": "As there are now millions of publicly available neural networks, searching\nand analyzing large model repositories becomes increasingly important.\nNavigating so many models requires an atlas, but as most models are poorly\ndocumented charting such an atlas is challenging. To explore the hidden\npotential of model repositories, we chart a preliminary atlas representing the\ndocumented fraction of Hugging Face. It provides stunning visualizations of the\nmodel landscape and evolution. We demonstrate several applications of this\natlas including predicting model attributes (e.g., accuracy), and analyzing\ntrends in computer vision models. However, as the current atlas remains\nincomplete, we propose a method for charting undocumented regions.\nSpecifically, we identify high-confidence structural priors based on dominant\nreal-world model training practices. Leveraging these priors, our approach\nenables accurate mapping of previously undocumented areas of the atlas. We\npublicly release our datasets, code, and interactive atlas.",
      "generated_abstract": "The Hugging Face model library offers a powerful API for creating and\nperforming fine-grained model exploration, allowing users to explore the\nvarious configurations of the models and their components. However, the\navailable documentation is scattered and not organized, making it difficult to\nfind specific information. We present a systematic approach to explore the\nHugging Face model library, focusing on the HF Transformers and HF T5 models.\nWe build a model explorer based on the Jupyter Notebook, enabling the\nexploration of the model's configuration in a user-friendly environment. We\nalso introduce the HF Explorer, a web-based app to navigate through the model\natlas, providing a better user experience for exploring the models. Finally, we\nevaluate the system on the HF Transformers and HF T5 models, providing insights\ninto the model's configuration.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18867924528301888,
          "p": 0.24691358024691357,
          "f": 0.213903738404873
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.04424778761061947,
          "f": 0.03952568675655048
        },
        "rouge-l": {
          "r": 0.1792452830188679,
          "p": 0.2345679012345679,
          "f": 0.2032085512390976
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.00772v1",
      "true_abstract": "With the rapid advancement of information technology and data collection\nsystems, large-scale spatial panel data presents new methodological and\ncomputational challenges. This paper introduces a dynamic spatial panel\nquantile model that incorporates unobserved heterogeneity. The proposed model\ncaptures the dynamic structure of panel data, high-dimensional cross-sectional\ndependence, and allows for heterogeneous regression coefficients. To estimate\nthe model, we propose a novel Bayesian Markov Chain Monte Carlo (MCMC)\nalgorithm. Contributions to Bayesian computation include the development of\nquantile randomization, a new Gibbs sampler for structural parameters, and\nstabilization of the tail behavior of the inverse Gaussian random generator. We\nestablish Bayesian consistency for the proposed estimation method as both the\ntime and cross-sectional dimensions of the panel approach infinity. Monte Carlo\nsimulations demonstrate the effectiveness of the method. Finally, we illustrate\nthe applicability of the approach through a case study on the quantile\nco-movement structure of the gasoline market.",
      "generated_abstract": "r presents a novel Bayesian methodology for the estimation and\ninterpolation of dynamic quantile models with interactive effects, such as\nquantile-quantile models (QQMs). The proposed methodology extends the\nmultivariate quantile regression (MVQR) approach to the spatial domain by\nintroducing a dynamic spatial quantile model (DSQM) with an interactive\ninteraction term. The spatial quantile regression (SQR) model is extended to\nthe spatial quantile regression with interactive effects (SQR-I) model, where\nthe interactive effect is modeled as an interaction term between the quantile\nvariable and the spatial location variable. Both models are estimated using a\nBayesian approach. The proposed approach is applied to the data on the\ndevelopment of the economy in the UK between 1997 and 2017. The results show\nthat the proposed methodology outperforms",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24742268041237114,
          "p": 0.35294117647058826,
          "f": 0.2909090860635446
        },
        "rouge-2": {
          "r": 0.09352517985611511,
          "p": 0.11711711711711711,
          "f": 0.10399999506272024
        },
        "rouge-l": {
          "r": 0.24742268041237114,
          "p": 0.35294117647058826,
          "f": 0.2909090860635446
        }
      }
    },
    {
      "paper_id": "cs.DC.cs/DC/2503.09917v1",
      "true_abstract": "MareNostrum5 is a pre-exascale supercomputer at the Barcelona Supercomputing\nCenter (BSC), part of the EuroHPC Joint Undertaking. With a peak performance of\n314 petaflops, MareNostrum5 features a hybrid architecture comprising Intel\nSapphire Rapids CPUs, NVIDIA Hopper GPUs, and DDR5 and high-bandwidth memory\n(HBM), organized into four partitions optimized for diverse workloads. This\ndocument evaluates MareNostrum5 through micro-benchmarks (floating-point\nperformance, memory bandwidth, interconnect throughput), HPC benchmarks (HPL\nand HPCG), and application studies using Alya, OpenFOAM, and IFS. It highlights\nMareNostrum5's scalability, efficiency, and energy performance, utilizing the\nEAR (Energy Aware Runtime) framework to assess power consumption and the\neffects of direct liquid cooling. Additionally, HBM and DDR5 configurations are\ncompared to examine memory performance trade-offs. Designed to complement\nstandard technical documentation, this study provides insights to guide both\nnew and experienced users in optimizing their workloads and maximizing\nMareNostrum5's computational capabilities.",
      "generated_abstract": "um5 is the fifth generation of the MareNostrum supercomputer system\nwhich is currently under construction in Barcelona, Spain. This paper\ndescribes the technical and scientific challenges posed by the system and the\napproaches used to address them. MareNostrum5 will offer 240,000 cores of\nhigh-performance computing, with a peak performance of 1.1 petaflops. It will\nserve a broad spectrum of scientific workloads, including deep learning,\nmachine learning, artificial intelligence, and numerical simulation. This paper\nwill describe the system architecture and the system software, including its\nadaptation to support the unique challenges of scientific workloads. The paper\nwill also discuss the system's energy efficiency, including the\nenergy-efficient design of the system and its software components. Finally,\nthis paper will describe the system's capabilities, including its performance\nand scal",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16964285714285715,
          "p": 0.24358974358974358,
          "f": 0.19999999516011094
        },
        "rouge-2": {
          "r": 0.036231884057971016,
          "p": 0.044642857142857144,
          "f": 0.03999999505408062
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.20512820512820512,
          "f": 0.16842104779168987
        }
      }
    },
    {
      "paper_id": "nlin.PS.q-bio/CB/2409.00623v1",
      "true_abstract": "For a cell-bulk ODE-PDE model in $\\mathbb{R}^2$, a hybrid\nasymptotic-numerical theory is developed to provide a new theoretical and\ncomputationally efficient approach for studying how oscillatory dynamics\nassociated with spatially segregated dynamically active ``units\" or ``cells\"\nare regulated by a PDE bulk diffusion field that is both produced and absorbed\nby the entire cell population. The study of oscillator synchronization in a PDE\ndiffusion field was one of the initial aims of Yoshiki Kuramoto's foundational\nwork. For this cell-bulk model, strong localized perturbation theory, as\nextended to a time-dependent setting, is used to derive a new\nintegro-differential ODE system that characterizes intracellular dynamics in a\nmemory-dependent bulk-diffusion field. For this nonlocal reduced system, a\nnovel fast time-marching scheme, relying in part on the\n\\emph{sum-of-exponentials method} to numerically treat convolution integrals,\nis developed to rapidly and accurately compute numerical solutions to the\nintegro-differential system over long time intervals. For the special case of\nSel'kov reaction kinetics, a wide variety of large-scale oscillatory dynamical\nbehavior including phase synchronization, mixed-mode oscillations, and\nquorum-sensing are illustrated for various ranges of the influx and efflux\npermeability parameters, the bulk degradation rate and bulk diffusivity, and\nthe specific spatial configuration of cells. Results from our fast algorithm,\nobtained in under one minute of CPU time on a laptop, are benchmarked against\nPDE simulations of the cell-bulk model, which are performed with a commercial\nPDE solver, that have run-times that are orders of magnitude larger.",
      "generated_abstract": "the coupled dynamics of a cell-bulk model in $\\mathbb{R}^2$\nand its intracellular counterpart coupled to a cell-bulk model in $\\mathbb{R}^2$.\nOur analysis focuses on the memory-dependent oscillatory dynamics of the\nintracellular model. We derive a coupled ODE-PDE system that describes the\ninterplay between the intracellular and cell-bulk models. Using a\ncontinuation-field method, we solve the coupled system and identify the\nsynchronized memory-dependent oscillations that arise from the coupling. We\ndemonstrate that the synchronized memory-dependent oscillations can be\nconsidered as a collective phenomenon of the cell-bulk model and the intracellular\nmodel. Furthermore, we derive the phase portraits of the coupled system and\nobtain analytical solutions for the synchron",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14012738853503184,
          "p": 0.4,
          "f": 0.2075471659687612
        },
        "rouge-2": {
          "r": 0.04405286343612335,
          "p": 0.11363636363636363,
          "f": 0.06349205946565913
        },
        "rouge-l": {
          "r": 0.1337579617834395,
          "p": 0.38181818181818183,
          "f": 0.19811320370461025
        }
      }
    },
    {
      "paper_id": "math-ph.math-ph/2503.09827v1",
      "true_abstract": "This paper defines coherent manifolds and discusses their properties and\ntheir application in quantum mechanics. Every coherent manifold with a large\ngroup of symmetries gives rise to a Hilbert space, the completed quantum space\nof $Z$, which contains a distinguished family of coherent states labeled by the\npoints of the manifold.\n  The second quantization map in quantum field theory is generalized to\nquantization operators on arbitrary coherent manifolds. It is shown how the\nSchr\\\"odinger equation on any such completed quantum space can be solved in\nterms of computations only involving the coherent product. In particular, this\napplies to a description of Bosonic Fock spaces as completed quantum spaces of\na class of coherent manifolds called Klauder spaces.",
      "generated_abstract": "We introduce a generalization of coherent manifolds that allows us to\ntake into account the non-local effects of the interaction between the\nparticles. The main idea is that we take the coherent manifold to be a\nnon-commutative analog of the classical phase space. In particular, we show\nthat the classical phase space of the classical particles is a coherent manifold\nwith respect to the standard Poisson bracket. We also show that the\nnon-commutative phase space of the interacting particles is a coherent manifold\nwith respect to the new Poisson bracket that we introduce.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18421052631578946,
          "p": 0.32558139534883723,
          "f": 0.2352941130315656
        },
        "rouge-2": {
          "r": 0.07407407407407407,
          "p": 0.11428571428571428,
          "f": 0.08988763567731373
        },
        "rouge-l": {
          "r": 0.17105263157894737,
          "p": 0.3023255813953488,
          "f": 0.21848739034249007
        }
      }
    },
    {
      "paper_id": "math.CO.cs/DM/2503.09525v1",
      "true_abstract": "The complexity of continuous piecewise affine (CPA) functions can be measured\nby the number of pieces $p$ or the number of distinct affine functions $n$. For\nCPA functions on $\\mathbb{R}^d$, this paper shows an upper bound of\n$p=O(n^{d+1})$ and constructs a family of functions achieving a lower bound of\n$p=\\Omega(n^{d+1-\\frac{c}{\\sqrt{\\log_2(n)}}})$.",
      "generated_abstract": "The number of pieces in a continuous piecewise affine function is a fundamental\nproblem in approximation theory and computational geometry. We show that\n$O(n\\log^2 n)$ pieces are necessary for any function whose graph contains\n$O(n\\log^2 n)$ vertices. We also show that $O(n)$ pieces suffice for any\nfunction whose graph contains $O(n^2)$ vertices.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24324324324324326,
          "p": 0.2571428571428571,
          "f": 0.2499999950038581
        },
        "rouge-2": {
          "r": 0.08695652173913043,
          "p": 0.09090909090909091,
          "f": 0.08888888389135831
        },
        "rouge-l": {
          "r": 0.16216216216216217,
          "p": 0.17142857142857143,
          "f": 0.16666666167052485
        }
      }
    },
    {
      "paper_id": "nlin.SI.nlin/SI/2503.06013v1",
      "true_abstract": "Hirota's discrete KdV (dKdV) equation is an integrable autonomous partial\ndifference equation on $\\mathbb{Z}^2$ that reduces to the Korteweg-de Vries\n(KdV) equation in a continuum limit. In this paper, we introduce a new\nnon-autonomous version of the dKdV equation. Furthermore, we show that the new\nequation is integrable and admits discrete Painlev\\'e transcendent solutions\ndescribed by $q$-Painlev\\'e equations of $A_J^{(1)}$-surface types\n($J=3,4,5,6$).",
      "generated_abstract": "We introduce a new non-autonomous version of Hirota's discrete KdV equation. We\ndiscuss the structure of its solutions, which are Painlev\\'{e} transcendents,\nand discuss the relation between the new equation and the discrete\nPainlev\\'{e} transcendent equations. We show that the new equation has a\ndifferent set of solutions than the original KdV equation and discuss the\nconnection with the discrete Painlev\\'{e} transcendent equations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3541666666666667,
          "p": 0.4857142857142857,
          "f": 0.4096385493395268
        },
        "rouge-2": {
          "r": 0.18333333333333332,
          "p": 0.21568627450980393,
          "f": 0.19819819323106902
        },
        "rouge-l": {
          "r": 0.3333333333333333,
          "p": 0.45714285714285713,
          "f": 0.3855421637973581
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.08231v1",
      "true_abstract": "We discuss necessary conditions for a PAC-Bayes bound to provide a meaningful\ngeneralisation guarantee. Our analysis reveals that the optimal generalisation\nguarantee depends solely on the distribution of the risk induced by the prior\ndistribution. In particular, achieving a target generalisation level is only\nachievable if the prior places sufficient mass on high-performing predictors.\nWe relate these requirements to the prevalent practice of using data-dependent\npriors in deep learning PAC-Bayes applications, and discuss the implications\nfor the claim that PAC-Bayes ``explains'' generalisation.",
      "generated_abstract": ", the popular Bayesian counterpart of the popular statistical\nlearning method PAC-learning, has been extensively studied in the literature.\nThis study focuses on the question of whether PAC-Bayes can be expected to be\nmore accurate than PAC-learning in explaining the generalisation performance of\nthe underlying learner. We show that, when the learner has a strong\nclassification property, such as the separability property, PAC-Bayes\noverfits the learner. When the learner has a generalisation property, such as\nthe uncorrelation property, PAC-Bayes is more accurate than PAC-learning in\nexplaining the generalisation performance of the learner. We also show that\nPAC-Bayes is more accurate than PAC-learning in explaining the generalisation\nperformance of the learner in a class of problems called the \\emph",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20689655172413793,
          "p": 0.21052631578947367,
          "f": 0.20869564717429126
        },
        "rouge-2": {
          "r": 0.0379746835443038,
          "p": 0.036585365853658534,
          "f": 0.037267075747078326
        },
        "rouge-l": {
          "r": 0.1724137931034483,
          "p": 0.17543859649122806,
          "f": 0.1739130384786391
        }
      }
    },
    {
      "paper_id": "stat.ML.q-bio/PE/2502.04730v1",
      "true_abstract": "Learning informative representations of phylogenetic tree structures is\nessential for analyzing evolutionary relationships. Classical distance-based\nmethods have been widely used to project phylogenetic trees into Euclidean\nspace, but they are often sensitive to the choice of distance metric and may\nlack sufficient resolution. In this paper, we introduce phylogenetic\nvariational autoencoders (PhyloVAEs), an unsupervised learning framework\ndesigned for representation learning and generative modeling of tree\ntopologies. Leveraging an efficient encoding mechanism inspired by\nautoregressive tree topology generation, we develop a deep latent-variable\ngenerative model that facilitates fast, parallelized topology generation.\nPhyloVAE combines this generative model with a collaborative inference model\nbased on learnable topological features, allowing for high-resolution\nrepresentations of phylogenetic tree samples. Extensive experiments demonstrate\nPhyloVAE's robust representation learning capabilities and fast generation of\nphylogenetic tree topologies.",
      "generated_abstract": "In this paper, we propose a novel approach for unsupervised learning of\nphylospheres, i.e., phylogenetic trees, by integrating variational\nautoencoders (VAEs) with a deep generative model. The VAE encodes the tree\nstructure, while the decoder reconstructs it. The encoder is trained to\nrepresent trees using a set of training trees, which are then used as\nsupervised data for training the decoder. The decoder is parameterized by\nrepresenting a phylogenetic tree with a neural network. We demonstrate that\nthis approach can be used to learn and reconstruct phylogenetic trees without\na ground truth. We apply our method to a dataset of bacterial phylogenies,\nshowing that it outperforms the existing state-of-the-art methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2916666666666667,
          "p": 0.35443037974683544,
          "f": 0.31999999504718374
        },
        "rouge-2": {
          "r": 0.09243697478991597,
          "p": 0.10091743119266056,
          "f": 0.09649122307979405
        },
        "rouge-l": {
          "r": 0.2708333333333333,
          "p": 0.3291139240506329,
          "f": 0.2971428521900409
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.06821v1",
      "true_abstract": "The exploration of Bird's-Eye View (BEV) mapping technology has driven\nsignificant innovation in visual perception technology for autonomous driving.\nBEV mapping models need to be applied to the unlabeled real world, making the\nstudy of unsupervised domain adaptation models an essential path. However,\nresearch on unsupervised domain adaptation for BEV mapping remains limited and\ncannot perfectly accommodate all BEV mapping tasks. To address this gap, this\npaper proposes HierDAMap, a universal and holistic BEV domain adaptation\nframework with hierarchical perspective priors. Unlike existing research that\nsolely focuses on image-level learning using prior knowledge, this paper\nexplores the guiding role of perspective prior knowledge across three distinct\nlevels: global, sparse, and instance levels. With these priors, HierDA consists\nof three essential components, including Semantic-Guided Pseudo Supervision\n(SGPS), Dynamic-Aware Coherence Learning (DACL), and Cross-Domain Frustum\nMixing (CDFM). SGPS constrains the cross-domain consistency of perspective\nfeature distribution through pseudo labels generated by vision foundation\nmodels in 2D space. To mitigate feature distribution discrepancies caused by\nspatial variations, DACL employs uncertainty-aware predicted depth as an\nintermediary to derive dynamic BEV labels from perspective pseudo-labels,\nthereby constraining the coarse BEV features derived from corresponding\nperspective features. CDFM, on the other hand, leverages perspective masks of\nview frustum to mix multi-view perspective images from both domains, which\nguides cross-domain view transformation and encoding learning through mixed BEV\nlabels. The proposed method is verified on multiple BEV mapping tasks, such as\nBEV semantic segmentation, high-definition semantic, and vectorized mapping.\nThe source code will be made publicly available at\nhttps://github.com/lynn-yu/HierDAMap.",
      "generated_abstract": "e HierDAMap, a novel framework for cross-domain BEV mapping that\ntransforms BEV images into the target domain using a hierarchical perspective\nprior (HPP) network. The HPP module first extracts HPP features from a pre-trained\nBEV encoder, and then aligns the HPP features with the target domain using\nperspective transformation. This alignment enables the HPP features to be\nappropriately adapted to the target domain, facilitating the generation of\nrealistic BEV images. The HPP features are further used as input to the\nencoder, enabling cross-domain BEV mapping. Extensive experiments on\nCUB-200-2011, ADE20K, and CUHK-SYN demonstrate that the proposed framework\nachieves state-of-the-art performance across all evaluation metrics,\ndemonstrating its effectiveness in en",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16853932584269662,
          "p": 0.40540540540540543,
          "f": 0.23809523394683804
        },
        "rouge-2": {
          "r": 0.024793388429752067,
          "p": 0.06315789473684211,
          "f": 0.03560830455670168
        },
        "rouge-l": {
          "r": 0.15168539325842698,
          "p": 0.36486486486486486,
          "f": 0.21428571013731426
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.01287v1",
      "true_abstract": "Simulation-based inference (SBI) methods typically require fully observed\ndata to infer parameters of models with intractable likelihood functions.\nHowever, datasets often contain missing values due to incomplete observations,\ndata corruptions (common in astrophysics), or instrument limitations (e.g., in\nhigh-energy physics applications). In such scenarios, missing data must be\nimputed before applying any SBI method. We formalize the problem of missing\ndata in SBI and demonstrate that naive imputation methods can introduce bias in\nthe estimation of SBI posterior. We also introduce a novel amortized method\nthat addresses this issue by jointly learning the imputation model and the\ninference network within a neural posterior estimation (NPE) framework.\nExtensive empirical results on SBI benchmarks show that our approach provides\nrobust inference outcomes compared to standard baselines for varying levels of\nmissing data. Moreover, we demonstrate the merits of our imputation model on\ntwo real-world bioactivity datasets (Adrenergic and Kinase assays). Code is\navailable at https://github.com/Aalto-QuML/RISE.",
      "generated_abstract": "In many applications, data are often missing at random. This can lead to\ninconsistent inference and modeling. In this paper, we develop a novel\nsimulation-based inference framework for missing data models based on neural\nprocesses. Specifically, we propose a novel neural process for missing data\nmodels, which extends the neural process framework in \\cite{shen2023robust} to\nthe case of missing data. We then develop a novel inference procedure for\nneural process models. Finally, we conduct extensive experiments to evaluate\nthe performance of the proposed method in various simulation and real-world\nsettings. The results show that our method achieves competitive or better\nperformance compared to existing methods, especially in high-dimensional settings.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26548672566371684,
          "p": 0.4225352112676056,
          "f": 0.3260869517822543
        },
        "rouge-2": {
          "r": 0.039735099337748346,
          "p": 0.06,
          "f": 0.04780876014666482
        },
        "rouge-l": {
          "r": 0.23893805309734514,
          "p": 0.38028169014084506,
          "f": 0.29347825613008044
        }
      }
    },
    {
      "paper_id": "cs.CG.cs/CG/2503.09115v1",
      "true_abstract": "We prove a quasi-linear upper bound on the size of $K_{t,t}$-free polygon\nvisibility graphs. For visibility graphs of star-shaped and monotone polygons\nwe show a linear bound. In the more general setting of $n$ points on a simple\nclosed curve and visibility pseudo-segments, we provide an $O(n \\log n)$ upper\nbound and an $\\Omega(n\\alpha(n))$ lower bound.",
      "generated_abstract": "visibility graphs of polygons on the unit square, which are graphs\nthat represent the visibility of polygons on the square. We prove that for any\npolygon $P$ on the square, there is a visibility graph $G$ with at most $2^P$\nvertices such that $G$ contains a $2$-connected subgraph whose vertices are the\nvertices of $P$. In particular, this implies that the visibility graph of a\npolygon has at most $2^P$ vertices. Furthermore, we show that for any $k \\geq\n3$, there is a $k$-connected visibility graph on $2^k$ vertices. Finally, we\nshow that for any $k \\geq 3$, there is a $k$-connected visibility graph on\n$2^{k-1}$ vertices if and only if $k$ is even. These results generalize and\nimprove the results of Pare",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.358974358974359,
          "p": 0.23728813559322035,
          "f": 0.28571428092253237
        },
        "rouge-2": {
          "r": 0.09433962264150944,
          "p": 0.05319148936170213,
          "f": 0.06802720627331237
        },
        "rouge-l": {
          "r": 0.358974358974359,
          "p": 0.23728813559322035,
          "f": 0.28571428092253237
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2502.17461v1",
      "true_abstract": "We prove that if a given reaction network $\\mathcal{N}$ has a weakly\nreversible deficiency zero realization for all choice of rate constants, then\nthere exists a $\\textit{unique}$ weakly reversible deficiency zero network\n$\\mathcal{N}'$ such that $\\mathcal{N}$ is realizable by $\\mathcal{N}'$.\nAdditionally, we propose an algorithm to find this weakly reversible deficiency\nzero network $\\mathcal{N}'$ when it exists.",
      "generated_abstract": "We show that weakly reversible deficiency zero realizations of a\nreaction network can be obtained by adding the same number of reversible\nreactions to the original network. This result extends the work of\nEckart-Mielke and Kassakova [Eckart-Mielke and Kassakova, J. Math. Biol.\n2006",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3,
          "p": 0.3333333333333333,
          "f": 0.31578946869806096
        },
        "rouge-2": {
          "r": 0.08333333333333333,
          "p": 0.09523809523809523,
          "f": 0.0888888839111114
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.2777777777777778,
          "f": 0.2631578897506926
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.14708v1",
      "true_abstract": "We conduct an incentivized laboratory experiment to study people's perception\nof generative artificial intelligence (GenAI) alignment in the context of\neconomic decision-making. Using a panel of economic problems spanning the\ndomains of risk, time preference, social preference, and strategic\ninteractions, we ask human subjects to make choices for themselves and to\npredict the choices made by GenAI on behalf of a human user. We find that\npeople overestimate the degree of alignment between GenAI's choices and human\nchoices. In every problem, human subjects' average prediction about GenAI's\nchoice is substantially closer to the average human-subject choice than it is\nto the GenAI choice. At the individual level, different subjects' predictions\nabout GenAI's choice in a given problem are highly correlated with their own\nchoices in the same problem. We explore the implications of people\noverestimating GenAI alignment in a simple theoretical model.",
      "generated_abstract": "We conduct a laboratory experiment to assess the human misperception of\ngenerative-AI alignment. We recruited 121 participants and randomly assigned\nthem to two conditions: (1) a simple baseline experiment in which participants\nread and rated a passage about generative-AI alignment and (2) a more\ncomplicated experiment in which participants read a passage about generative-AI\nmisalignment. We found that participants misperceived generative-AI alignment\nwhen the passage was about generative-AI alignment, but not when it was about\nmisalignment. These results suggest that misperception of generative-AI\nalignment is driven by human biases that we can mitigate by providing\ndeliberate training on misalignment. Our results underscore the need for\nconsideration of generative-AI alignment in AI safety research and policy.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23333333333333334,
          "p": 0.31343283582089554,
          "f": 0.2675159186741856
        },
        "rouge-2": {
          "r": 0.03731343283582089,
          "p": 0.05102040816326531,
          "f": 0.04310344339625501
        },
        "rouge-l": {
          "r": 0.2111111111111111,
          "p": 0.2835820895522388,
          "f": 0.24203821166781625
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/SC/2402.14887v4",
      "true_abstract": "Metabolic pathways are fundamental maps in biochemistry that detail how\nmolecules are transformed through various reactions. The complexity of\nmetabolic network, where a single compound can play a part in multiple\npathways, poses a challenge in inferring metabolic balance changes over time or\nafter different treatments. Isotopic labeling experiment is the standard method\nto infer metabolic flux, which is currently defined as the flow of a single\nmetabolite through a given pathway over time. However, there is still no way to\naccurately infer the metabolic balance changes after different treatments in an\nexperiment. This study introduces a different concept: molecular weight\ndistribution, which is the empirical distribution of the molecular weights of\nall metabolites of interest. By estimating the differences of the location and\nscale estimates of these distributions, it becomes possible to quantitatively\ninfer the metabolic balance changes even without requiring knowledge of the\nexact chemical structures of these compounds and their related pathways. This\nresearch article provides a mathematical framing for a classic biological\nconcept.",
      "generated_abstract": "weight distributions are ubiquitous in biological systems,\nrepresenting the distribution of molecular weights, which are key\nmeasurements for many biological processes. Here, we present a novel approach\nto infer molecular weight distributions from moment differences between\nmolecular weight distributions. The approach is based on the observation that\nmolecular weight distributions can be decomposed into two distinct, but related\nparts: the first part is the molecular weight distribution of molecules that are\nat a constant distance from the observer and the second part is the molecular\nweight distribution of molecules that are closer to the observer. We use this\ndecomposition to extract the molecular weight distribution of molecules that are\nat a distance from the observer from the moment difference of the molecular\nweight distribution. In a second step, we use the molecular weight distribution\nof molecules that are closer to the observer to infer the molecular weight",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.34375,
          "f": 0.2528735585678426
        },
        "rouge-2": {
          "r": 0.046357615894039736,
          "p": 0.0707070707070707,
          "f": 0.05599999521632041
        },
        "rouge-l": {
          "r": 0.17272727272727273,
          "p": 0.296875,
          "f": 0.21839079994715296
        }
      }
    },
    {
      "paper_id": "astro-ph.GA.astro-ph/GA/2503.09753v1",
      "true_abstract": "This thesis investigates the evolution of galaxies in diverse environments,\nutilizing Sloan Digital Sky Survey (SDSS) data to explore the impact of\nenvironmental richness on central and satellite galaxies across stellar mass\nranges, compared to isolated systems. The sample is limited to 0.03 < z < 0.1\nand apparent magnitudes brighter than 17.78, ensuring spectroscopic\ncompleteness and reliable stellar population estimates. Galaxies are\ncategorized by environment as field or cluster/group systems, with further\nseparation into satellites and centrals. By analyzing the star formation rate\n(SFR)-stellar mass plane, this work identifies systematic differences in the\nblue cloud (BC), green valley (GV), and red sequence (RS) across environments.\nMorphological and stellar population analyses reveal that T-type, metallicity,\nand stellar age transitions highlight the role of environmental quenching. A\nnewly introduced T-Type \\emph{vs.} specific SFR diagram provides evidence that\nmorphological transformation precedes full quenching. Correlating galaxy\nproperties with time since infall through projected phase space confirms the\ndelayed-then-rapid quenching model for low- and intermediate-mass galaxies,\nextending it to morphology. Time-scales for quenching and morphological\ntransitions are also derived as a function of stellar mass.",
      "generated_abstract": "tion of galaxy populations is a central question in galaxy\nstellar dynamics, galaxy evolution, and galaxy-galaxy interactions. In the\nlocal Universe, the central galaxy is a particularly important system, as it\nprovides an essential snapshot of the entire local galaxy population. We use\nthe GAMA survey to study the evolution of central galaxies and satellite\ngalaxies, focusing on the early-type galaxies (ETGs) and late-type galaxies\n(LTGs). We first explore the evolution of central and satellite galaxies in\nthe local Universe, and then examine the evolution of the central galaxies of\nearly-type galaxies (ETGs) and late-type galaxies (LTGs). Our results reveal\nthat the central galaxies of ETGs and LTGs have similar ages and metallicities\nbut different stellar masses. We find that the central galaxies of ET",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.3384615384615385,
          "f": 0.21153845724158662
        },
        "rouge-2": {
          "r": 0.05,
          "p": 0.09090909090909091,
          "f": 0.0645161244536944
        },
        "rouge-l": {
          "r": 0.13986013986013987,
          "p": 0.3076923076923077,
          "f": 0.1923076880108174
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.00578v1",
      "true_abstract": "I propose a model of aggregation of intervals relevant to the study of legal\nstandards of tolerance. Seven axioms: responsiveness, anonymity, continuity,\nstrategyproofness, and three variants of neutrality are then used to prove\nseveral important results about a new class of aggregation methods called\nendpoint rules. The class of endpoint rules includes extreme tolerance\n(allowing anything permitted by anyone) and a form of majoritarianism (the\nmedian rule).",
      "generated_abstract": "In a classic paper, von Th\u00fcnen introduced the notion of \"tolerance\" as a\nproperty of a decision maker that characterizes how far he is willing to\ndeviate from his original plan before giving up. We study the implications of\nthis property for the design of incentive-compatible mechanisms. We provide\nnecessary and sufficient conditions for tolerance to hold, and discuss their\ninterplay with the notion of \"strategic\" behavior. Finally, we show that in\ncertain settings, tolerance is equivalent to the requirement that a\ndecision maker \"believe in\" a plan. We provide conditions under which this\nproperty is incompatible with the design of incentive-compatible mechanisms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13725490196078433,
          "p": 0.1076923076923077,
          "f": 0.12068965024524396
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.01098901098901099,
          "f": 0.01298700815230404
        },
        "rouge-l": {
          "r": 0.13725490196078433,
          "p": 0.1076923076923077,
          "f": 0.12068965024524396
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.09174v1",
      "true_abstract": "This paper examines the number of communication modes, that is, the degrees\nof freedom (DoF), in a wireless setup comprising a small continuous linear\nintelligent antenna array in the near field of a large one. The framework\nallows for any orientations between the arrays and any positions in a\ntwo-dimensional space assuming that the transmitting array is placed at the\norigin. Therefore, apart from the length of the two continuous arrays, four key\nparameters determine the DoF and are hence considered in the analysis: the\nCartesian coordinates of the center of the receiving array and two angles that\nmodel the rotation of each array around its center. The paper starts with the\ncalculation of the deterministic DoF for a generic geometric setting, which\nextends beyond the widely studied paraxial case. Subsequently, a stochastic\ngeometry framework is proposed to study the statistical DoF, as a first step\ntowards the investigation of the system-level performance in near field\nnetworks. Numerical results applied to millimeter wave networks reveal the\nlarge number of DoF provided by near-field communications and unveiled key\nsystem-level insights.",
      "generated_abstract": "r presents a detailed analysis of the DoF of continuous linear\narrays in the near field. The analysis is based on the results of previous\nresearches and the concept of statistical analysis of the DoF of continuous\narrays. The DoF of continuous linear arrays in the near field are determined\nthrough the properties of the covariance matrix of the received signal. The\nproperties of the covariance matrix of the received signal are derived through\nthe analysis of the signal-to-interference-plus-noise ratio (SINR) in the near\nfield. The DoF of continuous linear arrays in the near field is derived by\nusing the SINR and the covariance matrix of the received signal. The\nstatistical analysis of the DoF of continuous linear arrays in the near field\nis conducted through the method of random matrices. The results of the\nanalysis are compared with the results of previous researches. The analysis",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1592920353982301,
          "p": 0.4,
          "f": 0.22784809719195645
        },
        "rouge-2": {
          "r": 0.041176470588235294,
          "p": 0.09210526315789473,
          "f": 0.05691056483574624
        },
        "rouge-l": {
          "r": 0.1504424778761062,
          "p": 0.37777777777777777,
          "f": 0.2151898693438552
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.03151v1",
      "true_abstract": "Subset selection is central to many wireless communication problems,\nincluding link scheduling, power allocation, and spectrum management. However,\nthese problems are often NP-complete, because of which heuristic algorithms\napplied to solve these problems struggle with scalability in large-scale\nsettings. To address this, we propose a determinantal point process-based\nlearning (DPPL) framework for efficiently solving general subset selection\nproblems in massive networks. The key idea is to model the optimal subset as a\nrealization of a determinantal point process (DPP), which balances the\ntrade-off between quality (signal strength) and similarity (mutual\ninterference) by enforcing negative correlation in the selection of {\\em\nsimilar} links (those that create significant mutual interference). However,\nconventional methods for constructing similarity matrices in DPP impose\ndecomposability and symmetry constraints that often do not hold in practice. To\novercome this, we introduce a new method based on the Gershgorin Circle Theorem\nfor constructing valid similarity matrices. The effectiveness of the proposed\napproach is demonstrated by applying it to two canonical wireless network\nsettings: an ad hoc network in 2D and a cellular network serving drones in 3D.\nSimulation results show that DPPL selects near-optimal subsets that maximize\nnetwork sum-rate while significantly reducing computational complexity compared\nto traditional optimization methods, demonstrating its scalability for\nlarge-scale networks.",
      "generated_abstract": "opment of distributed wireless networks is advancing rapidly,\nwith more and more wireless devices connected to the network. However,\ndeploying such networks presents a significant challenge, as it is difficult to\nchoose the number of wireless nodes and the number of devices to connect\nefficiently. To address this, we propose a determinantal learning method that\nuses the Conditional Value-at-Risk (CVaR) to select the subset of devices to\nconnect. By using the CVaR to select the subset, the number of devices to\nconnect can be determined efficiently without requiring any prior knowledge\nabout the network topology. The CVaR is calculated based on the data of the\nnetwork, which is collected over time. The method is based on the\nsub-Gaussianity assumption, which is a popular assumption in the literature.\nFurthermore, we show that the proposed method can achieve the optimal\nsub-Gaussianity property of the CVa",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22448979591836735,
          "p": 0.39759036144578314,
          "f": 0.28695651712627607
        },
        "rouge-2": {
          "r": 0.065,
          "p": 0.104,
          "f": 0.07999999526627248
        },
        "rouge-l": {
          "r": 0.2108843537414966,
          "p": 0.37349397590361444,
          "f": 0.26956521277844997
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.09560v1",
      "true_abstract": "Solving medical imaging data scarcity through semantic image generation has\nattracted significant attention in recent years. However, existing methods\nprimarily focus on generating whole-organ or large-tissue structures, showing\nlimited effectiveness for organs with fine-grained structure. Due to stringent\ntopological consistency, fragile coronary features, and complex 3D\nmorphological heterogeneity in cardiac imaging, accurately reconstructing\nfine-grained anatomical details of the heart remains a great challenge. To\naddress this problem, in this paper, we propose the Fine-grained Cardiac image\nSynthesis(FCaS) framework, established on 3D template conditional diffusion\nmodel. FCaS achieves precise cardiac structure generation using Template-guided\nConditional Diffusion Model (TCDM) through bidirectional mechanisms, which\nprovides the fine-grained topological structure information of target image\nthrough the guidance of template. Meanwhile, we design a deformable Mask\nGeneration Module (MGM) to mitigate the scarcity of high-quality and diverse\nreference mask in the generation process. Furthermore, to alleviate the\nconfusion caused by imprecise synthetic images, we propose a Confidence-aware\nAdaptive Learning (CAL) strategy to facilitate the pre-training of downstream\nsegmentation tasks. Specifically, we introduce the Skip-Sampling Variance (SSV)\nestimation to obtain confidence maps, which are subsequently employed to\nrectify the pre-training on downstream tasks. Experimental results demonstrate\nthat images generated from FCaS achieves state-of-the-art performance in\ntopological consistency and visual quality, which significantly facilitates the\ndownstream tasks as well. Code will be released in the future.",
      "generated_abstract": "mage synthesis aims to generate high-quality images from a 3D\nmodel of the heart. However, existing methods often rely on a large amount of\nexpert-curated data, which is costly and time-consuming to obtain. In this\npaper, we propose a novel approach, FCaS, which leverages a 3D template\nconditional diffusion model to generate cardiac images. Specifically, FCaS\ngenerates a 3D model of the heart from a 2D image using a diffusion model. Then,\nthe diffusion model is conditioned on the 3D model to generate cardiac images.\nFinally, the generated images are normalized using a pretrained template model\nto obtain the final synthetic images. We conduct extensive experiments on\nthree datasets to evaluate the effectiveness of FCaS. The results show that\nFCaS outperforms existing methods in terms of PSNR and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22784810126582278,
          "p": 0.4864864864864865,
          "f": 0.3103448232416766
        },
        "rouge-2": {
          "r": 0.06103286384976526,
          "p": 0.12037037037037036,
          "f": 0.08099688027018395
        },
        "rouge-l": {
          "r": 0.2088607594936709,
          "p": 0.44594594594594594,
          "f": 0.2844827542761594
        }
      }
    },
    {
      "paper_id": "math.GN.math/GN/2503.01241v1",
      "true_abstract": "This paper will discuss the problem of defining the new topological\ntransitivity. To do this several equivalent topological transitive and\nnon-wandering point has been discussed through this paper. This paper also\nconsider the ideal version of transitivity with the help of the amendment of\nthe result Remark $6.9(2)$ of \\cite{LL2013}. Corrected version of the Remark:\n``If $\\mathcal{\\bf I}$ is codense, then $\\mathcal{\\bf I}$-denseness,\n$*$-denseness and denseness are equivalent\" will be ``If $\\mathcal{\\bf I}$ is\ncompletely codense, then $\\mathcal{\\bf I}$-denseness, $*$-denseness and\ndenseness are equivalent\".",
      "generated_abstract": "aper, we study dynamical systems and topological transitivity via\ni\\p{n}deals, which are ideals of the category of rings. We provide a\ncharacterization of topological transitivity via ideals for a wide class of\ndynamical systems, including those of Morse type, and we prove the existence of\ntopological transitivity via ideals for certain dynamical systems of\nMorse-type. In particular, we prove the existence of topological transitivity\nvia ideals for the dynamical system $\\mathbb{T}_n$ of the $n$-th\nHopf-Coxeter group. In addition, we study the dynamics of the Hopf-Coxeter\ngroup $\\mathbb{T}_n$ and its dynamical system $\\mathbb{T}_n$ with the help of\nideals. In particular, we show that the Hopf-Coxeter group $\\mathbb{T}_n$ has",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17647058823529413,
          "p": 0.1875,
          "f": 0.18181817682277332
        },
        "rouge-2": {
          "r": 0.07246376811594203,
          "p": 0.0641025641025641,
          "f": 0.0680272059030963
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.1875,
          "f": 0.18181817682277332
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.02763v1",
      "true_abstract": "The Index of Dissimilarity (ID), widely utilized in economic literature as a\nmeasure of segregation, is inadequate for cross-country or time series studies\ndue to its failure to account for structural variations across countries' labor\nmarkets or changes over time within a single country's labor market. Building\non the works of Karmel and MacLachlan (1988) and Blackburn et al. (1993), we\npropose a new measure - the standardized ID - that isolates structural\ndifferences from true differences in segregation across space or time. A key\nadvantage of our proposed measure lies in its ease of implementation and\ninterpretation, even when working with datasets encompassing a large number of\ncountries or time periods. Moreover, our measure can be consistently applied in\nthe case of lumpy sectors or occupations that account for a large fraction of\nthe workforce. We illustrate the new measure in an analysis of the\ncross-country relationship between economic development (as measured by GDP per\ncapita) and occupational and sectoral gender segregation. Comparing the crude\nID with the standardized ID, we show that the crude ID overestimates the\npositive correlation between income and segregation, especially between low-\nand middle-income countries. This suggests that analyses relying on the crude\nID risk overestimating the importance of income differentials in explaining\ncross-country variation in gender segregation.",
      "generated_abstract": "sis of segregation in labor markets is a key challenge for\nmeasuring and understanding economic inequality. We introduce consistent\nsegregation metrics that account for the varying importance of different\nrepresentative groups in the labor market. These metrics take into account the\ndifferences in the importance of different groups across different countries,\nand are based on the recent work of Keefer et al. (2023). We use the\nmetric-based approach to estimate segregation in 120 countries from 2014 to 2021\nand compare it to the traditional approach based on the mean-median model. We\nfind that the metrics-based approach captures the same patterns and\ninterpretations as the traditional model. The metrics also yield more\naccurate estimates of segregation than the traditional model, which is\nespecially important when dealing with data that are not available for all\ngroups",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22900763358778625,
          "p": 0.35714285714285715,
          "f": 0.27906976268080047
        },
        "rouge-2": {
          "r": 0.055,
          "p": 0.08870967741935484,
          "f": 0.06790122984301207
        },
        "rouge-l": {
          "r": 0.19083969465648856,
          "p": 0.2976190476190476,
          "f": 0.23255813477382378
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.08646v1",
      "true_abstract": "We consider isotropic and Lagrangian embeddings of coadjoint orbits of\ncompact Lie groups into products of coadjoint orbits. After reviewing the known\nfacts in the case of $\\mathrm{SU}(n)$ we initiate a similar study for\n$\\mathrm{SO}$ and $\\mathrm{Sp}$ cases. In the second part we apply this to the\nstudy of dynamical systems with $\\mathrm{SU}(n)$ symmetry, proving equivalence\nbetween systems of two types: those describing magnetic geodesic flow on flag\nmanifolds and classical `spin chains' of a special type.",
      "generated_abstract": "We study the magnetic geodesic flow on the space of isotropic submanifolds of\nthe tangent bundle of the complex Grassmanian. We prove the existence of an\nisotropic geodesic flow on the tangent bundle of the Grassmanian, which is\nnon-autonomous, and establish the existence of a magnetic geodesic flow. In\nparticular, we show that the magnetic geodesic flow on the Grassmanian is\nautonomous and the isotropic geodesic flow on the tangent bundle of the Grassmanian\nis not autonomous.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22033898305084745,
          "p": 0.40625,
          "f": 0.28571428115445
        },
        "rouge-2": {
          "r": 0.05405405405405406,
          "p": 0.08163265306122448,
          "f": 0.06504064561306136
        },
        "rouge-l": {
          "r": 0.2033898305084746,
          "p": 0.375,
          "f": 0.263736259176428
        }
      }
    },
    {
      "paper_id": "cs.ET.cs/ET/2503.09237v1",
      "true_abstract": "We attempt to take a comprehensive look at the challenges of representing the\nspatio-temporal structures and dynamic processes defining a city's overall\ncharacteristics. For the task of urban planning and urban operation, we take\nthe stance that even if the necessary representations of these structures and\nprocesses can be achieved, the most important representation of the relevant\nmindsets of the citizens are, unfortunately, mostly neglected.\n  After a review of major \"traditional\" urban models of structures behind urban\nscale, form, and dynamics, we turn to major recent modeling approaches\ntriggered by recent advances in AI that enable multi-modal generative models.\nSome of these models can create representations of geometries, networks and\nimages, and reason flexibly at a human-compatible semantic level. They provide\nhuge amounts of knowledge extracted from Terabytes of text and image documents\nand cover the required rich representation spectrum including geographic\nknowledge by different knowledge sources, degrees of granularity and scales.\n  We then discuss what these new opportunities mean for the modeling challenges\nposed by cities, in particular with regard to the role and impact of citizens\nand their interactions within the city infrastructure. We propose to integrate\nthese possibilities with existing approaches, such as agent-based models, which\nopens up new modeling spaces including rich citizen models which are able to\nalso represent social interactions.\n  Finally, we put forward some thoughts about a vision of a \"social AI in a\ncity ecosystem\" that adds relevant citizen models to state-of-the-art\nstructural and process models. This extended city representation will enable\nurban planners to establish citizen-oriented planning of city infrastructures\nfor human culture, city resilience and sustainability.",
      "generated_abstract": "City modeling is the study of large, complex systems, such as cities, and the\nvarious interactions between them. This field has experienced a surge of\nresearch in recent years, with a wide range of topics being explored. In this\nreview, we provide an overview of the evolution of city modeling and its\ncurrent state of research. We first examine the origins of city modeling and its\nevolution through the years, focusing on the different stages of development.\nThen, we discuss the current state of research and its main branches, with a\nparticular focus on urban modeling. Finally, we provide a future outlook on\ncity modeling and its applications to various fields, including transportation,\nenvironmental management, and economic growth.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.32432432432432434,
          "f": 0.20338982620367718
        },
        "rouge-2": {
          "r": 0.015384615384615385,
          "p": 0.038834951456310676,
          "f": 0.02203856342842473
        },
        "rouge-l": {
          "r": 0.12345679012345678,
          "p": 0.2702702702702703,
          "f": 0.16949152111893143
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.00994v2",
      "true_abstract": "This study evaluates the performance of Vehicle-to-Vehicle Visible Light\nCommunication in dynamic environments, focusing on the effects of speed,\nhorizontal offset, and other factors on communication reliability. Using On-Off\nKeying modulation, we analyze the BER, optimal communication distance,\ncorrelation time and the maximum amount of data per communication. Our results\ndemonstrate that maintaining an optimal vehicle distance is critical for stable\ncommunication, with speed and horizontal offset significantly influencing\ncommunication. This work extends the analysis of V-VLC to real-world dynamic\nscenarios, providing insights for future research.",
      "generated_abstract": "2-Vision Light Communication (V2V-LLC) is a novel V2V communication\nsystem that leverages the unique characteristics of light to achieve high\ndata rates and low latency over short ranges, while maintaining\nrobustness and safety in motion scenarios. This paper presents the performance\nevaluation of V2V-LLC in motion scenarios using a coherence time (CT) and a\nthroughput (ThrP) metric. The CT metric measures the time required for the\nreception of a packet, while the ThrP metric quantifies the throughput of a\npacket over a certain time duration. The CT metric is derived from the\nreception of a packet, while the ThrP metric is derived from the transmission\nof a packet. The results show that the CT and ThrP metrics are highly\ncorrelated, with an R2 value of 0.99. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27941176470588236,
          "p": 0.25333333333333335,
          "f": 0.26573426074624684
        },
        "rouge-2": {
          "r": 0.023529411764705882,
          "p": 0.018518518518518517,
          "f": 0.020725383672046058
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.22666666666666666,
          "f": 0.23776223277421887
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.econ/GN/2412.14996v1",
      "true_abstract": "In socioeconomic systems, nonequilibrium dynamics naturally stem from the\ngenerically non-reciprocal interactions between self-interested agents, whereas\nequilibrium descriptions often only apply to scenarios where individuals act\nwith the common good in mind. We bridge these two contrasting paradigms by\nstudying a Sakoda-Schelling occupation model with both individualistic and\naltruistic agents, who, in isolation, follow nonequilibrium and equilibrium\ndynamics respectively. We investigate how the relative fraction of these two\npopulations impacts the behavior of the system. In particular, we find that\nwhen fluctuations in the agents' decision-making process are small (high\nrationality), a very moderate amount of altruistic agents mitigates the\nsub-optimal concentration of individualists in dense clusters. In the regime\nwhere fluctuations carry more weight (low rationality), on the other hand,\naltruism progressively allows the agents to coordinate in a way that is\nsignificantly more robust, which we understand by reducing the model to a\nsingle effective population studied through the lens of active matter physics.\nWe highlight that localizing the altruistic intervention at the right point in\nspace may be paramount for its effectiveness.",
      "generated_abstract": "ation Model (OM) is a microscopic model of human behavior that\nhas been extensively studied in the statistical physics literature. The OM\nrepresents a two-population model with population density as the only\ninteraction. In this work, we extend the OM by incorporating a\nnonequilibrium parameter into the dynamics of the occupation fraction,\nleading to a non-Markovian OM. We investigate the long-time behavior of the\nsystem under different nonequilibrium parameters. We find that the system\nundergoes a continuous transition between a nonequilibrium steady state (NESS)\nand a nonequilibrium stationary state (NESS). We analyze the effects of the\nnonequilibrium parameter on the behavior of the NESS. Additionally, we\ninvestigate the behavior of the system at the NESS using a mean-field\napproximation and find that the system exhib",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22580645161290322,
          "p": 0.3835616438356164,
          "f": 0.28426395472596566
        },
        "rouge-2": {
          "r": 0.06358381502890173,
          "p": 0.10377358490566038,
          "f": 0.07885304188332655
        },
        "rouge-l": {
          "r": 0.20161290322580644,
          "p": 0.3424657534246575,
          "f": 0.2538071019340875
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.04826v1",
      "true_abstract": "The reliance on large labeled datasets presents a significant challenge in\nmedical image segmentation. Few-shot learning offers a potential solution, but\nexisting methods often still require substantial training data. This paper\nproposes a novel approach that leverages the Segment Anything Model 2 (SAM2), a\nvision foundation model with strong video segmentation capabilities. We\nconceptualize 3D medical image volumes as video sequences, departing from the\ntraditional slice-by-slice paradigm. Our core innovation is a support-query\nmatching strategy: we perform extensive data augmentation on a single labeled\nsupport image and, for each frame in the query volume, algorithmically select\nthe most analogous augmented support image. This selected image, along with its\ncorresponding mask, is used as a mask prompt, driving SAM2's video\nsegmentation. This approach entirely avoids model retraining or parameter\nupdates. We demonstrate state-of-the-art performance on benchmark few-shot\nmedical image segmentation datasets, achieving significant improvements in\naccuracy and annotation efficiency. This plug-and-play method offers a powerful\nand generalizable solution for 3D medical image segmentation.",
      "generated_abstract": "vances in few-shot medical image segmentation have significantly\nchanged the landscape of the field, enabling models to generalize to\nunseen classes with limited training data. However, existing approaches often\nrely on extensive labeling efforts and/or large datasets, which are not\nscalable to real-world clinical scenarios. In this work, we present SAM2, a\nnovel training-free few-shot model that addresses this challenge by leveraging\nthe prominence of augmentative prompting and dynamic matching. SAM2 is a\npre-trained model that learns to predict the most likely augmented augmented\nprompt for each unseen class. This is achieved by combining two key components:\n1) an image-level conditional Gumbel-softmax model, which generates\naugmented prompts based on the input image; 2) a novel dynamic matching\nstrategy, which aligns the augment",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24786324786324787,
          "p": 0.3020833333333333,
          "f": 0.2723004645321696
        },
        "rouge-2": {
          "r": 0.05263157894736842,
          "p": 0.06779661016949153,
          "f": 0.05925925433854636
        },
        "rouge-l": {
          "r": 0.2222222222222222,
          "p": 0.2708333333333333,
          "f": 0.24413145044766257
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/CO/2501.15194v3",
      "true_abstract": "Short text clustering has gained significant attention in the data mining\ncommunity. However, the limited valuable information contained in short texts\noften leads to low-discriminative representations, increasing the difficulty of\nclustering. This paper proposes a novel short text clustering framework, called\nReliable \\textbf{P}seudo-labeling via \\textbf{O}ptimal \\textbf{T}ransport with\n\\textbf{A}ttention for Short Text Clustering (\\textbf{POTA}), that generate\nreliable pseudo-labels to aid discriminative representation learning for\nclustering. Specially, \\textbf{POTA} first implements an instance-level\nattention mechanism to capture the semantic relationships among samples, which\nare then incorporated as a semantic consistency regularization term into an\noptimal transport problem. By solving this OT problem, we can yield reliable\npseudo-labels that simultaneously account for sample-to-sample semantic\nconsistency and sample-to-cluster global structure information. Additionally,\nthe proposed OT can adaptively estimate cluster distributions, making\n\\textbf{POTA} well-suited for varying degrees of imbalanced datasets. Then, we\nutilize the pseudo-labels to guide contrastive learning to generate\ndiscriminative representations and achieve efficient clustering. Extensive\nexperiments demonstrate \\textbf{POTA} outperforms state-of-the-art methods. The\ncode is available at:\n\\href{https://github.com/YZH0905/POTA-STC/tree/main}{https://github.com/YZH0905/POTA-STC/tree/main}.",
      "generated_abstract": "g short text data is crucial for many applications, including\nsemantic similarity analysis, sentiment analysis, and event detection. This\ntask is often challenging due to the low density of training samples and the\nheterogeneity of labels, making traditional clustering methods ineffective.\nTo address this, we propose a novel method for pseudo-labeling short text data\nusing optimal transport (OT) with attention. Our approach leverages the\nrelationship between text and image to enable effective clustering. Specifically,\nwe propose a novel method for clustering text data using OT with attention. By\nintegrating the attention mechanism with the OT algorithm, we are able to\neffectively capture the relationship between text and image. We demonstrate\nthat our method outperforms traditional clustering methods, including K-Means\nand Hierarchical Clustering. Our results demonstrate that our method\nsignificantly outperforms traditional clustering methods",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.248,
          "p": 0.3875,
          "f": 0.30243901963117203
        },
        "rouge-2": {
          "r": 0.037267080745341616,
          "p": 0.05454545454545454,
          "f": 0.04428043798150949
        },
        "rouge-l": {
          "r": 0.224,
          "p": 0.35,
          "f": 0.2731707269482452
        }
      }
    },
    {
      "paper_id": "math.NA.cs/NA/2503.10199v1",
      "true_abstract": "The Bayesian inversion method demonstrates significant potential for solving\ninverse problems, enabling both point estimation and uncertainty\nquantification. However, Bayesian maximum a posteriori (MAP) estimation may\nbecome unstable when handling data from diverse distributions (e.g., solutions\nof stochastic partial differential equations (SPDEs)). Additionally, Monte\nCarlo sampling methods are computationally expensive. To address these\nchallenges, we propose a novel two-stage optimization method based on optimal\ncontrol theory and variational Bayesian methods. This method not only achieves\nstable solutions for stochastic inverse problems but also efficiently\nquantifies the uncertainty of the solutions. In the first stage, we introduce a\nnew weighting formulation to ensure the stability of the Bayesian MAP\nestimation. In the second stage, we derive the necessary condition to\nefficiently quantify the uncertainty of the solutions, by combining the new\nweighting formula with variational inference. Furthermore, we establish an\nerror estimation theorem that relates the exact solution to the optimally\nestimated solution under different amounts of observed data. Finally, the\nefficiency of the proposed method is demonstrated through numerical examples.",
      "generated_abstract": "This paper addresses the challenges of stochastic inverse problems through\nvariational Bayesian methods. The approach begins with a stochastic forward\nmodel that involves stochastic noise and an uncertain parameter. The goal is to\nestimate the unknown parameter or solution from a noisy observation. To do so,\nthe method employs a variational Bayesian approach, where the likelihood of\nthe observed data is modeled as a Gaussian distribution. This distribution\nincludes the noise terms as part of the prior distribution. The posterior\ndistribution, which is a Gaussian distribution, is then obtained by integrating\nover the prior distribution. In order to ensure convergence of the algorithm, a\nsmoothness constraint is imposed on the posterior distribution. This is done by\napplying a regularization term to the prior distribution. A numerical example\nis provided to illustrate the application of the proposed approach.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24793388429752067,
          "p": 0.3614457831325301,
          "f": 0.2941176422323145
        },
        "rouge-2": {
          "r": 0.06134969325153374,
          "p": 0.07936507936507936,
          "f": 0.06920414733109073
        },
        "rouge-l": {
          "r": 0.23140495867768596,
          "p": 0.3373493975903614,
          "f": 0.2745097990950596
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06318v1",
      "true_abstract": "This article discusses the key principles of radio spectrum management with a\nfocus on spectrum allocation and access. We show the current regime's inherent\nrigidity and constrained possibilities for introducing new radiocommunication\nservices and applications. The article proposes how governments and spectrum\nusers could cooperate in taking spectrum management to a qualitatively new\nlevel, characterized by light touch regulation and flexible use. This could be\nachieved through the broader introduction of emerging practices such as\nSpectrum Usage Rights, liberalized spectrum trading, and full shared spectrum\naccess. We conclude by presenting a vision for a 'perfect' spectrum management\narrangement and future research directions.",
      "generated_abstract": "The current spectrum management system is inefficient, fragmented, and\nlarge. This paper proposes an efficient and effective spectrum management\nsystem based on a perfect game framework, which combines the game theory and\nperfect information game to achieve the optimal spectrum allocation. The\nperfect information game framework is used to evaluate the optimal spectrum\nallocation, and the perfect game framework is used to determine the optimal\ngame value. The game value is used to determine the optimal spectrum allocation\nthrough a dynamic programming approach. The feasibility of the proposed\nsituation is verified through a case study.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1891891891891892,
          "p": 0.2916666666666667,
          "f": 0.22950819194840108
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.0273972602739726,
          "f": 0.02339180797236859
        },
        "rouge-l": {
          "r": 0.17567567567567569,
          "p": 0.2708333333333333,
          "f": 0.21311474932545024
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2412.16850v1",
      "true_abstract": "We introduce a model for limit order book of a certain security with two main\nfeatures: First, both the limit orders and market orders for the given asset\nare allowed to appear and interact with each other. Second, the high frequency\ntrading activities are allowed and described by the scaling limit of\nnearly-unstable multi-dimensional Hawkes processes with power law decay. The\nmodel has been derived as a stochastic partial differential equation (SPDE, for\nshort), under certain intuitive identifications. Its diffusion coefficient is\ndetermined by a Volterra integral equation driven by a Hawkes process, whose\nHurst exponent is less than 1/2 (so that the relevant process is negatively\ncorrelated). As a result, the volatility path of the SPDE is rougher than that\ndriven by a (standard) Brownian motion. The well-posedness follows from a\nresult in literature. Hence, a foundation is laid down for further studies in\nthis direction.",
      "generated_abstract": "uency trading (HFT) is a crucial component of financial markets,\nusing large orders to achieve high prices. This paper introduces a new\nhigh-frequency trading model, based on the rough volatility framework, to\nquantify the impact of rough volatility on the HFT process. The rough volatility\nis approximated by the generalized Gaussian model and the model is derived from\nthe diffusion process. The proposed model is a non-linear stochastic\ndifferential equation with a large number of parameters. To solve the model,\nwe propose a numerical algorithm based on a finite difference method. Numerical\nexperiments demonstrate that the proposed model can capture the main features\nof HFT. The rough volatility model is applied to the HFT process of the NYSE\nStock Index and the CBOE Volatility Index. The results show that the rough\nvolatility model can effectively capture",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22330097087378642,
          "p": 0.2987012987012987,
          "f": 0.2555555506598766
        },
        "rouge-2": {
          "r": 0.028169014084507043,
          "p": 0.034482758620689655,
          "f": 0.031007746988763484
        },
        "rouge-l": {
          "r": 0.1941747572815534,
          "p": 0.2597402597402597,
          "f": 0.2222222173265433
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/OT/2503.02645v1",
      "true_abstract": "Mixup is a widely adopted data augmentation technique known for enhancing the\ngeneralization of machine learning models by interpolating between data points.\nDespite its success and popularity, limited attention has been given to\nunderstanding the statistical properties of the synthetic data it generates. In\nthis paper, we delve into the theoretical underpinnings of mixup, specifically\nits effects on the statistical structure of synthesized data. We demonstrate\nthat while mixup improves model performance, it can distort key statistical\nproperties such as variance, potentially leading to unintended consequences in\ndata synthesis. To address this, we propose a novel mixup method that\nincorporates a generalized and flexible weighting scheme, better preserving the\noriginal data's structure. Through theoretical developments, we provide\nconditions under which our proposed method maintains the (co)variance and\ndistributional properties of the original dataset. Numerical experiments\nconfirm that the new approach not only preserves the statistical\ncharacteristics of the original data but also sustains model performance across\nrepeated synthesis, alleviating concerns of model collapse identified in\nprevious research.",
      "generated_abstract": "a widely used data augmentation technique for improving the\nstability of training data. However, existing theoretical studies on Mixup-based\ndata augmentation fail to address the generality and stability of Mixup,\nespecially when the augmentation operator is not a linear mapping. In this\npaper, we introduce a novel theory of Mixup, called the Generalized\nTheory of Mixup (GTM). The GTM provides a unified framework for analyzing\nvarious Mixup operators and provides a complete understanding of Mixup's\ngeneralization capabilities. Additionally, we provide a new theoretical\nguarantee for Mixup, which extends the existing guarantees for Cutout and\nClipping. Theoretical guarantees are provided for Mixup in the original\n$L_p$ norm and in the Wasserstein distance. We show that GTM provides\ntighter guarantees in both norms, which further establishes the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24793388429752067,
          "p": 0.36585365853658536,
          "f": 0.29556649764760135
        },
        "rouge-2": {
          "r": 0.056962025316455694,
          "p": 0.07627118644067797,
          "f": 0.06521738640936815
        },
        "rouge-l": {
          "r": 0.21487603305785125,
          "p": 0.3170731707317073,
          "f": 0.25615763065252745
        }
      }
    },
    {
      "paper_id": "physics.comp-ph.hep-ex/2503.09213v1",
      "true_abstract": "The scientific communities of nuclear, particle, and astroparticle physics\nare continuing to advance and are facing unprecedented software challenges due\nto growing data volumes, complex computing needs, and environmental\nconsiderations. As new experiments emerge, software and computing needs must be\nrecognised and integrated early in design phases. This document synthesises\ninsights from ECFA, NuPECC and APPEC, representing particle physics, nuclear\nphysics, and astroparticle physics, and presents collaborative strategies for\nimproving software, computing frameworks, infrastructure, and career\ndevelopment within these fields.",
      "generated_abstract": "rt presents the findings of the first working group of the\nJoint European Network for Accelerator-based Science (JENA) Computing Initiative\nand describes the state-of-the-art in software and heterogeneous architectures\nused in JENA research. The report is divided into two sections. The first\nsection describes the JENA software stack and the heterogeneous architecture\nof the JENA Computing Initiative. The second section provides a detailed\ndescription of the JENA computing infrastructure, including the JENA Computing\nInitiative computing nodes, the JENA Computing Initiative Grid, and the\nJENA Computing Initiative Virtual Machine. The JENA Computing Initiative\nsoftware stack is the most critical component of the JENA computing infrastructure\nand has undergone significant advancements since the initial release in 2023.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.140625,
          "p": 0.14754098360655737,
          "f": 0.14399999500288016
        },
        "rouge-2": {
          "r": 0.012987012987012988,
          "p": 0.01098901098901099,
          "f": 0.0119047569394862
        },
        "rouge-l": {
          "r": 0.140625,
          "p": 0.14754098360655737,
          "f": 0.14399999500288016
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.17005v1",
      "true_abstract": "This study documents the relationship between computer skills/digital\nliteracy and influenza vaccination take-up among older adults in Europe during\nand after the COVID-19 pandemic. Using data from the Survey of Health, Aging\nand Retirement in Europe, we find a positive partial association between\ninfluenza vaccination take-up and two indicators of computer skills/digital\nliteracy, self-assessed pre-pandemic computer skills and having used a computer\nat work in any pre-pandemic job. We do not estimate significant behavioural\nchanges for individuals with better computer skills that may have been driven\nby spillover effects from the pandemic experience.",
      "generated_abstract": "The 2009 influenza pandemic highlighted the need to improve vaccination\nhesitancy rates across Europe. This paper examines the association between\nvaccination hesitancy and digital literacy among the adult population, using\nthe Eurobarometer dataset. Results indicate that countries with lower levels of\ndigital literacy are more likely to report higher levels of vaccination\nhesitancy, particularly for the elderly. However, this association is not\nrobust, and other factors, such as income, education level, and country\ncomposition, are also significant. These results highlight the need for\nimproved digital literacy and educational initiatives to reduce vaccination\nhesitancy and improve the public health response to future epidemics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2463768115942029,
          "p": 0.2328767123287671,
          "f": 0.23943661472227742
        },
        "rouge-2": {
          "r": 0.022988505747126436,
          "p": 0.02127659574468085,
          "f": 0.022099442521291687
        },
        "rouge-l": {
          "r": 0.21739130434782608,
          "p": 0.2054794520547945,
          "f": 0.2112676006377704
        }
      }
    },
    {
      "paper_id": "physics.gen-ph.physics/gen-ph/2502.15841v1",
      "true_abstract": "The present paper reanalyzes the problem of the refractive properties of the\nphysical vacuum and their modification under the action of the gravitational\nfield and the electromagnetic field. This problem was studied in our previous\nworks and in the subsequent works of the researchers: Leuchs, Urban, Mainland\nand their collaborators. By modeling the physical vacuum as a\nparticle-antiparticle system, we can deduce with a certain approximation, in a\nsemiclassical theory, the properties of the free vacuum and the vacuum modified\nby the interaction with a gravitational field and an electromagnetic field.\nMore precise calculation of permittivities of free vacuum and near a particle\ncan lead to a non-point model of the particle. This modeling can follow both\nthe quantum and the general relativistic path as well as the phenomenological\npath, the results complementing each other.",
      "generated_abstract": "al studies of the vacuum permittivity and gravitational refractive\nindex are presented and reviewed. The vacuum permittivity and refractive\nindex are defined as the complex-valued permittivity and refractive index of\nthe vacuum, respectively, and are defined as the complex conjugate of the\ncomplex-valued permittivity and refractive index of the medium. The vacuum\npermittivity and refractive index are also referred to as the vacuum\ndielectric and gravitational dielectric functions, respectively. The vacuum\ndielectric function is defined as the complex conjugate of the vacuum\ngravitational dielectric function. The vacuum permittivity and refractive\nindex are also referred to as the vacuum dielectric and gravitational\nrefractive index, respectively. The vacuum permitt",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12162162162162163,
          "p": 0.28125,
          "f": 0.16981131653969397
        },
        "rouge-2": {
          "r": 0.02586206896551724,
          "p": 0.061224489795918366,
          "f": 0.036363632188062926
        },
        "rouge-l": {
          "r": 0.12162162162162163,
          "p": 0.28125,
          "f": 0.16981131653969397
        }
      }
    },
    {
      "paper_id": "math.FA.math/OA/2503.01331v1",
      "true_abstract": "We introduce a new family of non-negative real-valued functions on a\n$C^*$-algebra $\\mathcal{A}$, i.e., for $0\\leq \\mu \\leq 1,$\n$$\\|a\\|_{\\sigma_{\\mu}}= \\text{sup}\\left\\lbrace \\sqrt{|f(a)|^2 \\sigma_{\\mu}\nf(a^*a)}: f\\in \\mathcal{A}', \\, f(1)=\\|f\\|=1 \\right\\rbrace, \\quad $$ where\n$a\\in \\mathcal{A}$ and $\\sigma_{\\mu}$ is an interpolation path of the symmetric\nmean $\\sigma$. These functions are semi-norms as they satisfy the norm axioms,\nexcept for the triangle inequality. Special cases satisfying triangle\ninequality, and a complete equality characterization is also discussed. Various\nbounds and relationships will be established for this new family, with a\nconnection to the existing literature in the algebra of all bounded linear\noperators on a Hilbert space.",
      "generated_abstract": "We introduce a class of semi-norms on $C^*$-algebras, for which the\ndensity of the set of nonzero elements is continuous in the norm topology.\n  We prove that these semi-norms are in fact norm-compact.\n  We also show that a family of semi-norms on $C^*$-algebras, that we call\n$\\tau$-semi-norms, is a dense family in the norm topology, and is a\ncompactification of the family of seminorms in the sense of G.~B\\\"ugler,\n$C^*$-algebras with the norm topology, and has the same density of the set of\nnonzero elements.\n  We conclude by giving a characterisation of the family of $\\tau$-semi-norms\nthat is norm-compact.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18604651162790697,
          "p": 0.32653061224489793,
          "f": 0.23703703241262006
        },
        "rouge-2": {
          "r": 0.05825242718446602,
          "p": 0.07692307692307693,
          "f": 0.06629833763682465
        },
        "rouge-l": {
          "r": 0.16279069767441862,
          "p": 0.2857142857142857,
          "f": 0.2074074027829905
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.09917v1",
      "true_abstract": "Economists disagree about the factors driving the substantial increase in\nresidual wage inequality in the US over the past few decades. To identify\nchanges in the returns to unobserved skills, we make a novel assumption about\nthe dynamics of skills rather than about the stability of skill distributions\nacross cohorts, as is standard. We show that our assumption is supported by\ndata on test score dynamics for older workers in the HRS. Using survey data\nfrom the PSID and administrative data from the IRS and SSA, we estimate that\nthe returns to unobserved skills $declined$ substantially in the late-1980s and\n1990s despite an increase in residual inequality. Accounting for firm-specific\npay differences yields similar results. Extending our framework to consider\noccupational differences in returns to skill and multiple unobserved skills, we\nfurther show that skill returns display similar patterns for workers employed\nin each of cognitive, routine, and social occupations. Finally, our results\nsuggest that increasing skill dispersion, driven by rising skill volatility,\nexplains most of the growth in residual wage inequality since the 1980s.",
      "generated_abstract": "y introduces a novel methodology to estimate skill returns in\nthe United States. Unlike traditional approaches that use panel data to estimate\nskill returns, this study uses a large-scale dataset of 1.8 million respondents\nand 7.1 million observations to capture the full range of economic conditions.\nThis dataset allows us to capture the evolution of skill returns from the\nbeginning of the pandemic to today. Using a dynamic panel model, we estimate\nskill returns by aggregating observations over time periods. Our estimates\nreveal that skill returns in the United States have risen sharply in the past\ntwo years, reflecting the impact of the COVID-19 pandemic and the strong\neconomic recovery. We also find that the skill returns have been relatively\nstable in the post-pandemic period, suggesting that the recovery has not\nexacerbated skill gaps. These findings suggest that the recent increase in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20192307692307693,
          "p": 0.23863636363636365,
          "f": 0.21874999503472234
        },
        "rouge-2": {
          "r": 0.07741935483870968,
          "p": 0.096,
          "f": 0.08571428077168396
        },
        "rouge-l": {
          "r": 0.20192307692307693,
          "p": 0.23863636363636365,
          "f": 0.21874999503472234
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.00348v1",
      "true_abstract": "The increasing frequency of environmental hazards due to climate change\nunderscores the urgent need for effective monitoring systems. Current\napproaches either rely on expensive labelled datasets, struggle with seasonal\nvariations, or require multiple observations for confirmation (which delays\ndetection). To address these challenges, this work presents SHAZAM -\nSelf-Supervised Change Monitoring for Hazard Detection and Mapping. SHAZAM uses\na lightweight conditional UNet to generate expected images of a region of\ninterest (ROI) for any day of the year, allowing for the direct modelling of\nnormal seasonal changes and the ability to distinguish potential hazards. A\nmodified structural similarity measure compares the generated images with\nactual satellite observations to compute region-level anomaly scores and\npixel-level hazard maps. Additionally, a theoretically grounded seasonal\nthreshold eliminates the need for dataset-specific optimisation. Evaluated on\nfour diverse datasets that contain bushfires (wildfires), burned regions,\nextreme and out-of-season snowfall, floods, droughts, algal blooms, and\ndeforestation, SHAZAM achieved F1 score improvements of between 0.066 and 0.234\nover existing methods. This was achieved primarily through more effective\nhazard detection (higher recall) while using only 473K parameters. SHAZAM\ndemonstrated superior mapping capabilities through higher spatial resolution\nand improved ability to suppress background features while accentuating both\nimmediate and gradual hazards. SHAZAM has been established as an effective and\ngeneralisable solution for hazard detection and mapping across different\ngeographical regions and a diverse range of hazards. The Python code is\navailable at: https://github.com/WiseGamgee/SHAZAM",
      "generated_abstract": "years, earthquakes have become increasingly frequent and intense\nEarthquake prediction has become a crucial challenge due to the devastating\nimpacts of earthquakes. Although seismic sensors have been widely used to\nmonitor earthquake-induced hazards, the data-driven approaches for earthquake\nprediction still face challenges such as high costs, limited sample size, and\ninsufficient temporal resolution. In this paper, we propose a novel self-supervised\nchange monitoring framework named SHAZAM, which leverages the self-supervised\nlearning technique to extract hazard information from the earthquake data. The\nframework consists of a self-supervised feature extractor, a temporal\naggregator, and a self-supervised change monitor. The feature extractor\nextracts features from the earthquake data and generates a feature representation\nthat captures the temporal",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10857142857142857,
          "p": 0.24050632911392406,
          "f": 0.14960629492683997
        },
        "rouge-2": {
          "r": 0.01293103448275862,
          "p": 0.027777777777777776,
          "f": 0.01764705448858238
        },
        "rouge-l": {
          "r": 0.08571428571428572,
          "p": 0.189873417721519,
          "f": 0.11811023193471401
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.06331v1",
      "true_abstract": "Models with unnormalized probability density functions are ubiquitous in\nstatistics, artificial intelligence and many other fields. However, they face\nsignificant challenges in model selection if the normalizing constants are\nintractable. Existing methods to address this issue often incur high\ncomputational costs, either due to numerical approximations of normalizing\nconstants or evaluation of bias corrections in information criteria. In this\npaper, we propose a novel and fast selection criterion, T-GIC, for nested\nmodels, allowing direct data sampling from a possibly unnormalized probability\ndensity function. T-GIC gives a consistent selection under mild regularity\nconditions and is computationally efficient, benefiting from a multiplying\nfactor that depends only on the sample size and the model complexity. Extensive\nsimulation studies and real-data applications demonstrate the efficacy of T-GIC\nin the selection of nested models with unnormalized probability densities.",
      "generated_abstract": "We present a fast algorithm to select the nested models that explain most of\nthe variation in a multivariate time series. The algorithm is based on the\nconsistent selection of the conditional mean and covariance functions. The\nalgorithm is implemented in R and is available on GitHub at\nhttps://github.com/Jonathan-Mannor/nwm-selection.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15306122448979592,
          "p": 0.40540540540540543,
          "f": 0.22222221824307276
        },
        "rouge-2": {
          "r": 0.03968253968253968,
          "p": 0.10869565217391304,
          "f": 0.05813953096538696
        },
        "rouge-l": {
          "r": 0.12244897959183673,
          "p": 0.32432432432432434,
          "f": 0.17777777379862833
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2503.01148v1",
      "true_abstract": "This paper investigates the risk spillovers among AI ETFs, AI tokens, and\ngreen markets using the R2 decomposition method. We reveal several key\ninsights. First, the overall transmission connectedness index (TCI) closely\naligns with the contemporaneous TCI, while the lagged TCI is significantly\nlower. Second, AI ETFs and clean energy act as risk transmitters, whereas AI\ntokens and green bond function as risk receivers. Third, AI tokens are\ndifficult to hedge and provide limited hedging ability compared to AI ETFs and\ngreen assets. However, multivariate portfolios effectively reduce AI tokens\ninvestment risk. Among them, the minimum correlation portfolio outperforms the\nminimum variance and minimum connectedness portfolios.",
      "generated_abstract": "This study examines the dynamic spillover effects of artificial intelligence\n(AI) ETFs, AI token funds, and green market ETFs on traditional stock markets.\nThe findings indicate that, in the near term, the AI ETFs and AI token funds\nprovide more diversification and risk reduction benefits than green market\nETFs. However, the benefits of AI ETFs and AI token funds are likely to\ndecline over time due to the increasing competition and convergence of these\nmarkets. Meanwhile, the green market ETFs are expected to provide a more\npositive impact on stock markets, especially in the long term, due to their\nability to mitigate climate change.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18666666666666668,
          "p": 0.2222222222222222,
          "f": 0.202898545762445
        },
        "rouge-2": {
          "r": 0.041237113402061855,
          "p": 0.04395604395604396,
          "f": 0.042553186494455095
        },
        "rouge-l": {
          "r": 0.18666666666666668,
          "p": 0.2222222222222222,
          "f": 0.202898545762445
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2501.04718v1",
      "true_abstract": "Gene panel selection aims to identify the most informative genomic biomarkers\nin label-free genomic datasets. Traditional approaches, which rely on domain\nexpertise, embedded machine learning models, or heuristic-based iterative\noptimization, often introduce biases and inefficiencies, potentially obscuring\ncritical biological signals. To address these challenges, we present an\niterative gene panel selection strategy that harnesses ensemble knowledge from\nexisting gene selection algorithms to establish preliminary boundaries or prior\nknowledge, which guide the initial search space. Subsequently, we incorporate\nreinforcement learning through a reward function shaped by expert behavior,\nenabling dynamic refinement and targeted selection of gene panels. This\nintegration mitigates biases stemming from initial boundaries while\ncapitalizing on RL's stochastic adaptability. Comprehensive comparative\nexperiments, case studies, and downstream analyses demonstrate the\neffectiveness of our method, highlighting its improved precision and efficiency\nfor label-free biomarker discovery. Our results underscore the potential of\nthis approach to advance single-cell genomics data analysis.",
      "generated_abstract": "tudy, we present a reinforcement learning approach to identify\nbiomarkers from single-cell RNA sequencing (scRNA-seq) data. Specifically, we\napply the reinforcement learning framework to identify protein biomarkers from\nthe SPRITE data set, which is the largest and most comprehensive scRNA-seq\ndataset of its kind. The SPRITE data set includes over 200,000 scRNA-seq\nsamples, which are collected from 12 different cell types. We employ a\ndiverse set of reinforcement learning algorithms, including DQN, DQN-R,\nPPO, and PPO-R, to train and evaluate models that identify protein biomarkers.\nThe performance of each algorithm was evaluated through a number of\nmetrics, including accuracy, precision, recall, F1 score, and a novel metric\n(RR",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.26666666666666666,
          "f": 0.2051282003944774
        },
        "rouge-2": {
          "r": 0.03424657534246575,
          "p": 0.05,
          "f": 0.04065040167889541
        },
        "rouge-l": {
          "r": 0.15833333333333333,
          "p": 0.25333333333333335,
          "f": 0.19487179013806719
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.10260v1",
      "true_abstract": "Our study focuses on isolating swallowing dynamics from interfering patient\nmotion in videofluoroscopy, an X-ray technique that records patients swallowing\na radiopaque bolus. These recordings capture multiple motion sources, including\nhead movement, anatomical displacements, and bolus transit. To enable precise\nanalysis of swallowing physiology, we aim to eliminate distracting motion,\nparticularly head movement, while preserving essential swallowing-related\ndynamics. Optical flow methods fail due to artifacts like flickering and\ninstability, making them unreliable for distinguishing different motion groups.\nWe evaluated markerless tracking approaches (CoTracker, PIPs++, TAP-Net) and\nquantified tracking accuracy in key medical regions of interest. Our findings\nshow that even sparse tracking points generate morphing displacement fields\nthat outperform leading registration methods such as ANTs, LDDMM, and\nVoxelMorph. To compare all approaches, we assessed performance using MSE and\nSSIM metrics post-registration. We introduce a novel motion correction pipeline\nthat effectively removes disruptive motion while preserving swallowing dynamics\nand surpassing competitive registration techniques. Code will be available\nafter review.",
      "generated_abstract": "based registration (TBR) is a fundamental and widely used method\nfor motion correction in medical imaging. However, traditional methods often\nrely on manual labeling and require accurate and reliable patient-specific\nmarkers, which are prone to error. In this paper, we propose a novel\nmarkerless-TBR (MTBR) method for motion correction in medical imaging. The\nmain contribution of our approach is to eliminate the need for manual\nmarkers. Our method uses a learned affine transformation to register images\nacross different patient-specific image datasets, and it is built on the\nconcept of a diffusion-based diffusion model. The diffusion model allows for\ndiffusion-based registration of images between different datasets, leveraging\nthe inherent similarity between the patient images across different datasets.\nTo further enhance the robustness of the method, we propose a multi-scale\nlearning approach, which is tailored to the specific",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1532258064516129,
          "p": 0.22093023255813954,
          "f": 0.18095237611609988
        },
        "rouge-2": {
          "r": 0.01948051948051948,
          "p": 0.024193548387096774,
          "f": 0.02158272887117757
        },
        "rouge-l": {
          "r": 0.1532258064516129,
          "p": 0.22093023255813954,
          "f": 0.18095237611609988
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.10635v1",
      "true_abstract": "Large language models (LLMs) offer the potential to automate a large number\nof tasks that previously have not been possible to automate, including some in\nscience. There is considerable interest in whether LLMs can automate the\nprocess of causal inference by providing the information about causal links\nnecessary to build a structural model. We use the case of confounding in the\nCoronary Drug Project (CDP), for which there are several studies listing\nexpert-selected confounders that can serve as a ground truth. LLMs exhibit\nmediocre performance in identifying confounders in this setting, even though\ntext about the ground truth is in their training data. Variables that experts\nidentify as confounders are only slightly more likely to be labeled as\nconfounders by LLMs compared to variables that experts consider\nnon-confounders. Further, LLM judgment on confounder status is highly\ninconsistent across models, prompts, and irrelevant concerns like\nmultiple-choice option ordering. LLMs do not yet have the ability to automate\nthe reporting of causal links.",
      "generated_abstract": "guage Models (LLMs) have been adopted by academics and policy\nofficials as a powerful tool for generating and interpreting causal knowledge.\nYet, their application in causal inference remains under-explored. We study the\ndynamics of LLM-generated causal knowledge, focusing on whether they can be\nconsidered repositories of causal knowledge, a concept that has been explored in\nthe literature on the replication crisis in science. We first demonstrate that\nLLMs can generate causal knowledge, both explicit and implicit, but they are\nrarely explicit. Instead, they often generate complex, non-causal descriptions\nthat can be interpreted as latent causal knowledge. Our results highlight the\ndivergent roles of explicit and implicit causal knowledge in LLMs. We also\ninvestigate the extent to which LLMs can be considered repositories of causal\nknowledge",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22321428571428573,
          "p": 0.32051282051282054,
          "f": 0.263157889896953
        },
        "rouge-2": {
          "r": 0.0457516339869281,
          "p": 0.06306306306306306,
          "f": 0.053030298156853055
        },
        "rouge-l": {
          "r": 0.19642857142857142,
          "p": 0.28205128205128205,
          "f": 0.23157894252853198
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.02146v1",
      "true_abstract": "Reliance on stereotypes is a persistent feature of human decision-making and\nhas been extensively documented in educational settings, where it can shape\nstudents' confidence, performance, and long-term human capital accumulation.\nWhile effective techniques exist to mitigate these negative effects, a crucial\nfirst step is to establish whether teachers can recognize stereotypes in their\nprofessional environment. We introduce the Stereotype Identification Test\n(SIT), a novel survey tool that asks teachers to evaluate and comment on the\npresence of stereotypes in images randomly drawn from school textbooks. Their\nresponses are systematically linked to established measures of implicit bias\n(Implicit Association Test, IAT) and explicit bias (survey scales on teaching\nstereotypes and social values). Our findings demonstrate that the SIT is a\nvalid and reliable measure of stereotype recognition. Teachers' ability to\nrecognize stereotypes is linked to trainable traits such as implicit bias\nawareness and inclusive teaching practices. Moreover, providing personalized\nfeedback on implicit bias improves SIT scores by 0.25 standard deviations,\nreinforcing the idea that stereotype recognition is malleable and can be\nenhanced through targeted interventions.",
      "generated_abstract": "Stereotypes are a fundamental phenomenon in human cognition. However, it is\nknown that they can be altered by social influences. We show that this effect\ncan be amplified by the social interaction of individuals. For example, if a\nparticipant is confronted by a group of women, she may see all of them as\nwomen, even if they are in fact men. We propose a theoretical framework to\nunderstand this phenomenon and provide a mathematical proof of the existence\nof a threshold that separates the presence of stereotypes from their absence.\nWe also show that the threshold depends on the number of participants and the\ntype of interactions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1774193548387097,
          "p": 0.3188405797101449,
          "f": 0.22797927001745025
        },
        "rouge-2": {
          "r": 0.03614457831325301,
          "p": 0.057692307692307696,
          "f": 0.044444439708093794
        },
        "rouge-l": {
          "r": 0.1774193548387097,
          "p": 0.3188405797101449,
          "f": 0.22797927001745025
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.05946v1",
      "true_abstract": "Despite growing evidence that neighborhoods play a critical role in shaping\neconomic mobility and well-being, effective policies to address neighborhood\ndisadvantage remain elusive. This study evaluates the impact of the Promise\nZone program, which aims to revitalize disadvantaged neighborhoods through\nstreamlined federal support and grant incentives. I use an event study\nframework with newly obtained data on the location of failed finalist\napplications as a comparison group to estimate the effects of the program. The\nresults reveal significant improvements in poverty, household incomes, and\nemployment in Promise Zone neighborhoods, particularly in later-designated\nzones and initially low-status neighborhoods. I also find that effects are\ndriven partly by changes in residential composition, and that Promise Zones\nappear to induce positive spillovers in adjacent areas.",
      "generated_abstract": "We examine whether Promise Zone (PZ) programs, which provide incentives\nfor local governments to invest in neighborhoods, improve economic\nopportunity and reduce poverty. We find that PZs improve economic\nopportunity and reduce poverty, but only in areas that already had\nsignificant economic and social challenges. This suggests that PZs are\nineffective in areas where there is little existing economic disadvantage.\nUsing an instrumental variable approach, we find that PZs are associated\nwith a 0.22 percentage point increase in the unemployment rate, and a 0.18\npercentage point increase in the poverty rate, with no effect on employment or\nincome. These results suggest that PZs are not a significant factor in\nimproving economic opportunity in America's most economically challenged\nareas.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25806451612903225,
          "p": 0.3037974683544304,
          "f": 0.27906976247498655
        },
        "rouge-2": {
          "r": 0.01680672268907563,
          "p": 0.0196078431372549,
          "f": 0.01809954254089938
        },
        "rouge-l": {
          "r": 0.22580645161290322,
          "p": 0.26582278481012656,
          "f": 0.24418604154475398
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.01084v1",
      "true_abstract": "The United States leads the world in the number of violent mass shootings\nthat occur each year, and policy making on firearms remains polarized along\nparty lines. Are legislators responsive to mass shootings? We estimate the\nlatent positions of nearly 2,000 state legislators on gun policy from their\nroll-call voting records on firearm-related bills from 2011 to 2022. Employing\na staggered difference-in-differences design, we find that mass shootings\nwithin or near a state legislator's district do not alter their voting behavior\non firearm policy, on average, for members of both parties. Our estimated\neffects of mass shootings on treated legislators' support for restrictive gun\npolicies (on a -1 to 1 scale) range from a 4.8% reduction among California\nDemocrats and a 0.9% increase among California Republicans to, across six total\nstates, a 5% (among Democrats) and 7.1% (among Republicans) increase, with 95%\nconfidence intervals spanning opposite directions. We conclude that, on\naverage, mass shootings fail to produce changes in a legislator's support\n(opposition) for restrictive (permissive) firearms bills. Our findings suggest\nthat even the most heinous acts of mass violence -- that are squarely in the\ndomain of events that state legislators might respond to -- fail to produce any\nmeasurable effects on legislators' positions on firearm-related policy.",
      "generated_abstract": "e whether U.S. state legislators respond to mass shootings,\nusing a novel data set of state legislative votes on 28 bills introduced in\n2018-2022. We find that the majority of bills introduced during this period\ntarget gun control and do not have a direct impact on mass shootings. However,\nsome bills introduced during this period may have a short-term impact on gun\ncontrol by reducing access to firearms. We find that these bills had a\nsignificant impact on gun control, with a 1% increase in support for gun\ncontrol in the state legislature leading to a 1.4% increase in support for\nbackground checks and a 3.0% increase in support for red flag laws. We find no\nsignificant impact of bills introduced during the 2020-2022 period. These findings\nhighlight the importance",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19852941176470587,
          "p": 0.35526315789473684,
          "f": 0.2547169765325739
        },
        "rouge-2": {
          "r": 0.050505050505050504,
          "p": 0.09259259259259259,
          "f": 0.06535947255670928
        },
        "rouge-l": {
          "r": 0.18382352941176472,
          "p": 0.32894736842105265,
          "f": 0.2358490520042721
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.cond-mat/mes-hall/2503.10359v1",
      "true_abstract": "In this study, we systematically explore the non-Hermitian skin effect (NHSE)\nand its associated complex-frequency detection in the context of a\nfrequency-dependent non-Hermitian Hamiltonian. This Hamiltonian arises from the\nself-energy correction of the subsystem and can be calculated exactly within\nour theoretical model, without the need for non-Hermitian approximations.\nAdditionally, complex frequency detection, which encompasses complex frequency\nexcitation, synthesis, and fingerprint, enables us to detect the physical\nresponses driven by complex frequency excitations. Our calculations reveal that\nboth complex frequency excitation and synthesis are sensitive to the\nnon-Hermitian approximation and are unable to characterize the presence or\nabscence of the NHSE. In contrast, the complex-frequency fingerprint\nsuccessfully detects the novel responses induced by the NHSE through the\nintroduction of a double-frequency Green's function. Our work paves the way for\na rigorous understanding of non-Hermitian physics in quantum systems and their\nexperimental verification through complex frequency-domain techniques.",
      "generated_abstract": "We present a novel frequency detection technique for low-frequency signals\nin a subsystem, where a localized signal can be detected in the absence of\ninterference from the subsystem. The method utilizes the interference patterns\nof a localized signal, which can be described by a set of complex frequency\ncoefficients. These coefficients can be extracted from the interference\npatterns in the subsystem, and used to extract the localized signal. This\ntechnique can be used to detect weak signals in systems where the localized\nsignal is weak, and where interference from the subsystem is strong. The\nmethod can be used to detect weak signals in a subsystem, even when the\nsubsystem is operating at a high frequency. The method can be used to detect\nweak signals in a subsystem, even when the subsystem is operating at a high\nfrequency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20408163265306123,
          "p": 0.37735849056603776,
          "f": 0.2649006576957152
        },
        "rouge-2": {
          "r": 0.050724637681159424,
          "p": 0.08045977011494253,
          "f": 0.06222221747911147
        },
        "rouge-l": {
          "r": 0.19387755102040816,
          "p": 0.3584905660377358,
          "f": 0.25165562458313234
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.05512v1",
      "true_abstract": "Recently, large language model (LLM) based text-to-speech (TTS) systems have\ngradually become the mainstream in the industry due to their high naturalness\nand powerful zero-shot voice cloning capabilities.Here, we introduce the\nIndexTTS system, which is mainly based on the XTTS and Tortoise model. We add\nsome novel improvements. Specifically, in Chinese scenarios, we adopt a hybrid\nmodeling method that combines characters and pinyin, making the pronunciations\nof polyphonic characters and long-tail characters controllable. We also\nperformed a comparative analysis of the Vector Quantization (VQ) with\nFinite-Scalar Quantization (FSQ) for codebook utilization of acoustic speech\ntokens. To further enhance the effect and stability of voice cloning, we\nintroduce a conformer-based speech conditional encoder and replace the\nspeechcode decoder with BigVGAN2. Compared with XTTS, it has achieved\nsignificant improvements in naturalness, content consistency, and zero-shot\nvoice cloning. As for the popular TTS systems in the open-source, such as\nFish-Speech, CosyVoice2, FireRedTTS and F5-TTS, IndexTTS has a relatively\nsimple training process, more controllable usage, and faster inference speed.\nMoreover, its performance surpasses that of these systems. Our demos are\navailable at https://index-tts.github.io.",
      "generated_abstract": "This paper presents IndexTTS, an industrial-level, controllable, and\nefficient zero-shot text-to-speech (TTS) system. The system is based on the\npre-trained OpenTTS model, which is capable of producing high-quality speech\nthrough continuous speech synthesis. Additionally, the system is equipped with\na custom-designed TTS model, which can be controlled through three key\nparameters: pitch, loudness, and rhythm. The proposed system can be used to\nproduce natural, high-quality speech with a wide range of parameters, ensuring\nthat it can meet the diverse needs of different users and applications. In\naddition, the system is designed to be easily extended with new parameters and\ncontrols. This makes it ideal for use in various industrial settings,\nenhancing the capabilities of the system and expanding its applicability.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16030534351145037,
          "p": 0.25301204819277107,
          "f": 0.19626167749454113
        },
        "rouge-2": {
          "r": 0.028409090909090908,
          "p": 0.04504504504504504,
          "f": 0.034843200831381385
        },
        "rouge-l": {
          "r": 0.12213740458015267,
          "p": 0.1927710843373494,
          "f": 0.14953270553192435
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2410.07566v1",
      "true_abstract": "Transaction Fee Mechanism Design studies auctions run by untrusted miners for\ntransaction inclusion in a blockchain. Under previously-considered desiderata,\nan auction is considered `good' if, informally-speaking, each party (i.e., the\nminer, the users, and coalitions of both miners and users) has no incentive to\ndeviate from the fixed and pre-determined protocol.\n  In this paper, we propose a novel desideratum for transaction fee mechanisms.\nWe say that a TFM is off-chain influence proof when the miner cannot achieve\nadditional revenue by running a separate auction off-chain. While the\npreviously-highlighted EIP-1559 is the gold-standard according to prior\ndesiderata, we show that it does not satisfy off-chain influence proofness.\nIntuitively, this holds because a Bayesian revenue-maximizing miner can\nstrictly increase profits by persuasively threatening to censor any bids that\ndo not transfer a tip directly to the miner off-chain.\n  On the other hand, we reconsider the Cryptographic (multi-party computation\nassisted) Second Price Auction mechanism, which is technically not `simple for\nminers' according to previous desiderata (since miners may wish to set a\nreserve by fabricating bids). We show that, in a slightly different model where\nthe miner is allowed to set the reserve directly, this auction satisfies\nsimplicity for users and miners, and off-chain influence proofness.\n  Finally, we prove a strong impossibility result: no mechanism satisfies all\npreviously-considered properties along with off-chain influence proofness, even\nwith unlimited supply, and even after soliciting input from the miner.",
      "generated_abstract": "r revisits the design of transaction fee mechanism (TFM) with the\nobvious goal of providing a mechanism that is efficient and fair. However,\nexisting TFM designs suffer from a number of shortcomings. First, existing TFM\ndesigns do not address the issue of asymmetric information, which can be a\nsignificant challenge in real-world applications. Second, existing TFM designs\noften suffer from a lack of transparency, making it difficult for users to\nunderstand how their choices affect the outcomes of the TFM. Third, existing\nTFM designs often do not account for the risk of double spending, which can be\na significant concern in real-world applications. In this paper, we propose a\nnew approach to TFM design, referred to as the Primitives of Transaction Fee\nMechanism Design. The Primitives of Transaction Fee Mechanism Design\nrepresents",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1935483870967742,
          "p": 0.37037037037037035,
          "f": 0.25423728362719056
        },
        "rouge-2": {
          "r": 0.04954954954954955,
          "p": 0.1,
          "f": 0.06626505580998725
        },
        "rouge-l": {
          "r": 0.17419354838709677,
          "p": 0.3333333333333333,
          "f": 0.22881355481363116
        }
      }
    },
    {
      "paper_id": "gr-qc.quant-ph/2503.10348v1",
      "true_abstract": "The fact that quantum theory is non-differentiable, while general relativity\nis built on the assumption of differentiability sources an incompatibility\nbetween quantum theory and gravity. Higher order geometry addresses this issue\ndirectly by extending differential geometry, such that it can be applied to\ntheories that are non-differentiable, but have a certain degree of H\\\"older\nregularity. As this includes the path integral formulation of quantum theory,\nit provides a natural mathematical framework for describing the interplay\nbetween gravity and quantum theory. In this article, we review the motivation\nfor and the basic features of this framework and point towards future\ndevelopments.",
      "generated_abstract": "In this review, we investigate the relations between quantum theory,\nhigher-order geometry and the theory of gravity. We will consider the most\nbasic form of quantum theory, the theory of classical dynamics and the quantum\ntheory of a single particle. We will then consider the theory of quantum\ndynamics and gravity, using the formalism of higher-order geometry. We will\ndiscuss the equivalence between the quantum theory of gravity and the theory of\nclassical dynamics. Finally, we will discuss the relations between quantum\ntheory, gravity and higher-order geometry.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19444444444444445,
          "p": 0.4117647058823529,
          "f": 0.2641509390388039
        },
        "rouge-2": {
          "r": 0.07216494845360824,
          "p": 0.125,
          "f": 0.09150326333290638
        },
        "rouge-l": {
          "r": 0.18055555555555555,
          "p": 0.38235294117647056,
          "f": 0.24528301451050197
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.10552v1",
      "true_abstract": "In this paper, we propose a new workflow to analyze macrophage motion during\nwound healing. These immune cells are attracted to the wound after an injury\nand they move showing both directional and random motion. Thus, first, we\nsmooth the trajectories and we separate the random from the directional parts\nof the motion. The smoothing model is based on curve evolution where the curve\nmotion is influenced by the smoothing term and the attracting term. Once we\nobtain the random sub-trajectories, we analyze them using the mean squared\ndisplacement to characterize the type of diffusion. Finally, we compute the\nvelocities on the smoothed trajectories and use them as sparse samples to\nreconstruct the wound attractant field. To do that, we consider a minimization\nproblem for the vector components and lengths, which leads to solving the\nLaplace equation with Dirichlet conditions for the sparse samples and zero\nNeumann boundary conditions on the domain boundary.",
      "generated_abstract": "healing process involves complex interactions between cells,\nfibroblasts, and extracellular matrix components. This study focuses on\nunderstanding the immune response to skin wounds through the use of a numerical\nsimulation model. This model includes a cellular population, a fibroblast\npopulation, and a matrix. We use the numerical simulation model to investigate\nthe movement of immune cells, fibroblasts, and matrix during wound healing. We\nexplore the role of different cell types and the matrix in the wound healing\nprocess. Our numerical model simulates wound healing in an artificial skin\nmodel. The results of this study show that the immune cells can travel through\nthe matrix, leaving behind a scar. The immune cells can also travel through the\nmatrix and interact with fibroblasts, causing them to proliferate and form new\nfibroblasts",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20833333333333334,
          "p": 0.2857142857142857,
          "f": 0.24096385054434613
        },
        "rouge-2": {
          "r": 0.03424657534246575,
          "p": 0.043859649122807015,
          "f": 0.03846153353727874
        },
        "rouge-l": {
          "r": 0.1875,
          "p": 0.2571428571428571,
          "f": 0.2168674650021775
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2501.11983v2",
      "true_abstract": "We provide closed-form market equilibrium formula consolidating informational\nimperfections and investors beliefs. Based on Merton's model, we characterize\nthe equilibrium expected excess returns vector with incomplete information. We\nthen derive the corresponding market portfolio as the solution to a non-linear\nsystem of equations and analyze the sensitivities of extra excess returns to\nshadow-costs and market weights. We derive the market reference model for\nexcess returns under random shadow-costs. The conditional posterior\ndistribution of excess returns integrates the pick-matrix and pick-vector of\nviews and the vector of shadow-costs into a multivariate distribution with mean\nand covariance dependent on the reference model.",
      "generated_abstract": "In this paper, we propose a novel asset pricing model based on the\nmodel of imperfect information and subjective views (I/V model) for the\nprice process. In the model, we first derive the closed-form solutions for the\nprice process and the volatility process. Then, we use the method of\nparameterization to derive the analytical solutions for the mean-variance\noptimization problem, which are then used to obtain the optimal asset price\nprocess. Furthermore, we study the existence and uniqueness of the solution for\nthe mean-variance optimization problem. Finally, we apply the proposed model to\na case study on the stock market and obtain the optimal asset price process.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2786885245901639,
          "p": 0.2982456140350877,
          "f": 0.28813558822608454
        },
        "rouge-2": {
          "r": 0.06382978723404255,
          "p": 0.06666666666666667,
          "f": 0.06521738630671114
        },
        "rouge-l": {
          "r": 0.2459016393442623,
          "p": 0.2631578947368421,
          "f": 0.25423728314133875
        }
      }
    },
    {
      "paper_id": "cs.CR.econ/GN/2501.09025v2",
      "true_abstract": "The digital age, driven by the AI revolution, brings significant\nopportunities but also conceals security threats, which we refer to as cyber\nshadows. These threats pose risks at individual, organizational, and societal\nlevels. This paper examines the systemic impact of these cyber threats and\nproposes a comprehensive cybersecurity strategy that integrates AI-driven\nsolutions, such as Intrusion Detection Systems (IDS), with targeted policy\ninterventions. By combining technological and regulatory measures, we create a\nmultilevel defense capable of addressing both direct threats and indirect\nnegative externalities. We emphasize that the synergy between AI-driven\nsolutions and policy interventions is essential for neutralizing cyber threats\nand mitigating their negative impact on the digital economy. Finally, we\nunderscore the need for continuous adaptation of these strategies, especially\nin response to the rapid advancement of autonomous AI-driven attacks, to ensure\nthe creation of secure and resilient digital ecosystems.",
      "generated_abstract": "rity is a growing concern in both the public and private sectors,\nwith AI tools and policy measures playing an increasingly important role.\nHowever, the integration of AI tools with policy measures is challenging due\nto the potentially destabilizing effects of AI-driven cyberattacks on critical\ninfrastructure. This paper explores the potential of combining AI with\ntargeted policy measures to mitigate security threats. By integrating AI\ntechniques with policy measures, we can enhance cybersecurity by reducing the\nlikelihood of security breaches and minimizing the impact of cyberattacks. This\npaper examines the potential of AI-driven cybersecurity measures to mitigate\nsecurity threats and proposes a framework for integrating AI with policy\nmeasures to enhance cybersecurity. By leveraging AI techniques, we can\nenhance cybersecurity",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2692307692307692,
          "p": 0.4307692307692308,
          "f": 0.3313609420118344
        },
        "rouge-2": {
          "r": 0.08823529411764706,
          "p": 0.12244897959183673,
          "f": 0.1025640976959605
        },
        "rouge-l": {
          "r": 0.23076923076923078,
          "p": 0.36923076923076925,
          "f": 0.2840236639053255
        }
      }
    },
    {
      "paper_id": "cs.LG.q-fin/PR/2407.14573v6",
      "true_abstract": "Since the advent of generative artificial intelligence, every company and\nresearcher has been rushing to develop their own generative models, whether\ncommercial or not. Given the large number of users of these powerful new tools,\nthere is currently no intrinsically verifiable way to explain from the ground\nup what happens when LLMs (large language models) learn. For example, those\nbased on automatic speech recognition systems, which have to rely on huge and\nastronomical amounts of data collected from all over the web to produce fast\nand efficient results, In this article, we develop a backdoor attack called\nMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is\nmainly based on modern stock market models. In order to show the possible\nvulnerabilities of speech-based transformers that may rely on LLMs.",
      "generated_abstract": "nd investors are often interested in predicting market trends,\nforecasting the future, and determining the best time to enter or exit a trade.\nTrading and investing are complex and involve significant risk, and traders and\ninvestors often turn to machine learning models for guidance. However, many\nmodels rely on a training dataset with limited data and no real-world\napplication, and their performance is often limited by overfitting. In this\nwork, we introduce a real-world trading system that integrates machine learning\nwith a stock market. Our system combines historical data from the stock market\nwith a machine learning model to predict the market trend. The system uses\nhistorical market data to train a model that predicts the future trend of the\nstock market. Then the system uses this model to predict the next trading day's\nmarket trend. The system also uses historical data to train a Bay",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19801980198019803,
          "p": 0.25,
          "f": 0.2209944702054273
        },
        "rouge-2": {
          "r": 0.031746031746031744,
          "p": 0.031496062992125984,
          "f": 0.0316205483597627
        },
        "rouge-l": {
          "r": 0.18811881188118812,
          "p": 0.2375,
          "f": 0.20994474644852124
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.07834v1",
      "true_abstract": "The Uniswap is a Decentralized Exchange (DEX) protocol that facilitates\nautomatic token exchange without the need for traditional order books. Every\npair of tokens forms a liquidity pool on Uniswap, and each token can be paired\nwith any other token to create liquidity pools. This characteristic motivates\nus to employ a complex network approach to analyze the features of the Uniswap\nmarket. This research presents a comprehensive analysis of the Uniswap network\nusing complex network methods. The network on October 31, 2023, is built to\nobserve its recent features, showcasing both scale-free and core-periphery\nproperties. By employing node and edge-betweenness metrics, we detect the most\nimportant tokens and liquidity pools. Additionally, we construct daily networks\nspanning from the beginning of Uniswap V2 on May 5, 2020, until October 31,\n2023, and our findings demonstrate that the network becomes increasingly\nfragile over time. Furthermore, we conduct a robustness analysis by simulating\nthe deletion of nodes to estimate the impact of some extreme events such as the\nTerra collapse. The results indicate that the Uniswap network exhibits\nrobustness, yet it is notably fragile when deleting tokens with high\nbetweenness centrality. This finding highlights that, despite being a\ndecentralized exchange, Uniswap exhibits significant centralization tendencies\nin terms of token network connectivity and the distribution of TVL across nodes\n(tokens) and edges (liquidity pools).",
      "generated_abstract": "y explores the network structure of the Uniswap decentralized\nexchange (DEX) market, analyzing its criticality to the network and the\ninfluence of its centralized counterpart, Uniswap V3. The DEX network is\nconstructed using the network theory framework, including the degree centrality\nand closeness centrality. The study finds that the DEX network is strongly\nconnected, with a strong centralization effect on the DEX. The study also\nexamines the network's vulnerability to external threats, finding that the DEX\nis not immune to disruptions. The DEX network's centrality and vulnerability\nhave implications for the DEX's resilience, suggesting that strategies for\nenhancing network resilience may be necessary to mitigate the DEX's potential\nvulnerabilities. The study offers insights into the DEX",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14965986394557823,
          "p": 0.3142857142857143,
          "f": 0.202764972588078
        },
        "rouge-2": {
          "r": 0.023809523809523808,
          "p": 0.049019607843137254,
          "f": 0.03205127765039509
        },
        "rouge-l": {
          "r": 0.1360544217687075,
          "p": 0.2857142857142857,
          "f": 0.18433179286457568
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.q-bio/SC/2406.16569v1",
      "true_abstract": "In this article we present a comprehensive study of the totally asymmetric\nsimple exclusion process with pausing particles (pTASEP), a model initially\nintroduced to describe RNAP dynamics during transcription. We extend previous\nmean-field approaches and demonstrate that the pTASEP is equivalent to the\nexclusion process with dynamical defects (ddTASEP), thus broadening the scope\nof our investigation to a larger class of problems related to transcription and\ntranslation. We extend the mean-field theory to the open boundary case,\nrevealing the system's phase diagram and critical values of entry and exit\nrates. However, we identify a significant discrepancy between theory and\nsimulations in a region of the parameter space, indicating severe finite-size\neffects. To address this, we develop a single-cluster approximation that\ncaptures the relationship between current and lattice size, providing a more\naccurate representation of the system's dynamics. Finally, we extend our\napproach to open boundary conditions, demonstrating its applicability in\ndifferent scenarios. Our findings underscore the importance of considering\nfinite-size effects, often overlooked in the literature, when modelling\nbiological processes such as transcription and translation.",
      "generated_abstract": "a driven lattice gas with particle pausing and defects, which can be\napplied to biological systems such as bacterial populations and viruses.\n  Our model is described by the Ising model with an external driving field\n$\\Delta\\phi$, where $\\Delta\\phi=0$ is the equilibrium state and $\\Delta\\phi=1$\ncorresponds to the critical point. The model is described by a lattice of sites\nwith a time-dependent interaction strength $V(t)$ that varies in time due to\na driving force. In this work, we study the finite-size effects of the model on\nthe dynamics of the particles and the emergence of critical phenomena.\n  We first consider the case of a single site with the time-dependent\ninteraction strength, $V(t)=V_0\\cos(\\omega t)$, where $V_0$ is the\ninitial value and $\\omega$",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.3291139240506329,
          "f": 0.26530611763692213
        },
        "rouge-2": {
          "r": 0.036585365853658534,
          "p": 0.05309734513274336,
          "f": 0.04332129480848234
        },
        "rouge-l": {
          "r": 0.20512820512820512,
          "p": 0.3037974683544304,
          "f": 0.24489795437161607
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.19754v3",
      "true_abstract": "This paper examines whether artificial intelligence (AI) acts as a substitute\nor complement to human labour, drawing on 12 million online job vacancies from\nthe United States spanning 2018-2023. We adopt a two-pronged approach: first,\nanalysing \"internal effects\" within roles explicitly requiring AI, and second,\ninvestigating \"external effects\" that arise when industries, occupations, and\nregions experience increases in AI demand. Our focus centres on whether\ncomplementary skills-such as digital literacy, teamwork, resilience, agility,\nor analytical thinking-become more prevalent and valuable as AI adoption grows.\nResults show that AI-focused roles are nearly twice as likely to require skills\nlike resilience, agility, or analytical thinking compared to non-AI roles.\nFurthermore, these skills command a significant wage premium; data scientists,\nfor instance, are offered 5-10% higher salaries if they also possess resilience\nor ethics capabilities. We observe positive spillover effects: a doubling of\nAI-specific demand across industries correlates with a 5% increase in demand\nfor complementary skills, even outside AI-related roles. Conversely, tasks\nvulnerable to AI substitution, such as basic data skills or translation,\nexhibit modest declines in demand. However, the external effect is clearly net\npositive: Complementary effects are up to 1.7x larger than substitution\neffects. These results are consistent across economies, including the United\nKingdom and Australia. Our findings highlight the necessity of reskilling\nworkers in areas where human expertise remains increasingly valuable and\nensuring workers can effectively complement and leverage emerging AI\ntechnologies.",
      "generated_abstract": "r addresses the question of how the emerging field of Artificial\nIndustrial Revolution (AIIR) is affecting human capital demand. We develop a\ndynamic framework to measure the demand for human capital in AIIR-based\nindustries. We show that AIIR-based industries have a positive impact on the\ndemand for human capital, which is larger than the impact of technological\ninnovation. Furthermore, we find that AIIR-based industries have a negative\nimpact on the demand for human capital, which is smaller than the impact of\ntechnological innovation. These results suggest that the demand for human\ncapital is a complementary aspect of AIIR-based industries. We also show that\nAIIR-based industries have a positive impact on the demand for human capital,\nwhich is larger than the impact of technological innovation. Furthermore, we\nfind that AIIR-based",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1329479768786127,
          "p": 0.45098039215686275,
          "f": 0.20535713934032207
        },
        "rouge-2": {
          "r": 0.017543859649122806,
          "p": 0.0547945205479452,
          "f": 0.026578069415569873
        },
        "rouge-l": {
          "r": 0.11560693641618497,
          "p": 0.39215686274509803,
          "f": 0.17857142505460785
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.08013v1",
      "true_abstract": "Most of the existing research on pursuit-evasion game (PEG) is conducted in a\ntwo-dimensional (2D) environment. In this paper, we investigate the PEG in a 3D\nspace. We extend the Apollonius circle (AC) to the 3D space and introduce its\ndetailed analytical form. To enhance the capture efficiency, we derive the\noptimal motion space for both the pursuer and the evader. To address the issue\narising from a discrete state space, we design a fuzzy actor-critic learning\n(FACL) algorithm to obtain the agents' strategies. To improve learning\nperformance, we devise a reward function for the agents, which enables obstacle\navoidance functionality. The effectiveness of the proposed algorithm is\nvalidated through simulation experiments.",
      "generated_abstract": "r proposes a three-dimensional pursuit-evasion game based on\nfuzzy actor-critic learning algorithm. The proposed game is based on the\nthree-dimensional pursuit-evasion game, which consists of three agents and a\ncommon environment. The three agents are the pursuer, the evader, and the\nenvironment. The environment is represented as a three-dimensional space, and\nthe three agents are represented as agents in the space. In the game, the\npursuer and the evader are two agents, and the evader is a potential adversary\nof the pursuer. The evader can take different actions to change the state of\nthe environment, which is called the state of the environment. The pursuer\nhas to predict the actions of the evader and the state of the environment to\nensure the safety of the pursuer. The state of the environment can be\nrepresented as the three-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.30864197530864196,
          "p": 0.45454545454545453,
          "f": 0.3676470540062717
        },
        "rouge-2": {
          "r": 0.07407407407407407,
          "p": 0.07766990291262135,
          "f": 0.07582937888906391
        },
        "rouge-l": {
          "r": 0.2962962962962963,
          "p": 0.43636363636363634,
          "f": 0.35294117165333055
        }
      }
    },
    {
      "paper_id": "cs.LG.math/OC/2503.10352v1",
      "true_abstract": "Popular safe Bayesian optimization (BO) algorithms learn control policies for\nsafety-critical systems in unknown environments. However, most algorithms make\na smoothness assumption, which is encoded by a known bounded norm in a\nreproducing kernel Hilbert space (RKHS). The RKHS is a potentially\ninfinite-dimensional space, and it remains unclear how to reliably obtain the\nRKHS norm of an unknown function. In this work, we propose a safe BO algorithm\ncapable of estimating the RKHS norm from data. We provide statistical\nguarantees on the RKHS norm estimation, integrate the estimated RKHS norm into\nexisting confidence intervals and show that we retain theoretical guarantees,\nand prove safety of the resulting safe BO algorithm. We apply our algorithm to\nsafely optimize reinforcement learning policies on physics simulators and on a\nreal inverted pendulum, demonstrating improved performance, safety, and\nscalability compared to the state-of-the-art.",
      "generated_abstract": "We introduce a novel safe exploration strategy for gradient descent\ntrajectories in the reproducing kernel Hilbert space (RKHS) that are driven by\na finite number of gradient-based steps. Our approach leverages the structure\nof the RKHS to construct a stochastic gradient-based exploration policy that\nis safe for both Lipschitz and non-Lipschitz functions. Our method provides a\nrobust and principled approach for gradient-based optimization, offering\npromise for applications in high-dimensional optimization, non-convex\noptimization, and reinforcement learning.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20202020202020202,
          "p": 0.37037037037037035,
          "f": 0.26143790392925803
        },
        "rouge-2": {
          "r": 0.061068702290076333,
          "p": 0.10810810810810811,
          "f": 0.07804877587436078
        },
        "rouge-l": {
          "r": 0.1717171717171717,
          "p": 0.3148148148148148,
          "f": 0.22222221765474828
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2412.02435v1",
      "true_abstract": "In approval-based budget division, a budget needs to be distributed to some\ncandidates based on the voters' approval ballots over these candidates. In the\npursuit of simple, well-behaved, and approximately fair rules for this setting,\nwe introduce the class of sequential payment rules, where each voter controls a\npart of the budget and repeatedly spends his share on his approved candidates\nto determine the final distribution. We show that all sequential payment rules\nsatisfy a demanding population consistency notion and we identify two\nparticularly appealing rules within this class called the maximum payment rule\n(MP) and the $\\frac{1}{3}$-multiplicative sequential payment rule\n($\\frac{1}{3}$-MP). More specifically, we prove that (i) MP is, apart from one\nother rule, the only monotonic sequential payment rule and gives a\n$2$-approximation to a fairness notion called average fair share, and (ii)\n$\\frac{1}{3}$-MP gives a $\\frac{3}{2}$-approximation to average fair share,\nwhich is optimal among sequential payment rules.",
      "generated_abstract": "uce a simple spending dynamics that is guaranteed to produce a\nfair division of budget in a sequential payment setting. This framework\nprovides a simple, efficient way to analyze and approximate fair budget\ndivisions. Furthermore, we show that the dynamics we introduce is the\noptimal one, and we derive the optimal fairness condition. The dynamics are\ncomputationally efficient, and they are guaranteed to be strictly feasible. We\nalso analyze a special case, namely the case where the payee can spend all of\nhis/her budget before the end of the round, and the payee's spending rule is\noptimal. We further show that the dynamics we introduce is optimal for this\nspecial case. We also show that the dynamics we introduce is optimal for any\npayee with a strict budget, including the case where the payee can spend all\nof his/her budget before the end of the round. We show that the dynamics we",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2553191489361702,
          "p": 0.3333333333333333,
          "f": 0.2891566215938453
        },
        "rouge-2": {
          "r": 0.07194244604316546,
          "p": 0.08771929824561403,
          "f": 0.07905137844803105
        },
        "rouge-l": {
          "r": 0.22340425531914893,
          "p": 0.2916666666666667,
          "f": 0.25301204328059235
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.15849v1",
      "true_abstract": "Western music is an innately hierarchical system of interacting levels of\nstructure, from fine-grained melody to high-level form. In order to analyze\nmusic compositions holistically and at multiple granularities, we propose a\nunified, hierarchical meta-representation of musical structure called the\nstructural temporal graph (STG). For a single piece, the STG is a data\nstructure that defines a hierarchy of progressively finer structural musical\nfeatures and the temporal relationships between them. We use the STG to enable\na novel approach for deriving a representative structural summary of a music\ncorpus, which we formalize as a dually NP-hard combinatorial optimization\nproblem extending the Generalized Median Graph problem. Our approach first\napplies simulated annealing to develop a measure of structural distance between\ntwo music pieces rooted in graph isomorphism. Our approach then combines the\nformal guarantees of SMT solvers with nested simulated annealing over\nstructural distances to produce a structurally sound, representative centroid\nSTG for an entire corpus of STGs from individual pieces. To evaluate our\napproach, we conduct experiments verifying that structural distance accurately\ndifferentiates between music pieces, and that derived centroids accurately\nstructurally characterize their corpora.",
      "generated_abstract": "a complex, dynamic, and multimodal domain that is often analyzed\nthrough a combination of acoustic, rhythmic, and textual features. In this\nstudy, we present a novel method for extracting representative structure from\nmusic corpora. Our approach uses a hierarchical feature model, which first\nlearns a dictionary of features, followed by an encoder that encodes the\ndictionary into a high-dimensional feature space. The encoder is then used as a\nfeature extractor to generate a representative structure. This structure is\nthen used as a template for further feature learning, which is performed using\nan LSTM-based model. The resulting structure is then used as a template for\nfeature learning, which is performed using an LSTM-based model. We show that\nour approach performs well on both the WSJ0 and the WSJ1 music corpora,\ndemonstrating that it",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2231404958677686,
          "p": 0.34177215189873417,
          "f": 0.2699999952205001
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.04504504504504504,
          "f": 0.03436425644949935
        },
        "rouge-l": {
          "r": 0.19834710743801653,
          "p": 0.3037974683544304,
          "f": 0.23999999522050003
        }
      }
    },
    {
      "paper_id": "cs.GR.cs/GR/2503.09864v1",
      "true_abstract": "Recent advances in text-to-image (T2I) diffusion models have enabled\nremarkable control over various attributes, yet precise color specification\nremains a fundamental challenge. Existing approaches, such as ColorPeel, rely\non model personalization, requiring additional optimization and limiting\nflexibility in specifying arbitrary colors. In this work, we introduce\nColorWave, a novel training-free approach that achieves exact RGB-level color\ncontrol in diffusion models without fine-tuning. By systematically analyzing\nthe cross-attention mechanisms within IP-Adapter, we uncover an implicit\nbinding between textual color descriptors and reference image features.\nLeveraging this insight, our method rewires these bindings to enforce precise\ncolor attribution while preserving the generative capabilities of pretrained\nmodels. Our approach maintains generation quality and diversity, outperforming\nprior methods in accuracy and applicability across diverse object categories.\nThrough extensive evaluations, we demonstrate that ColorWave establishes a new\nparadigm for structured, color-consistent diffusion-based image synthesis.",
      "generated_abstract": "ork, we introduce a novel approach for color control in diffusion\nmodel-based rendering (DMBR), leveraging semantic attribute binding to\nautomatically generate free-lunch conditions that simultaneously control\nappearance and lighting. We propose a novel color generation approach that\nenables the use of semantic attribute binding in DMBR, allowing for the\nautomatic generation of free-lunch conditions. Our approach leverages\nattribute-based conditional sampling to generate a single color for the\nentire image, with each attribute conditioned on the semantic attributes of\nthe image. This enables the generation of free-lunch conditions that simultaneously\ncontrol appearance and lighting. Additionally, we propose a novel\nattribute-based conditional sampling approach to generate color conditions,\nwhich is more effective than traditional attribute-based conditional\nsampling approaches. Our experiments demonstrate that our method significantly\noutperforms state-of-the-art methods in rendering quality, rendering time, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20689655172413793,
          "p": 0.3333333333333333,
          "f": 0.25531914421004986
        },
        "rouge-2": {
          "r": 0.07352941176470588,
          "p": 0.09174311926605505,
          "f": 0.08163264812194948
        },
        "rouge-l": {
          "r": 0.20689655172413793,
          "p": 0.3333333333333333,
          "f": 0.25531914421004986
        }
      }
    },
    {
      "paper_id": "cond-mat.other.cond-mat/other/2503.04533v1",
      "true_abstract": "In this article it is argued that synthetic axions, emergent collective\nexcitations in topological insulators or Weyl semimetals hybridize with the\ncosmological axion, a compelling dark matter candidate via a common two photon\ndecay channel since they both couple to electromagnetic fields via a\nChern-Simons term. We point out an analogy to a V-type three level system with\nthe two upper levels identified with the synthetic and cosmological axions\ndecaying into a two-photon state. The Weisskopf-Wigner theory of spontaneous\ndecay in multi level atoms is complemented and extended to describe the\ndynamics of hybridization. The final two-photon state features both kinematic\nand polarization entanglement and displays quantum beats as a consequence of\nthe interference between the decay paths. An initial population of either axion\ninduces a population of the other via hybridization. Consequently, a dark\nmatter axion condensate induces a condensate of the synthetic axion, albeit\nwith small amplitude. We obtain a momentum and polarization resolved Hanbury-\nBrown Twiss (HBT) second order coherence describing coincident correlated\ntwo-photon detection. It exhibits quantum beats with a frequency given by the\ndifference between the energies of the synthetic and cosmological axion and\n\\emph{perhaps may be harnessed} to detect either type of axion excitations. The\ncase of synthetic axions individually is obtained in the limit of vanishing\ncoupling of the cosmological axion and features similar two-photon\ncorrelations. Hence second order (HBT) two-photon coherence \\emph{may} provide\nan alternative detection mechanism for emergent condensed matter axionic\ncollective excitations. Similarities and differences with parametrically down\nconverted photons are discussed.",
      "generated_abstract": "hat the entanglement of axions and photons can be enhanced in\ncosmological scenarios that involve a large number of axions, by means of the\nentanglement of photons. In particular, we show how to entangle the\nphotons and axions in the same cosmic string background, by means of a\nnon-trivial time-dependent potential. We show that, when the photons and the\naxions are in a non-degenerate, entangled state, the entanglement can be\nenhanced by a factor of $10^2-10^3$, depending on the axion mass and the\nnumber of photons. We also study the entanglement of photons and axions in the\ncase of a cosmological axion-photon hybrid background. We find that the\nentanglement is enhanced by a factor of $10^3-1",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1342281879194631,
          "p": 0.3508771929824561,
          "f": 0.19417475327881994
        },
        "rouge-2": {
          "r": 0.013157894736842105,
          "p": 0.03333333333333333,
          "f": 0.018867920469918992
        },
        "rouge-l": {
          "r": 0.10738255033557047,
          "p": 0.2807017543859649,
          "f": 0.15533980182250928
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2409.06877v1",
      "true_abstract": "We present some results helpful for parameterising positive equilibria, and\nbounding the number of positive nondegenerate equilibria, in mass action\nnetworks. Any mass action network naturally gives rise to a set of polynomial\nequations whose positive solutions are precisely the positive equilibria of the\nnetwork. Here we derive alternative systems of equations, often also\npolynomial, whose solutions are in smooth, one-to-one correspondence with\npositive equilibria of the network. Often these alternative systems are simpler\nthan the original mass action equations, and allow us to infer useful bounds on\nthe number of positive equilibria. The alternative equation systems can also be\nhelpful for parameterising the equilibrium set explicitly, for deriving\ndescriptions of the parameter regions for multistationarity, and for studying\nbifurcations. We present the main construction, some bounds which follow for\nparticular classes of networks, numerous examples, and some open questions and\nconjectures.",
      "generated_abstract": "We study the properties of positive equilibrium of mass action networks. We\nfind a lower bound on the number of stable equilibria, and a lower bound on the\nnumber of positive equilibria. We also give an upper bound on the number of\npositive equilibria in terms of the total number of states in the network. We\nalso study the stability of positive equilibria and give conditions under which\npositive equilibria are asymptotically stable.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2289156626506024,
          "p": 0.5428571428571428,
          "f": 0.3220338941324332
        },
        "rouge-2": {
          "r": 0.08,
          "p": 0.18867924528301888,
          "f": 0.11235954637987644
        },
        "rouge-l": {
          "r": 0.20481927710843373,
          "p": 0.4857142857142857,
          "f": 0.2881355890476875
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/SR/2503.09699v1",
      "true_abstract": "We introduce a novel approach to detecting microlensing events and other\ntransients in light curves, utilising the isolation forest (iForest) algorithm\nfor anomaly detection. Focusing on the Legacy Survey of Space and Time by the\nVera C. Rubin Observatory, we show that an iForest trained on signal-less light\ncurves can efficiently identify microlensing events by different types of dark\nobjects and binaries, as well as variable stars. We further show that the\niForest has real-time applicability through a drip-feed analysis, demonstrating\nits potential as a valuable tool for LSST alert brokers to efficiently\nprioritise and classify transient candidates for follow-up observations.",
      "generated_abstract": "The Large Synoptic Survey Telescope (LSST) is a new telescope with an\ndramatic 10-year survey time-series data set. With the new data, we have to\nfind anomaly detection methods to identify the transients in the time-series\ndata. We found the method is the time-series anomaly detection and the\ncomparative analysis between the time-series data and the model data. We\nanalyze the results, and find the time-series anomaly detection method can\nimprove the performance of the time-series data anomaly detection. In addition,\nwe also analyze the results, and find that the time-series anomaly detection\nmethod is very suitable for the LSST time-series data anomaly detection. In the\nfuture, we will further analyze the results of the time-series anomaly detection\nmethod and improve the time-series anomaly detection method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24675324675324675,
          "p": 0.3333333333333333,
          "f": 0.2835820846636222
        },
        "rouge-2": {
          "r": 0.030612244897959183,
          "p": 0.03409090909090909,
          "f": 0.03225805953058234
        },
        "rouge-l": {
          "r": 0.24675324675324675,
          "p": 0.3333333333333333,
          "f": 0.2835820846636222
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.cond-mat/supr-con/2503.09709v1",
      "true_abstract": "We study the interplay between quantum geometry, interactions, and external\nfields in complex band systems. When Wannier obstructions preclude a\ndescription based solely on atomic-like orbitals,this complicates the\nprediction of electromagnetic responses particularly in the presence of\ndisorder and interactions. In this work, we introduce a generalized Peierls\nsubstitution framework based on Lagrange multipliers to enforce the constraints\nof the Wannier obstruction in the band of interest. Thus we obtain effective\ndescriptions of interactions and disorder in the presence of non-trivial\nquantum geometry of that band. We apply our approach to examples including the\ndiamagnetic response in flat-band superconductors and delocalization effects in\nflat-band metals caused by interactions and disorder.",
      "generated_abstract": "We study the response of a Wannier state to the application of a generic\ngeneralized Peierls substitution (GPS) perturbation. The GPS perturbation\nincludes a unitary transformation of the Hamiltonian and a phase shift in the\nHamiltonian. We show that the GPS perturbation can induce disorder or\ninteractions to a Wannier state. We calculate the effective action of the\ndisorder and interaction perturbations on a Wannier state and analyze their\neffect on the Wannier state's dispersion. We also study the Wannier state's\nresponse to the GPS perturbation and show that the Wannier state's dispersion\ncan be modified due to the GPS perturbation. We apply our results to the\napplication of a magnetic field to a Wannier state.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2777777777777778,
          "p": 0.37735849056603776,
          "f": 0.3199999951155201
        },
        "rouge-2": {
          "r": 0.09803921568627451,
          "p": 0.11627906976744186,
          "f": 0.10638297375961997
        },
        "rouge-l": {
          "r": 0.2222222222222222,
          "p": 0.3018867924528302,
          "f": 0.25599999511552
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/AP/2503.08902v1",
      "true_abstract": "Mutual Information (MI) is a crucial measure for capturing dependencies\nbetween variables, but exact computation is challenging in high dimensions with\nintractable likelihoods, impacting accuracy and robustness. One idea is to use\nan auxiliary neural network to train an MI estimator; however, methods based on\nthe empirical distribution function (EDF) can introduce sharp fluctuations in\nthe MI loss due to poor out-of-sample performance, destabilizing convergence.\nWe present a Bayesian nonparametric (BNP) solution for training an MI estimator\nby constructing the MI loss with a finite representation of the Dirichlet\nprocess posterior to incorporate regularization in the training process. With\nthis regularization, the MI loss integrates both prior knowledge and empirical\ndata to reduce the loss sensitivity to fluctuations and outliers in the sample\ndata, especially in small sample settings like mini-batches. This approach\naddresses the challenge of balancing accuracy and low variance by effectively\nreducing variance, leading to stabilized and robust MI loss gradients during\ntraining and enhancing the convergence of the MI approximation while offering\nstronger theoretical guarantees for convergence. We explore the application of\nour estimator in maximizing MI between the data space and the latent space of a\nvariational autoencoder. Experimental results demonstrate significant\nimprovements in convergence over EDF-based methods, with applications across\nsynthetic and real datasets, notably in 3D CT image generation, yielding\nenhanced structure discovery and reduced overfitting in data synthesis. While\nthis paper focuses on generative models in application, the proposed estimator\nis not restricted to this setting and can be applied more broadly in various\nBNP learning procedures.",
      "generated_abstract": "Mutual information (MI) plays a crucial role in many practical tasks, such\nas classification, segmentation, and detection. However, MI is often ill-defined\nor not even defined in the Bayesian framework. To address this, we propose a\nBayesian nonparametric framework for MI estimation, where a novel Bayesian\nnonparametric posterior distribution is constructed to model the posterior\ndistribution of the MI estimator. The proposed model not only captures the\nuncertainty of the MI estimator but also accounts for the prior distribution of\nthe MI estimator, which is often unknown in practice. A novel Bayesian\nMonte Carlo method is proposed to sample from the proposed posterior\ndistribution. Numerical experiments demonstrate the effectiveness of the\nproposed framework.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13855421686746988,
          "p": 0.3333333333333333,
          "f": 0.1957446767029426
        },
        "rouge-2": {
          "r": 0.03305785123966942,
          "p": 0.08080808080808081,
          "f": 0.046920816993662275
        },
        "rouge-l": {
          "r": 0.13253012048192772,
          "p": 0.3188405797101449,
          "f": 0.18723403840507027
        }
      }
    },
    {
      "paper_id": "math.DG.math/AT/2503.08457v1",
      "true_abstract": "This paper explores foliated differential graded algebras (dga) and their\nrole in extending fundamental theorems of differential geometry to foliations.\nWe establish an $A_{\\infty}$ de Rham theorem for foliations, demonstrating that\nthe classical quasi-isomorphism between singular cochains and de Rham forms\nlifts to an $A_{\\infty}$ quasi-isomorphism in the foliated setting.\nFurthermore, we investigate the Riemann-Hilbert correspondence for foliations,\nbuilding upon the established higher Riemann-Hilbert correspondence for\nmanifolds. By constructing an integration functor, we prove a higher\nRiemann-Hilbert correspondence for foliations, revealing an equivalence between\n$\\infty$-representations of $L_{\\infty}$-algebroids and\n$\\infty$-representations of Lie $\\infty$-groupoids within the context of\nfoliations. This work generalizes the classical Riemann-Hilbert correspondence\nto foliations, providing a deeper understanding of the relationship between\nrepresentations of Lie algebroids and Lie groupoids in this framework.",
      "generated_abstract": "In this paper, we construct the Riemann-Hilbert correspondence between\nhigher order foliations on a compact K\\\"ahler manifold and a special class of\n$2$-forms on the K\\\"ahler manifold. For simplicity, we only consider\nK\\\"ahler-Einstein metrics on compact K\\\"ahler manifolds. We show that the\ncorrespondence is compatible with the Riemann-Hilbert correspondence between\nfoliations and $2$-forms on K\\\"ahler manifolds. Moreover, we obtain the\nRiemann-Hilbert correspondence between holomorphic foliations and\n$2$-forms on K\\\"ahler manifolds, which generalizes the Riemann-Hilbert\ncorrespondence between holomorphic foliations and $1$-forms on K\\\"ahler\nmanifolds.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20270270270270271,
          "p": 0.35714285714285715,
          "f": 0.2586206850356719
        },
        "rouge-2": {
          "r": 0.03636363636363636,
          "p": 0.06666666666666667,
          "f": 0.04705881896193816
        },
        "rouge-l": {
          "r": 0.17567567567567569,
          "p": 0.30952380952380953,
          "f": 0.22413792641498229
        }
      }
    },
    {
      "paper_id": "math.NT.cs/DM/2503.10158v1",
      "true_abstract": "Integral linear systems $Ax=b$ with matrices $A$, $b$ and solutions $x$ are\nalso required to be in integers, can be solved using invariant factors of $A$\n(by computing the Smith Canonical Form of $A$). This paper explores a new\nproblem which arises in applications, that of obtaining conditions for solving\nthe Modular Linear System $Ax=b\\rem n$ given $A,b$ in $\\zz_n$ for $x$ in\n$\\zz_n$ along with the constraint that the value of the linear function\n$\\phi(x)=<w,x>$ is coprime to $n$ for some solution $x$. In this paper we\ndevelop decomposition of the system to coprime moduli $p^{r(p)}$ which are\ndivisors of $n$ and show how such a decomposition simplifies the computation of\nSmith form. This extends the well known index calculus method of computing the\ndiscrete logarithm where the moduli over which the linear system is reduced\nwere assumed to be prime (to solve the reduced systems over prime fields) to\nthe case when the factors of the modulus are prime powers $p^{r(p)}$. It is\nshown how this problem can be addressed effciently using the invariant factors\nand Smith form of the augmented matrix $[A,-p^{r(p)}I]$ and conditions modulo\n$p$ satisfied by $w$, where $p^{r(p)}$ vary over all divisors of $n$ with $p$\nprime.",
      "generated_abstract": "We study the problem of solving modular linear systems in the context of\nthe extended Euclidean division (EE) modulo powers of prime divisors. This\nproblem arises naturally in the context of the theory of quadratic forms. Our\nmain result is a new parallel decomposition algorithm for solving\nmodular linear systems with a constraint, which we apply to solve the\nproblem of computing the solution set of a modular linear system. This problem\nwas recently studied in [Miller, S.S., Piatetski-Shapiro, G., and\nRubin, S.N. (2024), J. ACM, 61, 21, 1-36",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23684210526315788,
          "p": 0.42857142857142855,
          "f": 0.30508474117782247
        },
        "rouge-2": {
          "r": 0.042328042328042326,
          "p": 0.09876543209876543,
          "f": 0.05925925505925956
        },
        "rouge-l": {
          "r": 0.20175438596491227,
          "p": 0.36507936507936506,
          "f": 0.25988700106482815
        }
      }
    },
    {
      "paper_id": "q-bio.SC.q-bio/SC/2407.15697v1",
      "true_abstract": "Voltage distribution in sub-cellular micro-domains such as neuronal synapses,\nsmall protrusions or dendritic spines regulates the opening and closing of\nionic channels, energy production and thus cellular homeostasis and\nexcitability. Yet how voltage changes at such a small scale in vivo remains\nchallenging due to the experimental diffraction limit, large signal\nfluctuations and the still limited resolution of fast voltage indicators. Here,\nwe study the voltage distribution in nano-compartments using a computational\napproach based on the Poisson-Nernst-Planck equations for the electro-diffusion\nmotion of ions, where inward and outward fluxes are generated between channels.\nWe report a current-voltage (I-V) logarithmic relationship generalizing Nernst\nlaw that reveals how the local membrane curvature modulates the voltage. We\nfurther find that an influx current penetrating a cellular electrolyte can lead\nto perturbations from tens to hundreds of nanometers deep depending on the\nlocal channels organization. Finally, we show that the neck resistance of\ndendritic spines can be completely shunted by the transporters located on the\nhead boundary, facilitating ionic flow. To conclude, we propose that voltage is\nregulated at a subcellular level by channels organization, membrane curvature\nand narrow passages.",
      "generated_abstract": "Electro-diffusion is a novel approach for modeling voltage mapping in\nsubcellular domains. We introduce a novel model for subcellular domains that\ncaptures electro-diffusion behavior in the voltage-gated ion channel\nsubcellular domain. We show that our model can reproduce the voltage mapping\npatterns of subcellular domains, including the formation of bipolar domains. We\nalso show that our model captures the effect of voltage-dependent ion selectivity\non the voltage mapping pattern. Additionally, we show that our model\npredicts the voltage mapping pattern in the context of voltage-gated ion channel\nknockout and gain-of-function mutations. Finally, we show that our model can\nbe used to predict voltage mapping in other subcellular domains.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.37037037037037035,
          "f": 0.21739130020085073
        },
        "rouge-2": {
          "r": 0.033707865168539325,
          "p": 0.07407407407407407,
          "f": 0.046332042033363045
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.37037037037037035,
          "f": 0.21739130020085073
        }
      }
    },
    {
      "paper_id": "hep-ex.physics/ins-det/2503.09392v1",
      "true_abstract": "The muon anomalous magnetic moment, $a_\\mu=\\frac{g-2}{2}$, is a low-energy\nobservable which can be both measured and computed to high precision, making it\na sensitive test of the Standard Model and a probe for new physics. This\nanomaly was measured with a precision of $0.20$~parts per million (ppm) by the\nFermilab's Muon g-2 (E989) experiment. The final goal of the E989 experiment is\nto reach a precision of $0.14$~ppm. The experiment is based on the measurement\nof the muon spin anomalous precession frequency, $\\omega_a$, based on the\narrival time distribution of high-energy decay positrons observed by 24\nelectromagnetic calorimeters, placed around the inner circumference of a $14$~m\ndiameter storage ring, and on the precise knowledge of the storage ring\nmagnetic field and of the beam time and space distribution. Achieving this\nlevel of precision requires strict control over systematics, which is ensured\nthrough several diagnostic devices. At the accelerator level, these devices\nmonitor the quality of the injected beam (e.g., verifying that it has the\ncorrect momentum), while at the detector level, they track both the magnetic\nfield and the gain of the calorimeters. In this work the devices and techniques\nused by the E989 experiment will be presented.",
      "generated_abstract": "The Muon $g-2$ experiment at Fermilab, using the muon-antimuon beam\nproduction cycle, aims to measure the $g-2$ of the muon to a precision of 10\nparts in 10^6. This paper describes the diagnostic system, consisting of a\nspectrometer, a time-of-flight detector, and a calorimeter, that will be used\nto measure the muon-antimuon scattering rate in the muon beam. The diagnostic\nsystem will be used to measure the muon-antimuon scattering rate in the muon\nbeam, which is crucial for determining the $g-2$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.168,
          "p": 0.44680851063829785,
          "f": 0.24418604253988646
        },
        "rouge-2": {
          "r": 0.04371584699453552,
          "p": 0.125,
          "f": 0.06477732409578937
        },
        "rouge-l": {
          "r": 0.152,
          "p": 0.40425531914893614,
          "f": 0.2209302285863981
        }
      }
    },
    {
      "paper_id": "cs.CG.math/MG/2503.01988v1",
      "true_abstract": "Metric spaces defined within convex polygons, such as the Thompson, Funk,\nreverse Funk, and Hilbert metrics, are subjects of recent exploration and study\nin computational geometry. This paper contributes an educational piece of\nsoftware for understanding these unique geometries while also providing a tool\nto support their research. We provide dynamic software for manipulating the\nFunk, reverse Funk, and Thompson balls in convex polygonal domains.\nAdditionally, we provide a visualization program for traversing the Hilbert\npolygonal geometry.",
      "generated_abstract": "The Thompson and Funk polygonal geometry is a recently proposed geometry for\nheavyweight shape analysis and has been shown to perform well on large-scale\nshape datasets. The Thompson and Funk polygonal geometry has a natural\napplication in the analysis of medical images, as it allows for the\nrepresentation of multi-planar images as a collection of triangles, which is\nparticularly useful for multi-plane image analysis. We present an open-source\nsoftware package, based on the C++ programming language, to facilitate the\nanalysis of Thompson and Funk polygonal geometry data. The package contains\nseveral tools, including a program for the generation of Thompson and Funk\npolygonal data, a program for the calculation of Thompson and Funk polygonal\ngeometric parameters, and a program for the calculation of Thompson and Funk\npolygonal-based shape descriptors.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25862068965517243,
          "p": 0.22058823529411764,
          "f": 0.23809523312673228
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.020618556701030927,
          "f": 0.02366863416267038
        },
        "rouge-l": {
          "r": 0.2413793103448276,
          "p": 0.20588235294117646,
          "f": 0.22222221725371644
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2411.12375v3",
      "true_abstract": "In this paper, we introduce a novel pricing model for Uniswap V3, built upon\nstochastic processes and the Martingale Stopping Theorem. This model\ninnovatively frames the valuation of positions within Uniswap V3. We further\nconduct a numerical analysis and examine the sensitivities through Greek risk\nmeasures to elucidate the model's implications. The results underscore the\nmodel's significant academic contribution and its practical applicability for\nUniswap liquidity providers, particularly in assessing risk exposure and\nguiding hedging strategies.",
      "generated_abstract": "In this paper, we propose a risk-neutral pricing model of Uniswap liquidity\nproviding position. We define a new concept of liquidity providing position\n(LPP) by taking the trading fee and liquidity fee into account. We consider the\nrisk-neutral pricing of LPPs in Uniswap. We use a stopping time approach to\nderive a risk-neutral pricing formula for LPPs. We also propose a risk-neutral\nvaluation model for LPPs. Our model is based on the expected utility theory,\nwhich is a more advanced and practical approach than the standard risk-neutral\npricing formula.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2903225806451613,
          "p": 0.33962264150943394,
          "f": 0.31304347329149346
        },
        "rouge-2": {
          "r": 0.0821917808219178,
          "p": 0.07692307692307693,
          "f": 0.07947019368097923
        },
        "rouge-l": {
          "r": 0.2903225806451613,
          "p": 0.33962264150943394,
          "f": 0.31304347329149346
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/QM/2502.16660v3",
      "true_abstract": "The applications of large language models (LLMs) in various biological\ndomains have been explored recently, but their reasoning ability in complex\nbiological systems, such as pathways, remains underexplored, which is crucial\nfor predicting biological phenomena, formulating hypotheses, and designing\nexperiments. This work explores the potential of LLMs in pathway reasoning. We\nintroduce BioMaze, a dataset with 5.1K complex pathway problems derived from\nreal research, covering various biological contexts including natural dynamic\nchanges, disturbances, additional intervention conditions, and multi-scale\nresearch targets. Our evaluation of methods such as CoT and graph-augmented\nreasoning, shows that LLMs struggle with pathway reasoning, especially in\nperturbed systems. To address this, we propose PathSeeker, an LLM agent that\nenhances reasoning through interactive subgraph-based navigation, enabling a\nmore effective approach to handling the complexities of biological systems in a\nscientifically aligned manner. The dataset and code are available at\nhttps://github.com/zhao-ht/BioMaze.",
      "generated_abstract": "easoning is a fundamental component of biological understanding\nand has been widely adopted in scientific discovery and drug development.\nHowever, it is challenging for current large language models (LLMs) to\nunderstand complex biological pathways due to their limited capacity and\ninconsistent performance across different datasets. To address this issue, we\nintroduce BioMaze, a benchmarking dataset for pathway reasoning tasks with\ndiverse biological knowledge. BioMaze consists of 14,167 unique pathways from\n14 different organisms. It includes both pathways from known and unknown\norganisms, as well as pathways that are missing from the public domain.\nBioMaze also introduces the BioMaze-Pathway, a new benchmark for pathway\nreasoning. It consists of 1,600 pathways from 14 organisms, with 66,000\nrelevant",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25892857142857145,
          "p": 0.34523809523809523,
          "f": 0.2959183624489796
        },
        "rouge-2": {
          "r": 0.06382978723404255,
          "p": 0.08333333333333333,
          "f": 0.07228915171432751
        },
        "rouge-l": {
          "r": 0.23214285714285715,
          "p": 0.30952380952380953,
          "f": 0.26530611755102046
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.06538v1",
      "true_abstract": "In two-way contingency tables under an asymmetric situation, where the row\nand column variables are defined as explanatory and response variables\nrespectively, quantifying the extent to which the explanatory variable\ncontributes to predicting the response variable is important. One\nquantification method is the association measure, which indicates the degree of\nassociation in a range from $0$ to $1$. Among various measures, those based on\nproportional reduction in error (PRE) are particularly notable for their\nsimplicity and intuitive interpretation. These measures, including\nGoodman-Kruskal's lambda proposed in 1954, are widely implemented in\nstatistical software such as R and SAS and remain extensively used. However, a\nknown limitation of PRE measures is their potential to return a value of $0$\ndespite no independence. This issue arises because the measures are constructed\nbased solely on the maximum joint and marginal probabilities, failing to\nutilize the information available in the contingency table fully. To address\nthis problem, we propose new association measures designed for the proportional\nreduction in error with multiple categories. The properties of the proposed\nmeasure are examined and their utility is demonstrated through simulations and\nreal data analyses. The results suggest their potential as practical tools in\napplied statistics.",
      "generated_abstract": "aper, we propose two novel association measures, named as the\nproportional reduction in error (PRIE) and the multi-categorical proportional\nreduction in error (MC-PRIE), for two-way contingency tables, which are based\non the multi-categorical proportional reduction in error (MCRE). MCRE is a\nnovel statistical method that improves the accuracy of the multivariate\nlogistic regression model by reducing the error in the categorical variable.\nIt is a powerful method that can be applied to any multivariate model, such as\na two-way contingency table. MCRE is a simple method that involves the\ncalculation of the sum of squared errors of the multivariate logistic\nregression model. The MCRE is obtained by combining the logistic regression\nmodel and a set of independent categorical variables. MC-PRIE is a novel\nassociation measure based",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2366412213740458,
          "p": 0.45588235294117646,
          "f": 0.31155778444584736
        },
        "rouge-2": {
          "r": 0.0625,
          "p": 0.12121212121212122,
          "f": 0.08247422231480522
        },
        "rouge-l": {
          "r": 0.22137404580152673,
          "p": 0.4264705882352941,
          "f": 0.2914572819332846
        }
      }
    },
    {
      "paper_id": "hep-ph.nucl-th/2503.10157v1",
      "true_abstract": "In this work, we perform a systematic study on the multiplicity dependence of\nhadron productions at mid-rapidity ($|y|<0.5$), ranging from the light to the\ncharm sector in pp collisions at $\\sqrt{s}=13$ TeV. This study utilizes a\nmulti-phase transport model (AMPT) coupled with PYTHIA8 initial conditions. We\nhave investigated the baryon to meson ratios as well as the strange to\nnon-strange meson ratios varying with the charged particle density. By tuning\nthe coalescence parameters, the AMPT model provides a reasonable description to\nthe experimental data for inclusive productions of both light and charm\nhadrons, comparable to the string fragmentation model calculations with color\nreconnection effects. Additionally, we have analyzed the relative production of\nhadrons by examining self-normalized particle ratios as a function of the\ncharged hadron density. Our findings suggest that parton evolution effects and\nthe coalescence hadronization process in AMPT model lead to a strong flavor\nhierarchy in the multiplicity dependence of the baryon to meson ratio.\nFurthermore, our investigation on the $p_T$ differential double ratio of baryon\nto meson fraction between high and low multiplicity events indicates distinct\nmodifications to the flavor associated baryon to meson ratio $p_T$ shape in\nhigh multiplicity events when comparing the coalescence hadronization model to\nthe color reconnection model. These observations highlight the importance of\nunderstanding the hadronization process in high-energy proton-proton collisions\nthrough a comprehensive multiplicity dependent multi-flavor analysis.",
      "generated_abstract": "The flavor hierarchy is a fundamental property of the Standard Model (SM)\nand has been extensively studied in the context of flavor physics. In this\npaper, we study the flavor hierarchy for hadron production in high energy\nproton-proton collisions in the framework of the flavor-blind approximation.\nWe find that the flavor hierarchy is not affected by the choice of the\nflavor-blind approximation, but it is dependent on the multiplicity. We also\nshow that the multiplicity dependence can be easily understood in terms of the\nflavor-blind approximation. We conclude that the flavor hierarchy can be\nobserved in the hadron production process at the LHC.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20155038759689922,
          "p": 0.4262295081967213,
          "f": 0.27368420616675904
        },
        "rouge-2": {
          "r": 0.050505050505050504,
          "p": 0.12195121951219512,
          "f": 0.07142856728673494
        },
        "rouge-l": {
          "r": 0.20155038759689922,
          "p": 0.4262295081967213,
          "f": 0.27368420616675904
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.00800v1",
      "true_abstract": "The article examines the impact of 16 key parameters of the Georgian economy\non economic inequality, using the Perelman model and Ricci flow mathematical\nmethods. The study aims to conduct a deep analysis of the impact of\nsocio-economic challenges and technological progress on the dynamics of the\nGini coefficient. The article examines the following parameters: income\ndistribution, productivity (GDP per hour), unemployment rate, investment rate,\ninflation rate, migration (net negative), education level, social mobility,\ntrade infrastructure, capital flows, innovative activities, access to\nhealthcare, fiscal policy (budget deficit), international trade (turnover\nrelative to GDP), social protection programs, and technological access. The\nresults of the study confirm that technological innovations and social\nprotection programs have a positive impact on reducing inequality. Productivity\ngrowth, improving the quality of education, and strengthening R&D investments\nincrease the possibility of inclusive development. Sensitivity analysis shows\nthat social mobility and infrastructure are important factors that affect\neconomic stability. The accuracy of the model is confirmed by high R^2 values\n(80-90%) and the statistical reliability of the Z-statistic (<0.05). The study\nuses Ricci flow methods, which allow for a geometric analysis of the\ntransformation of economic parameters in time and space. Recommendations\ninclude the strategic introduction of technological progress, the expansion of\nsocial protection programs, improving the quality of education, and encouraging\ninternational trade, which will contribute to economic sustainability and\nreduce inequality. The article highlights multifaceted approaches that combine\ntechnological innovation and responses to socio-economic challenges to ensure\nsustainable and inclusive economic development.",
      "generated_abstract": "the relationship between economic inequality and the socio-economic\ncondition of a population. We employ the Perelman model to estimate the Ricci\nflow and we use the Ricci flow to analyze the effects of technology and\neconomic progress on inequality. Our main result shows that a higher\neconomic growth rate and a higher level of technological development decrease\nthe inequality in the population. In contrast, a higher level of socio-economic\ninequality in a population does not necessarily result in lower economic\ndevelopment. We also demonstrate that the Ricci flow is an effective tool for\nstudying the impact of socio-economic factors on inequality in the population.\nOur results can be applied to different types of inequality, such as absolute,\nrelative, and welfare-based inequality. The Ricci flow can be used to study\ndifferent types of inequality, such as absolute, relative, and welf",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18493150684931506,
          "p": 0.38028169014084506,
          "f": 0.24884792186455437
        },
        "rouge-2": {
          "r": 0.05429864253393665,
          "p": 0.10619469026548672,
          "f": 0.07185628294793675
        },
        "rouge-l": {
          "r": 0.18493150684931506,
          "p": 0.38028169014084506,
          "f": 0.24884792186455437
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2410.18062v1",
      "true_abstract": "Undergraduate graders are frequently important contributors to the teaching\nteam in post-secondary education settings. This study set out to investigate\nagreement for a team of undergraduate graders as they acquired training and\nexperience for scoring responses to open-ended tasks. Results demonstrate\ncompelling evidence that undergraduate students can develop the ability to\nestablish and sustain substantial agreement with an instructor, especially when\nequipped with proper training and a high-quality scoring rubric.",
      "generated_abstract": "In this paper, we investigate the problem of consistency among open-ended\nstatistics grading, which is a crucial issue for improving student learning\nand academic integrity. The method proposed in this paper involves three\nsteps: (1) identifying the set of correct answers; (2) assigning weights to\neach correct answer; and (3) assigning weights to all graded answers to\nproduce an optimal score. The method is applied to a 1000-item open-ended\nstatistics test that includes 100 items. The results show that the method\nproduces a score with high accuracy and consistency. The results also demonstrate\nthat the method can be applied to other open-ended statistics tests, making it a\npotential solution for improving academic integrity and student learning.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2727272727272727,
          "p": 0.2,
          "f": 0.23076922588757406
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.2545454545454545,
          "p": 0.18666666666666668,
          "f": 0.2153846105029587
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.09349v1",
      "true_abstract": "Correlation-based auditory attention decoding (AAD) algorithms exploit neural\ntracking mechanisms to determine listener attention among competing speech\nsources via, e.g., electroencephalography signals. The correlation coefficients\nbetween the decoded neural responses and encoded speech stimuli of the\ndifferent speakers then serve as AAD decision variables. A critical trade-off\nexists between the temporal resolution (the decision window length used to\ncompute these correlations) and the AAD accuracy. This trade-off is typically\ncharacterized by evaluating AAD accuracy across multiple window lengths,\nleading to the performance curve. We propose a novel method to model this\ntrade-off curve using labeled correlations from only a single decision window\nlength. Our approach models the (un)attended correlations with a normal\ndistribution after applying the Fisher transformation, enabling accurate AAD\naccuracy prediction across different window lengths. We validate the method on\ntwo distinct AAD implementations: a linear decoder and the non-linear VLAAI\ndeep neural network, evaluated on separate datasets. Results show consistently\nlow modeling errors of approximately 2 percent points, with 94% of true\naccuracies falling within estimated 95%-confidence intervals. The proposed\nmethod enables efficient performance curve modeling without extensive\nmulti-window length evaluation, facilitating practical applications in, e.g.,\nperformance tracking in neuro-steered hearing devices to continuously adapt the\nsystem parameters over time.",
      "generated_abstract": "l Decoding of Auditory Attention (NDA) framework provides a model-based\ncorrelation-based approach for decoding the neural responses to speech in the\nauditory cortex, without the need for ground-truth signals. However, the\nperformance of this model is not well understood. In this study, we analyze\nthe performance of NDA using a dataset of real speech signals recorded from the\nauditory cortex of 12 participants, and compare it to the performance of the\nstate-of-the-art neural network-based framework called CMU-Auditory-BrainNet.\nWe demonstrate that the NDA model is superior to the CMU-Auditory-BrainNet\nframework in terms of the correlation between the neural responses and the\nground-truth speech signals. Furthermore, we show that the NDA model\nsignificantly outperforms the CMU-Auditory-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1610738255033557,
          "p": 0.36363636363636365,
          "f": 0.22325580969864797
        },
        "rouge-2": {
          "r": 0.03553299492385787,
          "p": 0.0707070707070707,
          "f": 0.04729729284537113
        },
        "rouge-l": {
          "r": 0.1476510067114094,
          "p": 0.3333333333333333,
          "f": 0.20465115853585733
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.15505v1",
      "true_abstract": "We develop a dynamic model of the Bitcoin market where users set fees\nthemselves and miners decide whether to operate and whom to validate based on\nthose fees. Our analysis reveals how, in equilibrium, users adjust their bids\nin response to short-term congestion (i.e., the amount of pending\ntransactions), how miners decide when to start operating based on the level of\ncongestion, and how the interplay between these two factors shapes the overall\nmarket dynamics. The miners hold off operating when the congestion is mild,\nwhich harms social welfare. However, we show that a block reward (a fixed\nreward paid to miners upon a block production) can mitigate these\ninefficiencies. We characterize the socially optimal block reward and\ndemonstrate that it is always positive, suggesting that Bitcoin's halving\nschedule may be suboptimal.",
      "generated_abstract": "The Bitcoin network is a dynamic, decentralized market for mining,\nrewarding users for their computational work. This paper examines the dynamic\nevolution of miner competition and its implications for mining behavior. We\nempirically analyze the Bitcoin network from 2014 to 2024, focusing on\nuser-miner interactions through the lens of Bitcoin blockchain data. We find\nthat miners compete for scarce computing resources and that the average number\nof mining pools per user varies over time, with larger users exhibiting more\ncompetition. Our analysis also reveals that the network is characterized by\nstrong miner-user competition, with competitive behavior increasing over time.\nThese findings have significant implications for Bitcoin's long-term\nevolution and the role of mining in the network.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22340425531914893,
          "p": 0.25301204819277107,
          "f": 0.23728813061253162
        },
        "rouge-2": {
          "r": 0.015503875968992248,
          "p": 0.01834862385321101,
          "f": 0.01680671772438534
        },
        "rouge-l": {
          "r": 0.22340425531914893,
          "p": 0.25301204819277107,
          "f": 0.23728813061253162
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/MF/2411.13792v1",
      "true_abstract": "Traditional Markowitz portfolio optimization constrains daily portfolio\nvariance to a target value, optimising returns, Sharpe or variance within this\nconstraint. However, this approach overlooks the relationship between variance\nat different time scales, typically described by $\\sigma(\\Delta t) \\propto\n(\\Delta t)^{H}$ where $H$ is the Hurst exponent, most of the time assumed to be\n\\(\\frac{1}{2}\\). This paper introduces a multifrequency optimization framework\nthat allows investors to specify target portfolio variance across a range of\nfrequencies, characterized by a target Hurst exponent $H_{target}$, or optimize\nthe portfolio at multiple time scales. By incorporating this scaling behavior,\nwe enable a more nuanced and comprehensive risk management strategy that aligns\nwith investor preferences at various time scales. This approach effectively\nmanages portfolio risk across multiple frequencies and adapts to different\nmarket conditions, providing a robust tool for dynamic asset allocation. This\novercomes some of the traditional limitations of Markowitz, when it comes to\ndealing with crashes, regime changes, volatility clustering or multifractality\nin markets. We illustrate this concept with a toy example and discuss the\npractical implementation for assets with varying scaling behaviors.",
      "generated_abstract": "ork, we propose a multiscale Markowitz framework for the estimation\nof the tail distribution of the variance of the return of an asset. We define\na multiscale Markowitz portfolio strategy that can be applied to both long-only\nand multi-asset portfolios, as long as the assets are well-behaved and have\nsufficiently high moments. The methodology is based on a two-step process that\nconsists of first estimating the tail distribution of the variance of the return\nof the asset and then using this distribution to construct the tail-risk\nportfolio. Our results show that the tail-risk portfolio can be constructed\nusing a small number of assets, which allows the portfolio to be constructed\nwithin a very short time, and also shows that this method can be applied to\nhigh-dimensional portfolios with many assets. We also show that the\nmultiscale-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17886178861788618,
          "p": 0.2894736842105263,
          "f": 0.22110552291709817
        },
        "rouge-2": {
          "r": 0.028735632183908046,
          "p": 0.044642857142857144,
          "f": 0.03496503020001043
        },
        "rouge-l": {
          "r": 0.16260162601626016,
          "p": 0.2631578947368421,
          "f": 0.20100502040453533
        }
      }
    },
    {
      "paper_id": "math.ST.stat/ME/2502.20123v1",
      "true_abstract": "We study two G-modeling strategies for estimating the signal distribution\n(the empirical Bayesian's prior) from observations corrupted with normal noise.\nFirst, we choose the signal distribution by minimizing Stein's unbiased risk\nestimate (SURE) of the implied Eddington/Tweedie Bayes denoiser, an approach\nmotivated by optimal empirical Bayesian shrinkage estimation of the signals.\nSecond, we select the signal distribution by minimizing Hyv\\\"arinen's score\nmatching objective for the implied score (derivative of log-marginal density),\ntargeting minimal Fisher divergence between estimated and true marginal\ndensities. While these strategies appear distinct, they are known to be\nmathematically equivalent. We provide a unified analysis of SURE and score\nmatching under both well-specified signal distribution classes and\nmisspecification. In the classical well-specified setting with homoscedastic\nnoise and compactly supported signal distribution, we establish nearly\nparametric rates of convergence of the empirical Bayes regret and the Fisher\ndivergence. In a commonly studied misspecified model, we establish fast rates\nof convergence to the oracle denoiser and corresponding oracle inequalities.\nOur empirical results demonstrate competitiveness with nonparametric maximum\nlikelihood in well-specified settings, while showing superior performance under\nmisspecification, particularly in settings involving heteroscedasticity and\nside information.",
      "generated_abstract": "We consider the problem of estimating a functional $f:[0,\\infty)\\to\\mathbb{R}$\nwithout any restrictions on its functional derivative. We show that a Stein\nestimator exists that is $L_2$-optimal under mild conditions on the\nfunctional. In particular, if the functional is twice continuously differentiable\nand bounded, then the Stein estimator is the minimizer of a $L_2$-penalized\nrisk functional. We also prove that the Stein estimator is a score matching\npoint estimator for the functional. In particular, if the functional is twice\ncontinuously differentiable and bounded, then the Stein estimator is the\nminimizer of a $L_2$-penalized risk functional with a score matching point\nestimator. We apply these results to the estimation of the derivative of the\nlog-likelihood function of a Gaussian process regression model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.136,
          "p": 0.3090909090909091,
          "f": 0.18888888464506182
        },
        "rouge-2": {
          "r": 0.029411764705882353,
          "p": 0.05952380952380952,
          "f": 0.03937007431334913
        },
        "rouge-l": {
          "r": 0.112,
          "p": 0.2545454545454545,
          "f": 0.15555555131172852
        }
      }
    },
    {
      "paper_id": "physics.ao-ph.stat/AP/2502.19234v1",
      "true_abstract": "Arctic sea ice is in reduction and has been a key significant indicator of\nclimate change. In this paper, we explore Arctic Sea ice extent data to\nidentify teleconnection with weather change in the polar and sub-tropical jet\nstream intersection in eastern United States (US) and hence the potential\ninfluence in ground level ozone pollution. Several statistical methods\nincluding Bayesian techniques such as: spatio-temporal modelling and Bayesian\nnetwork are implemented to identify the teleconnection and also validated based\non theories in atmospheric science. We observe that the teleconnection is\nrelatively strong in autumn, winter and spring seasons compared to the summer.\nFurthermore, the sudden decremental effect of Arctic sea-ice extent in\nmid-2000s has a shifting influence in ozone pollutions compared to the previous\nyears. A similar downward shift in the Arctic sea-ice extent has been projected\nin 2030. These findings indicate to initiate further strategic policies for the\nArctic influence, ozone concentrations together the seasonal and global\nchanging patterns of climate.",
      "generated_abstract": "teleconnections between the Arctic, the jet stream, and regional\nclimate and ozone pollution, focusing on the East Coast of the United States.\nWe use monthly surface air temperature, sea surface temperature, and ozone\nconcentration from 2000 to 2017 and the ensemble Kalman filter method to\nestimate teleconnection indices. We find that the teleconnection indices are\nconsistent with the traditional model of the jet stream path. However, the\nteleconnection indices show different patterns depending on the region. The\nteleconnection indices are consistent with the traditional model for\ntemperature, but the teleconnection indices for sea surface temperature and\nozone pollution are inconsistent with the traditional model. The teleconnection\nindices for temperature and sea surface temperature show a stronger\nconsistency with the traditional model. The teleconnection indices for\nsea-surface temperature and ozone pollution",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19444444444444445,
          "p": 0.3387096774193548,
          "f": 0.24705881889550177
        },
        "rouge-2": {
          "r": 0.03355704697986577,
          "p": 0.05434782608695652,
          "f": 0.041493771213306094
        },
        "rouge-l": {
          "r": 0.17592592592592593,
          "p": 0.3064516129032258,
          "f": 0.22352940713079594
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/GN/2503.09934v1",
      "true_abstract": "It is time to move on from attempts to make the pharmacy benefit manager\n(PBM) reseller business model more transparent. Time and time again the Big 3\nPBMs have developed opaque alternatives to piece-meal 100% pass-through\nmandates. Time and time again PBMs have demonstrated expertise in finding\nloopholes in state government disclosure laws. The purpose of this paper is to\nprovide quantitative estimates of two transparent insurance business models as\na solution to the PBM agency issue. The key parameter used is an 8% gross\nprofit margin figure disclosed by the Big 3 PBMs themselves. Based on reported\ndrug trend delivered to plans, we use a $1,200 to $1,500 per member per year\n(PMPY) as the range for this key performance indicator (KPI). We propose that\ndiscussions of PBM insurance business models start with the following figures:\n(1) a fixed premium model with medical loss ratio ranging from 92% to 85%; (2)\na fee-for-service model ranging from $96 to $180 PMPY with risk sharing of\ndeviations from a contracted PMPY delivered drug spend.",
      "generated_abstract": "This paper introduces a pharmacy benefit manager (PBM) insurance business\nmodel that addresses the complexities of managing drug costs in the U.S. health\ncare system. Our model aligns the interests of the pharmacy benefit manager\n(PBM) with those of the insurer and the health plan by optimizing the PBM's\ndrug procurement and reimbursement policies. The model also employs a\nreinforcement learning (RL) agent to optimize the insurer's drug purchasing\npolicy, with the insurer's objective of reducing costs while maintaining the\nhealth plan's reimbursement policies. Our model demonstrates the feasibility of\nutilizing RL techniques to improve the effectiveness of pharmacy benefit\nmanagement.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1623931623931624,
          "p": 0.296875,
          "f": 0.20994474680992656
        },
        "rouge-2": {
          "r": 0.043209876543209874,
          "p": 0.07608695652173914,
          "f": 0.05511810561597162
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.28125,
          "f": 0.19889502305302045
        }
      }
    },
    {
      "paper_id": "math.OC.stat/OT/2403.10987v3",
      "true_abstract": "The Fundamental Risk Quadrangle (FRQ) is a unified framework linking risk\nmanagement, statistical estimation, and optimization. Distributionally robust\noptimization (DRO) based on $\\varphi$-divergence minimizes the maximal expected\nloss, where the maximum is over a $\\varphi$-divergence ambiguity set. This\npaper introduces the \\emph{extended} $\\varphi$-divergence and the extended\n$\\varphi$-divergence quadrangle, which integrates DRO into the FRQ framework.\nWe derive the primal and dual representations of the quadrangle elements (risk,\ndeviation, regret, error, and statistic). The dual representation provides an\ninterpretation of classification, portfolio optimization, and regression as\nrobust optimization based on the extended $\\varphi$-divergence. The primal\nrepresentation offers tractable formulations of these robust optimizations as\nconvex optimization. We provide illustrative examples showing that many common\nproblems, such as least-squares regression, quantile regression, support vector\nmachines, and CVaR optimization, fall within this framework. Additionally, we\nconduct a case study to visualize the optimal solution of the inner\nmaximization in robust optimization.",
      "generated_abstract": "We study risk quadrangle (RQ) problem, which is a variant of quadratic\noptimization where the objective function is a quadratic function of the\noptimal solution, and the constraint functions are non-quadratic functions. We\nintroduce an extended $\\varphi$-divergence based on the extended $\\varphi$-divergence\nfor RQ problem, which is a metric on the set of feasible solutions. This\nextension allows us to develop a robust optimization framework based on\nextended $\\varphi$-divergence. We provide a characterization of the\nrobust-optimal solution of RQ problem, and show that the robust-optimal\nsolution is the unique solution of RQ problem. We also develop a primal-dual\nalgorithm to solve the RQ problem. Numerical results demonstrate the effectiveness\nof the proposed framework.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2549019607843137,
          "p": 0.4262295081967213,
          "f": 0.3190184002243216
        },
        "rouge-2": {
          "r": 0.0851063829787234,
          "p": 0.12631578947368421,
          "f": 0.10169491044419729
        },
        "rouge-l": {
          "r": 0.21568627450980393,
          "p": 0.36065573770491804,
          "f": 0.2699386456230946
        }
      }
    },
    {
      "paper_id": "physics.class-ph.physics/class-ph/2503.06910v1",
      "true_abstract": "Hidden momentum is a puzzling phenomenon associated with magnetic dipoles and\nother extended relativistic systems. We point out that the origin of hidden\nmomentum lies in the effective change of individual particle masses of a\ncomposite body, during which the total momentum of the system is not equal to\nthe momentum of the center of mass. Defining the hidden momentum as the\ndifference between the total momentum and the center-of-mass momentum, we\nexplain in detail how hidden momentum arises in certain simple non-relativistic\nsystems, in typical relativistic systems due to velocity-dependent\n``relativistic mass'', and in magnetic dipoles as a special case of\nrelativistic systems.",
      "generated_abstract": "We study the hidden momentum of a composite body and a magnetic dipole. Both\ncomposite bodies and magnetic dipoles have a natural interpretation as\ncomposite particles. However, there is no general formula for their hidden\nmomentum, even though it is known to exist. We provide a formula for the\nhidden momentum of the composite body. For the magnetic dipole, we derive a\nformula that depends on the magnetic moment of the composite body. We show that\nthis formula is consistent with the conservation of momentum in the presence of\nelectric and magnetic fields. We discuss the implications of the hidden\nmomentum for particle production in high energy collisions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.30158730158730157,
          "p": 0.31666666666666665,
          "f": 0.30894308443386875
        },
        "rouge-2": {
          "r": 0.0851063829787234,
          "p": 0.08602150537634409,
          "f": 0.0855614923263465
        },
        "rouge-l": {
          "r": 0.2698412698412698,
          "p": 0.2833333333333333,
          "f": 0.2764227592306167
        }
      }
    },
    {
      "paper_id": "math.OC.q-bio/PE/2502.00509v1",
      "true_abstract": "The main aim of this study is to analyze a fractional parabolic SIR epidemic\nmodel of a reaction-diffusion, by using the nonlocal Caputo fractional\ntime-fractional derivative and employing the $p$-Laplacian operator. The\nimmunity is imposed through the vaccination program, which is regarded as a\ncontrol variable. Finding the optimal control pair that reduces the number of\nsick people, the associated vaccination, and treatment expenses across a\nconstrained time and space is our main study. The existence and uniqueness of\nthe nonnegative solution for the spatiotemporal SIR model are established. It\nis also demonstrated that an optimal control exists. In addition, we obtain a\ndescription of the optimal control in terms of state and adjoint functions.\nThen, the optimality system is resolved by a discrete iterative scheme that\nconverges after an appropriate test, similar to the forward-backward sweep\nmethod. Finally, numerical approximations are given to show the effectiveness\nof the proposed control program, which provides meaningful results using\ndifferent values of the fractional order and $p$, respectively the order of the\nCaputo derivative and the $p$-Laplacian operators.",
      "generated_abstract": "This paper deals with a reaction-diffusion system involving a fractional\ntime derivative, a nonlinear diffusion operator and a Caputo fractional\nderivative. The system is based on the SIR model, which has been widely\napplied in epidemiological models. The model is considered in the framework of\nthe fractional differential equation theory. The main objective of the\npresented research is to provide a solution to the system of fractional\ndifferential equations, which is expressed by a system of ordinary differential\nequations. The main focus is on the development of numerical algorithms for the\nsolution of the system. The proposed method is based on a novel numerical\ntechnique for the solution of the fractional differential equation, which\nrelies on a nonlinear iteration method and the use of the Caputo fractional\nderivative.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.375,
          "f": 0.2790697627690644
        },
        "rouge-2": {
          "r": 0.07878787878787878,
          "p": 0.12149532710280374,
          "f": 0.09558823052146434
        },
        "rouge-l": {
          "r": 0.18518518518518517,
          "p": 0.3125,
          "f": 0.23255813486208768
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.20788v1",
      "true_abstract": "The stock assessment model SAM contains a large number of age-dependent\nparameters that must be manually grouped together to obtain robust inference.\nThis can make the model selection process slow, non-extensive and highly\nsubjective, while producing unrealistic looking parameter estimates with\ndiscrete jumps. We propose to model age-dependent SAM parameters using\nsmoothing spline functions. This can lead to more smooth parameter estimates,\nwhile speeding up and making the model selection process more automatic and\nless subjective. We develop different spline models and compare them with\nalready existing SAM models for a selection of 17 different fish stocks, using\ncross- and forward-validation methods. The results show that our automated\nspline models overall outcompete the officially developed SAM models. We also\ndemonstrate how the developed spline models can be employed as a diagnostics\ntool for improving and better understanding properties of the officially\ndeveloped SAM models.",
      "generated_abstract": "The stock assessment model (SAM) has been widely used in marine fisheries\nmanagement and other scientific applications. While the SAM is an effective\napproach for stock assessment, it is not fully flexible in handling\nnon-linear relationships between catch and stock characteristics. We introduce\na novel approach based on smoothing splines for modeling these relationships,\nwhich are particularly useful when the relationship is non-linear. We show that\nsmoothing splines can improve the accuracy of stock assessments, particularly\nwhen the relationships between catch and stock characteristics are non-linear.\nThe effectiveness of the smoothing spline approach was demonstrated by\nimproving the stock assessment of Atlantic cod in the eastern North Atlantic.\nThis study highlights the potential of smoothing splines in improving stock\nassessment models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2087912087912088,
          "p": 0.25333333333333335,
          "f": 0.2289156576970534
        },
        "rouge-2": {
          "r": 0.045454545454545456,
          "p": 0.05454545454545454,
          "f": 0.04958677190082694
        },
        "rouge-l": {
          "r": 0.2087912087912088,
          "p": 0.25333333333333335,
          "f": 0.2289156576970534
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ME/2503.06690v1",
      "true_abstract": "Dynamic Treatment Regimes (DTRs) provide a systematic approach for making\nsequential treatment decisions that adapt to individual patient\ncharacteristics, particularly in clinical contexts where survival outcomes are\nof interest. Censoring-Aware Tree-Based Reinforcement Learning (CA-TRL) is a\nnovel framework to address the complexities associated with censored data when\nestimating optimal DTRs. We explore ways to learn effective DTRs, from\nobservational data. By enhancing traditional tree-based reinforcement learning\nmethods with augmented inverse probability weighting (AIPW) and censoring-aware\nmodifications, CA-TRL delivers robust and interpretable treatment strategies.\nWe demonstrate its effectiveness through extensive simulations and real-world\napplications using the SANAD epilepsy dataset, where it outperformed the\nrecently proposed ASCL method in key metrics such as restricted mean survival\ntime (RMST) and decision-making accuracy. This work represents a step forward\nin advancing personalized and data-driven treatment strategies across diverse\nhealthcare settings.",
      "generated_abstract": "This paper introduces a novel reinforcement learning (RL) method for\nestimating dynamic treatment regimes with censored outcomes. We develop a\ncensoring-aware tree-based RL algorithm that leverages the structure of the\ntreatment regime tree to efficiently approximate the posterior distribution of\nthe regime transition probabilities. The proposed method is computationally\nefficient and scalable to large-scale problems, and is applicable to both\ncontinuous and binary treatment regimes. The algorithm is shown to outperform\nstate-of-the-art methods for estimating dynamic treatment regimes with\ncensored outcomes, particularly in scenarios where the true treatment regime\ntree is ill-specified or highly sparse. Our empirical results demonstrate that\nthe proposed method achieves superior performance compared to state-of-the-art\nmethods across a wide range of simulation settings.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.37662337662337664,
          "f": 0.3005181299191925
        },
        "rouge-2": {
          "r": 0.029850746268656716,
          "p": 0.038461538461538464,
          "f": 0.033613440457595525
        },
        "rouge-l": {
          "r": 0.23275862068965517,
          "p": 0.35064935064935066,
          "f": 0.27979274131815624
        }
      }
    },
    {
      "paper_id": "math-ph.math/CV/2503.05690v1",
      "true_abstract": "Inspired by the duality between Jackiw-Teitelboim gravity and Schwarzian\nfield theory, we show identities between the Schwarzian action of a circle\ndiffeomorphism with (1) the hyperbolic area enclosed by the Epstein curve in\nthe hyperbolic disk $\\mathbb{D}$, (2) the asymptotic excess in the\nisoperimetric inequality for the equidistant Epstein foliation, (3) the\nvariation of the Loewner energy along its equipotential foliation, and (4) the\nasymptotic change in hyperbolic area under a conformal distortion near the\ncircle. From these geometric interpretations, we obtain two new proofs of the\nnon-negativity of the Schwarzian action for circle diffeomorphisms, one from\nthe isoperimetric inequality and the other from the monotonicity of the Loewner\nenergy. Moreover, we show that the horocycle truncation used in Epstein's\nconstruction of the Epstein curve also defines a renormalized length of\nhyperbolic geodesics in $\\mathbb{D}$, which coincides with the log of the\nbi-local observable. From this, we show that the bi-local observables on the\nedges of any ideal triangulation of $\\mathbb{D}$ determine the circle\ndiffeomorphism.",
      "generated_abstract": "We study the Schwarzian derivative, which is a generalization of the\ndifferential derivative that includes the fourth-order term. We prove that a\ngeneralized Einstein-Schwarzian gravity can be recovered from the Schwarzian\naction. As a consequence, we obtain a new formulation of holography and\ngeneralize the Einstein-Schwarzian gravity to the general case of the\nSchwarzian derivative. Furthermore, we show that the Schwarzian derivative is\nrelated to the Einstein-Weyl gravity by a gauge transformation. We discuss the\nimplications of the Schwarzian derivative in holography and in cosmology.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17777777777777778,
          "p": 0.3333333333333333,
          "f": 0.23188405343415258
        },
        "rouge-2": {
          "r": 0.057971014492753624,
          "p": 0.1095890410958904,
          "f": 0.07582937936075138
        },
        "rouge-l": {
          "r": 0.15555555555555556,
          "p": 0.2916666666666667,
          "f": 0.20289854618777578
        }
      }
    },
    {
      "paper_id": "cs.SI.q-fin/EC/2502.21037v2",
      "true_abstract": "Recent advances in artificial intelligence have led to the proliferation of\nartificial agents in social contexts, ranging from education to online social\nmedia and financial markets, among many others. The increasing rate at which\nartificial and human agents interact makes it urgent to understand the\nconsequences of human-machine interactions for the propagation of new ideas,\nproducts, and behaviors in society. Across two distinct empirical contexts, we\nfind here that artificial agents lead to significantly faster and wider social\ncontagion. To this end, we replicate a choice experiment previously conducted\nwith human subjects by using artificial agents powered by large language models\n(LLMs). We use the experiment's results to measure the adoption thresholds of\nartificial agents and their impact on the spread of social contagion. We find\nthat artificial agents tend to exhibit lower adoption thresholds than humans,\nwhich leads to wider network-based social contagions. Our findings suggest that\nthe increased presence of artificial agents in real-world networks may\naccelerate behavioral shifts, potentially in unforeseen ways.",
      "generated_abstract": "igate the amplification effect of artificial agents in social\ncontagion models. The most commonly used models in the literature, such as\nBayesian networks, rely on Bayesian inference to model social interactions.\nHowever, Bayesian inference does not account for the fact that agents may have\npreferences over the same stimulus, which can lead to overestimation of the\nprobability of exposure to a stimulus. We propose an artificial agent that\nrepresents the agent's preference over a stimulus. By incorporating the\npreference into the Bayesian network, we can eliminate the bias in the\nprediction of the agent's preference. We apply our approach to two models of\nsocial contagion: the social contagion model by Bouchaud et al. (1997), which\nis a Bayesian network model, and the simple Bayesian model by Kiefer et al.\n(2010",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21818181818181817,
          "p": 0.3037974683544304,
          "f": 0.2539682491027687
        },
        "rouge-2": {
          "r": 0.05228758169934641,
          "p": 0.06666666666666667,
          "f": 0.058608053681117836
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.27848101265822783,
          "f": 0.2328042279387476
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/CB/2411.12123v7",
      "true_abstract": "Colorectal cancer (CRC) poses a major public health challenge due to its\nincreasing prevalence, particularly among younger populations. Microsatellite\ninstability-high (MSI-H) CRC and deficient mismatch repair (dMMR) CRC\nconstitute 15% of all CRC and exhibit remarkable responsiveness to\nimmunotherapy, especially with PD-1 inhibitors. Despite this, there is a\nsignificant need to optimise immunotherapeutic regimens to maximise clinical\nefficacy and patient quality of life whilst minimising monetary costs. To\naddress this, we employ a novel framework driven by delay integro-differential\nequations to model the interactions among cancer cells, immune cells, and\nimmune checkpoints. Several of these components are being modelled\ndeterministically for the first time in cancer, paving the way for a deeper\nunderstanding of the complex underlying immune dynamics. We consider two\ncompartments: the tumour site and the tumour-draining lymph node, incorporating\nphenomena such as dendritic cell (DC) migration, T cell proliferation, and CD8+\nT cell exhaustion and reinvigoration. Parameter values and initial conditions\nare derived from experimental data, integrating various pharmacokinetic,\nbioanalytical, and radiographic studies, along with deconvolution of bulk\nRNA-sequencing data from the TCGA COADREAD and GSE26571 datasets. We finally\noptimise neoadjuvant treatment with pembrolizumab, a widely used PD-1\ninhibitor, to balance efficacy, efficiency, and toxicity in locally advanced\nMSI-H/dMMR CRC patients. We improve upon currently FDA-approved therapeutic\nregimens for metastatic MSI-H/dMMR CRC, demonstrating that a single\nmedium-to-high dose of pembrolizumab is sufficient for effective tumour\neradication whilst being efficient, safe and practical.",
      "generated_abstract": "to guide treatment decisions in clinical practice, an optimal\noptimisation of the neoadjuvant therapy for locally advanced MSI-H/dMMR\ncolorectal cancer (CRC) using data-driven delay integro-differential\nequations (DIDE) is presented. The model is obtained by coupling the\nDIDE-based neoadjuvant CRC model with a delay integro-differential\nequations model for the delay between pre-treatment and treatment, where the\ndelay is controlled by the prognostic factors of the tumour. The model\nconsiders the simultaneous effects of chemotherapy and immunotherapy. It\nallows for the treatment of CRC patients with the neoadjuvant treatment of\npembrolizumab, a monoclonal antibody against PD-1 receptors, with the\npro",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15606936416184972,
          "p": 0.43548387096774194,
          "f": 0.2297872301580806
        },
        "rouge-2": {
          "r": 0.04741379310344827,
          "p": 0.125,
          "f": 0.06874999601250023
        },
        "rouge-l": {
          "r": 0.14450867052023122,
          "p": 0.4032258064516129,
          "f": 0.21276595356233596
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.18261v2",
      "true_abstract": "Traditionally, the impact of minimum wages on employment has been studied,\nand it is generally believed to have a negative effect. Yet, some recent\nstudies have shown that the impact of minimum wages on employment can sometimes\nbe positive. In addition, certain recent proposals set a higher minimum wage\nthan the wage earned by some high-productivity workers. However, the impact of\nminimum wages on employment has been primarily studied on low-skilled workers,\nwhereas there is limited research on high-skilled workers. To address this gap\nand examine the effects of minimum wages on high-productivity workers'\nemployment, I construct a macroeconomic model incorporating productivity\nfluctuations, incomplete markets, directed search, and on-the-job search and\ncompare the steady-state distributions between the baseline model and the model\nwith a minimum wage. As a result, binding minimum wages increase the\nunemployment rate of both low and high-productivity workers.",
      "generated_abstract": "We study the effect of minimum wages on employment in the presence of\nemployment fluctuations. We focus on the case where the wage setting is\ndetermined by the labor market, and where the wage setting is influenced by\nproductivity shocks. We identify the equilibrium wage by a dual optimization\nproblem. We show that the equilibrium wage depends on the productivity\nshocks, and that the wage is larger in regions of high productivity. We also\nshow that the wage has a higher dispersion in regions where there is a larger\nfluctuation of productivity. Our results suggest that minimum wages can have a\npositive impact on employment when they are set in a way that accounts for\nproductivity shocks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.3728813559322034,
          "f": 0.2993197230857514
        },
        "rouge-2": {
          "r": 0.0743801652892562,
          "p": 0.09,
          "f": 0.08144795884605179
        },
        "rouge-l": {
          "r": 0.20454545454545456,
          "p": 0.3050847457627119,
          "f": 0.2448979543782684
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.01495v1",
      "true_abstract": "Vovk (2015) introduced cross-conformal prediction, a modification of split\nconformal designed to improve the width of prediction sets. The method, when\ntrained with a miscoverage rate equal to $\\alpha$ and $n \\gg K$, ensures a\nmarginal coverage of at least $1 - 2\\alpha - 2(1-\\alpha)(K-1)/(n+K)$, where $n$\nis the number of observations and $K$ denotes the number of folds. A simple\nmodification of the method achieves coverage of at least $1-2\\alpha$. In this\nwork, we propose new variants of both methods that yield smaller prediction\nsets without compromising the latter theoretical guarantee. The proposed\nmethods are based on recent results deriving more statistically efficient\ncombination of p-values that leverage exchangeability and randomization.\nSimulations confirm the theoretical findings and bring out some important\ntradeoffs.",
      "generated_abstract": "We consider the problem of predicting a target variable $y$ given a set of\nsamples $X_1, \\dots, X_n$ drawn from a distribution $P$ under consideration,\nand the prediction of a predictor $\\hat{y}$ given a set of samples $X_1, \\dots,\nX_m$ drawn from a distribution $Q$ under consideration. We are interested in\nestimating the conditional expectation $\\mathbb{E}[\\hat{y}|X_1, \\dots, X_n",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06521739130434782,
          "p": 0.16216216216216217,
          "f": 0.09302325172285338
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.06521739130434782,
          "p": 0.16216216216216217,
          "f": 0.09302325172285338
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.09812v1",
      "true_abstract": "Selective inference aims at providing valid inference after a data-driven\nselection of models or hypotheses. It is essential to avoid overconfident\nresults and replicability issues. While significant advances have been made in\nthis area for standard regression models, relatively little attention has been\ngiven to linear mixed models (LMMs), which are widely used for analyzing\nclustered or longitudinal data. This paper reviews the existing selective\ninference approaches developed for LMMs, focusing on selection of fixed\neffects, where the random effects structure is given. We present these methods\nin detail and, through comparative simulations, assess their practical\nperformance and computational feasibility under varying data structures. In\naddition, we apply them to a real-world biological dataset to examine how\nmethod choice can impact inference in practice. Our findings highlight an\nexisting trade-off between computational complexity and statistical power and\nemphasize the scarcity of methods that perform well as the number of variables\nincreases. In such scenarios, basic sample splitting emerges as the most\nreliable approach.",
      "generated_abstract": "inference for linear mixed models is a method that allows the\nexclusion of variables from a model while retaining information about them\nthrough the use of a set of auxiliary variables. It is commonly used to improve\nthe efficiency of model inference in linear regression models. This paper\npresents a review of the methods of selective inference for linear mixed models\nand evaluates them through an empirical comparison of five popular methods:\nconditional log-likelihood (CL), conditional likelihood ratio test (CLRT),\nconditional log-likelihood ratio test (CLR), conditional information criterion\n(CIC), and a novel method proposed by the author. The empirical comparison\nreveals that CLRT and CLR are significantly more efficient than the other\nmethods, with CLRT achieving the highest efficiency in terms of reduced\ninference time and number of parameters, while CLR achieves the highest\nefficiency in terms of reduced inference",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2076923076923077,
          "p": 0.3333333333333333,
          "f": 0.2559241658857619
        },
        "rouge-2": {
          "r": 0.05,
          "p": 0.06451612903225806,
          "f": 0.05633802324935572
        },
        "rouge-l": {
          "r": 0.17692307692307693,
          "p": 0.2839506172839506,
          "f": 0.21800947394263395
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.18541v2",
      "true_abstract": "In the dynamic landscape of contemporary society, the popularity of ideas,\nopinions, and interests fluctuates rapidly. Traditional dynamical models in\nsocial sciences often fail to capture this inherent volatility, attributing\nchanges to exogenous shocks rather than intrinsic features of the system. This\npaper introduces a novel, tractable model that simulates the natural rise and\nfall of ideas' popularity, offering a more accurate representation of\nreal-world dynamics. Building upon the SIRS (Susceptible, Infectious,\nRecovered, Susceptible) epidemiological model, we incorporate a feedback\nmechanism that allows the recovery rate to vary dynamically based on the\ncurrent state of the system. This modification reflects the cyclical nature of\nidea adoption and abandonment, driven by social saturation and renewed\ninterest. Our model successfully captures the rapid and recurrent shifts in\npopularity, providing valuable insights into the mechanisms behind these\nfluctuations. This approach offers a robust framework for studying the\ndiffusion dynamics of popular ideas, with potential applications across various\nfields such as marketing, technology adoption, and political movements.",
      "generated_abstract": "p a model to analyze the rise and fall of ideas' popularity by\nintroducing a novel measure of popularity based on a threshold-like process.\nOur model captures the idea's popularity dynamics, and we show that this model\nprovides a consistent description of the data. In particular, we show that the\nmodel provides a consistent description of the data up to year 2024. The\nevolution of the model's parameters over time is also analyzed. Our model\nprovides a novel explanation for the sudden rise of the idea 'cryptocurrency'\nand its rapid decline in popularity. We show that the model can explain the\nsudden rise of the idea 'cryptocurrency' and its rapid decline in popularity\nbetween 2014 and 2024. The model can also explain the sudden decline of the idea\n'blockchain' and its",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.184,
          "p": 0.3770491803278688,
          "f": 0.24731182354896525
        },
        "rouge-2": {
          "r": 0.050314465408805034,
          "p": 0.08602150537634409,
          "f": 0.06349205883503435
        },
        "rouge-l": {
          "r": 0.16,
          "p": 0.32786885245901637,
          "f": 0.21505375903283627
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/FL/2503.02719v1",
      "true_abstract": "Motion planning with simple objectives, such as collision-avoidance and\ngoal-reaching, can be solved efficiently using modern planners. However, the\ncomplexity of the allowed tasks for these planners is limited. On the other\nhand, signal temporal logic (STL) can specify complex requirements, but\nSTL-based motion planning and control algorithms often face scalability issues,\nespecially in large multi-robot systems with complex dynamics. In this paper,\nwe propose an algorithm that leverages the best of the two worlds. We first use\na single-robot motion planner to efficiently generate a set of alternative\nreference paths for each robot. Then coordination requirements are specified\nusing STL, which is defined over the assignment of paths and robots' progress\nalong those paths. We use a Mixed Integer Linear Program (MILP) to compute task\nassignments and robot progress targets over time such that the STL\nspecification is satisfied. Finally, a local controller is used to track the\ntarget progress. Simulations demonstrate that our method can handle tasks with\ncomplex constraints and scales to large multi-robot teams and intricate task\nallocation scenarios.",
      "generated_abstract": "r presents a novel framework for scalable multi-robot task\nallocation and coordination under the Signal Temporal Logic (STL) specifications.\nSTL is a specification language that defines the requirements of a system,\nproviding a foundation for the development of robust and reliable\nmulti-robot systems. The framework is built upon the STL-based multi-robot\ntask allocation and coordination (TRAC) algorithm, which is designed to\nefficiently handle complex tasks within the constraints of the STL specifications.\nThe proposed framework is applicable to a wide range of robotic systems,\nincluding collaborative robotic systems, intelligent transportation systems,\nand smart factory systems. The proposed framework is designed to accommodate\nboth deterministic and stochastic environments, providing robustness and\nflexibility. The performance of the proposed framework is evaluated through\nsimulation and experimental studies, demonstrating its effectiveness and\ne",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19672131147540983,
          "p": 0.3076923076923077,
          "f": 0.23999999524200005
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.05309734513274336,
          "f": 0.04270462152607037
        },
        "rouge-l": {
          "r": 0.1557377049180328,
          "p": 0.24358974358974358,
          "f": 0.18999999524200012
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.04225v1",
      "true_abstract": "This study presents the development and validation of a digital twin for a\nsemi-autogenous grinding (SAG) mill controlled by an expert system. The digital\ntwin integrates three key components of the closed-loop operation: (1) fuzzy\nlogic for expert control, (2) a state-space model for regulatory control, and\n(3) a recurrent neural network to simulate the SAG mill process. The digital\ntwin is combined with a statistical framework for automatically detecting\nprocess disturbances (or critical operations), which triggers model retraining\nonly when deviations from expected behaviour are identified, ensuring\ncontinuous updates with new data to enhance the SAG supervision. The model was\ntrained with 68 hours of operational industrial data and validated with an\nadditional 8 hours, allowing it to predict mill behaviour within a 2.5-minute\nhorizon at 30-second intervals with errors smaller than 5%.",
      "generated_abstract": "This paper presents a novel digital twin of an SAG mill, a critical\npoint of the mining industry, which is designed to support the decision-making\nprocess of a company that owns the mill. The digital twin is developed using\nmachine learning techniques, including convolutional neural networks, to capture\nthe mill's behavior in real time. The resulting model is used to evaluate the\nefficiency of different control strategies and to determine the optimal\nsettings of the parameters of the control loop. The model is validated using\nreal-world data, demonstrating the accuracy and robustness of the model in\nidentifying control parameters that optimize the mill's performance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20618556701030927,
          "p": 0.30303030303030304,
          "f": 0.2453987681869849
        },
        "rouge-2": {
          "r": 0.046153846153846156,
          "p": 0.061855670103092786,
          "f": 0.05286343122901713
        },
        "rouge-l": {
          "r": 0.1958762886597938,
          "p": 0.2878787878787879,
          "f": 0.23312882953667818
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.20939v1",
      "true_abstract": "Neuroblastoma is a paediatric extracranial solid cancer that arises from the\ndeveloping sympathetic nervous system and is characterised by an abnormal\ndistribution of cell types in tumours compared to healthy infant tissues. In\nthis paper, we propose a new mathematical model of cell differentiation during\nsympathoadrenal development. By performing Bayesian inference of the model\nparameters using clinical data from patient samples, we show that the model\nsuccessfully accounts for the observed differences in cell type heterogeneity\namong healthy adrenal tissues and four common types of neuroblastomas. Using a\nphenotypically structured model, we show that alterations in healthy\ndifferentiation dynamics are related to cell malignancy, and tumour volume\ngrowth. We use this model to analyse the evolution of malignant traits in a\ntumour. Our findings suggest that normal development dynamics make the\nembryonic sympathetic nervous system more robust to perturbations and\naccumulation of malignancies, and that the diversity of differentiation\ndynamics found in the neuroblastoma subtypes lead to unique risk profiles for\nneuroblastoma relapse after treatment.",
      "generated_abstract": "toma is the most common malignant tumor of the sympathetic nervous\nsystem and is the leading cause of childhood cancer deaths. Current therapies\nare largely based on the understanding of the molecular mechanisms that drive\ncancer cell proliferation and the ability to block them. We introduce a\ndifferentiation-based approach to model the development of neuroblastoma and\nthe dynamics of tumor cell populations. Using the differential equations model\ndeveloped by Weeks and colleagues, we focus on the mechanisms that control\ndifferentiation and growth. We use this approach to analyze the growth dynamics\nof neuroblastoma cells and the impact of treatment on tumor growth. Our model\npredicts that the response of neuroblastoma cells to differentiation\nmodulates tumor growth, with the most differentiated cells growing the fastest.\nWe show that the tumor's response",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2830188679245283,
          "p": 0.410958904109589,
          "f": 0.3351955258961955
        },
        "rouge-2": {
          "r": 0.0641025641025641,
          "p": 0.08620689655172414,
          "f": 0.0735294068728377
        },
        "rouge-l": {
          "r": 0.25471698113207547,
          "p": 0.3698630136986301,
          "f": 0.30167597282356984
        }
      }
    },
    {
      "paper_id": "math-ph.nlin/SI/2503.09502v1",
      "true_abstract": "An infinite 3-parametric family of superintegrable and exactly-solvable\nquantum models on a plane, admitting separation of variables in polar\ncoordinates, marked by integer index $k$ was introduced in Journ Phys A 42\n(2009) 242001 and was called in literature the TTW system. In this paper it is\nconjectured that the Hamiltonian and both integrals of TTW system have hidden\nalgebra $g^{(k)}$ - it was checked for $k=1,2,3,4$ - having its\nfinite-dimensional representation spaces as the invariant subspaces. It is\nchecked that for $k=1,2,3,4$ that the Hamiltonian $H$, two integrals ${\\cal\nI}_{1,2}$ and their commutator ${\\cal I}_{12} = [{\\cal I}_1,{\\cal I}_2]$ are\nfour generating elements of the polynomial algebra of integrals of the order\n$(k+1)$: $[{\\cal I}_1,{\\cal I}_{12}] = P_{k+1}(H, {\\cal I}_{1,2},{\\cal\nI}_{12})$, $[{\\cal I}_2,{\\cal I}_{12}] = Q_{k+1}(H, {\\cal I}_{1,2},{\\cal\nI}_{12})$, where $P_{k+1},Q_{k+1}$ are polynomials of degree $(k+1)$ written in\nterms of ordered monomials of $H, {\\cal I}_{1,2},{\\cal I}_{12}$. This implies\nthat polynomial algebra of integrals is subalgebra of $g^{(k)}$. It is\nconjectured that all is true for any integer $k$.",
      "generated_abstract": "We introduce the TTW system, a polynomial algebra of integrals over the\nalgebra of real functions of the moduli space of flat connections with\ninteger index $k$. The TTW system has been previously considered in the\nliterature as a system of differential equations of second order for the\ncomplexification of the moduli space of flat connections, but we provide a\nnew proof of its existence and a detailed study of its structure. We show that\nthe TTW system has a natural structure as a system of differential equations of\nsecond order on the complexification of the moduli space of flat connections.\nThis structure allows us to deduce the structure of the algebra of real\nfunctions of the moduli space of flat connections with integer index $k$. We\nalso show that the TTW system can be recovered as a special case of a general\nclass of systems of second order differential equations of second order that\nwas introduced in [1",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21495327102803738,
          "p": 0.3333333333333333,
          "f": 0.2613636315967201
        },
        "rouge-2": {
          "r": 0.0718954248366013,
          "p": 0.10476190476190476,
          "f": 0.08527131300252416
        },
        "rouge-l": {
          "r": 0.19626168224299065,
          "p": 0.30434782608695654,
          "f": 0.23863635886944737
        }
      }
    },
    {
      "paper_id": "math.OC.eess/SY/2503.07324v1",
      "true_abstract": "Distribution shifts have long been regarded as troublesome external forces\nthat a decision-maker should either counteract or conform to. An intriguing\nfeedback phenomenon termed decision dependence arises when the deployed\ndecision affects the environment and alters the data-generating distribution.\nIn the realm of performative prediction, this is encoded by distribution maps\nparameterized by decisions due to strategic behaviors. In contrast, we\nformalize an endogenous distribution shift as a feedback process featuring\nnonlinear dynamics that couple the evolving distribution with the decision.\nStochastic optimization in this dynamic regime provides a fertile ground to\nexamine the various roles played by dynamics in the composite problem\nstructure. To this end, we develop an online algorithm that achieves optimal\ndecision-making by both adapting to and shaping the dynamic distribution.\nThroughout the paper, we adopt a distributional perspective and demonstrate how\nthis view facilitates characterizations of distribution dynamics and the\noptimality and generalization performance of the proposed algorithm. We\nshowcase the theoretical results in an opinion dynamics context, where an\nopportunistic party maximizes the affinity of a dynamic polarized population,\nand in a recommender system scenario, featuring performance optimization with\ndiscrete distributions in the probability simplex.",
      "generated_abstract": "aper, we consider a stochastic optimization problem where the\noptimal solution is subject to a distribution. We propose a novel\nmulti-agent stochastic optimization model where the agents jointly optimize\ntheir decision and the distribution. We then propose an algorithm for\nsolving this model. Our analysis focuses on the case when the agents can\ncommunicate and we prove convergence of the proposed algorithm under mild\nassumptions. The convergence rate is established in terms of the difference in\nthe conditional distribution of the decision and the original distribution.\nAdditionally, we derive a lower bound on the convergence rate of the proposed\nalgorithm. We then derive a lower bound on the convergence rate of the\nproposed algorithm in the case when the agents communicate and the decision\ndistribution is Gaussian. We also consider the case when the agents communicate\nbut the decision distribution is not Gaussian. Our analysis highlights the\nrole of the distribution dynamics in determining the convergence rate.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1693548387096774,
          "p": 0.31343283582089554,
          "f": 0.2198952834034156
        },
        "rouge-2": {
          "r": 0.05291005291005291,
          "p": 0.09090909090909091,
          "f": 0.06688962745606905
        },
        "rouge-l": {
          "r": 0.16129032258064516,
          "p": 0.29850746268656714,
          "f": 0.2094240792149339
        }
      }
    },
    {
      "paper_id": "quant-ph.math-ph/2503.10400v1",
      "true_abstract": "We elucidate the requirements for quantum operations that achieve\nenvironment-assisted invariance (envariance), a symmetry of entanglement. While\nenvariance has traditionally been studied within the framework of local unitary\noperations, we extend the analysis to consider non-unitary local operations.\nFirst, we investigate the conditions imposed on operators acting on pure\nbipartite entanglement to attain envariance. We show that the local operations\nmust take a direct-sum form in their Kraus operator representations,\nestablishing decoherence-free subspaces. Furthermore, we prove that the unitary\noperation on the system's subspace uniquely determines the corresponding\nunitary operator on the environment's subspace. As an immediate consequence, we\ndemonstrate that environment-assisted shortcuts to adiabaticity cannot be\nachieved through non-unitary operations. In addition, we identify the\nrequirements that local operations must satisfy to ensure that the eternal\nblack hole states remain static in AdS/CFT.",
      "generated_abstract": "In this paper, we show that the existence of no-go theorems is an artifact\nof the no-go theorem approach to quantum mechanics. We present an alternative\napproach to the problem of the no-go theorem for the quantum theory of open\nsystems, which does not rely on the no-go theorem. In this approach, we show\nthat the existence of a no-go theorem is a consequence of the invariance of the\nHamiltonian under local unitary transformations, which is a general feature of\nthe quantum theory of open systems. We also discuss a possible extension of\nthis approach to the case of non-unitary dynamics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19318181818181818,
          "p": 0.3333333333333333,
          "f": 0.2446043119010404
        },
        "rouge-2": {
          "r": 0.031746031746031744,
          "p": 0.05194805194805195,
          "f": 0.039408862286394314
        },
        "rouge-l": {
          "r": 0.19318181818181818,
          "p": 0.3333333333333333,
          "f": 0.2446043119010404
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.04358v1",
      "true_abstract": "We propose a novel approach for learning causal response representations. Our\nmethod aims to extract directions in which a multidimensional outcome is most\ndirectly caused by a treatment variable. By bridging conditional independence\ntesting with causal representation learning, we formulate an optimisation\nproblem that maximises the evidence against conditional independence between\nthe treatment and outcome, given a conditioning set. This formulation employs\nflexible regression models tailored to specific applications, creating a\nversatile framework. The problem is addressed through a generalised eigenvalue\ndecomposition. We show that, under mild assumptions, the distribution of the\nlargest eigenvalue can be bounded by a known $F$-distribution, enabling\ntestable conditional independence. We also provide theoretical guarantees for\nthe optimality of the learned representation in terms of signal-to-noise ratio\nand Fisher information maximisation. Finally, we demonstrate the empirical\neffectiveness of our approach in simulation and real-world experiments. Our\nresults underscore the utility of this framework in uncovering direct causal\neffects within complex, multivariate settings.",
      "generated_abstract": "l representation of a population is a key attribute for making\nappropriate interventions, particularly in settings where the underlying\ncausal mechanisms are unknown. Existing methods for learning causal\nrepresentations from observational data rely on either model-based or model-free\nmethods. Model-based methods are often data-intensive, and their reliance on\nspecification of a model can lead to model misspecification. Model-free methods\nhave the advantage of being data-efficient, but they often suffer from\nover-fitting and lack of generalizability to new data. We present a new method\nfor learning causal representations from observational data through\ndirect-effect analysis. Our method first learns a causal model that encodes the\ncausal mechanism, and then uses the model to learn a conditional distribution\nof the outcome from the observed variables. We show that this approach provides\na simple and general way",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21739130434782608,
          "p": 0.29069767441860467,
          "f": 0.24875621400955428
        },
        "rouge-2": {
          "r": 0.039473684210526314,
          "p": 0.048,
          "f": 0.043321294686494594
        },
        "rouge-l": {
          "r": 0.19130434782608696,
          "p": 0.2558139534883721,
          "f": 0.21890546774089764
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.15980v1",
      "true_abstract": "This paper studies inter-firm heterogeneity in production. Unlike much of the\nexisting research, which primarily addresses heterogeneous production through\nunobserved fixed effects, our approach also focuses on differences in factors'\noutput elasticities. Using manufacturing data from Chile, Colombia, and Japan,\nwe apply an innovative Empirical Bayes methodology to estimate heterogeneous\nCobb-Douglas production functions. We uncover substantial heterogeneity in both\nfactor neutral productivity and factor elasticities, with a strong negative\ncorrelation between them. These findings are consistently observed across\ndatasets and remain robust when using CES and intensive Cobb-Douglas\nspecifications. We show that accounting for these features has significant\nimplications for issues such as markup estimation, firms' technology adoption,\nand productivity measurement.",
      "generated_abstract": "We study the inter-firm heterogeneity in production networks by introducing a\nmodel that captures the network structure and firm-firm interactions. We\nconsider a network of production firms with a common input and a common\noutput. The production firms' network structure depends on their interactions\nwith other firms, which are modeled as a network of network-structure\ninteractions. We derive the equilibrium production network and analyze its\nbehavior. We find that the network structure has an impact on the firm-level\nproduction processes and their interactions with other firms. The firm-level\nproduction process and its interactions with other firms are sensitive to the\nnetwork structure and firm heterogeneity.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20833333333333334,
          "p": 0.37735849056603776,
          "f": 0.26845637125534894
        },
        "rouge-2": {
          "r": 0.03669724770642202,
          "p": 0.04597701149425287,
          "f": 0.04081632159360743
        },
        "rouge-l": {
          "r": 0.1875,
          "p": 0.33962264150943394,
          "f": 0.24161073367145633
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.00642v1",
      "true_abstract": "Real-world low-light images captured by imaging devices suffer from poor\nvisibility and require a domain-specific enhancement to produce artifact-free\noutputs that reveal details. In this paper, we propose an unpaired low-light\nimage enhancement network leveraging novel controlled transformation-based\nself-supervision and unpaired self-conditioning strategies. The model\ndetermines the required degrees of enhancement at the input image pixels, which\nare learned from the unpaired low-lit and well-lit images without any direct\nsupervision. The self-supervision is based on a controlled transformation of\nthe input image and subsequent maintenance of its enhancement in spite of the\ntransformation. The self-conditioning performs training of the model on\nunpaired images such that it does not enhance an already-enhanced image or a\nwell-lit input image. The inherent noise in the input low-light images is\nhandled by employing low gradient magnitude suppression in a detail-preserving\nmanner. In addition, our noise handling is self-conditioned by preventing the\ndenoising of noise-free well-lit images. The training based on low-light image\nenhancement-specific attributes allows our model to avoid paired supervision\nwithout compromising significantly in performance. While our proposed\nself-supervision aids consistent enhancement, our novel self-conditioning\nfacilitates adequate enhancement. Extensive experiments on multiple standard\ndatasets demonstrate that our model, in general, outperforms the\nstate-of-the-art both quantitatively and subjectively. Ablation studies show\nthe effectiveness of our self-supervision and self-conditioning strategies, and\nthe related loss functions.",
      "generated_abstract": "image enhancement is a fundamental challenge in vision\nprocessors due to the significant reduction in light intensity. Recent\nadvances in neural networks have demonstrated remarkable performance in\nenhancing low-light images through self-supervised learning. However, existing\nmethods typically rely on high-quality data pairs, which are often expensive\nto obtain, limiting their generalizability. In this paper, we propose a\nself-supervised framework that leverages unpaired self-conditioning for low-light\nimage enhancement. The proposed method achieves self-supervised learning by\nleveraging a novel control function to transform the low-light image into a\nhigh-light image. The transformed image is then used as a self-conditioning\nfeature, which enables the network to learn the transformation process. Additionally,\nwe introduce a self-conditioning loss function to facilitate the network\nlearnability of the transformation process. Furthermore, we",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23255813953488372,
          "p": 0.3448275862068966,
          "f": 0.27777777296682105
        },
        "rouge-2": {
          "r": 0.057692307692307696,
          "p": 0.10344827586206896,
          "f": 0.07407406947721414
        },
        "rouge-l": {
          "r": 0.21705426356589147,
          "p": 0.3218390804597701,
          "f": 0.25925925444830256
        }
      }
    },
    {
      "paper_id": "math.SG.math/SG/2503.00123v1",
      "true_abstract": "This survey explores the geometry of three-dimensional Anosov flows from the\nperspective of contact and symplectic geometry, following the work of\nMitsumatsu, Eliashberg-Thurston, Hozoori, and the author. We also present a few\noriginal results and discuss various open questions and conjectures.",
      "generated_abstract": "We study Anosov flows with respect to a given symplectic structure, which\nis an important class of Hamiltonian systems with a symplectic structure\nassociated with the Hamiltonian. We provide a complete classification of these\nflows, which is a direct consequence of the classification of symplectic\nstructures on the circle. We also provide a geometric viewpoint on these\nflows, using the symplectic quotient construction. We show that for any\nAnosov flow with respect to a symplectic structure, there exists a\n$1$-parameter family of symplectic structures with the same flow. We also\nclassify all symplectic structures on the circle with a given symplectic\nstructure as a restriction of a symplectic structure on the quotient space.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24242424242424243,
          "p": 0.1509433962264151,
          "f": 0.18604650689832355
        },
        "rouge-2": {
          "r": 0.05,
          "p": 0.022727272727272728,
          "f": 0.0312499957031256
        },
        "rouge-l": {
          "r": 0.24242424242424243,
          "p": 0.1509433962264151,
          "f": 0.18604650689832355
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2502.02371v1",
      "true_abstract": "Accurate identification of druggable pockets is essential for structure-based\ndrug design. However, most pocket-identification algorithms prioritize their\ngeometric properties over downstream docking performance. To address this\nlimitation, we developed RAPID-Net, a pocket-finding algorithm for seamless\nintegration with docking workflows. When guiding AutoDock Vina, RAPID-Net\noutperforms DiffBindFR on the PoseBusters benchmark and enables blind docking\non large proteins that AlphaFold 3 cannot process as a whole. Furthermore,\nRAPID-Net surpasses PUResNet and Kalasanty in docking accuracy and\npocket-ligand intersection rates across diverse datasets, including\nPoseBusters, Astex Diverse Set, BU48, and Coach420. When accuracy is evaluated\nas ``at least one correct pose in the ensemble'', RAPID-Net outperforms\nAlphaFold 3 on the PoseBusters benchmark, suggesting that our approach can be\nfurther improved with a suitable pose reweighting tool offering a\ncost-effective and competitive alternative to AlphaFold 3 for docking. Finally,\nusing several therapeutically relevant examples, we demonstrate the ability of\nRAPID-Net to identify remote functional sites, highlighting its potential to\nfacilitate the development of innovative therapeutics.",
      "generated_abstract": "pocket identification for binding-site-agnostic docking is essential\nfor the development of more effective drug candidates and personalized\ntherapeutics. Recent advancements in deep learning and convolutional neural\nnetworks (CNNs) have demonstrated the potential of these methods for\naccurately predicting pockets in both small molecules and proteins. However,\nthese approaches often suffer from limitations in generalization, including\ndifficulty in identifying pockets with varying degrees of flexibility. To\naddress this, we developed a novel CNN architecture that is specifically\ndesigned for pocket identification. Our method utilizes a convolutional\nblock-pooling layer to process the input data and a residual connection to\naccurately identify pockets. Additionally, we propose a simple and efficient\ntraining strategy that reduces the computational costs of training. Our method\nwas tested on both small molecules and proteins and demonstrated superior\nperformance",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2396694214876033,
          "p": 0.32222222222222224,
          "f": 0.2748815116956044
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.04878048780487805,
          "f": 0.0430107477581229
        },
        "rouge-l": {
          "r": 0.23140495867768596,
          "p": 0.3111111111111111,
          "f": 0.26540283870982245
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.03557v1",
      "true_abstract": "Clinical practice guidelines are designed to guide clinical practice and\ninvolve causal language. Sometimes guidelines make or require stronger causal\nclaims than those in the references they rely on, a phenomenon we refer to as\n'causal language jump'. We evaluated the strength of expressed causation in\ndiabetes guidelines and the evidence they reference to assess the pattern of\njumps. We randomly sampled 300 guideline statements from four diabetes\nguidelines. We rated the causation strength in the statements and the\ndependence on causation in recommendations supported by these statements using\nexisting scales. Among the causal statements, the cited original studies were\nsimilarly assessed. We also assessed how well they report target trial\nemulation (TTE) components as a proxy for reliability. Of the sampled\nstatements, 114 (38.0%) were causal, and 76 (66.7%) expressed strong causation.\n27.2% (31/114) of causal guideline statements demonstrated a \"causal language\njump\", and 34.9% (29/83) of guideline recommendations cannot be effectively\nsupported. Of the 53 eligible studies for TTE rating, most did not report\ntreatment assignment and causal contrast in detail. Our findings suggest causal\nlanguage jumps were common among diabetes guidelines. While these jumps are\nsometimes inevitable, they should always be supported by good causal inference\npractices.",
      "generated_abstract": "nguage is a key component of clinical guidelines and is essential to\ndiagnosing and treating disease. However, it is notoriously difficult to\ninterpret, and can be misleading or incomplete. We conducted a systematic\nreview of all clinical guidelines for diabetes published in English between 2000\nand 2025. We identified 141 guidelines, of which 101 met inclusion criteria and\nwere reviewed. We analyzed the guidelines' causal language and assessed how\nwell it supported evidence-based clinical decisions. Most guidelines were\nwritten in a causal language that was not supported by the evidence, with a\nsignificant majority of guidelines using causal language that was inappropriate\nfor their setting. Guidelines that used the causal language were not always\nsupported by the evidence.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1968503937007874,
          "p": 0.32894736842105265,
          "f": 0.24630541403479833
        },
        "rouge-2": {
          "r": 0.03608247422680412,
          "p": 0.06422018348623854,
          "f": 0.046204615855526605
        },
        "rouge-l": {
          "r": 0.18110236220472442,
          "p": 0.3026315789473684,
          "f": 0.22660098053726138
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.08455v1",
      "true_abstract": "Instead of performing text-conditioned denoising in the image domain, latent\ndiffusion models (LDMs) operate in latent space of a variational autoencoder\n(VAE), enabling more efficient processing at reduced computational costs.\nHowever, while the diffusion process has moved to the latent space, the\ncontrastive language-image pre-training (CLIP) models, as used in many image\nprocessing tasks, still operate in pixel space. Doing so requires costly\nVAE-decoding of latent images before they can be processed. In this paper, we\nintroduce Latent-CLIP, a CLIP model that operates directly in the latent space.\nWe train Latent-CLIP on 2.7B pairs of latent images and descriptive texts, and\nshow that it matches zero-shot classification performance of similarly sized\nCLIP models on both the ImageNet benchmark and a LDM-generated version of it,\ndemonstrating its effectiveness in assessing both real and generated content.\nFurthermore, we construct Latent-CLIP rewards for reward-based noise\noptimization (ReNO) and show that they match the performance of their CLIP\ncounterparts on GenEval and T2I-CompBench while cutting the cost of the total\npipeline by 21%. Finally, we use Latent-CLIP to guide generation away from\nharmful content, achieving strong performance on the inappropriate image\nprompts (I2P) benchmark and a custom evaluation, without ever requiring the\ncostly step of decoding intermediate images.",
      "generated_abstract": "Latent diffusion models have demonstrated significant success in generating\nimages. However, the control of these models has remained largely unexplored.\nIn this paper, we propose a novel approach to control Latent Diffusion using\nLatent CLIP. We first train a Latent CLIP model to generate control inputs\nthat are similar to the target image. Then, we use this model to generate\nperturbations to the latent space of the target image, and apply these\nperturbations to the generated control inputs. We show that our approach can\ngenerate high-quality generated images while preserving the structure of the\ntarget image. Furthermore, we show that the generated images are more\ninteresting and have more details when compared to images generated using\nstandard Latent Diffusion models. We also provide a qualitative comparison with\nthe state-of-the-art methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21739130434782608,
          "p": 0.4166666666666667,
          "f": 0.2857142812081633
        },
        "rouge-2": {
          "r": 0.06735751295336788,
          "p": 0.11403508771929824,
          "f": 0.08469054907701966
        },
        "rouge-l": {
          "r": 0.21739130434782608,
          "p": 0.4166666666666667,
          "f": 0.2857142812081633
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2409.11569v1",
      "true_abstract": "We consider the Merton problem of optimizing expected power utility of\nterminal wealth in the case of an unobservable Markov-modulated drift. What\nmakes the model special is that the agent is allowed to purchase costly expert\nopinions of varying quality on the current state of the drift, leading to a\nmixed stochastic control problem with regular and impulse controls involving\nrandom consequences. Using ideas from filtering theory, we first embed the\noriginal problem with unobservable drift into a full information problem on a\nlarger state space. The value function of the full information problem is\ncharacterized as the unique viscosity solution of the dynamic programming PDE.\nThis characterization is achieved by a new variant of the stochastic Perron's\nmethod, which additionally allows us to show that, in between purchases of\nexpert opinions, the problem reduces to an exit time control problem which is\nknown to admit an optimal feedback control. Under the assumption of sufficient\nregularity of this feedback map, we are able to construct optimal trading and\nexpert opinion strategies.",
      "generated_abstract": "r studies optimal investment with costly expert opinions, in which\nthe expert is unaware of the investor's true information. The investor's\nexpectation of the expert's opinion is modeled as a stochastic control problem\nwith an optimal control formulation. We investigate the case where the\ninvestor knows the expert's opinion and the expert is unaware of the investor's\ntrue information. We prove that the investor's expected utility is maximized\nwithin the class of optimal control policies that are linear in the\nexpert's state. We then establish a sufficient condition for the existence of\nsuch optimal control policies in the class of linear controls. We show that the\nexistence of such optimal control policies depends on the investor's ability to\ninfer the true state of the market from the expert's opinion. Furthermore, we\nderive an explicit expression for the investor's",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3063063063063063,
          "p": 0.5,
          "f": 0.3798882634449612
        },
        "rouge-2": {
          "r": 0.0736196319018405,
          "p": 0.11428571428571428,
          "f": 0.08955223404015396
        },
        "rouge-l": {
          "r": 0.27927927927927926,
          "p": 0.45588235294117646,
          "f": 0.34636871037233546
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.17597v1",
      "true_abstract": "This paper presents a model of costly information acquisition where\ndecision-makers can choose whether to elaborate information superficially or\nprecisely. The former action is costless, while the latter entails a processing\ncost. Within this framework, decision-makers' beliefs may polarize even after\nthey have access to the same evidence. From the perspective of a Bayesian\nobserver who neglects information processing constraints, the decision-makers'\noptimal behavior and belief updating may appear consistent with biases such as\ndisconfirmation, underreaction to information, and confirmation bias. However,\nthese phenomena emerge naturally within the model and are fully compatible with\nstandard Bayesian inference and rational decision-making when accounting for\nthe costs of information acquisition.",
      "generated_abstract": "belief patterns with information processing, where a decision maker\nbelieves that there are $m$ distinct alternatives. If an alternative is\ndiscovered, the decision maker is allowed to change her beliefs. We investigate\nthe impact of the number of alternative and of the discovery phase on the\ndecision maker's belief pattern. For a fixed number of alternatives $m$, we\nstudy the situation in which the decision maker can only believe that an\nalternative has been discovered if she has information about the existence of\nthis alternative. In this case, the decision maker's belief pattern depends on\nher beliefs before the discovery. In particular, for $m=1$, the decision maker\nis always confident of the existence of an alternative, regardless of her\nbeliefs. For $m>1$, the decision maker's belief pattern depends on her beliefs\nbefore the discovery. If her beliefs are",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17857142857142858,
          "p": 0.21739130434782608,
          "f": 0.19607842642060758
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.17857142857142858,
          "p": 0.21739130434782608,
          "f": 0.19607842642060758
        }
      }
    },
    {
      "paper_id": "cs.CL.econ/GN/2503.01870v1",
      "true_abstract": "Identifying customer needs (CNs) is important for product management, product\ndevelopment, and marketing. Applications rely on professional analysts\ninterpreting textual data (e.g., interview transcripts, online reviews) to\nunderstand the nuances of customer experience and concisely formulate \"jobs to\nbe done.\" The task is cognitively complex and time-consuming. Current practice\nfacilitates the process with keyword search and machine learning but relies on\nhuman judgment to formulate CNs. We examine whether Large Language Models\n(LLMs) can automatically extract CNs. Because evaluating CNs requires\nprofessional judgment, we partnered with a marketing consulting firm to conduct\na blind study of CNs extracted by: (1) a foundational LLM with prompt\nengineering only (Base LLM), (2) an LLM fine-tuned with professionally\nidentified CNs (SFT LLM), and (3) professional analysts. The SFT LLM performs\nas well as or better than professional analysts when extracting CNs. The\nextracted CNs are well-formulated, sufficiently specific to identify\nopportunities, and justified by source content (no hallucinations). The SFT LLM\nis efficient and provides more complete coverage of CNs. The Base LLM was not\nsufficiently accurate or specific. Organizations can rely on SFT LLMs to reduce\nmanual effort, enhance the precision of CN articulation, and provide improved\ninsight for innovation and marketing strategy.",
      "generated_abstract": "In this study, we explore how large language models (LLMs) can extract\ncustomers' needs from unstructured reviews. We conduct a systematic literature\nreview and study 22 studies that aim to extract customers' needs from unstructured\nreviews. We find that, compared to professional analysts, LLMs often struggle\nwith extracting customers' needs, and often struggle to identify the\nrelevant attributes. We also find that LLMs often struggle with identifying\nattributes that are relevant to customers, and often struggle with identifying\nattributes that are relevant to customers. Additionally, we find that LLMs\noften struggle with identifying attributes that are relevant to customers, and\nlack the ability to identify attributes that are relevant to customers. These\nfindings highlight the need for further research to improve LLMs' ability to\nextract customers' needs.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1347517730496454,
          "p": 0.3064516129032258,
          "f": 0.1871921139838386
        },
        "rouge-2": {
          "r": 0.015384615384615385,
          "p": 0.03571428571428571,
          "f": 0.02150537213550782
        },
        "rouge-l": {
          "r": 0.12056737588652482,
          "p": 0.27419354838709675,
          "f": 0.16748768048630164
        }
      }
    },
    {
      "paper_id": "cs.PF.cs/PF/2503.02982v1",
      "true_abstract": "We consider a discrete-time parallel service system consisting of $n$\nheterogeneous single server queues with infinite capacity. Jobs arrive to the\nsystem as an i.i.d. process with rate proportional to $n$, and must be\nimmediately dispatched in the time slot that they arrive. The dispatcher is\nassumed to be able to exchange messages with the servers to obtain their queue\nlengths and make dispatching decisions, introducing an undesirable\ncommunication overhead.\n  In this setting, we propose a ultra-low communication overhead load balancing\npolicy dubbed $k$-Skip-the-$d$-Longest-Queues ($k$-SLQ-$d$), where queue\nlengths are only observed every $k(n-d)$ time slots and, between observations,\nincoming jobs are sent to a queue that is not one of the $d$ longest ones at\nthe time that the queues were last observed. For this policy, we establish\nconditions on $d$ for it to be throughput optimal and we show that, under that\ncondition, it is asymptotically delay-optimal in heavy-traffic for arbitrarily\nlow communication overheads (i.e., for arbitrarily large $k$).",
      "generated_abstract": "We study the problem of minimizing the sum of the lengths of longest queues in\na parallel service system with heterogeneous clients. We prove that the\noptimal service time is $\\Theta(1/\\alpha)$ for any $\\alpha \\in (0,1)$, and\nfurther show that this is optimal for any $\\alpha \\in [0.5,1",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1826086956521739,
          "p": 0.6,
          "f": 0.2799999964222223
        },
        "rouge-2": {
          "r": 0.02531645569620253,
          "p": 0.08888888888888889,
          "f": 0.03940886354437168
        },
        "rouge-l": {
          "r": 0.14782608695652175,
          "p": 0.4857142857142857,
          "f": 0.22666666308888897
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/GN/2405.10917v2",
      "true_abstract": "It is a challenge to estimate fund performance by compounded returns.\nArguably, it is incorrect to use yearly returns directly for compounding, with\nreported annualized return of above 60% for Medallion for the 31 years up to\n2018. We propose an estimation based on fund sizes and trading profits and\nobtain a compounded return of 31.8% before fees. Alternatively, we suggest\nusing the manager's wealth as a proxy and arriving at a compounded growth rate\nof 25.6% for Simons for the 33 years up to 2020. We conclude that the\nannualized compounded return of Medallion before fees is probably under 35%.\nOur findings have implications for correctly estimating fund performance.",
      "generated_abstract": "We analyze the Medallion portfolio, an institutional investment that is based\non the Medallion Index, a portfolio of exchange-traded funds that tracks the\nindex's annualized compounded return. We show that the annualized compounded\nreturn of Medallion is 34.5%, which is 5.1% higher than the 33.4% annualized\nreturn of the S&P 500 Index. The annualized compounded return of Medallion is\nalso 1.5% higher than the 1.4% annualized return of the S&P 500 Index. Our\nresults suggest that the Medallion portfolio is undervalued relative to the\nS&P 500 Index.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22972972972972974,
          "p": 0.3695652173913043,
          "f": 0.2833333286055556
        },
        "rouge-2": {
          "r": 0.08823529411764706,
          "p": 0.1323529411764706,
          "f": 0.1058823481411767
        },
        "rouge-l": {
          "r": 0.22972972972972974,
          "p": 0.3695652173913043,
          "f": 0.2833333286055556
        }
      }
    },
    {
      "paper_id": "cs.MA.q-fin/EC/2502.13267v1",
      "true_abstract": "BeforeIT is an open-source software for building and simulating\nstate-of-the-art macroeconomic agent-based models (macro ABMs) based on the\nrecently introduced macro ABM developed in [1] and here referred to as the base\nmodel. Written in Julia, it combines extraordinary computational efficiency\nwith user-friendliness and extensibility. We present the main structure of the\nsoftware, demonstrate its ease of use with illustrative examples, and benchmark\nits performance. Our benchmarks show that the base model built with BeforeIT is\norders of magnitude faster than a Matlab version, and significantly faster than\nMatlab-generated C code. BeforeIT is designed to facilitate reproducibility,\nextensibility, and experimentation. As the first open-source, industry-grade\nsoftware to build macro ABMs of the type of the base model, BeforeIT can\nsignificantly foster collaboration and innovation in the field of agent-based\nmacroeconomic modelling. The package, along with its documentation, is freely\navailable at https://github.com/bancaditalia/BeforeIT.jl under the AGPL-3.0.",
      "generated_abstract": "tutorial on using the Julia language to run agent-based macroeconomic\nmodels. The tutorial is designed for graduate students and researchers with\nbasic knowledge of statistics, economics, and programming. The tutorial is\nbased on the open-source model code and software developed by the authors. It\nincludes a comprehensive walk-through of the model code and software. The\ntutorial is organized into three parts: 1) model code, 2) data loading, and 3)\nmodeling. The code is available in the `Models` directory and is used in the\ntutorial to illustrate the basic model functionality. The data is available in\nthe `Data` directory. The tutorial includes a complete data loading and model\nrunning workflow, including a comparison of the model results to empirical data\nfrom the literature. The tutorial also includes a discussion of the limitations\nof the model and future directions for research. The tutorial is written in\nEnglish",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21359223300970873,
          "p": 0.2894736842105263,
          "f": 0.24581005097968234
        },
        "rouge-2": {
          "r": 0.04316546762589928,
          "p": 0.04918032786885246,
          "f": 0.04597700651546567
        },
        "rouge-l": {
          "r": 0.17475728155339806,
          "p": 0.23684210526315788,
          "f": 0.20111731354951481
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.07208v1",
      "true_abstract": "In this work, various analysis methods are conducted on frequency-dependent\nmethods on SED to further delve into their detailed characteristics and\nbehaviors on SED. While SED has been rapidly advancing through the adoption of\nvarious deep learning techniques from other pattern recognition fields, these\ntechniques are often not suitable for SED. To address this issue, two\nfrequency-dependent SED methods were previously proposed: FilterAugment, a data\naugmentation randomly weighting frequency bands, and frequency dynamic\nconvolution (FDY Conv), an architecture applying frequency adaptive convolution\nkernels. These methods have demonstrated superior performance in SED, and we\naim to further analyze their detailed effectiveness and characteristics in SED.\nWe compare class-wise performance to find out specific pros and cons of\nFilterAugment and FDY Conv. We apply Gradient-weighted Class Activation Mapping\n(Grad-CAM), which highlights time-frequency region that is more inferred by the\nmodel, on SED models with and without frequency masking and two types of\nFilterAugment to observe their detailed characteristics. We propose simpler\nfrequency dependent convolution methods and compare them with FDY Conv to\nfurther understand which components of FDY Conv affects SED performance.\nLastly, we apply PCA to show how FDY Conv adapts dynamic kernel across\nfrequency dimensions on different sound event classes. The results and\ndiscussions demonstrate that frequency dependency plays a significant role in\nsound event detection and further confirms the effectiveness of frequency\ndependent methods on SED.",
      "generated_abstract": "ection in sound is a challenging task due to the inherent\nsound event-to-sound event temporal variability and the limited training\ndata. To address these limitations, this paper proposes a new event detection\nmodel, named Frequency-aware Sound Event Detection (FSED), which leverages\nfrequency information to enhance event detection. The model is designed to\ncapture the temporal variability of sound events through frequency-aware\nrepresentations and incorporate frequency information through the attention\nmechanism. The model also includes a novel event-to-event alignment module to\nalign the frequency information of the event-level features and the\ntemporal-level features, effectively capturing the temporal variation of\nevents. Experimental results on the AudioCaps dataset demonstrate that FSED\nachieves state-of-the-art performance, outperforming state-of-the-art\nmethods by a large margin in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18705035971223022,
          "p": 0.3170731707317073,
          "f": 0.2352941129796688
        },
        "rouge-2": {
          "r": 0.023696682464454975,
          "p": 0.04504504504504504,
          "f": 0.031055896103353155
        },
        "rouge-l": {
          "r": 0.16546762589928057,
          "p": 0.2804878048780488,
          "f": 0.2081447917127005
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.07461v1",
      "true_abstract": "We study the optimal management of a photovoltaic system's battery owned by a\nself-consumption group that aims to minimize energy consumption costs. We\nassume that the photovoltaic system is composed of a photovoltaic panel and a\nbattery, where the photovoltaic panel produces energy according to a certain\nstochastic process. The management of the battery is the responsibility of a\ngroup administrator, who makes the joint decision to either store part of the\nphotovoltaic energy production and sell the remaining energy at the electricity\nspot price, or discharge part of the energy stored in the battery and sell it\nin the electricity market. Inspired by European Union and Italian legislation,\nwhich promote incentives for energy transition and renewable energy production,\nwe assume that the group receives a monetary incentive for the virtual\nself-consumed energy, defined as the minimum between the power bought from the\ngrid to satisfy the group's power demand and the energy sold to the market. In\nthis case, the energy sold by the group is a mix of part of the photovoltaic\nproduction that is not stored and part of the energy discharged from the\nbattery. We model the problem as a stochastic optimal control problem, where\nthe optimal strategy is the joint charge-discharge decision that minimizes the\ngroup's energy consumption costs. We find the solution numerically by applying\na finite difference scheme to solve the Hamilton-Jacobi-Bellman equation\nassociated with the value function of the optimal control problem.",
      "generated_abstract": "r examines the optimal management of energy storage for\nself-consumption groups in a distributed energy system. The model considers a\nnetwork of self-consuming households, each with a storage capacity that\ncoincides with the energy generation capacity of their solar panels. The\nstorage system is connected to the grid through a power-flow-controlled\nenergy-balancing network. The storage system is designed to minimize its\ncapacity-curve cost, which is defined as the sum of the marginal cost of each\nenergy unit stored and the cost of the time required for energy storage. The\nmodel is studied for a range of storage capacities and different storage\nmanagement policies. Results indicate that a cost-effective storage management\npolicy is to balance the energy flow through the energy-balancing network.\n  The analysis highlights the importance of storage management policies for\nself-consuming households. The optimal storage management policy depends on the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.32857142857142857,
          "f": 0.24864864394448508
        },
        "rouge-2": {
          "r": 0.075,
          "p": 0.12096774193548387,
          "f": 0.09259258786770334
        },
        "rouge-l": {
          "r": 0.19130434782608696,
          "p": 0.3142857142857143,
          "f": 0.23783783313367426
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.05403v2",
      "true_abstract": "We propose a decentralized framework for guaranteeing the small-signal\nstability of future power systems with grid-forming converters. Our approach\nleverages dynamic loop-shifting techniques to compensate for the lack of\npassivity in the network dynamics and establishes decentralized parametric\nstability certificates, depending on the local device-level controls and\nincorporating the effects of the network dynamics. By following practical\ntuning rules, we are able to ensure plug-and-play operation without centralized\ncoordination. Unlike prior works, our approach accommodates coupled frequency\nand voltage dynamics, incorporates network dynamics, and does not rely on\nspecific network configurations or operating points, offering a general and\nscalable solution for the integration of power-electronics-based devices into\nfuture power systems. We validate our theoretical stability results through\nnumerical case studies in a high-fidelity simulation model.",
      "generated_abstract": "This paper introduces quantitative decentralized stability certificates for\ngrids-forming converters (GFCs) to ensure the stability of the system under\nvariations in the power flow, power and frequency, and voltage. The proposed\ncertificates are based on the theory of the Lyapunov function and its\nderivatives, which are used to quantify the stability of the system. The\ncertificates are quantitative due to their properties, including monotonicity,\nconvexity, and positivity. A numerical example is presented to demonstrate the\neffectiveness of the proposed certificates.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16483516483516483,
          "p": 0.29411764705882354,
          "f": 0.2112676010305496
        },
        "rouge-2": {
          "r": 0.05042016806722689,
          "p": 0.08450704225352113,
          "f": 0.06315789005595603
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.2549019607843137,
          "f": 0.18309858694604256
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.20857v1",
      "true_abstract": "Sound event detection (SED) has significantly benefited from self-supervised\nlearning (SSL) approaches, particularly masked audio transformer for SED\n(MAT-SED), which leverages masked block prediction to reconstruct missing audio\nsegments. However, while effective in capturing global dependencies, masked\nblock prediction disrupts transient sound events and lacks explicit enforcement\nof temporal order, making it less suitable for fine-grained event boundary\ndetection. To address these limitations, we propose JiTTER (Jigsaw Temporal\nTransformer for Event Reconstruction), an SSL framework designed to enhance\ntemporal modeling in transformer-based SED. JiTTER introduces a hierarchical\ntemporal shuffle reconstruction strategy, where audio sequences are randomly\nshuffled at both the block-level and frame-level, forcing the model to\nreconstruct the correct temporal order. This pretraining objective encourages\nthe model to learn both global event structures and fine-grained transient\ndetails, improving its ability to detect events with sharp onset-offset\ncharacteristics. Additionally, we incorporate noise injection during block\nshuffle, providing a subtle perturbation mechanism that further regularizes\nfeature learning and enhances model robustness. Experimental results on the\nDESED dataset demonstrate that JiTTER outperforms MAT-SED, achieving a 5.89%\nimprovement in PSDS, highlighting the effectiveness of explicit temporal\nreasoning in SSL-based SED. Our findings suggest that structured temporal\nreconstruction tasks, rather than simple masked prediction, offer a more\neffective pretraining paradigm for sound event representation learning.",
      "generated_abstract": "nt detection (SED) is a fundamental task in audio processing,\nwith applications ranging from automatic speech recognition to audio-based\nsensing and surveillance. Traditional SED methods are heavily reliant on\nhuman-annotated event labels, and their performance is heavily dependent on\nthe quality of the labeled data. This paper introduces JiTTER, a self-supervised\nmodel that achieves state-of-the-art performance in SED without human-labeled\ndata. JiTTER leverages a novel jigsaw-like framework to generate a\ntemporal-temporal representation of audio events, enabling it to effectively\ncapture the complex temporal patterns of sound events. JiTTER further introduces\nan event-based loss function that adaptively balances the loss of different\nevents, ensuring that the model focuses on the most important events while\navoiding overfitting to the l",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2052980132450331,
          "p": 0.36470588235294116,
          "f": 0.2627118597978311
        },
        "rouge-2": {
          "r": 0.019417475728155338,
          "p": 0.034482758620689655,
          "f": 0.024844715887505194
        },
        "rouge-l": {
          "r": 0.17880794701986755,
          "p": 0.3176470588235294,
          "f": 0.2288135547130854
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2409.11451v1",
      "true_abstract": "Experiment 1. Rooting of quince hardwood cuttings: Rooting success was\ninfluenced by both the concentrations of IBA and the selection of rooting\nmedia. However, the control group (without IBA) notably enhanced rooting when\ncompared to the various IBA concentrations. Cuttings in the control group\n(without IBA) and those planted in river sand exhibited notably high\npercentages of successful rooting, underscoring the importance of the selected\nplanting medium. Experiment 2. Bench grafting of loquat: The success of\ngrafting loquat cutting stocks varied based on grafting dates, types of\ncuttings, and concentrations of IBA. However, IBA at different concentrations\ndid not have a significant impact. Notably, certain interactions such as\ngrafting on February 20 with loquat stock cuttings, yielded higher percentages\nof successful graft bud sprouting. Experiment 3. Performance of grafting\nloquats onto different rootstocks: Grafting success was notably influenced by\nthe selection of rootstock, with loquat rootstock demonstrating superior\nperformance compared to quince. The highest significant levels of successful\ngrafting were attained on February 20, underscoring the crucial role of\ngrafting dates. Experiment 4. Impact of tree stock types on grafting success:\nGrafting success percentage was higher in loquat tree stock when compared to\nquince. The consistency of grafting success percentages across three dates\nunderscores the significant influence of rootstock type. Experiment 5. Bench\ngrafting of loquat cutting stocks: Graft bud sprout percentages exhibited\nvariations, with loquat stock cuttings surpassing quince. Grafting success\ndemonstrated a consistent increase from February 20 to March 30, underscoring\nthe importance of selecting appropriate grafting dates.",
      "generated_abstract": "tibility of loquat (Eriobotrya japonica (Mill.) Kitam. & Nakai)\nscions with loquat and quince rootstocks is evaluated in this study. A total\nof 126 scions were planted on loquat and quince rootstocks and watered with\n50% and 100% distilled water. The scions were evaluated for growth parameters\n(height, diameter at breast height, leaf area, leaf area index, and number of\nleaves per shoot) and quality parameters (dry matter, acidity, total soluble\nsolids, and total phenolics) every three months for three years. The scions\ngrew well in all rootstocks and exhibited a significant positive correlation\nbetween the height, leaf area, and leaf area index of the scions and the\nrootstocks, whereas the scions showed a negative correlation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12598425196850394,
          "p": 0.21621621621621623,
          "f": 0.15920397544714254
        },
        "rouge-2": {
          "r": 0.023696682464454975,
          "p": 0.04716981132075472,
          "f": 0.03154573687348927
        },
        "rouge-l": {
          "r": 0.11811023622047244,
          "p": 0.20270270270270271,
          "f": 0.14925372669092365
        }
      }
    },
    {
      "paper_id": "cs.NE.q-fin/PM/2501.14736v1",
      "true_abstract": "In this study, we applied the NEAT (NeuroEvolution of Augmenting Topologies)\nalgorithm to stock trading using multiple technical indicators. Our approach\nfocused on maximizing earning, avoiding risk, and outperforming the Buy & Hold\nstrategy. We used progressive training data and a multi-objective fitness\nfunction to guide the evolution of the population towards these objectives. The\nresults of our study showed that the NEAT model achieved similar returns to the\nBuy & Hold strategy, but with lower risk exposure and greater stability. We\nalso identified some challenges in the training process, including the presence\nof a large number of unused nodes and connections in the model architecture. In\nfuture work, it may be worthwhile to explore ways to improve the NEAT algorithm\nand apply it to shorter interval data in order to assess the potential impact\non performance.",
      "generated_abstract": "Trading Strategy (STS) based on the Neural Algorithm of\nStochastic Optimization (NASO) is a popular and effective approach for\nautomated stock trading. However, NASO requires extensive computational power,\nmaking it infeasible for real-time applications. In this paper, we propose a\nmodified NASO based on the Neural Algorithm of Adaptive Stochastic Optimization\n(NASO-ADA), which is capable of handling real-time applications by utilizing a\nlightweight and efficient back-propagation neural network. The proposed NASO-ADA\nis able to enhance the trading efficiency by automatically adapting to\ndifferent market conditions. Furthermore, it incorporates a resonance mechanism\nto mitigate the effects of overfitting and provide more stable trading results.\nWe evaluate the effectiveness of the proposed NASO-ADA strategy using",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18556701030927836,
          "p": 0.22784810126582278,
          "f": 0.20454544959775323
        },
        "rouge-2": {
          "r": 0.023076923076923078,
          "p": 0.028846153846153848,
          "f": 0.02564102070275499
        },
        "rouge-l": {
          "r": 0.16494845360824742,
          "p": 0.20253164556962025,
          "f": 0.1818181768704805
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2503.01053v1",
      "true_abstract": "Each period, two players bargain over a unit of surplus. Each player chooses\nbetween remaining flexible and committing to a take-it-or-leave-it offer at a\ncost. If players' committed demands are incompatible, then the current-period\nsurplus is destroyed in the conflict. When both players are flexible, the\nsurplus is split according to the status quo, which is the division in the last\nperiod where there was no conflict. We show that when players are patient and\nthe cost of commitment is small, there exist a class of symmetric Markov\nPerfect equilibria that are asymptotically efficient and renegotiation proof,\nin which players commit to fair demands in almost all periods.",
      "generated_abstract": "aper, we analyze the equilibrium and stability of bargaining games\nunder commitment, where the bargainer has an obligation to commit to a\npreviously agreed-upon outcome. We show that, under certain conditions, this\ngame can have a unique Nash equilibrium, which is characterized by a unique\nstrategy profile and a unique Nash-optimal outcome. We then consider a model of\nconflict where the bargainer and counterparties are not mutually committed to\na specific outcome but have a common objective of achieving a desired outcome.\nWe show that, under certain conditions, the equilibrium outcome can be\ncharacterized by a unique Nash equilibrium, which is also characterized by a\nunique strategy profile and a unique Nash-optimal outcome. We then consider a\nmodel of status quo bargaining where the bargainer and counterparties are not\nmutually committed to a specific outcome but have a common objective",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.26229508196721313,
          "f": 0.24060149879360065
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.047058823529411764,
          "f": 0.04232803737857341
        },
        "rouge-l": {
          "r": 0.19444444444444445,
          "p": 0.22950819672131148,
          "f": 0.21052631082367584
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2501.12841v1",
      "true_abstract": "This study systematically examines how several alternative approaches\nconsidered affect three aspects that determine portfolio performance (the gross\nreturn, the transaction costs and the portfolio risk). We find that it is\ndifficult to exploit the possible predictability of asset returns. However, the\npredictability of asset return volatility produces obvious economic value,\nalthough in a highly correlated cryptocurrencies market.",
      "generated_abstract": "We study the performance of the naive 1/N strategy, which invests 1% of\nfunds in each asset class, in a highly correlated market. Our empirical\nanalysis shows that the strategy outperforms the optimal diversification\napproach by up to 50% in terms of annualized return. However, we find that the\ndifference between the two strategies is driven by the fact that the naive\nstrategy is more sensitive to extreme events, such as sharp corrections in the\nmarket. Consequently, it performs worse in volatile markets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.36,
          "p": 0.29508196721311475,
          "f": 0.32432431937342754
        },
        "rouge-2": {
          "r": 0.07272727272727272,
          "p": 0.05063291139240506,
          "f": 0.0597014876977059
        },
        "rouge-l": {
          "r": 0.34,
          "p": 0.2786885245901639,
          "f": 0.30630630135540954
        }
      }
    },
    {
      "paper_id": "math.OC.econ/TH/2502.11780v2",
      "true_abstract": "This paper studies distributionally robust optimization for a large class of\nrisk measures with ambiguity sets defined by $\\phi$-divergences. The risk\nmeasures are allowed to be non-linear in probabilities, are represented by a\nChoquet integral possibly induced by a probability weighting function, and\ninclude many well-known examples (for example, CVaR, Mean-Median Deviation,\nGini-type). Optimization for this class of robust risk measures is challenging\ndue to their rank-dependent nature. We show that for many types of probability\nweighting functions including concave, convex and inverse $S$-shaped, the\nrobust optimization problem can be reformulated into a rank-independent\nproblem. In the case of a concave probability weighting function, the problem\ncan be further reformulated into a convex optimization problem with finitely\nmany constraints that admits explicit conic representability for a collection\nof canonical examples. While the number of constraints in general scales\nexponentially with the dimension of the state space, we circumvent this\ndimensionality curse and provide two types of upper and lower bounds\nalgorithms. They yield tight upper and lower bounds on the exact optimal value\nand are formally shown to converge asymptotically. This is illustrated\nnumerically in two examples given by a robust newsvendor problem and a robust\nportfolio choice problem.",
      "generated_abstract": "er the problem of optimizing a rank-dependent model over a set of\nprobabilities. The model is rank-dependent in that the probability of a\nparticular rank is not known but is given by an unknown function of the\nprobabilities. The rank-dependent model is a linear regression model where the\nprobabilities are used as coefficients. We derive the optimality conditions for\nthe rank-dependent model and show that they are linear. We then show that\noptimality is attained when the model is given by a rank-dependent linear\nregression model. Finally, we establish that the optimality conditions are\nrobust in the sense that they remain valid if the rank-dependent model is\nsub-optimal. We prove that, under mild assumptions, the optimality conditions\nare satisfied by the rank-dependent linear regression model and that, under\nadditional assumptions, the optimality conditions are satisfied by the rank-\ndependent linear regression model and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1623931623931624,
          "p": 0.3064516129032258,
          "f": 0.21229049826534763
        },
        "rouge-2": {
          "r": 0.033707865168539325,
          "p": 0.057692307692307696,
          "f": 0.04255318683366079
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.2903225806451613,
          "f": 0.20111731390780574
        }
      }
    },
    {
      "paper_id": "stat.AP.physics/ao-ph/2503.09065v1",
      "true_abstract": "Contributions from photosynthesis and other natural components of the carbon\ncycle present the largest uncertainties in our understanding of carbon dioxide\n(CO$_2$) sources and sinks. While the global spatiotemporal distribution of the\nnet flux (the sum of all contributions) can be inferred from atmospheric CO$_2$\nconcentrations through flux inversion, attributing the net flux to its\nindividual components remains challenging. The advent of solar-induced\nfluorescence (SIF) satellite observations provides an opportunity to isolate\nnatural components by anchoring gross primary productivity (GPP), the\nphotosynthetic component of the net flux. Here, we introduce a novel\nstatistical flux-inversion framework that simultaneously assimilates\nobservations of SIF and CO$_2$ concentration, extending WOMBAT v2.0 (WOllongong\nMethodology for Bayesian Assimilation of Trace-gases, version 2.0) with a\nhierarchical model of spatiotemporal dependence between GPP and SIF processes.\nWe call the new framework WOMBAT v2.S, and we apply it to SIF and CO$_2$ data\nfrom NASA's Orbiting Carbon Observatory-2 (OCO-2) satellite and other\ninstruments to estimate natural fluxes over the globe during a recent six-year\nperiod. In a simulation experiment that matches OCO-2's retrieval\ncharacteristics, the inclusion of SIF improves accuracy and uncertainty\nquantification of component flux estimates. Comparing estimates from WOMBAT\nv2.S, v2.0, and the independent FLUXCOM initiative, we observe that linking GPP\nto SIF has little effect on net flux, as expected, but leads to spatial\nredistribution and more realistic seasonal structure in natural flux\ncomponents.",
      "generated_abstract": "t a Bayesian inversion framework for attributing global CO$_2$\nflux components to their primary sources and sinks, a key step in understanding\nthe historical evolution of the carbon cycle. The framework is based on\nmultivariate Gaussian processes, and is designed to integrate information from\nmultiple datasets, including the Atmospheric Radiation Measurement (ARM)\nsurface flux tower network, the Community Earth System Model (CESM) and the\nGlobal Carbon Budget (GCB). By explicitly modeling the uncertainty in the\ninversion, WOMBATv2.S addresses key challenges in data assimilation in the\ncarbon cycle, including the need for robust uncertainty quantification, the\nneed to model uncertainty in the forcings, and the need for accurate\nuncertainty quantification for the carbon cycle. The framework is implemented\nin a Python package, which is publicly available at\nhttps://",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1830065359477124,
          "p": 0.3333333333333333,
          "f": 0.23628691525503398
        },
        "rouge-2": {
          "r": 0.03225806451612903,
          "p": 0.061946902654867256,
          "f": 0.04242423792084529
        },
        "rouge-l": {
          "r": 0.1568627450980392,
          "p": 0.2857142857142857,
          "f": 0.20253164099343063
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2503.10132v1",
      "true_abstract": "This paper analyzes Shinohara Rock-Paper-Scissors (RPS), a variant of the\nclassic RPS game introduced by board game designer Yoshiteru Shinohara. In this\ngame, players compete against a host who always plays rock, so players choose\neither rock or paper. The catch is that if two or more players choose paper,\nthey are eliminated, creating strategic tension among the players. The last\nremaining player wins. We derive subgame perfect equilibria (SPE) of Shinohara\nRPS. A unique symmetric SPE exists, in which the probability of choosing paper\nsatisfies the equation $(1-p)^{n-1} + p^{n-1}/n = 1/n$. The game also admits a\ncontinuum of asymmetric SPE, making it unlikely that any specific SPE will be\nobserved in actual play.",
      "generated_abstract": "We study a market in which buyers and sellers can be any of five agents,\nand each agent is capable of playing two of three strategies: rock, paper,\nscissors. Buyers and sellers can exchange information about each other, but\nthey cannot share their private information. This model provides a simple\nillustration of the robustness of the Nash equilibrium to information\nrevelation, and provides a model for the \"Nash equilibrium in a market with\ninformation revelation\" in the literature on two-player games with information\nrevelation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15555555555555556,
          "p": 0.24561403508771928,
          "f": 0.19047618572816893
        },
        "rouge-2": {
          "r": 0.017699115044247787,
          "p": 0.025974025974025976,
          "f": 0.021052626758449856
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.21052631578947367,
          "f": 0.16326530137442746
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.16574v1",
      "true_abstract": "Algorithmic agents are used in a variety of competitive decision settings,\nnotably in making pricing decisions in contexts that range from online retail\nto residential home rentals. Business managers, algorithm designers, legal\nscholars, and regulators alike are all starting to consider the ramifications\nof \"algorithmic collusion.\" We study the emergent behavior of multi-armed\nbandit machine learning algorithms used in situations where agents are\ncompeting, but they have no information about the strategic interaction they\nare engaged in. Using a general-form repeated Prisoner's Dilemma game, agents\nengage in online learning with no prior model of game structure and no\nknowledge of competitors' states or actions (e.g., no observation of competing\nprices). We show that these context-free bandits, with no knowledge of\nopponents' choices or outcomes, still will consistently learn collusive\nbehavior - what we call \"naive collusion.\" We primarily study this system\nthrough an analytical model and examine perturbations to the model through\nsimulations.\n  Our findings have several notable implications for regulators. First, calls\nto limit algorithms from conditioning on competitors' prices are insufficient\nto prevent algorithmic collusion. This is a direct result of collusion arising\neven in the naive setting. Second, symmetry in algorithms can increase\ncollusion potential. This highlights a new, simple mechanism for\n\"hub-and-spoke\" algorithmic collusion. A central distributor need not imbue its\nalgorithm with supra-competitive tendencies for apparent collusion to arise; it\ncan simply arise by using certain (common) machine learning algorithms.\nFinally, we highlight that collusive outcomes depend starkly on the specific\nalgorithm being used, and we highlight market and algorithmic conditions under\nwhich it will be unknown a priori whether collusion occurs.",
      "generated_abstract": "We study the problem of learning to optimize a bandit algorithm from a single\noptimizer, under the assumption that the algorithm is naive and performs a\nsimple (but arbitrary) optimization of the observed reward. We characterize\nthe optimal bandit learning algorithm, and we show that, under mild assumptions\non the learning algorithm, the naive algorithm can never outperform any\noptimal bandit learning algorithm in terms of the expected reward obtained from\nthe optimal algorithm. We then extend our analysis to the case where the\nalgorithm is allowed to cooperate with an adversary, and we show that the\ncooperative naive algorithm can outperform any optimal bandit learning\nalgorithm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13333333333333333,
          "p": 0.42857142857142855,
          "f": 0.2033898268888251
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.07865168539325842,
          "f": 0.04022988125066095
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.42857142857142855,
          "f": 0.2033898268888251
        }
      }
    },
    {
      "paper_id": "cs.SD.cs/SD/2503.06984v1",
      "true_abstract": "Video-to-audio generation is essential for synthesizing realistic audio\ntracks that synchronize effectively with silent videos. Following the\nperspective of extracting essential signals from videos that can precisely\ncontrol the mature text-to-audio generative diffusion models, this paper\npresents how to balance the representation of mel-spectrograms in terms of\ncompleteness and complexity through a new approach called Mel\nQuantization-Continuum Decomposition (Mel-QCD). We decompose the\nmel-spectrogram into three distinct types of signals, employing quantization or\ncontinuity to them, we can effectively predict them from video by a devised\nvideo-to-all (V2X) predictor. Then, the predicted signals are recomposed and\nfed into a ControlNet, along with a textual inversion design, to control the\naudio generation process. Our proposed Mel-QCD method demonstrates\nstate-of-the-art performance across eight metrics, evaluating dimensions such\nas quality, synchronization, and semantic consistency. Our codes and demos will\nbe released at \\href{Website}{https://wjc2830.github.io/MelQCD/}.",
      "generated_abstract": "audio (V2A) generation is a crucial task for multimedia\ngenerative models, yet existing approaches typically generate audio clips as\nsingle-channel signals or audio-only features, which are limited in both\nperformance and flexibility. To address this challenge, we propose a novel\nSynchronized Video-to-Audio Generation (SV2A) framework, which introduces a\nmel-quantization-continuum decomposition (M-Q-C-D) module to quantize audio\nfeatures and decompose them into continuous mel bands. The M-Q-C-D module\nsynergizes the temporal consistency of mel bands with the discrete nature of\naudio signals, which enhances the representation capacity of the audio\nfeatures and improves the generation quality. To effectively capture the\ntemporal dynamics of audio, we further introduce a temporal alignment module\nthat estimates the temporal alignment of audio features. Extensive experiments\non",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.32941176470588235,
          "f": 0.28426395448478453
        },
        "rouge-2": {
          "r": 0.02158273381294964,
          "p": 0.02702702702702703,
          "f": 0.023999995062721016
        },
        "rouge-l": {
          "r": 0.20535714285714285,
          "p": 0.27058823529411763,
          "f": 0.23350253316498762
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.09762v1",
      "true_abstract": "We study a centralized discrete-time dynamic two-way matching model with\nfinitely many agent types. Agents arrive stochastically over time and join\ntheir type-dedicated queues waiting to be matched. We focus on\nstate-independent greedy policies that achieve constant regret at all times by\nmaking matching decisions based solely on agent availability across types,\nrather than requiring complete queue-length information. Such policies are\nparticularly appealing for life-saving applications such as kidney exchange, as\nthey require less information and provide more transparency compared to\nstate-dependent policies.\n  First, for acyclic matching networks, we analyze a deterministic priority\npolicy proposed by Kerimov et al. [2023] that follows a static priority order\nover matches. We derive the first explicit regret bound in terms of the general\nposition gap (GPG) parameter $\\epsilon$, which measures the distance of the\nfluid relaxation from degeneracy. Second, for general two-way matching\nnetworks, we design a randomized state-independent greedy policy that achieves\nconstant regret with optimal scaling $O(\\epsilon^{-1})$, matching the existing\nlower bound established by Kerimov et al. [2024].",
      "generated_abstract": "In this paper, we consider the dynamic matching problem with state-independent\npolicies and show that a class of policies achieves a constant regret. We show\nthat the regret is achieved by a class of policies that can be expressed as\npolicies in the class of state-independent policies. We also show that the\npolicies in the class of state-independent policies are the optimal\npolicies in the class of policies that can be expressed as state-independent\npolicies.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1721311475409836,
          "p": 0.6363636363636364,
          "f": 0.27096773858397505
        },
        "rouge-2": {
          "r": 0.01910828025477707,
          "p": 0.061224489795918366,
          "f": 0.029126209966538297
        },
        "rouge-l": {
          "r": 0.1721311475409836,
          "p": 0.6363636363636364,
          "f": 0.27096773858397505
        }
      }
    },
    {
      "paper_id": "math.AT.math/AT/2503.01614v1",
      "true_abstract": "Recently, bipath persistent homology has been proposed as an extension of\nstandard persistent homology, along with its visualization (bipath persistence\ndiagram) and computational methods. In the setting of standard persistent\nhomology, the stability theorem with respect to real-valued functions on a\ntopological space is one of the fundamental results, which gives a mathematical\njustification for using persistent homology to noisy data. In proving the\nstability theorem, the algebraic stability theorem/the isometry theorem for\npersistence modules plays a central role. In this point of view, the stability\nproperty for bipath persistent homology is desired for analyzing data. In this\npaper, we prove the stability theorem of bipath persistent homology with\nrespect to bipath functions on a topological space. This theorem suggests a\nstability of bipath persistence diagrams: small changes in a bipath function\n(except at their ends) result in only small changes in the bipath persistence\ndiagram. Similar to the stability theorem of standard persistent homology, we\ndeduce the stability theorem of bipath persistent homology by using the\nalgebraic stability theorem/the isometry theorem of bipath persistence modules.",
      "generated_abstract": "rsistence diagrams (BPDs) are a versatile tool for studying\nstability in dynamical systems. In this paper, we extend the notion of\npersistence to BPDs and propose a notion of stability of BPDs. We show that\nstability of BPDs is invariant under scaling and translation of the underlying\nspace. Moreover, we show that the persistence of a point in a BPD can be\ndescribed by the persistence of a point in its underlying space. Furthermore,\nwe prove that a BPD with one persistence is equivalent to a path in the\ntopological space of all paths in the underlying space. We also give conditions\non the topology of the underlying space that guarantee that a BPD is\npersistence. Furthermore, we show that a BPD can be decomposed into a\ncertain number of persistence diagrams. Finally, we show that BPDs are\nequivalent to persistence of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27710843373493976,
          "p": 0.359375,
          "f": 0.31292516515155727
        },
        "rouge-2": {
          "r": 0.06923076923076923,
          "p": 0.08181818181818182,
          "f": 0.07499999503472256
        },
        "rouge-l": {
          "r": 0.2289156626506024,
          "p": 0.296875,
          "f": 0.25850339644407433
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/TR/2412.19245v1",
      "true_abstract": "We investigate the efficacy of large language models (LLMs) in sentiment\nanalysis of U.S. financial news and their potential in predicting stock market\nreturns. We analyze a dataset comprising 965,375 news articles that span from\nJanuary 1, 2010, to June 30, 2023; we focus on the performance of various LLMs,\nincluding BERT, OPT, FINBERT, and the traditional Loughran-McDonald dictionary\nmodel, which has been a dominant methodology in the finance literature. The\nstudy documents a significant association between LLM scores and subsequent\ndaily stock returns. Specifically, OPT, which is a GPT-3 based LLM, shows the\nhighest accuracy in sentiment prediction with an accuracy of 74.4%, slightly\nahead of BERT (72.5%) and FINBERT (72.2%). In contrast, the Loughran-McDonald\ndictionary model demonstrates considerably lower effectiveness with only 50.1%\naccuracy. Regression analyses highlight a robust positive impact of OPT model\nscores on next-day stock returns, with coefficients of 0.274 and 0.254 in\ndifferent model specifications. BERT and FINBERT also exhibit predictive\nrelevance, though to a lesser extent. Notably, we do not observe a significant\nrelationship between the Loughran-McDonald dictionary model scores and stock\nreturns, challenging the efficacy of this traditional method in the current\nfinancial context. In portfolio performance, the long-short OPT strategy excels\nwith a Sharpe ratio of 3.05, compared to 2.11 for BERT and 2.07 for FINBERT\nlong-short strategies. Strategies based on the Loughran-McDonald dictionary\nyield the lowest Sharpe ratio of 1.23. Our findings emphasize the superior\nperformance of advanced LLMs, especially OPT, in financial market prediction\nand portfolio management, marking a significant shift in the landscape of\nfinancial analysis tools with implications to financial regulation and policy\nanalysis.",
      "generated_abstract": "the use of Large Language Models (LLMs) in sentiment analysis,\nusing the Transformer architecture. Specifically, we focus on the use of\nlarge language models in the trading process, with the aim of predicting stock\nprices based on textual data. We introduce the Stock Sentiment Model (SSM), a\nmultilingual model trained on stock sentiment data in 13 different languages,\nincluding English. We demonstrate that the SSM can successfully predict stock\nprices using only textual data, achieving an average root mean squared error\n(RMSE) of 0.72 and a median absolute error (MAE) of 0.77. This model can be\nused to trade stocks based on textual data, with the potential to reduce\ncosts and improve trading efficiency. The SSM also shows promise as a\ncomplementary tool to traditional sentiment analysis techniques, such as",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19879518072289157,
          "p": 0.36666666666666664,
          "f": 0.2578124954406739
        },
        "rouge-2": {
          "r": 0.050980392156862744,
          "p": 0.10569105691056911,
          "f": 0.06878306439279444
        },
        "rouge-l": {
          "r": 0.1927710843373494,
          "p": 0.35555555555555557,
          "f": 0.2499999954406739
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.12116v1",
      "true_abstract": "Do home prices incorporate flood risk in the immediate aftermath of specific\nflood events, or is it the repeated exposure over the years that plays a more\nsignificant role? We address this question through the first systematic study\nof the Italian housing market, which is an ideal case study because it is\nhighly exposed to floods, though unevenly distributed across the national\nterritory. Using a novel dataset containing about 550,000 mortgage-financed\ntransactions between 2016 and 2024, as well as hedonic regressions and a\ndifference-in-difference design, we find that: (i) specific floods do not\ndecrease home prices in areas at risk; (ii) the repeated exposure to floods in\nflood-prone areas leads to a price decline, up to 4\\% in the most frequently\nflooded regions; (iii) responses are heterogeneous by buyers' income and age.\nYoung buyers (with limited exposure to prior floods) do not obtain any price\nreduction for settling in risky areas, while experienced buyers do. At the same\ntime, buyers who settle in risky areas have lower incomes than buyers in safe\nareas in the most affected regions. Our results emphasize the importance of\ncultural and institutional factors in understanding how flood risk affects the\nhousing market and socioeconomic outcomes.",
      "generated_abstract": "r examines the impact of historical floods on the Italian housing\nmarket, using a panel dataset of 51,040 housing units and 3,421 floods in the\nyear 2022. The results indicate that, over the last two decades, floods have\nbeen associated with a decline in the price of the houses affected by the\ndisaster. This effect is more pronounced in the Lombardy region, where the\nfloods occurred twice during the period under study. The results also show that\nthe impact of floods on the price of affected houses is stronger for houses\nlocated near the river than for houses located far from the river, suggesting\nthat floods have a differential effect depending on their proximity to the\nriver. Finally, we find that the price impact of floods is higher for the\nfirst-time buyers of affected houses, suggesting that floods may reduce the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18571428571428572,
          "p": 0.32098765432098764,
          "f": 0.23529411300341932
        },
        "rouge-2": {
          "r": 0.042328042328042326,
          "p": 0.06666666666666667,
          "f": 0.05177993052439796
        },
        "rouge-l": {
          "r": 0.17142857142857143,
          "p": 0.2962962962962963,
          "f": 0.21719456549210714
        }
      }
    },
    {
      "paper_id": "math.HO.math/HO/2502.17533v1",
      "true_abstract": "The constant $\\pi$ has fascinated scholars for centuries, inspiring the\nderivation of countless formulas rooted in profound mathematical insight. This\nabundance of formulas raises a question: Are they interconnected, and can a\nunifying structure explain their relationships?\n  We propose a systematic methodology for discovering and proving formula\nequivalences, leveraging modern large language models, large-scale data\nprocessing, and novel mathematical algorithms. Analyzing 457,145 arXiv papers,\nover a third of the validated formulas for $\\pi$ were proven to be derivable\nfrom a single mathematical object - including formulas by Euler, Gauss, Lord\nBrouncker, and newer ones from algorithmic discoveries by the Ramanujan\nMachine.\n  Our approach extends to other constants, such as $e$, $\\zeta(3)$, and\nCatalan's constant, proving its broad applicability. This work represents a\nstep toward the automatic unification of mathematical knowledge, laying a\nfoundation for AI-driven discoveries of connections across scientific domains.",
      "generated_abstract": "h for the fundamental constants of physics has spanned millennia.\nInspired by the success of artificial intelligence in machine learning, we propose\na unifying framework for all the fundamental constants, which can be viewed as\nvarious representations of a general formula for any of them. We develop this\nframework using the idea of \"molecules of knowledge\" as a metaphor, and prove\nthat any formula for a fundamental constant can be obtained from a formula for\nany other fundamental constant by a series of algebraic operations,\nmathematical transformations, and mathematical manipulations. This proof is\nsimple and intuitive, and can be applied to any formula for a fundamental\nconstant, including those for the fundamental constants of gravity,\nelectromagnetism, and the fine-structure constant. We apply this framework to\nsolve the problem of finding the value of the fundamental constant of\ngravitation, which has been a subject of intense debate for more",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.2962962962962963,
          "f": 0.25396824907029486
        },
        "rouge-2": {
          "r": 0.04285714285714286,
          "p": 0.046511627906976744,
          "f": 0.04460966043587071
        },
        "rouge-l": {
          "r": 0.2037037037037037,
          "p": 0.2716049382716049,
          "f": 0.2328042279062737
        }
      }
    }
  ]
}